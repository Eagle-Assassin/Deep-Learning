{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f9b369",
   "metadata": {},
   "source": [
    "# Predicting Chess Game Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec065bc6",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a00f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim  as optim\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6351132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for processing\n"
     ]
    }
   ],
   "source": [
    "#Select GPu if available for processing\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using {device} for processing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8370aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "df = pd.read_csv('matmob_data_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fe77ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print shape and one example row\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a181de92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w.mat.mean</th>\n",
       "      <th>w.mat.sd</th>\n",
       "      <th>b.mat.mean</th>\n",
       "      <th>b.mat.sd</th>\n",
       "      <th>w.mob.mean</th>\n",
       "      <th>w.mob.sd</th>\n",
       "      <th>b.mob.mean</th>\n",
       "      <th>b.mob.sd</th>\n",
       "      <th>result</th>\n",
       "      <th>valid.games.row</th>\n",
       "      <th>w.mat.vector</th>\n",
       "      <th>b.mat.vector</th>\n",
       "      <th>w.mob.vector</th>\n",
       "      <th>b.mob.vector</th>\n",
       "      <th>pgn</th>\n",
       "      <th>half.moves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.166667</td>\n",
       "      <td>8.850414</td>\n",
       "      <td>28.135417</td>\n",
       "      <td>9.671031</td>\n",
       "      <td>39.3125</td>\n",
       "      <td>12.951885</td>\n",
       "      <td>27.125</td>\n",
       "      <td>11.296064</td>\n",
       "      <td>1-0</td>\n",
       "      <td>1343406</td>\n",
       "      <td>39, 39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38...</td>\n",
       "      <td>39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38, 38...</td>\n",
       "      <td>20, 28, 30, 32, 30, 40, 40, 45, 49, 47, 49, 51...</td>\n",
       "      <td>20, 22, 28, 34, 34, 35, 31, 32, 33, 30, 30, 28...</td>\n",
       "      <td>1.d4 Nf6 2.c4 e6 3.Nf3 d5 4.cxd5 exd5 5.Qc2 Be...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w.mat.mean  w.mat.sd  b.mat.mean  b.mat.sd  w.mob.mean   w.mob.sd  \\\n",
       "0   28.166667  8.850414   28.135417  9.671031     39.3125  12.951885   \n",
       "\n",
       "   b.mob.mean   b.mob.sd result  valid.games.row  \\\n",
       "0      27.125  11.296064    1-0          1343406   \n",
       "\n",
       "                                        w.mat.vector  \\\n",
       "0  39, 39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38...   \n",
       "\n",
       "                                        b.mat.vector  \\\n",
       "0  39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38, 38...   \n",
       "\n",
       "                                        w.mob.vector  \\\n",
       "0  20, 28, 30, 32, 30, 40, 40, 45, 49, 47, 49, 51...   \n",
       "\n",
       "                                        b.mob.vector  \\\n",
       "0  20, 22, 28, 34, 34, 35, 31, 32, 33, 30, 30, 28...   \n",
       "\n",
       "                                                 pgn  half.moves  \n",
       "0  1.d4 Nf6 2.c4 e6 3.Nf3 d5 4.cxd5 exd5 5.Qc2 Be...          95  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad82fbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w.mat.mean</th>\n",
       "      <th>w.mat.sd</th>\n",
       "      <th>b.mat.mean</th>\n",
       "      <th>b.mat.sd</th>\n",
       "      <th>w.mob.mean</th>\n",
       "      <th>w.mob.sd</th>\n",
       "      <th>b.mob.mean</th>\n",
       "      <th>b.mob.sd</th>\n",
       "      <th>valid.games.row</th>\n",
       "      <th>half.moves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.471540</td>\n",
       "      <td>7.518266</td>\n",
       "      <td>29.456648</td>\n",
       "      <td>7.556172</td>\n",
       "      <td>33.083914</td>\n",
       "      <td>9.300182</td>\n",
       "      <td>30.808188</td>\n",
       "      <td>8.824375</td>\n",
       "      <td>8.475512e+05</td>\n",
       "      <td>80.524530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.700711</td>\n",
       "      <td>3.590136</td>\n",
       "      <td>5.722296</td>\n",
       "      <td>3.608866</td>\n",
       "      <td>4.899840</td>\n",
       "      <td>2.659605</td>\n",
       "      <td>4.488267</td>\n",
       "      <td>2.583139</td>\n",
       "      <td>4.892308e+05</td>\n",
       "      <td>33.794235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.768683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.127049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.312500</td>\n",
       "      <td>1.707825</td>\n",
       "      <td>12.272727</td>\n",
       "      <td>2.387467</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.206546</td>\n",
       "      <td>4.674827</td>\n",
       "      <td>25.148607</td>\n",
       "      <td>4.691699</td>\n",
       "      <td>29.653061</td>\n",
       "      <td>7.244109</td>\n",
       "      <td>27.731616</td>\n",
       "      <td>6.879109</td>\n",
       "      <td>4.246160e+05</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.317267</td>\n",
       "      <td>7.677324</td>\n",
       "      <td>30.308824</td>\n",
       "      <td>7.716147</td>\n",
       "      <td>33.390244</td>\n",
       "      <td>9.153638</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>8.740191</td>\n",
       "      <td>8.492815e+05</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>10.550320</td>\n",
       "      <td>34.017938</td>\n",
       "      <td>10.608270</td>\n",
       "      <td>36.700000</td>\n",
       "      <td>11.164435</td>\n",
       "      <td>33.978723</td>\n",
       "      <td>10.618308</td>\n",
       "      <td>1.268899e+06</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>16.043718</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>16.394856</td>\n",
       "      <td>50.842105</td>\n",
       "      <td>21.449271</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>20.940381</td>\n",
       "      <td>1.696627e+06</td>\n",
       "      <td>369.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          w.mat.mean       w.mat.sd     b.mat.mean       b.mat.sd  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       29.471540       7.518266      29.456648       7.556172   \n",
       "std         5.700711       3.590136       5.722296       3.608866   \n",
       "min         7.768683       0.000000       9.127049       0.000000   \n",
       "25%        25.206546       4.674827      25.148607       4.691699   \n",
       "50%        30.317267       7.677324      30.308824       7.716147   \n",
       "75%        34.000000      10.550320      34.017938      10.608270   \n",
       "max        39.000000      16.043718      39.000000      16.394856   \n",
       "\n",
       "          w.mob.mean       w.mob.sd     b.mob.mean       b.mob.sd  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       33.083914       9.300182      30.808188       8.824375   \n",
       "std         4.899840       2.659605       4.488267       2.583139   \n",
       "min        11.312500       1.707825      12.272727       2.387467   \n",
       "25%        29.653061       7.244109      27.731616       6.879109   \n",
       "50%        33.390244       9.153638      31.000000       8.740191   \n",
       "75%        36.700000      11.164435      33.978723      10.618308   \n",
       "max        50.842105      21.449271      48.250000      20.940381   \n",
       "\n",
       "       valid.games.row     half.moves  \n",
       "count     1.000000e+05  100000.000000  \n",
       "mean      8.475512e+05      80.524530  \n",
       "std       4.892308e+05      33.794235  \n",
       "min       7.000000e+00       6.000000  \n",
       "25%       4.246160e+05      58.000000  \n",
       "50%       8.492815e+05      77.000000  \n",
       "75%       1.268899e+06     100.000000  \n",
       "max       1.696627e+06     369.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f796a2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w.mat.mean         float64\n",
       "w.mat.sd           float64\n",
       "b.mat.mean         float64\n",
       "b.mat.sd           float64\n",
       "w.mob.mean         float64\n",
       "w.mob.sd           float64\n",
       "b.mob.mean         float64\n",
       "b.mob.sd           float64\n",
       "result              object\n",
       "valid.games.row      int64\n",
       "w.mat.vector        object\n",
       "b.mat.vector        object\n",
       "w.mob.vector        object\n",
       "b.mob.vector        object\n",
       "pgn                 object\n",
       "half.moves           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba23078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result\n",
       "1-0        36485\n",
       "1/2-1/2    35810\n",
       "0-1        27705\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Distribution of the result variable using value_counts()\n",
    "df['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40cd370a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#A labeled bar chart of the result distribution\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "data=df['result'].value_counts().index\n",
    "values=list(df['result'].value_counts())\n",
    "ax = sns.barplot(x=data,y=values,palette='pastel', edgecolor='black')\n",
    "\n",
    "# Add labels on top of bars\n",
    "for i, row in enumerate(data):\n",
    "    ax.text(i, float(values[i])+ 0.1, values[i], ha='center', va='bottom')\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Result Distribution')\n",
    "plt.xlabel('Result')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bdfcc8",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5237225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w.mat.mean</th>\n",
       "      <th>b.mat.mean</th>\n",
       "      <th>w.mob.mean</th>\n",
       "      <th>b.mob.mean</th>\n",
       "      <th>half.moves</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.166667</td>\n",
       "      <td>28.135417</td>\n",
       "      <td>39.312500</td>\n",
       "      <td>27.125000</td>\n",
       "      <td>95</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.851852</td>\n",
       "      <td>33.703704</td>\n",
       "      <td>37.185185</td>\n",
       "      <td>33.074074</td>\n",
       "      <td>53</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.301587</td>\n",
       "      <td>31.920635</td>\n",
       "      <td>32.625000</td>\n",
       "      <td>37.451613</td>\n",
       "      <td>62</td>\n",
       "      <td>0-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.878261</td>\n",
       "      <td>23.260870</td>\n",
       "      <td>27.172414</td>\n",
       "      <td>32.035088</td>\n",
       "      <td>114</td>\n",
       "      <td>0-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.775510</td>\n",
       "      <td>34.959184</td>\n",
       "      <td>41.040000</td>\n",
       "      <td>32.958333</td>\n",
       "      <td>48</td>\n",
       "      <td>1/2-1/2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w.mat.mean  b.mat.mean  w.mob.mean  b.mob.mean  half.moves   result\n",
       "0   28.166667   28.135417   39.312500   27.125000          95      1-0\n",
       "1   33.851852   33.703704   37.185185   33.074074          53      1-0\n",
       "2   31.301587   31.920635   32.625000   37.451613          62      0-1\n",
       "3   21.878261   23.260870   27.172414   32.035088         114      0-1\n",
       "4   34.775510   34.959184   41.040000   32.958333          48  1/2-1/2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retain only the following 6 columns\n",
    "new_df=df[['w.mat.mean', 'b.mat.mean', 'w.mob.mean', 'b.mob.mean', 'half.moves', 'result']].copy()\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c4fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: w.mat.mean, dtype: float64)\n",
      "Series([], Name: b.mat.mean, dtype: float64)\n",
      "Series([], Name: w.mob.mean, dtype: float64)\n",
      "Series([], Name: b.mob.mean, dtype: float64)\n",
      "Series([], Name: half.moves, dtype: int64)\n",
      "Series([], Name: result, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "#Looking for nul values and empty values\n",
    "for cols in new_df.columns:\n",
    "    print(new_df[cols].loc[new_df[cols].isna()==True])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bad2aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the shape of new data \n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2da34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w.mat.mean    float64\n",
       "b.mat.mean    float64\n",
       "w.mob.mean    float64\n",
       "b.mob.mean    float64\n",
       "half.moves      int64\n",
       "result         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "494acda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w.mat.mean</th>\n",
       "      <th>b.mat.mean</th>\n",
       "      <th>w.mob.mean</th>\n",
       "      <th>b.mob.mean</th>\n",
       "      <th>half.moves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.471540</td>\n",
       "      <td>29.456648</td>\n",
       "      <td>33.083914</td>\n",
       "      <td>30.808188</td>\n",
       "      <td>80.524530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.700711</td>\n",
       "      <td>5.722296</td>\n",
       "      <td>4.899840</td>\n",
       "      <td>4.488267</td>\n",
       "      <td>33.794235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.768683</td>\n",
       "      <td>9.127049</td>\n",
       "      <td>11.312500</td>\n",
       "      <td>12.272727</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.206546</td>\n",
       "      <td>25.148607</td>\n",
       "      <td>29.653061</td>\n",
       "      <td>27.731616</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.317267</td>\n",
       "      <td>30.308824</td>\n",
       "      <td>33.390244</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.017938</td>\n",
       "      <td>36.700000</td>\n",
       "      <td>33.978723</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>50.842105</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>369.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          w.mat.mean     b.mat.mean     w.mob.mean     b.mob.mean  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       29.471540      29.456648      33.083914      30.808188   \n",
       "std         5.700711       5.722296       4.899840       4.488267   \n",
       "min         7.768683       9.127049      11.312500      12.272727   \n",
       "25%        25.206546      25.148607      29.653061      27.731616   \n",
       "50%        30.317267      30.308824      33.390244      31.000000   \n",
       "75%        34.000000      34.017938      36.700000      33.978723   \n",
       "max        39.000000      39.000000      50.842105      48.250000   \n",
       "\n",
       "          half.moves  \n",
       "count  100000.000000  \n",
       "mean       80.524530  \n",
       "std        33.794235  \n",
       "min         6.000000  \n",
       "25%        58.000000  \n",
       "50%        77.000000  \n",
       "75%       100.000000  \n",
       "max       369.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look for the data characteristics\n",
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "975c081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAAGGCAYAAACJ/96MAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKbZJREFUeJzt3Xl8VPW9//H3ZJsEshFCNoEY9jVgkcagSdgRhCtXpFpthRaolbiA6K1akEUUKrUUC3qRewsWQSxU9FrZy35ZRBYBkS1EQIEAAUIEwpL5/v7w5vwckvAlITIBXs/HYx6d+Z7vOecz33zLvD3nzBmXMcYIAAAApfLzdQEAAACVHYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQm4ibhcLo0YMcLXZXjZsGGD2rRpo6pVq8rlcmnLli2+LgkAyozABFyFadOmyeVyeT1iYmLUrl07zZ8/39flXbMdO3ZoxIgR+vrrryt0uxcvXlTv3r114sQJjR8/XtOnT1diYmKF7sNX5s2bV+nCKYAfT4CvCwBuJKNGjVJSUpKMMcrJydG0adPUrVs3ffLJJ+revbuvyyu3HTt2aOTIkWrbtq1uv/32CttuVlaW9u/frylTpqh///4Vtt3KYN68eZo0aRKhCbhFEJiAMujatavuvPNO53W/fv0UGxur999//4YOTD+Wo0ePSpIiIyN9WwgAXCNOyQHXIDIyUiEhIQoI8P5vjzNnzmjIkCGqVauW3G63GjZsqD/+8Y8yxkiSzp07p0aNGqlRo0Y6d+6cs96JEycUHx+vNm3aqLCwUJLUt29fhYaGat++ferSpYuqVq2qhIQEjRo1ytnelWzevFldu3ZVeHi4QkND1aFDB61bt85ZPm3aNPXu3VuS1K5dO+eU4/Lly6+43aVLlyotLU1Vq1ZVZGSk7r//fn311VfO8r59+yojI0OS1Lt3b7lcLrVt27bEbZ06dUr+/v568803nbbjx4/Lz89P1atX93qfTzzxhOLi4kqty+Vy6cknn9Ts2bPVpEkThYSEKDU1Vdu2bZMkTZ48WfXq1VNwcLDatm1b7DTkqlWr1Lt3b9WuXVtut1u1atXS4MGDvf5Offv21aRJk5z9FT2u5Pbbb1f37t21fPly3XnnnQoJCVHz5s2dcf7www/VvHlzBQcHq1WrVtq8eXOxbezcuVMPPvigoqKiFBwcrDvvvFP/8z//49XnxIkTeu6559S8eXOFhoYqPDxcXbt21RdffOHVb/ny5XK5XPr73/+uV199VTVr1lRwcLA6dOigvXv3XvG9ALckA8Bq6tSpRpJZsmSJOXbsmDl69KjZvn27efzxx42fn59ZtGiR09fj8Zj27dsbl8tl+vfvbyZOnGh69OhhJJlBgwY5/datW2f8/f3N4MGDnbaHH37YhISEmF27djltffr0McHBwaZ+/frml7/8pZk4caLp3r27kWSGDRvmVackM3z4cOf19u3bTdWqVU18fLx55ZVXzNixY01SUpJxu91m3bp1xhhjsrKyzNNPP20kmZdeeslMnz7dTJ8+3Rw5cqTU8Vi8eLEJCAgwDRo0MK+//roZOXKkiY6ONtWqVTPZ2dnGGGPWrFljXnrpJSPJPP3002b69Ole43S55ORk06tXL+f13LlzjZ+fn5Fktm/f7rQ3bdrUPPjgg6VuR5JJTk42tWrVMmPHjjVjx441ERERpnbt2mbixImmSZMm5o033jBDhw41QUFBpl27dl7rP/XUU6Zbt27mtddeM5MnTzb9+vUz/v7+Xvtcs2aN6dSpk5HkjNf06dNLrckYYxITE03Dhg1NfHy8GTFihBk/fry57bbbTGhoqHnvvfdM7dq1veqtV6+eKSwsdNbfvn27iYiIME2aNDF/+MMfzMSJE016erpxuVzmww8/dPpt2LDB1K1b17zwwgtm8uTJZtSoUea2224zERER5ttvv3X6LVu2zEgyd9xxh2nVqpUZP368GTFihKlSpYr56U9/esX3AtyKCEzAVSgKTJc/3G63mTZtmlffjz76yEgyo0eP9mp/8MEHjcvlMnv37nXaXnzxRePn52dWrlxpZs+ebSSZP//5z17r9enTx0gyTz31lNPm8XjMfffdZ4KCgsyxY8ec9ssDU8+ePU1QUJDJyspy2g4dOmTCwsJMenq601a072XLll3VeLRs2dLExMSY3Nxcp+2LL74wfn5+5rHHHnPaij6UZ8+ebd1mZmamiY2NdV4/++yzJj093cTExJi3337bGGNMbm6ucblcZsKECaVup+jvUhTcjDFm8uTJRpKJi4szp0+fdtpffPFFI8mr79mzZ4ttc8yYMcblcpn9+/d71VuW/+ZMTEw0ksyaNWuctoULFxpJJiQkxGvbRfX+8O/RoUMH07x5c1NQUOC0eTwe06ZNG1O/fn2nraCgwCtoGWNMdna2cbvdZtSoUU5b0d+mcePG5vz58077hAkTjCSzbdu2q35vwK2AU3JAGUyaNEmLFy/W4sWL9d5776ldu3bq37+/PvzwQ6fPvHnz5O/vr6efftpr3SFDhsgY4/WtuhEjRqhp06bq06ePBg4cqIyMjGLrFXnyySed50WnnS5cuKAlS5aU2L+wsFCLFi1Sz549VadOHac9Pj5ejzzyiFavXq3Tp0+XeQwOHz6sLVu2qG/fvoqKinLak5OT1alTJ82bN6/M25SktLQ05eTkaNeuXZK+PzWWnp6utLQ0rVq1SpK0evVqGWOUlpZ2xW116NDB6+L1lJQUSVKvXr0UFhZWrH3fvn1OW0hIiPP8zJkzOn78uNq0aSNjTImnycqiSZMmSk1NLbb/9u3bq3bt2qXWdeLECS1dulQ/+9nPlJ+fr+PHj+v48ePKzc1Vly5dtGfPHn377beSJLfbLT+/7/9pLywsVG5urkJDQ9WwYUNt2rSpWE2/+tWvFBQU5LwuGtsfjgkArmECyuSnP/2pOnbsqI4dO+rRRx/Vp59+qiZNmjjhRZL279+vhIQErw9mSWrcuLGzvEhQUJD++te/Kjs7W/n5+Zo6dWqJ18L4+fl5hR5JatCggSSVeiuAY8eO6ezZs2rYsGGxZY0bN5bH49HBgwev/s3/n6L6S9vu8ePHdebMmTJvt+iDetWqVTpz5ow2b96stLQ0paenO4Fp1apVCg8PV4sWLa64rR+GD0mKiIiQJNWqVavE9pMnTzptBw4ccMJgaGioatSo4VyLlZeXV+b3VRF17d27V8YYDRs2TDVq1PB6DB8+XNL/v8De4/Fo/Pjxql+/vtxut6Kjo1WjRg1t3bq1xPovr6latWpe+wbwPb4lB1wDPz8/tWvXThMmTNCePXvUtGnTMm9j4cKFkqSCggLt2bNHSUlJFV3mDSEhIUFJSUlauXKlbr/9dhljlJqaqho1auiZZ57R/v37tWrVKrVp08Y5glIaf3//MrWb/7uovLCwUJ06ddKJEyf0u9/9To0aNVLVqlX17bffqm/fvvJ4PNf0HstbV9F+n3vuOXXp0qXEvvXq1ZMkvfbaaxo2bJh+/etf65VXXlFUVJT8/Pw0aNCgEuu37RvA9whMwDW6dOmSJOm7776TJCUmJmrJkiXKz8/3Osq0c+dOZ3mRrVu3atSoUfrVr36lLVu2qH///tq2bZtzhKGIx+PRvn37nKNKkrR7925JKvW+STVq1FCVKlWcU1w/tHPnTvn5+TlHNmzf8PqhovpL2250dLSqVq161dv7obS0NK1cuVJJSUlq2bKlwsLC1KJFC0VERGjBggXatGmTRo4cWa5tX41t27Zp9+7devfdd/XYY4857YsXLy7Wtyxjdq2Kji4GBgaqY8eOV+w7Z84ctWvXTv/93//t1X7q1ClFR0f/aDUCNztOyQHX4OLFi1q0aJGCgoKcU27dunVTYWGhJk6c6NV3/Pjxcrlc6tq1q7Nu3759lZCQoAkTJmjatGnKycnR4MGDS9zXD7dnjNHEiRMVGBioDh06lNjf399fnTt31scff+x12i4nJ0czZ87UPffco/DwcElyAs6pU6es7zk+Pl4tW7bUu+++69V/+/btWrRokbp162bdRmnS0tL09ddf64MPPnBO0fn5+alNmzb605/+pIsXL3pdv7Rz504dOHCg3Pu7XNHRlh8eXTHGaMKECcX6XmnMsrKylJWVVWF1xcTEqG3btpo8ebIOHz5cbPmxY8ec5/7+/sWODs2ePdu5xglA+XCECSiD+fPnO0eKjh49qpkzZ2rPnj164YUXnPDRo0cPtWvXTr///e/19ddfq0WLFlq0aJE+/vhjDRo0SHXr1pUkjR49Wlu2bNG//vUvhYWFKTk5WS+//LKGDh2qBx980Ct4BAcHa8GCBerTp49SUlI0f/58ffrpp3rppZdUo0aNUusdPXq0Fi9erHvuuUcDBw5UQECAJk+erPPnz+v11193+rVs2VL+/v76wx/+oLy8PLndbrVv314xMTElbnfcuHHq2rWrUlNT1a9fP507d05/+ctfFBERcU13vi4KQ7t27dJrr73mtKenp2v+/Plyu91q3bq10964cWNlZGRY7xl1tRo1aqS6devqueee07fffqvw8HD94x//KPF6nlatWkmSnn76aXXp0kX+/v56+OGHJckJsRX5UzOTJk3SPffco+bNm2vAgAGqU6eOcnJytHbtWn3zzTfOfZa6d+/uHLVs06aNtm3bphkzZhS7Bg5AGfnmy3nAjaWk2woEBwebli1bmrffftt4PB6v/vn5+Wbw4MEmISHBBAYGmvr165tx48Y5/TZu3GgCAgK8bhVgjDGXLl0yrVu3NgkJCebkyZPGmO9vK1C1alWTlZVlOnfubKpUqWJiY2PN8OHDi319XJfdVsAYYzZt2mS6dOliQkNDTZUqVUy7du28vtpeZMqUKaZOnTrG39//qm4xsGTJEnP33XebkJAQEx4ebnr06GF27Njh1acstxUoEhMTYySZnJwcp2316tVGkklLSyv2fjMyMoq1ZWZmerVlZ2cbSWbcuHHW+nbs2GE6duxoQkNDTXR0tBkwYID54osvjCQzdepUp9+lS5fMU089ZWrUqGFcLpfXLQYSExNNYmKi174SExPNfffdV+z9lqXerKws89hjj5m4uDgTGBhobrvtNtO9e3czZ84cp09BQYEZMmSIiY+PNyEhIebuu+82a9euNRkZGV5jVdrfpmjfP3yvAIxxGcOVfUBl1rdvX82ZM8e5RgoAcP1xDRMAAIAFgQkAAMCCwAQAAGDBNUwAAAAWHGECAACwIDABAABYlPvGlR6PR4cOHVJYWNh1/YkAAACAimKMUX5+vhISEq74O5XlDkyHDh0q9gvbAAAAN6KDBw+qZs2apS4vd2Aq+lHRgwcPOj8JAQAAcCM5ffq0atWq5fVj6SUpd2AqOg0XHh5OYAIAADc02+VFXPQNAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYBPi6AACojHJycpSXl+frMnCDioiIUGxsrK/LQAUiMAHAZXJycvSLXz6mixfO+7oU3KACg9x6b/rfCE03EQITAFwmLy9PFy+c17k6GfIER/i6nBue37lTCsleqXNJ6fKERPq6nB+dX0GetG+F8vLyCEw3EQITAJTCExwhT9VoX5dx0/CERDKeuGFx0TcAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWB6QZVUFCg3bt3q6CgwNelAABQ4Srb5xyB6QZ14MAB/eY3v9GBAwd8XQoAABWusn3OEZgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAI8HUBpSksLNTWrVt14sQJRUVFKTk5Wf7+/pWqjqJlx48f16lTpxQZGamoqChJcl4XPQ8PD9e+fft0+PBhSVLjxo0VExPjbK+wsFCff/65PvjgAx09elQhISGqVauWYmJi9N1332nfvn3Kzc2V2+1WdHS0YmJiJEkej+e6jwkAALeaShmYVq5cqbfeektHjhxx2uLi4jRw4EClp6dXijokFVtWFh999JGzvbZt22ru3Lk6f/68V589e/aUuO6BAwec588//7yef/756zouAADcairdKbmVK1dq+PDhqlOnjiZNmqR58+Zp0qRJqlOnjoYPH66VK1f6vI6XX35Zw4cPV0REhCQpJSVF//Zv/+asGx0dLUmqXbu28zwwMNBZnpaW5hx9On/+vGbNmlUsLLlcrhLrurw9Pz//uo4LAAC3okoVmAoLC/XWW28pNTVVo0ePVtOmTVWlShU1bdpUo0ePVmpqqt5++20VFhb6rI6RI0fK7XYrKChIp06dUps2bTR69Gh99tlnSk1N1V133aWTJ0+qWrVqOn/+vHJzcxUQECCPx6O77rpLqamp2rt3rz744ANFRkbq5MmTzn6DgoIUFBSklJSUEgOTy+VSdHS0qlWr5tUeFBR0XcYFAIBb1VWfkjt//rzXUZDTp09XeDFbt27VkSNHNGzYMPn5eWc5Pz8/Pfroo8rMzNTWrVt1xx13VPj+r6aO7du3O+OQk5Ojl19+Wdu3b3f67969W+vWrVOnTp3097//XZKUnp6upUuXKiUlRQ0aNFBmZqZ27Nihzp07O30k6cKFC5KkmjVrav369cXqMsbo2LFj+tnPfua13vnz53X48GHNmzdPDRs2rPDxAG41+/fv93UJuAkwj65NZRu/qw5MY8aM0ciRI3/MWnTixAlJUlJSUonLi9qL+vmijsv3nZSUpLVr1zrPi/7ACQkJTp9GjRpp6dKlCg4O9noPP+zzQ5efnrtcfHx8ie1vvPHGFdcDAFw/r776qq9LQAW66sD04osv6tlnn3Venz59WrVq1arQYoq+YZadna2mTZsWW56dne3V78dypTou33d2drZX/6Kwc+jQIafPzp07JUkFBQVe72HXrl0l7t/tdl+xvqJv2l1uyJAhHGECKsD+/fv5sMM1+/3vf6/ExERfl3HDqmz/P7zqwOR2u60f5NcqOTlZcXFxmjFjhkaPHu11Oszj8WjGjBmKj49XcnKyz+po1qyZMw6RkZGaMWOGRo4cqbi4OL333nsyxsjf31+LFy9WbGysjh49qpUrV8rf31/r16/XZ599pvj4eDVp0qTYEbugoCBJ0jfffCM/P79itwwouoZp8eLFXu1ut1tRUVHq1q2bT269AAAoLjExUQ0aNPB1Gaggleqib39/fw0cOFBr167V0KFD9eWXX+rs2bP68ssvNXToUK1du1ZPPPHEjx4KrlTH8OHDdf78eV24cEGRkZFas2aNhg4dqtatW2vt2rVat26dqlWrppMnT8rtdqt69eq6dOmS/Pz8tG7dOq1du1Z169bVQw89pFOnTnldwH3hwgVduHBB69evlzGmWF3GGB0/ftzrQvGi9a7HuAAAcKuqdPdhSk9P18iRI/XWW28pMzPTaY+Pj9fIkSOv2/2GrlTHqFGjJH1/HyZJxS7QPn78uCTv+yVdvHjReb569WrneXBwsB5++OFi92EqKTCV1B4WFsZ9mAAA+JFVusAkfR9W7r77bp/f6dtWR9GyirjT94ABA8p8p+8FCxZo3LhxatSo0XUdFwAAbjWVMjBJ358W+zFvHVARdZS1xtatW19xPykpKUpJSbmqbe3evVsLFiwodtsDAABQ8fi0BQAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYHpBlW7dm298847ql27tq9LAQCgwlW2z7kAXxeA8gkODlaDBg18XQYAAD+KyvY5xxEmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAACLAF8XAACVlV9Bnq9LuCn4nTvl9b83O+bNzYnABACXiYiIUGCQW9q3wtel3FRCslf6uoTrJjDIrYiICF+XgQpEYAKAy8TGxuq96X9TXh5HClA+ERERio2N9XUZqEAEJgAoQWxsLB94ABxc9A0AAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALAhMAAAAFgQmAAAACwITAACABYEJAADAgsAEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYAIAALAgMAEAAFgElHdFY4wk6fTp0xVWDAAAwPVUlGOKck1pyh2Y8vPzJUm1atUq7yYAAAAqhfz8fEVERJS63GVskaoUHo9Hhw4dUlhYmFwuV7kLvNGcPn1atWrV0sGDBxUeHu7rcm5YjGPFYBwrBuNYMRjHisNYVoyrGUdjjPLz85WQkCA/v9KvVCr3ESY/Pz/VrFmzvKvf8MLDw5nEFYBxrBiMY8VgHCsG41hxGMuKYRvHKx1ZKsJF3wAAABYEJgAAAAsCUxm53W4NHz5cbrfb16Xc0BjHisE4VgzGsWIwjhWHsawYFTmO5b7oGwAA4FbBESYAAAALAhMAAIAFgQkAAMCCwFSKlStXqkePHkpISJDL5dJHH33ktdwYo5dfflnx8fEKCQlRx44dtWfPHt8UW4nZxrFv375yuVxej3vvvdc3xVZiY8aMUevWrRUWFqaYmBj17NlTu3bt8upTUFCgzMxMVa9eXaGhoerVq5dycnJ8VHHldDXj2LZt22Jz8re//a2PKq6c3n77bSUnJzv3tklNTdX8+fOd5czFq2MbR+Zi+YwdO1Yul0uDBg1y2ipiThKYSnHmzBm1aNFCkyZNKnH566+/rjfffFP/+Z//qfXr16tq1arq0qWLCgoKrnOllZttHCXp3nvv1eHDh53H+++/fx0rvDGsWLFCmZmZWrdunRYvXqyLFy+qc+fOOnPmjNNn8ODB+uSTTzR79mytWLFChw4d0gMPPODDqiufqxlHSRowYIDXnHz99dd9VHHlVLNmTY0dO1YbN27U559/rvbt2+v+++/Xl19+KYm5eLVs4ygxF8tqw4YNmjx5spKTk73aK2ROGlhJMnPnznVeezweExcXZ8aNG+e0nTp1yrjdbvP+++/7oMIbw+XjaIwxffr0Mffff79P6rmRHT161EgyK1asMMZ8P/8CAwPN7NmznT5fffWVkWTWrl3rqzIrvcvH0RhjMjIyzDPPPOO7om5Q1apVM//1X//FXLxGReNoDHOxrPLz8039+vXN4sWLvcauouYkR5jKITs7W0eOHFHHjh2dtoiICKWkpGjt2rU+rOzGtHz5csXExKhhw4Z64oknlJub6+uSKr28vDxJUlRUlCRp48aNunjxotecbNSokWrXrs2cvILLx7HIjBkzFB0drWbNmunFF1/U2bNnfVHeDaGwsFCzZs3SmTNnlJqaylwsp8vHsQhz8eplZmbqvvvu85p7UsX9+1ju35K7lR05ckSSFBsb69UeGxvrLMPVuffee/XAAw8oKSlJWVlZeumll9S1a1etXbtW/v7+vi6vUvJ4PBo0aJDuvvtuNWvWTNL3czIoKEiRkZFefZmTpStpHCXpkUceUWJiohISErR161b97ne/065du/Thhx/6sNrKZ9u2bUpNTVVBQYFCQ0M1d+5cNWnSRFu2bGEulkFp4ygxF8ti1qxZ2rRpkzZs2FBsWUX9+0hggk89/PDDzvPmzZsrOTlZdevW1fLly9WhQwcfVlZ5ZWZmavv27Vq9erWvS7mhlTaOv/nNb5znzZs3V3x8vDp06KCsrCzVrVv3epdZaTVs2FBbtmxRXl6e5syZoz59+mjFihW+LuuGU9o4NmnShLl4lQ4ePKhnnnlGixcvVnBw8I+2H07JlUNcXJwkFbvCPicnx1mG8qlTp46io6O1d+9eX5dSKT355JP65z//qWXLlqlmzZpOe1xcnC5cuKBTp0559WdOlqy0cSxJSkqKJDEnLxMUFKR69eqpVatWGjNmjFq0aKEJEyYwF8uotHEsCXOxZBs3btTRo0f1k5/8RAEBAQoICNCKFSv05ptvKiAgQLGxsRUyJwlM5ZCUlKS4uDj961//ctpOnz6t9evXe517Rtl98803ys3NVXx8vK9LqVSMMXryySc1d+5cLV26VElJSV7LW7VqpcDAQK85uWvXLh04cIA5+QO2cSzJli1bJIk5aeHxeHT+/Hnm4jUqGseSMBdL1qFDB23btk1btmxxHnfeeaceffRR53lFzElOyZXiu+++80rx2dnZ2rJli6KiolS7dm0NGjRIo0ePVv369ZWUlKRhw4YpISFBPXv29F3RldCVxjEqKkojR45Ur169FBcXp6ysLP3Hf/yH6tWrpy5duviw6sonMzNTM2fO1Mcff6ywsDDnvHtERIRCQkIUERGhfv366dlnn1VUVJTCw8P11FNPKTU1VXfddZePq688bOOYlZWlmTNnqlu3bqpevbq2bt2qwYMHKz09vdjXlG9lL774orp27aratWsrPz9fM2fO1PLly7Vw4ULmYhlcaRyZi1cvLCzM6zpESapataqqV6/utFfInKzYL/XdPJYtW2YkFXv06dPHGPP9rQWGDRtmYmNjjdvtNh06dDC7du3ybdGV0JXG8ezZs6Zz586mRo0aJjAw0CQmJpoBAwaYI0eO+LrsSqekMZRkpk6d6vQ5d+6cGThwoKlWrZqpUqWK+fd//3dz+PBh3xVdCdnG8cCBAyY9Pd1ERUUZt9tt6tWrZ55//nmTl5fn28IrmV//+tcmMTHRBAUFmRo1apgOHTqYRYsWOcuZi1fnSuPIXLw2l9+SoSLmpMsYY8qX6QAAAG4NXMMEAABgQWACAACwIDABAABYEJgAAAAsCEwAAAAWBCYAAAALAhMAAIAFgQkAAMCCwAQAAGBBYALgc19//bVcLpfz46IAUNkQmAAAACwITMAt6J///KciIyNVWFgoSdqyZYtcLpdeeOEFp0///v31i1/8wmu95cuXy+VyaeHChbrjjjsUEhKi9u3b6+jRo5o/f74aN26s8PBwPfLIIzp79qyz3oIFC3TPPfcoMjJS1atXV/fu3ZWVleUsT0pKkiTdcccdcrlcatu2bYl1l3f/Ho9HY8aMUVJSkkJCQtSiRQvNmTPHWV5YWKh+/fo5yxs2bKgJEyZ47btv377q2bOn/vjHPyo+Pl7Vq1dXZmamLl68WMbRB3BDqtCfBwZwQzh16pTx8/MzGzZsMMYY8+c//9lER0eblJQUp0+9evXMlClTvNZbtmyZkWTuuusus3r1arNp0yZTr149k5GRYTp37mw2bdpkVq5caapXr27Gjh3rrDdnzhzzj3/8w+zZs8ds3rzZ9OjRwzRv3twUFhYaY4z57LPPjCSzZMkSc/jwYZObm1ti3eXd/+jRo02jRo3MggULTFZWlpk6dapxu91m+fLlxhhjLly4YF5++WWzYcMGs2/fPvPee++ZKlWqmA8++MDZRp8+fUx4eLj57W9/a7766ivzySefmCpVqph33nnnGv8aAG4EBCbgFvWTn/zEjBs3zhhjTM+ePc2rr75qgoKCTH5+vvnmm2+MJLN7926vdYoCy5IlS5y2MWPGGEkmKyvLaXv88cdNly5dSt33sWPHjCSzbds2Y4wx2dnZRpLZvHnzFWsuz/4LCgpMlSpVzJo1a7y21a9fP/Pzn/+81H1lZmaaXr16Oa/79OljEhMTzaVLl5y23r17m4ceeuiKNQO4OXBKDrhFZWRkaPny5TLGaNWqVXrggQfUuHFjrV69WitWrFBCQoLq169f4rrJycnO89jYWFWpUkV16tTxajt69Kjzes+ePfr5z3+uOnXqKDw8XLfffrsk6cCBA+WqvSz737t3r86ePatOnTopNDTUefztb3/zOi04adIktWrVSjVq1FBoaKjeeeedYvU1bdpU/v7+zuv4+Hiv9wng5hXg6wIA+Ebbtm3117/+VV988YUCAwPVqFEjtW3bVsuXL9fJkyeVkZFR6rqBgYHOc5fL5fW6qM3j8Tive/ToocTERE2ZMkUJCQnyeDxq1qyZLly4UK7ay7L/7777TpL06aef6rbbbvPq53a7JUmzZs3Sc889pzfeeEOpqakKCwvTuHHjtH79+lL3W9L7BHDzIjABt6i0tDTl5+dr/PjxTjhq27atxo4dq5MnT2rIkCEVsp/c3Fzt2rVLU6ZMUVpamiRp9erVXn2CgoIkybkIvSI1adJEbrdbBw4cKDUE/u///q/atGmjgQMHOm0/PPoEAJySA25R1apVU3JysmbMmOF8Ky09PV2bNm3S7t27lZGRoblz56pRo0bXvJ/q1avrnXfe0d69e7V06VI9++yzXn1iYmIUEhKiBQsWKCcnR3l5eZJUIfsPCwvTc889p8GDB+vdd99VVlaWNm3apL/85S969913JUn169fX559/roULF2r37t0aNmyYNmzYcE37BXBzITABt7CMjAwVFhY6gSkqKkpNmjRRXFycGjZsqLy8PO3ateua9uHn56dZs2Zp48aNatasmQYPHqxx48Z59QkICNCbb76pyZMnKyEhQffff78kVcj+JemVV17RsGHDNGbMGDVu3Fj33nuvPv30U+d2Bo8//rgeeOABPfTQQ0pJSVFubq7X0SYAcBljjK+LAAAAqMw4wgQAAGBBYAIAALAgMAEAAFgQmAAAACwITAAAABYEJgAAAAsCEwAAgAWBCQAAwILABAAAYEFgAgAAsCAwAQAAWBCYAAAALP4ftbK2Qb1uxfkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# looking for how the data is spread\n",
    "\n",
    "for cols in new_df.columns:\n",
    "    plt.figure(figsize=(6, 4))  # Create a new figure for each boxplot\n",
    "    sns.boxplot(data=new_df, x=cols)\n",
    "    plt.title(f'Boxplot of {cols}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d73125",
   "metadata": {},
   "source": [
    "Looks like the data is spread too much and this needs to be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ccf76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode result to numeric using the following mapping\n",
    "new_df.result=new_df.result.map({'1-0':int(1), '0-1':int(0), '1/2-1/2':int(2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9638530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an 85% training / 15% test split using train_test_split()\n",
    "#Train test split\n",
    "X_train,X_test,y_train,y_test =train_test_split(new_df.drop('result',axis=1),new_df['result'],test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca6fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the 5 numeric predictor columns using StandardScaler() \n",
    "# Test upload\n",
    "scalar=StandardScaler()\n",
    "\n",
    "X_train=scalar.fit_transform(X_train)\n",
    "X_test=scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97131dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Dataset and the Data Loader\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self,features,labels):\n",
    "        self.features=torch.tensor(features,dtype=torch.float32)\n",
    "        self.labels=torch.tensor(np.array(labels),dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len( self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return(self.features[index],self.labels[index])\n",
    "\n",
    "train_dataset=CustomDataSet(X_train,y_train)\n",
    "test_dataset=CustomDataSet(X_test,y_test)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=128,shuffle=True,pin_memory=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=128,shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f8c26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Neural network Class\n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self,input_featutes):\n",
    "        super().__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Linear(input_featutes,50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,3),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c8a1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Learning rate and Epochs\n",
    "learning_rate=0.05\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eec8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Deine the model with the criterion and optimizer\n",
    "\n",
    "model=MyNN(X_train.shape[1])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=learning_rate,weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1fb2595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9476368933692014\n",
      "Epoch 2, Loss: 0.9090824578041421\n",
      "Epoch 3, Loss: 0.8801796050896322\n",
      "Epoch 4, Loss: 0.8495972777667798\n",
      "Epoch 5, Loss: 0.8268917082843924\n",
      "Epoch 6, Loss: 0.8157872054809915\n",
      "Epoch 7, Loss: 0.8103606513568333\n",
      "Epoch 8, Loss: 0.8072246610670162\n",
      "Epoch 9, Loss: 0.8053441673293149\n",
      "Epoch 10, Loss: 0.804260038746927\n",
      "Epoch 11, Loss: 0.8041069864330436\n",
      "Epoch 12, Loss: 0.8032097981388407\n",
      "Epoch 13, Loss: 0.802244398378788\n",
      "Epoch 14, Loss: 0.8022113690698953\n",
      "Epoch 15, Loss: 0.8021485403964393\n",
      "Epoch 16, Loss: 0.8022624282012308\n",
      "Epoch 17, Loss: 0.8016825613222625\n",
      "Epoch 18, Loss: 0.8014040142073667\n",
      "Epoch 19, Loss: 0.8011840391876106\n",
      "Epoch 20, Loss: 0.8011062711701358\n",
      "Epoch 21, Loss: 0.8004250289802265\n",
      "Epoch 22, Loss: 0.8004750630909339\n",
      "Epoch 23, Loss: 0.7997778180846594\n",
      "Epoch 24, Loss: 0.8004192598780295\n",
      "Epoch 25, Loss: 0.8000642431409736\n",
      "Epoch 26, Loss: 0.7997852226845303\n",
      "Epoch 27, Loss: 0.7996996991616443\n",
      "Epoch 28, Loss: 0.7991274834575509\n",
      "Epoch 29, Loss: 0.7991905918694977\n",
      "Epoch 30, Loss: 0.7994951180945662\n",
      "Epoch 31, Loss: 0.7979355000015488\n",
      "Epoch 32, Loss: 0.7984712108633572\n",
      "Epoch 33, Loss: 0.7993035179331787\n",
      "Epoch 34, Loss: 0.7987996455422022\n",
      "Epoch 35, Loss: 0.797263915377452\n",
      "Epoch 36, Loss: 0.7976772622058266\n",
      "Epoch 37, Loss: 0.7968829118219534\n",
      "Epoch 38, Loss: 0.7981368788202903\n",
      "Epoch 39, Loss: 0.7975874530641656\n",
      "Epoch 40, Loss: 0.7965709269494938\n",
      "Epoch 41, Loss: 0.7962632054673102\n",
      "Epoch 42, Loss: 0.7962995815994148\n",
      "Epoch 43, Loss: 0.7963616078957579\n",
      "Epoch 44, Loss: 0.7961261179214133\n",
      "Epoch 45, Loss: 0.7959363869258336\n",
      "Epoch 46, Loss: 0.7957186715943473\n",
      "Epoch 47, Loss: 0.7952697884767576\n",
      "Epoch 48, Loss: 0.7952747905164733\n",
      "Epoch 49, Loss: 0.7945865179811206\n",
      "Epoch 50, Loss: 0.7948595376839315\n",
      "Epoch 51, Loss: 0.7946565280283304\n",
      "Epoch 52, Loss: 0.7947664309265022\n",
      "Epoch 53, Loss: 0.7940132249566846\n",
      "Epoch 54, Loss: 0.7948257045638293\n",
      "Epoch 55, Loss: 0.7945893727747121\n",
      "Epoch 56, Loss: 0.7940551209270506\n",
      "Epoch 57, Loss: 0.7937504992449194\n",
      "Epoch 58, Loss: 0.7938711716716451\n",
      "Epoch 59, Loss: 0.7936156833082213\n",
      "Epoch 60, Loss: 0.7941880921672161\n",
      "Epoch 61, Loss: 0.7926611722860121\n",
      "Epoch 62, Loss: 0.7937995438288925\n",
      "Epoch 63, Loss: 0.7930938441950576\n",
      "Epoch 64, Loss: 0.7934867292418516\n",
      "Epoch 65, Loss: 0.7934311088762785\n",
      "Epoch 66, Loss: 0.7927920515375926\n",
      "Epoch 67, Loss: 0.7923421215293999\n",
      "Epoch 68, Loss: 0.7925733897022735\n",
      "Epoch 69, Loss: 0.7924273623559708\n",
      "Epoch 70, Loss: 0.7925105773416677\n",
      "Epoch 71, Loss: 0.7924070262371149\n",
      "Epoch 72, Loss: 0.792310897658642\n",
      "Epoch 73, Loss: 0.7919631118164923\n",
      "Epoch 74, Loss: 0.7914956571912407\n",
      "Epoch 75, Loss: 0.7924043473444486\n",
      "Epoch 76, Loss: 0.792185039717452\n",
      "Epoch 77, Loss: 0.7921358857836042\n",
      "Epoch 78, Loss: 0.7922421084310776\n",
      "Epoch 79, Loss: 0.7921361089649057\n",
      "Epoch 80, Loss: 0.7923413287428088\n",
      "Epoch 81, Loss: 0.7919760438732635\n",
      "Epoch 82, Loss: 0.792131861618587\n",
      "Epoch 83, Loss: 0.7922348835414513\n",
      "Epoch 84, Loss: 0.7921269756510741\n",
      "Epoch 85, Loss: 0.7912776006791825\n",
      "Epoch 86, Loss: 0.7916796967499238\n",
      "Epoch 87, Loss: 0.7918614605315646\n",
      "Epoch 88, Loss: 0.7919861351637015\n",
      "Epoch 89, Loss: 0.7912042111382449\n",
      "Epoch 90, Loss: 0.7911428555510098\n",
      "Epoch 91, Loss: 0.790965515868108\n",
      "Epoch 92, Loss: 0.7912014398359715\n",
      "Epoch 93, Loss: 0.7910292371771389\n",
      "Epoch 94, Loss: 0.7912514953684986\n",
      "Epoch 95, Loss: 0.790607201963439\n",
      "Epoch 96, Loss: 0.7916164377578219\n",
      "Epoch 97, Loss: 0.7913525958706562\n",
      "Epoch 98, Loss: 0.7917427462742741\n",
      "Epoch 99, Loss: 0.7908393887648905\n",
      "Epoch 100, Loss: 0.7908490296593286\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "total=0\n",
    "correct=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_epoch_loss=0\n",
    "\n",
    "    for batch_features,batch_labels in train_loader:\n",
    "\n",
    "        #move data to gpu\n",
    "        batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        outputs=model(batch_features)\n",
    "\n",
    "        #Calculate loss\n",
    "        loss= criterion(outputs,batch_labels)\n",
    "\n",
    "        #backpass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #update grads\n",
    "        optimizer.step()\n",
    "\n",
    "        total_epoch_loss= total_epoch_loss+loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "        # print(f'predicted:{predicted}Labels {batch_labels}')\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss=total_epoch_loss/len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    print(f'Epoch {epoch +1 }, Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b82cea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51b7d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy 0.6358\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and print test accuracy.\n",
    "\n",
    "#Set model to eval model\n",
    "model.eval()\n",
    "\n",
    "#Evaluation Code\n",
    "\n",
    "total=0\n",
    "correct=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "        outputs=model(batch_features)\n",
    "        _, predicted=torch.max(outputs,1)\n",
    "\n",
    "        total = total + batch_labels.shape[0]\n",
    "        correct= correct+(predicted==batch_labels).sum().item()\n",
    "print(f'Evaluation accuracy {correct/total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9501c31",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning via Manual Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc7eb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,hidden_layers,neurons_per_layer,act_functn):\n",
    "        super().__init__()\n",
    "\n",
    "        layers =[]\n",
    "        \n",
    "        for i in range(hidden_layers):\n",
    "            layers.append(nn.Linear(input_dim,neurons_per_layer))\n",
    "            if act_functn=='relu':\n",
    "                   layers.append(nn.ReLU())\n",
    "            elif act_functn=='tanh':\n",
    "                   layers.append(nn.Tanh())\n",
    "            elif act_functn=='sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            # layers.append(nn.Dropout(drop_out_rate))\n",
    "            input_dim=neurons_per_layer\n",
    "        layers.append(nn.Linear(neurons_per_layer,output_dim))\n",
    "        # layers.append(nn.Softmax())\n",
    "\n",
    "        self.model=nn.Sequential(*layers)\n",
    "\n",
    "    def forward (self,x):\n",
    "        x=self.model(x)\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d636f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space={'hidden_layers':[1,2],'neurons_per_layer':[50,60],'act_functn':['relu','tanh','sigmoid'],'optimizer_name':['Adam','SGD','RMSprop'],'learning_rate':[0.001, 0.01,0.02],'batch_size':[100,128,],'epochs':[75,100]}\n",
    "\n",
    "#objective function\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_layers=trial.suggest_categorical('hidden_layers',search_space['hidden_layers'])\n",
    "    act_functn=trial.suggest_categorical('act_functn',search_space['act_functn'])\n",
    "    optimizer_name=trial.suggest_categorical('optimizer_name',search_space['optimizer_name'])\n",
    "    learning_rate=trial.suggest_categorical('learning_rate',search_space['learning_rate'])\n",
    "    batch_size=trial.suggest_categorical('batch_size',search_space['batch_size'])\n",
    "    epochs=trial.suggest_categorical('epochs',search_space['epochs'])\n",
    "    neurons_per_layer=trial.suggest_categorical('neurons_per_layer',search_space['neurons_per_layer'])\n",
    "    # dropout=trial.suggest_categorical('dropout',search_space['dropout'])\n",
    "\n",
    "    input_shape=X_train.shape[1]\n",
    "    output_shape=3\n",
    "\n",
    "    model=MyNN(input_shape,output_shape,hidden_layers,neurons_per_layer,act_functn)\n",
    "    model.to(device)\n",
    "\n",
    "    #Loss Function\n",
    "    criterion= nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    if optimizer_name=='Adam':\n",
    "        optimizer =  optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "    elif optimizer_name=='SGD':\n",
    "        optimizer =  optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "    elif optimizer_name=='RMSprop':\n",
    "        optimizer =  optim.RMSprop(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    # print(optimizer)\n",
    "\n",
    "\n",
    "    train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "    test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_epoch_loss=0\n",
    "        total=0\n",
    "        correct=0\n",
    "\n",
    "        for batch_features,batch_labels in train_loader:\n",
    "\n",
    "            #move data to gpu\n",
    "            batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
    "\n",
    "            #Forward pass\n",
    "            outputs=model(batch_features)\n",
    "\n",
    "            #Calculate loss\n",
    "            loss= criterion(outputs,batch_labels)\n",
    "\n",
    "            #backpass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #update grads\n",
    "            optimizer.step()\n",
    "\n",
    "            total_epoch_loss= total_epoch_loss+loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "            # print(f'predicted:{predicted}Labels {batch_labels}')\n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        avg_loss=total_epoch_loss/len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        print(f'Epoch {epoch +1 }, Training Loss: {avg_loss}')\n",
    "\n",
    "        # # Plot\n",
    "        # plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.plot(train_losses, label='Training Loss')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.title('Training Loss')\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.plot(train_accuracies, label='Training Accuracy')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Accuracy (%)')\n",
    "        # plt.title('Training Accuracy')\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        # Evaluate and print test accuracy.\n",
    "\n",
    "    #Set model to eval model\n",
    "    model.eval()\n",
    "\n",
    "    #Evaluation Code\n",
    "\n",
    "    total=0\n",
    "    correct=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "            outputs=model(batch_features)\n",
    "            _, predicted=torch.max(outputs,1)\n",
    "\n",
    "            total = total + batch_labels.shape[0]\n",
    "            correct= correct+(predicted==batch_labels).sum().item()\n",
    "    accuracy=correct/total\n",
    "    \n",
    "    # print(f'Evaluation accuracy with model parameters:  Learning rate: {learning_rate} batchsize : {batch_size} is : {accuracy}')\n",
    "\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb67ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:21:06,084] A new study created in memory with name: no-name-b5fc7ceb-0151-44bc-9b36-60a9feb03b09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.9760593723549562\n",
      "Epoch 2, Training Loss: 0.9468434096785152\n",
      "Epoch 3, Training Loss: 0.9359801716664258\n",
      "Epoch 4, Training Loss: 0.9253400618188521\n",
      "Epoch 5, Training Loss: 0.9144791279119604\n",
      "Epoch 6, Training Loss: 0.9035625074190252\n",
      "Epoch 7, Training Loss: 0.8927469309638528\n",
      "Epoch 8, Training Loss: 0.8821313675712137\n",
      "Epoch 9, Training Loss: 0.8720270440157722\n",
      "Epoch 10, Training Loss: 0.8627826918573941\n",
      "Epoch 11, Training Loss: 0.8544744569413802\n",
      "Epoch 12, Training Loss: 0.8471237993941587\n",
      "Epoch 13, Training Loss: 0.8408524423487046\n",
      "Epoch 14, Training Loss: 0.8354724664547865\n",
      "Epoch 15, Training Loss: 0.8309378437434926\n",
      "Epoch 16, Training Loss: 0.827291054725647\n",
      "Epoch 17, Training Loss: 0.8242212590750526\n",
      "Epoch 18, Training Loss: 0.821631270717172\n",
      "Epoch 19, Training Loss: 0.8196618085749009\n",
      "Epoch 20, Training Loss: 0.817948413806803\n",
      "Epoch 21, Training Loss: 0.816525716010262\n",
      "Epoch 22, Training Loss: 0.8154157219914829\n",
      "Epoch 23, Training Loss: 0.8143001355143155\n",
      "Epoch 24, Training Loss: 0.8135675489902496\n",
      "Epoch 25, Training Loss: 0.8127133770549999\n",
      "Epoch 26, Training Loss: 0.8121707354573643\n",
      "Epoch 27, Training Loss: 0.8116791993028978\n",
      "Epoch 28, Training Loss: 0.811179003645392\n",
      "Epoch 29, Training Loss: 0.8107044691198012\n",
      "Epoch 30, Training Loss: 0.810394570757361\n",
      "Epoch 31, Training Loss: 0.8100320015234106\n",
      "Epoch 32, Training Loss: 0.8097760336539325\n",
      "Epoch 33, Training Loss: 0.8093229723677916\n",
      "Epoch 34, Training Loss: 0.8091276075559504\n",
      "Epoch 35, Training Loss: 0.808809683673522\n",
      "Epoch 36, Training Loss: 0.8085850650422713\n",
      "Epoch 37, Training Loss: 0.8083645366921144\n",
      "Epoch 38, Training Loss: 0.8079901895803564\n",
      "Epoch 39, Training Loss: 0.8078747943569632\n",
      "Epoch 40, Training Loss: 0.807608358299031\n",
      "Epoch 41, Training Loss: 0.8073795067562777\n",
      "Epoch 42, Training Loss: 0.8070594233625076\n",
      "Epoch 43, Training Loss: 0.8068034280748928\n",
      "Epoch 44, Training Loss: 0.80672559057965\n",
      "Epoch 45, Training Loss: 0.8064866166255054\n",
      "Epoch 46, Training Loss: 0.8062318759806016\n",
      "Epoch 47, Training Loss: 0.8059236690577338\n",
      "Epoch 48, Training Loss: 0.8058763485095081\n",
      "Epoch 49, Training Loss: 0.8055840018216301\n",
      "Epoch 50, Training Loss: 0.8054718310692731\n",
      "Epoch 51, Training Loss: 0.8051857808056999\n",
      "Epoch 52, Training Loss: 0.805023392088273\n",
      "Epoch 53, Training Loss: 0.8048284274690292\n",
      "Epoch 54, Training Loss: 0.8046800174432642\n",
      "Epoch 55, Training Loss: 0.8044411308625166\n",
      "Epoch 56, Training Loss: 0.8042612306510701\n",
      "Epoch 57, Training Loss: 0.8040110522158006\n",
      "Epoch 58, Training Loss: 0.8038830958394443\n",
      "Epoch 59, Training Loss: 0.8036290351082297\n",
      "Epoch 60, Training Loss: 0.80355849238003\n",
      "Epoch 61, Training Loss: 0.8033648962133071\n",
      "Epoch 62, Training Loss: 0.8032254809491775\n",
      "Epoch 63, Training Loss: 0.803101649424609\n",
      "Epoch 64, Training Loss: 0.8028576223289265\n",
      "Epoch 65, Training Loss: 0.8027927303314208\n",
      "Epoch 66, Training Loss: 0.8026192555006813\n",
      "Epoch 67, Training Loss: 0.8024377086583305\n",
      "Epoch 68, Training Loss: 0.8023594291771159\n",
      "Epoch 69, Training Loss: 0.802149933646707\n",
      "Epoch 70, Training Loss: 0.8021288072361665\n",
      "Epoch 71, Training Loss: 0.8018569952600143\n",
      "Epoch 72, Training Loss: 0.8017377084844253\n",
      "Epoch 73, Training Loss: 0.8016737740881302\n",
      "Epoch 74, Training Loss: 0.8016255333143122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:22:16,130] Trial 0 finished with value: 0.6339333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 0 with value: 0.6339333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8015832279008978\n",
      "Epoch 1, Training Loss: 0.8706890151913005\n",
      "Epoch 2, Training Loss: 0.8147549634589288\n",
      "Epoch 3, Training Loss: 0.8078567391947696\n",
      "Epoch 4, Training Loss: 0.8049548301481663\n",
      "Epoch 5, Training Loss: 0.8034344971628118\n",
      "Epoch 6, Training Loss: 0.798156381997847\n",
      "Epoch 7, Training Loss: 0.7980997842057307\n",
      "Epoch 8, Training Loss: 0.796421862365608\n",
      "Epoch 9, Training Loss: 0.794511308078479\n",
      "Epoch 10, Training Loss: 0.7946345857211522\n",
      "Epoch 11, Training Loss: 0.7935823856439805\n",
      "Epoch 12, Training Loss: 0.7940533562710411\n",
      "Epoch 13, Training Loss: 0.7924309618490979\n",
      "Epoch 14, Training Loss: 0.7932731805887437\n",
      "Epoch 15, Training Loss: 0.7922138090420486\n",
      "Epoch 16, Training Loss: 0.7919268655597715\n",
      "Epoch 17, Training Loss: 0.7911348597447675\n",
      "Epoch 18, Training Loss: 0.7906938070641425\n",
      "Epoch 19, Training Loss: 0.7921892290724848\n",
      "Epoch 20, Training Loss: 0.7904094282845805\n",
      "Epoch 21, Training Loss: 0.7907936940515848\n",
      "Epoch 22, Training Loss: 0.7906712796455039\n",
      "Epoch 23, Training Loss: 0.7898696224492295\n",
      "Epoch 24, Training Loss: 0.7889677404461051\n",
      "Epoch 25, Training Loss: 0.7900390510272263\n",
      "Epoch 26, Training Loss: 0.7895655544180619\n",
      "Epoch 27, Training Loss: 0.787572328354183\n",
      "Epoch 28, Training Loss: 0.7891664301542411\n",
      "Epoch 29, Training Loss: 0.7911110808974818\n",
      "Epoch 30, Training Loss: 0.7886855334267581\n",
      "Epoch 31, Training Loss: 0.7877324363342801\n",
      "Epoch 32, Training Loss: 0.7879619579566153\n",
      "Epoch 33, Training Loss: 0.7882941963081073\n",
      "Epoch 34, Training Loss: 0.7867046293459441\n",
      "Epoch 35, Training Loss: 0.7874860764446115\n",
      "Epoch 36, Training Loss: 0.7877043439033337\n",
      "Epoch 37, Training Loss: 0.7866920842263931\n",
      "Epoch 38, Training Loss: 0.786029411079292\n",
      "Epoch 39, Training Loss: 0.7875189916531843\n",
      "Epoch 40, Training Loss: 0.7858359935588406\n",
      "Epoch 41, Training Loss: 0.7866343613854028\n",
      "Epoch 42, Training Loss: 0.7857850301086454\n",
      "Epoch 43, Training Loss: 0.7853925359876532\n",
      "Epoch 44, Training Loss: 0.7863113773496527\n",
      "Epoch 45, Training Loss: 0.7863681429310849\n",
      "Epoch 46, Training Loss: 0.784501902680648\n",
      "Epoch 47, Training Loss: 0.7856295691396957\n",
      "Epoch 48, Training Loss: 0.7850236544931741\n",
      "Epoch 49, Training Loss: 0.7856851757020878\n",
      "Epoch 50, Training Loss: 0.7848845228216702\n",
      "Epoch 51, Training Loss: 0.7848904682281322\n",
      "Epoch 52, Training Loss: 0.7845086115643494\n",
      "Epoch 53, Training Loss: 0.7847821342317681\n",
      "Epoch 54, Training Loss: 0.7844711430090711\n",
      "Epoch 55, Training Loss: 0.7852812558188474\n",
      "Epoch 56, Training Loss: 0.783930075034163\n",
      "Epoch 57, Training Loss: 0.7831953075595368\n",
      "Epoch 58, Training Loss: 0.7844861620350888\n",
      "Epoch 59, Training Loss: 0.7836799741687631\n",
      "Epoch 60, Training Loss: 0.7844957636711293\n",
      "Epoch 61, Training Loss: 0.7835525606807909\n",
      "Epoch 62, Training Loss: 0.7847391025464338\n",
      "Epoch 63, Training Loss: 0.7837124103890326\n",
      "Epoch 64, Training Loss: 0.7837287224325022\n",
      "Epoch 65, Training Loss: 0.782960537591375\n",
      "Epoch 66, Training Loss: 0.7840455869983013\n",
      "Epoch 67, Training Loss: 0.7827813104579323\n",
      "Epoch 68, Training Loss: 0.7841969064303806\n",
      "Epoch 69, Training Loss: 0.7835875296951237\n",
      "Epoch 70, Training Loss: 0.783835864873757\n",
      "Epoch 71, Training Loss: 0.783778929531126\n",
      "Epoch 72, Training Loss: 0.7833636626265102\n",
      "Epoch 73, Training Loss: 0.7833816180551859\n",
      "Epoch 74, Training Loss: 0.7839900977629468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:23:41,183] Trial 1 finished with value: 0.6357333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 1 with value: 0.6357333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.784171792349421\n",
      "Epoch 1, Training Loss: 0.8883932950216181\n",
      "Epoch 2, Training Loss: 0.8117452129195718\n",
      "Epoch 3, Training Loss: 0.8052494128311382\n",
      "Epoch 4, Training Loss: 0.8030415272712708\n",
      "Epoch 5, Training Loss: 0.8006928433390225\n",
      "Epoch 6, Training Loss: 0.7986036473162034\n",
      "Epoch 7, Training Loss: 0.7978807954928454\n",
      "Epoch 8, Training Loss: 0.7969815034726087\n",
      "Epoch 9, Training Loss: 0.7956643078607671\n",
      "Epoch 10, Training Loss: 0.7945313697702745\n",
      "Epoch 11, Training Loss: 0.792880657490562\n",
      "Epoch 12, Training Loss: 0.7914567335212932\n",
      "Epoch 13, Training Loss: 0.7901396720549639\n",
      "Epoch 14, Training Loss: 0.78978880833177\n",
      "Epoch 15, Training Loss: 0.7892907768137315\n",
      "Epoch 16, Training Loss: 0.7888880688302657\n",
      "Epoch 17, Training Loss: 0.7883030907546773\n",
      "Epoch 18, Training Loss: 0.7876193375447217\n",
      "Epoch 19, Training Loss: 0.7869604532858905\n",
      "Epoch 20, Training Loss: 0.7863005924224854\n",
      "Epoch 21, Training Loss: 0.7858507076431723\n",
      "Epoch 22, Training Loss: 0.7857575666203218\n",
      "Epoch 23, Training Loss: 0.7853240491362179\n",
      "Epoch 24, Training Loss: 0.7853568544107324\n",
      "Epoch 25, Training Loss: 0.7841279949861414\n",
      "Epoch 26, Training Loss: 0.7845307023384992\n",
      "Epoch 27, Training Loss: 0.784064196839052\n",
      "Epoch 28, Training Loss: 0.783993758243673\n",
      "Epoch 29, Training Loss: 0.7834174139359418\n",
      "Epoch 30, Training Loss: 0.783301618309582\n",
      "Epoch 31, Training Loss: 0.7829370509876924\n",
      "Epoch 32, Training Loss: 0.7828250335244571\n",
      "Epoch 33, Training Loss: 0.7826076603637022\n",
      "Epoch 34, Training Loss: 0.7827387231938979\n",
      "Epoch 35, Training Loss: 0.7827502428082859\n",
      "Epoch 36, Training Loss: 0.7826379914143506\n",
      "Epoch 37, Training Loss: 0.7821946063462426\n",
      "Epoch 38, Training Loss: 0.782206326232237\n",
      "Epoch 39, Training Loss: 0.7818232633787043\n",
      "Epoch 40, Training Loss: 0.7816426516280455\n",
      "Epoch 41, Training Loss: 0.7813347070357379\n",
      "Epoch 42, Training Loss: 0.781695287438\n",
      "Epoch 43, Training Loss: 0.7813941771142623\n",
      "Epoch 44, Training Loss: 0.7816575736859266\n",
      "Epoch 45, Training Loss: 0.7812512591305901\n",
      "Epoch 46, Training Loss: 0.7812447614529554\n",
      "Epoch 47, Training Loss: 0.781026696878321\n",
      "Epoch 48, Training Loss: 0.7817602325888241\n",
      "Epoch 49, Training Loss: 0.780900611596949\n",
      "Epoch 50, Training Loss: 0.780706133632099\n",
      "Epoch 51, Training Loss: 0.7805897699384128\n",
      "Epoch 52, Training Loss: 0.7804867956217597\n",
      "Epoch 53, Training Loss: 0.7806326277816997\n",
      "Epoch 54, Training Loss: 0.7806977856159211\n",
      "Epoch 55, Training Loss: 0.7799690576160655\n",
      "Epoch 56, Training Loss: 0.7801471826609443\n",
      "Epoch 57, Training Loss: 0.7800011878855089\n",
      "Epoch 58, Training Loss: 0.7800157168332268\n",
      "Epoch 59, Training Loss: 0.7796865325114306\n",
      "Epoch 60, Training Loss: 0.7798334085240084\n",
      "Epoch 61, Training Loss: 0.7795193013724159\n",
      "Epoch 62, Training Loss: 0.7795508475163404\n",
      "Epoch 63, Training Loss: 0.7795002452766194\n",
      "Epoch 64, Training Loss: 0.7797953483637642\n",
      "Epoch 65, Training Loss: 0.7797496115460115\n",
      "Epoch 66, Training Loss: 0.7791338121891022\n",
      "Epoch 67, Training Loss: 0.7793695867061615\n",
      "Epoch 68, Training Loss: 0.7792906405995874\n",
      "Epoch 69, Training Loss: 0.779307748780531\n",
      "Epoch 70, Training Loss: 0.7787702073770411\n",
      "Epoch 71, Training Loss: 0.7792147853093989\n",
      "Epoch 72, Training Loss: 0.7788728864753948\n",
      "Epoch 73, Training Loss: 0.7791369843482971\n",
      "Epoch 74, Training Loss: 0.7790530725086436\n",
      "Epoch 75, Training Loss: 0.7787334467382993\n",
      "Epoch 76, Training Loss: 0.7786593121640822\n",
      "Epoch 77, Training Loss: 0.7784931619027081\n",
      "Epoch 78, Training Loss: 0.778317088099087\n",
      "Epoch 79, Training Loss: 0.7785606276287752\n",
      "Epoch 80, Training Loss: 0.7784623181819916\n",
      "Epoch 81, Training Loss: 0.7782937894849217\n",
      "Epoch 82, Training Loss: 0.778342392093995\n",
      "Epoch 83, Training Loss: 0.7783695729339823\n",
      "Epoch 84, Training Loss: 0.7783406015003429\n",
      "Epoch 85, Training Loss: 0.7783857729855705\n",
      "Epoch 86, Training Loss: 0.7778319172999438\n",
      "Epoch 87, Training Loss: 0.7779705951494329\n",
      "Epoch 88, Training Loss: 0.7776542112406563\n",
      "Epoch 89, Training Loss: 0.7781809557886684\n",
      "Epoch 90, Training Loss: 0.7775462325180278\n",
      "Epoch 91, Training Loss: 0.778101285906399\n",
      "Epoch 92, Training Loss: 0.7772601603059208\n",
      "Epoch 93, Training Loss: 0.7777398884296417\n",
      "Epoch 94, Training Loss: 0.7778213466616238\n",
      "Epoch 95, Training Loss: 0.7777556208301993\n",
      "Epoch 96, Training Loss: 0.7779969604576336\n",
      "Epoch 97, Training Loss: 0.777633018283283\n",
      "Epoch 98, Training Loss: 0.777524388888303\n",
      "Epoch 99, Training Loss: 0.7774925771180321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:25:53,169] Trial 2 finished with value: 0.6389333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 2 with value: 0.6389333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7770736574425416\n",
      "Epoch 1, Training Loss: 1.0675527129453772\n",
      "Epoch 2, Training Loss: 1.0286191394048578\n",
      "Epoch 3, Training Loss: 1.0053329504938686\n",
      "Epoch 4, Training Loss: 0.9900265312194825\n",
      "Epoch 5, Training Loss: 0.9798980452733881\n",
      "Epoch 6, Training Loss: 0.9732075503293206\n",
      "Epoch 7, Training Loss: 0.9687370807283064\n",
      "Epoch 8, Training Loss: 0.9656543507996728\n",
      "Epoch 9, Training Loss: 0.9634404901195975\n",
      "Epoch 10, Training Loss: 0.961740565440234\n",
      "Epoch 11, Training Loss: 0.9603508798515096\n",
      "Epoch 12, Training Loss: 0.9591361499533934\n",
      "Epoch 13, Training Loss: 0.9580292479430927\n",
      "Epoch 14, Training Loss: 0.9569862423223607\n",
      "Epoch 15, Training Loss: 0.9559699947693768\n",
      "Epoch 16, Training Loss: 0.954976412338369\n",
      "Epoch 17, Training Loss: 0.9539874856612262\n",
      "Epoch 18, Training Loss: 0.9529995620250702\n",
      "Epoch 19, Training Loss: 0.9519991064071656\n",
      "Epoch 20, Training Loss: 0.9510016686776105\n",
      "Epoch 21, Training Loss: 0.9499832019385169\n",
      "Epoch 22, Training Loss: 0.9489543520703035\n",
      "Epoch 23, Training Loss: 0.9479069168427411\n",
      "Epoch 24, Training Loss: 0.9468462543627795\n",
      "Epoch 25, Training Loss: 0.94576384193757\n",
      "Epoch 26, Training Loss: 0.9446645335590138\n",
      "Epoch 27, Training Loss: 0.9435373710884768\n",
      "Epoch 28, Training Loss: 0.9423964548110962\n",
      "Epoch 29, Training Loss: 0.9412234676585478\n",
      "Epoch 30, Training Loss: 0.9400142412325915\n",
      "Epoch 31, Training Loss: 0.9387823444254259\n",
      "Epoch 32, Training Loss: 0.9375367530654458\n",
      "Epoch 33, Training Loss: 0.9362286844674279\n",
      "Epoch 34, Training Loss: 0.9349155754902784\n",
      "Epoch 35, Training Loss: 0.9335519590798547\n",
      "Epoch 36, Training Loss: 0.9321608532176299\n",
      "Epoch 37, Training Loss: 0.9307196218827192\n",
      "Epoch 38, Training Loss: 0.9292421038711772\n",
      "Epoch 39, Training Loss: 0.9277260233374203\n",
      "Epoch 40, Training Loss: 0.92616815440795\n",
      "Epoch 41, Training Loss: 0.9245680182120379\n",
      "Epoch 42, Training Loss: 0.9229191651063807\n",
      "Epoch 43, Training Loss: 0.9212222752851599\n",
      "Epoch 44, Training Loss: 0.9194791150794309\n",
      "Epoch 45, Training Loss: 0.9176958295177011\n",
      "Epoch 46, Training Loss: 0.9158635649961584\n",
      "Epoch 47, Training Loss: 0.9139830145415138\n",
      "Epoch 48, Training Loss: 0.9120574254849377\n",
      "Epoch 49, Training Loss: 0.9100752050736372\n",
      "Epoch 50, Training Loss: 0.9080646065403434\n",
      "Epoch 51, Training Loss: 0.9059966514391058\n",
      "Epoch 52, Training Loss: 0.9038990556492525\n",
      "Epoch 53, Training Loss: 0.9017437120044932\n",
      "Epoch 54, Training Loss: 0.8995657026066499\n",
      "Epoch 55, Training Loss: 0.8973623172675862\n",
      "Epoch 56, Training Loss: 0.8951148791874156\n",
      "Epoch 57, Training Loss: 0.8928537874362048\n",
      "Epoch 58, Training Loss: 0.8905405802586499\n",
      "Epoch 59, Training Loss: 0.8882454140046063\n",
      "Epoch 60, Training Loss: 0.8859055294710048\n",
      "Epoch 61, Training Loss: 0.8835932517752928\n",
      "Epoch 62, Training Loss: 0.8812509220487931\n",
      "Epoch 63, Training Loss: 0.8789355244356043\n",
      "Epoch 64, Training Loss: 0.87661800959531\n",
      "Epoch 65, Training Loss: 0.8743097711310667\n",
      "Epoch 66, Training Loss: 0.8720054432925056\n",
      "Epoch 67, Training Loss: 0.869757966223885\n",
      "Epoch 68, Training Loss: 0.8675486616527333\n",
      "Epoch 69, Training Loss: 0.8653547122899223\n",
      "Epoch 70, Training Loss: 0.8632059935261222\n",
      "Epoch 71, Training Loss: 0.8610951993044685\n",
      "Epoch 72, Training Loss: 0.8590312492146212\n",
      "Epoch 73, Training Loss: 0.8570201262305764\n",
      "Epoch 74, Training Loss: 0.855095414834864\n",
      "Epoch 75, Training Loss: 0.8532137551027186\n",
      "Epoch 76, Training Loss: 0.8513680850758272\n",
      "Epoch 77, Training Loss: 0.8495768012018765\n",
      "Epoch 78, Training Loss: 0.8478909355752609\n",
      "Epoch 79, Training Loss: 0.8462072340179893\n",
      "Epoch 80, Training Loss: 0.8446556784826167\n",
      "Epoch 81, Training Loss: 0.8431217521779677\n",
      "Epoch 82, Training Loss: 0.8416530254307916\n",
      "Epoch 83, Training Loss: 0.8402603176762076\n",
      "Epoch 84, Training Loss: 0.8389166609679951\n",
      "Epoch 85, Training Loss: 0.8375978683724122\n",
      "Epoch 86, Training Loss: 0.8363861198986278\n",
      "Epoch 87, Training Loss: 0.8351928690601798\n",
      "Epoch 88, Training Loss: 0.8340701371781967\n",
      "Epoch 89, Training Loss: 0.8329691389027764\n",
      "Epoch 90, Training Loss: 0.831968889727312\n",
      "Epoch 91, Training Loss: 0.8309723002069137\n",
      "Epoch 92, Training Loss: 0.8300095708931193\n",
      "Epoch 93, Training Loss: 0.8291135138623855\n",
      "Epoch 94, Training Loss: 0.8282788732472588\n",
      "Epoch 95, Training Loss: 0.8274612014433916\n",
      "Epoch 96, Training Loss: 0.8266575128190657\n",
      "Epoch 97, Training Loss: 0.8259210456820095\n",
      "Epoch 98, Training Loss: 0.8251553128747379\n",
      "Epoch 99, Training Loss: 0.824516137417625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:27:37,233] Trial 3 finished with value: 0.6206 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 2 with value: 0.6389333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8238496854024775\n",
      "Epoch 1, Training Loss: 0.9864907323612886\n",
      "Epoch 2, Training Loss: 0.9390150266535142\n",
      "Epoch 3, Training Loss: 0.9284145656753989\n",
      "Epoch 4, Training Loss: 0.9207891790305867\n",
      "Epoch 5, Training Loss: 0.914372659220415\n",
      "Epoch 6, Training Loss: 0.9084101645385518\n",
      "Epoch 7, Training Loss: 0.9024661141283372\n",
      "Epoch 8, Training Loss: 0.8964842926754671\n",
      "Epoch 9, Training Loss: 0.8901482241995194\n",
      "Epoch 10, Training Loss: 0.8833308687630822\n",
      "Epoch 11, Training Loss: 0.8760969593244441\n",
      "Epoch 12, Training Loss: 0.8686967816072352\n",
      "Epoch 13, Training Loss: 0.8611265055572286\n",
      "Epoch 14, Training Loss: 0.8536994237058303\n",
      "Epoch 15, Training Loss: 0.846674872005687\n",
      "Epoch 16, Training Loss: 0.839994703391019\n",
      "Epoch 17, Training Loss: 0.8342465392982259\n",
      "Epoch 18, Training Loss: 0.8290080660230973\n",
      "Epoch 19, Training Loss: 0.8245808509518119\n",
      "Epoch 20, Training Loss: 0.8209525926674114\n",
      "Epoch 21, Training Loss: 0.8178869676589966\n",
      "Epoch 22, Training Loss: 0.8153010660760542\n",
      "Epoch 23, Training Loss: 0.813250331808539\n",
      "Epoch 24, Training Loss: 0.8115027866644018\n",
      "Epoch 25, Training Loss: 0.8100047462126788\n",
      "Epoch 26, Training Loss: 0.8087082512238446\n",
      "Epoch 27, Training Loss: 0.8077964282737059\n",
      "Epoch 28, Training Loss: 0.80691162424929\n",
      "Epoch 29, Training Loss: 0.8061122098389794\n",
      "Epoch 30, Training Loss: 0.8053748825718375\n",
      "Epoch 31, Training Loss: 0.8047552460081437\n",
      "Epoch 32, Training Loss: 0.8041889523057376\n",
      "Epoch 33, Training Loss: 0.8037259087141823\n",
      "Epoch 34, Training Loss: 0.8031904221983517\n",
      "Epoch 35, Training Loss: 0.8026457653326147\n",
      "Epoch 36, Training Loss: 0.8021296155452728\n",
      "Epoch 37, Training Loss: 0.8017014019629535\n",
      "Epoch 38, Training Loss: 0.8014231401331284\n",
      "Epoch 39, Training Loss: 0.8009772140138289\n",
      "Epoch 40, Training Loss: 0.8007101800862481\n",
      "Epoch 41, Training Loss: 0.8004035630646874\n",
      "Epoch 42, Training Loss: 0.8000495260603288\n",
      "Epoch 43, Training Loss: 0.799818609532188\n",
      "Epoch 44, Training Loss: 0.7995795444179984\n",
      "Epoch 45, Training Loss: 0.7994228974510642\n",
      "Epoch 46, Training Loss: 0.7991919526633094\n",
      "Epoch 47, Training Loss: 0.7991110742092132\n",
      "Epoch 48, Training Loss: 0.7988893454215106\n",
      "Epoch 49, Training Loss: 0.7987211331900428\n",
      "Epoch 50, Training Loss: 0.7986721469374264\n",
      "Epoch 51, Training Loss: 0.7985587676833658\n",
      "Epoch 52, Training Loss: 0.7982952934152939\n",
      "Epoch 53, Training Loss: 0.7981338721163133\n",
      "Epoch 54, Training Loss: 0.7980587054701412\n",
      "Epoch 55, Training Loss: 0.7979155185643364\n",
      "Epoch 56, Training Loss: 0.7977804999491748\n",
      "Epoch 57, Training Loss: 0.7976755480205312\n",
      "Epoch 58, Training Loss: 0.7975754836727591\n",
      "Epoch 59, Training Loss: 0.7976170993552488\n",
      "Epoch 60, Training Loss: 0.7974272979007048\n",
      "Epoch 61, Training Loss: 0.7973612194201526\n",
      "Epoch 62, Training Loss: 0.797255088932374\n",
      "Epoch 63, Training Loss: 0.7972349436142865\n",
      "Epoch 64, Training Loss: 0.797109630318249\n",
      "Epoch 65, Training Loss: 0.7970524253564722\n",
      "Epoch 66, Training Loss: 0.796883417858797\n",
      "Epoch 67, Training Loss: 0.7968251047414892\n",
      "Epoch 68, Training Loss: 0.796648488184985\n",
      "Epoch 69, Training Loss: 0.7966769338355345\n",
      "Epoch 70, Training Loss: 0.7966258063737084\n",
      "Epoch 71, Training Loss: 0.796444091165767\n",
      "Epoch 72, Training Loss: 0.7963359243028304\n",
      "Epoch 73, Training Loss: 0.7964074821331921\n",
      "Epoch 74, Training Loss: 0.7962080240249634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:28:46,272] Trial 4 finished with value: 0.6364 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 2 with value: 0.6389333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7960772259796367\n",
      "Epoch 1, Training Loss: 0.8498208867802339\n",
      "Epoch 2, Training Loss: 0.8210680193059585\n",
      "Epoch 3, Training Loss: 0.8173099057113423\n",
      "Epoch 4, Training Loss: 0.8148494346001569\n",
      "Epoch 5, Training Loss: 0.8123356537959155\n",
      "Epoch 6, Training Loss: 0.8121256495223326\n",
      "Epoch 7, Training Loss: 0.8099922975371866\n",
      "Epoch 8, Training Loss: 0.809430210239747\n",
      "Epoch 9, Training Loss: 0.8074684533652138\n",
      "Epoch 10, Training Loss: 0.807631158618366\n",
      "Epoch 11, Training Loss: 0.8068281350416295\n",
      "Epoch 12, Training Loss: 0.8090357408102821\n",
      "Epoch 13, Training Loss: 0.8073560556243448\n",
      "Epoch 14, Training Loss: 0.8070857003857108\n",
      "Epoch 15, Training Loss: 0.8060340033559238\n",
      "Epoch 16, Training Loss: 0.8045502256645876\n",
      "Epoch 17, Training Loss: 0.8056320413421182\n",
      "Epoch 18, Training Loss: 0.8059139569366679\n",
      "Epoch 19, Training Loss: 0.8045404483290279\n",
      "Epoch 20, Training Loss: 0.8058384239673615\n",
      "Epoch 21, Training Loss: 0.804000446656171\n",
      "Epoch 22, Training Loss: 0.8040138334386489\n",
      "Epoch 23, Training Loss: 0.8046992022149703\n",
      "Epoch 24, Training Loss: 0.8048313935363994\n",
      "Epoch 25, Training Loss: 0.8051825646793141\n",
      "Epoch 26, Training Loss: 0.8042995722854839\n",
      "Epoch 27, Training Loss: 0.8033533237962162\n",
      "Epoch 28, Training Loss: 0.8042121284849504\n",
      "Epoch 29, Training Loss: 0.8043892976115732\n",
      "Epoch 30, Training Loss: 0.8032448373121374\n",
      "Epoch 31, Training Loss: 0.8032821333408355\n",
      "Epoch 32, Training Loss: 0.803521141725428\n",
      "Epoch 33, Training Loss: 0.8035586899168351\n",
      "Epoch 34, Training Loss: 0.8032197108689476\n",
      "Epoch 35, Training Loss: 0.8029562044844908\n",
      "Epoch 36, Training Loss: 0.802650953391019\n",
      "Epoch 37, Training Loss: 0.8028353461097268\n",
      "Epoch 38, Training Loss: 0.8025331591157352\n",
      "Epoch 39, Training Loss: 0.801881899062325\n",
      "Epoch 40, Training Loss: 0.8024756273802589\n",
      "Epoch 41, Training Loss: 0.8016610381883733\n",
      "Epoch 42, Training Loss: 0.8008651018142701\n",
      "Epoch 43, Training Loss: 0.8019512415633482\n",
      "Epoch 44, Training Loss: 0.8012249930465922\n",
      "Epoch 45, Training Loss: 0.800769592032713\n",
      "Epoch 46, Training Loss: 0.8019590406558093\n",
      "Epoch 47, Training Loss: 0.8009221442306743\n",
      "Epoch 48, Training Loss: 0.801061677091262\n",
      "Epoch 49, Training Loss: 0.8008125021878411\n",
      "Epoch 50, Training Loss: 0.8003696743881001\n",
      "Epoch 51, Training Loss: 0.7996464335217195\n",
      "Epoch 52, Training Loss: 0.7992747384660385\n",
      "Epoch 53, Training Loss: 0.7991284018404343\n",
      "Epoch 54, Training Loss: 0.798702625877717\n",
      "Epoch 55, Training Loss: 0.799456616008983\n",
      "Epoch 56, Training Loss: 0.7989179652578691\n",
      "Epoch 57, Training Loss: 0.7981466667792376\n",
      "Epoch 58, Training Loss: 0.7984736582812141\n",
      "Epoch 59, Training Loss: 0.7983939578953911\n",
      "Epoch 60, Training Loss: 0.7973721300153171\n",
      "Epoch 61, Training Loss: 0.7987031797100516\n",
      "Epoch 62, Training Loss: 0.7981097426133997\n",
      "Epoch 63, Training Loss: 0.7966172143291025\n",
      "Epoch 64, Training Loss: 0.7975146260682274\n",
      "Epoch 65, Training Loss: 0.7968411346042857\n",
      "Epoch 66, Training Loss: 0.7969288365981159\n",
      "Epoch 67, Training Loss: 0.7967124988051022\n",
      "Epoch 68, Training Loss: 0.7969625379758722\n",
      "Epoch 69, Training Loss: 0.7967758516704335\n",
      "Epoch 70, Training Loss: 0.7961207442423877\n",
      "Epoch 71, Training Loss: 0.7971468923372381\n",
      "Epoch 72, Training Loss: 0.7961256539821625\n",
      "Epoch 73, Training Loss: 0.7958634705403271\n",
      "Epoch 74, Training Loss: 0.7971163662040934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:30:11,885] Trial 5 finished with value: 0.6294666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 2 with value: 0.6389333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7961943291215335\n",
      "Epoch 1, Training Loss: 0.8891367528719061\n",
      "Epoch 2, Training Loss: 0.8222721184001249\n",
      "Epoch 3, Training Loss: 0.8156676404616412\n",
      "Epoch 4, Training Loss: 0.8111233756822698\n",
      "Epoch 5, Training Loss: 0.8062971225906821\n",
      "Epoch 6, Training Loss: 0.8020406685155981\n",
      "Epoch 7, Training Loss: 0.7997197721284979\n",
      "Epoch 8, Training Loss: 0.7980636020968942\n",
      "Epoch 9, Training Loss: 0.796605109467226\n",
      "Epoch 10, Training Loss: 0.7947269178839291\n",
      "Epoch 11, Training Loss: 0.7940368291910956\n",
      "Epoch 12, Training Loss: 0.792315918487661\n",
      "Epoch 13, Training Loss: 0.7927128070242264\n",
      "Epoch 14, Training Loss: 0.7915098503757926\n",
      "Epoch 15, Training Loss: 0.7910292223621818\n",
      "Epoch 16, Training Loss: 0.790757698171279\n",
      "Epoch 17, Training Loss: 0.7905519455320695\n",
      "Epoch 18, Training Loss: 0.7902031315074247\n",
      "Epoch 19, Training Loss: 0.7899395561919493\n",
      "Epoch 20, Training Loss: 0.7891816448464113\n",
      "Epoch 21, Training Loss: 0.7894913395012126\n",
      "Epoch 22, Training Loss: 0.7890539397912867\n",
      "Epoch 23, Training Loss: 0.7888681052712834\n",
      "Epoch 24, Training Loss: 0.7886889426848468\n",
      "Epoch 25, Training Loss: 0.7883092474236207\n",
      "Epoch 26, Training Loss: 0.7885422256413628\n",
      "Epoch 27, Training Loss: 0.7880463531438042\n",
      "Epoch 28, Training Loss: 0.7875424467114841\n",
      "Epoch 29, Training Loss: 0.7880794429779052\n",
      "Epoch 30, Training Loss: 0.7875273231899037\n",
      "Epoch 31, Training Loss: 0.7876824188933653\n",
      "Epoch 32, Training Loss: 0.7872491996428546\n",
      "Epoch 33, Training Loss: 0.7869738196625429\n",
      "Epoch 34, Training Loss: 0.786906396150589\n",
      "Epoch 35, Training Loss: 0.7866268679674934\n",
      "Epoch 36, Training Loss: 0.7865398843148176\n",
      "Epoch 37, Training Loss: 0.7864935745912439\n",
      "Epoch 38, Training Loss: 0.7862642945962793\n",
      "Epoch 39, Training Loss: 0.7863107588711907\n",
      "Epoch 40, Training Loss: 0.786163069711012\n",
      "Epoch 41, Training Loss: 0.7860620408899643\n",
      "Epoch 42, Training Loss: 0.7859873611085555\n",
      "Epoch 43, Training Loss: 0.7860995584375718\n",
      "Epoch 44, Training Loss: 0.7854445709901697\n",
      "Epoch 45, Training Loss: 0.7853440215307124\n",
      "Epoch 46, Training Loss: 0.785097994594013\n",
      "Epoch 47, Training Loss: 0.7849368455129512\n",
      "Epoch 48, Training Loss: 0.7850461731938755\n",
      "Epoch 49, Training Loss: 0.7853083672243006\n",
      "Epoch 50, Training Loss: 0.7851843262419981\n",
      "Epoch 51, Training Loss: 0.7850894537392784\n",
      "Epoch 52, Training Loss: 0.7849789160840651\n",
      "Epoch 53, Training Loss: 0.7851374350575839\n",
      "Epoch 54, Training Loss: 0.7848561357049381\n",
      "Epoch 55, Training Loss: 0.7849994519878837\n",
      "Epoch 56, Training Loss: 0.7845213362048654\n",
      "Epoch 57, Training Loss: 0.7845788459918078\n",
      "Epoch 58, Training Loss: 0.7846717921425315\n",
      "Epoch 59, Training Loss: 0.7841896162313573\n",
      "Epoch 60, Training Loss: 0.7844808249613818\n",
      "Epoch 61, Training Loss: 0.783983191392001\n",
      "Epoch 62, Training Loss: 0.784656230211258\n",
      "Epoch 63, Training Loss: 0.7840002919645871\n",
      "Epoch 64, Training Loss: 0.7842864511994755\n",
      "Epoch 65, Training Loss: 0.7843272568899042\n",
      "Epoch 66, Training Loss: 0.784331528930103\n",
      "Epoch 67, Training Loss: 0.7840795035221998\n",
      "Epoch 68, Training Loss: 0.784330607582541\n",
      "Epoch 69, Training Loss: 0.7844920722176046\n",
      "Epoch 70, Training Loss: 0.7840320606792675\n",
      "Epoch 71, Training Loss: 0.7842016151372124\n",
      "Epoch 72, Training Loss: 0.7840978445025051\n",
      "Epoch 73, Training Loss: 0.7838414288969601\n",
      "Epoch 74, Training Loss: 0.7839102965242722\n",
      "Epoch 75, Training Loss: 0.784390826365527\n",
      "Epoch 76, Training Loss: 0.7841715176666484\n",
      "Epoch 77, Training Loss: 0.7837959337935728\n",
      "Epoch 78, Training Loss: 0.7838174402713776\n",
      "Epoch 79, Training Loss: 0.7837424071396099\n",
      "Epoch 80, Training Loss: 0.7836721329128041\n",
      "Epoch 81, Training Loss: 0.7836898980421179\n",
      "Epoch 82, Training Loss: 0.7834848471248851\n",
      "Epoch 83, Training Loss: 0.783818483282538\n",
      "Epoch 84, Training Loss: 0.7836451095693252\n",
      "Epoch 85, Training Loss: 0.7833062718195074\n",
      "Epoch 86, Training Loss: 0.7837027135316064\n",
      "Epoch 87, Training Loss: 0.7833156028214623\n",
      "Epoch 88, Training Loss: 0.7833513553703533\n",
      "Epoch 89, Training Loss: 0.7834376985185286\n",
      "Epoch 90, Training Loss: 0.783363386182224\n",
      "Epoch 91, Training Loss: 0.7830615166355582\n",
      "Epoch 92, Training Loss: 0.7835126645424787\n",
      "Epoch 93, Training Loss: 0.7835690123193404\n",
      "Epoch 94, Training Loss: 0.7829185927615446\n",
      "Epoch 95, Training Loss: 0.7829923526679768\n",
      "Epoch 96, Training Loss: 0.7834423602328581\n",
      "Epoch 97, Training Loss: 0.7829704036432154\n",
      "Epoch 98, Training Loss: 0.7830630129225113\n",
      "Epoch 99, Training Loss: 0.7832345146291396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:32:14,008] Trial 6 finished with value: 0.6372666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 2 with value: 0.6389333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7828595045734854\n",
      "Epoch 1, Training Loss: 0.8409616178139708\n",
      "Epoch 2, Training Loss: 0.814973198829737\n",
      "Epoch 3, Training Loss: 0.808300324759089\n",
      "Epoch 4, Training Loss: 0.8057904265877\n",
      "Epoch 5, Training Loss: 0.8037211254127044\n",
      "Epoch 6, Training Loss: 0.8062529084377719\n",
      "Epoch 7, Training Loss: 0.8029466582420177\n",
      "Epoch 8, Training Loss: 0.8014576998868383\n",
      "Epoch 9, Training Loss: 0.8016810708476189\n",
      "Epoch 10, Training Loss: 0.8005321494618752\n",
      "Epoch 11, Training Loss: 0.8014641483923546\n",
      "Epoch 12, Training Loss: 0.7985579627797119\n",
      "Epoch 13, Training Loss: 0.8010299341122907\n",
      "Epoch 14, Training Loss: 0.7996824018937304\n",
      "Epoch 15, Training Loss: 0.7994250388073741\n",
      "Epoch 16, Training Loss: 0.7981717091753967\n",
      "Epoch 17, Training Loss: 0.7993378739607961\n",
      "Epoch 18, Training Loss: 0.798233021201944\n",
      "Epoch 19, Training Loss: 0.7972221560047982\n",
      "Epoch 20, Training Loss: 0.7990195346057863\n",
      "Epoch 21, Training Loss: 0.7971442842842045\n",
      "Epoch 22, Training Loss: 0.7993182787321564\n",
      "Epoch 23, Training Loss: 0.7967219012124198\n",
      "Epoch 24, Training Loss: 0.7980930860777546\n",
      "Epoch 25, Training Loss: 0.7965878549375032\n",
      "Epoch 26, Training Loss: 0.7966719108416622\n",
      "Epoch 27, Training Loss: 0.7971580322523762\n",
      "Epoch 28, Training Loss: 0.7972558631036515\n",
      "Epoch 29, Training Loss: 0.7964693366136766\n",
      "Epoch 30, Training Loss: 0.7979418421150151\n",
      "Epoch 31, Training Loss: 0.79789781435988\n",
      "Epoch 32, Training Loss: 0.796172319050122\n",
      "Epoch 33, Training Loss: 0.7968633176688861\n",
      "Epoch 34, Training Loss: 0.7973127673443099\n",
      "Epoch 35, Training Loss: 0.7986671898598061\n",
      "Epoch 36, Training Loss: 0.7980546513894446\n",
      "Epoch 37, Training Loss: 0.7966152787208557\n",
      "Epoch 38, Training Loss: 0.79728712049642\n",
      "Epoch 39, Training Loss: 0.7977960979131827\n",
      "Epoch 40, Training Loss: 0.7958702832236326\n",
      "Epoch 41, Training Loss: 0.7972862296534661\n",
      "Epoch 42, Training Loss: 0.7966484667663287\n",
      "Epoch 43, Training Loss: 0.7957551907775994\n",
      "Epoch 44, Training Loss: 0.7972186245416341\n",
      "Epoch 45, Training Loss: 0.7969923847600033\n",
      "Epoch 46, Training Loss: 0.7957760446053699\n",
      "Epoch 47, Training Loss: 0.7968371713071838\n",
      "Epoch 48, Training Loss: 0.795691624709538\n",
      "Epoch 49, Training Loss: 0.7957852538366963\n",
      "Epoch 50, Training Loss: 0.796714487111658\n",
      "Epoch 51, Training Loss: 0.7965417285610858\n",
      "Epoch 52, Training Loss: 0.7953858266199442\n",
      "Epoch 53, Training Loss: 0.7955912953032587\n",
      "Epoch 54, Training Loss: 0.7947600652400713\n",
      "Epoch 55, Training Loss: 0.7961726267534988\n",
      "Epoch 56, Training Loss: 0.7964559861591884\n",
      "Epoch 57, Training Loss: 0.7971361291139646\n",
      "Epoch 58, Training Loss: 0.7957350939736331\n",
      "Epoch 59, Training Loss: 0.7957237771579198\n",
      "Epoch 60, Training Loss: 0.7961789722729447\n",
      "Epoch 61, Training Loss: 0.7960009931621695\n",
      "Epoch 62, Training Loss: 0.7959802609637268\n",
      "Epoch 63, Training Loss: 0.7957027159239116\n",
      "Epoch 64, Training Loss: 0.7963277441218384\n",
      "Epoch 65, Training Loss: 0.7955146922204728\n",
      "Epoch 66, Training Loss: 0.7954036345159201\n",
      "Epoch 67, Training Loss: 0.7951243762683151\n",
      "Epoch 68, Training Loss: 0.7949001096245042\n",
      "Epoch 69, Training Loss: 0.7959105735434625\n",
      "Epoch 70, Training Loss: 0.795436445931743\n",
      "Epoch 71, Training Loss: 0.7961971967740167\n",
      "Epoch 72, Training Loss: 0.7955311889935257\n",
      "Epoch 73, Training Loss: 0.7939821238804581\n",
      "Epoch 74, Training Loss: 0.7954388759189979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:33:39,058] Trial 7 finished with value: 0.634 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 2 with value: 0.6389333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7969200909585881\n",
      "Epoch 1, Training Loss: 0.8937653549631736\n",
      "Epoch 2, Training Loss: 0.8267773118234218\n",
      "Epoch 3, Training Loss: 0.8182755238131473\n",
      "Epoch 4, Training Loss: 0.8159709951931373\n",
      "Epoch 5, Training Loss: 0.8121080883463523\n",
      "Epoch 6, Training Loss: 0.8105923895549058\n",
      "Epoch 7, Training Loss: 0.808597215136191\n",
      "Epoch 8, Training Loss: 0.8081216087914947\n",
      "Epoch 9, Training Loss: 0.8065563945841968\n",
      "Epoch 10, Training Loss: 0.8068041476091944\n",
      "Epoch 11, Training Loss: 0.8041198532384141\n",
      "Epoch 12, Training Loss: 0.8047890794904609\n",
      "Epoch 13, Training Loss: 0.8046640640810916\n",
      "Epoch 14, Training Loss: 0.803121066451969\n",
      "Epoch 15, Training Loss: 0.8018484013421195\n",
      "Epoch 16, Training Loss: 0.8032328137777802\n",
      "Epoch 17, Training Loss: 0.8021031272142454\n",
      "Epoch 18, Training Loss: 0.802086936083055\n",
      "Epoch 19, Training Loss: 0.801636364675106\n",
      "Epoch 20, Training Loss: 0.8012702851815331\n",
      "Epoch 21, Training Loss: 0.8005966411497359\n",
      "Epoch 22, Training Loss: 0.8010202774427887\n",
      "Epoch 23, Training Loss: 0.8013397300153746\n",
      "Epoch 24, Training Loss: 0.8014789012141694\n",
      "Epoch 25, Training Loss: 0.8011620176465888\n",
      "Epoch 26, Training Loss: 0.7997666539106154\n",
      "Epoch 27, Training Loss: 0.8009454624993461\n",
      "Epoch 28, Training Loss: 0.8002328315175565\n",
      "Epoch 29, Training Loss: 0.7997907159920026\n",
      "Epoch 30, Training Loss: 0.7999054254445814\n",
      "Epoch 31, Training Loss: 0.7985782845576007\n",
      "Epoch 32, Training Loss: 0.7986272028514317\n",
      "Epoch 33, Training Loss: 0.8004488262915074\n",
      "Epoch 34, Training Loss: 0.7998965478481207\n",
      "Epoch 35, Training Loss: 0.7991291453963831\n",
      "Epoch 36, Training Loss: 0.7999311574419639\n",
      "Epoch 37, Training Loss: 0.7981367813017135\n",
      "Epoch 38, Training Loss: 0.7987839322341116\n",
      "Epoch 39, Training Loss: 0.7994199371875677\n",
      "Epoch 40, Training Loss: 0.7992430084630062\n",
      "Epoch 41, Training Loss: 0.7988451234380105\n",
      "Epoch 42, Training Loss: 0.7984696701056975\n",
      "Epoch 43, Training Loss: 0.7985868415438143\n",
      "Epoch 44, Training Loss: 0.7980366689818246\n",
      "Epoch 45, Training Loss: 0.7980363910359548\n",
      "Epoch 46, Training Loss: 0.7993398069439078\n",
      "Epoch 47, Training Loss: 0.7986817725618979\n",
      "Epoch 48, Training Loss: 0.7981088462628816\n",
      "Epoch 49, Training Loss: 0.7986846925620746\n",
      "Epoch 50, Training Loss: 0.79785382048528\n",
      "Epoch 51, Training Loss: 0.798309424317869\n",
      "Epoch 52, Training Loss: 0.7982236669475871\n",
      "Epoch 53, Training Loss: 0.7978672213124153\n",
      "Epoch 54, Training Loss: 0.7985374846852812\n",
      "Epoch 55, Training Loss: 0.7979030861890406\n",
      "Epoch 56, Training Loss: 0.7990492069631591\n",
      "Epoch 57, Training Loss: 0.7981238390270032\n",
      "Epoch 58, Training Loss: 0.7983934283256531\n",
      "Epoch 59, Training Loss: 0.7978272266854021\n",
      "Epoch 60, Training Loss: 0.7978896271017261\n",
      "Epoch 61, Training Loss: 0.798282672139935\n",
      "Epoch 62, Training Loss: 0.7975916885791865\n",
      "Epoch 63, Training Loss: 0.798469206713196\n",
      "Epoch 64, Training Loss: 0.7980450757464072\n",
      "Epoch 65, Training Loss: 0.7972542172087762\n",
      "Epoch 66, Training Loss: 0.7977321437426976\n",
      "Epoch 67, Training Loss: 0.7975978733901691\n",
      "Epoch 68, Training Loss: 0.7977526862818496\n",
      "Epoch 69, Training Loss: 0.797584939630408\n",
      "Epoch 70, Training Loss: 0.7973470089130832\n",
      "Epoch 71, Training Loss: 0.7973697163108596\n",
      "Epoch 72, Training Loss: 0.7982913654549677\n",
      "Epoch 73, Training Loss: 0.7976508465924658\n",
      "Epoch 74, Training Loss: 0.7980371077257887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:35:01,087] Trial 8 finished with value: 0.5224 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 2 with value: 0.6389333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7986572758595746\n",
      "Epoch 1, Training Loss: 0.9771949274399702\n",
      "Epoch 2, Training Loss: 0.9435537600517273\n",
      "Epoch 3, Training Loss: 0.9250832664265353\n",
      "Epoch 4, Training Loss: 0.9057494124244241\n",
      "Epoch 5, Training Loss: 0.8861456661364612\n",
      "Epoch 6, Training Loss: 0.8675628340945525\n",
      "Epoch 7, Training Loss: 0.8514152530361624\n",
      "Epoch 8, Training Loss: 0.8387264691380893\n",
      "Epoch 9, Training Loss: 0.8293338974784402\n",
      "Epoch 10, Training Loss: 0.8229183339371401\n",
      "Epoch 11, Training Loss: 0.8186043512119966\n",
      "Epoch 12, Training Loss: 0.8155342728250167\n",
      "Epoch 13, Training Loss: 0.81330006999128\n",
      "Epoch 14, Training Loss: 0.8117932192718281\n",
      "Epoch 15, Training Loss: 0.8107116388573367\n",
      "Epoch 16, Training Loss: 0.8098428672902724\n",
      "Epoch 17, Training Loss: 0.8089993716688717\n",
      "Epoch 18, Training Loss: 0.8081562479804544\n",
      "Epoch 19, Training Loss: 0.8079087591171265\n",
      "Epoch 20, Training Loss: 0.8071389182174907\n",
      "Epoch 21, Training Loss: 0.8066008566407596\n",
      "Epoch 22, Training Loss: 0.806092096637277\n",
      "Epoch 23, Training Loss: 0.8055483509512509\n",
      "Epoch 24, Training Loss: 0.8050467177699594\n",
      "Epoch 25, Training Loss: 0.8043697005159715\n",
      "Epoch 26, Training Loss: 0.8043015166591195\n",
      "Epoch 27, Training Loss: 0.8037568205945632\n",
      "Epoch 28, Training Loss: 0.8031896868172814\n",
      "Epoch 29, Training Loss: 0.8027419560797074\n",
      "Epoch 30, Training Loss: 0.8028808093070984\n",
      "Epoch 31, Training Loss: 0.8024508953094482\n",
      "Epoch 32, Training Loss: 0.8021915359356824\n",
      "Epoch 33, Training Loss: 0.8018477897083058\n",
      "Epoch 34, Training Loss: 0.8016110269462361\n",
      "Epoch 35, Training Loss: 0.8014407931355869\n",
      "Epoch 36, Training Loss: 0.8010149782545426\n",
      "Epoch 37, Training Loss: 0.800806413748685\n",
      "Epoch 38, Training Loss: 0.8007444259699653\n",
      "Epoch 39, Training Loss: 0.8006201732859892\n",
      "Epoch 40, Training Loss: 0.8003893461648156\n",
      "Epoch 41, Training Loss: 0.800108201714123\n",
      "Epoch 42, Training Loss: 0.8000391207723057\n",
      "Epoch 43, Training Loss: 0.7998732395031873\n",
      "Epoch 44, Training Loss: 0.7998473128851722\n",
      "Epoch 45, Training Loss: 0.7993952380909639\n",
      "Epoch 46, Training Loss: 0.7995401230279137\n",
      "Epoch 47, Training Loss: 0.7995004711431616\n",
      "Epoch 48, Training Loss: 0.7991837132678312\n",
      "Epoch 49, Training Loss: 0.7993486928238588\n",
      "Epoch 50, Training Loss: 0.7991879734572243\n",
      "Epoch 51, Training Loss: 0.7990842111671672\n",
      "Epoch 52, Training Loss: 0.7990552966033712\n",
      "Epoch 53, Training Loss: 0.7989652621746063\n",
      "Epoch 54, Training Loss: 0.7989111440321979\n",
      "Epoch 55, Training Loss: 0.7986204993023592\n",
      "Epoch 56, Training Loss: 0.798801510404138\n",
      "Epoch 57, Training Loss: 0.798689549740623\n",
      "Epoch 58, Training Loss: 0.798592415487065\n",
      "Epoch 59, Training Loss: 0.7986044577991261\n",
      "Epoch 60, Training Loss: 0.798696903551326\n",
      "Epoch 61, Training Loss: 0.7985181887710796\n",
      "Epoch 62, Training Loss: 0.7982481974012712\n",
      "Epoch 63, Training Loss: 0.7983222504223094\n",
      "Epoch 64, Training Loss: 0.7982998572377598\n",
      "Epoch 65, Training Loss: 0.798306814993129\n",
      "Epoch 66, Training Loss: 0.7982601383854361\n",
      "Epoch 67, Training Loss: 0.7984202384948731\n",
      "Epoch 68, Training Loss: 0.7982253949081196\n",
      "Epoch 69, Training Loss: 0.798095924503663\n",
      "Epoch 70, Training Loss: 0.7983024490580839\n",
      "Epoch 71, Training Loss: 0.7982292356911828\n",
      "Epoch 72, Training Loss: 0.7980847749990575\n",
      "Epoch 73, Training Loss: 0.7980673475125256\n",
      "Epoch 74, Training Loss: 0.7980256969788495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:36:09,872] Trial 9 finished with value: 0.6334666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 2 with value: 0.6389333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7979885742243599\n",
      "Epoch 1, Training Loss: 0.8948174834952635\n",
      "Epoch 2, Training Loss: 0.80953339324278\n",
      "Epoch 3, Training Loss: 0.8038190984024721\n",
      "Epoch 4, Training Loss: 0.8002939696171705\n",
      "Epoch 5, Training Loss: 0.7987435967781964\n",
      "Epoch 6, Training Loss: 0.7971267409885631\n",
      "Epoch 7, Training Loss: 0.7950128693440381\n",
      "Epoch 8, Training Loss: 0.7933382728520562\n",
      "Epoch 9, Training Loss: 0.7923899345538196\n",
      "Epoch 10, Training Loss: 0.7911579023389256\n",
      "Epoch 11, Training Loss: 0.7905484558554257\n",
      "Epoch 12, Training Loss: 0.7889412358929129\n",
      "Epoch 13, Training Loss: 0.7887323422992931\n",
      "Epoch 14, Training Loss: 0.7886734395167407\n",
      "Epoch 15, Training Loss: 0.7878544022756464\n",
      "Epoch 16, Training Loss: 0.7874632819259868\n",
      "Epoch 17, Training Loss: 0.7870879089832306\n",
      "Epoch 18, Training Loss: 0.7869174310039071\n",
      "Epoch 19, Training Loss: 0.7866378299628987\n",
      "Epoch 20, Training Loss: 0.7855469332021825\n",
      "Epoch 21, Training Loss: 0.7857517344811383\n",
      "Epoch 22, Training Loss: 0.7851277854162104\n",
      "Epoch 23, Training Loss: 0.784960714228013\n",
      "Epoch 24, Training Loss: 0.7844124312961803\n",
      "Epoch 25, Training Loss: 0.7846847546100616\n",
      "Epoch 26, Training Loss: 0.7838109252032112\n",
      "Epoch 27, Training Loss: 0.7840195096941556\n",
      "Epoch 28, Training Loss: 0.7834588063464445\n",
      "Epoch 29, Training Loss: 0.7834078891137067\n",
      "Epoch 30, Training Loss: 0.7829707333620857\n",
      "Epoch 31, Training Loss: 0.7825104701519012\n",
      "Epoch 32, Training Loss: 0.7826990749555476\n",
      "Epoch 33, Training Loss: 0.7823452432015363\n",
      "Epoch 34, Training Loss: 0.7823490620360655\n",
      "Epoch 35, Training Loss: 0.7817639137015623\n",
      "Epoch 36, Training Loss: 0.7820413795639487\n",
      "Epoch 37, Training Loss: 0.7818414697226356\n",
      "Epoch 38, Training Loss: 0.7818575023903567\n",
      "Epoch 39, Training Loss: 0.781053288824418\n",
      "Epoch 40, Training Loss: 0.7810282788557165\n",
      "Epoch 41, Training Loss: 0.7810972450060003\n",
      "Epoch 42, Training Loss: 0.7807766010480769\n",
      "Epoch 43, Training Loss: 0.7809736938336316\n",
      "Epoch 44, Training Loss: 0.7805885008503409\n",
      "Epoch 45, Training Loss: 0.7804114449024201\n",
      "Epoch 46, Training Loss: 0.7805121956853306\n",
      "Epoch 47, Training Loss: 0.7807249262753655\n",
      "Epoch 48, Training Loss: 0.7801030011738048\n",
      "Epoch 49, Training Loss: 0.780225615010542\n",
      "Epoch 50, Training Loss: 0.7798975090419545\n",
      "Epoch 51, Training Loss: 0.7793955781179316\n",
      "Epoch 52, Training Loss: 0.7797612650955424\n",
      "Epoch 53, Training Loss: 0.7797315070208382\n",
      "Epoch 54, Training Loss: 0.7797204002913307\n",
      "Epoch 55, Training Loss: 0.7797500844562755\n",
      "Epoch 56, Training Loss: 0.7792535478227278\n",
      "Epoch 57, Training Loss: 0.7796177350773531\n",
      "Epoch 58, Training Loss: 0.7792056829087874\n",
      "Epoch 59, Training Loss: 0.778848322910421\n",
      "Epoch 60, Training Loss: 0.7789436967933879\n",
      "Epoch 61, Training Loss: 0.7789179465349982\n",
      "Epoch 62, Training Loss: 0.7795299395392923\n",
      "Epoch 63, Training Loss: 0.7784234442430384\n",
      "Epoch 64, Training Loss: 0.7787352725337533\n",
      "Epoch 65, Training Loss: 0.7782778267299427\n",
      "Epoch 66, Training Loss: 0.7790316883956685\n",
      "Epoch 67, Training Loss: 0.7781678391905392\n",
      "Epoch 68, Training Loss: 0.7778926924396964\n",
      "Epoch 69, Training Loss: 0.7788564682708067\n",
      "Epoch 70, Training Loss: 0.7782126778714797\n",
      "Epoch 71, Training Loss: 0.7778185038706835\n",
      "Epoch 72, Training Loss: 0.7778390336036682\n",
      "Epoch 73, Training Loss: 0.7775801864792319\n",
      "Epoch 74, Training Loss: 0.7780933903245365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:37:48,486] Trial 10 finished with value: 0.6420666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7779432363369886\n",
      "Epoch 1, Training Loss: 1.0879346905035132\n",
      "Epoch 2, Training Loss: 1.0807973881328807\n",
      "Epoch 3, Training Loss: 1.0712079977989197\n",
      "Epoch 4, Training Loss: 1.0569628061967737\n",
      "Epoch 5, Training Loss: 1.0361524985818302\n",
      "Epoch 6, Training Loss: 1.0093252295606276\n",
      "Epoch 7, Training Loss: 0.9841788503001718\n",
      "Epoch 8, Training Loss: 0.9686284572937909\n",
      "Epoch 9, Training Loss: 0.961976515826057\n",
      "Epoch 10, Training Loss: 0.9590366629993214\n",
      "Epoch 11, Training Loss: 0.9573102384455063\n",
      "Epoch 12, Training Loss: 0.9558977758183199\n",
      "Epoch 13, Training Loss: 0.9542482793331146\n",
      "Epoch 14, Training Loss: 0.9528039726089029\n",
      "Epoch 15, Training Loss: 0.9511571213778327\n",
      "Epoch 16, Training Loss: 0.9494842109259437\n",
      "Epoch 17, Training Loss: 0.9477564390266643\n",
      "Epoch 18, Training Loss: 0.945853029980379\n",
      "Epoch 19, Training Loss: 0.9438207774302538\n",
      "Epoch 20, Training Loss: 0.9417354514318353\n",
      "Epoch 21, Training Loss: 0.9394288319699905\n",
      "Epoch 22, Training Loss: 0.936782394507352\n",
      "Epoch 23, Training Loss: 0.9340037523998934\n",
      "Epoch 24, Training Loss: 0.9308534948966083\n",
      "Epoch 25, Training Loss: 0.9275659157248104\n",
      "Epoch 26, Training Loss: 0.9238016319274902\n",
      "Epoch 27, Training Loss: 0.9194832638431998\n",
      "Epoch 28, Training Loss: 0.9149403820318335\n",
      "Epoch 29, Training Loss: 0.9098180445502786\n",
      "Epoch 30, Training Loss: 0.9042650024329915\n",
      "Epoch 31, Training Loss: 0.8982457812393413\n",
      "Epoch 32, Training Loss: 0.8917052673592287\n",
      "Epoch 33, Training Loss: 0.885241172243567\n",
      "Epoch 34, Training Loss: 0.878410093012978\n",
      "Epoch 35, Training Loss: 0.8715736242602853\n",
      "Epoch 36, Training Loss: 0.8650824628156775\n",
      "Epoch 37, Training Loss: 0.8585568773045259\n",
      "Epoch 38, Training Loss: 0.8528548429292792\n",
      "Epoch 39, Training Loss: 0.8475001232063069\n",
      "Epoch 40, Training Loss: 0.8428365954932044\n",
      "Epoch 41, Training Loss: 0.838747548496022\n",
      "Epoch 42, Training Loss: 0.8351407058799968\n",
      "Epoch 43, Training Loss: 0.8320536666757921\n",
      "Epoch 44, Training Loss: 0.8293601296929752\n",
      "Epoch 45, Training Loss: 0.8272876157480128\n",
      "Epoch 46, Training Loss: 0.8253938231748693\n",
      "Epoch 47, Training Loss: 0.8238289849197163\n",
      "Epoch 48, Training Loss: 0.8221833639986375\n",
      "Epoch 49, Training Loss: 0.8208424791868996\n",
      "Epoch 50, Training Loss: 0.8196765304312986\n",
      "Epoch 51, Training Loss: 0.8185382925061618\n",
      "Epoch 52, Training Loss: 0.8179416928571813\n",
      "Epoch 53, Training Loss: 0.8169882415322697\n",
      "Epoch 54, Training Loss: 0.8162252698926364\n",
      "Epoch 55, Training Loss: 0.815536932664759\n",
      "Epoch 56, Training Loss: 0.8149888496539172\n",
      "Epoch 57, Training Loss: 0.8143701688682332\n",
      "Epoch 58, Training Loss: 0.8139013429950265\n",
      "Epoch 59, Training Loss: 0.8132476301052991\n",
      "Epoch 60, Training Loss: 0.8127418989994947\n",
      "Epoch 61, Training Loss: 0.8123803370840409\n",
      "Epoch 62, Training Loss: 0.8119082093238831\n",
      "Epoch 63, Training Loss: 0.8113915983368368\n",
      "Epoch 64, Training Loss: 0.8112619616003598\n",
      "Epoch 65, Training Loss: 0.8108987414135652\n",
      "Epoch 66, Training Loss: 0.8105615778530345\n",
      "Epoch 67, Training Loss: 0.8103582410251393\n",
      "Epoch 68, Training Loss: 0.810054961793563\n",
      "Epoch 69, Training Loss: 0.809696429827634\n",
      "Epoch 70, Training Loss: 0.8095889440704794\n",
      "Epoch 71, Training Loss: 0.8092287585314583\n",
      "Epoch 72, Training Loss: 0.8092063433983747\n",
      "Epoch 73, Training Loss: 0.8088489440609427\n",
      "Epoch 74, Training Loss: 0.8087151493745691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:39:05,306] Trial 11 finished with value: 0.6280666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8084987454554614\n",
      "Epoch 1, Training Loss: 0.8898895963500528\n",
      "Epoch 2, Training Loss: 0.8215631102113162\n",
      "Epoch 3, Training Loss: 0.8129513109431548\n",
      "Epoch 4, Training Loss: 0.8099241669037763\n",
      "Epoch 5, Training Loss: 0.8052485512284672\n",
      "Epoch 6, Training Loss: 0.8023961068602169\n",
      "Epoch 7, Training Loss: 0.7996528812716989\n",
      "Epoch 8, Training Loss: 0.7975396275520324\n",
      "Epoch 9, Training Loss: 0.7956925869689269\n",
      "Epoch 10, Training Loss: 0.7946525935565724\n",
      "Epoch 11, Training Loss: 0.7941075473673204\n",
      "Epoch 12, Training Loss: 0.7935459791912752\n",
      "Epoch 13, Training Loss: 0.7921815030013813\n",
      "Epoch 14, Training Loss: 0.7920238108494703\n",
      "Epoch 15, Training Loss: 0.791031184757457\n",
      "Epoch 16, Training Loss: 0.7910503806086148\n",
      "Epoch 17, Training Loss: 0.7905488194437588\n",
      "Epoch 18, Training Loss: 0.7899749342133017\n",
      "Epoch 19, Training Loss: 0.7895062320372638\n",
      "Epoch 20, Training Loss: 0.7896019553436953\n",
      "Epoch 21, Training Loss: 0.7889618630268994\n",
      "Epoch 22, Training Loss: 0.7886875216399922\n",
      "Epoch 23, Training Loss: 0.7884686960192288\n",
      "Epoch 24, Training Loss: 0.7882705547529109\n",
      "Epoch 25, Training Loss: 0.7883723869744469\n",
      "Epoch 26, Training Loss: 0.7874935083529528\n",
      "Epoch 27, Training Loss: 0.7872410584197325\n",
      "Epoch 28, Training Loss: 0.7876850321713615\n",
      "Epoch 29, Training Loss: 0.7870850607928108\n",
      "Epoch 30, Training Loss: 0.7870157126118155\n",
      "Epoch 31, Training Loss: 0.7872449630849502\n",
      "Epoch 32, Training Loss: 0.7866653523725622\n",
      "Epoch 33, Training Loss: 0.7863949965729433\n",
      "Epoch 34, Training Loss: 0.7862713623748107\n",
      "Epoch 35, Training Loss: 0.7865635121569914\n",
      "Epoch 36, Training Loss: 0.7863074224836686\n",
      "Epoch 37, Training Loss: 0.7859115316587336\n",
      "Epoch 38, Training Loss: 0.7858405937166775\n",
      "Epoch 39, Training Loss: 0.7853113051723032\n",
      "Epoch 40, Training Loss: 0.7857766924886143\n",
      "Epoch 41, Training Loss: 0.785820992974674\n",
      "Epoch 42, Training Loss: 0.7853059001529918\n",
      "Epoch 43, Training Loss: 0.7849180716626785\n",
      "Epoch 44, Training Loss: 0.7850022983551025\n",
      "Epoch 45, Training Loss: 0.7848880230679232\n",
      "Epoch 46, Training Loss: 0.7855483881165\n",
      "Epoch 47, Training Loss: 0.7849951951644\n",
      "Epoch 48, Training Loss: 0.784636853091857\n",
      "Epoch 49, Training Loss: 0.7843626887658063\n",
      "Epoch 50, Training Loss: 0.7844867484008564\n",
      "Epoch 51, Training Loss: 0.7844296849475187\n",
      "Epoch 52, Training Loss: 0.784704740818809\n",
      "Epoch 53, Training Loss: 0.7841329815107233\n",
      "Epoch 54, Training Loss: 0.7843548012481016\n",
      "Epoch 55, Training Loss: 0.7842975217454574\n",
      "Epoch 56, Training Loss: 0.784419683638741\n",
      "Epoch 57, Training Loss: 0.784006989142474\n",
      "Epoch 58, Training Loss: 0.7838797615556156\n",
      "Epoch 59, Training Loss: 0.7842064242503223\n",
      "Epoch 60, Training Loss: 0.7842112426898059\n",
      "Epoch 61, Training Loss: 0.784211366667467\n",
      "Epoch 62, Training Loss: 0.7839462390366723\n",
      "Epoch 63, Training Loss: 0.7835121737508213\n",
      "Epoch 64, Training Loss: 0.7833101499781889\n",
      "Epoch 65, Training Loss: 0.7836414871496312\n",
      "Epoch 66, Training Loss: 0.7836928118677701\n",
      "Epoch 67, Training Loss: 0.7836660216836369\n",
      "Epoch 68, Training Loss: 0.7833726902569041\n",
      "Epoch 69, Training Loss: 0.7834967589378357\n",
      "Epoch 70, Training Loss: 0.7832725550847895\n",
      "Epoch 71, Training Loss: 0.7835312733229469\n",
      "Epoch 72, Training Loss: 0.7834688820558435\n",
      "Epoch 73, Training Loss: 0.7834455760787515\n",
      "Epoch 74, Training Loss: 0.7835717214556301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:40:36,750] Trial 12 finished with value: 0.6417333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7831360267891604\n",
      "Epoch 1, Training Loss: 0.8502603256612792\n",
      "Epoch 2, Training Loss: 0.820236499327466\n",
      "Epoch 3, Training Loss: 0.8175538388410009\n",
      "Epoch 4, Training Loss: 0.8162342253484224\n",
      "Epoch 5, Training Loss: 0.8134270223459803\n",
      "Epoch 6, Training Loss: 0.8109935742571838\n",
      "Epoch 7, Training Loss: 0.8122151821179497\n",
      "Epoch 8, Training Loss: 0.8119949788079226\n",
      "Epoch 9, Training Loss: 0.8132753153492633\n",
      "Epoch 10, Training Loss: 0.8125988582023105\n",
      "Epoch 11, Training Loss: 0.8112346050434542\n",
      "Epoch 12, Training Loss: 0.8102855396450014\n",
      "Epoch 13, Training Loss: 0.8107354212524299\n",
      "Epoch 14, Training Loss: 0.8095418134130034\n",
      "Epoch 15, Training Loss: 0.8088255340891674\n",
      "Epoch 16, Training Loss: 0.8092470727468791\n",
      "Epoch 17, Training Loss: 0.8087538863483228\n",
      "Epoch 18, Training Loss: 0.8091842185285755\n",
      "Epoch 19, Training Loss: 0.8100773004660929\n",
      "Epoch 20, Training Loss: 0.8088970046294363\n",
      "Epoch 21, Training Loss: 0.8100899915946157\n",
      "Epoch 22, Training Loss: 0.8075290750740166\n",
      "Epoch 23, Training Loss: 0.8097637675758591\n",
      "Epoch 24, Training Loss: 0.8091455958839646\n",
      "Epoch 25, Training Loss: 0.8085072492298327\n",
      "Epoch 26, Training Loss: 0.8069775807230096\n",
      "Epoch 27, Training Loss: 0.8092025753250696\n",
      "Epoch 28, Training Loss: 0.8067605261964009\n",
      "Epoch 29, Training Loss: 0.8076294112922554\n",
      "Epoch 30, Training Loss: 0.8064503935046662\n",
      "Epoch 31, Training Loss: 0.8058945042746407\n",
      "Epoch 32, Training Loss: 0.8075864729128386\n",
      "Epoch 33, Training Loss: 0.8056796185952381\n",
      "Epoch 34, Training Loss: 0.8042011297735057\n",
      "Epoch 35, Training Loss: 0.8042802263023262\n",
      "Epoch 36, Training Loss: 0.8043255529905621\n",
      "Epoch 37, Training Loss: 0.8046009293176177\n",
      "Epoch 38, Training Loss: 0.8041662911723431\n",
      "Epoch 39, Training Loss: 0.8033323505767306\n",
      "Epoch 40, Training Loss: 0.8029152012409124\n",
      "Epoch 41, Training Loss: 0.802475158791793\n",
      "Epoch 42, Training Loss: 0.8038890092892754\n",
      "Epoch 43, Training Loss: 0.8013970167117012\n",
      "Epoch 44, Training Loss: 0.8033827671431061\n",
      "Epoch 45, Training Loss: 0.8029381266213897\n",
      "Epoch 46, Training Loss: 0.8016189162892506\n",
      "Epoch 47, Training Loss: 0.8034798528915061\n",
      "Epoch 48, Training Loss: 0.8011382578010846\n",
      "Epoch 49, Training Loss: 0.8014073181869392\n",
      "Epoch 50, Training Loss: 0.8014792254992894\n",
      "Epoch 51, Training Loss: 0.8020261564649137\n",
      "Epoch 52, Training Loss: 0.8004387311469343\n",
      "Epoch 53, Training Loss: 0.7995616144703743\n",
      "Epoch 54, Training Loss: 0.8015210361409008\n",
      "Epoch 55, Training Loss: 0.8012903435785967\n",
      "Epoch 56, Training Loss: 0.7996820099819871\n",
      "Epoch 57, Training Loss: 0.8019234974581496\n",
      "Epoch 58, Training Loss: 0.8002677557163669\n",
      "Epoch 59, Training Loss: 0.7993265341099044\n",
      "Epoch 60, Training Loss: 0.8012230502035385\n",
      "Epoch 61, Training Loss: 0.7992586988255493\n",
      "Epoch 62, Training Loss: 0.800646538214576\n",
      "Epoch 63, Training Loss: 0.8000090938761718\n",
      "Epoch 64, Training Loss: 0.8003773100394055\n",
      "Epoch 65, Training Loss: 0.799752069953689\n",
      "Epoch 66, Training Loss: 0.7977752471328678\n",
      "Epoch 67, Training Loss: 0.8020938168791004\n",
      "Epoch 68, Training Loss: 0.7996538895413392\n",
      "Epoch 69, Training Loss: 0.7992858674293174\n",
      "Epoch 70, Training Loss: 0.800982657948831\n",
      "Epoch 71, Training Loss: 0.7986833547291002\n",
      "Epoch 72, Training Loss: 0.7990283970546005\n",
      "Epoch 73, Training Loss: 0.7994928264976444\n",
      "Epoch 74, Training Loss: 0.7991457646054433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:41:50,899] Trial 13 finished with value: 0.6306 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8001393605891923\n",
      "Epoch 1, Training Loss: 0.8793874687307022\n",
      "Epoch 2, Training Loss: 0.8375136636285221\n",
      "Epoch 3, Training Loss: 0.8315448971355662\n",
      "Epoch 4, Training Loss: 0.8253898293130538\n",
      "Epoch 5, Training Loss: 0.8233883011341095\n",
      "Epoch 6, Training Loss: 0.8210200234020457\n",
      "Epoch 7, Training Loss: 0.8200524095226737\n",
      "Epoch 8, Training Loss: 0.8196736981588252\n",
      "Epoch 9, Training Loss: 0.8183611262545866\n",
      "Epoch 10, Training Loss: 0.8171287699306713\n",
      "Epoch 11, Training Loss: 0.8181858965929817\n",
      "Epoch 12, Training Loss: 0.8177096913141363\n",
      "Epoch 13, Training Loss: 0.816869410907521\n",
      "Epoch 14, Training Loss: 0.8169141263120314\n",
      "Epoch 15, Training Loss: 0.8146364291275249\n",
      "Epoch 16, Training Loss: 0.8150688891551073\n",
      "Epoch 17, Training Loss: 0.814142128088895\n",
      "Epoch 18, Training Loss: 0.81418026173816\n",
      "Epoch 19, Training Loss: 0.814178775619058\n",
      "Epoch 20, Training Loss: 0.8139855802760405\n",
      "Epoch 21, Training Loss: 0.8135359047440922\n",
      "Epoch 22, Training Loss: 0.8138361622305478\n",
      "Epoch 23, Training Loss: 0.8131727160425747\n",
      "Epoch 24, Training Loss: 0.8143746085727916\n",
      "Epoch 25, Training Loss: 0.812626865751603\n",
      "Epoch 26, Training Loss: 0.8129340725786546\n",
      "Epoch 27, Training Loss: 0.8146167778968811\n",
      "Epoch 28, Training Loss: 0.8114523439547595\n",
      "Epoch 29, Training Loss: 0.8116522391403422\n",
      "Epoch 30, Training Loss: 0.8119095691512612\n",
      "Epoch 31, Training Loss: 0.8122169324930977\n",
      "Epoch 32, Training Loss: 0.8117533105962417\n",
      "Epoch 33, Training Loss: 0.8111535533035503\n",
      "Epoch 34, Training Loss: 0.8107037609464982\n",
      "Epoch 35, Training Loss: 0.8100260155341205\n",
      "Epoch 36, Training Loss: 0.810098883194082\n",
      "Epoch 37, Training Loss: 0.8109355681082782\n",
      "Epoch 38, Training Loss: 0.8093044418447158\n",
      "Epoch 39, Training Loss: 0.8091360882450552\n",
      "Epoch 40, Training Loss: 0.8101626553254969\n",
      "Epoch 41, Training Loss: 0.8083540570034701\n",
      "Epoch 42, Training Loss: 0.8097139140437631\n",
      "Epoch 43, Training Loss: 0.8107077069843517\n",
      "Epoch 44, Training Loss: 0.8093528941799613\n",
      "Epoch 45, Training Loss: 0.8095059744750752\n",
      "Epoch 46, Training Loss: 0.8091443002223968\n",
      "Epoch 47, Training Loss: 0.8072095063153435\n",
      "Epoch 48, Training Loss: 0.8081061717341927\n",
      "Epoch 49, Training Loss: 0.8077879693227655\n",
      "Epoch 50, Training Loss: 0.8073988498659694\n",
      "Epoch 51, Training Loss: 0.8077187987636117\n",
      "Epoch 52, Training Loss: 0.8070913600921631\n",
      "Epoch 53, Training Loss: 0.807998603512259\n",
      "Epoch 54, Training Loss: 0.8061820691473344\n",
      "Epoch 55, Training Loss: 0.8054545227920308\n",
      "Epoch 56, Training Loss: 0.8073385249165927\n",
      "Epoch 57, Training Loss: 0.8079896615533267\n",
      "Epoch 58, Training Loss: 0.806720245515599\n",
      "Epoch 59, Training Loss: 0.8058384121866787\n",
      "Epoch 60, Training Loss: 0.8068849025053136\n",
      "Epoch 61, Training Loss: 0.8073035012273228\n",
      "Epoch 62, Training Loss: 0.807034774317461\n",
      "Epoch 63, Training Loss: 0.8060356998443603\n",
      "Epoch 64, Training Loss: 0.8067734117367689\n",
      "Epoch 65, Training Loss: 0.8047715818881989\n",
      "Epoch 66, Training Loss: 0.8066365720945247\n",
      "Epoch 67, Training Loss: 0.8063174414634705\n",
      "Epoch 68, Training Loss: 0.8053741523097543\n",
      "Epoch 69, Training Loss: 0.8054313247344073\n",
      "Epoch 70, Training Loss: 0.8079138936014736\n",
      "Epoch 71, Training Loss: 0.8061817529622246\n",
      "Epoch 72, Training Loss: 0.8061335742473602\n",
      "Epoch 73, Training Loss: 0.8061849174780004\n",
      "Epoch 74, Training Loss: 0.8049146695698008\n",
      "Epoch 75, Training Loss: 0.8051580207488116\n",
      "Epoch 76, Training Loss: 0.8050856811860029\n",
      "Epoch 77, Training Loss: 0.8044602914417491\n",
      "Epoch 78, Training Loss: 0.8048026507742265\n",
      "Epoch 79, Training Loss: 0.8039104458163766\n",
      "Epoch 80, Training Loss: 0.8060181806367986\n",
      "Epoch 81, Training Loss: 0.8051064689019147\n",
      "Epoch 82, Training Loss: 0.8050976737807779\n",
      "Epoch 83, Training Loss: 0.8063125228180604\n",
      "Epoch 84, Training Loss: 0.8035520811641917\n",
      "Epoch 85, Training Loss: 0.8062501002059264\n",
      "Epoch 86, Training Loss: 0.8054220430289998\n",
      "Epoch 87, Training Loss: 0.8037651103384355\n",
      "Epoch 88, Training Loss: 0.8042785585627836\n",
      "Epoch 89, Training Loss: 0.8043582817386178\n",
      "Epoch 90, Training Loss: 0.8045546979763929\n",
      "Epoch 91, Training Loss: 0.8041015836070565\n",
      "Epoch 92, Training Loss: 0.8039204516130335\n",
      "Epoch 93, Training Loss: 0.8045167599706089\n",
      "Epoch 94, Training Loss: 0.803983612761778\n",
      "Epoch 95, Training Loss: 0.8047251781295327\n",
      "Epoch 96, Training Loss: 0.8043543396276586\n",
      "Epoch 97, Training Loss: 0.8056648602906396\n",
      "Epoch 98, Training Loss: 0.8034105275659\n",
      "Epoch 99, Training Loss: 0.8047211559379802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:43:38,822] Trial 14 finished with value: 0.6185333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8061259095107808\n",
      "Epoch 1, Training Loss: 0.8931497142595404\n",
      "Epoch 2, Training Loss: 0.8236289248746984\n",
      "Epoch 3, Training Loss: 0.819174143707051\n",
      "Epoch 4, Training Loss: 0.8157132030935849\n",
      "Epoch 5, Training Loss: 0.8126465398423812\n",
      "Epoch 6, Training Loss: 0.8121177064671236\n",
      "Epoch 7, Training Loss: 0.8105082170402302\n",
      "Epoch 8, Training Loss: 0.8088224477627698\n",
      "Epoch 9, Training Loss: 0.8078080347005059\n",
      "Epoch 10, Training Loss: 0.8074587216096766\n",
      "Epoch 11, Training Loss: 0.8064965232680825\n",
      "Epoch 12, Training Loss: 0.8064415962555829\n",
      "Epoch 13, Training Loss: 0.8055688819464515\n",
      "Epoch 14, Training Loss: 0.8057588174763848\n",
      "Epoch 15, Training Loss: 0.8048255594337688\n",
      "Epoch 16, Training Loss: 0.8048220296467051\n",
      "Epoch 17, Training Loss: 0.8042193352474886\n",
      "Epoch 18, Training Loss: 0.8041173104678884\n",
      "Epoch 19, Training Loss: 0.8036717816661386\n",
      "Epoch 20, Training Loss: 0.8034750093432034\n",
      "Epoch 21, Training Loss: 0.8029812279869528\n",
      "Epoch 22, Training Loss: 0.8029546976790709\n",
      "Epoch 23, Training Loss: 0.8025968553739435\n",
      "Epoch 24, Training Loss: 0.8021193146004396\n",
      "Epoch 25, Training Loss: 0.802446518645567\n",
      "Epoch 26, Training Loss: 0.8021528201944688\n",
      "Epoch 27, Training Loss: 0.8019561604892507\n",
      "Epoch 28, Training Loss: 0.8020134225312401\n",
      "Epoch 29, Training Loss: 0.801555543086108\n",
      "Epoch 30, Training Loss: 0.801293201937395\n",
      "Epoch 31, Training Loss: 0.8013871584219091\n",
      "Epoch 32, Training Loss: 0.8010222535273608\n",
      "Epoch 33, Training Loss: 0.8008161157720229\n",
      "Epoch 34, Training Loss: 0.8009629546894746\n",
      "Epoch 35, Training Loss: 0.8002260119774762\n",
      "Epoch 36, Training Loss: 0.8002259891173419\n",
      "Epoch 37, Training Loss: 0.8000520830294665\n",
      "Epoch 38, Training Loss: 0.7997630570916568\n",
      "Epoch 39, Training Loss: 0.8001349227568683\n",
      "Epoch 40, Training Loss: 0.7997989872623893\n",
      "Epoch 41, Training Loss: 0.7993719396871679\n",
      "Epoch 42, Training Loss: 0.7993612736112932\n",
      "Epoch 43, Training Loss: 0.7989552821832545\n",
      "Epoch 44, Training Loss: 0.7987879333075355\n",
      "Epoch 45, Training Loss: 0.7985376886760487\n",
      "Epoch 46, Training Loss: 0.7982618793319254\n",
      "Epoch 47, Training Loss: 0.7980898465829737\n",
      "Epoch 48, Training Loss: 0.7982876596030067\n",
      "Epoch 49, Training Loss: 0.7977134245283464\n",
      "Epoch 50, Training Loss: 0.7976868447836708\n",
      "Epoch 51, Training Loss: 0.7974914674899157\n",
      "Epoch 52, Training Loss: 0.7972044996654286\n",
      "Epoch 53, Training Loss: 0.7970466457394992\n",
      "Epoch 54, Training Loss: 0.7977964498716242\n",
      "Epoch 55, Training Loss: 0.79719878505258\n",
      "Epoch 56, Training Loss: 0.7966595820819631\n",
      "Epoch 57, Training Loss: 0.796912451211144\n",
      "Epoch 58, Training Loss: 0.7964004069216111\n",
      "Epoch 59, Training Loss: 0.7965854627244613\n",
      "Epoch 60, Training Loss: 0.7961415213697097\n",
      "Epoch 61, Training Loss: 0.7960189484848695\n",
      "Epoch 62, Training Loss: 0.7961621765529409\n",
      "Epoch 63, Training Loss: 0.7957763889256646\n",
      "Epoch 64, Training Loss: 0.7960299141967998\n",
      "Epoch 65, Training Loss: 0.7960264651915606\n",
      "Epoch 66, Training Loss: 0.7956575351602891\n",
      "Epoch 67, Training Loss: 0.7957591941076166\n",
      "Epoch 68, Training Loss: 0.7956325493139379\n",
      "Epoch 69, Training Loss: 0.795457041053211\n",
      "Epoch 70, Training Loss: 0.7955234132093542\n",
      "Epoch 71, Training Loss: 0.795267636495478\n",
      "Epoch 72, Training Loss: 0.7952752793536467\n",
      "Epoch 73, Training Loss: 0.794851016367183\n",
      "Epoch 74, Training Loss: 0.7947537705477546\n",
      "Epoch 75, Training Loss: 0.794866870361216\n",
      "Epoch 76, Training Loss: 0.7946767366633696\n",
      "Epoch 77, Training Loss: 0.7946950270147884\n",
      "Epoch 78, Training Loss: 0.7942270797140458\n",
      "Epoch 79, Training Loss: 0.7941310671497793\n",
      "Epoch 80, Training Loss: 0.7942467353624456\n",
      "Epoch 81, Training Loss: 0.7939011363422169\n",
      "Epoch 82, Training Loss: 0.7938320042806514\n",
      "Epoch 83, Training Loss: 0.7930977877448587\n",
      "Epoch 84, Training Loss: 0.7936064774148605\n",
      "Epoch 85, Training Loss: 0.7936255264983457\n",
      "Epoch 86, Training Loss: 0.7934613056042615\n",
      "Epoch 87, Training Loss: 0.7931961748179267\n",
      "Epoch 88, Training Loss: 0.7933212729762582\n",
      "Epoch 89, Training Loss: 0.7932822428731358\n",
      "Epoch 90, Training Loss: 0.7931061091142543\n",
      "Epoch 91, Training Loss: 0.7930992843824275\n",
      "Epoch 92, Training Loss: 0.7931419366247514\n",
      "Epoch 93, Training Loss: 0.7929876777003794\n",
      "Epoch 94, Training Loss: 0.7928525107047137\n",
      "Epoch 95, Training Loss: 0.7923699693820055\n",
      "Epoch 96, Training Loss: 0.7926972814868478\n",
      "Epoch 97, Training Loss: 0.7924036941107582\n",
      "Epoch 98, Training Loss: 0.7924561931806452\n",
      "Epoch 99, Training Loss: 0.7927437463928672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:45:25,368] Trial 15 finished with value: 0.6329333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7923240918271682\n",
      "Epoch 1, Training Loss: 1.0484144734260732\n",
      "Epoch 2, Training Loss: 0.9588680593590987\n",
      "Epoch 3, Training Loss: 0.9294610181249174\n",
      "Epoch 4, Training Loss: 0.9194811341457797\n",
      "Epoch 5, Training Loss: 0.9102774669353227\n",
      "Epoch 6, Training Loss: 0.9009264789129559\n",
      "Epoch 7, Training Loss: 0.8919500763254954\n",
      "Epoch 8, Training Loss: 0.8798358517481868\n",
      "Epoch 9, Training Loss: 0.8663798607381663\n",
      "Epoch 10, Training Loss: 0.8511187412685022\n",
      "Epoch 11, Training Loss: 0.8369373123448595\n",
      "Epoch 12, Training Loss: 0.824541903976211\n",
      "Epoch 13, Training Loss: 0.8171049132382959\n",
      "Epoch 14, Training Loss: 0.8120533593615195\n",
      "Epoch 15, Training Loss: 0.8085326176836974\n",
      "Epoch 16, Training Loss: 0.8073712254825391\n",
      "Epoch 17, Training Loss: 0.8056456996085949\n",
      "Epoch 18, Training Loss: 0.8041786963778331\n",
      "Epoch 19, Training Loss: 0.804304083487145\n",
      "Epoch 20, Training Loss: 0.8032882823979944\n",
      "Epoch 21, Training Loss: 0.8034164710152418\n",
      "Epoch 22, Training Loss: 0.8019108873560913\n",
      "Epoch 23, Training Loss: 0.8018851671900068\n",
      "Epoch 24, Training Loss: 0.8014443685237627\n",
      "Epoch 25, Training Loss: 0.8007107091129274\n",
      "Epoch 26, Training Loss: 0.8003425729005856\n",
      "Epoch 27, Training Loss: 0.8002625125691407\n",
      "Epoch 28, Training Loss: 0.7999261248380618\n",
      "Epoch 29, Training Loss: 0.7992757771248208\n",
      "Epoch 30, Training Loss: 0.7999159855950148\n",
      "Epoch 31, Training Loss: 0.7991753189187301\n",
      "Epoch 32, Training Loss: 0.798566228285768\n",
      "Epoch 33, Training Loss: 0.7983847454078216\n",
      "Epoch 34, Training Loss: 0.7983923819728364\n",
      "Epoch 35, Training Loss: 0.7979575434125455\n",
      "Epoch 36, Training Loss: 0.7978222493838547\n",
      "Epoch 37, Training Loss: 0.7980699333929477\n",
      "Epoch 38, Training Loss: 0.7970456049854594\n",
      "Epoch 39, Training Loss: 0.7969156241954718\n",
      "Epoch 40, Training Loss: 0.7962916769927606\n",
      "Epoch 41, Training Loss: 0.7964847368405278\n",
      "Epoch 42, Training Loss: 0.7961192728881549\n",
      "Epoch 43, Training Loss: 0.7956340538827996\n",
      "Epoch 44, Training Loss: 0.7957239965747174\n",
      "Epoch 45, Training Loss: 0.7956407335467804\n",
      "Epoch 46, Training Loss: 0.7961252487691721\n",
      "Epoch 47, Training Loss: 0.7952619512278335\n",
      "Epoch 48, Training Loss: 0.7946690898192557\n",
      "Epoch 49, Training Loss: 0.7950289551476787\n",
      "Epoch 50, Training Loss: 0.7945597400342611\n",
      "Epoch 51, Training Loss: 0.7940551342820763\n",
      "Epoch 52, Training Loss: 0.7938993471905701\n",
      "Epoch 53, Training Loss: 0.7940672549089991\n",
      "Epoch 54, Training Loss: 0.793842225146473\n",
      "Epoch 55, Training Loss: 0.7931570792556706\n",
      "Epoch 56, Training Loss: 0.7933146591473342\n",
      "Epoch 57, Training Loss: 0.7929390104193437\n",
      "Epoch 58, Training Loss: 0.7928809489522661\n",
      "Epoch 59, Training Loss: 0.7933432405156301\n",
      "Epoch 60, Training Loss: 0.792063251653112\n",
      "Epoch 61, Training Loss: 0.7926625121805004\n",
      "Epoch 62, Training Loss: 0.7924967199339903\n",
      "Epoch 63, Training Loss: 0.7918782226125101\n",
      "Epoch 64, Training Loss: 0.7926251100418263\n",
      "Epoch 65, Training Loss: 0.7926860276917765\n",
      "Epoch 66, Training Loss: 0.7917990988358519\n",
      "Epoch 67, Training Loss: 0.7917504334808292\n",
      "Epoch 68, Training Loss: 0.7911719990852184\n",
      "Epoch 69, Training Loss: 0.7912206895369336\n",
      "Epoch 70, Training Loss: 0.7912173447752358\n",
      "Epoch 71, Training Loss: 0.7916295644035913\n",
      "Epoch 72, Training Loss: 0.7916264520551926\n",
      "Epoch 73, Training Loss: 0.7908826853099622\n",
      "Epoch 74, Training Loss: 0.7907011700752086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:46:35,475] Trial 16 finished with value: 0.6330666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7902634879700223\n",
      "Epoch 1, Training Loss: 0.8602581762566286\n",
      "Epoch 2, Training Loss: 0.8267421387924867\n",
      "Epoch 3, Training Loss: 0.8218397834020502\n",
      "Epoch 4, Training Loss: 0.817988153555814\n",
      "Epoch 5, Training Loss: 0.8152151555173537\n",
      "Epoch 6, Training Loss: 0.8131108809919918\n",
      "Epoch 7, Training Loss: 0.8130280537465039\n",
      "Epoch 8, Training Loss: 0.8109629806350259\n",
      "Epoch 9, Training Loss: 0.8103793792864855\n",
      "Epoch 10, Training Loss: 0.8099153119676253\n",
      "Epoch 11, Training Loss: 0.809815500904532\n",
      "Epoch 12, Training Loss: 0.8083302236304564\n",
      "Epoch 13, Training Loss: 0.8086704016433043\n",
      "Epoch 14, Training Loss: 0.8073343986623427\n",
      "Epoch 15, Training Loss: 0.8082273164917441\n",
      "Epoch 16, Training Loss: 0.8074732045566334\n",
      "Epoch 17, Training Loss: 0.8073354775765363\n",
      "Epoch 18, Training Loss: 0.8072488809333128\n",
      "Epoch 19, Training Loss: 0.8071371595999773\n",
      "Epoch 20, Training Loss: 0.8068163539381589\n",
      "Epoch 21, Training Loss: 0.8062800662657794\n",
      "Epoch 22, Training Loss: 0.8061235513406642\n",
      "Epoch 23, Training Loss: 0.8064528531186721\n",
      "Epoch 24, Training Loss: 0.8059746029797722\n",
      "Epoch 25, Training Loss: 0.8057579752276925\n",
      "Epoch 26, Training Loss: 0.8055296226108776\n",
      "Epoch 27, Training Loss: 0.8046676471654106\n",
      "Epoch 28, Training Loss: 0.8048474881929509\n",
      "Epoch 29, Training Loss: 0.8045439985920401\n",
      "Epoch 30, Training Loss: 0.8049485635757446\n",
      "Epoch 31, Training Loss: 0.804638195178088\n",
      "Epoch 32, Training Loss: 0.8042822572062998\n",
      "Epoch 33, Training Loss: 0.8035697481912725\n",
      "Epoch 34, Training Loss: 0.8032528664084042\n",
      "Epoch 35, Training Loss: 0.8036511621755712\n",
      "Epoch 36, Training Loss: 0.8035828802164863\n",
      "Epoch 37, Training Loss: 0.8034264735614552\n",
      "Epoch 38, Training Loss: 0.8030318697761086\n",
      "Epoch 39, Training Loss: 0.8028315915079678\n",
      "Epoch 40, Training Loss: 0.8024777751109179\n",
      "Epoch 41, Training Loss: 0.802607707977295\n",
      "Epoch 42, Training Loss: 0.8022470308051389\n",
      "Epoch 43, Training Loss: 0.8027136407880222\n",
      "Epoch 44, Training Loss: 0.8019185353026671\n",
      "Epoch 45, Training Loss: 0.8014430255048415\n",
      "Epoch 46, Training Loss: 0.8014043946125928\n",
      "Epoch 47, Training Loss: 0.8016965074398938\n",
      "Epoch 48, Training Loss: 0.8012173816737007\n",
      "Epoch 49, Training Loss: 0.8009287241627189\n",
      "Epoch 50, Training Loss: 0.8014112885559307\n",
      "Epoch 51, Training Loss: 0.8004250761340647\n",
      "Epoch 52, Training Loss: 0.8009430522778455\n",
      "Epoch 53, Training Loss: 0.8005234520575579\n",
      "Epoch 54, Training Loss: 0.8006370763918933\n",
      "Epoch 55, Training Loss: 0.7994321168871487\n",
      "Epoch 56, Training Loss: 0.7994873933231129\n",
      "Epoch 57, Training Loss: 0.7998941957249361\n",
      "Epoch 58, Training Loss: 0.7995479544471292\n",
      "Epoch 59, Training Loss: 0.7994645859213436\n",
      "Epoch 60, Training Loss: 0.7990249722845414\n",
      "Epoch 61, Training Loss: 0.7996287065393785\n",
      "Epoch 62, Training Loss: 0.7992472145837896\n",
      "Epoch 63, Training Loss: 0.799316288653542\n",
      "Epoch 64, Training Loss: 0.7990369333239162\n",
      "Epoch 65, Training Loss: 0.7984229592014761\n",
      "Epoch 66, Training Loss: 0.7988907401701983\n",
      "Epoch 67, Training Loss: 0.7987653824862312\n",
      "Epoch 68, Training Loss: 0.7982603329770706\n",
      "Epoch 69, Training Loss: 0.7987739268471213\n",
      "Epoch 70, Training Loss: 0.7983811463327969\n",
      "Epoch 71, Training Loss: 0.7979891084222233\n",
      "Epoch 72, Training Loss: 0.7977799163145177\n",
      "Epoch 73, Training Loss: 0.798142887914882\n",
      "Epoch 74, Training Loss: 0.7979387225123012\n",
      "Epoch 75, Training Loss: 0.7977143974865184\n",
      "Epoch 76, Training Loss: 0.7972338271141052\n",
      "Epoch 77, Training Loss: 0.7978995522330788\n",
      "Epoch 78, Training Loss: 0.7971589988820693\n",
      "Epoch 79, Training Loss: 0.7972975749829236\n",
      "Epoch 80, Training Loss: 0.7975813226138844\n",
      "Epoch 81, Training Loss: 0.7968081473602968\n",
      "Epoch 82, Training Loss: 0.7969241522340214\n",
      "Epoch 83, Training Loss: 0.7966151220658246\n",
      "Epoch 84, Training Loss: 0.7967016991447\n",
      "Epoch 85, Training Loss: 0.796609255285824\n",
      "Epoch 86, Training Loss: 0.7969564901379977\n",
      "Epoch 87, Training Loss: 0.7967954212777755\n",
      "Epoch 88, Training Loss: 0.7966353221500622\n",
      "Epoch 89, Training Loss: 0.7963267165773055\n",
      "Epoch 90, Training Loss: 0.7965871974299936\n",
      "Epoch 91, Training Loss: 0.7956282820421107\n",
      "Epoch 92, Training Loss: 0.7964477181434632\n",
      "Epoch 93, Training Loss: 0.7960678885263556\n",
      "Epoch 94, Training Loss: 0.796323586211485\n",
      "Epoch 95, Training Loss: 0.7966434007532457\n",
      "Epoch 96, Training Loss: 0.795992903288673\n",
      "Epoch 97, Training Loss: 0.796375253831639\n",
      "Epoch 98, Training Loss: 0.7955191474802353\n",
      "Epoch 99, Training Loss: 0.7957754076228423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:48:23,135] Trial 17 finished with value: 0.6356666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7958075453954584\n",
      "Epoch 1, Training Loss: 1.0038014923123753\n",
      "Epoch 2, Training Loss: 0.9562672384346232\n",
      "Epoch 3, Training Loss: 0.9458635626820957\n",
      "Epoch 4, Training Loss: 0.9345075583457947\n",
      "Epoch 5, Training Loss: 0.920292606494006\n",
      "Epoch 6, Training Loss: 0.9023169592548819\n",
      "Epoch 7, Training Loss: 0.8813522146729862\n",
      "Epoch 8, Training Loss: 0.8607468579095953\n",
      "Epoch 9, Training Loss: 0.8438763645116021\n",
      "Epoch 10, Training Loss: 0.8320532355589025\n",
      "Epoch 11, Training Loss: 0.8241471679771648\n",
      "Epoch 12, Training Loss: 0.8189358503678266\n",
      "Epoch 13, Training Loss: 0.8155941163091098\n",
      "Epoch 14, Training Loss: 0.8133644240042742\n",
      "Epoch 15, Training Loss: 0.8116014315801509\n",
      "Epoch 16, Training Loss: 0.8105262063531314\n",
      "Epoch 17, Training Loss: 0.8094005713042091\n",
      "Epoch 18, Training Loss: 0.808665868324392\n",
      "Epoch 19, Training Loss: 0.8080614664975334\n",
      "Epoch 20, Training Loss: 0.807412354665644\n",
      "Epoch 21, Training Loss: 0.8068957358949325\n",
      "Epoch 22, Training Loss: 0.8063208028148202\n",
      "Epoch 23, Training Loss: 0.8060106917689829\n",
      "Epoch 24, Training Loss: 0.8052381136838127\n",
      "Epoch 25, Training Loss: 0.8049857588375315\n",
      "Epoch 26, Training Loss: 0.8046916838954477\n",
      "Epoch 27, Training Loss: 0.8040141131597407\n",
      "Epoch 28, Training Loss: 0.8038041389689726\n",
      "Epoch 29, Training Loss: 0.8034129940762239\n",
      "Epoch 30, Training Loss: 0.8028454404718736\n",
      "Epoch 31, Training Loss: 0.8028311654399423\n",
      "Epoch 32, Training Loss: 0.8023263630446266\n",
      "Epoch 33, Training Loss: 0.8021299007359673\n",
      "Epoch 34, Training Loss: 0.8015539073944091\n",
      "Epoch 35, Training Loss: 0.8016093467263614\n",
      "Epoch 36, Training Loss: 0.8013088555896983\n",
      "Epoch 37, Training Loss: 0.8010582869193134\n",
      "Epoch 38, Training Loss: 0.8007370062435375\n",
      "Epoch 39, Training Loss: 0.8004680548695957\n",
      "Epoch 40, Training Loss: 0.800330630681094\n",
      "Epoch 41, Training Loss: 0.8000193653387182\n",
      "Epoch 42, Training Loss: 0.7999987682174233\n",
      "Epoch 43, Training Loss: 0.7998803395383498\n",
      "Epoch 44, Training Loss: 0.7998738165462719\n",
      "Epoch 45, Training Loss: 0.7993630710068871\n",
      "Epoch 46, Training Loss: 0.7993806545874652\n",
      "Epoch 47, Training Loss: 0.7991017535153557\n",
      "Epoch 48, Training Loss: 0.799167902469635\n",
      "Epoch 49, Training Loss: 0.7988208686604219\n",
      "Epoch 50, Training Loss: 0.7989322760525872\n",
      "Epoch 51, Training Loss: 0.798732307728599\n",
      "Epoch 52, Training Loss: 0.7987366102022283\n",
      "Epoch 53, Training Loss: 0.7986101034809562\n",
      "Epoch 54, Training Loss: 0.7984309198575862\n",
      "Epoch 55, Training Loss: 0.798405799444984\n",
      "Epoch 56, Training Loss: 0.7983977269425112\n",
      "Epoch 57, Training Loss: 0.7981784428568447\n",
      "Epoch 58, Training Loss: 0.7982243565250845\n",
      "Epoch 59, Training Loss: 0.7981122081420001\n",
      "Epoch 60, Training Loss: 0.7980975135634927\n",
      "Epoch 61, Training Loss: 0.7980320385624381\n",
      "Epoch 62, Training Loss: 0.7979203738184536\n",
      "Epoch 63, Training Loss: 0.7977895186929141\n",
      "Epoch 64, Training Loss: 0.797726847003488\n",
      "Epoch 65, Training Loss: 0.7977905446641586\n",
      "Epoch 66, Training Loss: 0.7976050790618447\n",
      "Epoch 67, Training Loss: 0.7976949447042802\n",
      "Epoch 68, Training Loss: 0.7977479324621313\n",
      "Epoch 69, Training Loss: 0.797471555261051\n",
      "Epoch 70, Training Loss: 0.7974658440141117\n",
      "Epoch 71, Training Loss: 0.7974971203243031\n",
      "Epoch 72, Training Loss: 0.7972387844674728\n",
      "Epoch 73, Training Loss: 0.7972937136537889\n",
      "Epoch 74, Training Loss: 0.7971432732133304\n",
      "Epoch 75, Training Loss: 0.7971748786814072\n",
      "Epoch 76, Training Loss: 0.7969540396157433\n",
      "Epoch 77, Training Loss: 0.7970076830948101\n",
      "Epoch 78, Training Loss: 0.7971808808691362\n",
      "Epoch 79, Training Loss: 0.7970472592466018\n",
      "Epoch 80, Training Loss: 0.7969680460060344\n",
      "Epoch 81, Training Loss: 0.7970429235346177\n",
      "Epoch 82, Training Loss: 0.7970477554377388\n",
      "Epoch 83, Training Loss: 0.7968551330706652\n",
      "Epoch 84, Training Loss: 0.7967099839098313\n",
      "Epoch 85, Training Loss: 0.7969143929201014\n",
      "Epoch 86, Training Loss: 0.7968862903118134\n",
      "Epoch 87, Training Loss: 0.7967220792349647\n",
      "Epoch 88, Training Loss: 0.7966153795578901\n",
      "Epoch 89, Training Loss: 0.7965697916816262\n",
      "Epoch 90, Training Loss: 0.7964044563910541\n",
      "Epoch 91, Training Loss: 0.7966800414113437\n",
      "Epoch 92, Training Loss: 0.7964711405249203\n",
      "Epoch 93, Training Loss: 0.7963969908742343\n",
      "Epoch 94, Training Loss: 0.7966011869206148\n",
      "Epoch 95, Training Loss: 0.7962616936599507\n",
      "Epoch 96, Training Loss: 0.7962631715045255\n",
      "Epoch 97, Training Loss: 0.7964613453780903\n",
      "Epoch 98, Training Loss: 0.7963655173778534\n",
      "Epoch 99, Training Loss: 0.7962721497872296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:50:07,006] Trial 18 finished with value: 0.6324 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7961347886394052\n",
      "Epoch 1, Training Loss: 1.0887712103979927\n",
      "Epoch 2, Training Loss: 1.0823308817426065\n",
      "Epoch 3, Training Loss: 1.0748743612963454\n",
      "Epoch 4, Training Loss: 1.064385566137787\n",
      "Epoch 5, Training Loss: 1.0492741265691312\n",
      "Epoch 6, Training Loss: 1.028328285719219\n",
      "Epoch 7, Training Loss: 1.0052253013266657\n",
      "Epoch 8, Training Loss: 0.9854725463049753\n",
      "Epoch 9, Training Loss: 0.9729037767962405\n",
      "Epoch 10, Training Loss: 0.9654773255039875\n",
      "Epoch 11, Training Loss: 0.9612460191088511\n",
      "Epoch 12, Training Loss: 0.9585234800675758\n",
      "Epoch 13, Training Loss: 0.9561739995067281\n",
      "Epoch 14, Training Loss: 0.9551427469217688\n",
      "Epoch 15, Training Loss: 0.9532602746683853\n",
      "Epoch 16, Training Loss: 0.9518059142550132\n",
      "Epoch 17, Training Loss: 0.9500872999205625\n",
      "Epoch 18, Training Loss: 0.9485602665664559\n",
      "Epoch 19, Training Loss: 0.9470824501568214\n",
      "Epoch 20, Training Loss: 0.9451855845917436\n",
      "Epoch 21, Training Loss: 0.943051192634984\n",
      "Epoch 22, Training Loss: 0.9415314501389525\n",
      "Epoch 23, Training Loss: 0.9392478393432789\n",
      "Epoch 24, Training Loss: 0.9368189214763785\n",
      "Epoch 25, Training Loss: 0.9345359257289342\n",
      "Epoch 26, Training Loss: 0.9317883813291564\n",
      "Epoch 27, Training Loss: 0.9292262398210683\n",
      "Epoch 28, Training Loss: 0.9258746057524717\n",
      "Epoch 29, Training Loss: 0.9224225877819204\n",
      "Epoch 30, Training Loss: 0.9188312345877626\n",
      "Epoch 31, Training Loss: 0.914921932471426\n",
      "Epoch 32, Training Loss: 0.9105066518138226\n",
      "Epoch 33, Training Loss: 0.9058668000357492\n",
      "Epoch 34, Training Loss: 0.9012568693411978\n",
      "Epoch 35, Training Loss: 0.8958582924720936\n",
      "Epoch 36, Training Loss: 0.8912237073245801\n",
      "Epoch 37, Training Loss: 0.8846574680249494\n",
      "Epoch 38, Training Loss: 0.8797232964881381\n",
      "Epoch 39, Training Loss: 0.8737523467020881\n",
      "Epoch 40, Training Loss: 0.8682535826711726\n",
      "Epoch 41, Training Loss: 0.8622783667162846\n",
      "Epoch 42, Training Loss: 0.8572483969810314\n",
      "Epoch 43, Training Loss: 0.8524101552210356\n",
      "Epoch 44, Training Loss: 0.8480582989248118\n",
      "Epoch 45, Training Loss: 0.8436533063874209\n",
      "Epoch 46, Training Loss: 0.8395038535720424\n",
      "Epoch 47, Training Loss: 0.8363329450886948\n",
      "Epoch 48, Training Loss: 0.8333931870926592\n",
      "Epoch 49, Training Loss: 0.8310600297791617\n",
      "Epoch 50, Training Loss: 0.8282149514757601\n",
      "Epoch 51, Training Loss: 0.8264116859973821\n",
      "Epoch 52, Training Loss: 0.8243288871937229\n",
      "Epoch 53, Training Loss: 0.8226950712670061\n",
      "Epoch 54, Training Loss: 0.8210615264741998\n",
      "Epoch 55, Training Loss: 0.8204076275789648\n",
      "Epoch 56, Training Loss: 0.8190160742379669\n",
      "Epoch 57, Training Loss: 0.8185825752136403\n",
      "Epoch 58, Training Loss: 0.8172047086228106\n",
      "Epoch 59, Training Loss: 0.8162503168995219\n",
      "Epoch 60, Training Loss: 0.815918188973477\n",
      "Epoch 61, Training Loss: 0.8162921505763119\n",
      "Epoch 62, Training Loss: 0.81472955090659\n",
      "Epoch 63, Training Loss: 0.8140391635715514\n",
      "Epoch 64, Training Loss: 0.8135490852191036\n",
      "Epoch 65, Training Loss: 0.813293108814641\n",
      "Epoch 66, Training Loss: 0.8130082982823365\n",
      "Epoch 67, Training Loss: 0.8123244003245705\n",
      "Epoch 68, Training Loss: 0.8124817849998187\n",
      "Epoch 69, Training Loss: 0.8119191843764226\n",
      "Epoch 70, Training Loss: 0.8118055160780598\n",
      "Epoch 71, Training Loss: 0.8118649024712412\n",
      "Epoch 72, Training Loss: 0.8108502984943247\n",
      "Epoch 73, Training Loss: 0.8105877704190132\n",
      "Epoch 74, Training Loss: 0.810534966171236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:51:18,338] Trial 19 finished with value: 0.6235333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8103732411126445\n",
      "Epoch 1, Training Loss: 1.091699939025076\n",
      "Epoch 2, Training Loss: 1.0881025154787796\n",
      "Epoch 3, Training Loss: 1.0860922203924424\n",
      "Epoch 4, Training Loss: 1.0840496391282046\n",
      "Epoch 5, Training Loss: 1.0816065071220684\n",
      "Epoch 6, Training Loss: 1.0788122377897564\n",
      "Epoch 7, Training Loss: 1.0753968717460345\n",
      "Epoch 8, Training Loss: 1.0712570431537198\n",
      "Epoch 9, Training Loss: 1.066609269515016\n",
      "Epoch 10, Training Loss: 1.060777609330371\n",
      "Epoch 11, Training Loss: 1.0538191316719343\n",
      "Epoch 12, Training Loss: 1.046014066029312\n",
      "Epoch 13, Training Loss: 1.037176265393881\n",
      "Epoch 14, Training Loss: 1.0276274506310772\n",
      "Epoch 15, Training Loss: 1.0181665339864285\n",
      "Epoch 16, Training Loss: 1.009229426366046\n",
      "Epoch 17, Training Loss: 1.0010002332522456\n",
      "Epoch 18, Training Loss: 0.993873835954451\n",
      "Epoch 19, Training Loss: 0.9881035275925371\n",
      "Epoch 20, Training Loss: 0.9830243556123031\n",
      "Epoch 21, Training Loss: 0.9785342487177454\n",
      "Epoch 22, Training Loss: 0.9746271354811532\n",
      "Epoch 23, Training Loss: 0.970963780413893\n",
      "Epoch 24, Training Loss: 0.9680187702178955\n",
      "Epoch 25, Training Loss: 0.9648827386977977\n",
      "Epoch 26, Training Loss: 0.9628254934361107\n",
      "Epoch 27, Training Loss: 0.9606793954856414\n",
      "Epoch 28, Training Loss: 0.9586866132298807\n",
      "Epoch 29, Training Loss: 0.9578437365983662\n",
      "Epoch 30, Training Loss: 0.956292990813578\n",
      "Epoch 31, Training Loss: 0.9552567153048694\n",
      "Epoch 32, Training Loss: 0.9548154490334647\n",
      "Epoch 33, Training Loss: 0.9536881718420445\n",
      "Epoch 34, Training Loss: 0.9524238252101984\n",
      "Epoch 35, Training Loss: 0.9517564454473051\n",
      "Epoch 36, Training Loss: 0.9509397739754584\n",
      "Epoch 37, Training Loss: 0.9504182962546671\n",
      "Epoch 38, Training Loss: 0.9496897654425829\n",
      "Epoch 39, Training Loss: 0.9490602632214252\n",
      "Epoch 40, Training Loss: 0.9482544401534518\n",
      "Epoch 41, Training Loss: 0.9476174940740255\n",
      "Epoch 42, Training Loss: 0.9466347980320006\n",
      "Epoch 43, Training Loss: 0.9460696449853424\n",
      "Epoch 44, Training Loss: 0.945260909356569\n",
      "Epoch 45, Training Loss: 0.9441349353109088\n",
      "Epoch 46, Training Loss: 0.9433310959572182\n",
      "Epoch 47, Training Loss: 0.9423337476594108\n",
      "Epoch 48, Training Loss: 0.942203043905416\n",
      "Epoch 49, Training Loss: 0.9408535143486539\n",
      "Epoch 50, Training Loss: 0.9404313102700657\n",
      "Epoch 51, Training Loss: 0.9388188850610776\n",
      "Epoch 52, Training Loss: 0.9377248230733369\n",
      "Epoch 53, Training Loss: 0.9368728771245569\n",
      "Epoch 54, Training Loss: 0.9359339844015307\n",
      "Epoch 55, Training Loss: 0.9343217686602944\n",
      "Epoch 56, Training Loss: 0.9338072985634768\n",
      "Epoch 57, Training Loss: 0.9324672543016591\n",
      "Epoch 58, Training Loss: 0.9313206692387287\n",
      "Epoch 59, Training Loss: 0.9301583276655441\n",
      "Epoch 60, Training Loss: 0.928721611033705\n",
      "Epoch 61, Training Loss: 0.9273285341442079\n",
      "Epoch 62, Training Loss: 0.9254269910037966\n",
      "Epoch 63, Training Loss: 0.9242786776750608\n",
      "Epoch 64, Training Loss: 0.9228449320434627\n",
      "Epoch 65, Training Loss: 0.9212504000592052\n",
      "Epoch 66, Training Loss: 0.9193170790385483\n",
      "Epoch 67, Training Loss: 0.9177049663730134\n",
      "Epoch 68, Training Loss: 0.9158382759954696\n",
      "Epoch 69, Training Loss: 0.9144182500086333\n",
      "Epoch 70, Training Loss: 0.9120598814541236\n",
      "Epoch 71, Training Loss: 0.9098083367025046\n",
      "Epoch 72, Training Loss: 0.9078212326630614\n",
      "Epoch 73, Training Loss: 0.9058472976648718\n",
      "Epoch 74, Training Loss: 0.9036710437975431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:52:27,380] Trial 20 finished with value: 0.5716666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9014982310452856\n",
      "Epoch 1, Training Loss: 0.979025482079562\n",
      "Epoch 2, Training Loss: 0.9371519338383394\n",
      "Epoch 3, Training Loss: 0.9045703545037438\n",
      "Epoch 4, Training Loss: 0.8645324439160964\n",
      "Epoch 5, Training Loss: 0.8321297396631802\n",
      "Epoch 6, Training Loss: 0.8187236622501822\n",
      "Epoch 7, Training Loss: 0.8143696203652551\n",
      "Epoch 8, Training Loss: 0.8122875567043529\n",
      "Epoch 9, Training Loss: 0.8115044153437895\n",
      "Epoch 10, Training Loss: 0.810841820380267\n",
      "Epoch 11, Training Loss: 0.8103418406318216\n",
      "Epoch 12, Training Loss: 0.8094505791103138\n",
      "Epoch 13, Training Loss: 0.8086069806884317\n",
      "Epoch 14, Training Loss: 0.8078218324745403\n",
      "Epoch 15, Training Loss: 0.8074227210353403\n",
      "Epoch 16, Training Loss: 0.8066734360947329\n",
      "Epoch 17, Training Loss: 0.8061307619599735\n",
      "Epoch 18, Training Loss: 0.8052884141837849\n",
      "Epoch 19, Training Loss: 0.8053277305995716\n",
      "Epoch 20, Training Loss: 0.8043184647840612\n",
      "Epoch 21, Training Loss: 0.8043677522855647\n",
      "Epoch 22, Training Loss: 0.8036647430588217\n",
      "Epoch 23, Training Loss: 0.8036629321995904\n",
      "Epoch 24, Training Loss: 0.8031814028235043\n",
      "Epoch 25, Training Loss: 0.8024168266969569\n",
      "Epoch 26, Training Loss: 0.8027798146360061\n",
      "Epoch 27, Training Loss: 0.8022960414605982\n",
      "Epoch 28, Training Loss: 0.8018522397209616\n",
      "Epoch 29, Training Loss: 0.8021815619047951\n",
      "Epoch 30, Training Loss: 0.8020562390018912\n",
      "Epoch 31, Training Loss: 0.8018306698518641\n",
      "Epoch 32, Training Loss: 0.8012301018658806\n",
      "Epoch 33, Training Loss: 0.8015395287906423\n",
      "Epoch 34, Training Loss: 0.8015362273945528\n",
      "Epoch 35, Training Loss: 0.8011882632620194\n",
      "Epoch 36, Training Loss: 0.8009854567050934\n",
      "Epoch 37, Training Loss: 0.8007817639322842\n",
      "Epoch 38, Training Loss: 0.8004440874211929\n",
      "Epoch 39, Training Loss: 0.8005231277381673\n",
      "Epoch 40, Training Loss: 0.8003546922347125\n",
      "Epoch 41, Training Loss: 0.8001507562048294\n",
      "Epoch 42, Training Loss: 0.8002534410532783\n",
      "Epoch 43, Training Loss: 0.8000918808404137\n",
      "Epoch 44, Training Loss: 0.7997519874572754\n",
      "Epoch 45, Training Loss: 0.7999592808414908\n",
      "Epoch 46, Training Loss: 0.7996687455738292\n",
      "Epoch 47, Training Loss: 0.7994401515231413\n",
      "Epoch 48, Training Loss: 0.7996185241026037\n",
      "Epoch 49, Training Loss: 0.7995112817427691\n",
      "Epoch 50, Training Loss: 0.7992296042161829\n",
      "Epoch 51, Training Loss: 0.7992195509461796\n",
      "Epoch 52, Training Loss: 0.7990443060678595\n",
      "Epoch 53, Training Loss: 0.798792439699173\n",
      "Epoch 54, Training Loss: 0.799170165903428\n",
      "Epoch 55, Training Loss: 0.7988479306417353\n",
      "Epoch 56, Training Loss: 0.7984594170486226\n",
      "Epoch 57, Training Loss: 0.798612472450032\n",
      "Epoch 58, Training Loss: 0.7983645232284771\n",
      "Epoch 59, Training Loss: 0.7984029436812682\n",
      "Epoch 60, Training Loss: 0.7984650806118461\n",
      "Epoch 61, Training Loss: 0.7978072735141305\n",
      "Epoch 62, Training Loss: 0.7980812913530013\n",
      "Epoch 63, Training Loss: 0.7977231643480414\n",
      "Epoch 64, Training Loss: 0.7980804546440349\n",
      "Epoch 65, Training Loss: 0.7979472940809587\n",
      "Epoch 66, Training Loss: 0.7977255612261155\n",
      "Epoch 67, Training Loss: 0.797743166404612\n",
      "Epoch 68, Training Loss: 0.7975037611933316\n",
      "Epoch 69, Training Loss: 0.7973994777483099\n",
      "Epoch 70, Training Loss: 0.7972443499985863\n",
      "Epoch 71, Training Loss: 0.7973663321663351\n",
      "Epoch 72, Training Loss: 0.7972687233896817\n",
      "Epoch 73, Training Loss: 0.797177933735006\n",
      "Epoch 74, Training Loss: 0.7969305182905758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:53:59,344] Trial 21 finished with value: 0.635 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7965840300391702\n",
      "Epoch 1, Training Loss: 0.9179044233228927\n",
      "Epoch 2, Training Loss: 0.8253897232220585\n",
      "Epoch 3, Training Loss: 0.8073342044550673\n",
      "Epoch 4, Training Loss: 0.8034336984605718\n",
      "Epoch 5, Training Loss: 0.8012544398917292\n",
      "Epoch 6, Training Loss: 0.7993836393929962\n",
      "Epoch 7, Training Loss: 0.7985928046972232\n",
      "Epoch 8, Training Loss: 0.7973189423855086\n",
      "Epoch 9, Training Loss: 0.7959717416225519\n",
      "Epoch 10, Training Loss: 0.7944872018089868\n",
      "Epoch 11, Training Loss: 0.7933664820248023\n",
      "Epoch 12, Training Loss: 0.7915263366878481\n",
      "Epoch 13, Training Loss: 0.7910259259374518\n",
      "Epoch 14, Training Loss: 0.7906200075508061\n",
      "Epoch 15, Training Loss: 0.7901470931849085\n",
      "Epoch 16, Training Loss: 0.7898311349682342\n",
      "Epoch 17, Training Loss: 0.7881700463761064\n",
      "Epoch 18, Training Loss: 0.7886904314944618\n",
      "Epoch 19, Training Loss: 0.7868386841806254\n",
      "Epoch 20, Training Loss: 0.7870679109616388\n",
      "Epoch 21, Training Loss: 0.7872046334402901\n",
      "Epoch 22, Training Loss: 0.7882056962278553\n",
      "Epoch 23, Training Loss: 0.7867268111472739\n",
      "Epoch 24, Training Loss: 0.7864951211707036\n",
      "Epoch 25, Training Loss: 0.7854364617426592\n",
      "Epoch 26, Training Loss: 0.7851104159104196\n",
      "Epoch 27, Training Loss: 0.7854591925341384\n",
      "Epoch 28, Training Loss: 0.7846939619322468\n",
      "Epoch 29, Training Loss: 0.7847253279578417\n",
      "Epoch 30, Training Loss: 0.783809904362026\n",
      "Epoch 31, Training Loss: 0.7837573125846404\n",
      "Epoch 32, Training Loss: 0.7850248542047085\n",
      "Epoch 33, Training Loss: 0.7839948191678614\n",
      "Epoch 34, Training Loss: 0.783808281905669\n",
      "Epoch 35, Training Loss: 0.7833192784983413\n",
      "Epoch 36, Training Loss: 0.7832253783269036\n",
      "Epoch 37, Training Loss: 0.7835221606089656\n",
      "Epoch 38, Training Loss: 0.7834473131294537\n",
      "Epoch 39, Training Loss: 0.7835335606919196\n",
      "Epoch 40, Training Loss: 0.7826038149962747\n",
      "Epoch 41, Training Loss: 0.781858371702352\n",
      "Epoch 42, Training Loss: 0.7827323349794947\n",
      "Epoch 43, Training Loss: 0.7826357975041955\n",
      "Epoch 44, Training Loss: 0.7821828690686621\n",
      "Epoch 45, Training Loss: 0.7818105157156636\n",
      "Epoch 46, Training Loss: 0.7814499042984239\n",
      "Epoch 47, Training Loss: 0.7818886513996841\n",
      "Epoch 48, Training Loss: 0.7820459521802744\n",
      "Epoch 49, Training Loss: 0.7821903087142715\n",
      "Epoch 50, Training Loss: 0.781004560173006\n",
      "Epoch 51, Training Loss: 0.7810836501587602\n",
      "Epoch 52, Training Loss: 0.7811946049668735\n",
      "Epoch 53, Training Loss: 0.7805771687873324\n",
      "Epoch 54, Training Loss: 0.7807194293889784\n",
      "Epoch 55, Training Loss: 0.7815211306837269\n",
      "Epoch 56, Training Loss: 0.7808673565549061\n",
      "Epoch 57, Training Loss: 0.7808342377942308\n",
      "Epoch 58, Training Loss: 0.7808854664178719\n",
      "Epoch 59, Training Loss: 0.7802294966869785\n",
      "Epoch 60, Training Loss: 0.780063259511962\n",
      "Epoch 61, Training Loss: 0.7797577827496636\n",
      "Epoch 62, Training Loss: 0.7804014441662265\n",
      "Epoch 63, Training Loss: 0.7801210114830419\n",
      "Epoch 64, Training Loss: 0.7796805057310521\n",
      "Epoch 65, Training Loss: 0.7803678750991822\n",
      "Epoch 66, Training Loss: 0.7801618518685935\n",
      "Epoch 67, Training Loss: 0.7800090049442492\n",
      "Epoch 68, Training Loss: 0.7795974649881062\n",
      "Epoch 69, Training Loss: 0.7801783540195092\n",
      "Epoch 70, Training Loss: 0.7793669212133365\n",
      "Epoch 71, Training Loss: 0.7792372780634944\n",
      "Epoch 72, Training Loss: 0.77889885319803\n",
      "Epoch 73, Training Loss: 0.7792161553425896\n",
      "Epoch 74, Training Loss: 0.7793253183364868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:55:26,725] Trial 22 finished with value: 0.6396 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7790543588480555\n",
      "Epoch 1, Training Loss: 1.0702815130878898\n",
      "Epoch 2, Training Loss: 1.0322266670535591\n",
      "Epoch 3, Training Loss: 1.0097372070480795\n",
      "Epoch 4, Training Loss: 0.9928621207966524\n",
      "Epoch 5, Training Loss: 0.9796236100617577\n",
      "Epoch 6, Training Loss: 0.9691856586933136\n",
      "Epoch 7, Training Loss: 0.9609280030867633\n",
      "Epoch 8, Training Loss: 0.9544015461557052\n",
      "Epoch 9, Training Loss: 0.9492237066521364\n",
      "Epoch 10, Training Loss: 0.9450950045445387\n",
      "Epoch 11, Training Loss: 0.94177873990115\n",
      "Epoch 12, Training Loss: 0.9390711777350482\n",
      "Epoch 13, Training Loss: 0.9368262276228736\n",
      "Epoch 14, Training Loss: 0.9349426334745744\n",
      "Epoch 15, Training Loss: 0.9333237569472369\n",
      "Epoch 16, Training Loss: 0.9319147216572481\n",
      "Epoch 17, Training Loss: 0.9306596835220561\n",
      "Epoch 18, Training Loss: 0.9295327286860522\n",
      "Epoch 19, Training Loss: 0.9284976989381454\n",
      "Epoch 20, Training Loss: 0.9275333448718576\n",
      "Epoch 21, Training Loss: 0.9266321348442751\n",
      "Epoch 22, Training Loss: 0.9257677975822898\n",
      "Epoch 23, Training Loss: 0.9249498415694517\n",
      "Epoch 24, Training Loss: 0.9241539746873519\n",
      "Epoch 25, Training Loss: 0.9233912806651171\n",
      "Epoch 26, Training Loss: 0.9226456473855411\n",
      "Epoch 27, Training Loss: 0.9219096430610207\n",
      "Epoch 28, Training Loss: 0.9212039946107303\n",
      "Epoch 29, Training Loss: 0.9205078913885004\n",
      "Epoch 30, Training Loss: 0.9198276728742263\n",
      "Epoch 31, Training Loss: 0.9191501890210544\n",
      "Epoch 32, Training Loss: 0.9184922256890465\n",
      "Epoch 33, Training Loss: 0.9178381450035993\n",
      "Epoch 34, Training Loss: 0.9171925858890309\n",
      "Epoch 35, Training Loss: 0.9165529151523815\n",
      "Epoch 36, Training Loss: 0.9159196770191192\n",
      "Epoch 37, Training Loss: 0.9152953339324278\n",
      "Epoch 38, Training Loss: 0.914671935263802\n",
      "Epoch 39, Training Loss: 0.914056722837336\n",
      "Epoch 40, Training Loss: 0.913450021533405\n",
      "Epoch 41, Training Loss: 0.912840737174539\n",
      "Epoch 42, Training Loss: 0.9122353489959941\n",
      "Epoch 43, Training Loss: 0.9116353293026195\n",
      "Epoch 44, Training Loss: 0.9110367505690631\n",
      "Epoch 45, Training Loss: 0.9104409538998324\n",
      "Epoch 46, Training Loss: 0.90985366842326\n",
      "Epoch 47, Training Loss: 0.9092592259014354\n",
      "Epoch 48, Training Loss: 0.9086694031603196\n",
      "Epoch 49, Training Loss: 0.9080776176031898\n",
      "Epoch 50, Training Loss: 0.9074794428488787\n",
      "Epoch 51, Training Loss: 0.9069002765066484\n",
      "Epoch 52, Training Loss: 0.9063110611719244\n",
      "Epoch 53, Training Loss: 0.9057200593808118\n",
      "Epoch 54, Training Loss: 0.9051307106018066\n",
      "Epoch 55, Training Loss: 0.9045184488857494\n",
      "Epoch 56, Training Loss: 0.9039364559510175\n",
      "Epoch 57, Training Loss: 0.9033374408413383\n",
      "Epoch 58, Training Loss: 0.9027332634084365\n",
      "Epoch 59, Training Loss: 0.9021282129427965\n",
      "Epoch 60, Training Loss: 0.9015315701681025\n",
      "Epoch 61, Training Loss: 0.9009231786868152\n",
      "Epoch 62, Training Loss: 0.900317364580491\n",
      "Epoch 63, Training Loss: 0.89970057739931\n",
      "Epoch 64, Training Loss: 0.8990880945850821\n",
      "Epoch 65, Training Loss: 0.8984594439057743\n",
      "Epoch 66, Training Loss: 0.8978332216599408\n",
      "Epoch 67, Training Loss: 0.897207326187807\n",
      "Epoch 68, Training Loss: 0.8965687894821167\n",
      "Epoch 69, Training Loss: 0.8959352261879865\n",
      "Epoch 70, Training Loss: 0.8952859914302826\n",
      "Epoch 71, Training Loss: 0.8946400489526637\n",
      "Epoch 72, Training Loss: 0.8939858492682962\n",
      "Epoch 73, Training Loss: 0.8933273098749273\n",
      "Epoch 74, Training Loss: 0.8926545795272378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:56:36,029] Trial 23 finished with value: 0.5792666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8919848045882057\n",
      "Epoch 1, Training Loss: 1.076875969802632\n",
      "Epoch 2, Training Loss: 1.0424488688216489\n",
      "Epoch 3, Training Loss: 1.0200535659930285\n",
      "Epoch 4, Training Loss: 1.002931728924022\n",
      "Epoch 5, Training Loss: 0.9895726114862106\n",
      "Epoch 6, Training Loss: 0.9790996377608355\n",
      "Epoch 7, Training Loss: 0.9708820055512821\n",
      "Epoch 8, Training Loss: 0.9644112656396978\n",
      "Epoch 9, Training Loss: 0.9593107630224789\n",
      "Epoch 10, Training Loss: 0.9552557837261874\n",
      "Epoch 11, Training Loss: 0.9520016779619105\n",
      "Epoch 12, Training Loss: 0.9493483434705173\n",
      "Epoch 13, Training Loss: 0.947165678879794\n",
      "Epoch 14, Training Loss: 0.9453193542536568\n",
      "Epoch 15, Training Loss: 0.9437370948230519\n",
      "Epoch 16, Training Loss: 0.9423454085518332\n",
      "Epoch 17, Training Loss: 0.941106594800949\n",
      "Epoch 18, Training Loss: 0.9399805331230163\n",
      "Epoch 19, Training Loss: 0.9389436485486872\n",
      "Epoch 20, Training Loss: 0.937980150755714\n",
      "Epoch 21, Training Loss: 0.937063541061738\n",
      "Epoch 22, Training Loss: 0.936199732948752\n",
      "Epoch 23, Training Loss: 0.9353681058042189\n",
      "Epoch 24, Training Loss: 0.9345657779889949\n",
      "Epoch 25, Training Loss: 0.9337984659391291\n",
      "Epoch 26, Training Loss: 0.9330397373087266\n",
      "Epoch 27, Training Loss: 0.9323178409127628\n",
      "Epoch 28, Training Loss: 0.9316001726599301\n",
      "Epoch 29, Training Loss: 0.9309054136276245\n",
      "Epoch 30, Training Loss: 0.930219817091437\n",
      "Epoch 31, Training Loss: 0.9295493758425993\n",
      "Epoch 32, Training Loss: 0.9288895947792951\n",
      "Epoch 33, Training Loss: 0.92824979094898\n",
      "Epoch 34, Training Loss: 0.9276118059018079\n",
      "Epoch 35, Training Loss: 0.9269902258059558\n",
      "Epoch 36, Training Loss: 0.9263668715252595\n",
      "Epoch 37, Training Loss: 0.9257613427498761\n",
      "Epoch 38, Training Loss: 0.9251672255291659\n",
      "Epoch 39, Training Loss: 0.9245755726449629\n",
      "Epoch 40, Training Loss: 0.9239886290185592\n",
      "Epoch 41, Training Loss: 0.9234083183372722\n",
      "Epoch 42, Training Loss: 0.922839567380793\n",
      "Epoch 43, Training Loss: 0.9222665652808021\n",
      "Epoch 44, Training Loss: 0.9217060553326326\n",
      "Epoch 45, Training Loss: 0.9211475715216468\n",
      "Epoch 46, Training Loss: 0.9205967507642858\n",
      "Epoch 47, Training Loss: 0.9200408440477708\n",
      "Epoch 48, Training Loss: 0.9195023285641389\n",
      "Epoch 49, Training Loss: 0.9189611519084258\n",
      "Epoch 50, Training Loss: 0.9184192937963149\n",
      "Epoch 51, Training Loss: 0.9178838898855097\n",
      "Epoch 52, Training Loss: 0.9173515180279227\n",
      "Epoch 53, Training Loss: 0.9168217836408055\n",
      "Epoch 54, Training Loss: 0.9162909078598023\n",
      "Epoch 55, Training Loss: 0.9157710562032811\n",
      "Epoch 56, Training Loss: 0.9152466322393978\n",
      "Epoch 57, Training Loss: 0.9147212238171522\n",
      "Epoch 58, Training Loss: 0.914197442741955\n",
      "Epoch 59, Training Loss: 0.9136855466225567\n",
      "Epoch 60, Training Loss: 0.9131626205584582\n",
      "Epoch 61, Training Loss: 0.9126396027032067\n",
      "Epoch 62, Training Loss: 0.9121184401652392\n",
      "Epoch 63, Training Loss: 0.911588481314042\n",
      "Epoch 64, Training Loss: 0.9110768201070674\n",
      "Epoch 65, Training Loss: 0.9105508720173555\n",
      "Epoch 66, Training Loss: 0.9100258292170131\n",
      "Epoch 67, Training Loss: 0.9094953828699448\n",
      "Epoch 68, Training Loss: 0.9089548087821288\n",
      "Epoch 69, Training Loss: 0.9084304717709036\n",
      "Epoch 70, Training Loss: 0.9078931269224952\n",
      "Epoch 71, Training Loss: 0.9073580131811254\n",
      "Epoch 72, Training Loss: 0.9068137369436376\n",
      "Epoch 73, Training Loss: 0.9062650491209591\n",
      "Epoch 74, Training Loss: 0.905721338706858\n",
      "Epoch 75, Training Loss: 0.9051704577137443\n",
      "Epoch 76, Training Loss: 0.904615677875631\n",
      "Epoch 77, Training Loss: 0.9040550064339358\n",
      "Epoch 78, Training Loss: 0.9034955470702227\n",
      "Epoch 79, Training Loss: 0.9029315622413859\n",
      "Epoch 80, Training Loss: 0.9023602054399602\n",
      "Epoch 81, Training Loss: 0.901781751127804\n",
      "Epoch 82, Training Loss: 0.901214058118708\n",
      "Epoch 83, Training Loss: 0.9006283794431126\n",
      "Epoch 84, Training Loss: 0.9000513517856598\n",
      "Epoch 85, Training Loss: 0.8994533669948578\n",
      "Epoch 86, Training Loss: 0.8988649936283336\n",
      "Epoch 87, Training Loss: 0.8982664531118729\n",
      "Epoch 88, Training Loss: 0.8976641321883482\n",
      "Epoch 89, Training Loss: 0.8970522595153135\n",
      "Epoch 90, Training Loss: 0.8964562314398149\n",
      "Epoch 91, Training Loss: 0.8958319450125974\n",
      "Epoch 92, Training Loss: 0.8952288327497594\n",
      "Epoch 93, Training Loss: 0.8946105327325709\n",
      "Epoch 94, Training Loss: 0.8939836648632499\n",
      "Epoch 95, Training Loss: 0.8933622982922722\n",
      "Epoch 96, Training Loss: 0.8927305527294384\n",
      "Epoch 97, Training Loss: 0.8920953489752377\n",
      "Epoch 98, Training Loss: 0.8914475858211517\n",
      "Epoch 99, Training Loss: 0.8908014134098502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:58:07,623] Trial 24 finished with value: 0.5781333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8901576439773335\n",
      "Epoch 1, Training Loss: 0.9821711918185739\n",
      "Epoch 2, Training Loss: 0.9484802096030291\n",
      "Epoch 3, Training Loss: 0.932308741457322\n",
      "Epoch 4, Training Loss: 0.9117938115316279\n",
      "Epoch 5, Training Loss: 0.8888909181426553\n",
      "Epoch 6, Training Loss: 0.8664845317952773\n",
      "Epoch 7, Training Loss: 0.8480481424752404\n",
      "Epoch 8, Training Loss: 0.8348389007063473\n",
      "Epoch 9, Training Loss: 0.825667475532083\n",
      "Epoch 10, Training Loss: 0.8199232081104727\n",
      "Epoch 11, Training Loss: 0.8160644636434667\n",
      "Epoch 12, Training Loss: 0.8137293893449447\n",
      "Epoch 13, Training Loss: 0.8120045009781333\n",
      "Epoch 14, Training Loss: 0.8109240509481991\n",
      "Epoch 15, Training Loss: 0.8099429566719952\n",
      "Epoch 16, Training Loss: 0.8092556865075056\n",
      "Epoch 17, Training Loss: 0.8089619187747731\n",
      "Epoch 18, Training Loss: 0.8084733996671789\n",
      "Epoch 19, Training Loss: 0.8082109444982866\n",
      "Epoch 20, Training Loss: 0.8078361223024481\n",
      "Epoch 21, Training Loss: 0.8076185116347144\n",
      "Epoch 22, Training Loss: 0.8074650288329405\n",
      "Epoch 23, Training Loss: 0.8069268471353195\n",
      "Epoch 24, Training Loss: 0.8068773812406204\n",
      "Epoch 25, Training Loss: 0.8065271324971143\n",
      "Epoch 26, Training Loss: 0.8063806433537427\n",
      "Epoch 27, Training Loss: 0.8063167672998764\n",
      "Epoch 28, Training Loss: 0.8062221128800336\n",
      "Epoch 29, Training Loss: 0.8058430166805491\n",
      "Epoch 30, Training Loss: 0.8055919394773595\n",
      "Epoch 31, Training Loss: 0.8054077715032241\n",
      "Epoch 32, Training Loss: 0.8054681047972511\n",
      "Epoch 33, Training Loss: 0.8051002040330102\n",
      "Epoch 34, Training Loss: 0.8051769342141993\n",
      "Epoch 35, Training Loss: 0.8045236914999345\n",
      "Epoch 36, Training Loss: 0.8045944840767805\n",
      "Epoch 37, Training Loss: 0.8045988346548641\n",
      "Epoch 38, Training Loss: 0.8041469713519601\n",
      "Epoch 39, Training Loss: 0.8043271287749796\n",
      "Epoch 40, Training Loss: 0.8040689795157488\n",
      "Epoch 41, Training Loss: 0.8041144583505743\n",
      "Epoch 42, Training Loss: 0.8038937595311333\n",
      "Epoch 43, Training Loss: 0.8035710006601671\n",
      "Epoch 44, Training Loss: 0.8034866014648886\n",
      "Epoch 45, Training Loss: 0.8032999380195842\n",
      "Epoch 46, Training Loss: 0.8031099109789904\n",
      "Epoch 47, Training Loss: 0.8031360803632175\n",
      "Epoch 48, Training Loss: 0.8029809200763702\n",
      "Epoch 49, Training Loss: 0.8027579799820395\n",
      "Epoch 50, Training Loss: 0.8028902459845824\n",
      "Epoch 51, Training Loss: 0.802575240836424\n",
      "Epoch 52, Training Loss: 0.802540274367613\n",
      "Epoch 53, Training Loss: 0.8021654069423676\n",
      "Epoch 54, Training Loss: 0.8024257267923917\n",
      "Epoch 55, Training Loss: 0.8022310275190017\n",
      "Epoch 56, Training Loss: 0.8020643133275649\n",
      "Epoch 57, Training Loss: 0.8019970434553483\n",
      "Epoch 58, Training Loss: 0.8018117739172543\n",
      "Epoch 59, Training Loss: 0.8016519019884222\n",
      "Epoch 60, Training Loss: 0.8016894434480106\n",
      "Epoch 61, Training Loss: 0.8014542573339799\n",
      "Epoch 62, Training Loss: 0.8014636224858901\n",
      "Epoch 63, Training Loss: 0.801386608656715\n",
      "Epoch 64, Training Loss: 0.8012952528981602\n",
      "Epoch 65, Training Loss: 0.8010853348760044\n",
      "Epoch 66, Training Loss: 0.8010979389443117\n",
      "Epoch 67, Training Loss: 0.8009382442165823\n",
      "Epoch 68, Training Loss: 0.8008746165387771\n",
      "Epoch 69, Training Loss: 0.8006917919832117\n",
      "Epoch 70, Training Loss: 0.8006962077757892\n",
      "Epoch 71, Training Loss: 0.8004704204727622\n",
      "Epoch 72, Training Loss: 0.8003748685472152\n",
      "Epoch 73, Training Loss: 0.800347568357692\n",
      "Epoch 74, Training Loss: 0.8002802434388329\n",
      "Epoch 75, Training Loss: 0.8001812889295465\n",
      "Epoch 76, Training Loss: 0.8001487088904661\n",
      "Epoch 77, Training Loss: 0.8000565793934991\n",
      "Epoch 78, Training Loss: 0.8001035125816569\n",
      "Epoch 79, Training Loss: 0.7999784365121057\n",
      "Epoch 80, Training Loss: 0.7996670922812293\n",
      "Epoch 81, Training Loss: 0.7998224853768068\n",
      "Epoch 82, Training Loss: 0.7997649029423208\n",
      "Epoch 83, Training Loss: 0.7996040266401627\n",
      "Epoch 84, Training Loss: 0.7994647575125975\n",
      "Epoch 85, Training Loss: 0.7994226908683777\n",
      "Epoch 86, Training Loss: 0.7992112447233761\n",
      "Epoch 87, Training Loss: 0.7992319194008323\n",
      "Epoch 88, Training Loss: 0.7992110934678246\n",
      "Epoch 89, Training Loss: 0.7990922638247995\n",
      "Epoch 90, Training Loss: 0.7991098375881419\n",
      "Epoch 91, Training Loss: 0.7991255079998689\n",
      "Epoch 92, Training Loss: 0.7990731050687677\n",
      "Epoch 93, Training Loss: 0.7989307459662942\n",
      "Epoch 94, Training Loss: 0.7989824785905726\n",
      "Epoch 95, Training Loss: 0.798845450036666\n",
      "Epoch 96, Training Loss: 0.7988738669367398\n",
      "Epoch 97, Training Loss: 0.7988302544986501\n",
      "Epoch 98, Training Loss: 0.7987497148794286\n",
      "Epoch 99, Training Loss: 0.7986023184832405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 09:59:54,969] Trial 25 finished with value: 0.6346666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7985842300162596\n",
      "Epoch 1, Training Loss: 0.8544921052455902\n",
      "Epoch 2, Training Loss: 0.8210618678261252\n",
      "Epoch 3, Training Loss: 0.8149452013829175\n",
      "Epoch 4, Training Loss: 0.8124288638900308\n",
      "Epoch 5, Training Loss: 0.810218156786526\n",
      "Epoch 6, Training Loss: 0.8100415989931892\n",
      "Epoch 7, Training Loss: 0.8087248988712535\n",
      "Epoch 8, Training Loss: 0.8080909238142125\n",
      "Epoch 9, Training Loss: 0.8075157931972953\n",
      "Epoch 10, Training Loss: 0.8068987553961137\n",
      "Epoch 11, Training Loss: 0.8068326513907489\n",
      "Epoch 12, Training Loss: 0.8073194553571589\n",
      "Epoch 13, Training Loss: 0.8062394940151888\n",
      "Epoch 14, Training Loss: 0.8073585099332473\n",
      "Epoch 15, Training Loss: 0.8058559917702395\n",
      "Epoch 16, Training Loss: 0.8068925023078919\n",
      "Epoch 17, Training Loss: 0.8052851122267106\n",
      "Epoch 18, Training Loss: 0.8058531660893384\n",
      "Epoch 19, Training Loss: 0.8050561690330506\n",
      "Epoch 20, Training Loss: 0.8052339481606203\n",
      "Epoch 21, Training Loss: 0.8048133974215563\n",
      "Epoch 22, Training Loss: 0.8050408706945531\n",
      "Epoch 23, Training Loss: 0.8051870525584501\n",
      "Epoch 24, Training Loss: 0.8057392240271849\n",
      "Epoch 25, Training Loss: 0.8040323315648472\n",
      "Epoch 26, Training Loss: 0.8062869111229392\n",
      "Epoch 27, Training Loss: 0.8037627324637244\n",
      "Epoch 28, Training Loss: 0.805546158061308\n",
      "Epoch 29, Training Loss: 0.8038563681350035\n",
      "Epoch 30, Training Loss: 0.8048823179918178\n",
      "Epoch 31, Training Loss: 0.8033807553964503\n",
      "Epoch 32, Training Loss: 0.8034919504558339\n",
      "Epoch 33, Training Loss: 0.8028361594676972\n",
      "Epoch 34, Training Loss: 0.8028661076461567\n",
      "Epoch 35, Training Loss: 0.8035044153297649\n",
      "Epoch 36, Training Loss: 0.8024514350470374\n",
      "Epoch 37, Training Loss: 0.8028763358032002\n",
      "Epoch 38, Training Loss: 0.801473874835407\n",
      "Epoch 39, Training Loss: 0.802520505400265\n",
      "Epoch 40, Training Loss: 0.8017193243784063\n",
      "Epoch 41, Training Loss: 0.8008442791069255\n",
      "Epoch 42, Training Loss: 0.801539878354353\n",
      "Epoch 43, Training Loss: 0.8013420435961555\n",
      "Epoch 44, Training Loss: 0.800837335726794\n",
      "Epoch 45, Training Loss: 0.8003852413682376\n",
      "Epoch 46, Training Loss: 0.8006582312022938\n",
      "Epoch 47, Training Loss: 0.8010730623497683\n",
      "Epoch 48, Training Loss: 0.7996625028638279\n",
      "Epoch 49, Training Loss: 0.801079887712703\n",
      "Epoch 50, Training Loss: 0.800068113523371\n",
      "Epoch 51, Training Loss: 0.7990560521798975\n",
      "Epoch 52, Training Loss: 0.7993341465557323\n",
      "Epoch 53, Training Loss: 0.7988684668260462\n",
      "Epoch 54, Training Loss: 0.7999510462845073\n",
      "Epoch 55, Training Loss: 0.7985058878449832\n",
      "Epoch 56, Training Loss: 0.7989144215864293\n",
      "Epoch 57, Training Loss: 0.7988272410280565\n",
      "Epoch 58, Training Loss: 0.7990981083056506\n",
      "Epoch 59, Training Loss: 0.7998535229879267\n",
      "Epoch 60, Training Loss: 0.7978009406258079\n",
      "Epoch 61, Training Loss: 0.7979601339031668\n",
      "Epoch 62, Training Loss: 0.7985242200598998\n",
      "Epoch 63, Training Loss: 0.7985306706849267\n",
      "Epoch 64, Training Loss: 0.7981857071904576\n",
      "Epoch 65, Training Loss: 0.7993285005232867\n",
      "Epoch 66, Training Loss: 0.7986005102185642\n",
      "Epoch 67, Training Loss: 0.7987957312079037\n",
      "Epoch 68, Training Loss: 0.7971296510976904\n",
      "Epoch 69, Training Loss: 0.7981185316338258\n",
      "Epoch 70, Training Loss: 0.7976218363116769\n",
      "Epoch 71, Training Loss: 0.7983600721639745\n",
      "Epoch 72, Training Loss: 0.7972495144255021\n",
      "Epoch 73, Training Loss: 0.7982834870675031\n",
      "Epoch 74, Training Loss: 0.7985612271813786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:01:19,668] Trial 26 finished with value: 0.6353333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7969611306050245\n",
      "Epoch 1, Training Loss: 0.9666399750990026\n",
      "Epoch 2, Training Loss: 0.9354324748235591\n",
      "Epoch 3, Training Loss: 0.9153376682365642\n",
      "Epoch 4, Training Loss: 0.8950847705672769\n",
      "Epoch 5, Training Loss: 0.8759310294600094\n",
      "Epoch 6, Training Loss: 0.8585579549565034\n",
      "Epoch 7, Training Loss: 0.8446663669978871\n",
      "Epoch 8, Training Loss: 0.8341663345168618\n",
      "Epoch 9, Training Loss: 0.8266000323435839\n",
      "Epoch 10, Training Loss: 0.8213555239929873\n",
      "Epoch 11, Training Loss: 0.8176603442781112\n",
      "Epoch 12, Training Loss: 0.8151686865441939\n",
      "Epoch 13, Training Loss: 0.8134173722126905\n",
      "Epoch 14, Training Loss: 0.8120546713296105\n",
      "Epoch 15, Training Loss: 0.8109965360865874\n",
      "Epoch 16, Training Loss: 0.8101193045868593\n",
      "Epoch 17, Training Loss: 0.8094223826773026\n",
      "Epoch 18, Training Loss: 0.8088420509590822\n",
      "Epoch 19, Training Loss: 0.8081861759634579\n",
      "Epoch 20, Training Loss: 0.8077683572909411\n",
      "Epoch 21, Training Loss: 0.8072469622247359\n",
      "Epoch 22, Training Loss: 0.806994900633307\n",
      "Epoch 23, Training Loss: 0.8063888308581184\n",
      "Epoch 24, Training Loss: 0.8060582373422734\n",
      "Epoch 25, Training Loss: 0.8058938727659337\n",
      "Epoch 26, Training Loss: 0.8053757319730871\n",
      "Epoch 27, Training Loss: 0.8051347211529227\n",
      "Epoch 28, Training Loss: 0.8047911310195923\n",
      "Epoch 29, Training Loss: 0.8046084831742679\n",
      "Epoch 30, Training Loss: 0.8041126408296473\n",
      "Epoch 31, Training Loss: 0.8037518128928016\n",
      "Epoch 32, Training Loss: 0.8033726032341227\n",
      "Epoch 33, Training Loss: 0.803035084920771\n",
      "Epoch 34, Training Loss: 0.8030577142799602\n",
      "Epoch 35, Training Loss: 0.8027614773722256\n",
      "Epoch 36, Training Loss: 0.8024679244265837\n",
      "Epoch 37, Training Loss: 0.8022250213342554\n",
      "Epoch 38, Training Loss: 0.801963113896987\n",
      "Epoch 39, Training Loss: 0.8019303175982307\n",
      "Epoch 40, Training Loss: 0.8015034464527578\n",
      "Epoch 41, Training Loss: 0.8015215070107404\n",
      "Epoch 42, Training Loss: 0.801234382390976\n",
      "Epoch 43, Training Loss: 0.8009589066224939\n",
      "Epoch 44, Training Loss: 0.8009342596811406\n",
      "Epoch 45, Training Loss: 0.8007181232115802\n",
      "Epoch 46, Training Loss: 0.8004371199888342\n",
      "Epoch 47, Training Loss: 0.800424628608367\n",
      "Epoch 48, Training Loss: 0.8001233851208406\n",
      "Epoch 49, Training Loss: 0.8002347488964305\n",
      "Epoch 50, Training Loss: 0.8000184052130755\n",
      "Epoch 51, Training Loss: 0.7999636169040905\n",
      "Epoch 52, Training Loss: 0.7997174732825335\n",
      "Epoch 53, Training Loss: 0.7997535258180954\n",
      "Epoch 54, Training Loss: 0.7995091875160442\n",
      "Epoch 55, Training Loss: 0.7993836911986856\n",
      "Epoch 56, Training Loss: 0.7993369059702929\n",
      "Epoch 57, Training Loss: 0.7994338649861953\n",
      "Epoch 58, Training Loss: 0.79920430597137\n",
      "Epoch 59, Training Loss: 0.7991874109296238\n",
      "Epoch 60, Training Loss: 0.7989303240355323\n",
      "Epoch 61, Training Loss: 0.7988675955463859\n",
      "Epoch 62, Training Loss: 0.798870406220941\n",
      "Epoch 63, Training Loss: 0.7986889894569621\n",
      "Epoch 64, Training Loss: 0.7987473264862509\n",
      "Epoch 65, Training Loss: 0.7987414814444149\n",
      "Epoch 66, Training Loss: 0.7986056003149818\n",
      "Epoch 67, Training Loss: 0.7985836632111494\n",
      "Epoch 68, Training Loss: 0.7984045914341422\n",
      "Epoch 69, Training Loss: 0.7982067763805389\n",
      "Epoch 70, Training Loss: 0.7984035851674921\n",
      "Epoch 71, Training Loss: 0.7983640879042009\n",
      "Epoch 72, Training Loss: 0.7982217505398919\n",
      "Epoch 73, Training Loss: 0.798198520786622\n",
      "Epoch 74, Training Loss: 0.7982559760177836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:02:29,529] Trial 27 finished with value: 0.6358 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7982758228217854\n",
      "Epoch 1, Training Loss: 0.8828872736061321\n",
      "Epoch 2, Training Loss: 0.8148974109397215\n",
      "Epoch 3, Training Loss: 0.8101479277891271\n",
      "Epoch 4, Training Loss: 0.8086918715869679\n",
      "Epoch 5, Training Loss: 0.8054042637348175\n",
      "Epoch 6, Training Loss: 0.8035953701944912\n",
      "Epoch 7, Training Loss: 0.7991745558907004\n",
      "Epoch 8, Training Loss: 0.7975690094863668\n",
      "Epoch 9, Training Loss: 0.7952344740839565\n",
      "Epoch 10, Training Loss: 0.7927962880976059\n",
      "Epoch 11, Training Loss: 0.7925631811338313\n",
      "Epoch 12, Training Loss: 0.7921623411599328\n",
      "Epoch 13, Training Loss: 0.7904015017257017\n",
      "Epoch 14, Training Loss: 0.790338364278569\n",
      "Epoch 15, Training Loss: 0.7902043295607847\n",
      "Epoch 16, Training Loss: 0.7889745864447425\n",
      "Epoch 17, Training Loss: 0.7901759727562175\n",
      "Epoch 18, Training Loss: 0.7890852909929612\n",
      "Epoch 19, Training Loss: 0.7884987102536594\n",
      "Epoch 20, Training Loss: 0.7895513701438904\n",
      "Epoch 21, Training Loss: 0.7881729276741252\n",
      "Epoch 22, Training Loss: 0.787595156361075\n",
      "Epoch 23, Training Loss: 0.7878713212293738\n",
      "Epoch 24, Training Loss: 0.7877671748049119\n",
      "Epoch 25, Training Loss: 0.7879869267519782\n",
      "Epoch 26, Training Loss: 0.7864143570731668\n",
      "Epoch 27, Training Loss: 0.7861845871280221\n",
      "Epoch 28, Training Loss: 0.7873220104329727\n",
      "Epoch 29, Training Loss: 0.7862169074310976\n",
      "Epoch 30, Training Loss: 0.7868670289656695\n",
      "Epoch 31, Training Loss: 0.7864161823777591\n",
      "Epoch 32, Training Loss: 0.7861030472727383\n",
      "Epoch 33, Training Loss: 0.7858620911486008\n",
      "Epoch 34, Training Loss: 0.7863637923493105\n",
      "Epoch 35, Training Loss: 0.7859170919306138\n",
      "Epoch 36, Training Loss: 0.7854333959607517\n",
      "Epoch 37, Training Loss: 0.785165874747669\n",
      "Epoch 38, Training Loss: 0.7845166178310619\n",
      "Epoch 39, Training Loss: 0.7847677495900323\n",
      "Epoch 40, Training Loss: 0.7845044805021847\n",
      "Epoch 41, Training Loss: 0.7841521548523622\n",
      "Epoch 42, Training Loss: 0.7848731361417209\n",
      "Epoch 43, Training Loss: 0.7843040236304788\n",
      "Epoch 44, Training Loss: 0.7846370081340566\n",
      "Epoch 45, Training Loss: 0.7843392247312209\n",
      "Epoch 46, Training Loss: 0.784484463299022\n",
      "Epoch 47, Training Loss: 0.7836038531976588\n",
      "Epoch 48, Training Loss: 0.7840741344059214\n",
      "Epoch 49, Training Loss: 0.7839804551180671\n",
      "Epoch 50, Training Loss: 0.783792712057338\n",
      "Epoch 51, Training Loss: 0.7837121414437014\n",
      "Epoch 52, Training Loss: 0.7842058293959674\n",
      "Epoch 53, Training Loss: 0.7832989992113675\n",
      "Epoch 54, Training Loss: 0.7836671808186699\n",
      "Epoch 55, Training Loss: 0.7828991099666147\n",
      "Epoch 56, Training Loss: 0.7831051161709954\n",
      "Epoch 57, Training Loss: 0.7828335686290966\n",
      "Epoch 58, Training Loss: 0.78246894345564\n",
      "Epoch 59, Training Loss: 0.7828912525317249\n",
      "Epoch 60, Training Loss: 0.7829205129427068\n",
      "Epoch 61, Training Loss: 0.7830074421097251\n",
      "Epoch 62, Training Loss: 0.7822442307892967\n",
      "Epoch 63, Training Loss: 0.7826899682073032\n",
      "Epoch 64, Training Loss: 0.782457903693704\n",
      "Epoch 65, Training Loss: 0.7821177329736597\n",
      "Epoch 66, Training Loss: 0.7826081449845258\n",
      "Epoch 67, Training Loss: 0.7819425673344556\n",
      "Epoch 68, Training Loss: 0.7820782146033118\n",
      "Epoch 69, Training Loss: 0.7823011126237757\n",
      "Epoch 70, Training Loss: 0.781347268819809\n",
      "Epoch 71, Training Loss: 0.7815176007326912\n",
      "Epoch 72, Training Loss: 0.7819497588101555\n",
      "Epoch 73, Training Loss: 0.7817034756436068\n",
      "Epoch 74, Training Loss: 0.7809157047552221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:04:07,831] Trial 28 finished with value: 0.6375333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7819148037714116\n",
      "Epoch 1, Training Loss: 0.8439201131988974\n",
      "Epoch 2, Training Loss: 0.8183600935515235\n",
      "Epoch 3, Training Loss: 0.8145732566188364\n",
      "Epoch 4, Training Loss: 0.8117278593427995\n",
      "Epoch 5, Training Loss: 0.8098342136775746\n",
      "Epoch 6, Training Loss: 0.8086261799054987\n",
      "Epoch 7, Training Loss: 0.8077956362331615\n",
      "Epoch 8, Training Loss: 0.8071187646949992\n",
      "Epoch 9, Training Loss: 0.8059110090311836\n",
      "Epoch 10, Training Loss: 0.8044978742739733\n",
      "Epoch 11, Training Loss: 0.8043025464170119\n",
      "Epoch 12, Training Loss: 0.8016998377267052\n",
      "Epoch 13, Training Loss: 0.803472844432382\n",
      "Epoch 14, Training Loss: 0.8037726630182828\n",
      "Epoch 15, Training Loss: 0.8029868661656099\n",
      "Epoch 16, Training Loss: 0.800981220147189\n",
      "Epoch 17, Training Loss: 0.8024501459738788\n",
      "Epoch 18, Training Loss: 0.8044250904812532\n",
      "Epoch 19, Training Loss: 0.8016221753989949\n",
      "Epoch 20, Training Loss: 0.8030497672277338\n",
      "Epoch 21, Training Loss: 0.8008322454200072\n",
      "Epoch 22, Training Loss: 0.8021889557557947\n",
      "Epoch 23, Training Loss: 0.8017729050972883\n",
      "Epoch 24, Training Loss: 0.802183595124413\n",
      "Epoch 25, Training Loss: 0.8007331389539382\n",
      "Epoch 26, Training Loss: 0.801089885375079\n",
      "Epoch 27, Training Loss: 0.7998587278057547\n",
      "Epoch 28, Training Loss: 0.800571128059836\n",
      "Epoch 29, Training Loss: 0.800772861663033\n",
      "Epoch 30, Training Loss: 0.8003764616040623\n",
      "Epoch 31, Training Loss: 0.7989971139150507\n",
      "Epoch 32, Training Loss: 0.799749272290398\n",
      "Epoch 33, Training Loss: 0.8006185401187224\n",
      "Epoch 34, Training Loss: 0.8003260637030882\n",
      "Epoch 35, Training Loss: 0.7982122026471531\n",
      "Epoch 36, Training Loss: 0.8003924108252806\n",
      "Epoch 37, Training Loss: 0.7989537654904758\n",
      "Epoch 38, Training Loss: 0.7982051227373236\n",
      "Epoch 39, Training Loss: 0.7997542482263902\n",
      "Epoch 40, Training Loss: 0.7995183768693138\n",
      "Epoch 41, Training Loss: 0.799606073253295\n",
      "Epoch 42, Training Loss: 0.7976567047483781\n",
      "Epoch 43, Training Loss: 0.7990026531499975\n",
      "Epoch 44, Training Loss: 0.7987212723844191\n",
      "Epoch 45, Training Loss: 0.798907265663147\n",
      "Epoch 46, Training Loss: 0.7994743611532099\n",
      "Epoch 47, Training Loss: 0.7983923663111294\n",
      "Epoch 48, Training Loss: 0.7985064967239605\n",
      "Epoch 49, Training Loss: 0.7991582411177018\n",
      "Epoch 50, Training Loss: 0.7991266145425684\n",
      "Epoch 51, Training Loss: 0.7981080995587742\n",
      "Epoch 52, Training Loss: 0.7996848973807167\n",
      "Epoch 53, Training Loss: 0.7983563912616056\n",
      "Epoch 54, Training Loss: 0.7983946176837472\n",
      "Epoch 55, Training Loss: 0.7987442471700557\n",
      "Epoch 56, Training Loss: 0.7990888639758615\n",
      "Epoch 57, Training Loss: 0.7977532733889187\n",
      "Epoch 58, Training Loss: 0.7995813049288357\n",
      "Epoch 59, Training Loss: 0.7986662127691156\n",
      "Epoch 60, Training Loss: 0.7978073578021105\n",
      "Epoch 61, Training Loss: 0.7992649641457726\n",
      "Epoch 62, Training Loss: 0.7972124875293058\n",
      "Epoch 63, Training Loss: 0.7993883966698366\n",
      "Epoch 64, Training Loss: 0.7994592618942261\n",
      "Epoch 65, Training Loss: 0.7989758724324844\n",
      "Epoch 66, Training Loss: 0.7991908123212702\n",
      "Epoch 67, Training Loss: 0.7988446453739615\n",
      "Epoch 68, Training Loss: 0.7981061173186582\n",
      "Epoch 69, Training Loss: 0.7980925656767452\n",
      "Epoch 70, Training Loss: 0.798551898914225\n",
      "Epoch 71, Training Loss: 0.7980273496403414\n",
      "Epoch 72, Training Loss: 0.7985421162493088\n",
      "Epoch 73, Training Loss: 0.7985640295814065\n",
      "Epoch 74, Training Loss: 0.7981186994384317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:05:33,502] Trial 29 finished with value: 0.6348666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7992106197862064\n",
      "Epoch 1, Training Loss: 0.9671805322170257\n",
      "Epoch 2, Training Loss: 0.9318252574696261\n",
      "Epoch 3, Training Loss: 0.9105222702026368\n",
      "Epoch 4, Training Loss: 0.8891576633733862\n",
      "Epoch 5, Training Loss: 0.8692653696677264\n",
      "Epoch 6, Training Loss: 0.8523689957226024\n",
      "Epoch 7, Training Loss: 0.8390062402977663\n",
      "Epoch 8, Training Loss: 0.8295594970618977\n",
      "Epoch 9, Training Loss: 0.8229244680965648\n",
      "Epoch 10, Training Loss: 0.8184096199624679\n",
      "Epoch 11, Training Loss: 0.8154350452563341\n",
      "Epoch 12, Training Loss: 0.8131361011196585\n",
      "Epoch 13, Training Loss: 0.811732341191348\n",
      "Epoch 14, Training Loss: 0.8105040386845084\n",
      "Epoch 15, Training Loss: 0.8097258951383478\n",
      "Epoch 16, Training Loss: 0.8092541945681853\n",
      "Epoch 17, Training Loss: 0.8086212856629316\n",
      "Epoch 18, Training Loss: 0.8079587601914126\n",
      "Epoch 19, Training Loss: 0.807666865867727\n",
      "Epoch 20, Training Loss: 0.8068899277378531\n",
      "Epoch 21, Training Loss: 0.8064646349934971\n",
      "Epoch 22, Training Loss: 0.806192327597562\n",
      "Epoch 23, Training Loss: 0.8058678765857921\n",
      "Epoch 24, Training Loss: 0.8055954802737516\n",
      "Epoch 25, Training Loss: 0.8051149798140806\n",
      "Epoch 26, Training Loss: 0.8048474207345178\n",
      "Epoch 27, Training Loss: 0.8047073801826028\n",
      "Epoch 28, Training Loss: 0.8042549261626075\n",
      "Epoch 29, Training Loss: 0.8039958568180309\n",
      "Epoch 30, Training Loss: 0.8037320270959069\n",
      "Epoch 31, Training Loss: 0.8035394331286935\n",
      "Epoch 32, Training Loss: 0.8033797161018147\n",
      "Epoch 33, Training Loss: 0.8029256798239315\n",
      "Epoch 34, Training Loss: 0.8027708475729999\n",
      "Epoch 35, Training Loss: 0.8026055828262778\n",
      "Epoch 36, Training Loss: 0.8024279233287362\n",
      "Epoch 37, Training Loss: 0.8021874689354616\n",
      "Epoch 38, Training Loss: 0.8019516821468577\n",
      "Epoch 39, Training Loss: 0.801905283016317\n",
      "Epoch 40, Training Loss: 0.8016019253169789\n",
      "Epoch 41, Training Loss: 0.8015313504022711\n",
      "Epoch 42, Training Loss: 0.8012963909962598\n",
      "Epoch 43, Training Loss: 0.8010563702443066\n",
      "Epoch 44, Training Loss: 0.8010106370028327\n",
      "Epoch 45, Training Loss: 0.8007278034266303\n",
      "Epoch 46, Training Loss: 0.8005760371685028\n",
      "Epoch 47, Training Loss: 0.8006061520997215\n",
      "Epoch 48, Training Loss: 0.8004829136764302\n",
      "Epoch 49, Training Loss: 0.8003435646786409\n",
      "Epoch 50, Training Loss: 0.8002385638040654\n",
      "Epoch 51, Training Loss: 0.8000774100948782\n",
      "Epoch 52, Training Loss: 0.8001021153085373\n",
      "Epoch 53, Training Loss: 0.7999800554443808\n",
      "Epoch 54, Training Loss: 0.7996738960462458\n",
      "Epoch 55, Training Loss: 0.7994832437879899\n",
      "Epoch 56, Training Loss: 0.7995237029300016\n",
      "Epoch 57, Training Loss: 0.7994470601923326\n",
      "Epoch 58, Training Loss: 0.7994191165531382\n",
      "Epoch 59, Training Loss: 0.7992271161780637\n",
      "Epoch 60, Training Loss: 0.7991729762273676\n",
      "Epoch 61, Training Loss: 0.798931818078546\n",
      "Epoch 62, Training Loss: 0.7989738207003649\n",
      "Epoch 63, Training Loss: 0.7991294306867263\n",
      "Epoch 64, Training Loss: 0.7988560379953945\n",
      "Epoch 65, Training Loss: 0.7986958121552187\n",
      "Epoch 66, Training Loss: 0.7987210555637584\n",
      "Epoch 67, Training Loss: 0.7986973417506499\n",
      "Epoch 68, Training Loss: 0.7984676950819352\n",
      "Epoch 69, Training Loss: 0.7984789954213535\n",
      "Epoch 70, Training Loss: 0.7983992357815013\n",
      "Epoch 71, Training Loss: 0.798263597278034\n",
      "Epoch 72, Training Loss: 0.7983645796775818\n",
      "Epoch 73, Training Loss: 0.7982695087264566\n",
      "Epoch 74, Training Loss: 0.798223866294412\n",
      "Epoch 75, Training Loss: 0.7979538314482745\n",
      "Epoch 76, Training Loss: 0.7981037540996776\n",
      "Epoch 77, Training Loss: 0.7980620690654305\n",
      "Epoch 78, Training Loss: 0.7979056370258332\n",
      "Epoch 79, Training Loss: 0.7979332538913279\n",
      "Epoch 80, Training Loss: 0.7978691981119268\n",
      "Epoch 81, Training Loss: 0.7980711811430314\n",
      "Epoch 82, Training Loss: 0.7978189593904159\n",
      "Epoch 83, Training Loss: 0.7978015241202187\n",
      "Epoch 84, Training Loss: 0.7976923826862784\n",
      "Epoch 85, Training Loss: 0.7977274215221405\n",
      "Epoch 86, Training Loss: 0.7977626162416794\n",
      "Epoch 87, Training Loss: 0.7976470269876368\n",
      "Epoch 88, Training Loss: 0.7973330796466154\n",
      "Epoch 89, Training Loss: 0.797519154548645\n",
      "Epoch 90, Training Loss: 0.7975549371803508\n",
      "Epoch 91, Training Loss: 0.7976771734742557\n",
      "Epoch 92, Training Loss: 0.7973756250213174\n",
      "Epoch 93, Training Loss: 0.797560494296691\n",
      "Epoch 94, Training Loss: 0.7973276462274439\n",
      "Epoch 95, Training Loss: 0.7972707524019129\n",
      "Epoch 96, Training Loss: 0.7973111756408916\n",
      "Epoch 97, Training Loss: 0.797246435880661\n",
      "Epoch 98, Training Loss: 0.7972650845611796\n",
      "Epoch 99, Training Loss: 0.7971938629010145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:07:07,715] Trial 30 finished with value: 0.6308666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.797038024593802\n",
      "Epoch 1, Training Loss: 0.8503851783006711\n",
      "Epoch 2, Training Loss: 0.8191874665425236\n",
      "Epoch 3, Training Loss: 0.8107170377458844\n",
      "Epoch 4, Training Loss: 0.8085367927873941\n",
      "Epoch 5, Training Loss: 0.8092370727008447\n",
      "Epoch 6, Training Loss: 0.8108353473190079\n",
      "Epoch 7, Training Loss: 0.8064858229536759\n",
      "Epoch 8, Training Loss: 0.8066108543173711\n",
      "Epoch 9, Training Loss: 0.8069269887486795\n",
      "Epoch 10, Training Loss: 0.8078329819485657\n",
      "Epoch 11, Training Loss: 0.8055258727611456\n",
      "Epoch 12, Training Loss: 0.8082499881436054\n",
      "Epoch 13, Training Loss: 0.8046901952055164\n",
      "Epoch 14, Training Loss: 0.8056619434428395\n",
      "Epoch 15, Training Loss: 0.8063063419851145\n",
      "Epoch 16, Training Loss: 0.8056921903352092\n",
      "Epoch 17, Training Loss: 0.80425464667772\n",
      "Epoch 18, Training Loss: 0.8084700450861364\n",
      "Epoch 19, Training Loss: 0.8052379522108494\n",
      "Epoch 20, Training Loss: 0.8056578823498317\n",
      "Epoch 21, Training Loss: 0.8045053147731867\n",
      "Epoch 22, Training Loss: 0.8026081366198403\n",
      "Epoch 23, Training Loss: 0.8059697766053049\n",
      "Epoch 24, Training Loss: 0.8050641670262904\n",
      "Epoch 25, Training Loss: 0.806803610360712\n",
      "Epoch 26, Training Loss: 0.8045272332385071\n",
      "Epoch 27, Training Loss: 0.8040829869141256\n",
      "Epoch 28, Training Loss: 0.8044199990150623\n",
      "Epoch 29, Training Loss: 0.8043080124639927\n",
      "Epoch 30, Training Loss: 0.8059780624576081\n",
      "Epoch 31, Training Loss: 0.805273857959231\n",
      "Epoch 32, Training Loss: 0.8066102899106822\n",
      "Epoch 33, Training Loss: 0.8076486684325942\n",
      "Epoch 34, Training Loss: 0.8025031985196852\n",
      "Epoch 35, Training Loss: 0.804521611937903\n",
      "Epoch 36, Training Loss: 0.8059385393795214\n",
      "Epoch 37, Training Loss: 0.8027394524194245\n",
      "Epoch 38, Training Loss: 0.8058080063726669\n",
      "Epoch 39, Training Loss: 0.8062728999252606\n",
      "Epoch 40, Training Loss: 0.8028887262917999\n",
      "Epoch 41, Training Loss: 0.8048292889630884\n",
      "Epoch 42, Training Loss: 0.8053151073312401\n",
      "Epoch 43, Training Loss: 0.8035567892225165\n",
      "Epoch 44, Training Loss: 0.8053314362253462\n",
      "Epoch 45, Training Loss: 0.806023273073641\n",
      "Epoch 46, Training Loss: 0.8060252577738655\n",
      "Epoch 47, Training Loss: 0.8044568514465389\n",
      "Epoch 48, Training Loss: 0.8046636333142905\n",
      "Epoch 49, Training Loss: 0.8021827355363316\n",
      "Epoch 50, Training Loss: 0.8062667266766828\n",
      "Epoch 51, Training Loss: 0.8063396504947118\n",
      "Epoch 52, Training Loss: 0.8056278871414356\n",
      "Epoch 53, Training Loss: 0.8046367082380711\n",
      "Epoch 54, Training Loss: 0.8032156170758986\n",
      "Epoch 55, Training Loss: 0.8072769761981821\n",
      "Epoch 56, Training Loss: 0.8028681508132389\n",
      "Epoch 57, Training Loss: 0.805674197620019\n",
      "Epoch 58, Training Loss: 0.806263183830376\n",
      "Epoch 59, Training Loss: 0.8049645290338904\n",
      "Epoch 60, Training Loss: 0.8040245661161896\n",
      "Epoch 61, Training Loss: 0.8064700377614875\n",
      "Epoch 62, Training Loss: 0.8116633596276879\n",
      "Epoch 63, Training Loss: 0.8038467654608246\n",
      "Epoch 64, Training Loss: 0.8070834577531742\n",
      "Epoch 65, Training Loss: 0.8066887277409547\n",
      "Epoch 66, Training Loss: 0.8046667946908707\n",
      "Epoch 67, Training Loss: 0.803959162925419\n",
      "Epoch 68, Training Loss: 0.803276140618145\n",
      "Epoch 69, Training Loss: 0.8047827398866639\n",
      "Epoch 70, Training Loss: 0.807110839858091\n",
      "Epoch 71, Training Loss: 0.8054400057720958\n",
      "Epoch 72, Training Loss: 0.8077280610127556\n",
      "Epoch 73, Training Loss: 0.8050277212508639\n",
      "Epoch 74, Training Loss: 0.8032410354094398\n",
      "Epoch 75, Training Loss: 0.8038776646879383\n",
      "Epoch 76, Training Loss: 0.8066703484470683\n",
      "Epoch 77, Training Loss: 0.8035443161663256\n",
      "Epoch 78, Training Loss: 0.8062310302167907\n",
      "Epoch 79, Training Loss: 0.8043202700471519\n",
      "Epoch 80, Training Loss: 0.8066652042525155\n",
      "Epoch 81, Training Loss: 0.8041213239045968\n",
      "Epoch 82, Training Loss: 0.8079611892987014\n",
      "Epoch 83, Training Loss: 0.8034434202918432\n",
      "Epoch 84, Training Loss: 0.8028035564530165\n",
      "Epoch 85, Training Loss: 0.8058609270511713\n",
      "Epoch 86, Training Loss: 0.8043834623537566\n",
      "Epoch 87, Training Loss: 0.8067220395669005\n",
      "Epoch 88, Training Loss: 0.8052024310692809\n",
      "Epoch 89, Training Loss: 0.8040275859653502\n",
      "Epoch 90, Training Loss: 0.8048476891410082\n",
      "Epoch 91, Training Loss: 0.8088875712308669\n",
      "Epoch 92, Training Loss: 0.8076366626230398\n",
      "Epoch 93, Training Loss: 0.8051570370681304\n",
      "Epoch 94, Training Loss: 0.8063259840907907\n",
      "Epoch 95, Training Loss: 0.8060701518130482\n",
      "Epoch 96, Training Loss: 0.8105952886710489\n",
      "Epoch 97, Training Loss: 0.80965506510627\n",
      "Epoch 98, Training Loss: 0.8045364612923529\n",
      "Epoch 99, Training Loss: 0.8052463876573663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:09:03,030] Trial 31 finished with value: 0.6275333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.806234469987396\n",
      "Epoch 1, Training Loss: 0.9783942726079156\n",
      "Epoch 2, Training Loss: 0.932484972336713\n",
      "Epoch 3, Training Loss: 0.9006062370889327\n",
      "Epoch 4, Training Loss: 0.860161728999194\n",
      "Epoch 5, Training Loss: 0.8330367405975566\n",
      "Epoch 6, Training Loss: 0.8214045906066895\n",
      "Epoch 7, Training Loss: 0.8161136043071747\n",
      "Epoch 8, Training Loss: 0.8133104388152852\n",
      "Epoch 9, Training Loss: 0.8116009665937984\n",
      "Epoch 10, Training Loss: 0.8101499146573684\n",
      "Epoch 11, Training Loss: 0.8089490094605614\n",
      "Epoch 12, Training Loss: 0.807691499415566\n",
      "Epoch 13, Training Loss: 0.8070693078461816\n",
      "Epoch 14, Training Loss: 0.8061150398675133\n",
      "Epoch 15, Training Loss: 0.8048366390256321\n",
      "Epoch 16, Training Loss: 0.8045376370233648\n",
      "Epoch 17, Training Loss: 0.803804728493971\n",
      "Epoch 18, Training Loss: 0.8029788915550008\n",
      "Epoch 19, Training Loss: 0.8026814337337719\n",
      "Epoch 20, Training Loss: 0.8018999211928424\n",
      "Epoch 21, Training Loss: 0.801603470619987\n",
      "Epoch 22, Training Loss: 0.8014586245312411\n",
      "Epoch 23, Training Loss: 0.801036610252717\n",
      "Epoch 24, Training Loss: 0.8006172210328719\n",
      "Epoch 25, Training Loss: 0.8002956658251146\n",
      "Epoch 26, Training Loss: 0.8001686920137966\n",
      "Epoch 27, Training Loss: 0.8000550594049342\n",
      "Epoch 28, Training Loss: 0.7996913020049824\n",
      "Epoch 29, Training Loss: 0.7996240436329561\n",
      "Epoch 30, Training Loss: 0.7993322962171892\n",
      "Epoch 31, Training Loss: 0.7992571895262774\n",
      "Epoch 32, Training Loss: 0.7990178244254168\n",
      "Epoch 33, Training Loss: 0.7988966816313127\n",
      "Epoch 34, Training Loss: 0.7985689802029553\n",
      "Epoch 35, Training Loss: 0.798507990135866\n",
      "Epoch 36, Training Loss: 0.7986317925593432\n",
      "Epoch 37, Training Loss: 0.7982511223764981\n",
      "Epoch 38, Training Loss: 0.7980739999518675\n",
      "Epoch 39, Training Loss: 0.797971842359094\n",
      "Epoch 40, Training Loss: 0.7977978606083814\n",
      "Epoch 41, Training Loss: 0.7979641911562751\n",
      "Epoch 42, Training Loss: 0.7978067273252151\n",
      "Epoch 43, Training Loss: 0.7974419757899116\n",
      "Epoch 44, Training Loss: 0.7975625026927274\n",
      "Epoch 45, Training Loss: 0.7972807182283963\n",
      "Epoch 46, Training Loss: 0.7974013862189124\n",
      "Epoch 47, Training Loss: 0.7970870426823111\n",
      "Epoch 48, Training Loss: 0.7969264792694765\n",
      "Epoch 49, Training Loss: 0.7972205688672908\n",
      "Epoch 50, Training Loss: 0.7965343328784494\n",
      "Epoch 51, Training Loss: 0.7965484396149131\n",
      "Epoch 52, Training Loss: 0.7966667747497559\n",
      "Epoch 53, Training Loss: 0.7964466214179993\n",
      "Epoch 54, Training Loss: 0.7962838127332575\n",
      "Epoch 55, Training Loss: 0.7963791049929226\n",
      "Epoch 56, Training Loss: 0.796010889375911\n",
      "Epoch 57, Training Loss: 0.7958841910081751\n",
      "Epoch 58, Training Loss: 0.7954419464924756\n",
      "Epoch 59, Training Loss: 0.7955974880386801\n",
      "Epoch 60, Training Loss: 0.7956017130262711\n",
      "Epoch 61, Training Loss: 0.7954295232716728\n",
      "Epoch 62, Training Loss: 0.7951405797986423\n",
      "Epoch 63, Training Loss: 0.7952414565226611\n",
      "Epoch 64, Training Loss: 0.7949901108882006\n",
      "Epoch 65, Training Loss: 0.7946685619915232\n",
      "Epoch 66, Training Loss: 0.7947063462173237\n",
      "Epoch 67, Training Loss: 0.7946355169660905\n",
      "Epoch 68, Training Loss: 0.7943922798773821\n",
      "Epoch 69, Training Loss: 0.7942592406272888\n",
      "Epoch 70, Training Loss: 0.794072902132483\n",
      "Epoch 71, Training Loss: 0.7940123536306269\n",
      "Epoch 72, Training Loss: 0.7936772075120141\n",
      "Epoch 73, Training Loss: 0.7935568884541007\n",
      "Epoch 74, Training Loss: 0.7934103172666886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:10:21,983] Trial 32 finished with value: 0.6361333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7934123323244207\n",
      "Epoch 1, Training Loss: 1.0907928889855407\n",
      "Epoch 2, Training Loss: 1.0860406722341265\n",
      "Epoch 3, Training Loss: 1.0814119433998166\n",
      "Epoch 4, Training Loss: 1.0745326273423388\n",
      "Epoch 5, Training Loss: 1.0642783455382612\n",
      "Epoch 6, Training Loss: 1.0489578118001608\n",
      "Epoch 7, Training Loss: 1.0281022710011418\n",
      "Epoch 8, Training Loss: 1.0053038818495614\n",
      "Epoch 9, Training Loss: 0.986986237719543\n",
      "Epoch 10, Training Loss: 0.9742977504443405\n",
      "Epoch 11, Training Loss: 0.9663863750328695\n",
      "Epoch 12, Training Loss: 0.9611199395997184\n",
      "Epoch 13, Training Loss: 0.956903912967309\n",
      "Epoch 14, Training Loss: 0.9545417244273021\n",
      "Epoch 15, Training Loss: 0.9520187939916338\n",
      "Epoch 16, Training Loss: 0.9499857868467059\n",
      "Epoch 17, Training Loss: 0.9487399380906184\n",
      "Epoch 18, Training Loss: 0.9469652071931308\n",
      "Epoch 19, Training Loss: 0.9456250758995687\n",
      "Epoch 20, Training Loss: 0.9439616545698697\n",
      "Epoch 21, Training Loss: 0.9419399285674992\n",
      "Epoch 22, Training Loss: 0.9401241496093291\n",
      "Epoch 23, Training Loss: 0.938227941756858\n",
      "Epoch 24, Training Loss: 0.9360823611567791\n",
      "Epoch 25, Training Loss: 0.9342090569044414\n",
      "Epoch 26, Training Loss: 0.9312668776153622\n",
      "Epoch 27, Training Loss: 0.9286918304020301\n",
      "Epoch 28, Training Loss: 0.9256060485553025\n",
      "Epoch 29, Training Loss: 0.9224194325898823\n",
      "Epoch 30, Training Loss: 0.9194568850044021\n",
      "Epoch 31, Training Loss: 0.9157433664888368\n",
      "Epoch 32, Training Loss: 0.9117695194437988\n",
      "Epoch 33, Training Loss: 0.9076397469169215\n",
      "Epoch 34, Training Loss: 0.9031212237544526\n",
      "Epoch 35, Training Loss: 0.8982060920923276\n",
      "Epoch 36, Training Loss: 0.8932217991441712\n",
      "Epoch 37, Training Loss: 0.8881587750033328\n",
      "Epoch 38, Training Loss: 0.8831321664322588\n",
      "Epoch 39, Training Loss: 0.8776164221584348\n",
      "Epoch 40, Training Loss: 0.8723134518565988\n",
      "Epoch 41, Training Loss: 0.8676595055071035\n",
      "Epoch 42, Training Loss: 0.8629954661641802\n",
      "Epoch 43, Training Loss: 0.8582100564375856\n",
      "Epoch 44, Training Loss: 0.854231333822236\n",
      "Epoch 45, Training Loss: 0.8508994147293549\n",
      "Epoch 46, Training Loss: 0.8474815218968499\n",
      "Epoch 47, Training Loss: 0.8440255095187883\n",
      "Epoch 48, Training Loss: 0.8418162828101251\n",
      "Epoch 49, Training Loss: 0.8390129978495433\n",
      "Epoch 50, Training Loss: 0.8364384015699975\n",
      "Epoch 51, Training Loss: 0.8349581904877398\n",
      "Epoch 52, Training Loss: 0.8332997524648681\n",
      "Epoch 53, Training Loss: 0.8314626713444416\n",
      "Epoch 54, Training Loss: 0.8296247225955017\n",
      "Epoch 55, Training Loss: 0.8287460298466504\n",
      "Epoch 56, Training Loss: 0.8272592280144082\n",
      "Epoch 57, Training Loss: 0.8257595306052301\n",
      "Epoch 58, Training Loss: 0.8249319042478289\n",
      "Epoch 59, Training Loss: 0.8240211893741349\n",
      "Epoch 60, Training Loss: 0.8231054055959659\n",
      "Epoch 61, Training Loss: 0.8217127500171948\n",
      "Epoch 62, Training Loss: 0.8211560457272638\n",
      "Epoch 63, Training Loss: 0.8202188604756405\n",
      "Epoch 64, Training Loss: 0.8201371488714577\n",
      "Epoch 65, Training Loss: 0.8190693847218851\n",
      "Epoch 66, Training Loss: 0.8187726299565538\n",
      "Epoch 67, Training Loss: 0.8185870884056378\n",
      "Epoch 68, Training Loss: 0.817461241367168\n",
      "Epoch 69, Training Loss: 0.8170215292980797\n",
      "Epoch 70, Training Loss: 0.8168443769440615\n",
      "Epoch 71, Training Loss: 0.8158969500907381\n",
      "Epoch 72, Training Loss: 0.8148978774708913\n",
      "Epoch 73, Training Loss: 0.8146863900629201\n",
      "Epoch 74, Training Loss: 0.8147744204764976\n",
      "Epoch 75, Training Loss: 0.8147196196075669\n",
      "Epoch 76, Training Loss: 0.8136491632999334\n",
      "Epoch 77, Training Loss: 0.8132656265022163\n",
      "Epoch 78, Training Loss: 0.8130710551613256\n",
      "Epoch 79, Training Loss: 0.8125577206898452\n",
      "Epoch 80, Training Loss: 0.8125772034315238\n",
      "Epoch 81, Training Loss: 0.8118861805227466\n",
      "Epoch 82, Training Loss: 0.8119886997947119\n",
      "Epoch 83, Training Loss: 0.8114038796353161\n",
      "Epoch 84, Training Loss: 0.8114557453564235\n",
      "Epoch 85, Training Loss: 0.811570938278858\n",
      "Epoch 86, Training Loss: 0.8104943082744914\n",
      "Epoch 87, Training Loss: 0.811118053135119\n",
      "Epoch 88, Training Loss: 0.8101460536619774\n",
      "Epoch 89, Training Loss: 0.8105866022576067\n",
      "Epoch 90, Training Loss: 0.8103652281868726\n",
      "Epoch 91, Training Loss: 0.8094509367655991\n",
      "Epoch 92, Training Loss: 0.8103412312672551\n",
      "Epoch 93, Training Loss: 0.8097075272323494\n",
      "Epoch 94, Training Loss: 0.8095354625156947\n",
      "Epoch 95, Training Loss: 0.8089853689186555\n",
      "Epoch 96, Training Loss: 0.8092922935808512\n",
      "Epoch 97, Training Loss: 0.8094245013437773\n",
      "Epoch 98, Training Loss: 0.8099112049081272\n",
      "Epoch 99, Training Loss: 0.8085981764291462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:11:52,695] Trial 33 finished with value: 0.6277333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8090621563725006\n",
      "Epoch 1, Training Loss: 0.8564265543573043\n",
      "Epoch 2, Training Loss: 0.8172931516170502\n",
      "Epoch 3, Training Loss: 0.8135268139137941\n",
      "Epoch 4, Training Loss: 0.8113061371971579\n",
      "Epoch 5, Training Loss: 0.80926606816404\n",
      "Epoch 6, Training Loss: 0.8074372344858506\n",
      "Epoch 7, Training Loss: 0.8056865902508006\n",
      "Epoch 8, Training Loss: 0.8044289398193359\n",
      "Epoch 9, Training Loss: 0.8033104757000419\n",
      "Epoch 10, Training Loss: 0.8026224057814654\n",
      "Epoch 11, Training Loss: 0.8022251528852126\n",
      "Epoch 12, Training Loss: 0.8013097958003773\n",
      "Epoch 13, Training Loss: 0.8010863375663757\n",
      "Epoch 14, Training Loss: 0.8004419253152959\n",
      "Epoch 15, Training Loss: 0.7997571921348572\n",
      "Epoch 16, Training Loss: 0.7992800240656909\n",
      "Epoch 17, Training Loss: 0.7984386801018435\n",
      "Epoch 18, Training Loss: 0.7975253643007839\n",
      "Epoch 19, Training Loss: 0.7968020883728476\n",
      "Epoch 20, Training Loss: 0.7959157062979305\n",
      "Epoch 21, Training Loss: 0.7953586586082683\n",
      "Epoch 22, Training Loss: 0.794208278726129\n",
      "Epoch 23, Training Loss: 0.7929512392773348\n",
      "Epoch 24, Training Loss: 0.7925381145056556\n",
      "Epoch 25, Training Loss: 0.7914205206141752\n",
      "Epoch 26, Training Loss: 0.7899404633746427\n",
      "Epoch 27, Training Loss: 0.7895741669570698\n",
      "Epoch 28, Training Loss: 0.7892294291187735\n",
      "Epoch 29, Training Loss: 0.7883438282854417\n",
      "Epoch 30, Training Loss: 0.7879830955056584\n",
      "Epoch 31, Training Loss: 0.7875764760550331\n",
      "Epoch 32, Training Loss: 0.7870893070978277\n",
      "Epoch 33, Training Loss: 0.7868074144335354\n",
      "Epoch 34, Training Loss: 0.7865271563389722\n",
      "Epoch 35, Training Loss: 0.7863753930260153\n",
      "Epoch 36, Training Loss: 0.7859431896490209\n",
      "Epoch 37, Training Loss: 0.7859905928022721\n",
      "Epoch 38, Training Loss: 0.7848935600589303\n",
      "Epoch 39, Training Loss: 0.78544189453125\n",
      "Epoch 40, Training Loss: 0.7851304963055779\n",
      "Epoch 41, Training Loss: 0.7848943526604596\n",
      "Epoch 42, Training Loss: 0.784848641016904\n",
      "Epoch 43, Training Loss: 0.784802825310651\n",
      "Epoch 44, Training Loss: 0.7846318289812874\n",
      "Epoch 45, Training Loss: 0.784163251063403\n",
      "Epoch 46, Training Loss: 0.784143772055121\n",
      "Epoch 47, Training Loss: 0.7839214585107915\n",
      "Epoch 48, Training Loss: 0.7841753034731921\n",
      "Epoch 49, Training Loss: 0.7840613013856551\n",
      "Epoch 50, Training Loss: 0.7837119077233707\n",
      "Epoch 51, Training Loss: 0.7836211951339946\n",
      "Epoch 52, Training Loss: 0.7832817988535937\n",
      "Epoch 53, Training Loss: 0.7835644907811109\n",
      "Epoch 54, Training Loss: 0.783357001472922\n",
      "Epoch 55, Training Loss: 0.7834350305445054\n",
      "Epoch 56, Training Loss: 0.783161768212038\n",
      "Epoch 57, Training Loss: 0.7829694586641648\n",
      "Epoch 58, Training Loss: 0.7831316260029287\n",
      "Epoch 59, Training Loss: 0.7831154315611896\n",
      "Epoch 60, Training Loss: 0.7830507894123302\n",
      "Epoch 61, Training Loss: 0.7827795990775613\n",
      "Epoch 62, Training Loss: 0.7829632069784053\n",
      "Epoch 63, Training Loss: 0.782788206689498\n",
      "Epoch 64, Training Loss: 0.7826956143098719\n",
      "Epoch 65, Training Loss: 0.782561784211327\n",
      "Epoch 66, Training Loss: 0.782224491484025\n",
      "Epoch 67, Training Loss: 0.7825424637514002\n",
      "Epoch 68, Training Loss: 0.7825813266109017\n",
      "Epoch 69, Training Loss: 0.7823115432262421\n",
      "Epoch 70, Training Loss: 0.7822723756116979\n",
      "Epoch 71, Training Loss: 0.7824554141128764\n",
      "Epoch 72, Training Loss: 0.7820895879408892\n",
      "Epoch 73, Training Loss: 0.7820177301238564\n",
      "Epoch 74, Training Loss: 0.7821631645455079\n",
      "Epoch 75, Training Loss: 0.7821659951350268\n",
      "Epoch 76, Training Loss: 0.7822652210207547\n",
      "Epoch 77, Training Loss: 0.7821739826482885\n",
      "Epoch 78, Training Loss: 0.7819166219935698\n",
      "Epoch 79, Training Loss: 0.7815637498743394\n",
      "Epoch 80, Training Loss: 0.7817063221510718\n",
      "Epoch 81, Training Loss: 0.7816244406559888\n",
      "Epoch 82, Training Loss: 0.7818543716739206\n",
      "Epoch 83, Training Loss: 0.7817811351663926\n",
      "Epoch 84, Training Loss: 0.781451865715139\n",
      "Epoch 85, Training Loss: 0.7817408271396862\n",
      "Epoch 86, Training Loss: 0.7818577250312356\n",
      "Epoch 87, Training Loss: 0.7813215386166292\n",
      "Epoch 88, Training Loss: 0.781359243813683\n",
      "Epoch 89, Training Loss: 0.7814350300676682\n",
      "Epoch 90, Training Loss: 0.7815223686835345\n",
      "Epoch 91, Training Loss: 0.7812756356071023\n",
      "Epoch 92, Training Loss: 0.7814945024602553\n",
      "Epoch 93, Training Loss: 0.7812772282432108\n",
      "Epoch 94, Training Loss: 0.7812971775672015\n",
      "Epoch 95, Training Loss: 0.7813565261223737\n",
      "Epoch 96, Training Loss: 0.781242757544798\n",
      "Epoch 97, Training Loss: 0.7810941138688255\n",
      "Epoch 98, Training Loss: 0.7809908826912151\n",
      "Epoch 99, Training Loss: 0.7811449373469633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:13:56,977] Trial 34 finished with value: 0.6408666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.780837914943695\n",
      "Epoch 1, Training Loss: 1.0711191344081907\n",
      "Epoch 2, Training Loss: 1.0156784017283218\n",
      "Epoch 3, Training Loss: 0.9915975973122102\n",
      "Epoch 4, Training Loss: 0.9780073889216087\n",
      "Epoch 5, Training Loss: 0.9701497086008689\n",
      "Epoch 6, Training Loss: 0.9650486401149205\n",
      "Epoch 7, Training Loss: 0.9622141753820549\n",
      "Epoch 8, Training Loss: 0.9596709717485241\n",
      "Epoch 9, Training Loss: 0.9580605879762119\n",
      "Epoch 10, Training Loss: 0.956659530069595\n",
      "Epoch 11, Training Loss: 0.9558962071748605\n",
      "Epoch 12, Training Loss: 0.9548754323694043\n",
      "Epoch 13, Training Loss: 0.9538510704399051\n",
      "Epoch 14, Training Loss: 0.9531571352392211\n",
      "Epoch 15, Training Loss: 0.9520733348409036\n",
      "Epoch 16, Training Loss: 0.9511051501546587\n",
      "Epoch 17, Training Loss: 0.9503139349751006\n",
      "Epoch 18, Training Loss: 0.9498306360459865\n",
      "Epoch 19, Training Loss: 0.9490287484979271\n",
      "Epoch 20, Training Loss: 0.9488558956554958\n",
      "Epoch 21, Training Loss: 0.9477706849126888\n",
      "Epoch 22, Training Loss: 0.9470966694050266\n",
      "Epoch 23, Training Loss: 0.9465381886726035\n",
      "Epoch 24, Training Loss: 0.9455149499993575\n",
      "Epoch 25, Training Loss: 0.9449936836285698\n",
      "Epoch 26, Training Loss: 0.9444345338900286\n",
      "Epoch 27, Training Loss: 0.9435831773549991\n",
      "Epoch 28, Training Loss: 0.9436482352421696\n",
      "Epoch 29, Training Loss: 0.9427526610238212\n",
      "Epoch 30, Training Loss: 0.9418662714778929\n",
      "Epoch 31, Training Loss: 0.9407151338749362\n",
      "Epoch 32, Training Loss: 0.9403748949667565\n",
      "Epoch 33, Training Loss: 0.9392003780917118\n",
      "Epoch 34, Training Loss: 0.9390604876934138\n",
      "Epoch 35, Training Loss: 0.9381481530971096\n",
      "Epoch 36, Training Loss: 0.9371743686217114\n",
      "Epoch 37, Training Loss: 0.936672708414551\n",
      "Epoch 38, Training Loss: 0.93610597830966\n",
      "Epoch 39, Training Loss: 0.9349964322900414\n",
      "Epoch 40, Training Loss: 0.9344404989615419\n",
      "Epoch 41, Training Loss: 0.9340683036280754\n",
      "Epoch 42, Training Loss: 0.9329377142110266\n",
      "Epoch 43, Training Loss: 0.932664542359517\n",
      "Epoch 44, Training Loss: 0.9322129850100754\n",
      "Epoch 45, Training Loss: 0.9309824496283567\n",
      "Epoch 46, Training Loss: 0.9303571615004002\n",
      "Epoch 47, Training Loss: 0.9296450866792435\n",
      "Epoch 48, Training Loss: 0.9284072261107595\n",
      "Epoch 49, Training Loss: 0.9283196904605493\n",
      "Epoch 50, Training Loss: 0.9273710751892033\n",
      "Epoch 51, Training Loss: 0.9268079409025666\n",
      "Epoch 52, Training Loss: 0.9260337013051025\n",
      "Epoch 53, Training Loss: 0.9251946547873935\n",
      "Epoch 54, Training Loss: 0.9244082024222926\n",
      "Epoch 55, Training Loss: 0.9239105500672993\n",
      "Epoch 56, Training Loss: 0.9230858900493248\n",
      "Epoch 57, Training Loss: 0.9222050372819255\n",
      "Epoch 58, Training Loss: 0.9215083090882552\n",
      "Epoch 59, Training Loss: 0.9209903650714043\n",
      "Epoch 60, Training Loss: 0.9203027601528885\n",
      "Epoch 61, Training Loss: 0.9192819884845189\n",
      "Epoch 62, Training Loss: 0.919003903417659\n",
      "Epoch 63, Training Loss: 0.9175977693464523\n",
      "Epoch 64, Training Loss: 0.9174091050499364\n",
      "Epoch 65, Training Loss: 0.915906156095347\n",
      "Epoch 66, Training Loss: 0.915441736511718\n",
      "Epoch 67, Training Loss: 0.9146600483951712\n",
      "Epoch 68, Training Loss: 0.9143870850254718\n",
      "Epoch 69, Training Loss: 0.9133880511262363\n",
      "Epoch 70, Training Loss: 0.9124793864730606\n",
      "Epoch 71, Training Loss: 0.9119271238047377\n",
      "Epoch 72, Training Loss: 0.9112159940533172\n",
      "Epoch 73, Training Loss: 0.9105442028296621\n",
      "Epoch 74, Training Loss: 0.9095863485694828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:14:58,852] Trial 35 finished with value: 0.5680666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9088446834929904\n",
      "Epoch 1, Training Loss: 0.8627885610917035\n",
      "Epoch 2, Training Loss: 0.8277296027015237\n",
      "Epoch 3, Training Loss: 0.8226800623360803\n",
      "Epoch 4, Training Loss: 0.8187097709319171\n",
      "Epoch 5, Training Loss: 0.8160856838086072\n",
      "Epoch 6, Training Loss: 0.8141319441795349\n",
      "Epoch 7, Training Loss: 0.8128554511771483\n",
      "Epoch 8, Training Loss: 0.8120492195381838\n",
      "Epoch 9, Training Loss: 0.812245486203362\n",
      "Epoch 10, Training Loss: 0.8107372093200683\n",
      "Epoch 11, Training Loss: 0.8108025137817159\n",
      "Epoch 12, Training Loss: 0.8094095754623413\n",
      "Epoch 13, Training Loss: 0.8096420064393212\n",
      "Epoch 14, Training Loss: 0.8088092077479643\n",
      "Epoch 15, Training Loss: 0.8083336631690755\n",
      "Epoch 16, Training Loss: 0.8079350423111635\n",
      "Epoch 17, Training Loss: 0.808161312972798\n",
      "Epoch 18, Training Loss: 0.8078249150865218\n",
      "Epoch 19, Training Loss: 0.8071359625984641\n",
      "Epoch 20, Training Loss: 0.8075683465424706\n",
      "Epoch 21, Training Loss: 0.8065033378320582\n",
      "Epoch 22, Training Loss: 0.8068840015635771\n",
      "Epoch 23, Training Loss: 0.8062857218349682\n",
      "Epoch 24, Training Loss: 0.8060427696564618\n",
      "Epoch 25, Training Loss: 0.8059111861621633\n",
      "Epoch 26, Training Loss: 0.8059400137031779\n",
      "Epoch 27, Training Loss: 0.8063419619728537\n",
      "Epoch 28, Training Loss: 0.8056002223491668\n",
      "Epoch 29, Training Loss: 0.8056378922041725\n",
      "Epoch 30, Training Loss: 0.8058010103422053\n",
      "Epoch 31, Training Loss: 0.805164261004504\n",
      "Epoch 32, Training Loss: 0.8052157301061293\n",
      "Epoch 33, Training Loss: 0.8055602128365461\n",
      "Epoch 34, Training Loss: 0.8048672139644623\n",
      "Epoch 35, Training Loss: 0.8050647918617024\n",
      "Epoch 36, Training Loss: 0.8047054945721346\n",
      "Epoch 37, Training Loss: 0.8048414246474995\n",
      "Epoch 38, Training Loss: 0.8041057146296782\n",
      "Epoch 39, Training Loss: 0.8039496585902046\n",
      "Epoch 40, Training Loss: 0.8039491134531358\n",
      "Epoch 41, Training Loss: 0.8034762456837823\n",
      "Epoch 42, Training Loss: 0.8033701207357294\n",
      "Epoch 43, Training Loss: 0.8036056705082164\n",
      "Epoch 44, Training Loss: 0.8034331938098459\n",
      "Epoch 45, Training Loss: 0.8030078385857975\n",
      "Epoch 46, Training Loss: 0.8032226805125966\n",
      "Epoch 47, Training Loss: 0.8023603500338161\n",
      "Epoch 48, Training Loss: 0.8024403630985933\n",
      "Epoch 49, Training Loss: 0.8021414399147033\n",
      "Epoch 50, Training Loss: 0.8021249425411224\n",
      "Epoch 51, Training Loss: 0.8025276712810292\n",
      "Epoch 52, Training Loss: 0.8012720955820645\n",
      "Epoch 53, Training Loss: 0.8014936863674837\n",
      "Epoch 54, Training Loss: 0.8006539035544676\n",
      "Epoch 55, Training Loss: 0.8016642068414127\n",
      "Epoch 56, Training Loss: 0.8008109270123874\n",
      "Epoch 57, Training Loss: 0.8007551642726449\n",
      "Epoch 58, Training Loss: 0.8008903477472418\n",
      "Epoch 59, Training Loss: 0.800655863495434\n",
      "Epoch 60, Training Loss: 0.8002014421715455\n",
      "Epoch 61, Training Loss: 0.8001722821067361\n",
      "Epoch 62, Training Loss: 0.7998452080698575\n",
      "Epoch 63, Training Loss: 0.8002940747317145\n",
      "Epoch 64, Training Loss: 0.7995252769834855\n",
      "Epoch 65, Training Loss: 0.7998101772981532\n",
      "Epoch 66, Training Loss: 0.7988374185562134\n",
      "Epoch 67, Training Loss: 0.798667372675503\n",
      "Epoch 68, Training Loss: 0.7990049608314739\n",
      "Epoch 69, Training Loss: 0.7987603490492877\n",
      "Epoch 70, Training Loss: 0.798692661523819\n",
      "Epoch 71, Training Loss: 0.7977512682886685\n",
      "Epoch 72, Training Loss: 0.7984038809467765\n",
      "Epoch 73, Training Loss: 0.797713870791828\n",
      "Epoch 74, Training Loss: 0.7981992468413185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:16:20,887] Trial 36 finished with value: 0.635 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7980724984758041\n",
      "Epoch 1, Training Loss: 0.9992820645633497\n",
      "Epoch 2, Training Loss: 0.9500504551973558\n",
      "Epoch 3, Training Loss: 0.9345059295345967\n",
      "Epoch 4, Training Loss: 0.914345695918664\n",
      "Epoch 5, Training Loss: 0.8897105752077318\n",
      "Epoch 6, Training Loss: 0.8664882971828145\n",
      "Epoch 7, Training Loss: 0.8469132520202407\n",
      "Epoch 8, Training Loss: 0.8339325805355732\n",
      "Epoch 9, Training Loss: 0.8253777680540444\n",
      "Epoch 10, Training Loss: 0.8196040944945543\n",
      "Epoch 11, Training Loss: 0.8160522635718037\n",
      "Epoch 12, Training Loss: 0.8140513662108801\n",
      "Epoch 13, Training Loss: 0.8117803894487539\n",
      "Epoch 14, Training Loss: 0.8113285090690269\n",
      "Epoch 15, Training Loss: 0.8119435180398754\n",
      "Epoch 16, Training Loss: 0.8097860027972917\n",
      "Epoch 17, Training Loss: 0.8094614129317435\n",
      "Epoch 18, Training Loss: 0.8088937680524094\n",
      "Epoch 19, Training Loss: 0.8086471077194788\n",
      "Epoch 20, Training Loss: 0.8083527124017701\n",
      "Epoch 21, Training Loss: 0.8080176444878255\n",
      "Epoch 22, Training Loss: 0.8079795408069639\n",
      "Epoch 23, Training Loss: 0.8085102775939426\n",
      "Epoch 24, Training Loss: 0.8074206272462257\n",
      "Epoch 25, Training Loss: 0.8073025871040229\n",
      "Epoch 26, Training Loss: 0.8068438873255164\n",
      "Epoch 27, Training Loss: 0.8063660998093455\n",
      "Epoch 28, Training Loss: 0.806910292725814\n",
      "Epoch 29, Training Loss: 0.806370835196703\n",
      "Epoch 30, Training Loss: 0.806181015735282\n",
      "Epoch 31, Training Loss: 0.8059117959854298\n",
      "Epoch 32, Training Loss: 0.8052578676912121\n",
      "Epoch 33, Training Loss: 0.8057426328945877\n",
      "Epoch 34, Training Loss: 0.8052909339280953\n",
      "Epoch 35, Training Loss: 0.8054282106851277\n",
      "Epoch 36, Training Loss: 0.8048986607028129\n",
      "Epoch 37, Training Loss: 0.8052113853002849\n",
      "Epoch 38, Training Loss: 0.8054051192183244\n",
      "Epoch 39, Training Loss: 0.8045948560076549\n",
      "Epoch 40, Training Loss: 0.8038405929292951\n",
      "Epoch 41, Training Loss: 0.8048897578303975\n",
      "Epoch 42, Training Loss: 0.804130692858445\n",
      "Epoch 43, Training Loss: 0.8037103604553337\n",
      "Epoch 44, Training Loss: 0.8036796276730702\n",
      "Epoch 45, Training Loss: 0.8035732819621725\n",
      "Epoch 46, Training Loss: 0.8033692340205486\n",
      "Epoch 47, Training Loss: 0.8032736535359146\n",
      "Epoch 48, Training Loss: 0.803142517760284\n",
      "Epoch 49, Training Loss: 0.8032799350587945\n",
      "Epoch 50, Training Loss: 0.8031950228196337\n",
      "Epoch 51, Training Loss: 0.8031680457574084\n",
      "Epoch 52, Training Loss: 0.8023914319231994\n",
      "Epoch 53, Training Loss: 0.8031179453197278\n",
      "Epoch 54, Training Loss: 0.8026937130698584\n",
      "Epoch 55, Training Loss: 0.8023312781090127\n",
      "Epoch 56, Training Loss: 0.8026641000482373\n",
      "Epoch 57, Training Loss: 0.8020969614050442\n",
      "Epoch 58, Training Loss: 0.8017297568177818\n",
      "Epoch 59, Training Loss: 0.8022513181643378\n",
      "Epoch 60, Training Loss: 0.8016587902728777\n",
      "Epoch 61, Training Loss: 0.8012598308405482\n",
      "Epoch 62, Training Loss: 0.801903352163788\n",
      "Epoch 63, Training Loss: 0.8012221949441093\n",
      "Epoch 64, Training Loss: 0.8010864404807413\n",
      "Epoch 65, Training Loss: 0.8014527883744778\n",
      "Epoch 66, Training Loss: 0.8011496151300301\n",
      "Epoch 67, Training Loss: 0.8008176739950825\n",
      "Epoch 68, Training Loss: 0.8006428735596793\n",
      "Epoch 69, Training Loss: 0.8009345583449629\n",
      "Epoch 70, Training Loss: 0.8005975241947891\n",
      "Epoch 71, Training Loss: 0.8003781332109208\n",
      "Epoch 72, Training Loss: 0.8000620287163813\n",
      "Epoch 73, Training Loss: 0.8004625446814343\n",
      "Epoch 74, Training Loss: 0.8001514819331635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:17:35,632] Trial 37 finished with value: 0.6356 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8002745650764694\n",
      "Epoch 1, Training Loss: 0.882546184834312\n",
      "Epoch 2, Training Loss: 0.8156392615682938\n",
      "Epoch 3, Training Loss: 0.8110520864935482\n",
      "Epoch 4, Training Loss: 0.8074390365796931\n",
      "Epoch 5, Training Loss: 0.8060575150742251\n",
      "Epoch 6, Training Loss: 0.8032349273036508\n",
      "Epoch 7, Training Loss: 0.8008366072879118\n",
      "Epoch 8, Training Loss: 0.7965394568443298\n",
      "Epoch 9, Training Loss: 0.7956049032772289\n",
      "Epoch 10, Training Loss: 0.7937004288505105\n",
      "Epoch 11, Training Loss: 0.7923497758893405\n",
      "Epoch 12, Training Loss: 0.7924302954533521\n",
      "Epoch 13, Training Loss: 0.7913427146042095\n",
      "Epoch 14, Training Loss: 0.7907260546263526\n",
      "Epoch 15, Training Loss: 0.7903891099901761\n",
      "Epoch 16, Training Loss: 0.7896106855308308\n",
      "Epoch 17, Training Loss: 0.7903213780066546\n",
      "Epoch 18, Training Loss: 0.7891961939194623\n",
      "Epoch 19, Training Loss: 0.7895115322926465\n",
      "Epoch 20, Training Loss: 0.7881947630293229\n",
      "Epoch 21, Training Loss: 0.7881957064656651\n",
      "Epoch 22, Training Loss: 0.7881012235669529\n",
      "Epoch 23, Training Loss: 0.7883078858431648\n",
      "Epoch 24, Training Loss: 0.7879978609786314\n",
      "Epoch 25, Training Loss: 0.7878533666274127\n",
      "Epoch 26, Training Loss: 0.7888270014173844\n",
      "Epoch 27, Training Loss: 0.7876900946392732\n",
      "Epoch 28, Training Loss: 0.7870375548390781\n",
      "Epoch 29, Training Loss: 0.7861277464558096\n",
      "Epoch 30, Training Loss: 0.7868073373682358\n",
      "Epoch 31, Training Loss: 0.7859164478498347\n",
      "Epoch 32, Training Loss: 0.7868672030112323\n",
      "Epoch 33, Training Loss: 0.7860588654349832\n",
      "Epoch 34, Training Loss: 0.7852975235265844\n",
      "Epoch 35, Training Loss: 0.7856766840289621\n",
      "Epoch 36, Training Loss: 0.7859232956521651\n",
      "Epoch 37, Training Loss: 0.785578300672419\n",
      "Epoch 38, Training Loss: 0.7851183903918547\n",
      "Epoch 39, Training Loss: 0.7852714947391959\n",
      "Epoch 40, Training Loss: 0.7845638916071723\n",
      "Epoch 41, Training Loss: 0.7838969234158011\n",
      "Epoch 42, Training Loss: 0.785130680589115\n",
      "Epoch 43, Training Loss: 0.7847789854863111\n",
      "Epoch 44, Training Loss: 0.7842384987017688\n",
      "Epoch 45, Training Loss: 0.7845293297487147\n",
      "Epoch 46, Training Loss: 0.7830455812285928\n",
      "Epoch 47, Training Loss: 0.7835930522049175\n",
      "Epoch 48, Training Loss: 0.7833863592147827\n",
      "Epoch 49, Training Loss: 0.783924750440261\n",
      "Epoch 50, Training Loss: 0.7835890241931467\n",
      "Epoch 51, Training Loss: 0.7834592052768259\n",
      "Epoch 52, Training Loss: 0.783498731430839\n",
      "Epoch 53, Training Loss: 0.783103788880741\n",
      "Epoch 54, Training Loss: 0.7832676752174602\n",
      "Epoch 55, Training Loss: 0.7828736452495351\n",
      "Epoch 56, Training Loss: 0.782454586169299\n",
      "Epoch 57, Training Loss: 0.7831977927684783\n",
      "Epoch 58, Training Loss: 0.7822959679715774\n",
      "Epoch 59, Training Loss: 0.7824747467742247\n",
      "Epoch 60, Training Loss: 0.782325814261156\n",
      "Epoch 61, Training Loss: 0.7826532901034635\n",
      "Epoch 62, Training Loss: 0.781578440876568\n",
      "Epoch 63, Training Loss: 0.78202281517141\n",
      "Epoch 64, Training Loss: 0.7821992966708015\n",
      "Epoch 65, Training Loss: 0.7819677900566774\n",
      "Epoch 66, Training Loss: 0.7819638482262107\n",
      "Epoch 67, Training Loss: 0.7807239047218771\n",
      "Epoch 68, Training Loss: 0.7814852058887481\n",
      "Epoch 69, Training Loss: 0.7819757425785064\n",
      "Epoch 70, Training Loss: 0.7807548924754648\n",
      "Epoch 71, Training Loss: 0.7809254284466014\n",
      "Epoch 72, Training Loss: 0.780905944108963\n",
      "Epoch 73, Training Loss: 0.780601821717094\n",
      "Epoch 74, Training Loss: 0.7802935291739072\n",
      "Epoch 75, Training Loss: 0.7806163658815272\n",
      "Epoch 76, Training Loss: 0.7806623141905841\n",
      "Epoch 77, Training Loss: 0.7801984432865592\n",
      "Epoch 78, Training Loss: 0.7806799134787391\n",
      "Epoch 79, Training Loss: 0.7801850459154914\n",
      "Epoch 80, Training Loss: 0.7803491598017076\n",
      "Epoch 81, Training Loss: 0.7803716673570521\n",
      "Epoch 82, Training Loss: 0.7797625542388242\n",
      "Epoch 83, Training Loss: 0.7798280113584855\n",
      "Epoch 84, Training Loss: 0.780204322828966\n",
      "Epoch 85, Training Loss: 0.7798803356114555\n",
      "Epoch 86, Training Loss: 0.7794945979819579\n",
      "Epoch 87, Training Loss: 0.7795885033467237\n",
      "Epoch 88, Training Loss: 0.7790678557928871\n",
      "Epoch 89, Training Loss: 0.7796096771604875\n",
      "Epoch 90, Training Loss: 0.779632207996705\n",
      "Epoch 91, Training Loss: 0.7791231651165906\n",
      "Epoch 92, Training Loss: 0.7794574672334335\n",
      "Epoch 93, Training Loss: 0.7788064999440137\n",
      "Epoch 94, Training Loss: 0.7787469633887796\n",
      "Epoch 95, Training Loss: 0.7791182397393619\n",
      "Epoch 96, Training Loss: 0.7784951078190523\n",
      "Epoch 97, Training Loss: 0.7784104746229509\n",
      "Epoch 98, Training Loss: 0.7784441696195041\n",
      "Epoch 99, Training Loss: 0.7779940591840183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:19:49,884] Trial 38 finished with value: 0.6401333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7788233127313502\n",
      "Epoch 1, Training Loss: 0.8787569607706631\n",
      "Epoch 2, Training Loss: 0.8349581838355344\n",
      "Epoch 3, Training Loss: 0.8307100807919222\n",
      "Epoch 4, Training Loss: 0.8246199905171114\n",
      "Epoch 5, Training Loss: 0.8221474528312683\n",
      "Epoch 6, Training Loss: 0.8227541599554175\n",
      "Epoch 7, Training Loss: 0.8196174486945658\n",
      "Epoch 8, Training Loss: 0.8202081833166235\n",
      "Epoch 9, Training Loss: 0.8191111660003663\n",
      "Epoch 10, Training Loss: 0.8186736088640549\n",
      "Epoch 11, Training Loss: 0.8170725342105417\n",
      "Epoch 12, Training Loss: 0.8179881794312421\n",
      "Epoch 13, Training Loss: 0.8165709999028374\n",
      "Epoch 14, Training Loss: 0.8155914807319641\n",
      "Epoch 15, Training Loss: 0.8162286970194649\n",
      "Epoch 16, Training Loss: 0.8154622153674855\n",
      "Epoch 17, Training Loss: 0.8154529476165772\n",
      "Epoch 18, Training Loss: 0.8172460610726301\n",
      "Epoch 19, Training Loss: 0.8149069705430199\n",
      "Epoch 20, Training Loss: 0.8159816613618065\n",
      "Epoch 21, Training Loss: 0.8138827971149893\n",
      "Epoch 22, Training Loss: 0.8156944171120138\n",
      "Epoch 23, Training Loss: 0.8151189608433668\n",
      "Epoch 24, Training Loss: 0.816218085008509\n",
      "Epoch 25, Training Loss: 0.8143563720759224\n",
      "Epoch 26, Training Loss: 0.8149799063626457\n",
      "Epoch 27, Training Loss: 0.8156366939404431\n",
      "Epoch 28, Training Loss: 0.8152384499942555\n",
      "Epoch 29, Training Loss: 0.8154399766641505\n",
      "Epoch 30, Training Loss: 0.8152224476898418\n",
      "Epoch 31, Training Loss: 0.8148156508277444\n",
      "Epoch 32, Training Loss: 0.8138131367459017\n",
      "Epoch 33, Training Loss: 0.8142183662863338\n",
      "Epoch 34, Training Loss: 0.8135742563359878\n",
      "Epoch 35, Training Loss: 0.8133048470581279\n",
      "Epoch 36, Training Loss: 0.8133337529967813\n",
      "Epoch 37, Training Loss: 0.8131994422744302\n",
      "Epoch 38, Training Loss: 0.8126497617188622\n",
      "Epoch 39, Training Loss: 0.8134767735004425\n",
      "Epoch 40, Training Loss: 0.8137220750135534\n",
      "Epoch 41, Training Loss: 0.8136066386278937\n",
      "Epoch 42, Training Loss: 0.8123216191460104\n",
      "Epoch 43, Training Loss: 0.8131556089485393\n",
      "Epoch 44, Training Loss: 0.8126799254557666\n",
      "Epoch 45, Training Loss: 0.8111847943418167\n",
      "Epoch 46, Training Loss: 0.8111501049294191\n",
      "Epoch 47, Training Loss: 0.8111464671527638\n",
      "Epoch 48, Training Loss: 0.8109070086479186\n",
      "Epoch 49, Training Loss: 0.8097718833474552\n",
      "Epoch 50, Training Loss: 0.8104023235685686\n",
      "Epoch 51, Training Loss: 0.810427331573823\n",
      "Epoch 52, Training Loss: 0.8092213979889364\n",
      "Epoch 53, Training Loss: 0.8102420255717109\n",
      "Epoch 54, Training Loss: 0.8096471891683691\n",
      "Epoch 55, Training Loss: 0.8093841628467335\n",
      "Epoch 56, Training Loss: 0.8097480290777543\n",
      "Epoch 57, Training Loss: 0.8095131403558394\n",
      "Epoch 58, Training Loss: 0.8090012741790098\n",
      "Epoch 59, Training Loss: 0.80917569083326\n",
      "Epoch 60, Training Loss: 0.8094771068236407\n",
      "Epoch 61, Training Loss: 0.8079917604783002\n",
      "Epoch 62, Training Loss: 0.8076409832870259\n",
      "Epoch 63, Training Loss: 0.8081651686920839\n",
      "Epoch 64, Training Loss: 0.8074350976943969\n",
      "Epoch 65, Training Loss: 0.8068464926411124\n",
      "Epoch 66, Training Loss: 0.8078769804449643\n",
      "Epoch 67, Training Loss: 0.8081715317333446\n",
      "Epoch 68, Training Loss: 0.8086614932733424\n",
      "Epoch 69, Training Loss: 0.8068236300524543\n",
      "Epoch 70, Training Loss: 0.8072674615242902\n",
      "Epoch 71, Training Loss: 0.8081866753101349\n",
      "Epoch 72, Training Loss: 0.8070767173346352\n",
      "Epoch 73, Training Loss: 0.8079240359278286\n",
      "Epoch 74, Training Loss: 0.8063680349378025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:21:11,467] Trial 39 finished with value: 0.6341333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8075990962280947\n",
      "Epoch 1, Training Loss: 0.8487908236962513\n",
      "Epoch 2, Training Loss: 0.8116344307598314\n",
      "Epoch 3, Training Loss: 0.808560273701087\n",
      "Epoch 4, Training Loss: 0.8068393207134161\n",
      "Epoch 5, Training Loss: 0.8090183745649524\n",
      "Epoch 6, Training Loss: 0.8024923905394131\n",
      "Epoch 7, Training Loss: 0.8021055356900495\n",
      "Epoch 8, Training Loss: 0.801893947716046\n",
      "Epoch 9, Training Loss: 0.7990819434474286\n",
      "Epoch 10, Training Loss: 0.8005439766367576\n",
      "Epoch 11, Training Loss: 0.7998126323958089\n",
      "Epoch 12, Training Loss: 0.799330572974413\n",
      "Epoch 13, Training Loss: 0.7997996254074843\n",
      "Epoch 14, Training Loss: 0.7986533139881334\n",
      "Epoch 15, Training Loss: 0.7978053439828686\n",
      "Epoch 16, Training Loss: 0.7966159653842897\n",
      "Epoch 17, Training Loss: 0.7973665396073707\n",
      "Epoch 18, Training Loss: 0.7958295117195388\n",
      "Epoch 19, Training Loss: 0.7946364124914758\n",
      "Epoch 20, Training Loss: 0.7963264704646921\n",
      "Epoch 21, Training Loss: 0.7959046661405635\n",
      "Epoch 22, Training Loss: 0.7955297641736224\n",
      "Epoch 23, Training Loss: 0.7950724152694071\n",
      "Epoch 24, Training Loss: 0.7952409066651996\n",
      "Epoch 25, Training Loss: 0.7945170425830927\n",
      "Epoch 26, Training Loss: 0.793319434958293\n",
      "Epoch 27, Training Loss: 0.7951888970862654\n",
      "Epoch 28, Training Loss: 0.7944050787983085\n",
      "Epoch 29, Training Loss: 0.7939000289242967\n",
      "Epoch 30, Training Loss: 0.7956969511240048\n",
      "Epoch 31, Training Loss: 0.7946214481403954\n",
      "Epoch 32, Training Loss: 0.793573568458844\n",
      "Epoch 33, Training Loss: 0.7940241727613865\n",
      "Epoch 34, Training Loss: 0.7947548583037871\n",
      "Epoch 35, Training Loss: 0.7943247900869613\n",
      "Epoch 36, Training Loss: 0.7936618806724262\n",
      "Epoch 37, Training Loss: 0.7930270135850834\n",
      "Epoch 38, Training Loss: 0.7937078431136626\n",
      "Epoch 39, Training Loss: 0.7942371938461648\n",
      "Epoch 40, Training Loss: 0.7931667837881504\n",
      "Epoch 41, Training Loss: 0.7939239817454402\n",
      "Epoch 42, Training Loss: 0.7926763449396406\n",
      "Epoch 43, Training Loss: 0.793636225040694\n",
      "Epoch 44, Training Loss: 0.7931977604564867\n",
      "Epoch 45, Training Loss: 0.7926407632074858\n",
      "Epoch 46, Training Loss: 0.7936471419226855\n",
      "Epoch 47, Training Loss: 0.7934944782938276\n",
      "Epoch 48, Training Loss: 0.792235723473972\n",
      "Epoch 49, Training Loss: 0.7923722195446044\n",
      "Epoch 50, Training Loss: 0.7924302811013129\n",
      "Epoch 51, Training Loss: 0.7927772503150137\n",
      "Epoch 52, Training Loss: 0.7916525690179123\n",
      "Epoch 53, Training Loss: 0.7911593965121678\n",
      "Epoch 54, Training Loss: 0.7930775744574411\n",
      "Epoch 55, Training Loss: 0.7925942420959473\n",
      "Epoch 56, Training Loss: 0.7916884682232276\n",
      "Epoch 57, Training Loss: 0.7927261748708281\n",
      "Epoch 58, Training Loss: 0.7922870064140263\n",
      "Epoch 59, Training Loss: 0.7922803272878317\n",
      "Epoch 60, Training Loss: 0.7918766657212624\n",
      "Epoch 61, Training Loss: 0.7915414932078885\n",
      "Epoch 62, Training Loss: 0.7918251262571578\n",
      "Epoch 63, Training Loss: 0.7918464484071372\n",
      "Epoch 64, Training Loss: 0.7916513380251433\n",
      "Epoch 65, Training Loss: 0.7919622487591621\n",
      "Epoch 66, Training Loss: 0.7922972557239962\n",
      "Epoch 67, Training Loss: 0.7915091098699355\n",
      "Epoch 68, Training Loss: 0.790864196038784\n",
      "Epoch 69, Training Loss: 0.7920287689768282\n",
      "Epoch 70, Training Loss: 0.7913198228169205\n",
      "Epoch 71, Training Loss: 0.7924736831421243\n",
      "Epoch 72, Training Loss: 0.7911221103560656\n",
      "Epoch 73, Training Loss: 0.7916972641658065\n",
      "Epoch 74, Training Loss: 0.7912357454013107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:22:25,779] Trial 40 finished with value: 0.6341333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7920906950656633\n",
      "Epoch 1, Training Loss: 0.988954858851612\n",
      "Epoch 2, Training Loss: 0.9299346127904448\n",
      "Epoch 3, Training Loss: 0.8885933876933908\n",
      "Epoch 4, Training Loss: 0.842796818564709\n",
      "Epoch 5, Training Loss: 0.8204530936434753\n",
      "Epoch 6, Training Loss: 0.8147290427881972\n",
      "Epoch 7, Training Loss: 0.8130697312211632\n",
      "Epoch 8, Training Loss: 0.8114745780041344\n",
      "Epoch 9, Training Loss: 0.809469609511526\n",
      "Epoch 10, Training Loss: 0.8094064190871734\n",
      "Epoch 11, Training Loss: 0.8085689222902284\n",
      "Epoch 12, Training Loss: 0.8073490434123162\n",
      "Epoch 13, Training Loss: 0.806342154203501\n",
      "Epoch 14, Training Loss: 0.807185735738367\n",
      "Epoch 15, Training Loss: 0.8057241036479634\n",
      "Epoch 16, Training Loss: 0.8051542908625495\n",
      "Epoch 17, Training Loss: 0.8048440818499801\n",
      "Epoch 18, Training Loss: 0.8044285006989214\n",
      "Epoch 19, Training Loss: 0.8036846161784982\n",
      "Epoch 20, Training Loss: 0.8037781455463037\n",
      "Epoch 21, Training Loss: 0.8031092426830665\n",
      "Epoch 22, Training Loss: 0.8027190680790665\n",
      "Epoch 23, Training Loss: 0.8028766407106156\n",
      "Epoch 24, Training Loss: 0.8032951888285185\n",
      "Epoch 25, Training Loss: 0.802770723973898\n",
      "Epoch 26, Training Loss: 0.8018430187289877\n",
      "Epoch 27, Training Loss: 0.8017022459578693\n",
      "Epoch 28, Training Loss: 0.8015583574771881\n",
      "Epoch 29, Training Loss: 0.8021220871380397\n",
      "Epoch 30, Training Loss: 0.8015639574904191\n",
      "Epoch 31, Training Loss: 0.8011531478480289\n",
      "Epoch 32, Training Loss: 0.801026697714526\n",
      "Epoch 33, Training Loss: 0.8013473839688122\n",
      "Epoch 34, Training Loss: 0.8004043008151808\n",
      "Epoch 35, Training Loss: 0.8006755787627141\n",
      "Epoch 36, Training Loss: 0.8009188116941237\n",
      "Epoch 37, Training Loss: 0.8004793990823559\n",
      "Epoch 38, Training Loss: 0.7998343817273477\n",
      "Epoch 39, Training Loss: 0.7999040616634197\n",
      "Epoch 40, Training Loss: 0.8006354289843624\n",
      "Epoch 41, Training Loss: 0.7996620016886775\n",
      "Epoch 42, Training Loss: 0.8002639337589866\n",
      "Epoch 43, Training Loss: 0.8002133781748607\n",
      "Epoch 44, Training Loss: 0.7995264265770302\n",
      "Epoch 45, Training Loss: 0.8004973472509169\n",
      "Epoch 46, Training Loss: 0.7994766357249783\n",
      "Epoch 47, Training Loss: 0.799487560853026\n",
      "Epoch 48, Training Loss: 0.7989141837994855\n",
      "Epoch 49, Training Loss: 0.7998165137785718\n",
      "Epoch 50, Training Loss: 0.7985642141865609\n",
      "Epoch 51, Training Loss: 0.7994547548150658\n",
      "Epoch 52, Training Loss: 0.7989479887754397\n",
      "Epoch 53, Training Loss: 0.7990944407040015\n",
      "Epoch 54, Training Loss: 0.7980866115344198\n",
      "Epoch 55, Training Loss: 0.7978542021342686\n",
      "Epoch 56, Training Loss: 0.7982130106230427\n",
      "Epoch 57, Training Loss: 0.7989033757295824\n",
      "Epoch 58, Training Loss: 0.798612580084263\n",
      "Epoch 59, Training Loss: 0.798267782720408\n",
      "Epoch 60, Training Loss: 0.7976816413994122\n",
      "Epoch 61, Training Loss: 0.7982246561157972\n",
      "Epoch 62, Training Loss: 0.7977019181825165\n",
      "Epoch 63, Training Loss: 0.7970284690534262\n",
      "Epoch 64, Training Loss: 0.7973375782930762\n",
      "Epoch 65, Training Loss: 0.7970366066559813\n",
      "Epoch 66, Training Loss: 0.7973166533878872\n",
      "Epoch 67, Training Loss: 0.7973729164080512\n",
      "Epoch 68, Training Loss: 0.7964320248230956\n",
      "Epoch 69, Training Loss: 0.7968975053694015\n",
      "Epoch 70, Training Loss: 0.7965712178022342\n",
      "Epoch 71, Training Loss: 0.796372480903353\n",
      "Epoch 72, Training Loss: 0.7960831466922187\n",
      "Epoch 73, Training Loss: 0.7966900479524656\n",
      "Epoch 74, Training Loss: 0.7966543195839215\n",
      "Epoch 75, Training Loss: 0.7969534013504372\n",
      "Epoch 76, Training Loss: 0.796046615453591\n",
      "Epoch 77, Training Loss: 0.7958688356822594\n",
      "Epoch 78, Training Loss: 0.7966716273386676\n",
      "Epoch 79, Training Loss: 0.795669588737918\n",
      "Epoch 80, Training Loss: 0.7958208638026302\n",
      "Epoch 81, Training Loss: 0.7960856416171654\n",
      "Epoch 82, Training Loss: 0.7952513579139135\n",
      "Epoch 83, Training Loss: 0.7955557240579362\n",
      "Epoch 84, Training Loss: 0.7959504132880304\n",
      "Epoch 85, Training Loss: 0.7956033920883236\n",
      "Epoch 86, Training Loss: 0.7958555822981928\n",
      "Epoch 87, Training Loss: 0.795345543918753\n",
      "Epoch 88, Training Loss: 0.7948362190024297\n",
      "Epoch 89, Training Loss: 0.7964613665315442\n",
      "Epoch 90, Training Loss: 0.7950339249202183\n",
      "Epoch 91, Training Loss: 0.7944228208154664\n",
      "Epoch 92, Training Loss: 0.7943599622052415\n",
      "Epoch 93, Training Loss: 0.7947896651755598\n",
      "Epoch 94, Training Loss: 0.7944227554744347\n",
      "Epoch 95, Training Loss: 0.794556406745337\n",
      "Epoch 96, Training Loss: 0.7946775551129105\n",
      "Epoch 97, Training Loss: 0.7943440404153408\n",
      "Epoch 98, Training Loss: 0.7946078244904826\n",
      "Epoch 99, Training Loss: 0.7939502165729838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:24:21,525] Trial 41 finished with value: 0.6358 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7945542802487997\n",
      "Epoch 1, Training Loss: 0.9745287992900475\n",
      "Epoch 2, Training Loss: 0.9429239992808579\n",
      "Epoch 3, Training Loss: 0.9262664656441911\n",
      "Epoch 4, Training Loss: 0.9075125065961278\n",
      "Epoch 5, Training Loss: 0.8870584852713391\n",
      "Epoch 6, Training Loss: 0.8659426664051256\n",
      "Epoch 7, Training Loss: 0.8481398198837624\n",
      "Epoch 8, Training Loss: 0.8354042570393785\n",
      "Epoch 9, Training Loss: 0.8273840974148055\n",
      "Epoch 10, Training Loss: 0.8210297587222622\n",
      "Epoch 11, Training Loss: 0.8174151885778385\n",
      "Epoch 12, Training Loss: 0.8144347994847405\n",
      "Epoch 13, Training Loss: 0.8125844970233458\n",
      "Epoch 14, Training Loss: 0.8116672539173212\n",
      "Epoch 15, Training Loss: 0.8110599610142242\n",
      "Epoch 16, Training Loss: 0.8102060567167468\n",
      "Epoch 17, Training Loss: 0.8099763993930099\n",
      "Epoch 18, Training Loss: 0.809496863981835\n",
      "Epoch 19, Training Loss: 0.8086365372614753\n",
      "Epoch 20, Training Loss: 0.8084113222315795\n",
      "Epoch 21, Training Loss: 0.8081541723774788\n",
      "Epoch 22, Training Loss: 0.8082838264623082\n",
      "Epoch 23, Training Loss: 0.8086390812594192\n",
      "Epoch 24, Training Loss: 0.8077304892970207\n",
      "Epoch 25, Training Loss: 0.8076156571395415\n",
      "Epoch 26, Training Loss: 0.8077700827354776\n",
      "Epoch 27, Training Loss: 0.8075985220141877\n",
      "Epoch 28, Training Loss: 0.8076952582015131\n",
      "Epoch 29, Training Loss: 0.8067989126183933\n",
      "Epoch 30, Training Loss: 0.8072519316709131\n",
      "Epoch 31, Training Loss: 0.8068210413581447\n",
      "Epoch 32, Training Loss: 0.8065186765857209\n",
      "Epoch 33, Training Loss: 0.8066581697392284\n",
      "Epoch 34, Training Loss: 0.8065559171196214\n",
      "Epoch 35, Training Loss: 0.8061972392232795\n",
      "Epoch 36, Training Loss: 0.8058107705940878\n",
      "Epoch 37, Training Loss: 0.8059408062382748\n",
      "Epoch 38, Training Loss: 0.8055067172624115\n",
      "Epoch 39, Training Loss: 0.8057367641226689\n",
      "Epoch 40, Training Loss: 0.8055343488105258\n",
      "Epoch 41, Training Loss: 0.8058734603394243\n",
      "Epoch 42, Training Loss: 0.8056337220328195\n",
      "Epoch 43, Training Loss: 0.8046341961487792\n",
      "Epoch 44, Training Loss: 0.8053612847973529\n",
      "Epoch 45, Training Loss: 0.8046006050324978\n",
      "Epoch 46, Training Loss: 0.8042507967554537\n",
      "Epoch 47, Training Loss: 0.8045267078213225\n",
      "Epoch 48, Training Loss: 0.804509026663644\n",
      "Epoch 49, Training Loss: 0.8039649126224948\n",
      "Epoch 50, Training Loss: 0.8036901370923322\n",
      "Epoch 51, Training Loss: 0.8038696122348756\n",
      "Epoch 52, Training Loss: 0.8041465920613224\n",
      "Epoch 53, Training Loss: 0.8032698358808245\n",
      "Epoch 54, Training Loss: 0.803796670849162\n",
      "Epoch 55, Training Loss: 0.8029925934354165\n",
      "Epoch 56, Training Loss: 0.8031888079822511\n",
      "Epoch 57, Training Loss: 0.8041250370498887\n",
      "Epoch 58, Training Loss: 0.8032806839261736\n",
      "Epoch 59, Training Loss: 0.8028185418673924\n",
      "Epoch 60, Training Loss: 0.8030534723647555\n",
      "Epoch 61, Training Loss: 0.8029991311238225\n",
      "Epoch 62, Training Loss: 0.8027731034092437\n",
      "Epoch 63, Training Loss: 0.8022421699717529\n",
      "Epoch 64, Training Loss: 0.8023941037350131\n",
      "Epoch 65, Training Loss: 0.8025338078800001\n",
      "Epoch 66, Training Loss: 0.8025643239343973\n",
      "Epoch 67, Training Loss: 0.8019865284288736\n",
      "Epoch 68, Training Loss: 0.8024116831614558\n",
      "Epoch 69, Training Loss: 0.8021065411711098\n",
      "Epoch 70, Training Loss: 0.8014201255669271\n",
      "Epoch 71, Training Loss: 0.8011042242659662\n",
      "Epoch 72, Training Loss: 0.8012633804091833\n",
      "Epoch 73, Training Loss: 0.8014326809940482\n",
      "Epoch 74, Training Loss: 0.801075475735772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:25:32,351] Trial 42 finished with value: 0.6289333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8010728684583105\n",
      "Epoch 1, Training Loss: 0.8396884181445703\n",
      "Epoch 2, Training Loss: 0.8059579136676358\n",
      "Epoch 3, Training Loss: 0.8008868244357575\n",
      "Epoch 4, Training Loss: 0.7989328754575629\n",
      "Epoch 5, Training Loss: 0.8006821520346448\n",
      "Epoch 6, Training Loss: 0.7978590118257622\n",
      "Epoch 7, Training Loss: 0.798417635967857\n",
      "Epoch 8, Training Loss: 0.7956038476381087\n",
      "Epoch 9, Training Loss: 0.79500287430627\n",
      "Epoch 10, Training Loss: 0.7937146138427849\n",
      "Epoch 11, Training Loss: 0.792502478549355\n",
      "Epoch 12, Training Loss: 0.7923475620441867\n",
      "Epoch 13, Training Loss: 0.7931784280260703\n",
      "Epoch 14, Training Loss: 0.79253323633868\n",
      "Epoch 15, Training Loss: 0.792250292731407\n",
      "Epoch 16, Training Loss: 0.7924194452457858\n",
      "Epoch 17, Training Loss: 0.7917543864787969\n",
      "Epoch 18, Training Loss: 0.7920892375752442\n",
      "Epoch 19, Training Loss: 0.7912707033013939\n",
      "Epoch 20, Training Loss: 0.791911416484001\n",
      "Epoch 21, Training Loss: 0.7913930187548014\n",
      "Epoch 22, Training Loss: 0.7920941100981003\n",
      "Epoch 23, Training Loss: 0.7911967555383095\n",
      "Epoch 24, Training Loss: 0.7912541354509225\n",
      "Epoch 25, Training Loss: 0.7910576006523649\n",
      "Epoch 26, Training Loss: 0.7908702273117868\n",
      "Epoch 27, Training Loss: 0.7898892746832138\n",
      "Epoch 28, Training Loss: 0.7893512673395917\n",
      "Epoch 29, Training Loss: 0.7888387017680291\n",
      "Epoch 30, Training Loss: 0.7907872555847455\n",
      "Epoch 31, Training Loss: 0.7903285303510221\n",
      "Epoch 32, Training Loss: 0.789949311349625\n",
      "Epoch 33, Training Loss: 0.7898954041918418\n",
      "Epoch 34, Training Loss: 0.7898004362457677\n",
      "Epoch 35, Training Loss: 0.7895945172560842\n",
      "Epoch 36, Training Loss: 0.79057419613788\n",
      "Epoch 37, Training Loss: 0.7901404308197194\n",
      "Epoch 38, Training Loss: 0.7888486684713149\n",
      "Epoch 39, Training Loss: 0.7889929076782742\n",
      "Epoch 40, Training Loss: 0.7888865205578338\n",
      "Epoch 41, Training Loss: 0.7890425689238355\n",
      "Epoch 42, Training Loss: 0.7893491914397792\n",
      "Epoch 43, Training Loss: 0.7902960750393402\n",
      "Epoch 44, Training Loss: 0.7884680297141685\n",
      "Epoch 45, Training Loss: 0.7879173608650839\n",
      "Epoch 46, Training Loss: 0.7887386155307741\n",
      "Epoch 47, Training Loss: 0.7883604118698522\n",
      "Epoch 48, Training Loss: 0.7889707289243999\n",
      "Epoch 49, Training Loss: 0.7888069282797047\n",
      "Epoch 50, Training Loss: 0.7880733739164539\n",
      "Epoch 51, Training Loss: 0.7880085839364762\n",
      "Epoch 52, Training Loss: 0.7892507439269159\n",
      "Epoch 53, Training Loss: 0.7880257157006658\n",
      "Epoch 54, Training Loss: 0.7882958242767736\n",
      "Epoch 55, Training Loss: 0.7876475445758131\n",
      "Epoch 56, Training Loss: 0.7880032744622768\n",
      "Epoch 57, Training Loss: 0.7883953699491975\n",
      "Epoch 58, Training Loss: 0.7879194937254254\n",
      "Epoch 59, Training Loss: 0.7873089599430113\n",
      "Epoch 60, Training Loss: 0.7876616016814583\n",
      "Epoch 61, Training Loss: 0.7884484199652995\n",
      "Epoch 62, Training Loss: 0.7879249067234814\n",
      "Epoch 63, Training Loss: 0.7888002114188403\n",
      "Epoch 64, Training Loss: 0.7880807735866173\n",
      "Epoch 65, Training Loss: 0.7877415493914955\n",
      "Epoch 66, Training Loss: 0.7873992639376705\n",
      "Epoch 67, Training Loss: 0.7873448488407565\n",
      "Epoch 68, Training Loss: 0.7884707120128144\n",
      "Epoch 69, Training Loss: 0.7881052225155938\n",
      "Epoch 70, Training Loss: 0.788609076442575\n",
      "Epoch 71, Training Loss: 0.7880996034109503\n",
      "Epoch 72, Training Loss: 0.7874426096005547\n",
      "Epoch 73, Training Loss: 0.7871754951046822\n",
      "Epoch 74, Training Loss: 0.7871584600075743\n",
      "Epoch 75, Training Loss: 0.7870584202888317\n",
      "Epoch 76, Training Loss: 0.7877678973334176\n",
      "Epoch 77, Training Loss: 0.7882164060621333\n",
      "Epoch 78, Training Loss: 0.7888214684070501\n",
      "Epoch 79, Training Loss: 0.7876736054743143\n",
      "Epoch 80, Training Loss: 0.7881140300205776\n",
      "Epoch 81, Training Loss: 0.7882026260956786\n",
      "Epoch 82, Training Loss: 0.7869520909804151\n",
      "Epoch 83, Training Loss: 0.7870604917519075\n",
      "Epoch 84, Training Loss: 0.7870846381761077\n",
      "Epoch 85, Training Loss: 0.7872526016450466\n",
      "Epoch 86, Training Loss: 0.7871977646996204\n",
      "Epoch 87, Training Loss: 0.7863174923604592\n",
      "Epoch 88, Training Loss: 0.7864622704964831\n",
      "Epoch 89, Training Loss: 0.7864543934513751\n",
      "Epoch 90, Training Loss: 0.7875009544809958\n",
      "Epoch 91, Training Loss: 0.787071772984096\n",
      "Epoch 92, Training Loss: 0.7876829755037351\n",
      "Epoch 93, Training Loss: 0.7878071690860547\n",
      "Epoch 94, Training Loss: 0.7884904987829968\n",
      "Epoch 95, Training Loss: 0.7873889683780814\n",
      "Epoch 96, Training Loss: 0.7873161749732226\n",
      "Epoch 97, Training Loss: 0.785717282886792\n",
      "Epoch 98, Training Loss: 0.7863157836118139\n",
      "Epoch 99, Training Loss: 0.7861757829673308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:27:29,019] Trial 43 finished with value: 0.6318 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7866367548928225\n",
      "Epoch 1, Training Loss: 0.8416645324931425\n",
      "Epoch 2, Training Loss: 0.8161805948790382\n",
      "Epoch 3, Training Loss: 0.8141306825245128\n",
      "Epoch 4, Training Loss: 0.8119995457284591\n",
      "Epoch 5, Training Loss: 0.8097256456403171\n",
      "Epoch 6, Training Loss: 0.8108847118125242\n",
      "Epoch 7, Training Loss: 0.8076375736909754\n",
      "Epoch 8, Training Loss: 0.8080421982092015\n",
      "Epoch 9, Training Loss: 0.80533006401623\n",
      "Epoch 10, Training Loss: 0.8066679697176989\n",
      "Epoch 11, Training Loss: 0.8065159194609698\n",
      "Epoch 12, Training Loss: 0.8058309329958523\n",
      "Epoch 13, Training Loss: 0.8051211752611048\n",
      "Epoch 14, Training Loss: 0.8042075101067038\n",
      "Epoch 15, Training Loss: 0.803815711105571\n",
      "Epoch 16, Training Loss: 0.8054844515463885\n",
      "Epoch 17, Training Loss: 0.8030519246353822\n",
      "Epoch 18, Training Loss: 0.803075702120276\n",
      "Epoch 19, Training Loss: 0.8018267024965847\n",
      "Epoch 20, Training Loss: 0.8036694088402916\n",
      "Epoch 21, Training Loss: 0.8034797305920545\n",
      "Epoch 22, Training Loss: 0.8036583914476283\n",
      "Epoch 23, Training Loss: 0.8020246801656835\n",
      "Epoch 24, Training Loss: 0.8014265684520497\n",
      "Epoch 25, Training Loss: 0.8025452736546012\n",
      "Epoch 26, Training Loss: 0.802728905467426\n",
      "Epoch 27, Training Loss: 0.8013183604969698\n",
      "Epoch 28, Training Loss: 0.8014593352990992\n",
      "Epoch 29, Training Loss: 0.8006445691164802\n",
      "Epoch 30, Training Loss: 0.8012710221374736\n",
      "Epoch 31, Training Loss: 0.7993815677306232\n",
      "Epoch 32, Training Loss: 0.8001717640371884\n",
      "Epoch 33, Training Loss: 0.8007427148959216\n",
      "Epoch 34, Training Loss: 0.7993975126042085\n",
      "Epoch 35, Training Loss: 0.8003669922492084\n",
      "Epoch 36, Training Loss: 0.8007372460645787\n",
      "Epoch 37, Training Loss: 0.8009732188196743\n",
      "Epoch 38, Training Loss: 0.7993154964026283\n",
      "Epoch 39, Training Loss: 0.8006910681724548\n",
      "Epoch 40, Training Loss: 0.800634414027719\n",
      "Epoch 41, Training Loss: 0.8000572232639088\n",
      "Epoch 42, Training Loss: 0.7996996358562918\n",
      "Epoch 43, Training Loss: 0.7997252526704003\n",
      "Epoch 44, Training Loss: 0.8000136091428645\n",
      "Epoch 45, Training Loss: 0.8002234231023228\n",
      "Epoch 46, Training Loss: 0.7999271450323217\n",
      "Epoch 47, Training Loss: 0.7994800440704122\n",
      "Epoch 48, Training Loss: 0.7997397389832664\n",
      "Epoch 49, Training Loss: 0.7996196927743799\n",
      "Epoch 50, Training Loss: 0.7999385783952825\n",
      "Epoch 51, Training Loss: 0.8001809542319354\n",
      "Epoch 52, Training Loss: 0.8010153944352094\n",
      "Epoch 53, Training Loss: 0.7999495547659257\n",
      "Epoch 54, Training Loss: 0.7992438815621768\n",
      "Epoch 55, Training Loss: 0.7994104930232553\n",
      "Epoch 56, Training Loss: 0.7996925216562608\n",
      "Epoch 57, Training Loss: 0.799295298351961\n",
      "Epoch 58, Training Loss: 0.7991858403822955\n",
      "Epoch 59, Training Loss: 0.798869485504487\n",
      "Epoch 60, Training Loss: 0.8000515359289506\n",
      "Epoch 61, Training Loss: 0.7991182703130385\n",
      "Epoch 62, Training Loss: 0.7989936999713674\n",
      "Epoch 63, Training Loss: 0.7993388931190266\n",
      "Epoch 64, Training Loss: 0.7993050535286174\n",
      "Epoch 65, Training Loss: 0.7991591236871831\n",
      "Epoch 66, Training Loss: 0.7990578949451447\n",
      "Epoch 67, Training Loss: 0.7992972279296202\n",
      "Epoch 68, Training Loss: 0.7978004812493044\n",
      "Epoch 69, Training Loss: 0.7993251921148861\n",
      "Epoch 70, Training Loss: 0.7994277718487908\n",
      "Epoch 71, Training Loss: 0.7986597098322475\n",
      "Epoch 72, Training Loss: 0.7973465772937326\n",
      "Epoch 73, Training Loss: 0.798868192434311\n",
      "Epoch 74, Training Loss: 0.7984583736868466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:28:55,342] Trial 44 finished with value: 0.6345333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7982328710135291\n",
      "Epoch 1, Training Loss: 0.8385348214242692\n",
      "Epoch 2, Training Loss: 0.8063162459466691\n",
      "Epoch 3, Training Loss: 0.8023478904164824\n",
      "Epoch 4, Training Loss: 0.8004314075735278\n",
      "Epoch 5, Training Loss: 0.7978365486725829\n",
      "Epoch 6, Training Loss: 0.7985692494793942\n",
      "Epoch 7, Training Loss: 0.7977083205280447\n",
      "Epoch 8, Training Loss: 0.7963320668478657\n",
      "Epoch 9, Training Loss: 0.7962446063084709\n",
      "Epoch 10, Training Loss: 0.7944626583192581\n",
      "Epoch 11, Training Loss: 0.7936500230229887\n",
      "Epoch 12, Training Loss: 0.7932933600325334\n",
      "Epoch 13, Training Loss: 0.7942463583515998\n",
      "Epoch 14, Training Loss: 0.7936038467220794\n",
      "Epoch 15, Training Loss: 0.7925842729726232\n",
      "Epoch 16, Training Loss: 0.7930161522743397\n",
      "Epoch 17, Training Loss: 0.7924646109566653\n",
      "Epoch 18, Training Loss: 0.7913679316527862\n",
      "Epoch 19, Training Loss: 0.7927160670882777\n",
      "Epoch 20, Training Loss: 0.7897263993446092\n",
      "Epoch 21, Training Loss: 0.7918249974573465\n",
      "Epoch 22, Training Loss: 0.7909893075326332\n",
      "Epoch 23, Training Loss: 0.791574053477524\n",
      "Epoch 24, Training Loss: 0.7902622170914385\n",
      "Epoch 25, Training Loss: 0.791233409885177\n",
      "Epoch 26, Training Loss: 0.7892287836038977\n",
      "Epoch 27, Training Loss: 0.7893304852614725\n",
      "Epoch 28, Training Loss: 0.7903973610777604\n",
      "Epoch 29, Training Loss: 0.7894885410491685\n",
      "Epoch 30, Training Loss: 0.7898350317675368\n",
      "Epoch 31, Training Loss: 0.7907917637574046\n",
      "Epoch 32, Training Loss: 0.7902610708000068\n",
      "Epoch 33, Training Loss: 0.7889814418509491\n",
      "Epoch 34, Training Loss: 0.7889882733947352\n",
      "Epoch 35, Training Loss: 0.7902542731815712\n",
      "Epoch 36, Training Loss: 0.7905365932256656\n",
      "Epoch 37, Training Loss: 0.7890083941301905\n",
      "Epoch 38, Training Loss: 0.789261760030474\n",
      "Epoch 39, Training Loss: 0.7898615974232667\n",
      "Epoch 40, Training Loss: 0.7894991488385021\n",
      "Epoch 41, Training Loss: 0.7894424447439667\n",
      "Epoch 42, Training Loss: 0.7885633931124121\n",
      "Epoch 43, Training Loss: 0.788932396415481\n",
      "Epoch 44, Training Loss: 0.7890160305159433\n",
      "Epoch 45, Training Loss: 0.7884755982492203\n",
      "Epoch 46, Training Loss: 0.7880771121584383\n",
      "Epoch 47, Training Loss: 0.7884460434877782\n",
      "Epoch 48, Training Loss: 0.7882558683703716\n",
      "Epoch 49, Training Loss: 0.7895778130767936\n",
      "Epoch 50, Training Loss: 0.7883118595395769\n",
      "Epoch 51, Training Loss: 0.7881300376770192\n",
      "Epoch 52, Training Loss: 0.788686894653435\n",
      "Epoch 53, Training Loss: 0.7872311399395304\n",
      "Epoch 54, Training Loss: 0.7880435496344602\n",
      "Epoch 55, Training Loss: 0.7889091037269822\n",
      "Epoch 56, Training Loss: 0.7884376619991503\n",
      "Epoch 57, Training Loss: 0.7878176297460283\n",
      "Epoch 58, Training Loss: 0.7869430361833788\n",
      "Epoch 59, Training Loss: 0.7868987313786844\n",
      "Epoch 60, Training Loss: 0.7864895868122129\n",
      "Epoch 61, Training Loss: 0.7876673981659394\n",
      "Epoch 62, Training Loss: 0.7883823339204142\n",
      "Epoch 63, Training Loss: 0.7884525643255478\n",
      "Epoch 64, Training Loss: 0.7877733052225041\n",
      "Epoch 65, Training Loss: 0.7880581769728123\n",
      "Epoch 66, Training Loss: 0.7881733782309338\n",
      "Epoch 67, Training Loss: 0.7883765482364741\n",
      "Epoch 68, Training Loss: 0.7874189301540977\n",
      "Epoch 69, Training Loss: 0.7877489358858955\n",
      "Epoch 70, Training Loss: 0.7866352405763211\n",
      "Epoch 71, Training Loss: 0.7866558620804235\n",
      "Epoch 72, Training Loss: 0.7877461541864209\n",
      "Epoch 73, Training Loss: 0.7879099296447926\n",
      "Epoch 74, Training Loss: 0.7864936790071932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:30:21,842] Trial 45 finished with value: 0.6336666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7872276195906158\n",
      "Epoch 1, Training Loss: 0.8385890734195709\n",
      "Epoch 2, Training Loss: 0.8084462149704204\n",
      "Epoch 3, Training Loss: 0.8046986176687129\n",
      "Epoch 4, Training Loss: 0.8017784823389614\n",
      "Epoch 5, Training Loss: 0.798730876936632\n",
      "Epoch 6, Training Loss: 0.798269008398056\n",
      "Epoch 7, Training Loss: 0.7960045677072861\n",
      "Epoch 8, Training Loss: 0.7967933117642122\n",
      "Epoch 9, Training Loss: 0.796370142698288\n",
      "Epoch 10, Training Loss: 0.7966723781473496\n",
      "Epoch 11, Training Loss: 0.7946516595167272\n",
      "Epoch 12, Training Loss: 0.7952307860991534\n",
      "Epoch 13, Training Loss: 0.7944858981581295\n",
      "Epoch 14, Training Loss: 0.7944854413761812\n",
      "Epoch 15, Training Loss: 0.7936827247984269\n",
      "Epoch 16, Training Loss: 0.7943072549034568\n",
      "Epoch 17, Training Loss: 0.7929928283130422\n",
      "Epoch 18, Training Loss: 0.7927763934696422\n",
      "Epoch 19, Training Loss: 0.7932145097676445\n",
      "Epoch 20, Training Loss: 0.7925446804130779\n",
      "Epoch 21, Training Loss: 0.7921647388794842\n",
      "Epoch 22, Training Loss: 0.7915546621294582\n",
      "Epoch 23, Training Loss: 0.7919397064517526\n",
      "Epoch 24, Training Loss: 0.7907393287209904\n",
      "Epoch 25, Training Loss: 0.7914664977438309\n",
      "Epoch 26, Training Loss: 0.7919715083346648\n",
      "Epoch 27, Training Loss: 0.7919381826064166\n",
      "Epoch 28, Training Loss: 0.7910851775197422\n",
      "Epoch 29, Training Loss: 0.791133492343566\n",
      "Epoch 30, Training Loss: 0.7906180635620565\n",
      "Epoch 31, Training Loss: 0.7904216825260836\n",
      "Epoch 32, Training Loss: 0.7907225039426018\n",
      "Epoch 33, Training Loss: 0.7908147851158591\n",
      "Epoch 34, Training Loss: 0.790645721730064\n",
      "Epoch 35, Training Loss: 0.790294665589052\n",
      "Epoch 36, Training Loss: 0.7908294689655304\n",
      "Epoch 37, Training Loss: 0.790922227677177\n",
      "Epoch 38, Training Loss: 0.7897893751368803\n",
      "Epoch 39, Training Loss: 0.7902532677790698\n",
      "Epoch 40, Training Loss: 0.7896829856844509\n",
      "Epoch 41, Training Loss: 0.7899102502009447\n",
      "Epoch 42, Training Loss: 0.7905241266418905\n",
      "Epoch 43, Training Loss: 0.7889866487418904\n",
      "Epoch 44, Training Loss: 0.7898141189883737\n",
      "Epoch 45, Training Loss: 0.7901634827080894\n",
      "Epoch 46, Training Loss: 0.7901814664812649\n",
      "Epoch 47, Training Loss: 0.7884637608247644\n",
      "Epoch 48, Training Loss: 0.7888837441276102\n",
      "Epoch 49, Training Loss: 0.7889713920565212\n",
      "Epoch 50, Training Loss: 0.7894869253915899\n",
      "Epoch 51, Training Loss: 0.7895888883226058\n",
      "Epoch 52, Training Loss: 0.7888262285905726\n",
      "Epoch 53, Training Loss: 0.7898526481319876\n",
      "Epoch 54, Training Loss: 0.7886513991916881\n",
      "Epoch 55, Training Loss: 0.7885308448006125\n",
      "Epoch 56, Training Loss: 0.7896416511255152\n",
      "Epoch 57, Training Loss: 0.7893138841320486\n",
      "Epoch 58, Training Loss: 0.7884132145432865\n",
      "Epoch 59, Training Loss: 0.7889081246712628\n",
      "Epoch 60, Training Loss: 0.7883006947882035\n",
      "Epoch 61, Training Loss: 0.788867005740895\n",
      "Epoch 62, Training Loss: 0.7899814182169297\n",
      "Epoch 63, Training Loss: 0.7885167207437404\n",
      "Epoch 64, Training Loss: 0.7874215229819803\n",
      "Epoch 65, Training Loss: 0.787969547440024\n",
      "Epoch 66, Training Loss: 0.7886662411689759\n",
      "Epoch 67, Training Loss: 0.7886213092242971\n",
      "Epoch 68, Training Loss: 0.7881581446002511\n",
      "Epoch 69, Training Loss: 0.7879633393708397\n",
      "Epoch 70, Training Loss: 0.7879572015650133\n",
      "Epoch 71, Training Loss: 0.7893619155182557\n",
      "Epoch 72, Training Loss: 0.7885269073879018\n",
      "Epoch 73, Training Loss: 0.7887563919319825\n",
      "Epoch 74, Training Loss: 0.7883683158369625\n",
      "Epoch 75, Training Loss: 0.7878643116530251\n",
      "Epoch 76, Training Loss: 0.7877033774992999\n",
      "Epoch 77, Training Loss: 0.7884190703139585\n",
      "Epoch 78, Training Loss: 0.7887521416299483\n",
      "Epoch 79, Training Loss: 0.7883975035302779\n",
      "Epoch 80, Training Loss: 0.7886356843219083\n",
      "Epoch 81, Training Loss: 0.7884011023184833\n",
      "Epoch 82, Training Loss: 0.7876715339632595\n",
      "Epoch 83, Training Loss: 0.787432527331745\n",
      "Epoch 84, Training Loss: 0.7883585260896122\n",
      "Epoch 85, Training Loss: 0.7879313079749837\n",
      "Epoch 86, Training Loss: 0.7892036073348101\n",
      "Epoch 87, Training Loss: 0.7870388348663554\n",
      "Epoch 88, Training Loss: 0.7879340068732991\n",
      "Epoch 89, Training Loss: 0.7883578028398401\n",
      "Epoch 90, Training Loss: 0.7873784643762252\n",
      "Epoch 91, Training Loss: 0.787480103338466\n",
      "Epoch 92, Training Loss: 0.7876856534621295\n",
      "Epoch 93, Training Loss: 0.7878025259691126\n",
      "Epoch 94, Training Loss: 0.7874993717670441\n",
      "Epoch 95, Training Loss: 0.7876798568052404\n",
      "Epoch 96, Training Loss: 0.7871915308166952\n",
      "Epoch 97, Training Loss: 0.787465034863528\n",
      "Epoch 98, Training Loss: 0.7877269695786869\n",
      "Epoch 99, Training Loss: 0.7880341003221624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:32:34,402] Trial 46 finished with value: 0.6385333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7879582194019766\n",
      "Epoch 1, Training Loss: 0.90506851224971\n",
      "Epoch 2, Training Loss: 0.8201556809862753\n",
      "Epoch 3, Training Loss: 0.8137905360164499\n",
      "Epoch 4, Training Loss: 0.8117218413747342\n",
      "Epoch 5, Training Loss: 0.8105348411359286\n",
      "Epoch 6, Training Loss: 0.8088124053818839\n",
      "Epoch 7, Training Loss: 0.807841616376002\n",
      "Epoch 8, Training Loss: 0.8068549244027389\n",
      "Epoch 9, Training Loss: 0.8057084129268962\n",
      "Epoch 10, Training Loss: 0.8056134916785964\n",
      "Epoch 11, Training Loss: 0.8039790535779824\n",
      "Epoch 12, Training Loss: 0.804785516477169\n",
      "Epoch 13, Training Loss: 0.8040848792047429\n",
      "Epoch 14, Training Loss: 0.8033763423898166\n",
      "Epoch 15, Training Loss: 0.8025027018740661\n",
      "Epoch 16, Training Loss: 0.8035998355177112\n",
      "Epoch 17, Training Loss: 0.8026432062450208\n",
      "Epoch 18, Training Loss: 0.8029334206330149\n",
      "Epoch 19, Training Loss: 0.8032980975351836\n",
      "Epoch 20, Training Loss: 0.8027017570976028\n",
      "Epoch 21, Training Loss: 0.8009600480248157\n",
      "Epoch 22, Training Loss: 0.8019289944404946\n",
      "Epoch 23, Training Loss: 0.8018170250089545\n",
      "Epoch 24, Training Loss: 0.8015466842436253\n",
      "Epoch 25, Training Loss: 0.8007224505108999\n",
      "Epoch 26, Training Loss: 0.8007053600218063\n",
      "Epoch 27, Training Loss: 0.801594661052962\n",
      "Epoch 28, Training Loss: 0.8015782624259031\n",
      "Epoch 29, Training Loss: 0.8018388591314617\n",
      "Epoch 30, Training Loss: 0.7997032369886126\n",
      "Epoch 31, Training Loss: 0.7993674814252925\n",
      "Epoch 32, Training Loss: 0.8002129051918374\n",
      "Epoch 33, Training Loss: 0.7997871026060634\n",
      "Epoch 34, Training Loss: 0.8001558898983145\n",
      "Epoch 35, Training Loss: 0.7997317993551268\n",
      "Epoch 36, Training Loss: 0.7990932884072899\n",
      "Epoch 37, Training Loss: 0.7995544784947446\n",
      "Epoch 38, Training Loss: 0.7999555866969259\n",
      "Epoch 39, Training Loss: 0.7988296807260442\n",
      "Epoch 40, Training Loss: 0.799962253947007\n",
      "Epoch 41, Training Loss: 0.7987927331960291\n",
      "Epoch 42, Training Loss: 0.7989366763516477\n",
      "Epoch 43, Training Loss: 0.799193563317894\n",
      "Epoch 44, Training Loss: 0.7993734341815002\n",
      "Epoch 45, Training Loss: 0.7981992660608507\n",
      "Epoch 46, Training Loss: 0.7985321448261576\n",
      "Epoch 47, Training Loss: 0.7990317158232955\n",
      "Epoch 48, Training Loss: 0.7991744030687146\n",
      "Epoch 49, Training Loss: 0.7980391748865744\n",
      "Epoch 50, Training Loss: 0.7981461360938567\n",
      "Epoch 51, Training Loss: 0.798855814001614\n",
      "Epoch 52, Training Loss: 0.7980241071012684\n",
      "Epoch 53, Training Loss: 0.7978564776872333\n",
      "Epoch 54, Training Loss: 0.7987276910839224\n",
      "Epoch 55, Training Loss: 0.7976337258977101\n",
      "Epoch 56, Training Loss: 0.7970294118823862\n",
      "Epoch 57, Training Loss: 0.7967695481795117\n",
      "Epoch 58, Training Loss: 0.796365471531574\n",
      "Epoch 59, Training Loss: 0.7967655820954115\n",
      "Epoch 60, Training Loss: 0.7982951779114572\n",
      "Epoch 61, Training Loss: 0.7982329768345768\n",
      "Epoch 62, Training Loss: 0.7966742935933565\n",
      "Epoch 63, Training Loss: 0.7962217920704892\n",
      "Epoch 64, Training Loss: 0.7965864475508382\n",
      "Epoch 65, Training Loss: 0.7965289000281714\n",
      "Epoch 66, Training Loss: 0.7963276444521166\n",
      "Epoch 67, Training Loss: 0.7957544984674095\n",
      "Epoch 68, Training Loss: 0.7957711249365842\n",
      "Epoch 69, Training Loss: 0.7964473852537628\n",
      "Epoch 70, Training Loss: 0.7960653005686021\n",
      "Epoch 71, Training Loss: 0.795704804596148\n",
      "Epoch 72, Training Loss: 0.7950792582411516\n",
      "Epoch 73, Training Loss: 0.7954329361592917\n",
      "Epoch 74, Training Loss: 0.7951296089287091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:33:49,117] Trial 47 finished with value: 0.6344 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7949890073977018\n",
      "Epoch 1, Training Loss: 1.0847824254430327\n",
      "Epoch 2, Training Loss: 1.0446277696387212\n",
      "Epoch 3, Training Loss: 1.0178362545214201\n",
      "Epoch 4, Training Loss: 0.9985540609610708\n",
      "Epoch 5, Training Loss: 0.9850549800951678\n",
      "Epoch 6, Training Loss: 0.9755946768853897\n",
      "Epoch 7, Training Loss: 0.969068303592223\n",
      "Epoch 8, Training Loss: 0.9648367073302878\n",
      "Epoch 9, Training Loss: 0.9616237590187474\n",
      "Epoch 10, Training Loss: 0.9594661860537709\n",
      "Epoch 11, Training Loss: 0.9577563697234133\n",
      "Epoch 12, Training Loss: 0.9567384762871535\n",
      "Epoch 13, Training Loss: 0.9557280561977759\n",
      "Epoch 14, Training Loss: 0.9551304074158345\n",
      "Epoch 15, Training Loss: 0.9540010579546592\n",
      "Epoch 16, Training Loss: 0.9533854609145258\n",
      "Epoch 17, Training Loss: 0.9527544424946147\n",
      "Epoch 18, Training Loss: 0.9526175004199036\n",
      "Epoch 19, Training Loss: 0.9511698437812633\n",
      "Epoch 20, Training Loss: 0.9505335473476496\n",
      "Epoch 21, Training Loss: 0.9495771008326594\n",
      "Epoch 22, Training Loss: 0.9493546736867804\n",
      "Epoch 23, Training Loss: 0.9486060370179944\n",
      "Epoch 24, Training Loss: 0.9477695615668046\n",
      "Epoch 25, Training Loss: 0.9472121625018299\n",
      "Epoch 26, Training Loss: 0.946308203747398\n",
      "Epoch 27, Training Loss: 0.9459158906363007\n",
      "Epoch 28, Training Loss: 0.9449933232221388\n",
      "Epoch 29, Training Loss: 0.944071806821608\n",
      "Epoch 30, Training Loss: 0.9432792490586303\n",
      "Epoch 31, Training Loss: 0.9425190285632484\n",
      "Epoch 32, Training Loss: 0.9419365505526837\n",
      "Epoch 33, Training Loss: 0.9415423827063768\n",
      "Epoch 34, Training Loss: 0.9404968948292552\n",
      "Epoch 35, Training Loss: 0.9394365972146056\n",
      "Epoch 36, Training Loss: 0.9387452448221077\n",
      "Epoch 37, Training Loss: 0.9382271805203947\n",
      "Epoch 38, Training Loss: 0.9367032445463023\n",
      "Epoch 39, Training Loss: 0.9361528322212678\n",
      "Epoch 40, Training Loss: 0.93487526008061\n",
      "Epoch 41, Training Loss: 0.934398401142063\n",
      "Epoch 42, Training Loss: 0.9335798942056813\n",
      "Epoch 43, Training Loss: 0.9324276991356585\n",
      "Epoch 44, Training Loss: 0.93145554379413\n",
      "Epoch 45, Training Loss: 0.9307766630237264\n",
      "Epoch 46, Training Loss: 0.9293054512568882\n",
      "Epoch 47, Training Loss: 0.928158950716033\n",
      "Epoch 48, Training Loss: 0.9273837921314669\n",
      "Epoch 49, Training Loss: 0.9261828621527306\n",
      "Epoch 50, Training Loss: 0.9252173571658314\n",
      "Epoch 51, Training Loss: 0.9241770911933784\n",
      "Epoch 52, Training Loss: 0.9229057263610955\n",
      "Epoch 53, Training Loss: 0.9217676223668837\n",
      "Epoch 54, Training Loss: 0.9205637887904519\n",
      "Epoch 55, Training Loss: 0.9192858440535409\n",
      "Epoch 56, Training Loss: 0.9183474553258796\n",
      "Epoch 57, Training Loss: 0.9171376133323612\n",
      "Epoch 58, Training Loss: 0.9155850613027587\n",
      "Epoch 59, Training Loss: 0.9141554153055177\n",
      "Epoch 60, Training Loss: 0.912628294023356\n",
      "Epoch 61, Training Loss: 0.9115791847831325\n",
      "Epoch 62, Training Loss: 0.9105222615980564\n",
      "Epoch 63, Training Loss: 0.9088054034046661\n",
      "Epoch 64, Training Loss: 0.9069762490745774\n",
      "Epoch 65, Training Loss: 0.9058991191978741\n",
      "Epoch 66, Training Loss: 0.904120605780666\n",
      "Epoch 67, Training Loss: 0.9024696696073489\n",
      "Epoch 68, Training Loss: 0.9017312190586463\n",
      "Epoch 69, Training Loss: 0.8999036321962687\n",
      "Epoch 70, Training Loss: 0.8984225661234748\n",
      "Epoch 71, Training Loss: 0.8965078869260343\n",
      "Epoch 72, Training Loss: 0.8952640908105033\n",
      "Epoch 73, Training Loss: 0.8937825565947626\n",
      "Epoch 74, Training Loss: 0.8914420437095757\n",
      "Epoch 75, Training Loss: 0.8896531476114029\n",
      "Epoch 76, Training Loss: 0.8881451168454679\n",
      "Epoch 77, Training Loss: 0.8861945695446846\n",
      "Epoch 78, Training Loss: 0.8846905086273538\n",
      "Epoch 79, Training Loss: 0.8829331003633657\n",
      "Epoch 80, Training Loss: 0.8806408637896517\n",
      "Epoch 81, Training Loss: 0.8793462519358871\n",
      "Epoch 82, Training Loss: 0.8780111549492169\n",
      "Epoch 83, Training Loss: 0.8759345903432458\n",
      "Epoch 84, Training Loss: 0.8743906505125806\n",
      "Epoch 85, Training Loss: 0.8727947039711744\n",
      "Epoch 86, Training Loss: 0.87086427857105\n",
      "Epoch 87, Training Loss: 0.8692982167229617\n",
      "Epoch 88, Training Loss: 0.8677857358652846\n",
      "Epoch 89, Training Loss: 0.8660655300420029\n",
      "Epoch 90, Training Loss: 0.8645041160117415\n",
      "Epoch 91, Training Loss: 0.8627078792206326\n",
      "Epoch 92, Training Loss: 0.8615677958144281\n",
      "Epoch 93, Training Loss: 0.859260930781974\n",
      "Epoch 94, Training Loss: 0.8580923164697518\n",
      "Epoch 95, Training Loss: 0.8568388294456597\n",
      "Epoch 96, Training Loss: 0.8552257969863433\n",
      "Epoch 97, Training Loss: 0.8538986073400742\n",
      "Epoch 98, Training Loss: 0.8521232184610869\n",
      "Epoch 99, Training Loss: 0.8507067661536367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:35:24,212] Trial 48 finished with value: 0.6059333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.849604664619704\n",
      "Epoch 1, Training Loss: 0.8745849624612277\n",
      "Epoch 2, Training Loss: 0.8254004292022017\n",
      "Epoch 3, Training Loss: 0.8173210122531518\n",
      "Epoch 4, Training Loss: 0.8137746981212071\n",
      "Epoch 5, Training Loss: 0.8112390567485551\n",
      "Epoch 6, Training Loss: 0.8081994088072526\n",
      "Epoch 7, Training Loss: 0.8075576472103148\n",
      "Epoch 8, Training Loss: 0.805168366701083\n",
      "Epoch 9, Training Loss: 0.8045982633318219\n",
      "Epoch 10, Training Loss: 0.8031831463476768\n",
      "Epoch 11, Training Loss: 0.8031254700251989\n",
      "Epoch 12, Training Loss: 0.8040528214067445\n",
      "Epoch 13, Training Loss: 0.8010672476954926\n",
      "Epoch 14, Training Loss: 0.8018431732529088\n",
      "Epoch 15, Training Loss: 0.80322353302088\n",
      "Epoch 16, Training Loss: 0.8021441179110591\n",
      "Epoch 17, Training Loss: 0.8004555888642045\n",
      "Epoch 18, Training Loss: 0.8011304673395658\n",
      "Epoch 19, Training Loss: 0.7991198894672824\n",
      "Epoch 20, Training Loss: 0.799680287228491\n",
      "Epoch 21, Training Loss: 0.7998567098961737\n",
      "Epoch 22, Training Loss: 0.7998401525325345\n",
      "Epoch 23, Training Loss: 0.7987560136873919\n",
      "Epoch 24, Training Loss: 0.7982388455168645\n",
      "Epoch 25, Training Loss: 0.797440286955439\n",
      "Epoch 26, Training Loss: 0.7983995722648792\n",
      "Epoch 27, Training Loss: 0.7976269982811204\n",
      "Epoch 28, Training Loss: 0.798143586538788\n",
      "Epoch 29, Training Loss: 0.7977049868805964\n",
      "Epoch 30, Training Loss: 0.7976053968408054\n",
      "Epoch 31, Training Loss: 0.7968498508733018\n",
      "Epoch 32, Training Loss: 0.797368686181262\n",
      "Epoch 33, Training Loss: 0.7975201647084459\n",
      "Epoch 34, Training Loss: 0.7968934929012356\n",
      "Epoch 35, Training Loss: 0.7969554867959561\n",
      "Epoch 36, Training Loss: 0.7974541531469589\n",
      "Epoch 37, Training Loss: 0.7965571601588027\n",
      "Epoch 38, Training Loss: 0.7971270775436459\n",
      "Epoch 39, Training Loss: 0.7971708828345277\n",
      "Epoch 40, Training Loss: 0.7959488262807516\n",
      "Epoch 41, Training Loss: 0.7966107454515041\n",
      "Epoch 42, Training Loss: 0.7964717524392264\n",
      "Epoch 43, Training Loss: 0.7962493321949378\n",
      "Epoch 44, Training Loss: 0.7959339918050551\n",
      "Epoch 45, Training Loss: 0.7962900507718997\n",
      "Epoch 46, Training Loss: 0.796313426369115\n",
      "Epoch 47, Training Loss: 0.7955326083011197\n",
      "Epoch 48, Training Loss: 0.7958417195126526\n",
      "Epoch 49, Training Loss: 0.7956565470623791\n",
      "Epoch 50, Training Loss: 0.7959385765226263\n",
      "Epoch 51, Training Loss: 0.7956186754362924\n",
      "Epoch 52, Training Loss: 0.7948414987191221\n",
      "Epoch 53, Training Loss: 0.7958922548401625\n",
      "Epoch 54, Training Loss: 0.7956956793491106\n",
      "Epoch 55, Training Loss: 0.7952420451587304\n",
      "Epoch 56, Training Loss: 0.7956795312408218\n",
      "Epoch 57, Training Loss: 0.7958052526739307\n",
      "Epoch 58, Training Loss: 0.7960494001108901\n",
      "Epoch 59, Training Loss: 0.7956466840622121\n",
      "Epoch 60, Training Loss: 0.7946310034371856\n",
      "Epoch 61, Training Loss: 0.7945064873623668\n",
      "Epoch 62, Training Loss: 0.7951015517227632\n",
      "Epoch 63, Training Loss: 0.7951548879307911\n",
      "Epoch 64, Training Loss: 0.7956988882301446\n",
      "Epoch 65, Training Loss: 0.7958953496208765\n",
      "Epoch 66, Training Loss: 0.7953591613841237\n",
      "Epoch 67, Training Loss: 0.7950924595495812\n",
      "Epoch 68, Training Loss: 0.7950922318867275\n",
      "Epoch 69, Training Loss: 0.7948700138500758\n",
      "Epoch 70, Training Loss: 0.7946229238259165\n",
      "Epoch 71, Training Loss: 0.7950544858337345\n",
      "Epoch 72, Training Loss: 0.7952952020150378\n",
      "Epoch 73, Training Loss: 0.794245707899108\n",
      "Epoch 74, Training Loss: 0.7963155829816833\n",
      "Epoch 75, Training Loss: 0.7956964086769218\n",
      "Epoch 76, Training Loss: 0.7945891946778262\n",
      "Epoch 77, Training Loss: 0.7936332610316743\n",
      "Epoch 78, Training Loss: 0.7948993271454833\n",
      "Epoch 79, Training Loss: 0.7942327880321589\n",
      "Epoch 80, Training Loss: 0.7945085371347298\n",
      "Epoch 81, Training Loss: 0.79413544844864\n",
      "Epoch 82, Training Loss: 0.7934687713931378\n",
      "Epoch 83, Training Loss: 0.7941329887935094\n",
      "Epoch 84, Training Loss: 0.7946645957186707\n",
      "Epoch 85, Training Loss: 0.7939634861802696\n",
      "Epoch 86, Training Loss: 0.7949805543835001\n",
      "Epoch 87, Training Loss: 0.794984491606404\n",
      "Epoch 88, Training Loss: 0.7943790598919517\n",
      "Epoch 89, Training Loss: 0.7937661810925133\n",
      "Epoch 90, Training Loss: 0.7948473107545896\n",
      "Epoch 91, Training Loss: 0.7942598462104797\n",
      "Epoch 92, Training Loss: 0.7954143996525528\n",
      "Epoch 93, Training Loss: 0.7938982077559135\n",
      "Epoch 94, Training Loss: 0.7933836864349537\n",
      "Epoch 95, Training Loss: 0.7938609012087485\n",
      "Epoch 96, Training Loss: 0.7945475122982398\n",
      "Epoch 97, Training Loss: 0.7939287453665769\n",
      "Epoch 98, Training Loss: 0.7937371537201386\n",
      "Epoch 99, Training Loss: 0.7935243407586463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:37:14,139] Trial 49 finished with value: 0.6071333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7933002360333178\n",
      "Epoch 1, Training Loss: 0.8829895904484917\n",
      "Epoch 2, Training Loss: 0.8193385656440959\n",
      "Epoch 3, Training Loss: 0.8122410448859719\n",
      "Epoch 4, Training Loss: 0.8067134293387918\n",
      "Epoch 5, Training Loss: 0.8040384089245516\n",
      "Epoch 6, Training Loss: 0.8018901206465329\n",
      "Epoch 7, Training Loss: 0.7990644790845759\n",
      "Epoch 8, Training Loss: 0.7982446868980632\n",
      "Epoch 9, Training Loss: 0.7977592560824226\n",
      "Epoch 10, Training Loss: 0.7965522983494927\n",
      "Epoch 11, Training Loss: 0.7957321795996498\n",
      "Epoch 12, Training Loss: 0.7954512533019571\n",
      "Epoch 13, Training Loss: 0.7947226495602552\n",
      "Epoch 14, Training Loss: 0.7946128413256477\n",
      "Epoch 15, Training Loss: 0.7931035146292518\n",
      "Epoch 16, Training Loss: 0.7929434574351591\n",
      "Epoch 17, Training Loss: 0.792762566243901\n",
      "Epoch 18, Training Loss: 0.7926664034759298\n",
      "Epoch 19, Training Loss: 0.7925761918460622\n",
      "Epoch 20, Training Loss: 0.791331439298742\n",
      "Epoch 21, Training Loss: 0.7912619281516355\n",
      "Epoch 22, Training Loss: 0.7914042813637677\n",
      "Epoch 23, Training Loss: 0.7909230684532839\n",
      "Epoch 24, Training Loss: 0.7907097934975343\n",
      "Epoch 25, Training Loss: 0.7903819373775931\n",
      "Epoch 26, Training Loss: 0.7905562804025762\n",
      "Epoch 27, Training Loss: 0.7902264361521777\n",
      "Epoch 28, Training Loss: 0.790741240347133\n",
      "Epoch 29, Training Loss: 0.7899923886271084\n",
      "Epoch 30, Training Loss: 0.7900232315764708\n",
      "Epoch 31, Training Loss: 0.7892599497823154\n",
      "Epoch 32, Training Loss: 0.7889183598406174\n",
      "Epoch 33, Training Loss: 0.7889169512776768\n",
      "Epoch 34, Training Loss: 0.7890845595387852\n",
      "Epoch 35, Training Loss: 0.788431623472887\n",
      "Epoch 36, Training Loss: 0.7889123990956475\n",
      "Epoch 37, Training Loss: 0.7880376731648164\n",
      "Epoch 38, Training Loss: 0.7887083273775437\n",
      "Epoch 39, Training Loss: 0.788527544666739\n",
      "Epoch 40, Training Loss: 0.7874255267311545\n",
      "Epoch 41, Training Loss: 0.7879113269553465\n",
      "Epoch 42, Training Loss: 0.7877476297406589\n",
      "Epoch 43, Training Loss: 0.7882525944008547\n",
      "Epoch 44, Training Loss: 0.7876388719502617\n",
      "Epoch 45, Training Loss: 0.7872567497281467\n",
      "Epoch 46, Training Loss: 0.7872534033831428\n",
      "Epoch 47, Training Loss: 0.787527558593189\n",
      "Epoch 48, Training Loss: 0.7870056182496687\n",
      "Epoch 49, Training Loss: 0.7873580516787136\n",
      "Epoch 50, Training Loss: 0.7870867415736703\n",
      "Epoch 51, Training Loss: 0.7868159573919633\n",
      "Epoch 52, Training Loss: 0.7865593762958751\n",
      "Epoch 53, Training Loss: 0.7865784793741563\n",
      "Epoch 54, Training Loss: 0.786791066422182\n",
      "Epoch 55, Training Loss: 0.7863328740176032\n",
      "Epoch 56, Training Loss: 0.7862366268915288\n",
      "Epoch 57, Training Loss: 0.7863880939343396\n",
      "Epoch 58, Training Loss: 0.7868048334121704\n",
      "Epoch 59, Training Loss: 0.7856858982759364\n",
      "Epoch 60, Training Loss: 0.7860294713693506\n",
      "Epoch 61, Training Loss: 0.7861706752636853\n",
      "Epoch 62, Training Loss: 0.7860119586131152\n",
      "Epoch 63, Training Loss: 0.7859200367506812\n",
      "Epoch 64, Training Loss: 0.7859241937889773\n",
      "Epoch 65, Training Loss: 0.7864420953217675\n",
      "Epoch 66, Training Loss: 0.7857394101339228\n",
      "Epoch 67, Training Loss: 0.7862251286646899\n",
      "Epoch 68, Training Loss: 0.7860866029823528\n",
      "Epoch 69, Training Loss: 0.7858828139305115\n",
      "Epoch 70, Training Loss: 0.785692976993673\n",
      "Epoch 71, Training Loss: 0.7864079552538255\n",
      "Epoch 72, Training Loss: 0.7856111901647904\n",
      "Epoch 73, Training Loss: 0.7856996964707094\n",
      "Epoch 74, Training Loss: 0.786199293907951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:38:47,215] Trial 50 finished with value: 0.6374 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7860236141260932\n",
      "Epoch 1, Training Loss: 0.8921863598931105\n",
      "Epoch 2, Training Loss: 0.8271252372211083\n",
      "Epoch 3, Training Loss: 0.8194250891083166\n",
      "Epoch 4, Training Loss: 0.815892467552558\n",
      "Epoch 5, Training Loss: 0.8143662796880966\n",
      "Epoch 6, Training Loss: 0.8115089258753267\n",
      "Epoch 7, Training Loss: 0.8111091966915848\n",
      "Epoch 8, Training Loss: 0.8100842812007532\n",
      "Epoch 9, Training Loss: 0.8093799087337982\n",
      "Epoch 10, Training Loss: 0.8080650540222799\n",
      "Epoch 11, Training Loss: 0.8062341733982689\n",
      "Epoch 12, Training Loss: 0.8074236568651701\n",
      "Epoch 13, Training Loss: 0.8072319602607785\n",
      "Epoch 14, Training Loss: 0.8060632227955008\n",
      "Epoch 15, Training Loss: 0.805229862202379\n",
      "Epoch 16, Training Loss: 0.8058167301622549\n",
      "Epoch 17, Training Loss: 0.8056985892747578\n",
      "Epoch 18, Training Loss: 0.8052449304358403\n",
      "Epoch 19, Training Loss: 0.804356470771302\n",
      "Epoch 20, Training Loss: 0.8034445456096104\n",
      "Epoch 21, Training Loss: 0.8036644776960961\n",
      "Epoch 22, Training Loss: 0.8029590417567949\n",
      "Epoch 23, Training Loss: 0.8026500854277073\n",
      "Epoch 24, Training Loss: 0.8021825417540127\n",
      "Epoch 25, Training Loss: 0.8021237596533353\n",
      "Epoch 26, Training Loss: 0.8015922310657071\n",
      "Epoch 27, Training Loss: 0.8010264724717104\n",
      "Epoch 28, Training Loss: 0.8010171455548222\n",
      "Epoch 29, Training Loss: 0.8008606629264086\n",
      "Epoch 30, Training Loss: 0.7996884018854987\n",
      "Epoch 31, Training Loss: 0.8008578094324671\n",
      "Epoch 32, Training Loss: 0.8004806259520968\n",
      "Epoch 33, Training Loss: 0.7995145190031009\n",
      "Epoch 34, Training Loss: 0.7991823584513557\n",
      "Epoch 35, Training Loss: 0.7989628682459208\n",
      "Epoch 36, Training Loss: 0.7983426028624513\n",
      "Epoch 37, Training Loss: 0.7975974176163064\n",
      "Epoch 38, Training Loss: 0.7984064061838881\n",
      "Epoch 39, Training Loss: 0.7974158400431611\n",
      "Epoch 40, Training Loss: 0.798190827118723\n",
      "Epoch 41, Training Loss: 0.7971073683939482\n",
      "Epoch 42, Training Loss: 0.7972551562732324\n",
      "Epoch 43, Training Loss: 0.7973415200871633\n",
      "Epoch 44, Training Loss: 0.7972802811099174\n",
      "Epoch 45, Training Loss: 0.797073751553557\n",
      "Epoch 46, Training Loss: 0.7968171849286646\n",
      "Epoch 47, Training Loss: 0.7961306307548867\n",
      "Epoch 48, Training Loss: 0.7962229652512343\n",
      "Epoch 49, Training Loss: 0.7970970680839137\n",
      "Epoch 50, Training Loss: 0.7962403550183863\n",
      "Epoch 51, Training Loss: 0.7955353558511662\n",
      "Epoch 52, Training Loss: 0.795683931587334\n",
      "Epoch 53, Training Loss: 0.7955365448069752\n",
      "Epoch 54, Training Loss: 0.7949630510089989\n",
      "Epoch 55, Training Loss: 0.7947357537154864\n",
      "Epoch 56, Training Loss: 0.7949770367235169\n",
      "Epoch 57, Training Loss: 0.7950634651614311\n",
      "Epoch 58, Training Loss: 0.7947244083971009\n",
      "Epoch 59, Training Loss: 0.7949855795480255\n",
      "Epoch 60, Training Loss: 0.7949229230558066\n",
      "Epoch 61, Training Loss: 0.7941315350675942\n",
      "Epoch 62, Training Loss: 0.7938078911680925\n",
      "Epoch 63, Training Loss: 0.7947862677108076\n",
      "Epoch 64, Training Loss: 0.7943305958482556\n",
      "Epoch 65, Training Loss: 0.7946529213647197\n",
      "Epoch 66, Training Loss: 0.794427872690043\n",
      "Epoch 67, Training Loss: 0.7947580078490695\n",
      "Epoch 68, Training Loss: 0.7931948988061203\n",
      "Epoch 69, Training Loss: 0.7938883672979541\n",
      "Epoch 70, Training Loss: 0.7933969460931936\n",
      "Epoch 71, Training Loss: 0.7936046339515457\n",
      "Epoch 72, Training Loss: 0.7941540226004178\n",
      "Epoch 73, Training Loss: 0.7932859859072177\n",
      "Epoch 74, Training Loss: 0.7930484486701793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:39:57,837] Trial 51 finished with value: 0.609 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7936320200002283\n",
      "Epoch 1, Training Loss: 0.8477016281380373\n",
      "Epoch 2, Training Loss: 0.8155392498128554\n",
      "Epoch 3, Training Loss: 0.8089429349057815\n",
      "Epoch 4, Training Loss: 0.8081604274581461\n",
      "Epoch 5, Training Loss: 0.8059715149683111\n",
      "Epoch 6, Training Loss: 0.8040980243682861\n",
      "Epoch 7, Training Loss: 0.802904645835652\n",
      "Epoch 8, Training Loss: 0.8022306731167962\n",
      "Epoch 9, Training Loss: 0.8014081211651073\n",
      "Epoch 10, Training Loss: 0.8009285262051751\n",
      "Epoch 11, Training Loss: 0.8006959732841042\n",
      "Epoch 12, Training Loss: 0.8005330394997316\n",
      "Epoch 13, Training Loss: 0.799929979057873\n",
      "Epoch 14, Training Loss: 0.7996284847399767\n",
      "Epoch 15, Training Loss: 0.7987478694495033\n",
      "Epoch 16, Training Loss: 0.7994205267990336\n",
      "Epoch 17, Training Loss: 0.7983343142621657\n",
      "Epoch 18, Training Loss: 0.7977140669962939\n",
      "Epoch 19, Training Loss: 0.798063294466804\n",
      "Epoch 20, Training Loss: 0.7975657590697793\n",
      "Epoch 21, Training Loss: 0.7967549574375152\n",
      "Epoch 22, Training Loss: 0.7976415269515094\n",
      "Epoch 23, Training Loss: 0.7969449656149921\n",
      "Epoch 24, Training Loss: 0.7971672081246095\n",
      "Epoch 25, Training Loss: 0.7961970147665809\n",
      "Epoch 26, Training Loss: 0.7972372898634742\n",
      "Epoch 27, Training Loss: 0.7963462862547707\n",
      "Epoch 28, Training Loss: 0.7959436884347131\n",
      "Epoch 29, Training Loss: 0.7960858137467328\n",
      "Epoch 30, Training Loss: 0.7972895495330586\n",
      "Epoch 31, Training Loss: 0.7961840383445515\n",
      "Epoch 32, Training Loss: 0.7962105382891262\n",
      "Epoch 33, Training Loss: 0.7956874529053183\n",
      "Epoch 34, Training Loss: 0.7961866156493916\n",
      "Epoch 35, Training Loss: 0.7956690267955555\n",
      "Epoch 36, Training Loss: 0.7942617312599631\n",
      "Epoch 37, Training Loss: 0.7955416673772475\n",
      "Epoch 38, Training Loss: 0.7941858119824353\n",
      "Epoch 39, Training Loss: 0.7952577585332534\n",
      "Epoch 40, Training Loss: 0.7943888827632455\n",
      "Epoch 41, Training Loss: 0.7945487638080821\n",
      "Epoch 42, Training Loss: 0.7945345841435825\n",
      "Epoch 43, Training Loss: 0.7940675906574025\n",
      "Epoch 44, Training Loss: 0.7938632776456721\n",
      "Epoch 45, Training Loss: 0.7947258917724385\n",
      "Epoch 46, Training Loss: 0.7939026889380286\n",
      "Epoch 47, Training Loss: 0.7943218273275039\n",
      "Epoch 48, Training Loss: 0.7942810137131635\n",
      "Epoch 49, Training Loss: 0.7940748646680047\n",
      "Epoch 50, Training Loss: 0.7937557774431565\n",
      "Epoch 51, Training Loss: 0.7929533444432652\n",
      "Epoch 52, Training Loss: 0.7929919763172374\n",
      "Epoch 53, Training Loss: 0.7930023326593287\n",
      "Epoch 54, Training Loss: 0.7932740555791293\n",
      "Epoch 55, Training Loss: 0.7933181997607736\n",
      "Epoch 56, Training Loss: 0.7935901252662434\n",
      "Epoch 57, Training Loss: 0.7932166968373692\n",
      "Epoch 58, Training Loss: 0.7933758522482479\n",
      "Epoch 59, Training Loss: 0.7923016949260936\n",
      "Epoch 60, Training Loss: 0.7927458842361674\n",
      "Epoch 61, Training Loss: 0.7937248719439787\n",
      "Epoch 62, Training Loss: 0.7933407942687764\n",
      "Epoch 63, Training Loss: 0.7933030353574192\n",
      "Epoch 64, Training Loss: 0.792636600522434\n",
      "Epoch 65, Training Loss: 0.7924285269484801\n",
      "Epoch 66, Training Loss: 0.792614799527561\n",
      "Epoch 67, Training Loss: 0.7929928077669705\n",
      "Epoch 68, Training Loss: 0.7924291593888226\n",
      "Epoch 69, Training Loss: 0.7920159176517936\n",
      "Epoch 70, Training Loss: 0.7929301433703478\n",
      "Epoch 71, Training Loss: 0.7931823735377368\n",
      "Epoch 72, Training Loss: 0.7929920373243444\n",
      "Epoch 73, Training Loss: 0.7929603669222663\n",
      "Epoch 74, Training Loss: 0.7930442711886238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:41:23,814] Trial 52 finished with value: 0.6365333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7927582599836237\n",
      "Epoch 1, Training Loss: 0.8708758223325687\n",
      "Epoch 2, Training Loss: 0.8264373952284791\n",
      "Epoch 3, Training Loss: 0.8210431313156186\n",
      "Epoch 4, Training Loss: 0.8181964092684868\n",
      "Epoch 5, Training Loss: 0.8144049872133069\n",
      "Epoch 6, Training Loss: 0.8138403034747992\n",
      "Epoch 7, Training Loss: 0.8117986780360229\n",
      "Epoch 8, Training Loss: 0.8119799337889019\n",
      "Epoch 9, Training Loss: 0.8091136259243901\n",
      "Epoch 10, Training Loss: 0.8088120081370934\n",
      "Epoch 11, Training Loss: 0.8077460101672581\n",
      "Epoch 12, Training Loss: 0.8073521682194301\n",
      "Epoch 13, Training Loss: 0.8072510720195627\n",
      "Epoch 14, Training Loss: 0.8070791800219314\n",
      "Epoch 15, Training Loss: 0.8066368727755726\n",
      "Epoch 16, Training Loss: 0.8051499103244982\n",
      "Epoch 17, Training Loss: 0.8049201251868915\n",
      "Epoch 18, Training Loss: 0.8052751221154866\n",
      "Epoch 19, Training Loss: 0.8035415406513932\n",
      "Epoch 20, Training Loss: 0.8045889420616895\n",
      "Epoch 21, Training Loss: 0.8045486269140603\n",
      "Epoch 22, Training Loss: 0.8042169744807078\n",
      "Epoch 23, Training Loss: 0.8033069776413136\n",
      "Epoch 24, Training Loss: 0.8042820063748755\n",
      "Epoch 25, Training Loss: 0.8026205753921566\n",
      "Epoch 26, Training Loss: 0.8026243207149936\n",
      "Epoch 27, Training Loss: 0.8026758884128772\n",
      "Epoch 28, Training Loss: 0.8020627889418064\n",
      "Epoch 29, Training Loss: 0.8024058556198177\n",
      "Epoch 30, Training Loss: 0.802983005781819\n",
      "Epoch 31, Training Loss: 0.802566691628076\n",
      "Epoch 32, Training Loss: 0.8026883017747922\n",
      "Epoch 33, Training Loss: 0.8026187477255227\n",
      "Epoch 34, Training Loss: 0.8011583524539059\n",
      "Epoch 35, Training Loss: 0.8023633373411079\n",
      "Epoch 36, Training Loss: 0.8004499764370739\n",
      "Epoch 37, Training Loss: 0.801308910828784\n",
      "Epoch 38, Training Loss: 0.800869353731772\n",
      "Epoch 39, Training Loss: 0.8006889350432203\n",
      "Epoch 40, Training Loss: 0.8011853434985742\n",
      "Epoch 41, Training Loss: 0.8002748700909148\n",
      "Epoch 42, Training Loss: 0.801947469818861\n",
      "Epoch 43, Training Loss: 0.8006592169740147\n",
      "Epoch 44, Training Loss: 0.801712343388034\n",
      "Epoch 45, Training Loss: 0.799909836546819\n",
      "Epoch 46, Training Loss: 0.7995507451824676\n",
      "Epoch 47, Training Loss: 0.8011902369950947\n",
      "Epoch 48, Training Loss: 0.8000806201669507\n",
      "Epoch 49, Training Loss: 0.8006111917639137\n",
      "Epoch 50, Training Loss: 0.8002119643347604\n",
      "Epoch 51, Training Loss: 0.7996417248159423\n",
      "Epoch 52, Training Loss: 0.7997099060761301\n",
      "Epoch 53, Training Loss: 0.8000805760684766\n",
      "Epoch 54, Training Loss: 0.8006918339801014\n",
      "Epoch 55, Training Loss: 0.799851916159006\n",
      "Epoch 56, Training Loss: 0.7994915402921519\n",
      "Epoch 57, Training Loss: 0.8004844443242353\n",
      "Epoch 58, Training Loss: 0.7995714513879073\n",
      "Epoch 59, Training Loss: 0.799727341644746\n",
      "Epoch 60, Training Loss: 0.7996907644702079\n",
      "Epoch 61, Training Loss: 0.799410661539637\n",
      "Epoch 62, Training Loss: 0.8001461874273487\n",
      "Epoch 63, Training Loss: 0.7991457925703292\n",
      "Epoch 64, Training Loss: 0.7991330620041467\n",
      "Epoch 65, Training Loss: 0.7994248739758828\n",
      "Epoch 66, Training Loss: 0.7995648115201104\n",
      "Epoch 67, Training Loss: 0.799440936665786\n",
      "Epoch 68, Training Loss: 0.7987200000232324\n",
      "Epoch 69, Training Loss: 0.7989882026399885\n",
      "Epoch 70, Training Loss: 0.7985458564937563\n",
      "Epoch 71, Training Loss: 0.7987430666622363\n",
      "Epoch 72, Training Loss: 0.7987831593456125\n",
      "Epoch 73, Training Loss: 0.7986452851976668\n",
      "Epoch 74, Training Loss: 0.7983334253605147\n",
      "Epoch 75, Training Loss: 0.798281374909824\n",
      "Epoch 76, Training Loss: 0.7982192320034917\n",
      "Epoch 77, Training Loss: 0.7982791912734957\n",
      "Epoch 78, Training Loss: 0.7991591197207458\n",
      "Epoch 79, Training Loss: 0.7984450885675903\n",
      "Epoch 80, Training Loss: 0.7979430720322114\n",
      "Epoch 81, Training Loss: 0.7976024562254884\n",
      "Epoch 82, Training Loss: 0.7991180773068192\n",
      "Epoch 83, Training Loss: 0.797798350341338\n",
      "Epoch 84, Training Loss: 0.7979416968230915\n",
      "Epoch 85, Training Loss: 0.7981196072764862\n",
      "Epoch 86, Training Loss: 0.7991965704394463\n",
      "Epoch 87, Training Loss: 0.7976431827796133\n",
      "Epoch 88, Training Loss: 0.7991625167373427\n",
      "Epoch 89, Training Loss: 0.7983346583251666\n",
      "Epoch 90, Training Loss: 0.797703997084969\n",
      "Epoch 91, Training Loss: 0.7982099620023169\n",
      "Epoch 92, Training Loss: 0.7981385256114759\n",
      "Epoch 93, Training Loss: 0.7982509449908608\n",
      "Epoch 94, Training Loss: 0.7977656581348046\n",
      "Epoch 95, Training Loss: 0.7973558949348621\n",
      "Epoch 96, Training Loss: 0.7982586905472261\n",
      "Epoch 97, Training Loss: 0.7981777019070503\n",
      "Epoch 98, Training Loss: 0.7979994174233056\n",
      "Epoch 99, Training Loss: 0.7985586770495078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:42:58,381] Trial 53 finished with value: 0.5933333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7986356677865624\n",
      "Epoch 1, Training Loss: 0.989526563389857\n",
      "Epoch 2, Training Loss: 0.9554398310811896\n",
      "Epoch 3, Training Loss: 0.9484891567911421\n",
      "Epoch 4, Training Loss: 0.9419607549681699\n",
      "Epoch 5, Training Loss: 0.9347422960109281\n",
      "Epoch 6, Training Loss: 0.9278627473608891\n",
      "Epoch 7, Training Loss: 0.9206339629072892\n",
      "Epoch 8, Training Loss: 0.9127911367810758\n",
      "Epoch 9, Training Loss: 0.9061620249784083\n",
      "Epoch 10, Training Loss: 0.8979171334352708\n",
      "Epoch 11, Training Loss: 0.88993566520232\n",
      "Epoch 12, Training Loss: 0.8820062830035847\n",
      "Epoch 13, Training Loss: 0.874243878870082\n",
      "Epoch 14, Training Loss: 0.8677085125356688\n",
      "Epoch 15, Training Loss: 0.8603983270494562\n",
      "Epoch 16, Training Loss: 0.8539083194911928\n",
      "Epoch 17, Training Loss: 0.8481390910937374\n",
      "Epoch 18, Training Loss: 0.8424724786801445\n",
      "Epoch 19, Training Loss: 0.8378911222730364\n",
      "Epoch 20, Training Loss: 0.834270153189064\n",
      "Epoch 21, Training Loss: 0.8305903254595018\n",
      "Epoch 22, Training Loss: 0.8274316220355213\n",
      "Epoch 23, Training Loss: 0.8244524601706885\n",
      "Epoch 24, Training Loss: 0.8228038106645856\n",
      "Epoch 25, Training Loss: 0.8206368599619184\n",
      "Epoch 26, Training Loss: 0.8185439330294616\n",
      "Epoch 27, Training Loss: 0.8172850232375296\n",
      "Epoch 28, Training Loss: 0.8164405727744999\n",
      "Epoch 29, Training Loss: 0.8151899690914871\n",
      "Epoch 30, Training Loss: 0.8138210380884041\n",
      "Epoch 31, Training Loss: 0.8131706943189291\n",
      "Epoch 32, Training Loss: 0.8126126726767174\n",
      "Epoch 33, Training Loss: 0.8117856171794404\n",
      "Epoch 34, Training Loss: 0.8116315916964882\n",
      "Epoch 35, Training Loss: 0.8111774795933774\n",
      "Epoch 36, Training Loss: 0.8101745008525992\n",
      "Epoch 37, Training Loss: 0.81013254456054\n",
      "Epoch 38, Training Loss: 0.8096896014715496\n",
      "Epoch 39, Training Loss: 0.8100374359833566\n",
      "Epoch 40, Training Loss: 0.8090504219657496\n",
      "Epoch 41, Training Loss: 0.8092539277291836\n",
      "Epoch 42, Training Loss: 0.8082859095774199\n",
      "Epoch 43, Training Loss: 0.8083196375603067\n",
      "Epoch 44, Training Loss: 0.8077012222512324\n",
      "Epoch 45, Training Loss: 0.8077063596338258\n",
      "Epoch 46, Training Loss: 0.8073080963658211\n",
      "Epoch 47, Training Loss: 0.8072235456086639\n",
      "Epoch 48, Training Loss: 0.8070204809196013\n",
      "Epoch 49, Training Loss: 0.8067127432141985\n",
      "Epoch 50, Training Loss: 0.8062238146487931\n",
      "Epoch 51, Training Loss: 0.8062547423785791\n",
      "Epoch 52, Training Loss: 0.8059787372001131\n",
      "Epoch 53, Training Loss: 0.8057585544155953\n",
      "Epoch 54, Training Loss: 0.8055613582295583\n",
      "Epoch 55, Training Loss: 0.8052890514072619\n",
      "Epoch 56, Training Loss: 0.80539579606594\n",
      "Epoch 57, Training Loss: 0.8052823148275676\n",
      "Epoch 58, Training Loss: 0.8046476045049222\n",
      "Epoch 59, Training Loss: 0.8057491401084383\n",
      "Epoch 60, Training Loss: 0.8050032390687699\n",
      "Epoch 61, Training Loss: 0.8045457012671277\n",
      "Epoch 62, Training Loss: 0.804376753111531\n",
      "Epoch 63, Training Loss: 0.8043988527211928\n",
      "Epoch 64, Training Loss: 0.8043134088803054\n",
      "Epoch 65, Training Loss: 0.8038705936051849\n",
      "Epoch 66, Training Loss: 0.8036095294737278\n",
      "Epoch 67, Training Loss: 0.8035192506653922\n",
      "Epoch 68, Training Loss: 0.8037464512021918\n",
      "Epoch 69, Training Loss: 0.8038102196571523\n",
      "Epoch 70, Training Loss: 0.8035852966452004\n",
      "Epoch 71, Training Loss: 0.8032053529768062\n",
      "Epoch 72, Training Loss: 0.8036520098385058\n",
      "Epoch 73, Training Loss: 0.8028325751311797\n",
      "Epoch 74, Training Loss: 0.8024161973842104\n",
      "Epoch 75, Training Loss: 0.8026230072616635\n",
      "Epoch 76, Training Loss: 0.8026349960413194\n",
      "Epoch 77, Training Loss: 0.8023659326976403\n",
      "Epoch 78, Training Loss: 0.8021862381382993\n",
      "Epoch 79, Training Loss: 0.8024752903701667\n",
      "Epoch 80, Training Loss: 0.8020052245685032\n",
      "Epoch 81, Training Loss: 0.8016203328182823\n",
      "Epoch 82, Training Loss: 0.8021681160855114\n",
      "Epoch 83, Training Loss: 0.802023975114177\n",
      "Epoch 84, Training Loss: 0.8022764876372832\n",
      "Epoch 85, Training Loss: 0.8014487057700193\n",
      "Epoch 86, Training Loss: 0.8016363416399275\n",
      "Epoch 87, Training Loss: 0.8014545076771786\n",
      "Epoch 88, Training Loss: 0.8015715489710183\n",
      "Epoch 89, Training Loss: 0.8013280025102142\n",
      "Epoch 90, Training Loss: 0.8015605777726138\n",
      "Epoch 91, Training Loss: 0.801512863313345\n",
      "Epoch 92, Training Loss: 0.8013453946974044\n",
      "Epoch 93, Training Loss: 0.8018364717189531\n",
      "Epoch 94, Training Loss: 0.8008997055820952\n",
      "Epoch 95, Training Loss: 0.8008046086569478\n",
      "Epoch 96, Training Loss: 0.800247959131585\n",
      "Epoch 97, Training Loss: 0.8006276077793953\n",
      "Epoch 98, Training Loss: 0.8003836079647667\n",
      "Epoch 99, Training Loss: 0.8001054236763402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:44:20,826] Trial 54 finished with value: 0.6332666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8005764219993935\n",
      "Epoch 1, Training Loss: 0.9034761807076017\n",
      "Epoch 2, Training Loss: 0.8238061757912313\n",
      "Epoch 3, Training Loss: 0.8164165644717396\n",
      "Epoch 4, Training Loss: 0.812288973295599\n",
      "Epoch 5, Training Loss: 0.8069065800286773\n",
      "Epoch 6, Training Loss: 0.8029820219914716\n",
      "Epoch 7, Training Loss: 0.8010820400445982\n",
      "Epoch 8, Training Loss: 0.7984100715558332\n",
      "Epoch 9, Training Loss: 0.7967939044299879\n",
      "Epoch 10, Training Loss: 0.7958970016106627\n",
      "Epoch 11, Training Loss: 0.7952692976571564\n",
      "Epoch 12, Training Loss: 0.7929829873088607\n",
      "Epoch 13, Training Loss: 0.7929887638056189\n",
      "Epoch 14, Training Loss: 0.7922206408099124\n",
      "Epoch 15, Training Loss: 0.7916353757220104\n",
      "Epoch 16, Training Loss: 0.7902528958213061\n",
      "Epoch 17, Training Loss: 0.7914179634330865\n",
      "Epoch 18, Training Loss: 0.7908280531266578\n",
      "Epoch 19, Training Loss: 0.7903388042198984\n",
      "Epoch 20, Training Loss: 0.7895665106020475\n",
      "Epoch 21, Training Loss: 0.7887672712928371\n",
      "Epoch 22, Training Loss: 0.7900144750014284\n",
      "Epoch 23, Training Loss: 0.7890394603399405\n",
      "Epoch 24, Training Loss: 0.7883899748325348\n",
      "Epoch 25, Training Loss: 0.7876443585506956\n",
      "Epoch 26, Training Loss: 0.7882831312659988\n",
      "Epoch 27, Training Loss: 0.7877620346564099\n",
      "Epoch 28, Training Loss: 0.7877154538505956\n",
      "Epoch 29, Training Loss: 0.787664248441395\n",
      "Epoch 30, Training Loss: 0.7871995611298354\n",
      "Epoch 31, Training Loss: 0.7873468882159183\n",
      "Epoch 32, Training Loss: 0.7870875584451775\n",
      "Epoch 33, Training Loss: 0.7872251022130923\n",
      "Epoch 34, Training Loss: 0.7860670337999673\n",
      "Epoch 35, Training Loss: 0.7861527502088619\n",
      "Epoch 36, Training Loss: 0.7853278092871931\n",
      "Epoch 37, Training Loss: 0.7869727884916434\n",
      "Epoch 38, Training Loss: 0.7861536714367401\n",
      "Epoch 39, Training Loss: 0.7861643521409286\n",
      "Epoch 40, Training Loss: 0.7862384261045241\n",
      "Epoch 41, Training Loss: 0.7854083595419289\n",
      "Epoch 42, Training Loss: 0.7852611286299569\n",
      "Epoch 43, Training Loss: 0.7866499661502981\n",
      "Epoch 44, Training Loss: 0.7850338061949365\n",
      "Epoch 45, Training Loss: 0.7859077388182618\n",
      "Epoch 46, Training Loss: 0.7846896455700236\n",
      "Epoch 47, Training Loss: 0.7848680388658567\n",
      "Epoch 48, Training Loss: 0.7850829851358456\n",
      "Epoch 49, Training Loss: 0.7848085005480544\n",
      "Epoch 50, Training Loss: 0.7850223133438512\n",
      "Epoch 51, Training Loss: 0.7839072663981216\n",
      "Epoch 52, Training Loss: 0.7843143475683112\n",
      "Epoch 53, Training Loss: 0.7848222984407182\n",
      "Epoch 54, Training Loss: 0.7841904366823067\n",
      "Epoch 55, Training Loss: 0.7844013004374684\n",
      "Epoch 56, Training Loss: 0.7842772985759534\n",
      "Epoch 57, Training Loss: 0.7838561663950296\n",
      "Epoch 58, Training Loss: 0.7843305495448578\n",
      "Epoch 59, Training Loss: 0.7838171376321549\n",
      "Epoch 60, Training Loss: 0.7840513656910201\n",
      "Epoch 61, Training Loss: 0.7831130422147593\n",
      "Epoch 62, Training Loss: 0.783296890948948\n",
      "Epoch 63, Training Loss: 0.7836911397769039\n",
      "Epoch 64, Training Loss: 0.783786376078326\n",
      "Epoch 65, Training Loss: 0.7830003922146962\n",
      "Epoch 66, Training Loss: 0.7841409045950811\n",
      "Epoch 67, Training Loss: 0.7831499348905749\n",
      "Epoch 68, Training Loss: 0.7831539621926789\n",
      "Epoch 69, Training Loss: 0.7832508541587601\n",
      "Epoch 70, Training Loss: 0.7833738483880696\n",
      "Epoch 71, Training Loss: 0.783001320702689\n",
      "Epoch 72, Training Loss: 0.7822849371379479\n",
      "Epoch 73, Training Loss: 0.7825054537981077\n",
      "Epoch 74, Training Loss: 0.7828113794326782\n",
      "Epoch 75, Training Loss: 0.7826191030050579\n",
      "Epoch 76, Training Loss: 0.7830061183836227\n",
      "Epoch 77, Training Loss: 0.7826242733718758\n",
      "Epoch 78, Training Loss: 0.7826063892895118\n",
      "Epoch 79, Training Loss: 0.7832230547316988\n",
      "Epoch 80, Training Loss: 0.782646684270156\n",
      "Epoch 81, Training Loss: 0.7827723426926405\n",
      "Epoch 82, Training Loss: 0.7817520596030959\n",
      "Epoch 83, Training Loss: 0.7828430464393215\n",
      "Epoch 84, Training Loss: 0.7827739206471838\n",
      "Epoch 85, Training Loss: 0.7828304627784213\n",
      "Epoch 86, Training Loss: 0.782584089623358\n",
      "Epoch 87, Training Loss: 0.7817192398515859\n",
      "Epoch 88, Training Loss: 0.7818801109055827\n",
      "Epoch 89, Training Loss: 0.7811093035945319\n",
      "Epoch 90, Training Loss: 0.7813676948386027\n",
      "Epoch 91, Training Loss: 0.7811472299851869\n",
      "Epoch 92, Training Loss: 0.7817206043049805\n",
      "Epoch 93, Training Loss: 0.7826102423488646\n",
      "Epoch 94, Training Loss: 0.7819624959974361\n",
      "Epoch 95, Training Loss: 0.7811024608468651\n",
      "Epoch 96, Training Loss: 0.7809107796590131\n",
      "Epoch 97, Training Loss: 0.7814285636844491\n",
      "Epoch 98, Training Loss: 0.7819970780745484\n",
      "Epoch 99, Training Loss: 0.7810782343821419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:46:11,052] Trial 55 finished with value: 0.6327333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7806059391875017\n",
      "Epoch 1, Training Loss: 0.8678142466264612\n",
      "Epoch 2, Training Loss: 0.8139096431171193\n",
      "Epoch 3, Training Loss: 0.8059179953967823\n",
      "Epoch 4, Training Loss: 0.8028107060404385\n",
      "Epoch 5, Training Loss: 0.800716211234822\n",
      "Epoch 6, Training Loss: 0.7982353514783522\n",
      "Epoch 7, Training Loss: 0.7971542742673089\n",
      "Epoch 8, Training Loss: 0.7970481592767379\n",
      "Epoch 9, Training Loss: 0.7955313397155088\n",
      "Epoch 10, Training Loss: 0.7942832688724294\n",
      "Epoch 11, Training Loss: 0.7939270893265219\n",
      "Epoch 12, Training Loss: 0.7933229430984048\n",
      "Epoch 13, Training Loss: 0.7924708447736852\n",
      "Epoch 14, Training Loss: 0.7930500771017636\n",
      "Epoch 15, Training Loss: 0.7923411210144268\n",
      "Epoch 16, Training Loss: 0.7920455057480756\n",
      "Epoch 17, Training Loss: 0.791447811126709\n",
      "Epoch 18, Training Loss: 0.791463932430043\n",
      "Epoch 19, Training Loss: 0.7902833205110886\n",
      "Epoch 20, Training Loss: 0.7907837021350861\n",
      "Epoch 21, Training Loss: 0.7909242280791787\n",
      "Epoch 22, Training Loss: 0.7902308243863723\n",
      "Epoch 23, Training Loss: 0.789353927934871\n",
      "Epoch 24, Training Loss: 0.789451420657775\n",
      "Epoch 25, Training Loss: 0.7889921874158523\n",
      "Epoch 26, Training Loss: 0.7892325591339785\n",
      "Epoch 27, Training Loss: 0.7894341658143437\n",
      "Epoch 28, Training Loss: 0.7887593674659729\n",
      "Epoch 29, Training Loss: 0.7885069504906149\n",
      "Epoch 30, Training Loss: 0.7880903065905851\n",
      "Epoch 31, Training Loss: 0.7876948161686168\n",
      "Epoch 32, Training Loss: 0.788092515188105\n",
      "Epoch 33, Training Loss: 0.7881109984482035\n",
      "Epoch 34, Training Loss: 0.7876012794410481\n",
      "Epoch 35, Training Loss: 0.7868751129683327\n",
      "Epoch 36, Training Loss: 0.787104478373247\n",
      "Epoch 37, Training Loss: 0.7883766473040861\n",
      "Epoch 38, Training Loss: 0.7882909631729126\n",
      "Epoch 39, Training Loss: 0.7867786496527055\n",
      "Epoch 40, Training Loss: 0.7863059440079857\n",
      "Epoch 41, Training Loss: 0.7866087200361139\n",
      "Epoch 42, Training Loss: 0.7879874735720017\n",
      "Epoch 43, Training Loss: 0.7876908156451057\n",
      "Epoch 44, Training Loss: 0.7863653104445514\n",
      "Epoch 45, Training Loss: 0.786586309110417\n",
      "Epoch 46, Training Loss: 0.7867809768284069\n",
      "Epoch 47, Training Loss: 0.7864080565115985\n",
      "Epoch 48, Training Loss: 0.7860007858276368\n",
      "Epoch 49, Training Loss: 0.7860530192711774\n",
      "Epoch 50, Training Loss: 0.7863713713253245\n",
      "Epoch 51, Training Loss: 0.7855433379902559\n",
      "Epoch 52, Training Loss: 0.7850828534715316\n",
      "Epoch 53, Training Loss: 0.7853001459205852\n",
      "Epoch 54, Training Loss: 0.7854965052183936\n",
      "Epoch 55, Training Loss: 0.7857415401234347\n",
      "Epoch 56, Training Loss: 0.7855551005812252\n",
      "Epoch 57, Training Loss: 0.7857614391691544\n",
      "Epoch 58, Training Loss: 0.7851817531445447\n",
      "Epoch 59, Training Loss: 0.7844496504699483\n",
      "Epoch 60, Training Loss: 0.7855395561106064\n",
      "Epoch 61, Training Loss: 0.7843885814442354\n",
      "Epoch 62, Training Loss: 0.7841777222998002\n",
      "Epoch 63, Training Loss: 0.7846817532006432\n",
      "Epoch 64, Training Loss: 0.784526892339482\n",
      "Epoch 65, Training Loss: 0.7851001577517566\n",
      "Epoch 66, Training Loss: 0.7842740910894731\n",
      "Epoch 67, Training Loss: 0.7841395686654483\n",
      "Epoch 68, Training Loss: 0.784014768039479\n",
      "Epoch 69, Training Loss: 0.7844118442254908\n",
      "Epoch 70, Training Loss: 0.7837556174923392\n",
      "Epoch 71, Training Loss: 0.7840850163207335\n",
      "Epoch 72, Training Loss: 0.7839784158678615\n",
      "Epoch 73, Training Loss: 0.7834016558703254\n",
      "Epoch 74, Training Loss: 0.7838581571158241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:47:49,575] Trial 56 finished with value: 0.6414 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7840609807126663\n",
      "Epoch 1, Training Loss: 0.8951539336232578\n",
      "Epoch 2, Training Loss: 0.8190154483739067\n",
      "Epoch 3, Training Loss: 0.8138016336104449\n",
      "Epoch 4, Training Loss: 0.812387002215666\n",
      "Epoch 5, Training Loss: 0.8105667877197266\n",
      "Epoch 6, Training Loss: 0.8083460450172424\n",
      "Epoch 7, Training Loss: 0.8072849998053382\n",
      "Epoch 8, Training Loss: 0.8068537975760067\n",
      "Epoch 9, Training Loss: 0.8054935415352092\n",
      "Epoch 10, Training Loss: 0.8045923865542692\n",
      "Epoch 11, Training Loss: 0.8046331006639144\n",
      "Epoch 12, Training Loss: 0.8045061839328093\n",
      "Epoch 13, Training Loss: 0.8040419524557451\n",
      "Epoch 14, Training Loss: 0.8032687775527729\n",
      "Epoch 15, Training Loss: 0.8031602811112123\n",
      "Epoch 16, Training Loss: 0.8022257535597858\n",
      "Epoch 17, Training Loss: 0.8026988034388598\n",
      "Epoch 18, Training Loss: 0.8022594468733844\n",
      "Epoch 19, Training Loss: 0.8015424834980684\n",
      "Epoch 20, Training Loss: 0.8021354077844058\n",
      "Epoch 21, Training Loss: 0.8012296462059021\n",
      "Epoch 22, Training Loss: 0.8015949686835794\n",
      "Epoch 23, Training Loss: 0.8016734496284934\n",
      "Epoch 24, Training Loss: 0.8006205291607801\n",
      "Epoch 25, Training Loss: 0.8010641638671651\n",
      "Epoch 26, Training Loss: 0.8010460538723889\n",
      "Epoch 27, Training Loss: 0.8008289154838113\n",
      "Epoch 28, Training Loss: 0.8009632409320158\n",
      "Epoch 29, Training Loss: 0.8005282306671142\n",
      "Epoch 30, Training Loss: 0.7999882426682641\n",
      "Epoch 31, Training Loss: 0.8001062867220711\n",
      "Epoch 32, Training Loss: 0.8009569056595073\n",
      "Epoch 33, Training Loss: 0.7995048369379605\n",
      "Epoch 34, Training Loss: 0.7992889431644888\n",
      "Epoch 35, Training Loss: 0.7997169624356663\n",
      "Epoch 36, Training Loss: 0.7997228398042566\n",
      "Epoch 37, Training Loss: 0.7996425204417285\n",
      "Epoch 38, Training Loss: 0.7992814523332259\n",
      "Epoch 39, Training Loss: 0.7990357814115636\n",
      "Epoch 40, Training Loss: 0.7993620938413284\n",
      "Epoch 41, Training Loss: 0.7986915245476891\n",
      "Epoch 42, Training Loss: 0.7984916636523078\n",
      "Epoch 43, Training Loss: 0.7988002443313599\n",
      "Epoch 44, Training Loss: 0.7979227440497455\n",
      "Epoch 45, Training Loss: 0.7979423513833215\n",
      "Epoch 46, Training Loss: 0.797998441808364\n",
      "Epoch 47, Training Loss: 0.7980925915521734\n",
      "Epoch 48, Training Loss: 0.7982719687153311\n",
      "Epoch 49, Training Loss: 0.7976454776876113\n",
      "Epoch 50, Training Loss: 0.7969186320024378\n",
      "Epoch 51, Training Loss: 0.7975360408250023\n",
      "Epoch 52, Training Loss: 0.7961427554663489\n",
      "Epoch 53, Training Loss: 0.7966483494814705\n",
      "Epoch 54, Training Loss: 0.7962179382408366\n",
      "Epoch 55, Training Loss: 0.796487452212502\n",
      "Epoch 56, Training Loss: 0.7964556668786441\n",
      "Epoch 57, Training Loss: 0.7961691485433018\n",
      "Epoch 58, Training Loss: 0.7961214712788077\n",
      "Epoch 59, Training Loss: 0.795781453146654\n",
      "Epoch 60, Training Loss: 0.7956970641192268\n",
      "Epoch 61, Training Loss: 0.7957391164583318\n",
      "Epoch 62, Training Loss: 0.7951064566303702\n",
      "Epoch 63, Training Loss: 0.7950597818458781\n",
      "Epoch 64, Training Loss: 0.7958942755530862\n",
      "Epoch 65, Training Loss: 0.7957963453320895\n",
      "Epoch 66, Training Loss: 0.79466087846195\n",
      "Epoch 67, Training Loss: 0.795962769985199\n",
      "Epoch 68, Training Loss: 0.7954062088798074\n",
      "Epoch 69, Training Loss: 0.7949440388118519\n",
      "Epoch 70, Training Loss: 0.7955574339277605\n",
      "Epoch 71, Training Loss: 0.7944330846562105\n",
      "Epoch 72, Training Loss: 0.7955123861397014\n",
      "Epoch 73, Training Loss: 0.7946570609597599\n",
      "Epoch 74, Training Loss: 0.7946798785995035\n",
      "Epoch 75, Training Loss: 0.7941614371187546\n",
      "Epoch 76, Training Loss: 0.7937957935473499\n",
      "Epoch 77, Training Loss: 0.7936881648091709\n",
      "Epoch 78, Training Loss: 0.7934135669820449\n",
      "Epoch 79, Training Loss: 0.7939676565282485\n",
      "Epoch 80, Training Loss: 0.7935344245854546\n",
      "Epoch 81, Training Loss: 0.7940449597555048\n",
      "Epoch 82, Training Loss: 0.7928679154900944\n",
      "Epoch 83, Training Loss: 0.7932175688182607\n",
      "Epoch 84, Training Loss: 0.793276851457708\n",
      "Epoch 85, Training Loss: 0.79286656183355\n",
      "Epoch 86, Training Loss: 0.7931598769215976\n",
      "Epoch 87, Training Loss: 0.7927439890188329\n",
      "Epoch 88, Training Loss: 0.7928036316002116\n",
      "Epoch 89, Training Loss: 0.792604285548715\n",
      "Epoch 90, Training Loss: 0.7927645928018233\n",
      "Epoch 91, Training Loss: 0.7929462168497198\n",
      "Epoch 92, Training Loss: 0.7927565578152151\n",
      "Epoch 93, Training Loss: 0.7926792519934037\n",
      "Epoch 94, Training Loss: 0.792035342244541\n",
      "Epoch 95, Training Loss: 0.7916885241340188\n",
      "Epoch 96, Training Loss: 0.7918567648354699\n",
      "Epoch 97, Training Loss: 0.7920766268758213\n",
      "Epoch 98, Training Loss: 0.7920588562067817\n",
      "Epoch 99, Training Loss: 0.7915708295737995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:49:45,331] Trial 57 finished with value: 0.6359333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7924100321881912\n",
      "Epoch 1, Training Loss: 0.8649585720370797\n",
      "Epoch 2, Training Loss: 0.8160021044226253\n",
      "Epoch 3, Training Loss: 0.8117959732167861\n",
      "Epoch 4, Training Loss: 0.8096638378676246\n",
      "Epoch 5, Training Loss: 0.8070788288116455\n",
      "Epoch 6, Training Loss: 0.8051945007548613\n",
      "Epoch 7, Training Loss: 0.8046686867405387\n",
      "Epoch 8, Training Loss: 0.8036085533394534\n",
      "Epoch 9, Training Loss: 0.8024729723790113\n",
      "Epoch 10, Training Loss: 0.8021654752422781\n",
      "Epoch 11, Training Loss: 0.8019642045217402\n",
      "Epoch 12, Training Loss: 0.8009919016501483\n",
      "Epoch 13, Training Loss: 0.8006122675362756\n",
      "Epoch 14, Training Loss: 0.8003189535701976\n",
      "Epoch 15, Training Loss: 0.7995662494967966\n",
      "Epoch 16, Training Loss: 0.7993461921635796\n",
      "Epoch 17, Training Loss: 0.7990768328133752\n",
      "Epoch 18, Training Loss: 0.7987274650966419\n",
      "Epoch 19, Training Loss: 0.797946430865456\n",
      "Epoch 20, Training Loss: 0.7978597662729375\n",
      "Epoch 21, Training Loss: 0.7969540781834547\n",
      "Epoch 22, Training Loss: 0.796498150545008\n",
      "Epoch 23, Training Loss: 0.795668979322209\n",
      "Epoch 24, Training Loss: 0.7949152912813074\n",
      "Epoch 25, Training Loss: 0.7940082477120792\n",
      "Epoch 26, Training Loss: 0.7933865774379057\n",
      "Epoch 27, Training Loss: 0.7924797434666577\n",
      "Epoch 28, Training Loss: 0.7919452040335712\n",
      "Epoch 29, Training Loss: 0.7907967716104845\n",
      "Epoch 30, Training Loss: 0.7904488288655\n",
      "Epoch 31, Training Loss: 0.7894195805577671\n",
      "Epoch 32, Training Loss: 0.7887375195587383\n",
      "Epoch 33, Training Loss: 0.7881613995748408\n",
      "Epoch 34, Training Loss: 0.7875364122671239\n",
      "Epoch 35, Training Loss: 0.7870832432718838\n",
      "Epoch 36, Training Loss: 0.7870553750150344\n",
      "Epoch 37, Training Loss: 0.7867257581738865\n",
      "Epoch 38, Training Loss: 0.7860941023686353\n",
      "Epoch 39, Training Loss: 0.7860161861952614\n",
      "Epoch 40, Training Loss: 0.7858759492285111\n",
      "Epoch 41, Training Loss: 0.7857113395017736\n",
      "Epoch 42, Training Loss: 0.7855942222651313\n",
      "Epoch 43, Training Loss: 0.785391893527087\n",
      "Epoch 44, Training Loss: 0.7851742573345408\n",
      "Epoch 45, Training Loss: 0.784973467167686\n",
      "Epoch 46, Training Loss: 0.7847185713403365\n",
      "Epoch 47, Training Loss: 0.7846911526427549\n",
      "Epoch 48, Training Loss: 0.7845122714603648\n",
      "Epoch 49, Training Loss: 0.7844484406359056\n",
      "Epoch 50, Training Loss: 0.784170442609226\n",
      "Epoch 51, Training Loss: 0.7844253277778626\n",
      "Epoch 52, Training Loss: 0.7839665245308596\n",
      "Epoch 53, Training Loss: 0.7840324798051048\n",
      "Epoch 54, Training Loss: 0.7838318628423354\n",
      "Epoch 55, Training Loss: 0.7836830888776218\n",
      "Epoch 56, Training Loss: 0.7834583044753355\n",
      "Epoch 57, Training Loss: 0.7833964685832753\n",
      "Epoch 58, Training Loss: 0.7835746476930731\n",
      "Epoch 59, Training Loss: 0.7834564062427072\n",
      "Epoch 60, Training Loss: 0.7832354868860806\n",
      "Epoch 61, Training Loss: 0.7830277522171245\n",
      "Epoch 62, Training Loss: 0.7831089804453009\n",
      "Epoch 63, Training Loss: 0.7829375440934125\n",
      "Epoch 64, Training Loss: 0.7829792836834403\n",
      "Epoch 65, Training Loss: 0.7828514375406153\n",
      "Epoch 66, Training Loss: 0.7827438666540034\n",
      "Epoch 67, Training Loss: 0.78293957955697\n",
      "Epoch 68, Training Loss: 0.7828659031671636\n",
      "Epoch 69, Training Loss: 0.7825853478908539\n",
      "Epoch 70, Training Loss: 0.782485288030961\n",
      "Epoch 71, Training Loss: 0.7821201897368711\n",
      "Epoch 72, Training Loss: 0.7826608016911675\n",
      "Epoch 73, Training Loss: 0.7823490046753603\n",
      "Epoch 74, Training Loss: 0.7822966041985681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:51:18,583] Trial 58 finished with value: 0.6386666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7825454915271086\n",
      "Epoch 1, Training Loss: 1.102175481695878\n",
      "Epoch 2, Training Loss: 1.0907550205861716\n",
      "Epoch 3, Training Loss: 1.090467763843393\n",
      "Epoch 4, Training Loss: 1.0901913452865486\n",
      "Epoch 5, Training Loss: 1.0899938284006334\n",
      "Epoch 6, Training Loss: 1.0896750489571938\n",
      "Epoch 7, Training Loss: 1.0894005155204831\n",
      "Epoch 8, Training Loss: 1.0892130514732876\n",
      "Epoch 9, Training Loss: 1.0888388085186034\n",
      "Epoch 10, Training Loss: 1.0887109087822133\n",
      "Epoch 11, Training Loss: 1.088317340477965\n",
      "Epoch 12, Training Loss: 1.0879002486852776\n",
      "Epoch 13, Training Loss: 1.0877026937957992\n",
      "Epoch 14, Training Loss: 1.087439775646181\n",
      "Epoch 15, Training Loss: 1.087103141698622\n",
      "Epoch 16, Training Loss: 1.086800973935235\n",
      "Epoch 17, Training Loss: 1.0865681940451601\n",
      "Epoch 18, Training Loss: 1.0861795068683482\n",
      "Epoch 19, Training Loss: 1.0861617513169024\n",
      "Epoch 20, Training Loss: 1.0856735788789906\n",
      "Epoch 21, Training Loss: 1.0854282472366676\n",
      "Epoch 22, Training Loss: 1.0850212889506405\n",
      "Epoch 23, Training Loss: 1.0847716690006113\n",
      "Epoch 24, Training Loss: 1.084419037524919\n",
      "Epoch 25, Training Loss: 1.0842111727348844\n",
      "Epoch 26, Training Loss: 1.0837831025733087\n",
      "Epoch 27, Training Loss: 1.0834872444769494\n",
      "Epoch 28, Training Loss: 1.0833228043147496\n",
      "Epoch 29, Training Loss: 1.082785233519131\n",
      "Epoch 30, Training Loss: 1.082570815444889\n",
      "Epoch 31, Training Loss: 1.0820965835026333\n",
      "Epoch 32, Training Loss: 1.0818303199638997\n",
      "Epoch 33, Training Loss: 1.081455688010481\n",
      "Epoch 34, Training Loss: 1.081140597063796\n",
      "Epoch 35, Training Loss: 1.0808325378518355\n",
      "Epoch 36, Training Loss: 1.0803868383393251\n",
      "Epoch 37, Training Loss: 1.0799522308478677\n",
      "Epoch 38, Training Loss: 1.0795101386263855\n",
      "Epoch 39, Training Loss: 1.0793784100310246\n",
      "Epoch 40, Training Loss: 1.0788177124539713\n",
      "Epoch 41, Training Loss: 1.0783859018096351\n",
      "Epoch 42, Training Loss: 1.0781415788750899\n",
      "Epoch 43, Training Loss: 1.0776877123610418\n",
      "Epoch 44, Training Loss: 1.07724852759139\n",
      "Epoch 45, Training Loss: 1.0767495705669088\n",
      "Epoch 46, Training Loss: 1.0762620296693386\n",
      "Epoch 47, Training Loss: 1.0759079899106707\n",
      "Epoch 48, Training Loss: 1.0753844617900992\n",
      "Epoch 49, Training Loss: 1.0750006939235486\n",
      "Epoch 50, Training Loss: 1.0744693171709103\n",
      "Epoch 51, Training Loss: 1.0739888616074296\n",
      "Epoch 52, Training Loss: 1.0735856412944937\n",
      "Epoch 53, Training Loss: 1.0729154886159682\n",
      "Epoch 54, Training Loss: 1.0724737161980535\n",
      "Epoch 55, Training Loss: 1.0719696742251403\n",
      "Epoch 56, Training Loss: 1.0713433048778906\n",
      "Epoch 57, Training Loss: 1.0707749259203\n",
      "Epoch 58, Training Loss: 1.070401652414996\n",
      "Epoch 59, Training Loss: 1.069756947961965\n",
      "Epoch 60, Training Loss: 1.0691033383061115\n",
      "Epoch 61, Training Loss: 1.068470383048954\n",
      "Epoch 62, Training Loss: 1.0679264746214214\n",
      "Epoch 63, Training Loss: 1.0673061478406862\n",
      "Epoch 64, Training Loss: 1.0666006767660154\n",
      "Epoch 65, Training Loss: 1.0660745930850954\n",
      "Epoch 66, Training Loss: 1.0655258158992107\n",
      "Epoch 67, Training Loss: 1.0646119594573975\n",
      "Epoch 68, Training Loss: 1.0640673519077157\n",
      "Epoch 69, Training Loss: 1.063265828082436\n",
      "Epoch 70, Training Loss: 1.0625487155484077\n",
      "Epoch 71, Training Loss: 1.061925804883914\n",
      "Epoch 72, Training Loss: 1.0611933747628577\n",
      "Epoch 73, Training Loss: 1.0601952980335494\n",
      "Epoch 74, Training Loss: 1.0595560833923798\n",
      "Epoch 75, Training Loss: 1.0588254962648664\n",
      "Epoch 76, Training Loss: 1.0580235595989944\n",
      "Epoch 77, Training Loss: 1.0571030476935823\n",
      "Epoch 78, Training Loss: 1.0563885167129057\n",
      "Epoch 79, Training Loss: 1.0555975935512916\n",
      "Epoch 80, Training Loss: 1.0547823672904109\n",
      "Epoch 81, Training Loss: 1.0537339421143208\n",
      "Epoch 82, Training Loss: 1.0529075699641293\n",
      "Epoch 83, Training Loss: 1.0520416417516263\n",
      "Epoch 84, Training Loss: 1.0511697102310067\n",
      "Epoch 85, Training Loss: 1.0501592662997712\n",
      "Epoch 86, Training Loss: 1.0492487520203555\n",
      "Epoch 87, Training Loss: 1.048383807060414\n",
      "Epoch 88, Training Loss: 1.047428948359382\n",
      "Epoch 89, Training Loss: 1.0464524003796112\n",
      "Epoch 90, Training Loss: 1.0454314233665178\n",
      "Epoch 91, Training Loss: 1.0445242949894495\n",
      "Epoch 92, Training Loss: 1.0435461031763178\n",
      "Epoch 93, Training Loss: 1.0425040047867853\n",
      "Epoch 94, Training Loss: 1.0413606260055885\n",
      "Epoch 95, Training Loss: 1.0403582104166647\n",
      "Epoch 96, Training Loss: 1.0392633199691772\n",
      "Epoch 97, Training Loss: 1.0383400823836937\n",
      "Epoch 98, Training Loss: 1.03724621489532\n",
      "Epoch 99, Training Loss: 1.0362016577469675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:52:50,872] Trial 59 finished with value: 0.48133333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.0350485234332263\n",
      "Epoch 1, Training Loss: 0.841539326377381\n",
      "Epoch 2, Training Loss: 0.8069470453979377\n",
      "Epoch 3, Training Loss: 0.8027507459310661\n",
      "Epoch 4, Training Loss: 0.8002463559907181\n",
      "Epoch 5, Training Loss: 0.7984277225974807\n",
      "Epoch 6, Training Loss: 0.7972612593407021\n",
      "Epoch 7, Training Loss: 0.7974478286011775\n",
      "Epoch 8, Training Loss: 0.7967144378145835\n",
      "Epoch 9, Training Loss: 0.7951866588198153\n",
      "Epoch 10, Training Loss: 0.7940394677165755\n",
      "Epoch 11, Training Loss: 0.794994178302306\n",
      "Epoch 12, Training Loss: 0.7934834699881704\n",
      "Epoch 13, Training Loss: 0.7942263221382199\n",
      "Epoch 14, Training Loss: 0.7937214206932183\n",
      "Epoch 15, Training Loss: 0.7923823353939486\n",
      "Epoch 16, Training Loss: 0.7924972641736941\n",
      "Epoch 17, Training Loss: 0.7918240137566301\n",
      "Epoch 18, Training Loss: 0.7908718147672209\n",
      "Epoch 19, Training Loss: 0.7918655974524361\n",
      "Epoch 20, Training Loss: 0.7929648694239164\n",
      "Epoch 21, Training Loss: 0.7911086106658878\n",
      "Epoch 22, Training Loss: 0.7915589744883372\n",
      "Epoch 23, Training Loss: 0.7911679728586871\n",
      "Epoch 24, Training Loss: 0.7907129491182198\n",
      "Epoch 25, Training Loss: 0.7907759216495026\n",
      "Epoch 26, Training Loss: 0.7909970102453591\n",
      "Epoch 27, Training Loss: 0.7898674255923221\n",
      "Epoch 28, Training Loss: 0.7895593858302984\n",
      "Epoch 29, Training Loss: 0.7891647880238698\n",
      "Epoch 30, Training Loss: 0.7888784258885491\n",
      "Epoch 31, Training Loss: 0.7910665768429749\n",
      "Epoch 32, Training Loss: 0.7896145557102404\n",
      "Epoch 33, Training Loss: 0.7890705117605683\n",
      "Epoch 34, Training Loss: 0.7900360040198592\n",
      "Epoch 35, Training Loss: 0.7897148600198273\n",
      "Epoch 36, Training Loss: 0.7885863903769873\n",
      "Epoch 37, Training Loss: 0.7887378701590058\n",
      "Epoch 38, Training Loss: 0.788932702774392\n",
      "Epoch 39, Training Loss: 0.7886954422283889\n",
      "Epoch 40, Training Loss: 0.7884824768044895\n",
      "Epoch 41, Training Loss: 0.7888812891522744\n",
      "Epoch 42, Training Loss: 0.7893852034009489\n",
      "Epoch 43, Training Loss: 0.7897908583619541\n",
      "Epoch 44, Training Loss: 0.7885253298103361\n",
      "Epoch 45, Training Loss: 0.7882522822322702\n",
      "Epoch 46, Training Loss: 0.7885253457198466\n",
      "Epoch 47, Training Loss: 0.7887009777520833\n",
      "Epoch 48, Training Loss: 0.7881674785363046\n",
      "Epoch 49, Training Loss: 0.787870346513906\n",
      "Epoch 50, Training Loss: 0.7882225199749595\n",
      "Epoch 51, Training Loss: 0.7878399701046764\n",
      "Epoch 52, Training Loss: 0.7889790075165884\n",
      "Epoch 53, Training Loss: 0.7884243790368388\n",
      "Epoch 54, Training Loss: 0.7889241540342345\n",
      "Epoch 55, Training Loss: 0.7875352479013286\n",
      "Epoch 56, Training Loss: 0.7884783820102089\n",
      "Epoch 57, Training Loss: 0.7893576388072251\n",
      "Epoch 58, Training Loss: 0.7876452958673463\n",
      "Epoch 59, Training Loss: 0.7885336769254584\n",
      "Epoch 60, Training Loss: 0.7882613597955919\n",
      "Epoch 61, Training Loss: 0.7886733586626842\n",
      "Epoch 62, Training Loss: 0.7880223696393178\n",
      "Epoch 63, Training Loss: 0.7882970154733586\n",
      "Epoch 64, Training Loss: 0.7876920304800334\n",
      "Epoch 65, Training Loss: 0.7891340882258308\n",
      "Epoch 66, Training Loss: 0.7872548007427301\n",
      "Epoch 67, Training Loss: 0.7878476565941832\n",
      "Epoch 68, Training Loss: 0.7872640320232936\n",
      "Epoch 69, Training Loss: 0.7885397853707908\n",
      "Epoch 70, Training Loss: 0.7875220128468104\n",
      "Epoch 71, Training Loss: 0.7869283016462971\n",
      "Epoch 72, Training Loss: 0.78772543588079\n",
      "Epoch 73, Training Loss: 0.787134085203472\n",
      "Epoch 74, Training Loss: 0.7879626258871609\n",
      "Epoch 75, Training Loss: 0.7876287767761632\n",
      "Epoch 76, Training Loss: 0.787756932140293\n",
      "Epoch 77, Training Loss: 0.788542540091321\n",
      "Epoch 78, Training Loss: 0.7880507602727502\n",
      "Epoch 79, Training Loss: 0.7870379808253811\n",
      "Epoch 80, Training Loss: 0.7879356547405846\n",
      "Epoch 81, Training Loss: 0.7880391700823505\n",
      "Epoch 82, Training Loss: 0.7880585181085686\n",
      "Epoch 83, Training Loss: 0.7872064877273445\n",
      "Epoch 84, Training Loss: 0.7867446774826911\n",
      "Epoch 85, Training Loss: 0.787637720430704\n",
      "Epoch 86, Training Loss: 0.7875079848712548\n",
      "Epoch 87, Training Loss: 0.7874315228677333\n",
      "Epoch 88, Training Loss: 0.7864682798098801\n",
      "Epoch 89, Training Loss: 0.7884232299668449\n",
      "Epoch 90, Training Loss: 0.787807287667927\n",
      "Epoch 91, Training Loss: 0.7879061893412941\n",
      "Epoch 92, Training Loss: 0.7864885570411395\n",
      "Epoch 93, Training Loss: 0.7872403584028546\n",
      "Epoch 94, Training Loss: 0.7869171583562865\n",
      "Epoch 95, Training Loss: 0.7860422588828812\n",
      "Epoch 96, Training Loss: 0.7872380729008438\n",
      "Epoch 97, Training Loss: 0.7864871149672601\n",
      "Epoch 98, Training Loss: 0.7878867702376574\n",
      "Epoch 99, Training Loss: 0.7873273766130433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:54:44,696] Trial 60 finished with value: 0.6352 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7873832642583919\n",
      "Epoch 1, Training Loss: 0.8726427752130171\n",
      "Epoch 2, Training Loss: 0.8126405955763424\n",
      "Epoch 3, Training Loss: 0.8067247254007003\n",
      "Epoch 4, Training Loss: 0.8035948562622071\n",
      "Epoch 5, Training Loss: 0.801315934938543\n",
      "Epoch 6, Training Loss: 0.799130173220354\n",
      "Epoch 7, Training Loss: 0.797025684188394\n",
      "Epoch 8, Training Loss: 0.7946402884932126\n",
      "Epoch 9, Training Loss: 0.7929469912893632\n",
      "Epoch 10, Training Loss: 0.7919260447165545\n",
      "Epoch 11, Training Loss: 0.7906272898000829\n",
      "Epoch 12, Training Loss: 0.7901684820652009\n",
      "Epoch 13, Training Loss: 0.7888230460531571\n",
      "Epoch 14, Training Loss: 0.7882690453529357\n",
      "Epoch 15, Training Loss: 0.7877528149240157\n",
      "Epoch 16, Training Loss: 0.7872493732676786\n",
      "Epoch 17, Training Loss: 0.7864985296305488\n",
      "Epoch 18, Training Loss: 0.7861721439922558\n",
      "Epoch 19, Training Loss: 0.7853212426690495\n",
      "Epoch 20, Training Loss: 0.7851496387229246\n",
      "Epoch 21, Training Loss: 0.7852733701116899\n",
      "Epoch 22, Training Loss: 0.7843153694096734\n",
      "Epoch 23, Training Loss: 0.7845612938965069\n",
      "Epoch 24, Training Loss: 0.7839180341187646\n",
      "Epoch 25, Training Loss: 0.7841099101655623\n",
      "Epoch 26, Training Loss: 0.7834866452217102\n",
      "Epoch 27, Training Loss: 0.7832916493976817\n",
      "Epoch 28, Training Loss: 0.783176724840613\n",
      "Epoch 29, Training Loss: 0.7828455766509561\n",
      "Epoch 30, Training Loss: 0.7825144972520716\n",
      "Epoch 31, Training Loss: 0.7827954281778896\n",
      "Epoch 32, Training Loss: 0.7823536403740153\n",
      "Epoch 33, Training Loss: 0.7823256977866677\n",
      "Epoch 34, Training Loss: 0.7816901227305917\n",
      "Epoch 35, Training Loss: 0.7817625557675081\n",
      "Epoch 36, Training Loss: 0.7815102449585409\n",
      "Epoch 37, Training Loss: 0.7816376922411077\n",
      "Epoch 38, Training Loss: 0.7814080521639656\n",
      "Epoch 39, Training Loss: 0.7810183508255902\n",
      "Epoch 40, Training Loss: 0.7810135912895203\n",
      "Epoch 41, Training Loss: 0.7807133295255548\n",
      "Epoch 42, Training Loss: 0.7804898959047654\n",
      "Epoch 43, Training Loss: 0.780631507284501\n",
      "Epoch 44, Training Loss: 0.7802332385147319\n",
      "Epoch 45, Training Loss: 0.7802544155541589\n",
      "Epoch 46, Training Loss: 0.7802834197352915\n",
      "Epoch 47, Training Loss: 0.7798036124425776\n",
      "Epoch 48, Training Loss: 0.7799664779270397\n",
      "Epoch 49, Training Loss: 0.7796282888160032\n",
      "Epoch 50, Training Loss: 0.7797843850360198\n",
      "Epoch 51, Training Loss: 0.7795332548898809\n",
      "Epoch 52, Training Loss: 0.7790322940489826\n",
      "Epoch 53, Training Loss: 0.7791666277717142\n",
      "Epoch 54, Training Loss: 0.7789421977716334\n",
      "Epoch 55, Training Loss: 0.7791398316972395\n",
      "Epoch 56, Training Loss: 0.7790331561425153\n",
      "Epoch 57, Training Loss: 0.778612224073971\n",
      "Epoch 58, Training Loss: 0.7788277252281414\n",
      "Epoch 59, Training Loss: 0.7788278478033402\n",
      "Epoch 60, Training Loss: 0.7786601594616385\n",
      "Epoch 61, Training Loss: 0.7784975642316482\n",
      "Epoch 62, Training Loss: 0.7784647771891425\n",
      "Epoch 63, Training Loss: 0.7782792739307179\n",
      "Epoch 64, Training Loss: 0.7783201532504138\n",
      "Epoch 65, Training Loss: 0.7781892848014832\n",
      "Epoch 66, Training Loss: 0.777883329110987\n",
      "Epoch 67, Training Loss: 0.7782672063042135\n",
      "Epoch 68, Training Loss: 0.7777323571373435\n",
      "Epoch 69, Training Loss: 0.7779440337770125\n",
      "Epoch 70, Training Loss: 0.7776808979932\n",
      "Epoch 71, Training Loss: 0.777841587417266\n",
      "Epoch 72, Training Loss: 0.777431266448077\n",
      "Epoch 73, Training Loss: 0.7773255586624146\n",
      "Epoch 74, Training Loss: 0.7774439916189979\n",
      "Epoch 75, Training Loss: 0.7775559195350198\n",
      "Epoch 76, Training Loss: 0.7773042427792268\n",
      "Epoch 77, Training Loss: 0.7773172884828904\n",
      "Epoch 78, Training Loss: 0.7772311857167412\n",
      "Epoch 79, Training Loss: 0.776773781636182\n",
      "Epoch 80, Training Loss: 0.7771585500941557\n",
      "Epoch 81, Training Loss: 0.7768998831861159\n",
      "Epoch 82, Training Loss: 0.7768697103332071\n",
      "Epoch 83, Training Loss: 0.7767465280785281\n",
      "Epoch 84, Training Loss: 0.776778334870058\n",
      "Epoch 85, Training Loss: 0.7765822944220374\n",
      "Epoch 86, Training Loss: 0.7767458109294667\n",
      "Epoch 87, Training Loss: 0.7765279278334449\n",
      "Epoch 88, Training Loss: 0.7764709860437057\n",
      "Epoch 89, Training Loss: 0.776460695477093\n",
      "Epoch 90, Training Loss: 0.7764141133252312\n",
      "Epoch 91, Training Loss: 0.7764403996046851\n",
      "Epoch 92, Training Loss: 0.7760743016355178\n",
      "Epoch 93, Training Loss: 0.776172518940533\n",
      "Epoch 94, Training Loss: 0.776031202288235\n",
      "Epoch 95, Training Loss: 0.776055489988888\n",
      "Epoch 96, Training Loss: 0.7762073640262379\n",
      "Epoch 97, Training Loss: 0.7759320348851821\n",
      "Epoch 98, Training Loss: 0.7760563745218165\n",
      "Epoch 99, Training Loss: 0.7757818049543044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:56:49,512] Trial 61 finished with value: 0.6382 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7757609334412743\n",
      "Epoch 1, Training Loss: 1.0889389900600208\n",
      "Epoch 2, Training Loss: 1.085407313038321\n",
      "Epoch 3, Training Loss: 1.0818703678075006\n",
      "Epoch 4, Training Loss: 1.0774121046066285\n",
      "Epoch 5, Training Loss: 1.0717762847507701\n",
      "Epoch 6, Training Loss: 1.0645740403848536\n",
      "Epoch 7, Training Loss: 1.0555123458189122\n",
      "Epoch 8, Training Loss: 1.0438864447088803\n",
      "Epoch 9, Training Loss: 1.030110714786193\n",
      "Epoch 10, Training Loss: 1.0150160739001106\n",
      "Epoch 11, Training Loss: 1.0000425414478078\n",
      "Epoch 12, Training Loss: 0.9870866055348341\n",
      "Epoch 13, Training Loss: 0.9771406300628886\n",
      "Epoch 14, Training Loss: 0.9701506492670845\n",
      "Epoch 15, Training Loss: 0.965340179345187\n",
      "Epoch 16, Training Loss: 0.9620900987176334\n",
      "Epoch 17, Training Loss: 0.9598003940722521\n",
      "Epoch 18, Training Loss: 0.9579503083930296\n",
      "Epoch 19, Training Loss: 0.956777801583795\n",
      "Epoch 20, Training Loss: 0.9556772347057567\n",
      "Epoch 21, Training Loss: 0.9547288395376766\n",
      "Epoch 22, Training Loss: 0.9538701920649585\n",
      "Epoch 23, Training Loss: 0.9530020421392777\n",
      "Epoch 24, Training Loss: 0.9521083182447097\n",
      "Epoch 25, Training Loss: 0.9510818777364843\n",
      "Epoch 26, Training Loss: 0.9503963056732626\n",
      "Epoch 27, Training Loss: 0.9494331915238324\n",
      "Epoch 28, Training Loss: 0.9485583828477299\n",
      "Epoch 29, Training Loss: 0.9475886072130765\n",
      "Epoch 30, Training Loss: 0.9465113944867078\n",
      "Epoch 31, Training Loss: 0.9455912120202008\n",
      "Epoch 32, Training Loss: 0.9445084951204412\n",
      "Epoch 33, Training Loss: 0.9434314976720249\n",
      "Epoch 34, Training Loss: 0.9421331443506129\n",
      "Epoch 35, Training Loss: 0.9410469303411596\n",
      "Epoch 36, Training Loss: 0.9398767549851361\n",
      "Epoch 37, Training Loss: 0.9385415126295651\n",
      "Epoch 38, Training Loss: 0.9372312916727628\n",
      "Epoch 39, Training Loss: 0.935834757159738\n",
      "Epoch 40, Training Loss: 0.9343873847232146\n",
      "Epoch 41, Training Loss: 0.9328759832241956\n",
      "Epoch 42, Training Loss: 0.9312031664567835\n",
      "Epoch 43, Training Loss: 0.9294809116335476\n",
      "Epoch 44, Training Loss: 0.9276371472022112\n",
      "Epoch 45, Training Loss: 0.9258715127496159\n",
      "Epoch 46, Training Loss: 0.9238544319657719\n",
      "Epoch 47, Training Loss: 0.9217733601261587\n",
      "Epoch 48, Training Loss: 0.9196756786458633\n",
      "Epoch 49, Training Loss: 0.9173007275777705\n",
      "Epoch 50, Training Loss: 0.914857029914856\n",
      "Epoch 51, Training Loss: 0.9124026283797095\n",
      "Epoch 52, Training Loss: 0.9096863675117492\n",
      "Epoch 53, Training Loss: 0.9069131878544302\n",
      "Epoch 54, Training Loss: 0.9040914699610542\n",
      "Epoch 55, Training Loss: 0.9010900542315314\n",
      "Epoch 56, Training Loss: 0.8980243650604697\n",
      "Epoch 57, Training Loss: 0.8948940498688641\n",
      "Epoch 58, Training Loss: 0.8916051345712999\n",
      "Epoch 59, Training Loss: 0.8883087517934687\n",
      "Epoch 60, Training Loss: 0.8849613977179808\n",
      "Epoch 61, Training Loss: 0.8815533440954545\n",
      "Epoch 62, Training Loss: 0.8780977417440975\n",
      "Epoch 63, Training Loss: 0.8747739622172187\n",
      "Epoch 64, Training Loss: 0.8714196939328137\n",
      "Epoch 65, Training Loss: 0.8681420626359827\n",
      "Epoch 66, Training Loss: 0.8650392174019533\n",
      "Epoch 67, Training Loss: 0.861928295878803\n",
      "Epoch 68, Training Loss: 0.8590176988349242\n",
      "Epoch 69, Training Loss: 0.8561077828267042\n",
      "Epoch 70, Training Loss: 0.8532904089899624\n",
      "Epoch 71, Training Loss: 0.8508264097746681\n",
      "Epoch 72, Training Loss: 0.8483287311301512\n",
      "Epoch 73, Training Loss: 0.8459722342210657\n",
      "Epoch 74, Training Loss: 0.8438975058583652\n",
      "Epoch 75, Training Loss: 0.841857239078073\n",
      "Epoch 76, Training Loss: 0.8399249941461226\n",
      "Epoch 77, Training Loss: 0.8379713531802683\n",
      "Epoch 78, Training Loss: 0.8364727827380686\n",
      "Epoch 79, Training Loss: 0.8351040766519658\n",
      "Epoch 80, Training Loss: 0.8335395650302663\n",
      "Epoch 81, Training Loss: 0.8322645549914416\n",
      "Epoch 82, Training Loss: 0.8310570923721089\n",
      "Epoch 83, Training Loss: 0.8298231266526614\n",
      "Epoch 84, Training Loss: 0.8287632507436415\n",
      "Epoch 85, Training Loss: 0.8277938917805167\n",
      "Epoch 86, Training Loss: 0.8268007095421062\n",
      "Epoch 87, Training Loss: 0.8258425182454726\n",
      "Epoch 88, Training Loss: 0.8250444020944483\n",
      "Epoch 89, Training Loss: 0.8243601606873905\n",
      "Epoch 90, Training Loss: 0.823654700307285\n",
      "Epoch 91, Training Loss: 0.8228704142570495\n",
      "Epoch 92, Training Loss: 0.8222284830317778\n",
      "Epoch 93, Training Loss: 0.8216498149843777\n",
      "Epoch 94, Training Loss: 0.8209780742841608\n",
      "Epoch 95, Training Loss: 0.820377917990965\n",
      "Epoch 96, Training Loss: 0.819956325713326\n",
      "Epoch 97, Training Loss: 0.8195113377711353\n",
      "Epoch 98, Training Loss: 0.8189425089780022\n",
      "Epoch 99, Training Loss: 0.8184718520501081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 10:58:36,008] Trial 62 finished with value: 0.6217333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8180447212387534\n",
      "Epoch 1, Training Loss: 1.035475652919096\n",
      "Epoch 2, Training Loss: 0.9814703287797816\n",
      "Epoch 3, Training Loss: 0.9653668315270367\n",
      "Epoch 4, Training Loss: 0.9604898549528683\n",
      "Epoch 5, Training Loss: 0.9578951814595391\n",
      "Epoch 6, Training Loss: 0.955534152283388\n",
      "Epoch 7, Training Loss: 0.9534692781111773\n",
      "Epoch 8, Training Loss: 0.9513444272209616\n",
      "Epoch 9, Training Loss: 0.948947760848438\n",
      "Epoch 10, Training Loss: 0.9464094828858095\n",
      "Epoch 11, Training Loss: 0.9438131573620965\n",
      "Epoch 12, Training Loss: 0.9409122138163623\n",
      "Epoch 13, Training Loss: 0.9379828568767099\n",
      "Epoch 14, Training Loss: 0.9348120133315816\n",
      "Epoch 15, Training Loss: 0.9313961204360514\n",
      "Epoch 16, Training Loss: 0.927717305281583\n",
      "Epoch 17, Training Loss: 0.9238656881276299\n",
      "Epoch 18, Training Loss: 0.9198317294261035\n",
      "Epoch 19, Training Loss: 0.9156738845039817\n",
      "Epoch 20, Training Loss: 0.9111547233777888\n",
      "Epoch 21, Training Loss: 0.9064758391941294\n",
      "Epoch 22, Training Loss: 0.901701940017588\n",
      "Epoch 23, Training Loss: 0.8967421249782338\n",
      "Epoch 24, Training Loss: 0.8916698416541604\n",
      "Epoch 25, Training Loss: 0.8865943304230185\n",
      "Epoch 26, Training Loss: 0.8814486426465652\n",
      "Epoch 27, Training Loss: 0.8764241023624645\n",
      "Epoch 28, Training Loss: 0.8715018646156086\n",
      "Epoch 29, Training Loss: 0.8666549331300399\n",
      "Epoch 30, Training Loss: 0.8618710017905515\n",
      "Epoch 31, Training Loss: 0.85749591904528\n",
      "Epoch 32, Training Loss: 0.8532294233406291\n",
      "Epoch 33, Training Loss: 0.849212130097782\n",
      "Epoch 34, Training Loss: 0.8454854798316955\n",
      "Epoch 35, Training Loss: 0.8419880739380332\n",
      "Epoch 36, Training Loss: 0.8385935223803801\n",
      "Epoch 37, Training Loss: 0.835746104927624\n",
      "Epoch 38, Training Loss: 0.8330334308568169\n",
      "Epoch 39, Training Loss: 0.8305619799389559\n",
      "Epoch 40, Training Loss: 0.8283179666014279\n",
      "Epoch 41, Training Loss: 0.8262463417473961\n",
      "Epoch 42, Training Loss: 0.8244505096884335\n",
      "Epoch 43, Training Loss: 0.8228822596634136\n",
      "Epoch 44, Training Loss: 0.8213681354242213\n",
      "Epoch 45, Training Loss: 0.8201157591623418\n",
      "Epoch 46, Training Loss: 0.8190411955468795\n",
      "Epoch 47, Training Loss: 0.8179321883005254\n",
      "Epoch 48, Training Loss: 0.8170671483348397\n",
      "Epoch 49, Training Loss: 0.8162487449365504\n",
      "Epoch 50, Training Loss: 0.8154141918350668\n",
      "Epoch 51, Training Loss: 0.8149200698908637\n",
      "Epoch 52, Training Loss: 0.8143760296877692\n",
      "Epoch 53, Training Loss: 0.813794598719653\n",
      "Epoch 54, Training Loss: 0.8132711204360513\n",
      "Epoch 55, Training Loss: 0.8129943634481991\n",
      "Epoch 56, Training Loss: 0.812576043325312\n",
      "Epoch 57, Training Loss: 0.8122579884529114\n",
      "Epoch 58, Training Loss: 0.8119692367665908\n",
      "Epoch 59, Training Loss: 0.8116942586618311\n",
      "Epoch 60, Training Loss: 0.8114284683676327\n",
      "Epoch 61, Training Loss: 0.8112761719787822\n",
      "Epoch 62, Training Loss: 0.8110608335102306\n",
      "Epoch 63, Training Loss: 0.8107948221178616\n",
      "Epoch 64, Training Loss: 0.8107119570760166\n",
      "Epoch 65, Training Loss: 0.8105461922813865\n",
      "Epoch 66, Training Loss: 0.810352285048541\n",
      "Epoch 67, Training Loss: 0.8103641721781563\n",
      "Epoch 68, Training Loss: 0.8100853811993318\n",
      "Epoch 69, Training Loss: 0.8100759553208071\n",
      "Epoch 70, Training Loss: 0.8098696221323575\n",
      "Epoch 71, Training Loss: 0.8095850916469798\n",
      "Epoch 72, Training Loss: 0.8095581184415256\n",
      "Epoch 73, Training Loss: 0.8094628857163821\n",
      "Epoch 74, Training Loss: 0.8094943010807037\n",
      "Epoch 75, Training Loss: 0.8094674134955687\n",
      "Epoch 76, Training Loss: 0.8092861791919259\n",
      "Epoch 77, Training Loss: 0.8091726474902209\n",
      "Epoch 78, Training Loss: 0.8090440145660849\n",
      "Epoch 79, Training Loss: 0.8089909937802483\n",
      "Epoch 80, Training Loss: 0.8089616821793949\n",
      "Epoch 81, Training Loss: 0.8087959451535168\n",
      "Epoch 82, Training Loss: 0.8087313820334042\n",
      "Epoch 83, Training Loss: 0.8085681048561545\n",
      "Epoch 84, Training Loss: 0.8084708308472353\n",
      "Epoch 85, Training Loss: 0.808373663705938\n",
      "Epoch 86, Training Loss: 0.8084565199823941\n",
      "Epoch 87, Training Loss: 0.8083048985986149\n",
      "Epoch 88, Training Loss: 0.8082509254006779\n",
      "Epoch 89, Training Loss: 0.8080909847511965\n",
      "Epoch 90, Training Loss: 0.8081016070702497\n",
      "Epoch 91, Training Loss: 0.807978387930814\n",
      "Epoch 92, Training Loss: 0.807819394644569\n",
      "Epoch 93, Training Loss: 0.8078172213890973\n",
      "Epoch 94, Training Loss: 0.8077835366305183\n",
      "Epoch 95, Training Loss: 0.8076421260132509\n",
      "Epoch 96, Training Loss: 0.8075524110653821\n",
      "Epoch 97, Training Loss: 0.8075275067722096\n",
      "Epoch 98, Training Loss: 0.8074286726643057\n",
      "Epoch 99, Training Loss: 0.8072987796278561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:00:08,836] Trial 63 finished with value: 0.6307333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8072077228742487\n",
      "Epoch 1, Training Loss: 0.9076397012050886\n",
      "Epoch 2, Training Loss: 0.8226093428475516\n",
      "Epoch 3, Training Loss: 0.8144702850427843\n",
      "Epoch 4, Training Loss: 0.8122148613284405\n",
      "Epoch 5, Training Loss: 0.8112982855703598\n",
      "Epoch 6, Training Loss: 0.8080338448510134\n",
      "Epoch 7, Training Loss: 0.8078557997717893\n",
      "Epoch 8, Training Loss: 0.8070803411921165\n",
      "Epoch 9, Training Loss: 0.8074806814803217\n",
      "Epoch 10, Training Loss: 0.804518331262402\n",
      "Epoch 11, Training Loss: 0.8070769788627338\n",
      "Epoch 12, Training Loss: 0.8056440815889746\n",
      "Epoch 13, Training Loss: 0.8047456352334273\n",
      "Epoch 14, Training Loss: 0.804096151653089\n",
      "Epoch 15, Training Loss: 0.8041529324717988\n",
      "Epoch 16, Training Loss: 0.8044908712681075\n",
      "Epoch 17, Training Loss: 0.8024512222834996\n",
      "Epoch 18, Training Loss: 0.8034858159552839\n",
      "Epoch 19, Training Loss: 0.802780302664391\n",
      "Epoch 20, Training Loss: 0.8043931084467952\n",
      "Epoch 21, Training Loss: 0.8025722611219364\n",
      "Epoch 22, Training Loss: 0.8029121670507847\n",
      "Epoch 23, Training Loss: 0.8027633526271447\n",
      "Epoch 24, Training Loss: 0.8012465095609651\n",
      "Epoch 25, Training Loss: 0.8023188766680266\n",
      "Epoch 26, Training Loss: 0.8012679022057612\n",
      "Epoch 27, Training Loss: 0.8014341048728254\n",
      "Epoch 28, Training Loss: 0.8010081681093775\n",
      "Epoch 29, Training Loss: 0.8010752259340501\n",
      "Epoch 30, Training Loss: 0.8018317176883382\n",
      "Epoch 31, Training Loss: 0.8006085910295185\n",
      "Epoch 32, Training Loss: 0.8008523094026666\n",
      "Epoch 33, Training Loss: 0.8012603656689924\n",
      "Epoch 34, Training Loss: 0.8006921564726005\n",
      "Epoch 35, Training Loss: 0.8002633057142559\n",
      "Epoch 36, Training Loss: 0.7993873989223538\n",
      "Epoch 37, Training Loss: 0.8004237181261966\n",
      "Epoch 38, Training Loss: 0.7998516212728687\n",
      "Epoch 39, Training Loss: 0.799857489327739\n",
      "Epoch 40, Training Loss: 0.7994843186292433\n",
      "Epoch 41, Training Loss: 0.7991364175215699\n",
      "Epoch 42, Training Loss: 0.7995776434590046\n",
      "Epoch 43, Training Loss: 0.799000007362294\n",
      "Epoch 44, Training Loss: 0.7986858932595504\n",
      "Epoch 45, Training Loss: 0.7995600648392412\n",
      "Epoch 46, Training Loss: 0.7990573619541369\n",
      "Epoch 47, Training Loss: 0.7992193609252012\n",
      "Epoch 48, Training Loss: 0.7984553865024022\n",
      "Epoch 49, Training Loss: 0.7988329643593695\n",
      "Epoch 50, Training Loss: 0.7980108934237544\n",
      "Epoch 51, Training Loss: 0.7987396632370196\n",
      "Epoch 52, Training Loss: 0.797776654698795\n",
      "Epoch 53, Training Loss: 0.7984083744816314\n",
      "Epoch 54, Training Loss: 0.7980850557635601\n",
      "Epoch 55, Training Loss: 0.7976769883829848\n",
      "Epoch 56, Training Loss: 0.7974691375753933\n",
      "Epoch 57, Training Loss: 0.79752837258174\n",
      "Epoch 58, Training Loss: 0.7975878703863101\n",
      "Epoch 59, Training Loss: 0.797203335770987\n",
      "Epoch 60, Training Loss: 0.7981415092496944\n",
      "Epoch 61, Training Loss: 0.7969607908922927\n",
      "Epoch 62, Training Loss: 0.7973897640866444\n",
      "Epoch 63, Training Loss: 0.7965103358254397\n",
      "Epoch 64, Training Loss: 0.7971788752347904\n",
      "Epoch 65, Training Loss: 0.7966938770803293\n",
      "Epoch 66, Training Loss: 0.7962186245093669\n",
      "Epoch 67, Training Loss: 0.7962544506653807\n",
      "Epoch 68, Training Loss: 0.7958122613734768\n",
      "Epoch 69, Training Loss: 0.7956081583983916\n",
      "Epoch 70, Training Loss: 0.7954418753322802\n",
      "Epoch 71, Training Loss: 0.7965599156860123\n",
      "Epoch 72, Training Loss: 0.7966608710755083\n",
      "Epoch 73, Training Loss: 0.7955484923563505\n",
      "Epoch 74, Training Loss: 0.7947174414656216\n",
      "Epoch 75, Training Loss: 0.7964602529554439\n",
      "Epoch 76, Training Loss: 0.7952016809829195\n",
      "Epoch 77, Training Loss: 0.7957582670943182\n",
      "Epoch 78, Training Loss: 0.7961747409705829\n",
      "Epoch 79, Training Loss: 0.7951072370199332\n",
      "Epoch 80, Training Loss: 0.7943118367876325\n",
      "Epoch 81, Training Loss: 0.7955684511285079\n",
      "Epoch 82, Training Loss: 0.794303759372324\n",
      "Epoch 83, Training Loss: 0.7946497636630123\n",
      "Epoch 84, Training Loss: 0.7942718089075017\n",
      "Epoch 85, Training Loss: 0.7947318389003438\n",
      "Epoch 86, Training Loss: 0.7930930017528678\n",
      "Epoch 87, Training Loss: 0.7931389931449316\n",
      "Epoch 88, Training Loss: 0.793722605257106\n",
      "Epoch 89, Training Loss: 0.7928109351853679\n",
      "Epoch 90, Training Loss: 0.7928153554299721\n",
      "Epoch 91, Training Loss: 0.7933562906164872\n",
      "Epoch 92, Training Loss: 0.7941690850974922\n",
      "Epoch 93, Training Loss: 0.7931318567211466\n",
      "Epoch 94, Training Loss: 0.7935398134073817\n",
      "Epoch 95, Training Loss: 0.7931678374010818\n",
      "Epoch 96, Training Loss: 0.7932660824374149\n",
      "Epoch 97, Training Loss: 0.7919227964896008\n",
      "Epoch 98, Training Loss: 0.7933934845422443\n",
      "Epoch 99, Training Loss: 0.7931727809117253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:01:46,491] Trial 64 finished with value: 0.6349333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7929326919684733\n",
      "Epoch 1, Training Loss: 0.8575688730969149\n",
      "Epoch 2, Training Loss: 0.814027957425398\n",
      "Epoch 3, Training Loss: 0.8083283394224503\n",
      "Epoch 4, Training Loss: 0.8050708760233486\n",
      "Epoch 5, Training Loss: 0.8025870139458601\n",
      "Epoch 6, Training Loss: 0.8012800140240613\n",
      "Epoch 7, Training Loss: 0.7999416381471297\n",
      "Epoch 8, Training Loss: 0.7996588988163892\n",
      "Epoch 9, Training Loss: 0.7980875945091248\n",
      "Epoch 10, Training Loss: 0.7975774253115935\n",
      "Epoch 11, Training Loss: 0.797455128291074\n",
      "Epoch 12, Training Loss: 0.7966553929974051\n",
      "Epoch 13, Training Loss: 0.7969013418870814\n",
      "Epoch 14, Training Loss: 0.7959417989674736\n",
      "Epoch 15, Training Loss: 0.7965722268469193\n",
      "Epoch 16, Training Loss: 0.7948814526024987\n",
      "Epoch 17, Training Loss: 0.7945856551563039\n",
      "Epoch 18, Training Loss: 0.794980166238897\n",
      "Epoch 19, Training Loss: 0.7938257542778464\n",
      "Epoch 20, Training Loss: 0.7942529744961683\n",
      "Epoch 21, Training Loss: 0.7939847443384282\n",
      "Epoch 22, Training Loss: 0.7933985265563516\n",
      "Epoch 23, Training Loss: 0.7942329820464639\n",
      "Epoch 24, Training Loss: 0.7933023637883804\n",
      "Epoch 25, Training Loss: 0.7930311970149769\n",
      "Epoch 26, Training Loss: 0.7931917636534747\n",
      "Epoch 27, Training Loss: 0.7925921415581423\n",
      "Epoch 28, Training Loss: 0.7926816894026364\n",
      "Epoch 29, Training Loss: 0.793365609926336\n",
      "Epoch 30, Training Loss: 0.792112317015143\n",
      "Epoch 31, Training Loss: 0.7926209624374614\n",
      "Epoch 32, Training Loss: 0.7919812457000508\n",
      "Epoch 33, Training Loss: 0.7914450927341685\n",
      "Epoch 34, Training Loss: 0.7917623970087837\n",
      "Epoch 35, Training Loss: 0.792159484835232\n",
      "Epoch 36, Training Loss: 0.7923657600318684\n",
      "Epoch 37, Training Loss: 0.7916935861811918\n",
      "Epoch 38, Training Loss: 0.7915329604289111\n",
      "Epoch 39, Training Loss: 0.7914808292949901\n",
      "Epoch 40, Training Loss: 0.791365822834127\n",
      "Epoch 41, Training Loss: 0.7915927893273971\n",
      "Epoch 42, Training Loss: 0.7911773403953103\n",
      "Epoch 43, Training Loss: 0.7911619033532984\n",
      "Epoch 44, Training Loss: 0.7908135361531201\n",
      "Epoch 45, Training Loss: 0.790585094760446\n",
      "Epoch 46, Training Loss: 0.7908851385817808\n",
      "Epoch 47, Training Loss: 0.790684945863836\n",
      "Epoch 48, Training Loss: 0.7909395403020523\n",
      "Epoch 49, Training Loss: 0.7910354297301349\n",
      "Epoch 50, Training Loss: 0.7909649564939387\n",
      "Epoch 51, Training Loss: 0.7908132717889897\n",
      "Epoch 52, Training Loss: 0.7905165591660668\n",
      "Epoch 53, Training Loss: 0.7906142349804149\n",
      "Epoch 54, Training Loss: 0.7897244360166438\n",
      "Epoch 55, Training Loss: 0.790734320528367\n",
      "Epoch 56, Training Loss: 0.7903636609806733\n",
      "Epoch 57, Training Loss: 0.7902090349618126\n",
      "Epoch 58, Training Loss: 0.7903757245400372\n",
      "Epoch 59, Training Loss: 0.7902644364974079\n",
      "Epoch 60, Training Loss: 0.7898647640031927\n",
      "Epoch 61, Training Loss: 0.7902307555955999\n",
      "Epoch 62, Training Loss: 0.789979961198919\n",
      "Epoch 63, Training Loss: 0.7909685240072363\n",
      "Epoch 64, Training Loss: 0.7901012054611655\n",
      "Epoch 65, Training Loss: 0.7899834769613603\n",
      "Epoch 66, Training Loss: 0.7899071188534007\n",
      "Epoch 67, Training Loss: 0.7902386161860298\n",
      "Epoch 68, Training Loss: 0.7897785027588115\n",
      "Epoch 69, Training Loss: 0.7900923307502971\n",
      "Epoch 70, Training Loss: 0.7904521982108845\n",
      "Epoch 71, Training Loss: 0.7896174327064963\n",
      "Epoch 72, Training Loss: 0.7904128634228426\n",
      "Epoch 73, Training Loss: 0.7901771241075852\n",
      "Epoch 74, Training Loss: 0.7897559703097624\n",
      "Epoch 75, Training Loss: 0.7900299079979167\n",
      "Epoch 76, Training Loss: 0.789769660094205\n",
      "Epoch 77, Training Loss: 0.7899232262723586\n",
      "Epoch 78, Training Loss: 0.7905003969108357\n",
      "Epoch 79, Training Loss: 0.7897160642988541\n",
      "Epoch 80, Training Loss: 0.7895920914060929\n",
      "Epoch 81, Training Loss: 0.7900135490473579\n",
      "Epoch 82, Training Loss: 0.7897496223449707\n",
      "Epoch 83, Training Loss: 0.7894744565206415\n",
      "Epoch 84, Training Loss: 0.7893776259702795\n",
      "Epoch 85, Training Loss: 0.7891482924012577\n",
      "Epoch 86, Training Loss: 0.7889650037008173\n",
      "Epoch 87, Training Loss: 0.7894942726107205\n",
      "Epoch 88, Training Loss: 0.7901970012748942\n",
      "Epoch 89, Training Loss: 0.7896060687654158\n",
      "Epoch 90, Training Loss: 0.7896706816028146\n",
      "Epoch 91, Training Loss: 0.7897216292689828\n",
      "Epoch 92, Training Loss: 0.7894509351253509\n",
      "Epoch 93, Training Loss: 0.7894300570207483\n",
      "Epoch 94, Training Loss: 0.7897877940009622\n",
      "Epoch 95, Training Loss: 0.7895293453160455\n",
      "Epoch 96, Training Loss: 0.7892870013152852\n",
      "Epoch 97, Training Loss: 0.7891550570375779\n",
      "Epoch 98, Training Loss: 0.7891260580455556\n",
      "Epoch 99, Training Loss: 0.7893017681907205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:03:54,033] Trial 65 finished with value: 0.6334666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7890427496152765\n",
      "Epoch 1, Training Loss: 1.0906584403094124\n",
      "Epoch 2, Training Loss: 1.0897707406212302\n",
      "Epoch 3, Training Loss: 1.0895134384491865\n",
      "Epoch 4, Training Loss: 1.0892648545433494\n",
      "Epoch 5, Training Loss: 1.089010012290057\n",
      "Epoch 6, Training Loss: 1.0887458495532765\n",
      "Epoch 7, Training Loss: 1.0885058421247145\n",
      "Epoch 8, Training Loss: 1.088234027413761\n",
      "Epoch 9, Training Loss: 1.0879759429482854\n",
      "Epoch 10, Training Loss: 1.0877029890172623\n",
      "Epoch 11, Training Loss: 1.08745692505556\n",
      "Epoch 12, Training Loss: 1.0871713272263022\n",
      "Epoch 13, Training Loss: 1.0868787405070137\n",
      "Epoch 14, Training Loss: 1.0866303972636953\n",
      "Epoch 15, Training Loss: 1.086359867067898\n",
      "Epoch 16, Training Loss: 1.0860843511188731\n",
      "Epoch 17, Training Loss: 1.0858001124157626\n",
      "Epoch 18, Training Loss: 1.0855187627848457\n",
      "Epoch 19, Training Loss: 1.0852288171824287\n",
      "Epoch 20, Training Loss: 1.0849280291445116\n",
      "Epoch 21, Training Loss: 1.0846251842554877\n",
      "Epoch 22, Training Loss: 1.0843217680033514\n",
      "Epoch 23, Training Loss: 1.084014859900755\n",
      "Epoch 24, Training Loss: 1.083691760932698\n",
      "Epoch 25, Training Loss: 1.0833831099902882\n",
      "Epoch 26, Training Loss: 1.0830514799847322\n",
      "Epoch 27, Training Loss: 1.0827163212439592\n",
      "Epoch 28, Training Loss: 1.082358504183152\n",
      "Epoch 29, Training Loss: 1.082028767641853\n",
      "Epoch 30, Training Loss: 1.0816767745859484\n",
      "Epoch 31, Training Loss: 1.0813006775519427\n",
      "Epoch 32, Training Loss: 1.080935127174153\n",
      "Epoch 33, Training Loss: 1.0805569703438702\n",
      "Epoch 34, Training Loss: 1.0801616794922773\n",
      "Epoch 35, Training Loss: 1.0797701752887052\n",
      "Epoch 36, Training Loss: 1.079366888999939\n",
      "Epoch 37, Training Loss: 1.078919397241929\n",
      "Epoch 38, Training Loss: 1.078543670457952\n",
      "Epoch 39, Training Loss: 1.0780959390191471\n",
      "Epoch 40, Training Loss: 1.0776485712387982\n",
      "Epoch 41, Training Loss: 1.0771975507455713\n",
      "Epoch 42, Training Loss: 1.0767377786075367\n",
      "Epoch 43, Training Loss: 1.0762383887347053\n",
      "Epoch 44, Training Loss: 1.0757502634385052\n",
      "Epoch 45, Training Loss: 1.0752546450671028\n",
      "Epoch 46, Training Loss: 1.074712851047516\n",
      "Epoch 47, Training Loss: 1.0742122335994946\n",
      "Epoch 48, Training Loss: 1.0736563503040988\n",
      "Epoch 49, Training Loss: 1.0731010731528787\n",
      "Epoch 50, Training Loss: 1.0725255984418531\n",
      "Epoch 51, Training Loss: 1.0719368161874658\n",
      "Epoch 52, Training Loss: 1.0713296717755936\n",
      "Epoch 53, Training Loss: 1.070712817696964\n",
      "Epoch 54, Training Loss: 1.0700870253058041\n",
      "Epoch 55, Training Loss: 1.0694358623729032\n",
      "Epoch 56, Training Loss: 1.0687626664778767\n",
      "Epoch 57, Training Loss: 1.0680836402668672\n",
      "Epoch 58, Training Loss: 1.0673720139615677\n",
      "Epoch 59, Training Loss: 1.0666486947676714\n",
      "Epoch 60, Training Loss: 1.0659120155783262\n",
      "Epoch 61, Training Loss: 1.065144668186412\n",
      "Epoch 62, Training Loss: 1.0643735685068019\n",
      "Epoch 63, Training Loss: 1.0635607318317188\n",
      "Epoch 64, Training Loss: 1.0627456759004033\n",
      "Epoch 65, Training Loss: 1.0619104465316325\n",
      "Epoch 66, Training Loss: 1.0610447039323694\n",
      "Epoch 67, Training Loss: 1.0601725963985218\n",
      "Epoch 68, Training Loss: 1.0592639489734874\n",
      "Epoch 69, Training Loss: 1.05832907185835\n",
      "Epoch 70, Training Loss: 1.0573834715170018\n",
      "Epoch 71, Training Loss: 1.0564208001248976\n",
      "Epoch 72, Training Loss: 1.055422384037691\n",
      "Epoch 73, Training Loss: 1.0544081246151644\n",
      "Epoch 74, Training Loss: 1.0533815455436706\n",
      "Epoch 75, Training Loss: 1.0523236050325282\n",
      "Epoch 76, Training Loss: 1.051236620089587\n",
      "Epoch 77, Training Loss: 1.0501253374885111\n",
      "Epoch 78, Training Loss: 1.0490003921003903\n",
      "Epoch 79, Training Loss: 1.0478706236446604\n",
      "Epoch 80, Training Loss: 1.0466886500751271\n",
      "Epoch 81, Training Loss: 1.045506698103512\n",
      "Epoch 82, Training Loss: 1.044300237683689\n",
      "Epoch 83, Training Loss: 1.0430731039888719\n",
      "Epoch 84, Training Loss: 1.0418300762597252\n",
      "Epoch 85, Training Loss: 1.0405620530773612\n",
      "Epoch 86, Training Loss: 1.0392742490067202\n",
      "Epoch 87, Training Loss: 1.0379753561580882\n",
      "Epoch 88, Training Loss: 1.0366652552520528\n",
      "Epoch 89, Training Loss: 1.035332428777919\n",
      "Epoch 90, Training Loss: 1.0339912145278034\n",
      "Epoch 91, Training Loss: 1.032628873726901\n",
      "Epoch 92, Training Loss: 1.0312702368287479\n",
      "Epoch 93, Training Loss: 1.0298891360619489\n",
      "Epoch 94, Training Loss: 1.0285186101408565\n",
      "Epoch 95, Training Loss: 1.027120068283642\n",
      "Epoch 96, Training Loss: 1.0257292204043444\n",
      "Epoch 97, Training Loss: 1.024331357268726\n",
      "Epoch 98, Training Loss: 1.0229412231725805\n",
      "Epoch 99, Training Loss: 1.0215430437116062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:05:38,614] Trial 66 finished with value: 0.4942666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 10 with value: 0.6420666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.0201411812445698\n",
      "Epoch 1, Training Loss: 0.8848573285691879\n",
      "Epoch 2, Training Loss: 0.8139386345358456\n",
      "Epoch 3, Training Loss: 0.8099098117211285\n",
      "Epoch 4, Training Loss: 0.8072348108011134\n",
      "Epoch 5, Training Loss: 0.8053623049399432\n",
      "Epoch 6, Training Loss: 0.8038077687515932\n",
      "Epoch 7, Training Loss: 0.8030466104956234\n",
      "Epoch 8, Training Loss: 0.8017113083951614\n",
      "Epoch 9, Training Loss: 0.800371419541976\n",
      "Epoch 10, Training Loss: 0.8005242065822377\n",
      "Epoch 11, Training Loss: 0.8001136460724999\n",
      "Epoch 12, Training Loss: 0.7996677020718069\n",
      "Epoch 13, Training Loss: 0.7991038654131047\n",
      "Epoch 14, Training Loss: 0.7994125662130468\n",
      "Epoch 15, Training Loss: 0.7983800952574786\n",
      "Epoch 16, Training Loss: 0.7978890744377585\n",
      "Epoch 17, Training Loss: 0.7976184212460238\n",
      "Epoch 18, Training Loss: 0.7975861216993892\n",
      "Epoch 19, Training Loss: 0.7975045137545642\n",
      "Epoch 20, Training Loss: 0.7962811759640189\n",
      "Epoch 21, Training Loss: 0.7952004572924446\n",
      "Epoch 22, Training Loss: 0.7943750862514272\n",
      "Epoch 23, Training Loss: 0.7942599672429702\n",
      "Epoch 24, Training Loss: 0.7937741703846876\n",
      "Epoch 25, Training Loss: 0.7934079853927388\n",
      "Epoch 26, Training Loss: 0.7925602381369646\n",
      "Epoch 27, Training Loss: 0.7916641463251675\n",
      "Epoch 28, Training Loss: 0.7911640593584846\n",
      "Epoch 29, Training Loss: 0.7898899999085595\n",
      "Epoch 30, Training Loss: 0.7894358210002674\n",
      "Epoch 31, Training Loss: 0.789560157621608\n",
      "Epoch 32, Training Loss: 0.7888550090088564\n",
      "Epoch 33, Training Loss: 0.7878638630754807\n",
      "Epoch 34, Training Loss: 0.7871661848881666\n",
      "Epoch 35, Training Loss: 0.7870611503544975\n",
      "Epoch 36, Training Loss: 0.7870773135914522\n",
      "Epoch 37, Training Loss: 0.7861861584467046\n",
      "Epoch 38, Training Loss: 0.7863856926385094\n",
      "Epoch 39, Training Loss: 0.7859442900910097\n",
      "Epoch 40, Training Loss: 0.785802305726444\n",
      "Epoch 41, Training Loss: 0.7856872862928054\n",
      "Epoch 42, Training Loss: 0.7852519661538742\n",
      "Epoch 43, Training Loss: 0.7851346834968118\n",
      "Epoch 44, Training Loss: 0.7851985059064978\n",
      "Epoch 45, Training Loss: 0.7847727950881509\n",
      "Epoch 46, Training Loss: 0.7846499781748828\n",
      "Epoch 47, Training Loss: 0.7841380251856411\n",
      "Epoch 48, Training Loss: 0.7844125549933489\n",
      "Epoch 49, Training Loss: 0.7839473515398362\n",
      "Epoch 50, Training Loss: 0.7844159615741056\n",
      "Epoch 51, Training Loss: 0.7840555414732765\n",
      "Epoch 52, Training Loss: 0.7837390645111308\n",
      "Epoch 53, Training Loss: 0.7844029437794405\n",
      "Epoch 54, Training Loss: 0.7837532839354346\n",
      "Epoch 55, Training Loss: 0.7839030858348398\n",
      "Epoch 56, Training Loss: 0.7835573197813595\n",
      "Epoch 57, Training Loss: 0.7835354865298552\n",
      "Epoch 58, Training Loss: 0.7833081020327175\n",
      "Epoch 59, Training Loss: 0.7834257906324723\n",
      "Epoch 60, Training Loss: 0.7830819101193371\n",
      "Epoch 61, Training Loss: 0.7835926862323985\n",
      "Epoch 62, Training Loss: 0.7833428183723898\n",
      "Epoch 63, Training Loss: 0.7829361300608692\n",
      "Epoch 64, Training Loss: 0.7827542919271132\n",
      "Epoch 65, Training Loss: 0.7825355189688066\n",
      "Epoch 66, Training Loss: 0.7830078220367431\n",
      "Epoch 67, Training Loss: 0.7828334653377533\n",
      "Epoch 68, Training Loss: 0.7827367746128755\n",
      "Epoch 69, Training Loss: 0.7822598845117232\n",
      "Epoch 70, Training Loss: 0.7825544569071602\n",
      "Epoch 71, Training Loss: 0.7824539706286262\n",
      "Epoch 72, Training Loss: 0.7827008928972132\n",
      "Epoch 73, Training Loss: 0.782729526758194\n",
      "Epoch 74, Training Loss: 0.7822448632997625\n",
      "Epoch 75, Training Loss: 0.7819897801034591\n",
      "Epoch 76, Training Loss: 0.7822368933874018\n",
      "Epoch 77, Training Loss: 0.782208591559354\n",
      "Epoch 78, Training Loss: 0.7820693139468923\n",
      "Epoch 79, Training Loss: 0.7820168226606705\n",
      "Epoch 80, Training Loss: 0.7821778531635509\n",
      "Epoch 81, Training Loss: 0.7818953830354354\n",
      "Epoch 82, Training Loss: 0.7819749646327074\n",
      "Epoch 83, Training Loss: 0.7815641126211952\n",
      "Epoch 84, Training Loss: 0.7819803821339327\n",
      "Epoch 85, Training Loss: 0.7820012532262242\n",
      "Epoch 86, Training Loss: 0.7812606027547051\n",
      "Epoch 87, Training Loss: 0.7816141313665054\n",
      "Epoch 88, Training Loss: 0.7813687668127172\n",
      "Epoch 89, Training Loss: 0.7815685765883502\n",
      "Epoch 90, Training Loss: 0.7812296504132887\n",
      "Epoch 91, Training Loss: 0.7814730820936315\n",
      "Epoch 92, Training Loss: 0.7816469681964201\n",
      "Epoch 93, Training Loss: 0.7815498215310713\n",
      "Epoch 94, Training Loss: 0.7808175452316508\n",
      "Epoch 95, Training Loss: 0.7810073238961837\n",
      "Epoch 96, Training Loss: 0.7809359712460462\n",
      "Epoch 97, Training Loss: 0.7808655440807343\n",
      "Epoch 98, Training Loss: 0.7809577256791732\n",
      "Epoch 99, Training Loss: 0.7809339086448445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:07:52,232] Trial 67 finished with value: 0.6433333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7810243314154008\n",
      "Epoch 1, Training Loss: 0.9008453748279944\n",
      "Epoch 2, Training Loss: 0.8252815849798962\n",
      "Epoch 3, Training Loss: 0.8151900061091086\n",
      "Epoch 4, Training Loss: 0.8101849794387818\n",
      "Epoch 5, Training Loss: 0.8060377313678426\n",
      "Epoch 6, Training Loss: 0.8028547910819376\n",
      "Epoch 7, Training Loss: 0.8013922106054493\n",
      "Epoch 8, Training Loss: 0.8005321119960985\n",
      "Epoch 9, Training Loss: 0.7986115770232408\n",
      "Epoch 10, Training Loss: 0.7968566066788552\n",
      "Epoch 11, Training Loss: 0.797102931926125\n",
      "Epoch 12, Training Loss: 0.7961325917028843\n",
      "Epoch 13, Training Loss: 0.7952322129020117\n",
      "Epoch 14, Training Loss: 0.7943378328380728\n",
      "Epoch 15, Training Loss: 0.7946449799645215\n",
      "Epoch 16, Training Loss: 0.7936378434188384\n",
      "Epoch 17, Training Loss: 0.7937789467044343\n",
      "Epoch 18, Training Loss: 0.7925651170257338\n",
      "Epoch 19, Training Loss: 0.7921857852684824\n",
      "Epoch 20, Training Loss: 0.7932230175885939\n",
      "Epoch 21, Training Loss: 0.7915970355048215\n",
      "Epoch 22, Training Loss: 0.7918487727193904\n",
      "Epoch 23, Training Loss: 0.7917095628896154\n",
      "Epoch 24, Training Loss: 0.7912536204309392\n",
      "Epoch 25, Training Loss: 0.7897903148841141\n",
      "Epoch 26, Training Loss: 0.7902274841652777\n",
      "Epoch 27, Training Loss: 0.791171383678465\n",
      "Epoch 28, Training Loss: 0.789818578331094\n",
      "Epoch 29, Training Loss: 0.7909889784970678\n",
      "Epoch 30, Training Loss: 0.7907188382363857\n",
      "Epoch 31, Training Loss: 0.7897322235250832\n",
      "Epoch 32, Training Loss: 0.7898617950149048\n",
      "Epoch 33, Training Loss: 0.7892411517917661\n",
      "Epoch 34, Training Loss: 0.7894914469324557\n",
      "Epoch 35, Training Loss: 0.7891780621127078\n",
      "Epoch 36, Training Loss: 0.7898382072161911\n",
      "Epoch 37, Training Loss: 0.789750788086339\n",
      "Epoch 38, Training Loss: 0.7894195629241771\n",
      "Epoch 39, Training Loss: 0.7887941304902385\n",
      "Epoch 40, Training Loss: 0.7884714460910711\n",
      "Epoch 41, Training Loss: 0.7887616125264563\n",
      "Epoch 42, Training Loss: 0.789332187713537\n",
      "Epoch 43, Training Loss: 0.7889945957893716\n",
      "Epoch 44, Training Loss: 0.7884617080365804\n",
      "Epoch 45, Training Loss: 0.7887457907648014\n",
      "Epoch 46, Training Loss: 0.7882604408084898\n",
      "Epoch 47, Training Loss: 0.7884237878304675\n",
      "Epoch 48, Training Loss: 0.7887986858088271\n",
      "Epoch 49, Training Loss: 0.7882245270381296\n",
      "Epoch 50, Training Loss: 0.788564994460658\n",
      "Epoch 51, Training Loss: 0.7878889382333684\n",
      "Epoch 52, Training Loss: 0.7882152896178396\n",
      "Epoch 53, Training Loss: 0.7885532501944922\n",
      "Epoch 54, Training Loss: 0.787946823665074\n",
      "Epoch 55, Training Loss: 0.7879796729051978\n",
      "Epoch 56, Training Loss: 0.7886005883826349\n",
      "Epoch 57, Training Loss: 0.7880456108795969\n",
      "Epoch 58, Training Loss: 0.7884618316377913\n",
      "Epoch 59, Training Loss: 0.7871786049434117\n",
      "Epoch 60, Training Loss: 0.7883080724486731\n",
      "Epoch 61, Training Loss: 0.7882433193966858\n",
      "Epoch 62, Training Loss: 0.7883684119783846\n",
      "Epoch 63, Training Loss: 0.7872518728550215\n",
      "Epoch 64, Training Loss: 0.7886154667775434\n",
      "Epoch 65, Training Loss: 0.7882477171438977\n",
      "Epoch 66, Training Loss: 0.7878691671486188\n",
      "Epoch 67, Training Loss: 0.7873523359908197\n",
      "Epoch 68, Training Loss: 0.788262503129199\n",
      "Epoch 69, Training Loss: 0.7878137199502242\n",
      "Epoch 70, Training Loss: 0.7891158114698597\n",
      "Epoch 71, Training Loss: 0.7878881464327189\n",
      "Epoch 72, Training Loss: 0.7874323589461191\n",
      "Epoch 73, Training Loss: 0.7877045591971031\n",
      "Epoch 74, Training Loss: 0.7876297239074134\n",
      "Epoch 75, Training Loss: 0.7871692298050214\n",
      "Epoch 76, Training Loss: 0.7874323947089059\n",
      "Epoch 77, Training Loss: 0.787597181922511\n",
      "Epoch 78, Training Loss: 0.7864923349448613\n",
      "Epoch 79, Training Loss: 0.7881575319103729\n",
      "Epoch 80, Training Loss: 0.7884386005258202\n",
      "Epoch 81, Training Loss: 0.7879274528725703\n",
      "Epoch 82, Training Loss: 0.7885351891804459\n",
      "Epoch 83, Training Loss: 0.7882854595220179\n",
      "Epoch 84, Training Loss: 0.7873876231057303\n",
      "Epoch 85, Training Loss: 0.7877216850008283\n",
      "Epoch 86, Training Loss: 0.7881945104527294\n",
      "Epoch 87, Training Loss: 0.7871954814832013\n",
      "Epoch 88, Training Loss: 0.7872887585396157\n",
      "Epoch 89, Training Loss: 0.7877858245283141\n",
      "Epoch 90, Training Loss: 0.7870216783724333\n",
      "Epoch 91, Training Loss: 0.7879977530106566\n",
      "Epoch 92, Training Loss: 0.7867995576750964\n",
      "Epoch 93, Training Loss: 0.7875012882670066\n",
      "Epoch 94, Training Loss: 0.7881383223641187\n",
      "Epoch 95, Training Loss: 0.7866196966709051\n",
      "Epoch 96, Training Loss: 0.7877505371445104\n",
      "Epoch 97, Training Loss: 0.787004448148541\n",
      "Epoch 98, Training Loss: 0.7872251347491616\n",
      "Epoch 99, Training Loss: 0.7882371858546608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:09:41,723] Trial 68 finished with value: 0.6317333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7879264682755435\n",
      "Epoch 1, Training Loss: 1.013771402835846\n",
      "Epoch 2, Training Loss: 0.9388173453789904\n",
      "Epoch 3, Training Loss: 0.9199878551906213\n",
      "Epoch 4, Training Loss: 0.9043448267126442\n",
      "Epoch 5, Training Loss: 0.8842996782826301\n",
      "Epoch 6, Training Loss: 0.8573915488737867\n",
      "Epoch 7, Training Loss: 0.8321101959486653\n",
      "Epoch 8, Training Loss: 0.8175070301034397\n",
      "Epoch 9, Training Loss: 0.8096252303374442\n",
      "Epoch 10, Training Loss: 0.8076296694296643\n",
      "Epoch 11, Training Loss: 0.8051081552541346\n",
      "Epoch 12, Training Loss: 0.8037501458834885\n",
      "Epoch 13, Training Loss: 0.8026730921931733\n",
      "Epoch 14, Training Loss: 0.8026071552047156\n",
      "Epoch 15, Training Loss: 0.8016483951331977\n",
      "Epoch 16, Training Loss: 0.8005055389009921\n",
      "Epoch 17, Training Loss: 0.8000727625717794\n",
      "Epoch 18, Training Loss: 0.7995266920641849\n",
      "Epoch 19, Training Loss: 0.7998087086175617\n",
      "Epoch 20, Training Loss: 0.7990218272782806\n",
      "Epoch 21, Training Loss: 0.7979984234150191\n",
      "Epoch 22, Training Loss: 0.7971989671090491\n",
      "Epoch 23, Training Loss: 0.7972642174340728\n",
      "Epoch 24, Training Loss: 0.7963386299914883\n",
      "Epoch 25, Training Loss: 0.7964574611276612\n",
      "Epoch 26, Training Loss: 0.7953904261266379\n",
      "Epoch 27, Training Loss: 0.7951899722106475\n",
      "Epoch 28, Training Loss: 0.794752061546297\n",
      "Epoch 29, Training Loss: 0.7944456213398984\n",
      "Epoch 30, Training Loss: 0.7942658304271841\n",
      "Epoch 31, Training Loss: 0.7943825404446824\n",
      "Epoch 32, Training Loss: 0.7934245622247682\n",
      "Epoch 33, Training Loss: 0.7939872915583446\n",
      "Epoch 34, Training Loss: 0.7930569703417613\n",
      "Epoch 35, Training Loss: 0.793101950426747\n",
      "Epoch 36, Training Loss: 0.7930460181451382\n",
      "Epoch 37, Training Loss: 0.7917133973057109\n",
      "Epoch 38, Training Loss: 0.7916817456259764\n",
      "Epoch 39, Training Loss: 0.7919027688808011\n",
      "Epoch 40, Training Loss: 0.7907093651760789\n",
      "Epoch 41, Training Loss: 0.7904951630678392\n",
      "Epoch 42, Training Loss: 0.7895444910777243\n",
      "Epoch 43, Training Loss: 0.790054642616358\n",
      "Epoch 44, Training Loss: 0.7899139841696373\n",
      "Epoch 45, Training Loss: 0.7892942667903756\n",
      "Epoch 46, Training Loss: 0.789234439502085\n",
      "Epoch 47, Training Loss: 0.7893692938008703\n",
      "Epoch 48, Training Loss: 0.7886198393384317\n",
      "Epoch 49, Training Loss: 0.7881940038580644\n",
      "Epoch 50, Training Loss: 0.7882124586212904\n",
      "Epoch 51, Training Loss: 0.7883693712098258\n",
      "Epoch 52, Training Loss: 0.7875585595467933\n",
      "Epoch 53, Training Loss: 0.787739140109012\n",
      "Epoch 54, Training Loss: 0.7873487811339529\n",
      "Epoch 55, Training Loss: 0.7865687033287565\n",
      "Epoch 56, Training Loss: 0.7867418491750732\n",
      "Epoch 57, Training Loss: 0.7871913948453458\n",
      "Epoch 58, Training Loss: 0.786331528021877\n",
      "Epoch 59, Training Loss: 0.7862845079343121\n",
      "Epoch 60, Training Loss: 0.7858404793237385\n",
      "Epoch 61, Training Loss: 0.7862406481477551\n",
      "Epoch 62, Training Loss: 0.7857721515167925\n",
      "Epoch 63, Training Loss: 0.7858432115468764\n",
      "Epoch 64, Training Loss: 0.7859259937042581\n",
      "Epoch 65, Training Loss: 0.7855118234354751\n",
      "Epoch 66, Training Loss: 0.785112564545825\n",
      "Epoch 67, Training Loss: 0.7852226553106667\n",
      "Epoch 68, Training Loss: 0.7848129823691863\n",
      "Epoch 69, Training Loss: 0.7845595092701733\n",
      "Epoch 70, Training Loss: 0.7847106411044759\n",
      "Epoch 71, Training Loss: 0.7847036884243327\n",
      "Epoch 72, Training Loss: 0.785163692782696\n",
      "Epoch 73, Training Loss: 0.7842787471032681\n",
      "Epoch 74, Training Loss: 0.78419583399493\n",
      "Epoch 75, Training Loss: 0.7842393800728303\n",
      "Epoch 76, Training Loss: 0.783282950184399\n",
      "Epoch 77, Training Loss: 0.7834628062140673\n",
      "Epoch 78, Training Loss: 0.7833710138959096\n",
      "Epoch 79, Training Loss: 0.7845808822409551\n",
      "Epoch 80, Training Loss: 0.7832485596936448\n",
      "Epoch 81, Training Loss: 0.7838872582392585\n",
      "Epoch 82, Training Loss: 0.7835015191171403\n",
      "Epoch 83, Training Loss: 0.7833009576438961\n",
      "Epoch 84, Training Loss: 0.7831191669729419\n",
      "Epoch 85, Training Loss: 0.7837902177545361\n",
      "Epoch 86, Training Loss: 0.7844725322006341\n",
      "Epoch 87, Training Loss: 0.783251090426194\n",
      "Epoch 88, Training Loss: 0.7831932166465243\n",
      "Epoch 89, Training Loss: 0.7836535828454154\n",
      "Epoch 90, Training Loss: 0.7829764979226249\n",
      "Epoch 91, Training Loss: 0.7827173141608561\n",
      "Epoch 92, Training Loss: 0.7826781849215801\n",
      "Epoch 93, Training Loss: 0.782681877989518\n",
      "Epoch 94, Training Loss: 0.7821996128648744\n",
      "Epoch 95, Training Loss: 0.7819949705349771\n",
      "Epoch 96, Training Loss: 0.7828941714494748\n",
      "Epoch 97, Training Loss: 0.7826242702347892\n",
      "Epoch 98, Training Loss: 0.7829542690649964\n",
      "Epoch 99, Training Loss: 0.7822371525872023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:11:15,679] Trial 69 finished with value: 0.6392 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7821753440046669\n",
      "Epoch 1, Training Loss: 0.8571554039653979\n",
      "Epoch 2, Training Loss: 0.8210392896394084\n",
      "Epoch 3, Training Loss: 0.8155507377215794\n",
      "Epoch 4, Training Loss: 0.8121440733285775\n",
      "Epoch 5, Training Loss: 0.8131907877169158\n",
      "Epoch 6, Training Loss: 0.8098996568443184\n",
      "Epoch 7, Training Loss: 0.8099942462784904\n",
      "Epoch 8, Training Loss: 0.8082882070003595\n",
      "Epoch 9, Training Loss: 0.8085092062340643\n",
      "Epoch 10, Training Loss: 0.8075915328542093\n",
      "Epoch 11, Training Loss: 0.8069914848284614\n",
      "Epoch 12, Training Loss: 0.8062556722110376\n",
      "Epoch 13, Training Loss: 0.8080713633307837\n",
      "Epoch 14, Training Loss: 0.8066064652643705\n",
      "Epoch 15, Training Loss: 0.8047430570860554\n",
      "Epoch 16, Training Loss: 0.8064609984706219\n",
      "Epoch 17, Training Loss: 0.805801424765049\n",
      "Epoch 18, Training Loss: 0.8048635560767095\n",
      "Epoch 19, Training Loss: 0.8061081402283862\n",
      "Epoch 20, Training Loss: 0.8046509496251443\n",
      "Epoch 21, Training Loss: 0.8039126709887856\n",
      "Epoch 22, Training Loss: 0.8058856774989823\n",
      "Epoch 23, Training Loss: 0.8053176956965511\n",
      "Epoch 24, Training Loss: 0.8041633903532099\n",
      "Epoch 25, Training Loss: 0.8039049845889099\n",
      "Epoch 26, Training Loss: 0.8040740293667729\n",
      "Epoch 27, Training Loss: 0.8040928563677279\n",
      "Epoch 28, Training Loss: 0.8035687820355695\n",
      "Epoch 29, Training Loss: 0.802862837081565\n",
      "Epoch 30, Training Loss: 0.8023152207969723\n",
      "Epoch 31, Training Loss: 0.8033592753840568\n",
      "Epoch 32, Training Loss: 0.8040268562790146\n",
      "Epoch 33, Training Loss: 0.8033323843676344\n",
      "Epoch 34, Training Loss: 0.8028097062182605\n",
      "Epoch 35, Training Loss: 0.8020323129524862\n",
      "Epoch 36, Training Loss: 0.8036915031590857\n",
      "Epoch 37, Training Loss: 0.8016049636037726\n",
      "Epoch 38, Training Loss: 0.8018018704159815\n",
      "Epoch 39, Training Loss: 0.802802508666103\n",
      "Epoch 40, Training Loss: 0.8026924930120769\n",
      "Epoch 41, Training Loss: 0.8023584431275389\n",
      "Epoch 42, Training Loss: 0.8007946195459007\n",
      "Epoch 43, Training Loss: 0.8026151643659836\n",
      "Epoch 44, Training Loss: 0.8026958043413951\n",
      "Epoch 45, Training Loss: 0.8012732735253815\n",
      "Epoch 46, Training Loss: 0.7999682227471717\n",
      "Epoch 47, Training Loss: 0.8018274490098308\n",
      "Epoch 48, Training Loss: 0.801141971842687\n",
      "Epoch 49, Training Loss: 0.8008906921049705\n",
      "Epoch 50, Training Loss: 0.8011887878403627\n",
      "Epoch 51, Training Loss: 0.8005745950498079\n",
      "Epoch 52, Training Loss: 0.8005182716183197\n",
      "Epoch 53, Training Loss: 0.8012269777462895\n",
      "Epoch 54, Training Loss: 0.8014867725228905\n",
      "Epoch 55, Training Loss: 0.8011152704855553\n",
      "Epoch 56, Training Loss: 0.8012200267691362\n",
      "Epoch 57, Training Loss: 0.8002722436324098\n",
      "Epoch 58, Training Loss: 0.7992692883749654\n",
      "Epoch 59, Training Loss: 0.7983147222744791\n",
      "Epoch 60, Training Loss: 0.7992474129325465\n",
      "Epoch 61, Training Loss: 0.8002569630630034\n",
      "Epoch 62, Training Loss: 0.7992965865852242\n",
      "Epoch 63, Training Loss: 0.7989264792069457\n",
      "Epoch 64, Training Loss: 0.7993792982926046\n",
      "Epoch 65, Training Loss: 0.7980019796163516\n",
      "Epoch 66, Training Loss: 0.7987302262980239\n",
      "Epoch 67, Training Loss: 0.79748906282554\n",
      "Epoch 68, Training Loss: 0.7982276713041435\n",
      "Epoch 69, Training Loss: 0.7977113595582489\n",
      "Epoch 70, Training Loss: 0.797532274982983\n",
      "Epoch 71, Training Loss: 0.7976596177072454\n",
      "Epoch 72, Training Loss: 0.7974352460158499\n",
      "Epoch 73, Training Loss: 0.7979931374241535\n",
      "Epoch 74, Training Loss: 0.7979083573011527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:12:30,209] Trial 70 finished with value: 0.6353333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7974564691235249\n",
      "Epoch 1, Training Loss: 0.8948744231596926\n",
      "Epoch 2, Training Loss: 0.8220682645202579\n",
      "Epoch 3, Training Loss: 0.8135271981246489\n",
      "Epoch 4, Training Loss: 0.8085196006566958\n",
      "Epoch 5, Training Loss: 0.8039129529680524\n",
      "Epoch 6, Training Loss: 0.8026175142230844\n",
      "Epoch 7, Training Loss: 0.8005475682423527\n",
      "Epoch 8, Training Loss: 0.7999485160175123\n",
      "Epoch 9, Training Loss: 0.7974590090880717\n",
      "Epoch 10, Training Loss: 0.7972486864355274\n",
      "Epoch 11, Training Loss: 0.7962227727237501\n",
      "Epoch 12, Training Loss: 0.7949988202941148\n",
      "Epoch 13, Training Loss: 0.7958882306751452\n",
      "Epoch 14, Training Loss: 0.7937108534619324\n",
      "Epoch 15, Training Loss: 0.793456090392923\n",
      "Epoch 16, Training Loss: 0.7939152681737914\n",
      "Epoch 17, Training Loss: 0.7932914299176151\n",
      "Epoch 18, Training Loss: 0.7929318495262835\n",
      "Epoch 19, Training Loss: 0.792464740473525\n",
      "Epoch 20, Training Loss: 0.7912004650087285\n",
      "Epoch 21, Training Loss: 0.7929632236186723\n",
      "Epoch 22, Training Loss: 0.7914789445418164\n",
      "Epoch 23, Training Loss: 0.7908864352936135\n",
      "Epoch 24, Training Loss: 0.79031532626403\n",
      "Epoch 25, Training Loss: 0.791102859848424\n",
      "Epoch 26, Training Loss: 0.7908413367163866\n",
      "Epoch 27, Training Loss: 0.7901073989115264\n",
      "Epoch 28, Training Loss: 0.790217027359439\n",
      "Epoch 29, Training Loss: 0.7898722702399232\n",
      "Epoch 30, Training Loss: 0.7895594497372334\n",
      "Epoch 31, Training Loss: 0.790133972185895\n",
      "Epoch 32, Training Loss: 0.7890242381203444\n",
      "Epoch 33, Training Loss: 0.7889219136166393\n",
      "Epoch 34, Training Loss: 0.7896977002459361\n",
      "Epoch 35, Training Loss: 0.7884096064961943\n",
      "Epoch 36, Training Loss: 0.7878561066057449\n",
      "Epoch 37, Training Loss: 0.78824834617457\n",
      "Epoch 38, Training Loss: 0.789179247572906\n",
      "Epoch 39, Training Loss: 0.7882672835113411\n",
      "Epoch 40, Training Loss: 0.7880343250762251\n",
      "Epoch 41, Training Loss: 0.7877515023812316\n",
      "Epoch 42, Training Loss: 0.7881156975165345\n",
      "Epoch 43, Training Loss: 0.7875542193427122\n",
      "Epoch 44, Training Loss: 0.7879084226780368\n",
      "Epoch 45, Training Loss: 0.7876280552462528\n",
      "Epoch 46, Training Loss: 0.7877610223633903\n",
      "Epoch 47, Training Loss: 0.7877058964922912\n",
      "Epoch 48, Training Loss: 0.7870747145853545\n",
      "Epoch 49, Training Loss: 0.7874509359660902\n",
      "Epoch 50, Training Loss: 0.7866885443379108\n",
      "Epoch 51, Training Loss: 0.7866976869733711\n",
      "Epoch 52, Training Loss: 0.7872170109497874\n",
      "Epoch 53, Training Loss: 0.7863019761286284\n",
      "Epoch 54, Training Loss: 0.786453782526174\n",
      "Epoch 55, Training Loss: 0.7870624623800578\n",
      "Epoch 56, Training Loss: 0.7863504727083938\n",
      "Epoch 57, Training Loss: 0.7863855551956291\n",
      "Epoch 58, Training Loss: 0.7870062310892837\n",
      "Epoch 59, Training Loss: 0.7874866981255381\n",
      "Epoch 60, Training Loss: 0.7867617854498383\n",
      "Epoch 61, Training Loss: 0.7867864095178762\n",
      "Epoch 62, Training Loss: 0.7875894633450903\n",
      "Epoch 63, Training Loss: 0.7861947195870536\n",
      "Epoch 64, Training Loss: 0.785759463166832\n",
      "Epoch 65, Training Loss: 0.7859068256571777\n",
      "Epoch 66, Training Loss: 0.7866431445107425\n",
      "Epoch 67, Training Loss: 0.7862096808906784\n",
      "Epoch 68, Training Loss: 0.786058903816051\n",
      "Epoch 69, Training Loss: 0.7857392831852562\n",
      "Epoch 70, Training Loss: 0.7863671892567685\n",
      "Epoch 71, Training Loss: 0.7857111206628327\n",
      "Epoch 72, Training Loss: 0.7855696779444702\n",
      "Epoch 73, Training Loss: 0.7861522352785096\n",
      "Epoch 74, Training Loss: 0.7861788652893296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:13:51,312] Trial 71 finished with value: 0.5998666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7866680057425248\n",
      "Epoch 1, Training Loss: 0.9027215381313983\n",
      "Epoch 2, Training Loss: 0.8252405552039469\n",
      "Epoch 3, Training Loss: 0.8155753474486501\n",
      "Epoch 4, Training Loss: 0.8116098188816157\n",
      "Epoch 5, Training Loss: 0.808733079756113\n",
      "Epoch 6, Training Loss: 0.8057577944339667\n",
      "Epoch 7, Training Loss: 0.802022053155684\n",
      "Epoch 8, Training Loss: 0.8000768367509197\n",
      "Epoch 9, Training Loss: 0.7985698028614646\n",
      "Epoch 10, Training Loss: 0.7970113294913357\n",
      "Epoch 11, Training Loss: 0.7961432866584089\n",
      "Epoch 12, Training Loss: 0.7953389978050289\n",
      "Epoch 13, Training Loss: 0.7952803496131323\n",
      "Epoch 14, Training Loss: 0.7947988977109579\n",
      "Epoch 15, Training Loss: 0.7938903262740687\n",
      "Epoch 16, Training Loss: 0.7929192500006884\n",
      "Epoch 17, Training Loss: 0.7916995141739236\n",
      "Epoch 18, Training Loss: 0.7921303239083828\n",
      "Epoch 19, Training Loss: 0.7918518493946334\n",
      "Epoch 20, Training Loss: 0.7908117502255547\n",
      "Epoch 21, Training Loss: 0.7911060274095464\n",
      "Epoch 22, Training Loss: 0.789856015112167\n",
      "Epoch 23, Training Loss: 0.7903581446274779\n",
      "Epoch 24, Training Loss: 0.7904364786650006\n",
      "Epoch 25, Training Loss: 0.789948742819908\n",
      "Epoch 26, Training Loss: 0.7901267412013577\n",
      "Epoch 27, Training Loss: 0.7897431988465159\n",
      "Epoch 28, Training Loss: 0.7892642515942566\n",
      "Epoch 29, Training Loss: 0.7898047885500399\n",
      "Epoch 30, Training Loss: 0.7886256744538931\n",
      "Epoch 31, Training Loss: 0.7886575305372252\n",
      "Epoch 32, Training Loss: 0.7883997495013072\n",
      "Epoch 33, Training Loss: 0.7873659868437545\n",
      "Epoch 34, Training Loss: 0.7885017677357322\n",
      "Epoch 35, Training Loss: 0.7881985679604954\n",
      "Epoch 36, Training Loss: 0.7879533549000446\n",
      "Epoch 37, Training Loss: 0.7879699018664826\n",
      "Epoch 38, Training Loss: 0.7869115439572729\n",
      "Epoch 39, Training Loss: 0.7874214728075759\n",
      "Epoch 40, Training Loss: 0.7875369302312234\n",
      "Epoch 41, Training Loss: 0.7877687829777711\n",
      "Epoch 42, Training Loss: 0.7872057930867474\n",
      "Epoch 43, Training Loss: 0.7870239272153468\n",
      "Epoch 44, Training Loss: 0.7865101703127524\n",
      "Epoch 45, Training Loss: 0.7866933879099394\n",
      "Epoch 46, Training Loss: 0.7866228904042926\n",
      "Epoch 47, Training Loss: 0.7865964885941126\n",
      "Epoch 48, Training Loss: 0.7862381763924333\n",
      "Epoch 49, Training Loss: 0.78682035772424\n",
      "Epoch 50, Training Loss: 0.7853025815092531\n",
      "Epoch 51, Training Loss: 0.7859287958396108\n",
      "Epoch 52, Training Loss: 0.7860414125865564\n",
      "Epoch 53, Training Loss: 0.7860582101613955\n",
      "Epoch 54, Training Loss: 0.7857856316674025\n",
      "Epoch 55, Training Loss: 0.7862846480276352\n",
      "Epoch 56, Training Loss: 0.7859111231072504\n",
      "Epoch 57, Training Loss: 0.7855036737327289\n",
      "Epoch 58, Training Loss: 0.7848707524457372\n",
      "Epoch 59, Training Loss: 0.7854792983012092\n",
      "Epoch 60, Training Loss: 0.7851982543342992\n",
      "Epoch 61, Training Loss: 0.7861132470288671\n",
      "Epoch 62, Training Loss: 0.7847094396003207\n",
      "Epoch 63, Training Loss: 0.7857016965858918\n",
      "Epoch 64, Training Loss: 0.7843318020490776\n",
      "Epoch 65, Training Loss: 0.7847062066981667\n",
      "Epoch 66, Training Loss: 0.7845234660277689\n",
      "Epoch 67, Training Loss: 0.7839472839706823\n",
      "Epoch 68, Training Loss: 0.7844747418747808\n",
      "Epoch 69, Training Loss: 0.7854074354458572\n",
      "Epoch 70, Training Loss: 0.7839790864994651\n",
      "Epoch 71, Training Loss: 0.7844098965924485\n",
      "Epoch 72, Training Loss: 0.7843440629485855\n",
      "Epoch 73, Training Loss: 0.7841721382356228\n",
      "Epoch 74, Training Loss: 0.7848304374773699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:15:13,470] Trial 72 finished with value: 0.6242666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7841248623410562\n",
      "Epoch 1, Training Loss: 1.114794173097252\n",
      "Epoch 2, Training Loss: 1.0588937203686937\n",
      "Epoch 3, Training Loss: 1.0317397296876836\n",
      "Epoch 4, Training Loss: 1.0123718313704757\n",
      "Epoch 5, Training Loss: 0.9979749734240367\n",
      "Epoch 6, Training Loss: 0.9867733771639658\n",
      "Epoch 7, Training Loss: 0.9783049535034294\n",
      "Epoch 8, Training Loss: 0.9715857981739188\n",
      "Epoch 9, Training Loss: 0.966328846063829\n",
      "Epoch 10, Training Loss: 0.9623427714620317\n",
      "Epoch 11, Training Loss: 0.95894781996433\n",
      "Epoch 12, Training Loss: 0.9560016021692663\n",
      "Epoch 13, Training Loss: 0.9533404111862183\n",
      "Epoch 14, Training Loss: 0.9513225696140662\n",
      "Epoch 15, Training Loss: 0.9493394556798433\n",
      "Epoch 16, Training Loss: 0.9480372296240097\n",
      "Epoch 17, Training Loss: 0.9465382045372984\n",
      "Epoch 18, Training Loss: 0.9451674291065761\n",
      "Epoch 19, Training Loss: 0.944073330101214\n",
      "Epoch 20, Training Loss: 0.9428846097530279\n",
      "Epoch 21, Training Loss: 0.9414536843622537\n",
      "Epoch 22, Training Loss: 0.9409411203592344\n",
      "Epoch 23, Training Loss: 0.9397811560702504\n",
      "Epoch 24, Training Loss: 0.9387053248577548\n",
      "Epoch 25, Training Loss: 0.9378049650586637\n",
      "Epoch 26, Training Loss: 0.9372925313791834\n",
      "Epoch 27, Training Loss: 0.936040200774831\n",
      "Epoch 28, Training Loss: 0.9353763907475579\n",
      "Epoch 29, Training Loss: 0.9346812224925909\n",
      "Epoch 30, Training Loss: 0.934311186370993\n",
      "Epoch 31, Training Loss: 0.9327720011087288\n",
      "Epoch 32, Training Loss: 0.93192621926616\n",
      "Epoch 33, Training Loss: 0.9316716123344306\n",
      "Epoch 34, Training Loss: 0.9305722496563331\n",
      "Epoch 35, Training Loss: 0.9301442870520111\n",
      "Epoch 36, Training Loss: 0.9297030643412941\n",
      "Epoch 37, Training Loss: 0.9288211906762948\n",
      "Epoch 38, Training Loss: 0.9280501120072558\n",
      "Epoch 39, Training Loss: 0.9271952387085535\n",
      "Epoch 40, Training Loss: 0.9268011387129476\n",
      "Epoch 41, Training Loss: 0.9261277369090489\n",
      "Epoch 42, Training Loss: 0.9255807752896072\n",
      "Epoch 43, Training Loss: 0.9246919709937017\n",
      "Epoch 44, Training Loss: 0.9241813955450416\n",
      "Epoch 45, Training Loss: 0.923491268588188\n",
      "Epoch 46, Training Loss: 0.9228554214750018\n",
      "Epoch 47, Training Loss: 0.9223845636037956\n",
      "Epoch 48, Training Loss: 0.9215164466908103\n",
      "Epoch 49, Training Loss: 0.9209819141187165\n",
      "Epoch 50, Training Loss: 0.9208357812766742\n",
      "Epoch 51, Training Loss: 0.919466279234205\n",
      "Epoch 52, Training Loss: 0.9194146856329495\n",
      "Epoch 53, Training Loss: 0.9187694151598708\n",
      "Epoch 54, Training Loss: 0.9178783116484047\n",
      "Epoch 55, Training Loss: 0.9177197555850323\n",
      "Epoch 56, Training Loss: 0.9171827769817267\n",
      "Epoch 57, Training Loss: 0.9166754678676002\n",
      "Epoch 58, Training Loss: 0.9157143513959153\n",
      "Epoch 59, Training Loss: 0.9150595766261108\n",
      "Epoch 60, Training Loss: 0.9149231902638773\n",
      "Epoch 61, Training Loss: 0.9141366954136612\n",
      "Epoch 62, Training Loss: 0.9131556752032803\n",
      "Epoch 63, Training Loss: 0.9128144809177944\n",
      "Epoch 64, Training Loss: 0.9129989281632847\n",
      "Epoch 65, Training Loss: 0.9122000553554162\n",
      "Epoch 66, Training Loss: 0.9116018962143059\n",
      "Epoch 67, Training Loss: 0.9110457184619474\n",
      "Epoch 68, Training Loss: 0.9100223797604554\n",
      "Epoch 69, Training Loss: 0.9097308000227562\n",
      "Epoch 70, Training Loss: 0.9087506313969318\n",
      "Epoch 71, Training Loss: 0.9084815389231632\n",
      "Epoch 72, Training Loss: 0.9078537359273523\n",
      "Epoch 73, Training Loss: 0.9075568533481512\n",
      "Epoch 74, Training Loss: 0.90685813561418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:16:15,983] Trial 73 finished with value: 0.5668666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9061685060199939\n",
      "Epoch 1, Training Loss: 0.9781859338283538\n",
      "Epoch 2, Training Loss: 0.8861956217008479\n",
      "Epoch 3, Training Loss: 0.8761239877869101\n",
      "Epoch 4, Training Loss: 0.8716149180776933\n",
      "Epoch 5, Training Loss: 0.8697103803999284\n",
      "Epoch 6, Training Loss: 0.8675119230326485\n",
      "Epoch 7, Training Loss: 0.8645040374643662\n",
      "Epoch 8, Training Loss: 0.8633204165626974\n",
      "Epoch 9, Training Loss: 0.8613619826120489\n",
      "Epoch 10, Training Loss: 0.8607365688155679\n",
      "Epoch 11, Training Loss: 0.8609060386349173\n",
      "Epoch 12, Training Loss: 0.8614272823754479\n",
      "Epoch 13, Training Loss: 0.8576859115853029\n",
      "Epoch 14, Training Loss: 0.8625626857140485\n",
      "Epoch 15, Training Loss: 0.8601668181138881\n",
      "Epoch 16, Training Loss: 0.8599729990959167\n",
      "Epoch 17, Training Loss: 0.8593779332497541\n",
      "Epoch 18, Training Loss: 0.8560774294067831\n",
      "Epoch 19, Training Loss: 0.8621442822849049\n",
      "Epoch 20, Training Loss: 0.8588384979612687\n",
      "Epoch 21, Training Loss: 0.8582738243131076\n",
      "Epoch 22, Training Loss: 0.8585954766413745\n",
      "Epoch 23, Training Loss: 0.8582203956912545\n",
      "Epoch 24, Training Loss: 0.8549721253619474\n",
      "Epoch 25, Training Loss: 0.8559598424154169\n",
      "Epoch 26, Training Loss: 0.8535665399887983\n",
      "Epoch 27, Training Loss: 0.8555451277424307\n",
      "Epoch 28, Training Loss: 0.8536458070839152\n",
      "Epoch 29, Training Loss: 0.8565966754801133\n",
      "Epoch 30, Training Loss: 0.856503362234901\n",
      "Epoch 31, Training Loss: 0.8538609068533953\n",
      "Epoch 32, Training Loss: 0.8547775774142321\n",
      "Epoch 33, Training Loss: 0.854405825699077\n",
      "Epoch 34, Training Loss: 0.8529299380498774\n",
      "Epoch 35, Training Loss: 0.8528574136425467\n",
      "Epoch 36, Training Loss: 0.8499694124390097\n",
      "Epoch 37, Training Loss: 0.85241887828883\n",
      "Epoch 38, Training Loss: 0.8546393587308772\n",
      "Epoch 39, Training Loss: 0.8535371600179111\n",
      "Epoch 40, Training Loss: 0.8524004887833315\n",
      "Epoch 41, Training Loss: 0.8528050827980042\n",
      "Epoch 42, Training Loss: 0.8516344287816215\n",
      "Epoch 43, Training Loss: 0.8520720097597908\n",
      "Epoch 44, Training Loss: 0.8524423466710483\n",
      "Epoch 45, Training Loss: 0.8530204781363993\n",
      "Epoch 46, Training Loss: 0.853090884475147\n",
      "Epoch 47, Training Loss: 0.8511957852980669\n",
      "Epoch 48, Training Loss: 0.8490595606495352\n",
      "Epoch 49, Training Loss: 0.8523932508160086\n",
      "Epoch 50, Training Loss: 0.8515297480891733\n",
      "Epoch 51, Training Loss: 0.8532736865211935\n",
      "Epoch 52, Training Loss: 0.8527710143958821\n",
      "Epoch 53, Training Loss: 0.8498939765200896\n",
      "Epoch 54, Training Loss: 0.8526496077986324\n",
      "Epoch 55, Training Loss: 0.8497960555553437\n",
      "Epoch 56, Training Loss: 0.8499526920038111\n",
      "Epoch 57, Training Loss: 0.850149722029181\n",
      "Epoch 58, Training Loss: 0.8506518881461199\n",
      "Epoch 59, Training Loss: 0.850968948953292\n",
      "Epoch 60, Training Loss: 0.8519732061554404\n",
      "Epoch 61, Training Loss: 0.8503779414822074\n",
      "Epoch 62, Training Loss: 0.8493044825161205\n",
      "Epoch 63, Training Loss: 0.851188791289049\n",
      "Epoch 64, Training Loss: 0.8550012903353748\n",
      "Epoch 65, Training Loss: 0.8517610257513383\n",
      "Epoch 66, Training Loss: 0.8519471600476434\n",
      "Epoch 67, Training Loss: 0.8516290403814877\n",
      "Epoch 68, Training Loss: 0.8530055212273318\n",
      "Epoch 69, Training Loss: 0.8515336338211509\n",
      "Epoch 70, Training Loss: 0.8499150065814748\n",
      "Epoch 71, Training Loss: 0.8505021185734692\n",
      "Epoch 72, Training Loss: 0.8506180924527785\n",
      "Epoch 73, Training Loss: 0.8464219527384814\n",
      "Epoch 74, Training Loss: 0.8495829901975744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:17:50,172] Trial 74 finished with value: 0.6099333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8482743310928345\n",
      "Epoch 1, Training Loss: 0.947544396835215\n",
      "Epoch 2, Training Loss: 0.8773675857571994\n",
      "Epoch 3, Training Loss: 0.8339116146985223\n",
      "Epoch 4, Training Loss: 0.817580880207174\n",
      "Epoch 5, Training Loss: 0.8117550408138948\n",
      "Epoch 6, Training Loss: 0.8096359476622413\n",
      "Epoch 7, Training Loss: 0.8082749403224272\n",
      "Epoch 8, Training Loss: 0.8076487823093639\n",
      "Epoch 9, Training Loss: 0.8063712519757887\n",
      "Epoch 10, Training Loss: 0.8065595380698933\n",
      "Epoch 11, Training Loss: 0.8054732886482687\n",
      "Epoch 12, Training Loss: 0.80507184288081\n",
      "Epoch 13, Training Loss: 0.804949895774617\n",
      "Epoch 14, Training Loss: 0.8040560523902669\n",
      "Epoch 15, Training Loss: 0.8038900893576005\n",
      "Epoch 16, Training Loss: 0.8032454685603871\n",
      "Epoch 17, Training Loss: 0.8030639899478239\n",
      "Epoch 18, Training Loss: 0.8027317824784447\n",
      "Epoch 19, Training Loss: 0.8025347802218269\n",
      "Epoch 20, Training Loss: 0.8021021839450387\n",
      "Epoch 21, Training Loss: 0.8018524868348066\n",
      "Epoch 22, Training Loss: 0.8017690904701458\n",
      "Epoch 23, Training Loss: 0.8014618937408223\n",
      "Epoch 24, Training Loss: 0.8010251167241265\n",
      "Epoch 25, Training Loss: 0.8006369357249316\n",
      "Epoch 26, Training Loss: 0.8006690557564006\n",
      "Epoch 27, Training Loss: 0.8004613282400019\n",
      "Epoch 28, Training Loss: 0.8005065128382515\n",
      "Epoch 29, Training Loss: 0.8004461907639223\n",
      "Epoch 30, Training Loss: 0.8000595352930181\n",
      "Epoch 31, Training Loss: 0.8001262335216298\n",
      "Epoch 32, Training Loss: 0.7998241259070004\n",
      "Epoch 33, Training Loss: 0.7995736331799451\n",
      "Epoch 34, Training Loss: 0.7993013165277594\n",
      "Epoch 35, Training Loss: 0.7989168373276205\n",
      "Epoch 36, Training Loss: 0.7989251191475812\n",
      "Epoch 37, Training Loss: 0.7987613499865812\n",
      "Epoch 38, Training Loss: 0.7989980283905478\n",
      "Epoch 39, Training Loss: 0.7987418215415057\n",
      "Epoch 40, Training Loss: 0.7986481759127448\n",
      "Epoch 41, Training Loss: 0.7987451765116523\n",
      "Epoch 42, Training Loss: 0.7985954103750341\n",
      "Epoch 43, Training Loss: 0.7981991714589736\n",
      "Epoch 44, Training Loss: 0.798268029759912\n",
      "Epoch 45, Training Loss: 0.7985547638640684\n",
      "Epoch 46, Training Loss: 0.7983181063567891\n",
      "Epoch 47, Training Loss: 0.798210740440032\n",
      "Epoch 48, Training Loss: 0.7979048890927258\n",
      "Epoch 49, Training Loss: 0.7981895404703477\n",
      "Epoch 50, Training Loss: 0.7978947854743285\n",
      "Epoch 51, Training Loss: 0.7980146540613735\n",
      "Epoch 52, Training Loss: 0.7981065673687878\n",
      "Epoch 53, Training Loss: 0.7976540881044725\n",
      "Epoch 54, Training Loss: 0.7977290983059827\n",
      "Epoch 55, Training Loss: 0.7977286247646107\n",
      "Epoch 56, Training Loss: 0.7976369458787581\n",
      "Epoch 57, Training Loss: 0.7974067420117995\n",
      "Epoch 58, Training Loss: 0.7974955085445853\n",
      "Epoch 59, Training Loss: 0.797500154130599\n",
      "Epoch 60, Training Loss: 0.7976005967224346\n",
      "Epoch 61, Training Loss: 0.7976452927729663\n",
      "Epoch 62, Training Loss: 0.7970966158894932\n",
      "Epoch 63, Training Loss: 0.7976276154377882\n",
      "Epoch 64, Training Loss: 0.7972820411710179\n",
      "Epoch 65, Training Loss: 0.7972401565663955\n",
      "Epoch 66, Training Loss: 0.7969791732816135\n",
      "Epoch 67, Training Loss: 0.7971156902172987\n",
      "Epoch 68, Training Loss: 0.7971623753800111\n",
      "Epoch 69, Training Loss: 0.797024613759097\n",
      "Epoch 70, Training Loss: 0.7971081967213575\n",
      "Epoch 71, Training Loss: 0.7972459394090315\n",
      "Epoch 72, Training Loss: 0.7970717889421126\n",
      "Epoch 73, Training Loss: 0.7969120916198281\n",
      "Epoch 74, Training Loss: 0.79700042738634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:19:16,502] Trial 75 finished with value: 0.6373333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7967372890079722\n",
      "Epoch 1, Training Loss: 0.9289445427127351\n",
      "Epoch 2, Training Loss: 0.8604365260081184\n",
      "Epoch 3, Training Loss: 0.8270537103925433\n",
      "Epoch 4, Training Loss: 0.8157597872547637\n",
      "Epoch 5, Training Loss: 0.8119417560727973\n",
      "Epoch 6, Training Loss: 0.8100351763847179\n",
      "Epoch 7, Training Loss: 0.809151876420903\n",
      "Epoch 8, Training Loss: 0.8086019025709396\n",
      "Epoch 9, Training Loss: 0.8080130295645922\n",
      "Epoch 10, Training Loss: 0.8072114724861948\n",
      "Epoch 11, Training Loss: 0.8077393384804403\n",
      "Epoch 12, Training Loss: 0.8074221913975881\n",
      "Epoch 13, Training Loss: 0.8062200436018463\n",
      "Epoch 14, Training Loss: 0.8056655725142113\n",
      "Epoch 15, Training Loss: 0.8055561659927655\n",
      "Epoch 16, Training Loss: 0.8062194410123323\n",
      "Epoch 17, Training Loss: 0.8048966357582494\n",
      "Epoch 18, Training Loss: 0.8050504221952051\n",
      "Epoch 19, Training Loss: 0.8044783211292181\n",
      "Epoch 20, Training Loss: 0.8042647352792267\n",
      "Epoch 21, Training Loss: 0.8041043283347796\n",
      "Epoch 22, Training Loss: 0.8035493311129118\n",
      "Epoch 23, Training Loss: 0.8044256083051065\n",
      "Epoch 24, Training Loss: 0.8038099487921349\n",
      "Epoch 25, Training Loss: 0.802777172703492\n",
      "Epoch 26, Training Loss: 0.8029201495020013\n",
      "Epoch 27, Training Loss: 0.8023168150643657\n",
      "Epoch 28, Training Loss: 0.8025134668314368\n",
      "Epoch 29, Training Loss: 0.8022237144018475\n",
      "Epoch 30, Training Loss: 0.8017788146671496\n",
      "Epoch 31, Training Loss: 0.802048633510905\n",
      "Epoch 32, Training Loss: 0.8019873823438372\n",
      "Epoch 33, Training Loss: 0.8014640757912084\n",
      "Epoch 34, Training Loss: 0.8012357624849878\n",
      "Epoch 35, Training Loss: 0.8011419895896338\n",
      "Epoch 36, Training Loss: 0.8013242575459014\n",
      "Epoch 37, Training Loss: 0.8010883371632799\n",
      "Epoch 38, Training Loss: 0.8008129797483745\n",
      "Epoch 39, Training Loss: 0.800932538688631\n",
      "Epoch 40, Training Loss: 0.8004682735392922\n",
      "Epoch 41, Training Loss: 0.8002918806291164\n",
      "Epoch 42, Training Loss: 0.8003769097471596\n",
      "Epoch 43, Training Loss: 0.8003284470479292\n",
      "Epoch 44, Training Loss: 0.8005278654564593\n",
      "Epoch 45, Training Loss: 0.8001085343217491\n",
      "Epoch 46, Training Loss: 0.8004925899935844\n",
      "Epoch 47, Training Loss: 0.7994556637634909\n",
      "Epoch 48, Training Loss: 0.7998857643371238\n",
      "Epoch 49, Training Loss: 0.8002537438743993\n",
      "Epoch 50, Training Loss: 0.7995928318876969\n",
      "Epoch 51, Training Loss: 0.7992222452522221\n",
      "Epoch 52, Training Loss: 0.7997063202069218\n",
      "Epoch 53, Training Loss: 0.7993972589198808\n",
      "Epoch 54, Training Loss: 0.799624418674555\n",
      "Epoch 55, Training Loss: 0.7985901133010261\n",
      "Epoch 56, Training Loss: 0.7991576391951483\n",
      "Epoch 57, Training Loss: 0.7989755143796591\n",
      "Epoch 58, Training Loss: 0.7986324440267749\n",
      "Epoch 59, Training Loss: 0.7988197666361816\n",
      "Epoch 60, Training Loss: 0.7985646566053979\n",
      "Epoch 61, Training Loss: 0.7987549986158099\n",
      "Epoch 62, Training Loss: 0.7979730836430886\n",
      "Epoch 63, Training Loss: 0.7985661277197358\n",
      "Epoch 64, Training Loss: 0.7979585495210232\n",
      "Epoch 65, Training Loss: 0.798126683199316\n",
      "Epoch 66, Training Loss: 0.7984175653385936\n",
      "Epoch 67, Training Loss: 0.7979378915370855\n",
      "Epoch 68, Training Loss: 0.7974933034495304\n",
      "Epoch 69, Training Loss: 0.7987990050387562\n",
      "Epoch 70, Training Loss: 0.7981627981465562\n",
      "Epoch 71, Training Loss: 0.7975909090131745\n",
      "Epoch 72, Training Loss: 0.7976307953210702\n",
      "Epoch 73, Training Loss: 0.797918070796737\n",
      "Epoch 74, Training Loss: 0.7976734232185478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:20:28,277] Trial 76 finished with value: 0.6326666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7976097596319098\n",
      "Epoch 1, Training Loss: 0.9724888927796308\n",
      "Epoch 2, Training Loss: 0.9209382293504828\n",
      "Epoch 3, Training Loss: 0.9064382637248319\n",
      "Epoch 4, Training Loss: 0.8933384672333212\n",
      "Epoch 5, Training Loss: 0.8790192730286542\n",
      "Epoch 6, Training Loss: 0.8635985439665177\n",
      "Epoch 7, Training Loss: 0.848646209380206\n",
      "Epoch 8, Training Loss: 0.8358940886048709\n",
      "Epoch 9, Training Loss: 0.8259144346153034\n",
      "Epoch 10, Training Loss: 0.8187717095543356\n",
      "Epoch 11, Training Loss: 0.8138167029969833\n",
      "Epoch 12, Training Loss: 0.8103294291215785\n",
      "Epoch 13, Training Loss: 0.8081500411734861\n",
      "Epoch 14, Training Loss: 0.8066797432478736\n",
      "Epoch 15, Training Loss: 0.8054975417782279\n",
      "Epoch 16, Training Loss: 0.804412729880389\n",
      "Epoch 17, Training Loss: 0.8038966455179102\n",
      "Epoch 18, Training Loss: 0.803188256446053\n",
      "Epoch 19, Training Loss: 0.8025761931784012\n",
      "Epoch 20, Training Loss: 0.8021101316984962\n",
      "Epoch 21, Training Loss: 0.8017520651396584\n",
      "Epoch 22, Training Loss: 0.801376282046823\n",
      "Epoch 23, Training Loss: 0.8012175387494704\n",
      "Epoch 24, Training Loss: 0.8007499201858745\n",
      "Epoch 25, Training Loss: 0.8005732541925767\n",
      "Epoch 26, Training Loss: 0.8002545497697943\n",
      "Epoch 27, Training Loss: 0.8002209212499506\n",
      "Epoch 28, Training Loss: 0.7999353443875032\n",
      "Epoch 29, Training Loss: 0.7997861654618207\n",
      "Epoch 30, Training Loss: 0.7998741520152373\n",
      "Epoch 31, Training Loss: 0.7995052259108599\n",
      "Epoch 32, Training Loss: 0.7995685995326323\n",
      "Epoch 33, Training Loss: 0.7992258518583635\n",
      "Epoch 34, Training Loss: 0.7992232685930589\n",
      "Epoch 35, Training Loss: 0.7990149603170507\n",
      "Epoch 36, Training Loss: 0.7988904039298786\n",
      "Epoch 37, Training Loss: 0.7988194131149965\n",
      "Epoch 38, Training Loss: 0.7985680964413812\n",
      "Epoch 39, Training Loss: 0.7986131241742302\n",
      "Epoch 40, Training Loss: 0.7985730476239148\n",
      "Epoch 41, Training Loss: 0.7983808734136469\n",
      "Epoch 42, Training Loss: 0.7981954236591563\n",
      "Epoch 43, Training Loss: 0.7982337956568774\n",
      "Epoch 44, Training Loss: 0.7980076234480914\n",
      "Epoch 45, Training Loss: 0.7980048760947059\n",
      "Epoch 46, Training Loss: 0.7978711242535536\n",
      "Epoch 47, Training Loss: 0.7976965974358952\n",
      "Epoch 48, Training Loss: 0.7975968118274913\n",
      "Epoch 49, Training Loss: 0.7975400999013116\n",
      "Epoch 50, Training Loss: 0.7973698963838465\n",
      "Epoch 51, Training Loss: 0.797343316709294\n",
      "Epoch 52, Training Loss: 0.7970498631982242\n",
      "Epoch 53, Training Loss: 0.7968225894254797\n",
      "Epoch 54, Training Loss: 0.796832942682154\n",
      "Epoch 55, Training Loss: 0.7969014128516702\n",
      "Epoch 56, Training Loss: 0.7967940859934863\n",
      "Epoch 57, Training Loss: 0.7965726265486549\n",
      "Epoch 58, Training Loss: 0.7965416139013627\n",
      "Epoch 59, Training Loss: 0.7962783703383277\n",
      "Epoch 60, Training Loss: 0.7961393756024978\n",
      "Epoch 61, Training Loss: 0.7961356163024902\n",
      "Epoch 62, Training Loss: 0.7957393009522382\n",
      "Epoch 63, Training Loss: 0.7958263238037334\n",
      "Epoch 64, Training Loss: 0.7956303141397588\n",
      "Epoch 65, Training Loss: 0.7954314096534953\n",
      "Epoch 66, Training Loss: 0.7953358434228336\n",
      "Epoch 67, Training Loss: 0.7954062985672671\n",
      "Epoch 68, Training Loss: 0.7952989902917077\n",
      "Epoch 69, Training Loss: 0.795043079993304\n",
      "Epoch 70, Training Loss: 0.7951139184306649\n",
      "Epoch 71, Training Loss: 0.7948511123657227\n",
      "Epoch 72, Training Loss: 0.7947191441760344\n",
      "Epoch 73, Training Loss: 0.7946666232978596\n",
      "Epoch 74, Training Loss: 0.7945851446600521\n",
      "Epoch 75, Training Loss: 0.7945148080236771\n",
      "Epoch 76, Training Loss: 0.7945084915441625\n",
      "Epoch 77, Training Loss: 0.7943465574348674\n",
      "Epoch 78, Training Loss: 0.794198089277043\n",
      "Epoch 79, Training Loss: 0.7939990858470692\n",
      "Epoch 80, Training Loss: 0.7938598193841822\n",
      "Epoch 81, Training Loss: 0.7937367104782778\n",
      "Epoch 82, Training Loss: 0.7936828862218296\n",
      "Epoch 83, Training Loss: 0.7933835476987502\n",
      "Epoch 84, Training Loss: 0.7934899812586167\n",
      "Epoch 85, Training Loss: 0.7934164186786202\n",
      "Epoch 86, Training Loss: 0.7933258349054\n",
      "Epoch 87, Training Loss: 0.7931944148680743\n",
      "Epoch 88, Training Loss: 0.7931137103894178\n",
      "Epoch 89, Training Loss: 0.7930832094304702\n",
      "Epoch 90, Training Loss: 0.7929163962953231\n",
      "Epoch 91, Training Loss: 0.7929212713241577\n",
      "Epoch 92, Training Loss: 0.7929429832626792\n",
      "Epoch 93, Training Loss: 0.792671075778849\n",
      "Epoch 94, Training Loss: 0.7924590814113617\n",
      "Epoch 95, Training Loss: 0.7924642123194302\n",
      "Epoch 96, Training Loss: 0.7922940528392792\n",
      "Epoch 97, Training Loss: 0.7922910003802356\n",
      "Epoch 98, Training Loss: 0.792244896958856\n",
      "Epoch 99, Training Loss: 0.7920696166683646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:22:02,160] Trial 77 finished with value: 0.6380666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7919400546831243\n",
      "Epoch 1, Training Loss: 0.9613006237675162\n",
      "Epoch 2, Training Loss: 0.9242524191211252\n",
      "Epoch 3, Training Loss: 0.9107693320863387\n",
      "Epoch 4, Training Loss: 0.8988318271496717\n",
      "Epoch 5, Training Loss: 0.8858251146007987\n",
      "Epoch 6, Training Loss: 0.8713824083524592\n",
      "Epoch 7, Training Loss: 0.8562107498505537\n",
      "Epoch 8, Training Loss: 0.8422222077846527\n",
      "Epoch 9, Training Loss: 0.8307437081196729\n",
      "Epoch 10, Training Loss: 0.8220685015005224\n",
      "Epoch 11, Training Loss: 0.8160315923129811\n",
      "Epoch 12, Training Loss: 0.8117677797990687\n",
      "Epoch 13, Training Loss: 0.8088972173017615\n",
      "Epoch 14, Training Loss: 0.8069693722444422\n",
      "Epoch 15, Training Loss: 0.8053876695212195\n",
      "Epoch 16, Training Loss: 0.8044043383177589\n",
      "Epoch 17, Training Loss: 0.8033999527903164\n",
      "Epoch 18, Training Loss: 0.8028351133711198\n",
      "Epoch 19, Training Loss: 0.8024677637745352\n",
      "Epoch 20, Training Loss: 0.8020489442348481\n",
      "Epoch 21, Training Loss: 0.8014474150713752\n",
      "Epoch 22, Training Loss: 0.8011886588966145\n",
      "Epoch 23, Training Loss: 0.8008407172736\n",
      "Epoch 24, Training Loss: 0.8006006571124582\n",
      "Epoch 25, Training Loss: 0.8005114139528835\n",
      "Epoch 26, Training Loss: 0.8003353244416854\n",
      "Epoch 27, Training Loss: 0.8001083798268263\n",
      "Epoch 28, Training Loss: 0.7998596565162435\n",
      "Epoch 29, Training Loss: 0.7996445422312792\n",
      "Epoch 30, Training Loss: 0.7994124319272883\n",
      "Epoch 31, Training Loss: 0.7993486543963937\n",
      "Epoch 32, Training Loss: 0.7990622848622939\n",
      "Epoch 33, Training Loss: 0.7992706481148215\n",
      "Epoch 34, Training Loss: 0.7990756978708154\n",
      "Epoch 35, Training Loss: 0.7987450969920439\n",
      "Epoch 36, Training Loss: 0.798639674467199\n",
      "Epoch 37, Training Loss: 0.7986230666497175\n",
      "Epoch 38, Training Loss: 0.7984008762415717\n",
      "Epoch 39, Training Loss: 0.7984232512642355\n",
      "Epoch 40, Training Loss: 0.7982579099430758\n",
      "Epoch 41, Training Loss: 0.7980460157815148\n",
      "Epoch 42, Training Loss: 0.7979871179075803\n",
      "Epoch 43, Training Loss: 0.7981077961360707\n",
      "Epoch 44, Training Loss: 0.7979501934612498\n",
      "Epoch 45, Training Loss: 0.7977753761936637\n",
      "Epoch 46, Training Loss: 0.797549161420149\n",
      "Epoch 47, Training Loss: 0.7975680986572714\n",
      "Epoch 48, Training Loss: 0.7974052608013154\n",
      "Epoch 49, Training Loss: 0.7973924462935503\n",
      "Epoch 50, Training Loss: 0.7973560286269469\n",
      "Epoch 51, Training Loss: 0.7972057850921855\n",
      "Epoch 52, Training Loss: 0.7972248834020951\n",
      "Epoch 53, Training Loss: 0.7970286169472862\n",
      "Epoch 54, Training Loss: 0.7970825928800246\n",
      "Epoch 55, Training Loss: 0.797088139407775\n",
      "Epoch 56, Training Loss: 0.7970946625400992\n",
      "Epoch 57, Training Loss: 0.7969408268788282\n",
      "Epoch 58, Training Loss: 0.7968009576376747\n",
      "Epoch 59, Training Loss: 0.7968046600678388\n",
      "Epoch 60, Training Loss: 0.7967956852211672\n",
      "Epoch 61, Training Loss: 0.7967385474373313\n",
      "Epoch 62, Training Loss: 0.7966376367036034\n",
      "Epoch 63, Training Loss: 0.7965632471617531\n",
      "Epoch 64, Training Loss: 0.7966723159481497\n",
      "Epoch 65, Training Loss: 0.7964533916641684\n",
      "Epoch 66, Training Loss: 0.7963874921377967\n",
      "Epoch 67, Training Loss: 0.796318051815033\n",
      "Epoch 68, Training Loss: 0.796350826515871\n",
      "Epoch 69, Training Loss: 0.7961978842931635\n",
      "Epoch 70, Training Loss: 0.7962284678571364\n",
      "Epoch 71, Training Loss: 0.7959893567421857\n",
      "Epoch 72, Training Loss: 0.7960005214635064\n",
      "Epoch 73, Training Loss: 0.7959233733485727\n",
      "Epoch 74, Training Loss: 0.7959795329851262\n",
      "Epoch 75, Training Loss: 0.7959462059946621\n",
      "Epoch 76, Training Loss: 0.7958159997883965\n",
      "Epoch 77, Training Loss: 0.7958392570299261\n",
      "Epoch 78, Training Loss: 0.7957262079154744\n",
      "Epoch 79, Training Loss: 0.7957181826759787\n",
      "Epoch 80, Training Loss: 0.7954421885574565\n",
      "Epoch 81, Training Loss: 0.7955568227347206\n",
      "Epoch 82, Training Loss: 0.7954616749286652\n",
      "Epoch 83, Training Loss: 0.7953301623288322\n",
      "Epoch 84, Training Loss: 0.794949291453642\n",
      "Epoch 85, Training Loss: 0.7951104873769423\n",
      "Epoch 86, Training Loss: 0.7951452436166652\n",
      "Epoch 87, Training Loss: 0.7950961699906518\n",
      "Epoch 88, Training Loss: 0.7948189433883218\n",
      "Epoch 89, Training Loss: 0.794816877561457\n",
      "Epoch 90, Training Loss: 0.7947738795420702\n",
      "Epoch 91, Training Loss: 0.7945379185676574\n",
      "Epoch 92, Training Loss: 0.7946902144656461\n",
      "Epoch 93, Training Loss: 0.7944489889285143\n",
      "Epoch 94, Training Loss: 0.7945124154231128\n",
      "Epoch 95, Training Loss: 0.7942875869133893\n",
      "Epoch 96, Training Loss: 0.7944065365370582\n",
      "Epoch 97, Training Loss: 0.7941641224131865\n",
      "Epoch 98, Training Loss: 0.7942244665762958\n",
      "Epoch 99, Training Loss: 0.794021904188044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:23:35,129] Trial 78 finished with value: 0.6380666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7939766614577349\n",
      "Epoch 1, Training Loss: 0.9223568382119773\n",
      "Epoch 2, Training Loss: 0.8797999467168536\n",
      "Epoch 3, Training Loss: 0.8433741282699699\n",
      "Epoch 4, Training Loss: 0.8211969272982805\n",
      "Epoch 5, Training Loss: 0.8122625089229498\n",
      "Epoch 6, Training Loss: 0.8074944003184039\n",
      "Epoch 7, Training Loss: 0.8059568937559773\n",
      "Epoch 8, Training Loss: 0.804289718498861\n",
      "Epoch 9, Training Loss: 0.8032093873597625\n",
      "Epoch 10, Training Loss: 0.802794569327419\n",
      "Epoch 11, Training Loss: 0.8019843767460128\n",
      "Epoch 12, Training Loss: 0.801695265985073\n",
      "Epoch 13, Training Loss: 0.8012347561076172\n",
      "Epoch 14, Training Loss: 0.8011048415549715\n",
      "Epoch 15, Training Loss: 0.8001284489058014\n",
      "Epoch 16, Training Loss: 0.7997228158147711\n",
      "Epoch 17, Training Loss: 0.800457333173967\n",
      "Epoch 18, Training Loss: 0.7997212694103556\n",
      "Epoch 19, Training Loss: 0.7996589545020484\n",
      "Epoch 20, Training Loss: 0.7991139496179451\n",
      "Epoch 21, Training Loss: 0.7999730188147466\n",
      "Epoch 22, Training Loss: 0.798656700069743\n",
      "Epoch 23, Training Loss: 0.7993265171696369\n",
      "Epoch 24, Training Loss: 0.799054895935202\n",
      "Epoch 25, Training Loss: 0.7989750341365212\n",
      "Epoch 26, Training Loss: 0.7983129369585138\n",
      "Epoch 27, Training Loss: 0.7992748773187623\n",
      "Epoch 28, Training Loss: 0.7978110757985509\n",
      "Epoch 29, Training Loss: 0.7984840815228628\n",
      "Epoch 30, Training Loss: 0.7975501870750484\n",
      "Epoch 31, Training Loss: 0.7973571303195524\n",
      "Epoch 32, Training Loss: 0.7978253718605615\n",
      "Epoch 33, Training Loss: 0.7978402636104956\n",
      "Epoch 34, Training Loss: 0.7974456212574378\n",
      "Epoch 35, Training Loss: 0.7978382691404873\n",
      "Epoch 36, Training Loss: 0.7968223559228997\n",
      "Epoch 37, Training Loss: 0.7970679635392096\n",
      "Epoch 38, Training Loss: 0.796796803277238\n",
      "Epoch 39, Training Loss: 0.7962635296627991\n",
      "Epoch 40, Training Loss: 0.7961302371849691\n",
      "Epoch 41, Training Loss: 0.7958668252579252\n",
      "Epoch 42, Training Loss: 0.7953228864454686\n",
      "Epoch 43, Training Loss: 0.7949907260729854\n",
      "Epoch 44, Training Loss: 0.7949961547564743\n",
      "Epoch 45, Training Loss: 0.7946887409776673\n",
      "Epoch 46, Training Loss: 0.7939436282430377\n",
      "Epoch 47, Training Loss: 0.7940645893713585\n",
      "Epoch 48, Training Loss: 0.7934581627523093\n",
      "Epoch 49, Training Loss: 0.7935510015129147\n",
      "Epoch 50, Training Loss: 0.7937079366884734\n",
      "Epoch 51, Training Loss: 0.7937372147588801\n",
      "Epoch 52, Training Loss: 0.7923586112216003\n",
      "Epoch 53, Training Loss: 0.7926445136392923\n",
      "Epoch 54, Training Loss: 0.7926016899876128\n",
      "Epoch 55, Training Loss: 0.7927196136094574\n",
      "Epoch 56, Training Loss: 0.7926914388972117\n",
      "Epoch 57, Training Loss: 0.7931516371275249\n",
      "Epoch 58, Training Loss: 0.792108530657632\n",
      "Epoch 59, Training Loss: 0.7920868867322018\n",
      "Epoch 60, Training Loss: 0.7918135495114147\n",
      "Epoch 61, Training Loss: 0.7917757361455071\n",
      "Epoch 62, Training Loss: 0.7913360617214575\n",
      "Epoch 63, Training Loss: 0.7917331891848629\n",
      "Epoch 64, Training Loss: 0.7915014858532669\n",
      "Epoch 65, Training Loss: 0.7909409903942194\n",
      "Epoch 66, Training Loss: 0.7917417073608342\n",
      "Epoch 67, Training Loss: 0.7904906954532279\n",
      "Epoch 68, Training Loss: 0.7908785706175897\n",
      "Epoch 69, Training Loss: 0.7913879330893209\n",
      "Epoch 70, Training Loss: 0.7907311906491904\n",
      "Epoch 71, Training Loss: 0.7906576129726898\n",
      "Epoch 72, Training Loss: 0.7911475193231625\n",
      "Epoch 73, Training Loss: 0.7903391608618255\n",
      "Epoch 74, Training Loss: 0.790794310982066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:24:44,850] Trial 79 finished with value: 0.6334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7905692963671863\n",
      "Epoch 1, Training Loss: 0.849651526002323\n",
      "Epoch 2, Training Loss: 0.8126526590655831\n",
      "Epoch 3, Training Loss: 0.8077142522615545\n",
      "Epoch 4, Training Loss: 0.8039997351870817\n",
      "Epoch 5, Training Loss: 0.8015448860561146\n",
      "Epoch 6, Training Loss: 0.8009505782407873\n",
      "Epoch 7, Training Loss: 0.7994771895689122\n",
      "Epoch 8, Training Loss: 0.7993489478616154\n",
      "Epoch 9, Training Loss: 0.7977862673647264\n",
      "Epoch 10, Training Loss: 0.7980446961346794\n",
      "Epoch 11, Training Loss: 0.7968773640604581\n",
      "Epoch 12, Training Loss: 0.7967882739796358\n",
      "Epoch 13, Training Loss: 0.7967070902796353\n",
      "Epoch 14, Training Loss: 0.7960162039364086\n",
      "Epoch 15, Training Loss: 0.7953196318710551\n",
      "Epoch 16, Training Loss: 0.7948027979626375\n",
      "Epoch 17, Training Loss: 0.7950839754413156\n",
      "Epoch 18, Training Loss: 0.7947379716704873\n",
      "Epoch 19, Training Loss: 0.7941490094801958\n",
      "Epoch 20, Training Loss: 0.7937437354115879\n",
      "Epoch 21, Training Loss: 0.7942859445599949\n",
      "Epoch 22, Training Loss: 0.7942161458380083\n",
      "Epoch 23, Training Loss: 0.7937877812105066\n",
      "Epoch 24, Training Loss: 0.7931476419813492\n",
      "Epoch 25, Training Loss: 0.7936679156387554\n",
      "Epoch 26, Training Loss: 0.7926548154213849\n",
      "Epoch 27, Training Loss: 0.7924675022854525\n",
      "Epoch 28, Training Loss: 0.7923512633407818\n",
      "Epoch 29, Training Loss: 0.7924641904410193\n",
      "Epoch 30, Training Loss: 0.7929926627523759\n",
      "Epoch 31, Training Loss: 0.7915663365055533\n",
      "Epoch 32, Training Loss: 0.792167350544649\n",
      "Epoch 33, Training Loss: 0.7920437917288612\n",
      "Epoch 34, Training Loss: 0.7915862148649553\n",
      "Epoch 35, Training Loss: 0.791562209479949\n",
      "Epoch 36, Training Loss: 0.7917373940523933\n",
      "Epoch 37, Training Loss: 0.7919083781102124\n",
      "Epoch 38, Training Loss: 0.7915237151875215\n",
      "Epoch 39, Training Loss: 0.7916138369896832\n",
      "Epoch 40, Training Loss: 0.7910514151348788\n",
      "Epoch 41, Training Loss: 0.7909615328732659\n",
      "Epoch 42, Training Loss: 0.7915428609006545\n",
      "Epoch 43, Training Loss: 0.7908333482461817\n",
      "Epoch 44, Training Loss: 0.7914446080432219\n",
      "Epoch 45, Training Loss: 0.7908825933933258\n",
      "Epoch 46, Training Loss: 0.7915856060561012\n",
      "Epoch 47, Training Loss: 0.7908491715964149\n",
      "Epoch 48, Training Loss: 0.791049499020857\n",
      "Epoch 49, Training Loss: 0.7914920773926903\n",
      "Epoch 50, Training Loss: 0.7907665985472062\n",
      "Epoch 51, Training Loss: 0.7909489021581761\n",
      "Epoch 52, Training Loss: 0.7907325203278486\n",
      "Epoch 53, Training Loss: 0.790866522438386\n",
      "Epoch 54, Training Loss: 0.7908371284428765\n",
      "Epoch 55, Training Loss: 0.7906576822084539\n",
      "Epoch 56, Training Loss: 0.7905203090695774\n",
      "Epoch 57, Training Loss: 0.7908678849304424\n",
      "Epoch 58, Training Loss: 0.7908884964970981\n",
      "Epoch 59, Training Loss: 0.7904227392112507\n",
      "Epoch 60, Training Loss: 0.7905530733921948\n",
      "Epoch 61, Training Loss: 0.7911582288321327\n",
      "Epoch 62, Training Loss: 0.7909477250015035\n",
      "Epoch 63, Training Loss: 0.7900526217152091\n",
      "Epoch 64, Training Loss: 0.7903153970662286\n",
      "Epoch 65, Training Loss: 0.7901739818208358\n",
      "Epoch 66, Training Loss: 0.7908503939123714\n",
      "Epoch 67, Training Loss: 0.7898409623258255\n",
      "Epoch 68, Training Loss: 0.7904114274417653\n",
      "Epoch 69, Training Loss: 0.7899052319807165\n",
      "Epoch 70, Training Loss: 0.790372180447859\n",
      "Epoch 71, Training Loss: 0.7891045530403361\n",
      "Epoch 72, Training Loss: 0.7900403722594767\n",
      "Epoch 73, Training Loss: 0.7898470790947185\n",
      "Epoch 74, Training Loss: 0.7898458204549902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:26:19,743] Trial 80 finished with value: 0.6361333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7894089042439181\n",
      "Epoch 1, Training Loss: 0.8484627521739286\n",
      "Epoch 2, Training Loss: 0.8123634393074933\n",
      "Epoch 3, Training Loss: 0.809689601168913\n",
      "Epoch 4, Training Loss: 0.8074962835452136\n",
      "Epoch 5, Training Loss: 0.8069038711575901\n",
      "Epoch 6, Training Loss: 0.8041415762901306\n",
      "Epoch 7, Training Loss: 0.80340817044763\n",
      "Epoch 8, Training Loss: 0.8024356878505033\n",
      "Epoch 9, Training Loss: 0.8021450539196239\n",
      "Epoch 10, Training Loss: 0.8004436401759877\n",
      "Epoch 11, Training Loss: 0.7999045134291929\n",
      "Epoch 12, Training Loss: 0.7997232860677382\n",
      "Epoch 13, Training Loss: 0.7992199840966393\n",
      "Epoch 14, Training Loss: 0.7978467418866999\n",
      "Epoch 15, Training Loss: 0.7974048042297364\n",
      "Epoch 16, Training Loss: 0.7976408996301538\n",
      "Epoch 17, Training Loss: 0.797972681872985\n",
      "Epoch 18, Training Loss: 0.7971298415520612\n",
      "Epoch 19, Training Loss: 0.796749782632379\n",
      "Epoch 20, Training Loss: 0.7961685068467084\n",
      "Epoch 21, Training Loss: 0.7963650516201468\n",
      "Epoch 22, Training Loss: 0.79549341692644\n",
      "Epoch 23, Training Loss: 0.7952863888880786\n",
      "Epoch 24, Training Loss: 0.7952893128815819\n",
      "Epoch 25, Training Loss: 0.7952887226553524\n",
      "Epoch 26, Training Loss: 0.7951722264289856\n",
      "Epoch 27, Training Loss: 0.7948588728203493\n",
      "Epoch 28, Training Loss: 0.7955782093020046\n",
      "Epoch 29, Training Loss: 0.7942343957283917\n",
      "Epoch 30, Training Loss: 0.7940929332200218\n",
      "Epoch 31, Training Loss: 0.7950074704254375\n",
      "Epoch 32, Training Loss: 0.7941823281961329\n",
      "Epoch 33, Training Loss: 0.7943237012975356\n",
      "Epoch 34, Training Loss: 0.7942288497616263\n",
      "Epoch 35, Training Loss: 0.7938784030605764\n",
      "Epoch 36, Training Loss: 0.794275773413041\n",
      "Epoch 37, Training Loss: 0.7933345635498271\n",
      "Epoch 38, Training Loss: 0.794193994437947\n",
      "Epoch 39, Training Loss: 0.7935289849954493\n",
      "Epoch 40, Training Loss: 0.7936253493673662\n",
      "Epoch 41, Training Loss: 0.7929000664458555\n",
      "Epoch 42, Training Loss: 0.7938066002200631\n",
      "Epoch 43, Training Loss: 0.7937422884211821\n",
      "Epoch 44, Training Loss: 0.7941983613547157\n",
      "Epoch 45, Training Loss: 0.7939861291997573\n",
      "Epoch 46, Training Loss: 0.7938899510748246\n",
      "Epoch 47, Training Loss: 0.7929361594424528\n",
      "Epoch 48, Training Loss: 0.7934377807028153\n",
      "Epoch 49, Training Loss: 0.7934525538192075\n",
      "Epoch 50, Training Loss: 0.7930102393907659\n",
      "Epoch 51, Training Loss: 0.7927643892344306\n",
      "Epoch 52, Training Loss: 0.793078602622537\n",
      "Epoch 53, Training Loss: 0.7934222149848938\n",
      "Epoch 54, Training Loss: 0.7931304728283601\n",
      "Epoch 55, Training Loss: 0.7928381022285013\n",
      "Epoch 56, Training Loss: 0.7927372485749862\n",
      "Epoch 57, Training Loss: 0.7925520426385543\n",
      "Epoch 58, Training Loss: 0.7928013209034415\n",
      "Epoch 59, Training Loss: 0.7922716214376337\n",
      "Epoch 60, Training Loss: 0.7924055708857144\n",
      "Epoch 61, Training Loss: 0.7927269847252789\n",
      "Epoch 62, Training Loss: 0.7917029348541709\n",
      "Epoch 63, Training Loss: 0.7928140335924485\n",
      "Epoch 64, Training Loss: 0.7915404176010805\n",
      "Epoch 65, Training Loss: 0.7925264493858113\n",
      "Epoch 66, Training Loss: 0.7923405171843136\n",
      "Epoch 67, Training Loss: 0.792819691545823\n",
      "Epoch 68, Training Loss: 0.7921953641667085\n",
      "Epoch 69, Training Loss: 0.791718768232009\n",
      "Epoch 70, Training Loss: 0.7926655960083008\n",
      "Epoch 71, Training Loss: 0.7922125467833351\n",
      "Epoch 72, Training Loss: 0.7921713003691505\n",
      "Epoch 73, Training Loss: 0.7922789027410395\n",
      "Epoch 74, Training Loss: 0.7918339764370638\n",
      "Epoch 75, Training Loss: 0.7923324049921596\n",
      "Epoch 76, Training Loss: 0.7921516052414389\n",
      "Epoch 77, Training Loss: 0.7914073256885304\n",
      "Epoch 78, Training Loss: 0.7921315818674424\n",
      "Epoch 79, Training Loss: 0.7920372942615957\n",
      "Epoch 80, Training Loss: 0.7918511436266058\n",
      "Epoch 81, Training Loss: 0.7918078696727753\n",
      "Epoch 82, Training Loss: 0.7918489427426282\n",
      "Epoch 83, Training Loss: 0.7916056405095493\n",
      "Epoch 84, Training Loss: 0.7915077573411605\n",
      "Epoch 85, Training Loss: 0.7923441571347853\n",
      "Epoch 86, Training Loss: 0.7917796526936923\n",
      "Epoch 87, Training Loss: 0.7909452008499819\n",
      "Epoch 88, Training Loss: 0.7917032354719499\n",
      "Epoch 89, Training Loss: 0.7913607729883755\n",
      "Epoch 90, Training Loss: 0.7921749927717097\n",
      "Epoch 91, Training Loss: 0.7912077169558581\n",
      "Epoch 92, Training Loss: 0.7913855627003838\n",
      "Epoch 93, Training Loss: 0.7911293242959415\n",
      "Epoch 94, Training Loss: 0.7924528260792003\n",
      "Epoch 95, Training Loss: 0.7916279318753411\n",
      "Epoch 96, Training Loss: 0.7921165852686938\n",
      "Epoch 97, Training Loss: 0.7914993256681105\n",
      "Epoch 98, Training Loss: 0.7907701119955848\n",
      "Epoch 99, Training Loss: 0.7911899010574116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:28:15,273] Trial 81 finished with value: 0.6400666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7914451602627249\n",
      "Epoch 1, Training Loss: 0.8486345794621636\n",
      "Epoch 2, Training Loss: 0.8235938199828653\n",
      "Epoch 3, Training Loss: 0.8162824817264781\n",
      "Epoch 4, Training Loss: 0.8161145247431363\n",
      "Epoch 5, Training Loss: 0.8142931306362152\n",
      "Epoch 6, Training Loss: 0.8123613713769352\n",
      "Epoch 7, Training Loss: 0.8139747825790854\n",
      "Epoch 8, Training Loss: 0.8122552497246686\n",
      "Epoch 9, Training Loss: 0.8125081377169665\n",
      "Epoch 10, Training Loss: 0.811859318859437\n",
      "Epoch 11, Training Loss: 0.8099571876666125\n",
      "Epoch 12, Training Loss: 0.8118234717144686\n",
      "Epoch 13, Training Loss: 0.8104965873325573\n",
      "Epoch 14, Training Loss: 0.809427341222763\n",
      "Epoch 15, Training Loss: 0.8084685148912317\n",
      "Epoch 16, Training Loss: 0.8092346624065848\n",
      "Epoch 17, Training Loss: 0.8097328478448531\n",
      "Epoch 18, Training Loss: 0.8099837353650261\n",
      "Epoch 19, Training Loss: 0.8095541860075558\n",
      "Epoch 20, Training Loss: 0.8087478039545172\n",
      "Epoch 21, Training Loss: 0.8087514723749721\n",
      "Epoch 22, Training Loss: 0.806832654616412\n",
      "Epoch 23, Training Loss: 0.8082262229218202\n",
      "Epoch 24, Training Loss: 0.8056517663422753\n",
      "Epoch 25, Training Loss: 0.8093493663563448\n",
      "Epoch 26, Training Loss: 0.8046098050650429\n",
      "Epoch 27, Training Loss: 0.8074480870190789\n",
      "Epoch 28, Training Loss: 0.8055360369822558\n",
      "Epoch 29, Training Loss: 0.8042239064328811\n",
      "Epoch 30, Training Loss: 0.8060373587468092\n",
      "Epoch 31, Training Loss: 0.8062202904504888\n",
      "Epoch 32, Training Loss: 0.8046595003324396\n",
      "Epoch 33, Training Loss: 0.8026362484342912\n",
      "Epoch 34, Training Loss: 0.8059077367361854\n",
      "Epoch 35, Training Loss: 0.8033757052000832\n",
      "Epoch 36, Training Loss: 0.8037151936222525\n",
      "Epoch 37, Training Loss: 0.8029833620436051\n",
      "Epoch 38, Training Loss: 0.8021242137516246\n",
      "Epoch 39, Training Loss: 0.8058220534464893\n",
      "Epoch 40, Training Loss: 0.8037141922642203\n",
      "Epoch 41, Training Loss: 0.8032718562378602\n",
      "Epoch 42, Training Loss: 0.8018152245353249\n",
      "Epoch 43, Training Loss: 0.8017672336101532\n",
      "Epoch 44, Training Loss: 0.8022600368892445\n",
      "Epoch 45, Training Loss: 0.8030870649393868\n",
      "Epoch 46, Training Loss: 0.8007532823085785\n",
      "Epoch 47, Training Loss: 0.8022885772059946\n",
      "Epoch 48, Training Loss: 0.8025888266282923\n",
      "Epoch 49, Training Loss: 0.8010910546078401\n",
      "Epoch 50, Training Loss: 0.8021998150208417\n",
      "Epoch 51, Training Loss: 0.8019225663998548\n",
      "Epoch 52, Training Loss: 0.8023948451350716\n",
      "Epoch 53, Training Loss: 0.8015353133397943\n",
      "Epoch 54, Training Loss: 0.801289725373773\n",
      "Epoch 55, Training Loss: 0.8009244078047135\n",
      "Epoch 56, Training Loss: 0.8008730877147001\n",
      "Epoch 57, Training Loss: 0.8015917385325713\n",
      "Epoch 58, Training Loss: 0.7995727536958807\n",
      "Epoch 59, Training Loss: 0.8005235409736633\n",
      "Epoch 60, Training Loss: 0.8014242484289057\n",
      "Epoch 61, Training Loss: 0.8009379220710081\n",
      "Epoch 62, Training Loss: 0.8005444274930393\n",
      "Epoch 63, Training Loss: 0.800519906352548\n",
      "Epoch 64, Training Loss: 0.8021713640409357\n",
      "Epoch 65, Training Loss: 0.801818712458891\n",
      "Epoch 66, Training Loss: 0.8002126655157875\n",
      "Epoch 67, Training Loss: 0.7995926871019251\n",
      "Epoch 68, Training Loss: 0.800420531875947\n",
      "Epoch 69, Training Loss: 0.799445079424802\n",
      "Epoch 70, Training Loss: 0.8001245233591865\n",
      "Epoch 71, Training Loss: 0.8017357598332798\n",
      "Epoch 72, Training Loss: 0.8001005876765531\n",
      "Epoch 73, Training Loss: 0.7999534973677467\n",
      "Epoch 74, Training Loss: 0.7993574946768144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:29:43,022] Trial 82 finished with value: 0.6322 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7999460369699142\n",
      "Epoch 1, Training Loss: 1.0831479846729952\n",
      "Epoch 2, Training Loss: 1.048252627989825\n",
      "Epoch 3, Training Loss: 1.0253620631554548\n",
      "Epoch 4, Training Loss: 1.007474353944554\n",
      "Epoch 5, Training Loss: 0.9931770054031821\n",
      "Epoch 6, Training Loss: 0.9816873892615823\n",
      "Epoch 7, Training Loss: 0.9724035860510434\n",
      "Epoch 8, Training Loss: 0.9648963770445655\n",
      "Epoch 9, Training Loss: 0.9588201310354121\n",
      "Epoch 10, Training Loss: 0.9538875059520497\n",
      "Epoch 11, Training Loss: 0.9498626523158129\n",
      "Epoch 12, Training Loss: 0.9465458125927869\n",
      "Epoch 13, Training Loss: 0.9437921420265647\n",
      "Epoch 14, Training Loss: 0.9414669747212354\n",
      "Epoch 15, Training Loss: 0.9394894798362956\n",
      "Epoch 16, Training Loss: 0.9377711911061231\n",
      "Epoch 17, Training Loss: 0.9362669075236602\n",
      "Epoch 18, Training Loss: 0.9349185475181131\n",
      "Epoch 19, Training Loss: 0.9337082990478067\n",
      "Epoch 20, Training Loss: 0.9325951539067661\n",
      "Epoch 21, Training Loss: 0.9315535866512972\n",
      "Epoch 22, Training Loss: 0.930580117001253\n",
      "Epoch 23, Training Loss: 0.9296727798966801\n",
      "Epoch 24, Training Loss: 0.9287909258113188\n",
      "Epoch 25, Training Loss: 0.9279619674822863\n",
      "Epoch 26, Training Loss: 0.9271541347924401\n",
      "Epoch 27, Training Loss: 0.9263668180213255\n",
      "Epoch 28, Training Loss: 0.9256149099854862\n",
      "Epoch 29, Training Loss: 0.9248715284291436\n",
      "Epoch 30, Training Loss: 0.9241630866247065\n",
      "Epoch 31, Training Loss: 0.9234534677337197\n",
      "Epoch 32, Training Loss: 0.9227632148826823\n",
      "Epoch 33, Training Loss: 0.922082395623712\n",
      "Epoch 34, Training Loss: 0.9214115492035361\n",
      "Epoch 35, Training Loss: 0.9207481668977177\n",
      "Epoch 36, Training Loss: 0.9200872771880206\n",
      "Epoch 37, Training Loss: 0.9194424721072701\n",
      "Epoch 38, Training Loss: 0.9188007314064923\n",
      "Epoch 39, Training Loss: 0.9181624221801757\n",
      "Epoch 40, Training Loss: 0.9175326052834006\n",
      "Epoch 41, Training Loss: 0.9169031086388756\n",
      "Epoch 42, Training Loss: 0.9162758306895985\n",
      "Epoch 43, Training Loss: 0.9156595421538634\n",
      "Epoch 44, Training Loss: 0.9150356059214648\n",
      "Epoch 45, Training Loss: 0.9144206563164207\n",
      "Epoch 46, Training Loss: 0.9137939265195061\n",
      "Epoch 47, Training Loss: 0.9131857245108661\n",
      "Epoch 48, Training Loss: 0.9125616350594689\n",
      "Epoch 49, Training Loss: 0.9119415605068206\n",
      "Epoch 50, Training Loss: 0.911317334245233\n",
      "Epoch 51, Training Loss: 0.9106915980226853\n",
      "Epoch 52, Training Loss: 0.9100728778278127\n",
      "Epoch 53, Training Loss: 0.9094400295089273\n",
      "Epoch 54, Training Loss: 0.9088200163140017\n",
      "Epoch 55, Training Loss: 0.9081812729555018\n",
      "Epoch 56, Training Loss: 0.9075529003844541\n",
      "Epoch 57, Training Loss: 0.9069185997458066\n",
      "Epoch 58, Training Loss: 0.9062815809951109\n",
      "Epoch 59, Training Loss: 0.9056532111588647\n",
      "Epoch 60, Training Loss: 0.9049981727319605\n",
      "Epoch 61, Training Loss: 0.9043586313023286\n",
      "Epoch 62, Training Loss: 0.9037092235509087\n",
      "Epoch 63, Training Loss: 0.9030616037985858\n",
      "Epoch 64, Training Loss: 0.9024042238207425\n",
      "Epoch 65, Training Loss: 0.9017583363196429\n",
      "Epoch 66, Training Loss: 0.9010960198149962\n",
      "Epoch 67, Training Loss: 0.9004357106545392\n",
      "Epoch 68, Training Loss: 0.899768652565339\n",
      "Epoch 69, Training Loss: 0.8991038494951584\n",
      "Epoch 70, Training Loss: 0.8984294155064751\n",
      "Epoch 71, Training Loss: 0.8977507987443138\n",
      "Epoch 72, Training Loss: 0.8970751516258015\n",
      "Epoch 73, Training Loss: 0.8963894381943871\n",
      "Epoch 74, Training Loss: 0.8957145882354063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:30:53,207] Trial 83 finished with value: 0.5773333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8950303229864905\n",
      "Epoch 1, Training Loss: 0.8805202646816478\n",
      "Epoch 2, Training Loss: 0.8165020503717311\n",
      "Epoch 3, Training Loss: 0.8129943532102248\n",
      "Epoch 4, Training Loss: 0.8091611722637625\n",
      "Epoch 5, Training Loss: 0.8052898665736703\n",
      "Epoch 6, Training Loss: 0.803715266900904\n",
      "Epoch 7, Training Loss: 0.8007753423382254\n",
      "Epoch 8, Training Loss: 0.7970518185110653\n",
      "Epoch 9, Training Loss: 0.7948291179713081\n",
      "Epoch 10, Training Loss: 0.7939162241010105\n",
      "Epoch 11, Training Loss: 0.7927721859427059\n",
      "Epoch 12, Training Loss: 0.7920394437453326\n",
      "Epoch 13, Training Loss: 0.7919157528877259\n",
      "Epoch 14, Training Loss: 0.7914100480780882\n",
      "Epoch 15, Training Loss: 0.7901012679408578\n",
      "Epoch 16, Training Loss: 0.7898743285151089\n",
      "Epoch 17, Training Loss: 0.7901708566441256\n",
      "Epoch 18, Training Loss: 0.7893171088835772\n",
      "Epoch 19, Training Loss: 0.788277426887961\n",
      "Epoch 20, Training Loss: 0.7886556443046121\n",
      "Epoch 21, Training Loss: 0.7885611720646128\n",
      "Epoch 22, Training Loss: 0.7879746139750761\n",
      "Epoch 23, Training Loss: 0.7879677776028128\n",
      "Epoch 24, Training Loss: 0.7873432716902564\n",
      "Epoch 25, Training Loss: 0.7871812262254603\n",
      "Epoch 26, Training Loss: 0.7874068399036632\n",
      "Epoch 27, Training Loss: 0.7865479071701275\n",
      "Epoch 28, Training Loss: 0.7869304934669944\n",
      "Epoch 29, Training Loss: 0.7864091025380527\n",
      "Epoch 30, Training Loss: 0.7864851532262914\n",
      "Epoch 31, Training Loss: 0.7853889358043671\n",
      "Epoch 32, Training Loss: 0.7864995675928452\n",
      "Epoch 33, Training Loss: 0.7868242943286896\n",
      "Epoch 34, Training Loss: 0.7862607554127188\n",
      "Epoch 35, Training Loss: 0.7857505602696363\n",
      "Epoch 36, Training Loss: 0.7857641060212079\n",
      "Epoch 37, Training Loss: 0.7855353884135976\n",
      "Epoch 38, Training Loss: 0.7845809050167308\n",
      "Epoch 39, Training Loss: 0.7854908598170561\n",
      "Epoch 40, Training Loss: 0.7847191027332755\n",
      "Epoch 41, Training Loss: 0.7846911897378809\n",
      "Epoch 42, Training Loss: 0.7846737919134252\n",
      "Epoch 43, Training Loss: 0.7846497788148767\n",
      "Epoch 44, Training Loss: 0.7848585626658271\n",
      "Epoch 45, Training Loss: 0.7840911891179926\n",
      "Epoch 46, Training Loss: 0.7849274415829602\n",
      "Epoch 47, Training Loss: 0.7840110196085537\n",
      "Epoch 48, Training Loss: 0.7839815106111414\n",
      "Epoch 49, Training Loss: 0.7840060359590194\n",
      "Epoch 50, Training Loss: 0.7844205113719491\n",
      "Epoch 51, Training Loss: 0.783563253809424\n",
      "Epoch 52, Training Loss: 0.7839875825012431\n",
      "Epoch 53, Training Loss: 0.7829413933613721\n",
      "Epoch 54, Training Loss: 0.7838611400828642\n",
      "Epoch 55, Training Loss: 0.7831837732651654\n",
      "Epoch 56, Training Loss: 0.7839156694271985\n",
      "Epoch 57, Training Loss: 0.7833191631822025\n",
      "Epoch 58, Training Loss: 0.7826826561899746\n",
      "Epoch 59, Training Loss: 0.7824214161143583\n",
      "Epoch 60, Training Loss: 0.7826581520192764\n",
      "Epoch 61, Training Loss: 0.7831889133593616\n",
      "Epoch 62, Training Loss: 0.7829311280390796\n",
      "Epoch 63, Training Loss: 0.7820655364849988\n",
      "Epoch 64, Training Loss: 0.7819471904810738\n",
      "Epoch 65, Training Loss: 0.7824188482761383\n",
      "Epoch 66, Training Loss: 0.782396496183732\n",
      "Epoch 67, Training Loss: 0.7816620603729697\n",
      "Epoch 68, Training Loss: 0.7820774262792924\n",
      "Epoch 69, Training Loss: 0.7823224524189444\n",
      "Epoch 70, Training Loss: 0.7819655003968408\n",
      "Epoch 71, Training Loss: 0.7814089666394627\n",
      "Epoch 72, Training Loss: 0.7815955397661994\n",
      "Epoch 73, Training Loss: 0.7813851510777193\n",
      "Epoch 74, Training Loss: 0.7812462344590355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:32:33,681] Trial 84 finished with value: 0.6378 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7810341471784255\n",
      "Epoch 1, Training Loss: 0.8886069620462288\n",
      "Epoch 2, Training Loss: 0.8394828386772845\n",
      "Epoch 3, Training Loss: 0.834157437489445\n",
      "Epoch 4, Training Loss: 0.8271439692131559\n",
      "Epoch 5, Training Loss: 0.8235211150986808\n",
      "Epoch 6, Training Loss: 0.8230161847924827\n",
      "Epoch 7, Training Loss: 0.8220888460489144\n",
      "Epoch 8, Training Loss: 0.8204361531071197\n",
      "Epoch 9, Training Loss: 0.8248435134278205\n",
      "Epoch 10, Training Loss: 0.8168894319606007\n",
      "Epoch 11, Training Loss: 0.8196534396114206\n",
      "Epoch 12, Training Loss: 0.8194842567121176\n",
      "Epoch 13, Training Loss: 0.8188521716827737\n",
      "Epoch 14, Training Loss: 0.816491357007421\n",
      "Epoch 15, Training Loss: 0.8207092092449504\n",
      "Epoch 16, Training Loss: 0.8167769996743454\n",
      "Epoch 17, Training Loss: 0.8183167031384948\n",
      "Epoch 18, Training Loss: 0.8143799732502242\n",
      "Epoch 19, Training Loss: 0.8215619105145447\n",
      "Epoch 20, Training Loss: 0.8134336668745916\n",
      "Epoch 21, Training Loss: 0.8163074121439368\n",
      "Epoch 22, Training Loss: 0.8152868493159015\n",
      "Epoch 23, Training Loss: 0.8150681569163961\n",
      "Epoch 24, Training Loss: 0.8127064417179366\n",
      "Epoch 25, Training Loss: 0.8126058809739306\n",
      "Epoch 26, Training Loss: 0.8130697323863668\n",
      "Epoch 27, Training Loss: 0.8132685252598354\n",
      "Epoch 28, Training Loss: 0.8133912518508453\n",
      "Epoch 29, Training Loss: 0.8115100877177447\n",
      "Epoch 30, Training Loss: 0.8122184871730947\n",
      "Epoch 31, Training Loss: 0.8127442868132341\n",
      "Epoch 32, Training Loss: 0.8121679034448208\n",
      "Epoch 33, Training Loss: 0.8103277130234511\n",
      "Epoch 34, Training Loss: 0.8140183253395826\n",
      "Epoch 35, Training Loss: 0.8104265675508886\n",
      "Epoch 36, Training Loss: 0.8102180711308816\n",
      "Epoch 37, Training Loss: 0.8098888489536773\n",
      "Epoch 38, Training Loss: 0.8132283436624628\n",
      "Epoch 39, Training Loss: 0.8114635347423697\n",
      "Epoch 40, Training Loss: 0.8101531950154699\n",
      "Epoch 41, Training Loss: 0.8098694626550029\n",
      "Epoch 42, Training Loss: 0.8111350375010555\n",
      "Epoch 43, Training Loss: 0.8137849207211257\n",
      "Epoch 44, Training Loss: 0.8106091636464112\n",
      "Epoch 45, Training Loss: 0.8084288442941536\n",
      "Epoch 46, Training Loss: 0.8080210073549945\n",
      "Epoch 47, Training Loss: 0.809650985459636\n",
      "Epoch 48, Training Loss: 0.8091853878551856\n",
      "Epoch 49, Training Loss: 0.8094197500917248\n",
      "Epoch 50, Training Loss: 0.8068416478938626\n",
      "Epoch 51, Training Loss: 0.8087915765611748\n",
      "Epoch 52, Training Loss: 0.8093343090293998\n",
      "Epoch 53, Training Loss: 0.8092633708079059\n",
      "Epoch 54, Training Loss: 0.8080496076354406\n",
      "Epoch 55, Training Loss: 0.8096123151313094\n",
      "Epoch 56, Training Loss: 0.8068219846800754\n",
      "Epoch 57, Training Loss: 0.8074173578642365\n",
      "Epoch 58, Training Loss: 0.8098743219124643\n",
      "Epoch 59, Training Loss: 0.8087877957444441\n",
      "Epoch 60, Training Loss: 0.8077410299078862\n",
      "Epoch 61, Training Loss: 0.8090953998995903\n",
      "Epoch 62, Training Loss: 0.8086077074359234\n",
      "Epoch 63, Training Loss: 0.8104896836711052\n",
      "Epoch 64, Training Loss: 0.8091884682053014\n",
      "Epoch 65, Training Loss: 0.8097644856997899\n",
      "Epoch 66, Training Loss: 0.8087303381217154\n",
      "Epoch 67, Training Loss: 0.8080025879960311\n",
      "Epoch 68, Training Loss: 0.804065964454995\n",
      "Epoch 69, Training Loss: 0.8098097310030371\n",
      "Epoch 70, Training Loss: 0.806465516681958\n",
      "Epoch 71, Training Loss: 0.8081048752132215\n",
      "Epoch 72, Training Loss: 0.8043749698122641\n",
      "Epoch 73, Training Loss: 0.8060247398856887\n",
      "Epoch 74, Training Loss: 0.8082459220312592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:33:45,656] Trial 85 finished with value: 0.5762 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8039541460517654\n",
      "Epoch 1, Training Loss: 0.8695034755678738\n",
      "Epoch 2, Training Loss: 0.8181069332711837\n",
      "Epoch 3, Training Loss: 0.8152425423790427\n",
      "Epoch 4, Training Loss: 0.8121244084133822\n",
      "Epoch 5, Training Loss: 0.8103165970830356\n",
      "Epoch 6, Training Loss: 0.808661536469179\n",
      "Epoch 7, Training Loss: 0.809253146648407\n",
      "Epoch 8, Training Loss: 0.8082940970448886\n",
      "Epoch 9, Training Loss: 0.8090166524578544\n",
      "Epoch 10, Training Loss: 0.8069556688561159\n",
      "Epoch 11, Training Loss: 0.8056891561956967\n",
      "Epoch 12, Training Loss: 0.8049497886265026\n",
      "Epoch 13, Training Loss: 0.8060863456305336\n",
      "Epoch 14, Training Loss: 0.8056457399620729\n",
      "Epoch 15, Training Loss: 0.8061065896819619\n",
      "Epoch 16, Training Loss: 0.8038397231522728\n",
      "Epoch 17, Training Loss: 0.8036897676131305\n",
      "Epoch 18, Training Loss: 0.8030279048751382\n",
      "Epoch 19, Training Loss: 0.802647089327083\n",
      "Epoch 20, Training Loss: 0.802325277188245\n",
      "Epoch 21, Training Loss: 0.8029463472787072\n",
      "Epoch 22, Training Loss: 0.8020900378507726\n",
      "Epoch 23, Training Loss: 0.8019871280473821\n",
      "Epoch 24, Training Loss: 0.8014450600567986\n",
      "Epoch 25, Training Loss: 0.799968636526781\n",
      "Epoch 26, Training Loss: 0.8007806438558241\n",
      "Epoch 27, Training Loss: 0.8006090108787313\n",
      "Epoch 28, Training Loss: 0.80055892292191\n",
      "Epoch 29, Training Loss: 0.7994156721760245\n",
      "Epoch 30, Training Loss: 0.7997236202744876\n",
      "Epoch 31, Training Loss: 0.799573248204063\n",
      "Epoch 32, Training Loss: 0.7988425474307116\n",
      "Epoch 33, Training Loss: 0.7977758433538324\n",
      "Epoch 34, Training Loss: 0.7989929175376892\n",
      "Epoch 35, Training Loss: 0.7983380219515632\n",
      "Epoch 36, Training Loss: 0.7982413336108712\n",
      "Epoch 37, Training Loss: 0.7970433706395766\n",
      "Epoch 38, Training Loss: 0.7972877675645491\n",
      "Epoch 39, Training Loss: 0.7969655384035671\n",
      "Epoch 40, Training Loss: 0.7962468173924614\n",
      "Epoch 41, Training Loss: 0.795860664914636\n",
      "Epoch 42, Training Loss: 0.7951687009194318\n",
      "Epoch 43, Training Loss: 0.7954237839053658\n",
      "Epoch 44, Training Loss: 0.7944497857374303\n",
      "Epoch 45, Training Loss: 0.7955969569963567\n",
      "Epoch 46, Training Loss: 0.7951867250835194\n",
      "Epoch 47, Training Loss: 0.794826816180173\n",
      "Epoch 48, Training Loss: 0.7943464873818791\n",
      "Epoch 49, Training Loss: 0.7943932100604563\n",
      "Epoch 50, Training Loss: 0.7940859089879428\n",
      "Epoch 51, Training Loss: 0.7944748161820805\n",
      "Epoch 52, Training Loss: 0.7941928644741283\n",
      "Epoch 53, Training Loss: 0.7938456234511208\n",
      "Epoch 54, Training Loss: 0.7940827723811654\n",
      "Epoch 55, Training Loss: 0.7936585541332469\n",
      "Epoch 56, Training Loss: 0.7935260246781742\n",
      "Epoch 57, Training Loss: 0.7933228839846218\n",
      "Epoch 58, Training Loss: 0.7933142086337595\n",
      "Epoch 59, Training Loss: 0.7921213053955751\n",
      "Epoch 60, Training Loss: 0.7924915651714101\n",
      "Epoch 61, Training Loss: 0.7936928122183856\n",
      "Epoch 62, Training Loss: 0.7930369688482846\n",
      "Epoch 63, Training Loss: 0.7926290908280541\n",
      "Epoch 64, Training Loss: 0.7921764164812425\n",
      "Epoch 65, Training Loss: 0.7924916298249188\n",
      "Epoch 66, Training Loss: 0.7922514083104976\n",
      "Epoch 67, Training Loss: 0.7927689788622014\n",
      "Epoch 68, Training Loss: 0.7919487094177919\n",
      "Epoch 69, Training Loss: 0.792238888389924\n",
      "Epoch 70, Training Loss: 0.7926692560139824\n",
      "Epoch 71, Training Loss: 0.7923592150211334\n",
      "Epoch 72, Training Loss: 0.7921405199696036\n",
      "Epoch 73, Training Loss: 0.7920095419182497\n",
      "Epoch 74, Training Loss: 0.791681294932085\n",
      "Epoch 75, Training Loss: 0.7917558752087985\n",
      "Epoch 76, Training Loss: 0.7925681535636677\n",
      "Epoch 77, Training Loss: 0.7916018758801853\n",
      "Epoch 78, Training Loss: 0.7914853232748368\n",
      "Epoch 79, Training Loss: 0.7922862531157101\n",
      "Epoch 80, Training Loss: 0.7912778849461499\n",
      "Epoch 81, Training Loss: 0.7912623672625598\n",
      "Epoch 82, Training Loss: 0.7909292244911194\n",
      "Epoch 83, Training Loss: 0.7913799872117884\n",
      "Epoch 84, Training Loss: 0.7909020058547749\n",
      "Epoch 85, Training Loss: 0.7909075161989998\n",
      "Epoch 86, Training Loss: 0.7912629814708934\n",
      "Epoch 87, Training Loss: 0.791705343022066\n",
      "Epoch 88, Training Loss: 0.7909103283461403\n",
      "Epoch 89, Training Loss: 0.7907590398367713\n",
      "Epoch 90, Training Loss: 0.7915177687476663\n",
      "Epoch 91, Training Loss: 0.7911407761714038\n",
      "Epoch 92, Training Loss: 0.7907669477603014\n",
      "Epoch 93, Training Loss: 0.7912071644558626\n",
      "Epoch 94, Training Loss: 0.7911367865169749\n",
      "Epoch 95, Training Loss: 0.7905729757337009\n",
      "Epoch 96, Training Loss: 0.7912099647521973\n",
      "Epoch 97, Training Loss: 0.790328027641072\n",
      "Epoch 98, Training Loss: 0.7907016275209539\n",
      "Epoch 99, Training Loss: 0.7909303037559285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:35:40,759] Trial 86 finished with value: 0.6363333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7899215001218459\n",
      "Epoch 1, Training Loss: 1.0916492458572962\n",
      "Epoch 2, Training Loss: 1.085328843898343\n",
      "Epoch 3, Training Loss: 1.078732791520599\n",
      "Epoch 4, Training Loss: 1.06946981849527\n",
      "Epoch 5, Training Loss: 1.0558193956102644\n",
      "Epoch 6, Training Loss: 1.0366743275993748\n",
      "Epoch 7, Training Loss: 1.0142739636557443\n",
      "Epoch 8, Training Loss: 0.994480820526754\n",
      "Epoch 9, Training Loss: 0.9807691257699092\n",
      "Epoch 10, Training Loss: 0.9719244870924412\n",
      "Epoch 11, Training Loss: 0.9663169800787044\n",
      "Epoch 12, Training Loss: 0.9633587130926605\n",
      "Epoch 13, Training Loss: 0.9597986985866288\n",
      "Epoch 14, Training Loss: 0.9576417785838134\n",
      "Epoch 15, Training Loss: 0.9558854756498696\n",
      "Epoch 16, Training Loss: 0.955172342196443\n",
      "Epoch 17, Training Loss: 0.9540202994095651\n",
      "Epoch 18, Training Loss: 0.9521408370562963\n",
      "Epoch 19, Training Loss: 0.9511652236594293\n",
      "Epoch 20, Training Loss: 0.9500950323907953\n",
      "Epoch 21, Training Loss: 0.94856732743127\n",
      "Epoch 22, Training Loss: 0.9468386347132518\n",
      "Epoch 23, Training Loss: 0.9454312784331186\n",
      "Epoch 24, Training Loss: 0.9442085516183896\n",
      "Epoch 25, Training Loss: 0.9419980064370579\n",
      "Epoch 26, Training Loss: 0.940175848258169\n",
      "Epoch 27, Training Loss: 0.9383688833480491\n",
      "Epoch 28, Training Loss: 0.9365952481004528\n",
      "Epoch 29, Training Loss: 0.9339481521369819\n",
      "Epoch 30, Training Loss: 0.9317175216244575\n",
      "Epoch 31, Training Loss: 0.9296577555792672\n",
      "Epoch 32, Training Loss: 0.9263169385436782\n",
      "Epoch 33, Training Loss: 0.9235273299360633\n",
      "Epoch 34, Training Loss: 0.9201304692074769\n",
      "Epoch 35, Training Loss: 0.9164846157669124\n",
      "Epoch 36, Training Loss: 0.9123033676828657\n",
      "Epoch 37, Training Loss: 0.9091650349753243\n",
      "Epoch 38, Training Loss: 0.9045959456522662\n",
      "Epoch 39, Training Loss: 0.8995546248622407\n",
      "Epoch 40, Training Loss: 0.8944699945306419\n",
      "Epoch 41, Training Loss: 0.889757395955853\n",
      "Epoch 42, Training Loss: 0.8842339836565175\n",
      "Epoch 43, Training Loss: 0.8792459504944937\n",
      "Epoch 44, Training Loss: 0.8739597509678145\n",
      "Epoch 45, Training Loss: 0.8689922503958968\n",
      "Epoch 46, Training Loss: 0.8641579496233087\n",
      "Epoch 47, Training Loss: 0.8589548308150212\n",
      "Epoch 48, Training Loss: 0.8550600512583453\n",
      "Epoch 49, Training Loss: 0.8510610069547381\n",
      "Epoch 50, Training Loss: 0.8468174726443183\n",
      "Epoch 51, Training Loss: 0.8442458659186399\n",
      "Epoch 52, Training Loss: 0.8411181826340525\n",
      "Epoch 53, Training Loss: 0.8381886248301743\n",
      "Epoch 54, Training Loss: 0.8353452571352622\n",
      "Epoch 55, Training Loss: 0.8334983372150507\n",
      "Epoch 56, Training Loss: 0.8312937550078657\n",
      "Epoch 57, Training Loss: 0.8298255996596544\n",
      "Epoch 58, Training Loss: 0.8280184498406891\n",
      "Epoch 59, Training Loss: 0.8267830998377692\n",
      "Epoch 60, Training Loss: 0.8254143861899699\n",
      "Epoch 61, Training Loss: 0.8250210807735758\n",
      "Epoch 62, Training Loss: 0.8234788916164771\n",
      "Epoch 63, Training Loss: 0.821827477082274\n",
      "Epoch 64, Training Loss: 0.8214942143375712\n",
      "Epoch 65, Training Loss: 0.8205833101631107\n",
      "Epoch 66, Training Loss: 0.8196121268702629\n",
      "Epoch 67, Training Loss: 0.8188902314444234\n",
      "Epoch 68, Training Loss: 0.818044025915906\n",
      "Epoch 69, Training Loss: 0.8178243966030895\n",
      "Epoch 70, Training Loss: 0.8171400593635731\n",
      "Epoch 71, Training Loss: 0.8167335669797166\n",
      "Epoch 72, Training Loss: 0.8156406909899604\n",
      "Epoch 73, Training Loss: 0.8155071759582462\n",
      "Epoch 74, Training Loss: 0.81516675975986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:36:50,412] Trial 87 finished with value: 0.6261333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8141341895089114\n",
      "Epoch 1, Training Loss: 0.9756893853495892\n",
      "Epoch 2, Training Loss: 0.9334272580039232\n",
      "Epoch 3, Training Loss: 0.9210788999285017\n",
      "Epoch 4, Training Loss: 0.9114584096392294\n",
      "Epoch 5, Training Loss: 0.9015969356199852\n",
      "Epoch 6, Training Loss: 0.890941293705675\n",
      "Epoch 7, Training Loss: 0.8795591112366297\n",
      "Epoch 8, Training Loss: 0.867006414187582\n",
      "Epoch 9, Training Loss: 0.8547079256602697\n",
      "Epoch 10, Training Loss: 0.843131749432786\n",
      "Epoch 11, Training Loss: 0.8330866626330784\n",
      "Epoch 12, Training Loss: 0.8248527673850382\n",
      "Epoch 13, Training Loss: 0.8187878540584019\n",
      "Epoch 14, Training Loss: 0.8144708504354147\n",
      "Epoch 15, Training Loss: 0.8106496683637002\n",
      "Epoch 16, Training Loss: 0.8087094658299496\n",
      "Epoch 17, Training Loss: 0.8067845699482394\n",
      "Epoch 18, Training Loss: 0.8049667512563835\n",
      "Epoch 19, Training Loss: 0.8039198036480667\n",
      "Epoch 20, Training Loss: 0.8034336157311174\n",
      "Epoch 21, Training Loss: 0.8028858027063814\n",
      "Epoch 22, Training Loss: 0.8020154587756422\n",
      "Epoch 23, Training Loss: 0.8019802266493776\n",
      "Epoch 24, Training Loss: 0.8020959314547087\n",
      "Epoch 25, Training Loss: 0.8017030461390215\n",
      "Epoch 26, Training Loss: 0.8011976910713023\n",
      "Epoch 27, Training Loss: 0.8010877138689945\n",
      "Epoch 28, Training Loss: 0.8004426607511994\n",
      "Epoch 29, Training Loss: 0.8004166903352379\n",
      "Epoch 30, Training Loss: 0.8002024986690148\n",
      "Epoch 31, Training Loss: 0.7999520141379277\n",
      "Epoch 32, Training Loss: 0.7999419821832413\n",
      "Epoch 33, Training Loss: 0.7999162682016989\n",
      "Epoch 34, Training Loss: 0.7995431087967149\n",
      "Epoch 35, Training Loss: 0.7994217770440238\n",
      "Epoch 36, Training Loss: 0.7991201934061553\n",
      "Epoch 37, Training Loss: 0.7990908128874642\n",
      "Epoch 38, Training Loss: 0.7990950484921161\n",
      "Epoch 39, Training Loss: 0.7986347330243964\n",
      "Epoch 40, Training Loss: 0.7993700719417486\n",
      "Epoch 41, Training Loss: 0.7987393078947426\n",
      "Epoch 42, Training Loss: 0.7983268056597028\n",
      "Epoch 43, Training Loss: 0.798831924370357\n",
      "Epoch 44, Training Loss: 0.7981993093526453\n",
      "Epoch 45, Training Loss: 0.7989300969848059\n",
      "Epoch 46, Training Loss: 0.7983644246158743\n",
      "Epoch 47, Training Loss: 0.7984735228065262\n",
      "Epoch 48, Training Loss: 0.7980821183749608\n",
      "Epoch 49, Training Loss: 0.7979490114333935\n",
      "Epoch 50, Training Loss: 0.7981502618108477\n",
      "Epoch 51, Training Loss: 0.7978096248511981\n",
      "Epoch 52, Training Loss: 0.7974348149801556\n",
      "Epoch 53, Training Loss: 0.7973358916160755\n",
      "Epoch 54, Training Loss: 0.7981665257224463\n",
      "Epoch 55, Training Loss: 0.7986296443114603\n",
      "Epoch 56, Training Loss: 0.7973874860240104\n",
      "Epoch 57, Training Loss: 0.7972339548562702\n",
      "Epoch 58, Training Loss: 0.7973658993728179\n",
      "Epoch 59, Training Loss: 0.7976348557866606\n",
      "Epoch 60, Training Loss: 0.7976352213916922\n",
      "Epoch 61, Training Loss: 0.7972473918047166\n",
      "Epoch 62, Training Loss: 0.7974082975459278\n",
      "Epoch 63, Training Loss: 0.797288557909485\n",
      "Epoch 64, Training Loss: 0.7969462656437006\n",
      "Epoch 65, Training Loss: 0.7971340700199729\n",
      "Epoch 66, Training Loss: 0.7974589289579176\n",
      "Epoch 67, Training Loss: 0.7977669524070912\n",
      "Epoch 68, Training Loss: 0.7970598326589828\n",
      "Epoch 69, Training Loss: 0.7968297888461808\n",
      "Epoch 70, Training Loss: 0.7964501798152923\n",
      "Epoch 71, Training Loss: 0.7969498763407084\n",
      "Epoch 72, Training Loss: 0.7964482581705079\n",
      "Epoch 73, Training Loss: 0.7971009523348701\n",
      "Epoch 74, Training Loss: 0.7963663758191847\n",
      "Epoch 75, Training Loss: 0.796844286309149\n",
      "Epoch 76, Training Loss: 0.7964542427457365\n",
      "Epoch 77, Training Loss: 0.7963262569635434\n",
      "Epoch 78, Training Loss: 0.7965624944608014\n",
      "Epoch 79, Training Loss: 0.7963148087487185\n",
      "Epoch 80, Training Loss: 0.7964983081459103\n",
      "Epoch 81, Training Loss: 0.7963858462814102\n",
      "Epoch 82, Training Loss: 0.7960498436052996\n",
      "Epoch 83, Training Loss: 0.7959387734420318\n",
      "Epoch 84, Training Loss: 0.7961457573381582\n",
      "Epoch 85, Training Loss: 0.7959295432370408\n",
      "Epoch 86, Training Loss: 0.7957302206440976\n",
      "Epoch 87, Training Loss: 0.7952711379617676\n",
      "Epoch 88, Training Loss: 0.7956767702461185\n",
      "Epoch 89, Training Loss: 0.7956082422034185\n",
      "Epoch 90, Training Loss: 0.7958265969627782\n",
      "Epoch 91, Training Loss: 0.7949995144865567\n",
      "Epoch 92, Training Loss: 0.7951236329580608\n",
      "Epoch 93, Training Loss: 0.7947603786798348\n",
      "Epoch 94, Training Loss: 0.7945408622125038\n",
      "Epoch 95, Training Loss: 0.7953760762860004\n",
      "Epoch 96, Training Loss: 0.794302413338109\n",
      "Epoch 97, Training Loss: 0.7946391111029718\n",
      "Epoch 98, Training Loss: 0.7944976539540112\n",
      "Epoch 99, Training Loss: 0.7944453148017252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:38:16,277] Trial 88 finished with value: 0.636 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7949923393421603\n",
      "Epoch 1, Training Loss: 0.9994427517840737\n",
      "Epoch 2, Training Loss: 0.9449319307965444\n",
      "Epoch 3, Training Loss: 0.9202735602407527\n",
      "Epoch 4, Training Loss: 0.8813918965203421\n",
      "Epoch 5, Training Loss: 0.8396678963998206\n",
      "Epoch 6, Training Loss: 0.8190060098368422\n",
      "Epoch 7, Training Loss: 0.8142451234329912\n",
      "Epoch 8, Training Loss: 0.8115269118681886\n",
      "Epoch 9, Training Loss: 0.8105091081526047\n",
      "Epoch 10, Training Loss: 0.8093332434955396\n",
      "Epoch 11, Training Loss: 0.808082746204577\n",
      "Epoch 12, Training Loss: 0.8082895714537541\n",
      "Epoch 13, Training Loss: 0.8067027560750345\n",
      "Epoch 14, Training Loss: 0.8071613626372546\n",
      "Epoch 15, Training Loss: 0.8048144743406683\n",
      "Epoch 16, Training Loss: 0.8044912168854161\n",
      "Epoch 17, Training Loss: 0.8039669804106978\n",
      "Epoch 18, Training Loss: 0.804222165641928\n",
      "Epoch 19, Training Loss: 0.803249778873042\n",
      "Epoch 20, Training Loss: 0.8029956876783443\n",
      "Epoch 21, Training Loss: 0.8026909442772543\n",
      "Epoch 22, Training Loss: 0.8021778406953453\n",
      "Epoch 23, Training Loss: 0.8023236635036038\n",
      "Epoch 24, Training Loss: 0.8021666772383497\n",
      "Epoch 25, Training Loss: 0.8018311952289782\n",
      "Epoch 26, Training Loss: 0.8026417358477312\n",
      "Epoch 27, Training Loss: 0.8018227465170666\n",
      "Epoch 28, Training Loss: 0.8006310840298358\n",
      "Epoch 29, Training Loss: 0.8005654634389662\n",
      "Epoch 30, Training Loss: 0.8010753796512919\n",
      "Epoch 31, Training Loss: 0.8007260476736198\n",
      "Epoch 32, Training Loss: 0.8010935018833418\n",
      "Epoch 33, Training Loss: 0.8005401534245427\n",
      "Epoch 34, Training Loss: 0.8002060984758507\n",
      "Epoch 35, Training Loss: 0.8006137440975447\n",
      "Epoch 36, Training Loss: 0.8005029244530469\n",
      "Epoch 37, Training Loss: 0.8008080656367137\n",
      "Epoch 38, Training Loss: 0.7998988082534388\n",
      "Epoch 39, Training Loss: 0.7998937692857326\n",
      "Epoch 40, Training Loss: 0.7993949982456695\n",
      "Epoch 41, Training Loss: 0.7995099703172096\n",
      "Epoch 42, Training Loss: 0.7991680990484424\n",
      "Epoch 43, Training Loss: 0.7993492143494743\n",
      "Epoch 44, Training Loss: 0.7987455691610064\n",
      "Epoch 45, Training Loss: 0.7985142795214976\n",
      "Epoch 46, Training Loss: 0.7983193898559513\n",
      "Epoch 47, Training Loss: 0.7982750103885966\n",
      "Epoch 48, Training Loss: 0.7992164102712072\n",
      "Epoch 49, Training Loss: 0.7982998513637629\n",
      "Epoch 50, Training Loss: 0.798560158919571\n",
      "Epoch 51, Training Loss: 0.7979980939312985\n",
      "Epoch 52, Training Loss: 0.7977558406672083\n",
      "Epoch 53, Training Loss: 0.7973396742702427\n",
      "Epoch 54, Training Loss: 0.797467717640382\n",
      "Epoch 55, Training Loss: 0.7978513387809122\n",
      "Epoch 56, Training Loss: 0.7980572229937504\n",
      "Epoch 57, Training Loss: 0.7969992864400821\n",
      "Epoch 58, Training Loss: 0.7979954170105152\n",
      "Epoch 59, Training Loss: 0.7970740254660298\n",
      "Epoch 60, Training Loss: 0.7965711402713804\n",
      "Epoch 61, Training Loss: 0.7968596997117637\n",
      "Epoch 62, Training Loss: 0.7967065836253919\n",
      "Epoch 63, Training Loss: 0.796235099681338\n",
      "Epoch 64, Training Loss: 0.7968926169818505\n",
      "Epoch 65, Training Loss: 0.7963468910159921\n",
      "Epoch 66, Training Loss: 0.7971558538594641\n",
      "Epoch 67, Training Loss: 0.7968203507867971\n",
      "Epoch 68, Training Loss: 0.7965516336430284\n",
      "Epoch 69, Training Loss: 0.7968438147602225\n",
      "Epoch 70, Training Loss: 0.7963330198947648\n",
      "Epoch 71, Training Loss: 0.795410166109415\n",
      "Epoch 72, Training Loss: 0.7960477071597164\n",
      "Epoch 73, Training Loss: 0.7956462411952198\n",
      "Epoch 74, Training Loss: 0.7957612892738859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:39:42,330] Trial 89 finished with value: 0.6330666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7955478324925989\n",
      "Epoch 1, Training Loss: 0.9131634644901051\n",
      "Epoch 2, Training Loss: 0.8653218695696663\n",
      "Epoch 3, Training Loss: 0.8299450101571925\n",
      "Epoch 4, Training Loss: 0.813503772651448\n",
      "Epoch 5, Training Loss: 0.8072843010285321\n",
      "Epoch 6, Training Loss: 0.8044181670160855\n",
      "Epoch 7, Training Loss: 0.8025455183141372\n",
      "Epoch 8, Training Loss: 0.8019442321272457\n",
      "Epoch 9, Training Loss: 0.8015393615470213\n",
      "Epoch 10, Training Loss: 0.8008810147818397\n",
      "Epoch 11, Training Loss: 0.8005355325867148\n",
      "Epoch 12, Training Loss: 0.8004224674140705\n",
      "Epoch 13, Training Loss: 0.7997117761303397\n",
      "Epoch 14, Training Loss: 0.7999194223740521\n",
      "Epoch 15, Training Loss: 0.7995441568599028\n",
      "Epoch 16, Training Loss: 0.7992453349337858\n",
      "Epoch 17, Training Loss: 0.7993309922078077\n",
      "Epoch 18, Training Loss: 0.7990692843409145\n",
      "Epoch 19, Training Loss: 0.7985377496130326\n",
      "Epoch 20, Training Loss: 0.7986100576905644\n",
      "Epoch 21, Training Loss: 0.7980926571874057\n",
      "Epoch 22, Training Loss: 0.7980252045743605\n",
      "Epoch 23, Training Loss: 0.7979529839403489\n",
      "Epoch 24, Training Loss: 0.7976980015810798\n",
      "Epoch 25, Training Loss: 0.7975595049297108\n",
      "Epoch 26, Training Loss: 0.797380075524835\n",
      "Epoch 27, Training Loss: 0.797221263997695\n",
      "Epoch 28, Training Loss: 0.7968529281896704\n",
      "Epoch 29, Training Loss: 0.7964438378109652\n",
      "Epoch 30, Training Loss: 0.7963339980209575\n",
      "Epoch 31, Training Loss: 0.7956557328560773\n",
      "Epoch 32, Training Loss: 0.7950739871053135\n",
      "Epoch 33, Training Loss: 0.7945808914829703\n",
      "Epoch 34, Training Loss: 0.7941449828007642\n",
      "Epoch 35, Training Loss: 0.7937784869530622\n",
      "Epoch 36, Training Loss: 0.7936378554736867\n",
      "Epoch 37, Training Loss: 0.7931423493693857\n",
      "Epoch 38, Training Loss: 0.7927182250864365\n",
      "Epoch 39, Training Loss: 0.7925765393060796\n",
      "Epoch 40, Training Loss: 0.7922232656619128\n",
      "Epoch 41, Training Loss: 0.7922673920322867\n",
      "Epoch 42, Training Loss: 0.7918235154712902\n",
      "Epoch 43, Training Loss: 0.7917203786092646\n",
      "Epoch 44, Training Loss: 0.7915173663111295\n",
      "Epoch 45, Training Loss: 0.7915024719518774\n",
      "Epoch 46, Training Loss: 0.7913374755663031\n",
      "Epoch 47, Training Loss: 0.7910291160555447\n",
      "Epoch 48, Training Loss: 0.790894174786175\n",
      "Epoch 49, Training Loss: 0.7908265404140248\n",
      "Epoch 50, Training Loss: 0.7905340420498568\n",
      "Epoch 51, Training Loss: 0.790502679418115\n",
      "Epoch 52, Training Loss: 0.7905606489321765\n",
      "Epoch 53, Training Loss: 0.7902547541786643\n",
      "Epoch 54, Training Loss: 0.7903488387781031\n",
      "Epoch 55, Training Loss: 0.7902721682716819\n",
      "Epoch 56, Training Loss: 0.7901859403357786\n",
      "Epoch 57, Training Loss: 0.7903173659829532\n",
      "Epoch 58, Training Loss: 0.7902597831277286\n",
      "Epoch 59, Training Loss: 0.7898933559305528\n",
      "Epoch 60, Training Loss: 0.7901580101602218\n",
      "Epoch 61, Training Loss: 0.7898230122117436\n",
      "Epoch 62, Training Loss: 0.7898790703801548\n",
      "Epoch 63, Training Loss: 0.7899086687845343\n",
      "Epoch 64, Training Loss: 0.7896540763097651\n",
      "Epoch 65, Training Loss: 0.7898786407358506\n",
      "Epoch 66, Training Loss: 0.7897005361669204\n",
      "Epoch 67, Training Loss: 0.7897203021890977\n",
      "Epoch 68, Training Loss: 0.7893845779054305\n",
      "Epoch 69, Training Loss: 0.7895494570451624\n",
      "Epoch 70, Training Loss: 0.7895416233118843\n",
      "Epoch 71, Training Loss: 0.78930866690243\n",
      "Epoch 72, Training Loss: 0.7891012129362892\n",
      "Epoch 73, Training Loss: 0.7890616236013525\n",
      "Epoch 74, Training Loss: 0.7891086126075072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:41:04,576] Trial 90 finished with value: 0.6362 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7889054825726678\n",
      "Epoch 1, Training Loss: 0.9081774935686499\n",
      "Epoch 2, Training Loss: 0.8557692382568703\n",
      "Epoch 3, Training Loss: 0.8252932137116453\n",
      "Epoch 4, Training Loss: 0.8116515181118384\n",
      "Epoch 5, Training Loss: 0.8062309978599835\n",
      "Epoch 6, Training Loss: 0.8038997866157302\n",
      "Epoch 7, Training Loss: 0.8024774036909404\n",
      "Epoch 8, Training Loss: 0.8020351774710461\n",
      "Epoch 9, Training Loss: 0.8011069670655674\n",
      "Epoch 10, Training Loss: 0.8000716806354379\n",
      "Epoch 11, Training Loss: 0.8003397760534645\n",
      "Epoch 12, Training Loss: 0.8001223526502911\n",
      "Epoch 13, Training Loss: 0.7995185482770877\n",
      "Epoch 14, Training Loss: 0.7994584329146192\n",
      "Epoch 15, Training Loss: 0.7985348294552107\n",
      "Epoch 16, Training Loss: 0.798496957649862\n",
      "Epoch 17, Training Loss: 0.7983020982347933\n",
      "Epoch 18, Training Loss: 0.7984409593101731\n",
      "Epoch 19, Training Loss: 0.7989095252259333\n",
      "Epoch 20, Training Loss: 0.798006786708545\n",
      "Epoch 21, Training Loss: 0.7981692711213477\n",
      "Epoch 22, Training Loss: 0.797476423503761\n",
      "Epoch 23, Training Loss: 0.7977556942997123\n",
      "Epoch 24, Training Loss: 0.7974593832976836\n",
      "Epoch 25, Training Loss: 0.7979879551364067\n",
      "Epoch 26, Training Loss: 0.7970990350371913\n",
      "Epoch 27, Training Loss: 0.7972728509651987\n",
      "Epoch 28, Training Loss: 0.7969317869136208\n",
      "Epoch 29, Training Loss: 0.7963678507876576\n",
      "Epoch 30, Training Loss: 0.7959535698245342\n",
      "Epoch 31, Training Loss: 0.7963673125532337\n",
      "Epoch 32, Training Loss: 0.7969036675037298\n",
      "Epoch 33, Training Loss: 0.7960923956749134\n",
      "Epoch 34, Training Loss: 0.7958680693368266\n",
      "Epoch 35, Training Loss: 0.7962471471693283\n",
      "Epoch 36, Training Loss: 0.7962938943303617\n",
      "Epoch 37, Training Loss: 0.7952937836485697\n",
      "Epoch 38, Training Loss: 0.7951728758955361\n",
      "Epoch 39, Training Loss: 0.7954826225911764\n",
      "Epoch 40, Training Loss: 0.7953024499398426\n",
      "Epoch 41, Training Loss: 0.7948839280838357\n",
      "Epoch 42, Training Loss: 0.7950066684780265\n",
      "Epoch 43, Training Loss: 0.7949839621558226\n",
      "Epoch 44, Training Loss: 0.7941008782028256\n",
      "Epoch 45, Training Loss: 0.7940949595960459\n",
      "Epoch 46, Training Loss: 0.7938089354593951\n",
      "Epoch 47, Training Loss: 0.7937719733195198\n",
      "Epoch 48, Training Loss: 0.793229067235961\n",
      "Epoch 49, Training Loss: 0.7929039867300737\n",
      "Epoch 50, Training Loss: 0.7920890444203427\n",
      "Epoch 51, Training Loss: 0.7919674128518068\n",
      "Epoch 52, Training Loss: 0.7913022942112801\n",
      "Epoch 53, Training Loss: 0.7915025901077385\n",
      "Epoch 54, Training Loss: 0.7909831757832291\n",
      "Epoch 55, Training Loss: 0.7915615846339922\n",
      "Epoch 56, Training Loss: 0.790432889524259\n",
      "Epoch 57, Training Loss: 0.7900760911012951\n",
      "Epoch 58, Training Loss: 0.7904505778972367\n",
      "Epoch 59, Training Loss: 0.7907417006958696\n",
      "Epoch 60, Training Loss: 0.7907815482383383\n",
      "Epoch 61, Training Loss: 0.7903217036921278\n",
      "Epoch 62, Training Loss: 0.7897884385030073\n",
      "Epoch 63, Training Loss: 0.7898897084078395\n",
      "Epoch 64, Training Loss: 0.7901661119962994\n",
      "Epoch 65, Training Loss: 0.7900317744204872\n",
      "Epoch 66, Training Loss: 0.7892934994590014\n",
      "Epoch 67, Training Loss: 0.7902034331981401\n",
      "Epoch 68, Training Loss: 0.7892190515546871\n",
      "Epoch 69, Training Loss: 0.7897023912659265\n",
      "Epoch 70, Training Loss: 0.7892779940949347\n",
      "Epoch 71, Training Loss: 0.7891385301611478\n",
      "Epoch 72, Training Loss: 0.7888575909281136\n",
      "Epoch 73, Training Loss: 0.7888317518216327\n",
      "Epoch 74, Training Loss: 0.7903593343003352\n",
      "Epoch 75, Training Loss: 0.7894417154161554\n",
      "Epoch 76, Training Loss: 0.7886080439377549\n",
      "Epoch 77, Training Loss: 0.7886206444941068\n",
      "Epoch 78, Training Loss: 0.7887690955983069\n",
      "Epoch 79, Training Loss: 0.7895727128014529\n",
      "Epoch 80, Training Loss: 0.7887052398875244\n",
      "Epoch 81, Training Loss: 0.7883263560614191\n",
      "Epoch 82, Training Loss: 0.7886789581829444\n",
      "Epoch 83, Training Loss: 0.7884782528518735\n",
      "Epoch 84, Training Loss: 0.7887211804999444\n",
      "Epoch 85, Training Loss: 0.788457351161125\n",
      "Epoch 86, Training Loss: 0.7884149054835613\n",
      "Epoch 87, Training Loss: 0.7891839467493215\n",
      "Epoch 88, Training Loss: 0.7891683491549097\n",
      "Epoch 89, Training Loss: 0.7892721617132201\n",
      "Epoch 90, Training Loss: 0.7888092317079243\n",
      "Epoch 91, Training Loss: 0.7887469755975823\n",
      "Epoch 92, Training Loss: 0.7882102400736701\n",
      "Epoch 93, Training Loss: 0.7882654947445805\n",
      "Epoch 94, Training Loss: 0.7888460013203155\n",
      "Epoch 95, Training Loss: 0.7881732162676359\n",
      "Epoch 96, Training Loss: 0.7879827344327941\n",
      "Epoch 97, Training Loss: 0.7882245196435684\n",
      "Epoch 98, Training Loss: 0.7885568917245793\n",
      "Epoch 99, Training Loss: 0.7881313888650191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:42:38,591] Trial 91 finished with value: 0.6336 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7885363663049568\n",
      "Epoch 1, Training Loss: 0.9773869778829463\n",
      "Epoch 2, Training Loss: 0.9460858286829555\n",
      "Epoch 3, Training Loss: 0.9353787609408883\n",
      "Epoch 4, Training Loss: 0.9249916202881757\n",
      "Epoch 5, Training Loss: 0.9144592246588539\n",
      "Epoch 6, Training Loss: 0.9040407692684846\n",
      "Epoch 7, Training Loss: 0.8937588597045225\n",
      "Epoch 8, Training Loss: 0.883723406791687\n",
      "Epoch 9, Training Loss: 0.874129756899441\n",
      "Epoch 10, Training Loss: 0.8651892477624556\n",
      "Epoch 11, Training Loss: 0.8569291439477135\n",
      "Epoch 12, Training Loss: 0.8495679508938508\n",
      "Epoch 13, Training Loss: 0.8431313245436725\n",
      "Epoch 14, Training Loss: 0.8375669379094067\n",
      "Epoch 15, Training Loss: 0.8328494419771082\n",
      "Epoch 16, Training Loss: 0.828924221571754\n",
      "Epoch 17, Training Loss: 0.8255778561620152\n",
      "Epoch 18, Training Loss: 0.8228548778505886\n",
      "Epoch 19, Training Loss: 0.8205633673247169\n",
      "Epoch 20, Training Loss: 0.8186647429185755\n",
      "Epoch 21, Training Loss: 0.8170617041868322\n",
      "Epoch 22, Training Loss: 0.8157701445327086\n",
      "Epoch 23, Training Loss: 0.8147844175731435\n",
      "Epoch 24, Training Loss: 0.813788888875176\n",
      "Epoch 25, Training Loss: 0.8129513951610117\n",
      "Epoch 26, Training Loss: 0.8123641172577353\n",
      "Epoch 27, Training Loss: 0.8117444004030788\n",
      "Epoch 28, Training Loss: 0.81122025847435\n",
      "Epoch 29, Training Loss: 0.8107691378453199\n",
      "Epoch 30, Training Loss: 0.8103221144395716\n",
      "Epoch 31, Training Loss: 0.8099614288526423\n",
      "Epoch 32, Training Loss: 0.8096211333835827\n",
      "Epoch 33, Training Loss: 0.8093291846443625\n",
      "Epoch 34, Training Loss: 0.8090232267800499\n",
      "Epoch 35, Training Loss: 0.8087840127243715\n",
      "Epoch 36, Training Loss: 0.8085186400133021\n",
      "Epoch 37, Training Loss: 0.8083166097192204\n",
      "Epoch 38, Training Loss: 0.8080985122568467\n",
      "Epoch 39, Training Loss: 0.8079054440470302\n",
      "Epoch 40, Training Loss: 0.8076510968629051\n",
      "Epoch 41, Training Loss: 0.8074089081147138\n",
      "Epoch 42, Training Loss: 0.8072801639051999\n",
      "Epoch 43, Training Loss: 0.8070491601439084\n",
      "Epoch 44, Training Loss: 0.8068515680817997\n",
      "Epoch 45, Training Loss: 0.8067562279981725\n",
      "Epoch 46, Training Loss: 0.8065442707258113\n",
      "Epoch 47, Training Loss: 0.8063546717166901\n",
      "Epoch 48, Training Loss: 0.8062183936904458\n",
      "Epoch 49, Training Loss: 0.8061335860981661\n",
      "Epoch 50, Training Loss: 0.8058120355185341\n",
      "Epoch 51, Training Loss: 0.8056750237240511\n",
      "Epoch 52, Training Loss: 0.8055150346896228\n",
      "Epoch 53, Training Loss: 0.8054482180931989\n",
      "Epoch 54, Training Loss: 0.8051972629042232\n",
      "Epoch 55, Training Loss: 0.8050577679101159\n",
      "Epoch 56, Training Loss: 0.8050275789990144\n",
      "Epoch 57, Training Loss: 0.8048644471168518\n",
      "Epoch 58, Training Loss: 0.8047707874634686\n",
      "Epoch 59, Training Loss: 0.8045560848712922\n",
      "Epoch 60, Training Loss: 0.8044001884320203\n",
      "Epoch 61, Training Loss: 0.8042307336190168\n",
      "Epoch 62, Training Loss: 0.8041083510483013\n",
      "Epoch 63, Training Loss: 0.8039537332338446\n",
      "Epoch 64, Training Loss: 0.8039262592792511\n",
      "Epoch 65, Training Loss: 0.8036146410072551\n",
      "Epoch 66, Training Loss: 0.8037086974172031\n",
      "Epoch 67, Training Loss: 0.8034771005546345\n",
      "Epoch 68, Training Loss: 0.803371107788647\n",
      "Epoch 69, Training Loss: 0.8031696536260493\n",
      "Epoch 70, Training Loss: 0.8030602130469154\n",
      "Epoch 71, Training Loss: 0.802995831265169\n",
      "Epoch 72, Training Loss: 0.80279280648512\n",
      "Epoch 73, Training Loss: 0.8026992480895099\n",
      "Epoch 74, Training Loss: 0.8027136726239148\n",
      "Epoch 75, Training Loss: 0.80243120228543\n",
      "Epoch 76, Training Loss: 0.8023517549739164\n",
      "Epoch 77, Training Loss: 0.8022879799674539\n",
      "Epoch 78, Training Loss: 0.802029304784887\n",
      "Epoch 79, Training Loss: 0.8020903618195477\n",
      "Epoch 80, Training Loss: 0.8019151590852176\n",
      "Epoch 81, Training Loss: 0.8018145952505223\n",
      "Epoch 82, Training Loss: 0.8016450479451348\n",
      "Epoch 83, Training Loss: 0.8015739424789653\n",
      "Epoch 84, Training Loss: 0.8014194828622482\n",
      "Epoch 85, Training Loss: 0.8014657853631413\n",
      "Epoch 86, Training Loss: 0.8013430887811324\n",
      "Epoch 87, Training Loss: 0.8011450681966894\n",
      "Epoch 88, Training Loss: 0.8010799368690041\n",
      "Epoch 89, Training Loss: 0.8010413753285127\n",
      "Epoch 90, Training Loss: 0.8008568602449754\n",
      "Epoch 91, Training Loss: 0.8007548241054311\n",
      "Epoch 92, Training Loss: 0.8006653479267569\n",
      "Epoch 93, Training Loss: 0.800663874499938\n",
      "Epoch 94, Training Loss: 0.8005322905849008\n",
      "Epoch 95, Training Loss: 0.8004059835041271\n",
      "Epoch 96, Training Loss: 0.8004095240200267\n",
      "Epoch 97, Training Loss: 0.8002678191661835\n",
      "Epoch 98, Training Loss: 0.8002210570082945\n",
      "Epoch 99, Training Loss: 0.8001649638484506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:44:11,777] Trial 92 finished with value: 0.6347333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8001599766226376\n",
      "Epoch 1, Training Loss: 0.8609068255675466\n",
      "Epoch 2, Training Loss: 0.8147685589646935\n",
      "Epoch 3, Training Loss: 0.8086254602984378\n",
      "Epoch 4, Training Loss: 0.8060600015453826\n",
      "Epoch 5, Training Loss: 0.8026200728308885\n",
      "Epoch 6, Training Loss: 0.8015296532695455\n",
      "Epoch 7, Training Loss: 0.8010845311602256\n",
      "Epoch 8, Training Loss: 0.7990045711958319\n",
      "Epoch 9, Training Loss: 0.7983262840070222\n",
      "Epoch 10, Training Loss: 0.7982341246497362\n",
      "Epoch 11, Training Loss: 0.796346274085511\n",
      "Epoch 12, Training Loss: 0.7963125884981084\n",
      "Epoch 13, Training Loss: 0.7964788266142508\n",
      "Epoch 14, Training Loss: 0.7953924355650307\n",
      "Epoch 15, Training Loss: 0.7964978097973013\n",
      "Epoch 16, Training Loss: 0.7950042079265852\n",
      "Epoch 17, Training Loss: 0.7944622465542385\n",
      "Epoch 18, Training Loss: 0.7935638939079486\n",
      "Epoch 19, Training Loss: 0.7939168443357137\n",
      "Epoch 20, Training Loss: 0.793917045019623\n",
      "Epoch 21, Training Loss: 0.7929408385341329\n",
      "Epoch 22, Training Loss: 0.7931457252430737\n",
      "Epoch 23, Training Loss: 0.7929549529140157\n",
      "Epoch 24, Training Loss: 0.792841042522201\n",
      "Epoch 25, Training Loss: 0.7921354640695385\n",
      "Epoch 26, Training Loss: 0.7924210750070729\n",
      "Epoch 27, Training Loss: 0.7928428293170785\n",
      "Epoch 28, Training Loss: 0.79230044398989\n",
      "Epoch 29, Training Loss: 0.7916754245758056\n",
      "Epoch 30, Training Loss: 0.7921525676447646\n",
      "Epoch 31, Training Loss: 0.7917467007959695\n",
      "Epoch 32, Training Loss: 0.7923794411178818\n",
      "Epoch 33, Training Loss: 0.7917500155312674\n",
      "Epoch 34, Training Loss: 0.7916385008876485\n",
      "Epoch 35, Training Loss: 0.7906888970755097\n",
      "Epoch 36, Training Loss: 0.7910328732845479\n",
      "Epoch 37, Training Loss: 0.7908938047581149\n",
      "Epoch 38, Training Loss: 0.7908478468880618\n",
      "Epoch 39, Training Loss: 0.791372543349302\n",
      "Epoch 40, Training Loss: 0.7899003814933891\n",
      "Epoch 41, Training Loss: 0.790715605961649\n",
      "Epoch 42, Training Loss: 0.7904732998152425\n",
      "Epoch 43, Training Loss: 0.791247598658827\n",
      "Epoch 44, Training Loss: 0.7899311326500168\n",
      "Epoch 45, Training Loss: 0.790361478767897\n",
      "Epoch 46, Training Loss: 0.7910346405846732\n",
      "Epoch 47, Training Loss: 0.791117773289071\n",
      "Epoch 48, Training Loss: 0.7904303350842985\n",
      "Epoch 49, Training Loss: 0.7899782389626467\n",
      "Epoch 50, Training Loss: 0.7894609747076393\n",
      "Epoch 51, Training Loss: 0.7903155518653697\n",
      "Epoch 52, Training Loss: 0.7898440555522316\n",
      "Epoch 53, Training Loss: 0.7887548196584658\n",
      "Epoch 54, Training Loss: 0.7910234942472071\n",
      "Epoch 55, Training Loss: 0.7898896537329021\n",
      "Epoch 56, Training Loss: 0.7891855540132164\n",
      "Epoch 57, Training Loss: 0.7896675368897\n",
      "Epoch 58, Training Loss: 0.7894549702343188\n",
      "Epoch 59, Training Loss: 0.7896439924275964\n",
      "Epoch 60, Training Loss: 0.7881192463681214\n",
      "Epoch 61, Training Loss: 0.7885878165413562\n",
      "Epoch 62, Training Loss: 0.7889937649095865\n",
      "Epoch 63, Training Loss: 0.7890582636782998\n",
      "Epoch 64, Training Loss: 0.7886004808253811\n",
      "Epoch 65, Training Loss: 0.7891811650498469\n",
      "Epoch 66, Training Loss: 0.7890532604733804\n",
      "Epoch 67, Training Loss: 0.7886974854128701\n",
      "Epoch 68, Training Loss: 0.7885037148805489\n",
      "Epoch 69, Training Loss: 0.7886312148624793\n",
      "Epoch 70, Training Loss: 0.7880884995137839\n",
      "Epoch 71, Training Loss: 0.7891492300463798\n",
      "Epoch 72, Training Loss: 0.7882991404461681\n",
      "Epoch 73, Training Loss: 0.7886934480272738\n",
      "Epoch 74, Training Loss: 0.7891769142079174\n",
      "Epoch 75, Training Loss: 0.7881076729387269\n",
      "Epoch 76, Training Loss: 0.7886854268554458\n",
      "Epoch 77, Training Loss: 0.788729346963696\n",
      "Epoch 78, Training Loss: 0.7889451428463584\n",
      "Epoch 79, Training Loss: 0.7894303862313579\n",
      "Epoch 80, Training Loss: 0.7898218554661687\n",
      "Epoch 81, Training Loss: 0.7887398693794595\n",
      "Epoch 82, Training Loss: 0.7885837715371211\n",
      "Epoch 83, Training Loss: 0.7881904523175461\n",
      "Epoch 84, Training Loss: 0.7886093247205691\n",
      "Epoch 85, Training Loss: 0.7882004406219139\n",
      "Epoch 86, Training Loss: 0.7885110039460032\n",
      "Epoch 87, Training Loss: 0.7887691761318006\n",
      "Epoch 88, Training Loss: 0.7880524507142548\n",
      "Epoch 89, Training Loss: 0.7899835466442252\n",
      "Epoch 90, Training Loss: 0.7891586698087535\n",
      "Epoch 91, Training Loss: 0.7879540760714309\n",
      "Epoch 92, Training Loss: 0.7877059194378386\n",
      "Epoch 93, Training Loss: 0.7881776772047344\n",
      "Epoch 94, Training Loss: 0.7878384624656878\n",
      "Epoch 95, Training Loss: 0.7881979452936273\n",
      "Epoch 96, Training Loss: 0.7884387520022859\n",
      "Epoch 97, Training Loss: 0.7879373415072162\n",
      "Epoch 98, Training Loss: 0.7889177381544185\n",
      "Epoch 99, Training Loss: 0.7882292733156592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:46:01,566] Trial 93 finished with value: 0.5964666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7885074727517322\n",
      "Epoch 1, Training Loss: 1.0236950114895316\n",
      "Epoch 2, Training Loss: 0.9512725288727705\n",
      "Epoch 3, Training Loss: 0.9344570129759171\n",
      "Epoch 4, Training Loss: 0.9261287383472219\n",
      "Epoch 5, Training Loss: 0.9194877170113956\n",
      "Epoch 6, Training Loss: 0.9135344196768368\n",
      "Epoch 7, Training Loss: 0.9078266521061168\n",
      "Epoch 8, Training Loss: 0.9019530804718242\n",
      "Epoch 9, Training Loss: 0.8957329918356502\n",
      "Epoch 10, Training Loss: 0.8889177445804372\n",
      "Epoch 11, Training Loss: 0.8814962107293746\n",
      "Epoch 12, Training Loss: 0.8735878822382759\n",
      "Epoch 13, Training Loss: 0.8653464766109691\n",
      "Epoch 14, Training Loss: 0.857105133673724\n",
      "Epoch 15, Training Loss: 0.8492139990189497\n",
      "Epoch 16, Training Loss: 0.8419369618331685\n",
      "Epoch 17, Training Loss: 0.8354671536473667\n",
      "Epoch 18, Training Loss: 0.8299291991486268\n",
      "Epoch 19, Training Loss: 0.8251132822036743\n",
      "Epoch 20, Training Loss: 0.8210222251976238\n",
      "Epoch 21, Training Loss: 0.8178873240246493\n",
      "Epoch 22, Training Loss: 0.815022692750482\n",
      "Epoch 23, Training Loss: 0.8128694958546583\n",
      "Epoch 24, Training Loss: 0.8111324801164514\n",
      "Epoch 25, Training Loss: 0.80961902681519\n",
      "Epoch 26, Training Loss: 0.8084858850170584\n",
      "Epoch 27, Training Loss: 0.8074678291993983\n",
      "Epoch 28, Training Loss: 0.8065872146101559\n",
      "Epoch 29, Training Loss: 0.805791797988555\n",
      "Epoch 30, Training Loss: 0.8052714477567112\n",
      "Epoch 31, Training Loss: 0.804705506773556\n",
      "Epoch 32, Training Loss: 0.8041924295705908\n",
      "Epoch 33, Training Loss: 0.8038988611978644\n",
      "Epoch 34, Training Loss: 0.8035139452709871\n",
      "Epoch 35, Training Loss: 0.8033080849226784\n",
      "Epoch 36, Training Loss: 0.8029879965501673\n",
      "Epoch 37, Training Loss: 0.8026341061031117\n",
      "Epoch 38, Training Loss: 0.8025484725307016\n",
      "Epoch 39, Training Loss: 0.8021205666485954\n",
      "Epoch 40, Training Loss: 0.8020009880206164\n",
      "Epoch 41, Training Loss: 0.8015430936392616\n",
      "Epoch 42, Training Loss: 0.801398659804288\n",
      "Epoch 43, Training Loss: 0.8012334670038784\n",
      "Epoch 44, Training Loss: 0.8009945360352011\n",
      "Epoch 45, Training Loss: 0.8008072732476628\n",
      "Epoch 46, Training Loss: 0.8007646666554844\n",
      "Epoch 47, Training Loss: 0.8004738241784712\n",
      "Epoch 48, Training Loss: 0.8004888965101803\n",
      "Epoch 49, Training Loss: 0.8003181916825911\n",
      "Epoch 50, Training Loss: 0.8003003698938033\n",
      "Epoch 51, Training Loss: 0.8001371517602135\n",
      "Epoch 52, Training Loss: 0.8000029436279745\n",
      "Epoch 53, Training Loss: 0.7999983415182899\n",
      "Epoch 54, Training Loss: 0.7998621201515198\n",
      "Epoch 55, Training Loss: 0.7997541964755339\n",
      "Epoch 56, Training Loss: 0.7996442810226889\n",
      "Epoch 57, Training Loss: 0.7996260117082035\n",
      "Epoch 58, Training Loss: 0.7995142824509565\n",
      "Epoch 59, Training Loss: 0.7994432648490457\n",
      "Epoch 60, Training Loss: 0.7991993749141693\n",
      "Epoch 61, Training Loss: 0.7991188456731684\n",
      "Epoch 62, Training Loss: 0.7991917395591736\n",
      "Epoch 63, Training Loss: 0.7989997534190907\n",
      "Epoch 64, Training Loss: 0.7990554049435784\n",
      "Epoch 65, Training Loss: 0.7989552834454705\n",
      "Epoch 66, Training Loss: 0.7988213489336126\n",
      "Epoch 67, Training Loss: 0.7988611076158636\n",
      "Epoch 68, Training Loss: 0.7987076935347389\n",
      "Epoch 69, Training Loss: 0.7986673595624811\n",
      "Epoch 70, Training Loss: 0.7983952738958247\n",
      "Epoch 71, Training Loss: 0.7984242756927714\n",
      "Epoch 72, Training Loss: 0.7985604681688196\n",
      "Epoch 73, Training Loss: 0.7983619404540342\n",
      "Epoch 74, Training Loss: 0.7982284264704761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:47:11,438] Trial 94 finished with value: 0.6333333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7982746681045083\n",
      "Epoch 1, Training Loss: 0.8380525535210631\n",
      "Epoch 2, Training Loss: 0.8163108442959033\n",
      "Epoch 3, Training Loss: 0.8100840128902206\n",
      "Epoch 4, Training Loss: 0.8061103651398107\n",
      "Epoch 5, Training Loss: 0.8083001053422914\n",
      "Epoch 6, Training Loss: 0.8044578988749281\n",
      "Epoch 7, Training Loss: 0.8028826201768746\n",
      "Epoch 8, Training Loss: 0.8022527412364358\n",
      "Epoch 9, Training Loss: 0.8028552658575818\n",
      "Epoch 10, Training Loss: 0.8033931284022511\n",
      "Epoch 11, Training Loss: 0.8024170314458976\n",
      "Epoch 12, Training Loss: 0.8016569320420573\n",
      "Epoch 13, Training Loss: 0.8010670979220168\n",
      "Epoch 14, Training Loss: 0.8003015658012906\n",
      "Epoch 15, Training Loss: 0.8012541238526653\n",
      "Epoch 16, Training Loss: 0.8004116372058266\n",
      "Epoch 17, Training Loss: 0.7990117678068634\n",
      "Epoch 18, Training Loss: 0.7983133327692075\n",
      "Epoch 19, Training Loss: 0.7991263973981815\n",
      "Epoch 20, Training Loss: 0.7969737710809349\n",
      "Epoch 21, Training Loss: 0.798367206315349\n",
      "Epoch 22, Training Loss: 0.7990460991859436\n",
      "Epoch 23, Training Loss: 0.8009366408326573\n",
      "Epoch 24, Training Loss: 0.7988668818222849\n",
      "Epoch 25, Training Loss: 0.7979872840239589\n",
      "Epoch 26, Training Loss: 0.7971451558564838\n",
      "Epoch 27, Training Loss: 0.7977975654422789\n",
      "Epoch 28, Training Loss: 0.7991537456225631\n",
      "Epoch 29, Training Loss: 0.7975274673081878\n",
      "Epoch 30, Training Loss: 0.7960576809886702\n",
      "Epoch 31, Training Loss: 0.7981368114177446\n",
      "Epoch 32, Training Loss: 0.7985558948122469\n",
      "Epoch 33, Training Loss: 0.7979058992593808\n",
      "Epoch 34, Training Loss: 0.7981539203708333\n",
      "Epoch 35, Training Loss: 0.7971176517637153\n",
      "Epoch 36, Training Loss: 0.7982425619785051\n",
      "Epoch 37, Training Loss: 0.7973583140767606\n",
      "Epoch 38, Training Loss: 0.797092730748026\n",
      "Epoch 39, Training Loss: 0.7974662719812609\n",
      "Epoch 40, Training Loss: 0.7956803421328839\n",
      "Epoch 41, Training Loss: 0.7954422060708354\n",
      "Epoch 42, Training Loss: 0.7962165287562779\n",
      "Epoch 43, Training Loss: 0.7966819317717301\n",
      "Epoch 44, Training Loss: 0.7981805474238288\n",
      "Epoch 45, Training Loss: 0.7959391940805248\n",
      "Epoch 46, Training Loss: 0.7952813534808338\n",
      "Epoch 47, Training Loss: 0.7957089577402388\n",
      "Epoch 48, Training Loss: 0.7952778405712959\n",
      "Epoch 49, Training Loss: 0.7972672587050531\n",
      "Epoch 50, Training Loss: 0.7947419250818123\n",
      "Epoch 51, Training Loss: 0.7959946229045552\n",
      "Epoch 52, Training Loss: 0.7965638052252002\n",
      "Epoch 53, Training Loss: 0.7970424916511192\n",
      "Epoch 54, Training Loss: 0.7957361121823017\n",
      "Epoch 55, Training Loss: 0.7937429140384932\n",
      "Epoch 56, Training Loss: 0.7948583386894456\n",
      "Epoch 57, Training Loss: 0.7952359180701406\n",
      "Epoch 58, Training Loss: 0.7943844019022203\n",
      "Epoch 59, Training Loss: 0.7957620225454631\n",
      "Epoch 60, Training Loss: 0.7940153027835645\n",
      "Epoch 61, Training Loss: 0.7963675304463035\n",
      "Epoch 62, Training Loss: 0.7952285957515688\n",
      "Epoch 63, Training Loss: 0.7943413786422041\n",
      "Epoch 64, Training Loss: 0.7951663192053486\n",
      "Epoch 65, Training Loss: 0.7947057108233746\n",
      "Epoch 66, Training Loss: 0.7936595473970686\n",
      "Epoch 67, Training Loss: 0.7950390881165526\n",
      "Epoch 68, Training Loss: 0.7945853183144017\n",
      "Epoch 69, Training Loss: 0.7938647410027067\n",
      "Epoch 70, Training Loss: 0.7939688161799782\n",
      "Epoch 71, Training Loss: 0.7940388253756931\n",
      "Epoch 72, Training Loss: 0.7955364596574827\n",
      "Epoch 73, Training Loss: 0.794070711888765\n",
      "Epoch 74, Training Loss: 0.7942677086457274\n",
      "Epoch 75, Training Loss: 0.7939484056673551\n",
      "Epoch 76, Training Loss: 0.7943103511530654\n",
      "Epoch 77, Training Loss: 0.7953603984718036\n",
      "Epoch 78, Training Loss: 0.7940547551427569\n",
      "Epoch 79, Training Loss: 0.7960967914502424\n",
      "Epoch 80, Training Loss: 0.7972125415515182\n",
      "Epoch 81, Training Loss: 0.7943854940565009\n",
      "Epoch 82, Training Loss: 0.79461801437507\n",
      "Epoch 83, Training Loss: 0.7948073651557578\n",
      "Epoch 84, Training Loss: 0.793810250346822\n",
      "Epoch 85, Training Loss: 0.7947030313929221\n",
      "Epoch 86, Training Loss: 0.795229705204641\n",
      "Epoch 87, Training Loss: 0.7946596407352533\n",
      "Epoch 88, Training Loss: 0.7945929149936016\n",
      "Epoch 89, Training Loss: 0.7942875033034418\n",
      "Epoch 90, Training Loss: 0.7940678728254218\n",
      "Epoch 91, Training Loss: 0.7939240484309376\n",
      "Epoch 92, Training Loss: 0.7956008602802018\n",
      "Epoch 93, Training Loss: 0.7945532679557801\n",
      "Epoch 94, Training Loss: 0.7934946008194658\n",
      "Epoch 95, Training Loss: 0.794383754586815\n",
      "Epoch 96, Training Loss: 0.7933956142654992\n",
      "Epoch 97, Training Loss: 0.792967833164043\n",
      "Epoch 98, Training Loss: 0.7930753378043497\n",
      "Epoch 99, Training Loss: 0.79315556372915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:49:07,054] Trial 95 finished with value: 0.6386666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7939554261085683\n",
      "Epoch 1, Training Loss: 1.0887190152617061\n",
      "Epoch 2, Training Loss: 1.0737144011609694\n",
      "Epoch 3, Training Loss: 1.0606685472937192\n",
      "Epoch 4, Training Loss: 1.0477264980708851\n",
      "Epoch 5, Training Loss: 1.0345021892996396\n",
      "Epoch 6, Training Loss: 1.0210114153693703\n",
      "Epoch 7, Training Loss: 1.007551746298285\n",
      "Epoch 8, Training Loss: 0.9945960452276118\n",
      "Epoch 9, Training Loss: 0.9826460001749151\n",
      "Epoch 10, Training Loss: 0.972065564323874\n",
      "Epoch 11, Training Loss: 0.9630358596409069\n",
      "Epoch 12, Training Loss: 0.9555350876555724\n",
      "Epoch 13, Training Loss: 0.9494114874390995\n",
      "Epoch 14, Training Loss: 0.9444784995387582\n",
      "Epoch 15, Training Loss: 0.9405054798546959\n",
      "Epoch 16, Training Loss: 0.937284208536148\n",
      "Epoch 17, Training Loss: 0.9346428280016955\n",
      "Epoch 18, Training Loss: 0.9324004718836616\n",
      "Epoch 19, Training Loss: 0.9304575577904196\n",
      "Epoch 20, Training Loss: 0.9287321143991807\n",
      "Epoch 21, Training Loss: 0.9271571850075441\n",
      "Epoch 22, Training Loss: 0.9256907918873956\n",
      "Epoch 23, Training Loss: 0.9243044088868534\n",
      "Epoch 24, Training Loss: 0.9229863964810091\n",
      "Epoch 25, Training Loss: 0.9217032641523024\n",
      "Epoch 26, Training Loss: 0.9204556962321786\n",
      "Epoch 27, Training Loss: 0.9192303123193629\n",
      "Epoch 28, Training Loss: 0.9180430923489963\n",
      "Epoch 29, Training Loss: 0.9168708363701316\n",
      "Epoch 30, Training Loss: 0.9157161429349114\n",
      "Epoch 31, Training Loss: 0.9145753364002004\n",
      "Epoch 32, Training Loss: 0.913440349592882\n",
      "Epoch 33, Training Loss: 0.9123041571589077\n",
      "Epoch 34, Training Loss: 0.9111739036616157\n",
      "Epoch 35, Training Loss: 0.9100370388872483\n",
      "Epoch 36, Training Loss: 0.908907732332454\n",
      "Epoch 37, Training Loss: 0.9077780269875246\n",
      "Epoch 38, Training Loss: 0.9066371066430036\n",
      "Epoch 39, Training Loss: 0.9054741050916559\n",
      "Epoch 40, Training Loss: 0.9043379175662994\n",
      "Epoch 41, Training Loss: 0.9031783418795641\n",
      "Epoch 42, Training Loss: 0.9020030423472909\n",
      "Epoch 43, Training Loss: 0.9008206211819368\n",
      "Epoch 44, Training Loss: 0.8996093778750476\n",
      "Epoch 45, Training Loss: 0.8983992763126598\n",
      "Epoch 46, Training Loss: 0.897175813702976\n",
      "Epoch 47, Training Loss: 0.895924354581272\n",
      "Epoch 48, Training Loss: 0.894652577638626\n",
      "Epoch 49, Training Loss: 0.8933552458707024\n",
      "Epoch 50, Training Loss: 0.8920606829138363\n",
      "Epoch 51, Training Loss: 0.8907209648805506\n",
      "Epoch 52, Training Loss: 0.8893668980458204\n",
      "Epoch 53, Training Loss: 0.8879576463559095\n",
      "Epoch 54, Training Loss: 0.8865485631017124\n",
      "Epoch 55, Training Loss: 0.8850933412243338\n",
      "Epoch 56, Training Loss: 0.8836384748711306\n",
      "Epoch 57, Training Loss: 0.8821273249037126\n",
      "Epoch 58, Training Loss: 0.8805938996286953\n",
      "Epoch 59, Training Loss: 0.8790324136790107\n",
      "Epoch 60, Training Loss: 0.8774248016581816\n",
      "Epoch 61, Training Loss: 0.8757845395452836\n",
      "Epoch 62, Training Loss: 0.8741192913756651\n",
      "Epoch 63, Training Loss: 0.8724225132605609\n",
      "Epoch 64, Training Loss: 0.8707030612580916\n",
      "Epoch 65, Training Loss: 0.868929171071333\n",
      "Epoch 66, Training Loss: 0.8671553980602937\n",
      "Epoch 67, Training Loss: 0.8653170710451463\n",
      "Epoch 68, Training Loss: 0.8635127311594346\n",
      "Epoch 69, Training Loss: 0.8616587396930245\n",
      "Epoch 70, Training Loss: 0.8597953183510724\n",
      "Epoch 71, Training Loss: 0.857910295234007\n",
      "Epoch 72, Training Loss: 0.8560320580005646\n",
      "Epoch 73, Training Loss: 0.8541426713326398\n",
      "Epoch 74, Training Loss: 0.8522579870504492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:50:27,201] Trial 96 finished with value: 0.6029333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8503724709679099\n",
      "Epoch 1, Training Loss: 0.8802200141373803\n",
      "Epoch 2, Training Loss: 0.8253866354156943\n",
      "Epoch 3, Training Loss: 0.818240024061764\n",
      "Epoch 4, Training Loss: 0.8143214304306928\n",
      "Epoch 5, Training Loss: 0.8115246222299688\n",
      "Epoch 6, Training Loss: 0.8102705599981196\n",
      "Epoch 7, Training Loss: 0.8084717596278471\n",
      "Epoch 8, Training Loss: 0.8068876847800086\n",
      "Epoch 9, Training Loss: 0.806198511614519\n",
      "Epoch 10, Training Loss: 0.805271874105229\n",
      "Epoch 11, Training Loss: 0.8058979314215043\n",
      "Epoch 12, Training Loss: 0.8062272806027356\n",
      "Epoch 13, Training Loss: 0.8043710421113407\n",
      "Epoch 14, Training Loss: 0.8050462981532601\n",
      "Epoch 15, Training Loss: 0.8043933447669535\n",
      "Epoch 16, Training Loss: 0.8038816154003143\n",
      "Epoch 17, Training Loss: 0.8036456986034618\n",
      "Epoch 18, Training Loss: 0.8038552470066969\n",
      "Epoch 19, Training Loss: 0.8035718413661508\n",
      "Epoch 20, Training Loss: 0.8037291076604057\n",
      "Epoch 21, Training Loss: 0.8023619503834668\n",
      "Epoch 22, Training Loss: 0.8026129840402042\n",
      "Epoch 23, Training Loss: 0.8023347572018118\n",
      "Epoch 24, Training Loss: 0.8021036103893728\n",
      "Epoch 25, Training Loss: 0.8017844029735116\n",
      "Epoch 26, Training Loss: 0.8019074402837192\n",
      "Epoch 27, Training Loss: 0.8019423270225525\n",
      "Epoch 28, Training Loss: 0.801767726505504\n",
      "Epoch 29, Training Loss: 0.8013671417096082\n",
      "Epoch 30, Training Loss: 0.802028488481746\n",
      "Epoch 31, Training Loss: 0.8012709020165836\n",
      "Epoch 32, Training Loss: 0.8008675548609565\n",
      "Epoch 33, Training Loss: 0.8015930047455956\n",
      "Epoch 34, Training Loss: 0.800882402938955\n",
      "Epoch 35, Training Loss: 0.8014572593745063\n",
      "Epoch 36, Training Loss: 0.8005329524769502\n",
      "Epoch 37, Training Loss: 0.8015161330559675\n",
      "Epoch 38, Training Loss: 0.8012382975746604\n",
      "Epoch 39, Training Loss: 0.8015074399639578\n",
      "Epoch 40, Training Loss: 0.800696225306567\n",
      "Epoch 41, Training Loss: 0.8005068478163551\n",
      "Epoch 42, Training Loss: 0.8007995360739091\n",
      "Epoch 43, Training Loss: 0.8010516506082871\n",
      "Epoch 44, Training Loss: 0.8007741461781894\n",
      "Epoch 45, Training Loss: 0.8008688257722294\n",
      "Epoch 46, Training Loss: 0.8004441235345953\n",
      "Epoch 47, Training Loss: 0.8001641056818121\n",
      "Epoch 48, Training Loss: 0.8006705979739919\n",
      "Epoch 49, Training Loss: 0.799812354480519\n",
      "Epoch 50, Training Loss: 0.8004075075598324\n",
      "Epoch 51, Training Loss: 0.8008119921824511\n",
      "Epoch 52, Training Loss: 0.800332060210845\n",
      "Epoch 53, Training Loss: 0.7999196706098669\n",
      "Epoch 54, Training Loss: 0.8002551761094262\n",
      "Epoch 55, Training Loss: 0.799796397896374\n",
      "Epoch 56, Training Loss: 0.8013707335556255\n",
      "Epoch 57, Training Loss: 0.8005456248451682\n",
      "Epoch 58, Training Loss: 0.8004548882035648\n",
      "Epoch 59, Training Loss: 0.7999520329166862\n",
      "Epoch 60, Training Loss: 0.800221707610523\n",
      "Epoch 61, Training Loss: 0.7994464503316319\n",
      "Epoch 62, Training Loss: 0.8000148608404047\n",
      "Epoch 63, Training Loss: 0.8004070881535025\n",
      "Epoch 64, Training Loss: 0.8000342041604659\n",
      "Epoch 65, Training Loss: 0.8002926855928758\n",
      "Epoch 66, Training Loss: 0.80006983665859\n",
      "Epoch 67, Training Loss: 0.7997213672890383\n",
      "Epoch 68, Training Loss: 0.7995010988151325\n",
      "Epoch 69, Training Loss: 0.7995645920669331\n",
      "Epoch 70, Training Loss: 0.8001189173670376\n",
      "Epoch 71, Training Loss: 0.8004782448095434\n",
      "Epoch 72, Training Loss: 0.7992629903204301\n",
      "Epoch 73, Training Loss: 0.7998121319097631\n",
      "Epoch 74, Training Loss: 0.7997283191540662\n",
      "Epoch 75, Training Loss: 0.7994541952189277\n",
      "Epoch 76, Training Loss: 0.7994948448854334\n",
      "Epoch 77, Training Loss: 0.7990930111267988\n",
      "Epoch 78, Training Loss: 0.8004269484211417\n",
      "Epoch 79, Training Loss: 0.7996509387913873\n",
      "Epoch 80, Training Loss: 0.8000103418266072\n",
      "Epoch 81, Training Loss: 0.7996475561927346\n",
      "Epoch 82, Training Loss: 0.7994226266356076\n",
      "Epoch 83, Training Loss: 0.8002443232255824\n",
      "Epoch 84, Training Loss: 0.8000704084424411\n",
      "Epoch 85, Training Loss: 0.7993274500089533\n",
      "Epoch 86, Training Loss: 0.7996288833898656\n",
      "Epoch 87, Training Loss: 0.8001248727826511\n",
      "Epoch 88, Training Loss: 0.798944766591577\n",
      "Epoch 89, Training Loss: 0.7990646013091592\n",
      "Epoch 90, Training Loss: 0.7992177691179163\n",
      "Epoch 91, Training Loss: 0.7992883737648234\n",
      "Epoch 92, Training Loss: 0.7996758977104635\n",
      "Epoch 93, Training Loss: 0.7990174412727356\n",
      "Epoch 94, Training Loss: 0.799179507003111\n",
      "Epoch 95, Training Loss: 0.7994150271135217\n",
      "Epoch 96, Training Loss: 0.799441270758124\n",
      "Epoch 97, Training Loss: 0.7992484969952527\n",
      "Epoch 98, Training Loss: 0.8000699739596423\n",
      "Epoch 99, Training Loss: 0.7997031196425943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:52:33,222] Trial 97 finished with value: 0.6370666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.799045769887812\n",
      "Epoch 1, Training Loss: 1.0883954199622659\n",
      "Epoch 2, Training Loss: 1.0842422483949101\n",
      "Epoch 3, Training Loss: 1.0795479088671067\n",
      "Epoch 4, Training Loss: 1.0735841525302214\n",
      "Epoch 5, Training Loss: 1.065821063939263\n",
      "Epoch 6, Training Loss: 1.0558853908146129\n",
      "Epoch 7, Training Loss: 1.0434252094521241\n",
      "Epoch 8, Training Loss: 1.029099043186973\n",
      "Epoch 9, Training Loss: 1.0142275673501633\n",
      "Epoch 10, Training Loss: 1.0009317205933963\n",
      "Epoch 11, Training Loss: 0.9901521669415867\n",
      "Epoch 12, Training Loss: 0.9820969839657054\n",
      "Epoch 13, Training Loss: 0.9761910579485051\n",
      "Epoch 14, Training Loss: 0.9714126380050884\n",
      "Epoch 15, Training Loss: 0.967345715340446\n",
      "Epoch 16, Training Loss: 0.9639748505283805\n",
      "Epoch 17, Training Loss: 0.9610822787004358\n",
      "Epoch 18, Training Loss: 0.9586209254405078\n",
      "Epoch 19, Training Loss: 0.9567742222898147\n",
      "Epoch 20, Training Loss: 0.9550950946527369\n",
      "Epoch 21, Training Loss: 0.9539827317350051\n",
      "Epoch 22, Training Loss: 0.95287358361132\n",
      "Epoch 23, Training Loss: 0.9519691260421977\n",
      "Epoch 24, Training Loss: 0.9509901574078728\n",
      "Epoch 25, Training Loss: 0.9500072572511785\n",
      "Epoch 26, Training Loss: 0.9492433665079228\n",
      "Epoch 27, Training Loss: 0.9483254723689135\n",
      "Epoch 28, Training Loss: 0.9475042073866901\n",
      "Epoch 29, Training Loss: 0.9465908307187697\n",
      "Epoch 30, Training Loss: 0.9455546693240895\n",
      "Epoch 31, Training Loss: 0.9445519236256095\n",
      "Epoch 32, Training Loss: 0.9436415204581092\n",
      "Epoch 33, Training Loss: 0.9425623714923859\n",
      "Epoch 34, Training Loss: 0.9414692439752467\n",
      "Epoch 35, Training Loss: 0.9404088592529297\n",
      "Epoch 36, Training Loss: 0.9391490181754617\n",
      "Epoch 37, Training Loss: 0.9379566642817329\n",
      "Epoch 38, Training Loss: 0.9366332300971536\n",
      "Epoch 39, Training Loss: 0.9352449659740224\n",
      "Epoch 40, Training Loss: 0.933913824488135\n",
      "Epoch 41, Training Loss: 0.9323758196129518\n",
      "Epoch 42, Training Loss: 0.9307471216426176\n",
      "Epoch 43, Training Loss: 0.9291234470114988\n",
      "Epoch 44, Training Loss: 0.9274055044791277\n",
      "Epoch 45, Training Loss: 0.9255317040050731\n",
      "Epoch 46, Training Loss: 0.923595828519148\n",
      "Epoch 47, Training Loss: 0.921498959344976\n",
      "Epoch 48, Training Loss: 0.9193486367253696\n",
      "Epoch 49, Training Loss: 0.9171183832252727\n",
      "Epoch 50, Training Loss: 0.9147479728390189\n",
      "Epoch 51, Training Loss: 0.9122588931812959\n",
      "Epoch 52, Training Loss: 0.9095905078158659\n",
      "Epoch 53, Training Loss: 0.9069039622475119\n",
      "Epoch 54, Training Loss: 0.9041437517194187\n",
      "Epoch 55, Training Loss: 0.9011870518151451\n",
      "Epoch 56, Training Loss: 0.8980532315899344\n",
      "Epoch 57, Training Loss: 0.8949846215107862\n",
      "Epoch 58, Training Loss: 0.8917641473517698\n",
      "Epoch 59, Training Loss: 0.8885687908004312\n",
      "Epoch 60, Training Loss: 0.8853273331417757\n",
      "Epoch 61, Training Loss: 0.8821595369367039\n",
      "Epoch 62, Training Loss: 0.8788460685926325\n",
      "Epoch 63, Training Loss: 0.8756754943202524\n",
      "Epoch 64, Training Loss: 0.8724485742344575\n",
      "Epoch 65, Training Loss: 0.8693246844235588\n",
      "Epoch 66, Training Loss: 0.8663891370156233\n",
      "Epoch 67, Training Loss: 0.8634741370818194\n",
      "Epoch 68, Training Loss: 0.860688541987363\n",
      "Epoch 69, Training Loss: 0.8580310937236337\n",
      "Epoch 70, Training Loss: 0.8554707114135518\n",
      "Epoch 71, Training Loss: 0.8529665253442876\n",
      "Epoch 72, Training Loss: 0.850790288027595\n",
      "Epoch 73, Training Loss: 0.8486159830934861\n",
      "Epoch 74, Training Loss: 0.8465983038088855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:53:52,938] Trial 98 finished with value: 0.607 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8446059014516718\n",
      "Epoch 1, Training Loss: 0.9000794254331028\n",
      "Epoch 2, Training Loss: 0.819269530913409\n",
      "Epoch 3, Training Loss: 0.8074764535707586\n",
      "Epoch 4, Training Loss: 0.8048122908087337\n",
      "Epoch 5, Training Loss: 0.8021168968958013\n",
      "Epoch 6, Training Loss: 0.7992584500593297\n",
      "Epoch 7, Training Loss: 0.797329899633632\n",
      "Epoch 8, Training Loss: 0.7956525165894452\n",
      "Epoch 9, Training Loss: 0.7944544449273278\n",
      "Epoch 10, Training Loss: 0.7931523584618287\n",
      "Epoch 11, Training Loss: 0.792189825703116\n",
      "Epoch 12, Training Loss: 0.7913401782512665\n",
      "Epoch 13, Training Loss: 0.790236539069344\n",
      "Epoch 14, Training Loss: 0.7897274183525759\n",
      "Epoch 15, Training Loss: 0.789062742766212\n",
      "Epoch 16, Training Loss: 0.788827201128006\n",
      "Epoch 17, Training Loss: 0.7883172015582814\n",
      "Epoch 18, Training Loss: 0.7876239820087657\n",
      "Epoch 19, Training Loss: 0.7871187313865213\n",
      "Epoch 20, Training Loss: 0.7869040231844958\n",
      "Epoch 21, Training Loss: 0.7868197576438679\n",
      "Epoch 22, Training Loss: 0.7860051300245173\n",
      "Epoch 23, Training Loss: 0.7860534973705516\n",
      "Epoch 24, Training Loss: 0.785680549845976\n",
      "Epoch 25, Training Loss: 0.7854827596159543\n",
      "Epoch 26, Training Loss: 0.7850045899783864\n",
      "Epoch 27, Training Loss: 0.7849106961839339\n",
      "Epoch 28, Training Loss: 0.7847041911938611\n",
      "Epoch 29, Training Loss: 0.7844305093849406\n",
      "Epoch 30, Training Loss: 0.7842712825887344\n",
      "Epoch 31, Training Loss: 0.7838371807687423\n",
      "Epoch 32, Training Loss: 0.7835475317169638\n",
      "Epoch 33, Training Loss: 0.783149385171778\n",
      "Epoch 34, Training Loss: 0.7833159366775961\n",
      "Epoch 35, Training Loss: 0.7831974700619193\n",
      "Epoch 36, Training Loss: 0.7827750460540547\n",
      "Epoch 37, Training Loss: 0.783022326932234\n",
      "Epoch 38, Training Loss: 0.7825181507363039\n",
      "Epoch 39, Training Loss: 0.7824265133633334\n",
      "Epoch 40, Training Loss: 0.782081186280531\n",
      "Epoch 41, Training Loss: 0.7817643770750831\n",
      "Epoch 42, Training Loss: 0.7820441392590017\n",
      "Epoch 43, Training Loss: 0.7816188589264365\n",
      "Epoch 44, Training Loss: 0.781422999606413\n",
      "Epoch 45, Training Loss: 0.7813906774100136\n",
      "Epoch 46, Training Loss: 0.7813427716142991\n",
      "Epoch 47, Training Loss: 0.7809718754011042\n",
      "Epoch 48, Training Loss: 0.7810679722533507\n",
      "Epoch 49, Training Loss: 0.7809214977657094\n",
      "Epoch 50, Training Loss: 0.7810170610512004\n",
      "Epoch 51, Training Loss: 0.7808354722752291\n",
      "Epoch 52, Training Loss: 0.7803278843094321\n",
      "Epoch 53, Training Loss: 0.7807184952146867\n",
      "Epoch 54, Training Loss: 0.7805268681750578\n",
      "Epoch 55, Training Loss: 0.7804161332635319\n",
      "Epoch 56, Training Loss: 0.7803032863140106\n",
      "Epoch 57, Training Loss: 0.7801855124445523\n",
      "Epoch 58, Training Loss: 0.7800663753116832\n",
      "Epoch 59, Training Loss: 0.7798299825191498\n",
      "Epoch 60, Training Loss: 0.7802130809952231\n",
      "Epoch 61, Training Loss: 0.7798455267793992\n",
      "Epoch 62, Training Loss: 0.7795510374798494\n",
      "Epoch 63, Training Loss: 0.7797664659864763\n",
      "Epoch 64, Training Loss: 0.7796610654802884\n",
      "Epoch 65, Training Loss: 0.7795100540974561\n",
      "Epoch 66, Training Loss: 0.7793275863984052\n",
      "Epoch 67, Training Loss: 0.7793291901139652\n",
      "Epoch 68, Training Loss: 0.7792736477711621\n",
      "Epoch 69, Training Loss: 0.7793637924334582\n",
      "Epoch 70, Training Loss: 0.7789591356586008\n",
      "Epoch 71, Training Loss: 0.7791680146666135\n",
      "Epoch 72, Training Loss: 0.7790150240589591\n",
      "Epoch 73, Training Loss: 0.7792395267065834\n",
      "Epoch 74, Training Loss: 0.7788267265347874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:55:26,981] Trial 99 finished with value: 0.6385333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7789105387294993\n",
      "Epoch 1, Training Loss: 0.9400765164459453\n",
      "Epoch 2, Training Loss: 0.866829602578107\n",
      "Epoch 3, Training Loss: 0.8606678084766164\n",
      "Epoch 4, Training Loss: 0.8578462909950929\n",
      "Epoch 5, Training Loss: 0.8552202888797311\n",
      "Epoch 6, Training Loss: 0.8524603796005249\n",
      "Epoch 7, Training Loss: 0.8521842543517842\n",
      "Epoch 8, Training Loss: 0.852521970131818\n",
      "Epoch 9, Training Loss: 0.8533044068252339\n",
      "Epoch 10, Training Loss: 0.8504086572282454\n",
      "Epoch 11, Training Loss: 0.8495634820882012\n",
      "Epoch 12, Training Loss: 0.850510472760481\n",
      "Epoch 13, Training Loss: 0.8489358021932489\n",
      "Epoch 14, Training Loss: 0.8478407216072082\n",
      "Epoch 15, Training Loss: 0.8470871319490321\n",
      "Epoch 16, Training Loss: 0.8478957621490254\n",
      "Epoch 17, Training Loss: 0.8472032690749449\n",
      "Epoch 18, Training Loss: 0.8476346067821279\n",
      "Epoch 19, Training Loss: 0.8459516218129326\n",
      "Epoch 20, Training Loss: 0.8477338969006258\n",
      "Epoch 21, Training Loss: 0.846702463907354\n",
      "Epoch 22, Training Loss: 0.8453488927027758\n",
      "Epoch 23, Training Loss: 0.8455908923990586\n",
      "Epoch 24, Training Loss: 0.842818284034729\n",
      "Epoch 25, Training Loss: 0.8452113335272845\n",
      "Epoch 26, Training Loss: 0.8463841826775494\n",
      "Epoch 27, Training Loss: 0.8457104476059184\n",
      "Epoch 28, Training Loss: 0.8452931085053612\n",
      "Epoch 29, Training Loss: 0.8453807100127725\n",
      "Epoch 30, Training Loss: 0.842757559593986\n",
      "Epoch 31, Training Loss: 0.8436455123564777\n",
      "Epoch 32, Training Loss: 0.8422597326951868\n",
      "Epoch 33, Training Loss: 0.8457624354081995\n",
      "Epoch 34, Training Loss: 0.8450994394807254\n",
      "Epoch 35, Training Loss: 0.8413474879545324\n",
      "Epoch 36, Training Loss: 0.8438776810028974\n",
      "Epoch 37, Training Loss: 0.8414223854682025\n",
      "Epoch 38, Training Loss: 0.8444190570887398\n",
      "Epoch 39, Training Loss: 0.8419199807503644\n",
      "Epoch 40, Training Loss: 0.8419399436782388\n",
      "Epoch 41, Training Loss: 0.84300652700312\n",
      "Epoch 42, Training Loss: 0.8431466071745929\n",
      "Epoch 43, Training Loss: 0.8441692941329059\n",
      "Epoch 44, Training Loss: 0.8433008387509514\n",
      "Epoch 45, Training Loss: 0.8427126523326425\n",
      "Epoch 46, Training Loss: 0.8417245229552773\n",
      "Epoch 47, Training Loss: 0.8420180040948532\n",
      "Epoch 48, Training Loss: 0.8410473728881163\n",
      "Epoch 49, Training Loss: 0.8417526272465201\n",
      "Epoch 50, Training Loss: 0.8403456000019522\n",
      "Epoch 51, Training Loss: 0.8407904027490055\n",
      "Epoch 52, Training Loss: 0.83985362943481\n",
      "Epoch 53, Training Loss: 0.8424114922916188\n",
      "Epoch 54, Training Loss: 0.8420906730960397\n",
      "Epoch 55, Training Loss: 0.8415260721655453\n",
      "Epoch 56, Training Loss: 0.840162726430332\n",
      "Epoch 57, Training Loss: 0.8395622875409968\n",
      "Epoch 58, Training Loss: 0.8419269674665788\n",
      "Epoch 59, Training Loss: 0.8419166212222156\n",
      "Epoch 60, Training Loss: 0.841854214738397\n",
      "Epoch 61, Training Loss: 0.8420176865072811\n",
      "Epoch 62, Training Loss: 0.8417537361032823\n",
      "Epoch 63, Training Loss: 0.8418705299321343\n",
      "Epoch 64, Training Loss: 0.8421286343826967\n",
      "Epoch 65, Training Loss: 0.840227975284352\n",
      "Epoch 66, Training Loss: 0.8433429658412933\n",
      "Epoch 67, Training Loss: 0.8406615463425131\n",
      "Epoch 68, Training Loss: 0.8419565134889939\n",
      "Epoch 69, Training Loss: 0.8403114675774294\n",
      "Epoch 70, Training Loss: 0.8378909627830281\n",
      "Epoch 71, Training Loss: 0.8419543870757608\n",
      "Epoch 72, Training Loss: 0.8398680040415596\n",
      "Epoch 73, Training Loss: 0.840970086069668\n",
      "Epoch 74, Training Loss: 0.8410207094865687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:57:00,841] Trial 100 finished with value: 0.6214666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.839951551100787\n",
      "Epoch 1, Training Loss: 0.8969994603185093\n",
      "Epoch 2, Training Loss: 0.8241877087424783\n",
      "Epoch 3, Training Loss: 0.8183382799344904\n",
      "Epoch 4, Training Loss: 0.815794233083725\n",
      "Epoch 5, Training Loss: 0.8127086504767923\n",
      "Epoch 6, Training Loss: 0.8112683300411\n",
      "Epoch 7, Training Loss: 0.8095799703457777\n",
      "Epoch 8, Training Loss: 0.8086555139457479\n",
      "Epoch 9, Training Loss: 0.8079914164543152\n",
      "Epoch 10, Training Loss: 0.8069483313139747\n",
      "Epoch 11, Training Loss: 0.8065506104160758\n",
      "Epoch 12, Training Loss: 0.8063894116177278\n",
      "Epoch 13, Training Loss: 0.8052987616903642\n",
      "Epoch 14, Training Loss: 0.8054307783351226\n",
      "Epoch 15, Training Loss: 0.8051947144901052\n",
      "Epoch 16, Training Loss: 0.804652637313394\n",
      "Epoch 17, Training Loss: 0.8038182708095102\n",
      "Epoch 18, Training Loss: 0.8041632363375495\n",
      "Epoch 19, Training Loss: 0.8041753603430355\n",
      "Epoch 20, Training Loss: 0.8036435753457687\n",
      "Epoch 21, Training Loss: 0.802540788790759\n",
      "Epoch 22, Training Loss: 0.8031348278241999\n",
      "Epoch 23, Training Loss: 0.8029455206674688\n",
      "Epoch 24, Training Loss: 0.8019067552510429\n",
      "Epoch 25, Training Loss: 0.8024621350624982\n",
      "Epoch 26, Training Loss: 0.8022317591134239\n",
      "Epoch 27, Training Loss: 0.8020283662571627\n",
      "Epoch 28, Training Loss: 0.8018801042612861\n",
      "Epoch 29, Training Loss: 0.8018102944598479\n",
      "Epoch 30, Training Loss: 0.8020861958054936\n",
      "Epoch 31, Training Loss: 0.8013485269686755\n",
      "Epoch 32, Training Loss: 0.8014766642626594\n",
      "Epoch 33, Training Loss: 0.8012667538137996\n",
      "Epoch 34, Training Loss: 0.8010631162980023\n",
      "Epoch 35, Training Loss: 0.8006331384181976\n",
      "Epoch 36, Training Loss: 0.8005703573367174\n",
      "Epoch 37, Training Loss: 0.8000923232471242\n",
      "Epoch 38, Training Loss: 0.799802610172945\n",
      "Epoch 39, Training Loss: 0.7998126143567702\n",
      "Epoch 40, Training Loss: 0.7998489543269662\n",
      "Epoch 41, Training Loss: 0.7998836366569295\n",
      "Epoch 42, Training Loss: 0.7997001710358788\n",
      "Epoch 43, Training Loss: 0.7996152857471915\n",
      "Epoch 44, Training Loss: 0.7989437617975123\n",
      "Epoch 45, Training Loss: 0.7991112259556266\n",
      "Epoch 46, Training Loss: 0.7987724799969617\n",
      "Epoch 47, Training Loss: 0.7985167919888216\n",
      "Epoch 48, Training Loss: 0.7978879677548129\n",
      "Epoch 49, Training Loss: 0.798238197705325\n",
      "Epoch 50, Training Loss: 0.7979185372941634\n",
      "Epoch 51, Training Loss: 0.7980880228210898\n",
      "Epoch 52, Training Loss: 0.7975762755029342\n",
      "Epoch 53, Training Loss: 0.7972413887697107\n",
      "Epoch 54, Training Loss: 0.7972330186647527\n",
      "Epoch 55, Training Loss: 0.7976976617644815\n",
      "Epoch 56, Training Loss: 0.796672636901631\n",
      "Epoch 57, Training Loss: 0.7969702680671916\n",
      "Epoch 58, Training Loss: 0.7968598309685202\n",
      "Epoch 59, Training Loss: 0.7965866995559019\n",
      "Epoch 60, Training Loss: 0.7966090864994947\n",
      "Epoch 61, Training Loss: 0.7963696563945097\n",
      "Epoch 62, Training Loss: 0.7958578840424033\n",
      "Epoch 63, Training Loss: 0.7961679497887106\n",
      "Epoch 64, Training Loss: 0.7960166194859672\n",
      "Epoch 65, Training Loss: 0.795249021544176\n",
      "Epoch 66, Training Loss: 0.7958259343399721\n",
      "Epoch 67, Training Loss: 0.7957896282392389\n",
      "Epoch 68, Training Loss: 0.7953919648422915\n",
      "Epoch 69, Training Loss: 0.7955709811519174\n",
      "Epoch 70, Training Loss: 0.7954173786499921\n",
      "Epoch 71, Training Loss: 0.7951499447401832\n",
      "Epoch 72, Training Loss: 0.7949970813358531\n",
      "Epoch 73, Training Loss: 0.7950565323408912\n",
      "Epoch 74, Training Loss: 0.79469458082143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:58:22,939] Trial 101 finished with value: 0.6362666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.794653177541845\n",
      "Epoch 1, Training Loss: 0.8616690173569848\n",
      "Epoch 2, Training Loss: 0.8254139257879818\n",
      "Epoch 3, Training Loss: 0.8212940052677603\n",
      "Epoch 4, Training Loss: 0.8175616795876447\n",
      "Epoch 5, Training Loss: 0.8157792853607851\n",
      "Epoch 6, Training Loss: 0.8134799708338345\n",
      "Epoch 7, Training Loss: 0.8124729490280151\n",
      "Epoch 8, Training Loss: 0.8104185839260326\n",
      "Epoch 9, Training Loss: 0.8097484784967759\n",
      "Epoch 10, Training Loss: 0.8083285695665023\n",
      "Epoch 11, Training Loss: 0.808290708135156\n",
      "Epoch 12, Training Loss: 0.8076934195967281\n",
      "Epoch 13, Training Loss: 0.8065233290896696\n",
      "Epoch 14, Training Loss: 0.8067827106924618\n",
      "Epoch 15, Training Loss: 0.805670412568485\n",
      "Epoch 16, Training Loss: 0.8050762436670416\n",
      "Epoch 17, Training Loss: 0.8052041383350597\n",
      "Epoch 18, Training Loss: 0.805137639466454\n",
      "Epoch 19, Training Loss: 0.8048014490043416\n",
      "Epoch 20, Training Loss: 0.8043074757912579\n",
      "Epoch 21, Training Loss: 0.8043728841052336\n",
      "Epoch 22, Training Loss: 0.8044050833056955\n",
      "Epoch 23, Training Loss: 0.8042617134486928\n",
      "Epoch 24, Training Loss: 0.8030224978923798\n",
      "Epoch 25, Training Loss: 0.8034991406693178\n",
      "Epoch 26, Training Loss: 0.802688427251928\n",
      "Epoch 27, Training Loss: 0.803502687145682\n",
      "Epoch 28, Training Loss: 0.8030845189795774\n",
      "Epoch 29, Training Loss: 0.8032903155859779\n",
      "Epoch 30, Training Loss: 0.8025702411286971\n",
      "Epoch 31, Training Loss: 0.8022832137696884\n",
      "Epoch 32, Training Loss: 0.8024895535497104\n",
      "Epoch 33, Training Loss: 0.8014012664205887\n",
      "Epoch 34, Training Loss: 0.8020214593410492\n",
      "Epoch 35, Training Loss: 0.8021727701495676\n",
      "Epoch 36, Training Loss: 0.8018640065193177\n",
      "Epoch 37, Training Loss: 0.8019508266448975\n",
      "Epoch 38, Training Loss: 0.8017868853316588\n",
      "Epoch 39, Training Loss: 0.801737866822411\n",
      "Epoch 40, Training Loss: 0.8017794907093048\n",
      "Epoch 41, Training Loss: 0.8023467150155236\n",
      "Epoch 42, Training Loss: 0.8014243381163654\n",
      "Epoch 43, Training Loss: 0.8018072516076705\n",
      "Epoch 44, Training Loss: 0.8018018786346212\n",
      "Epoch 45, Training Loss: 0.8009634000413558\n",
      "Epoch 46, Training Loss: 0.8016215852428885\n",
      "Epoch 47, Training Loss: 0.8008277346807368\n",
      "Epoch 48, Training Loss: 0.8016163448025199\n",
      "Epoch 49, Training Loss: 0.8009965778799618\n",
      "Epoch 50, Training Loss: 0.8007477360613205\n",
      "Epoch 51, Training Loss: 0.800822493539137\n",
      "Epoch 52, Training Loss: 0.8001904833316803\n",
      "Epoch 53, Training Loss: 0.8008553382929634\n",
      "Epoch 54, Training Loss: 0.8008263557798723\n",
      "Epoch 55, Training Loss: 0.8010303073069629\n",
      "Epoch 56, Training Loss: 0.8011310812305001\n",
      "Epoch 57, Training Loss: 0.8012055581457475\n",
      "Epoch 58, Training Loss: 0.8003892009398517\n",
      "Epoch 59, Training Loss: 0.8003360288283404\n",
      "Epoch 60, Training Loss: 0.800516136043212\n",
      "Epoch 61, Training Loss: 0.800406713625964\n",
      "Epoch 62, Training Loss: 0.8002912494715523\n",
      "Epoch 63, Training Loss: 0.8006640765947454\n",
      "Epoch 64, Training Loss: 0.8000987536065719\n",
      "Epoch 65, Training Loss: 0.8001412997526282\n",
      "Epoch 66, Training Loss: 0.7999061020682839\n",
      "Epoch 67, Training Loss: 0.8000083822362564\n",
      "Epoch 68, Training Loss: 0.8006637760470895\n",
      "Epoch 69, Training Loss: 0.8002081966400146\n",
      "Epoch 70, Training Loss: 0.7993970462855171\n",
      "Epoch 71, Training Loss: 0.8001647089509403\n",
      "Epoch 72, Training Loss: 0.8004293404607212\n",
      "Epoch 73, Training Loss: 0.7994301878704744\n",
      "Epoch 74, Training Loss: 0.8000802964322707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 11:59:44,342] Trial 102 finished with value: 0.6325333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7996731152253992\n",
      "Epoch 1, Training Loss: 0.8906702313002418\n",
      "Epoch 2, Training Loss: 0.8208085973122541\n",
      "Epoch 3, Training Loss: 0.8145572282286251\n",
      "Epoch 4, Training Loss: 0.8127972734675688\n",
      "Epoch 5, Training Loss: 0.8093968271507936\n",
      "Epoch 6, Training Loss: 0.8093324979613808\n",
      "Epoch 7, Training Loss: 0.8086396572870367\n",
      "Epoch 8, Training Loss: 0.8068495295328253\n",
      "Epoch 9, Training Loss: 0.805616629895042\n",
      "Epoch 10, Training Loss: 0.8057696538111743\n",
      "Epoch 11, Training Loss: 0.8043996940640842\n",
      "Epoch 12, Training Loss: 0.8040267576189603\n",
      "Epoch 13, Training Loss: 0.804072403346791\n",
      "Epoch 14, Training Loss: 0.8042664944424349\n",
      "Epoch 15, Training Loss: 0.8039355240849888\n",
      "Epoch 16, Training Loss: 0.8039363186499652\n",
      "Epoch 17, Training Loss: 0.8027449045461766\n",
      "Epoch 18, Training Loss: 0.8034817423540003\n",
      "Epoch 19, Training Loss: 0.8022949737661025\n",
      "Epoch 20, Training Loss: 0.8029168579157661\n",
      "Epoch 21, Training Loss: 0.8022954290754655\n",
      "Epoch 22, Training Loss: 0.8035804565513835\n",
      "Epoch 23, Training Loss: 0.8019403416268965\n",
      "Epoch 24, Training Loss: 0.8013623558773714\n",
      "Epoch 25, Training Loss: 0.8021393430233001\n",
      "Epoch 26, Training Loss: 0.8016083098860348\n",
      "Epoch 27, Training Loss: 0.8016145055434283\n",
      "Epoch 28, Training Loss: 0.8014871424085954\n",
      "Epoch 29, Training Loss: 0.8013324180070092\n",
      "Epoch 30, Training Loss: 0.8007468126801883\n",
      "Epoch 31, Training Loss: 0.8006717759721419\n",
      "Epoch 32, Training Loss: 0.7998921938503489\n",
      "Epoch 33, Training Loss: 0.8006231514145347\n",
      "Epoch 34, Training Loss: 0.8000538474671981\n",
      "Epoch 35, Training Loss: 0.7998397850289064\n",
      "Epoch 36, Training Loss: 0.8003749073252958\n",
      "Epoch 37, Training Loss: 0.7999434717262492\n",
      "Epoch 38, Training Loss: 0.7995699031212751\n",
      "Epoch 39, Training Loss: 0.7993317759037017\n",
      "Epoch 40, Training Loss: 0.8003775378535776\n",
      "Epoch 41, Training Loss: 0.8001069717547473\n",
      "Epoch 42, Training Loss: 0.7991323426891775\n",
      "Epoch 43, Training Loss: 0.7991005532881793\n",
      "Epoch 44, Training Loss: 0.7987191837675431\n",
      "Epoch 45, Training Loss: 0.7983602307824528\n",
      "Epoch 46, Training Loss: 0.7983000619972453\n",
      "Epoch 47, Training Loss: 0.7983987262669732\n",
      "Epoch 48, Training Loss: 0.7976439064390519\n",
      "Epoch 49, Training Loss: 0.797716835456736\n",
      "Epoch 50, Training Loss: 0.7973717208469615\n",
      "Epoch 51, Training Loss: 0.7980962828327628\n",
      "Epoch 52, Training Loss: 0.7972999057349037\n",
      "Epoch 53, Training Loss: 0.7974633531710681\n",
      "Epoch 54, Training Loss: 0.7969811031397651\n",
      "Epoch 55, Training Loss: 0.796894264571807\n",
      "Epoch 56, Training Loss: 0.7965416719632991\n",
      "Epoch 57, Training Loss: 0.7969602725786321\n",
      "Epoch 58, Training Loss: 0.7971661953365101\n",
      "Epoch 59, Training Loss: 0.7964068721322453\n",
      "Epoch 60, Training Loss: 0.7964185782741098\n",
      "Epoch 61, Training Loss: 0.7963526740494896\n",
      "Epoch 62, Training Loss: 0.7956410147162045\n",
      "Epoch 63, Training Loss: 0.7968555995997261\n",
      "Epoch 64, Training Loss: 0.7962694346904755\n",
      "Epoch 65, Training Loss: 0.7958409604605506\n",
      "Epoch 66, Training Loss: 0.7953233049196355\n",
      "Epoch 67, Training Loss: 0.7954525319267721\n",
      "Epoch 68, Training Loss: 0.795693919097676\n",
      "Epoch 69, Training Loss: 0.795956214946859\n",
      "Epoch 70, Training Loss: 0.7955130985203911\n",
      "Epoch 71, Training Loss: 0.7955935555345872\n",
      "Epoch 72, Training Loss: 0.7950717726174523\n",
      "Epoch 73, Training Loss: 0.7941277133016025\n",
      "Epoch 74, Training Loss: 0.7946926957719467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:01:10,604] Trial 103 finished with value: 0.6383333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7951418176819296\n",
      "Epoch 1, Training Loss: 0.8485238024166653\n",
      "Epoch 2, Training Loss: 0.8173982598727807\n",
      "Epoch 3, Training Loss: 0.8115694375862753\n",
      "Epoch 4, Training Loss: 0.8106633257148858\n",
      "Epoch 5, Training Loss: 0.80988111522861\n",
      "Epoch 6, Training Loss: 0.8101748055981514\n",
      "Epoch 7, Training Loss: 0.8077981638729124\n",
      "Epoch 8, Training Loss: 0.8099988351190897\n",
      "Epoch 9, Training Loss: 0.8063210512462415\n",
      "Epoch 10, Training Loss: 0.8060058185480591\n",
      "Epoch 11, Training Loss: 0.8071792870535887\n",
      "Epoch 12, Training Loss: 0.807461278599904\n",
      "Epoch 13, Training Loss: 0.8065104905824015\n",
      "Epoch 14, Training Loss: 0.8046980310203438\n",
      "Epoch 15, Training Loss: 0.8068589295659746\n",
      "Epoch 16, Training Loss: 0.8069981421743121\n",
      "Epoch 17, Training Loss: 0.8043831252513971\n",
      "Epoch 18, Training Loss: 0.8060992514280448\n",
      "Epoch 19, Training Loss: 0.8062718484634743\n",
      "Epoch 20, Training Loss: 0.8090214671048903\n",
      "Epoch 21, Training Loss: 0.804606584857281\n",
      "Epoch 22, Training Loss: 0.8062907836491005\n",
      "Epoch 23, Training Loss: 0.8063533642238244\n",
      "Epoch 24, Training Loss: 0.8054235495122751\n",
      "Epoch 25, Training Loss: 0.8066532517734327\n",
      "Epoch 26, Training Loss: 0.8051331035176614\n",
      "Epoch 27, Training Loss: 0.8029763039789701\n",
      "Epoch 28, Training Loss: 0.8048156164642564\n",
      "Epoch 29, Training Loss: 0.8053791661011546\n",
      "Epoch 30, Training Loss: 0.8059915662708139\n",
      "Epoch 31, Training Loss: 0.8065568223035425\n",
      "Epoch 32, Training Loss: 0.8033083015366604\n",
      "Epoch 33, Training Loss: 0.804354783287622\n",
      "Epoch 34, Training Loss: 0.8050083081525071\n",
      "Epoch 35, Training Loss: 0.8038433278861798\n",
      "Epoch 36, Training Loss: 0.8044856441648383\n",
      "Epoch 37, Training Loss: 0.8053202240986932\n",
      "Epoch 38, Training Loss: 0.8037537796156747\n",
      "Epoch 39, Training Loss: 0.8029352258022566\n",
      "Epoch 40, Training Loss: 0.8051050498969573\n",
      "Epoch 41, Training Loss: 0.803599854967648\n",
      "Epoch 42, Training Loss: 0.8064198636470881\n",
      "Epoch 43, Training Loss: 0.8027477293086231\n",
      "Epoch 44, Training Loss: 0.8045780078809064\n",
      "Epoch 45, Training Loss: 0.8053428676791657\n",
      "Epoch 46, Training Loss: 0.8021722951329741\n",
      "Epoch 47, Training Loss: 0.8047975717630601\n",
      "Epoch 48, Training Loss: 0.8071434796304631\n",
      "Epoch 49, Training Loss: 0.8045978164314327\n",
      "Epoch 50, Training Loss: 0.8051582175089901\n",
      "Epoch 51, Training Loss: 0.8066240365343883\n",
      "Epoch 52, Training Loss: 0.8050410138036972\n",
      "Epoch 53, Training Loss: 0.8042259063935817\n",
      "Epoch 54, Training Loss: 0.8042535215840304\n",
      "Epoch 55, Training Loss: 0.8049124955234671\n",
      "Epoch 56, Training Loss: 0.8054351092281198\n",
      "Epoch 57, Training Loss: 0.8043270559239208\n",
      "Epoch 58, Training Loss: 0.803288094396878\n",
      "Epoch 59, Training Loss: 0.804213181384524\n",
      "Epoch 60, Training Loss: 0.8045516628968088\n",
      "Epoch 61, Training Loss: 0.8065585877662315\n",
      "Epoch 62, Training Loss: 0.811004818740644\n",
      "Epoch 63, Training Loss: 0.804747654351973\n",
      "Epoch 64, Training Loss: 0.8049276324143088\n",
      "Epoch 65, Training Loss: 0.8039858563502033\n",
      "Epoch 66, Training Loss: 0.8050992813325466\n",
      "Epoch 67, Training Loss: 0.8047637743161137\n",
      "Epoch 68, Training Loss: 0.8070962454143323\n",
      "Epoch 69, Training Loss: 0.8058028432659636\n",
      "Epoch 70, Training Loss: 0.8044919303485325\n",
      "Epoch 71, Training Loss: 0.8068144606468373\n",
      "Epoch 72, Training Loss: 0.8046679772828754\n",
      "Epoch 73, Training Loss: 0.805134788133148\n",
      "Epoch 74, Training Loss: 0.805734912703808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:02:37,269] Trial 104 finished with value: 0.6256 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8018447667136228\n",
      "Epoch 1, Training Loss: 0.8618535202391008\n",
      "Epoch 2, Training Loss: 0.8283151660947239\n",
      "Epoch 3, Training Loss: 0.8228146879112019\n",
      "Epoch 4, Training Loss: 0.8188888216719908\n",
      "Epoch 5, Training Loss: 0.8165253372753367\n",
      "Epoch 6, Training Loss: 0.8131635561410119\n",
      "Epoch 7, Training Loss: 0.8128827761902528\n",
      "Epoch 8, Training Loss: 0.8110710411212023\n",
      "Epoch 9, Training Loss: 0.8105052361067604\n",
      "Epoch 10, Training Loss: 0.8104120973979726\n",
      "Epoch 11, Training Loss: 0.8102375029115116\n",
      "Epoch 12, Training Loss: 0.8091957218506757\n",
      "Epoch 13, Training Loss: 0.8090871787071228\n",
      "Epoch 14, Training Loss: 0.8081912697764004\n",
      "Epoch 15, Training Loss: 0.8075528613258811\n",
      "Epoch 16, Training Loss: 0.8073845027474796\n",
      "Epoch 17, Training Loss: 0.807556234738406\n",
      "Epoch 18, Training Loss: 0.8078515714757583\n",
      "Epoch 19, Training Loss: 0.8066307045431698\n",
      "Epoch 20, Training Loss: 0.8064455509185791\n",
      "Epoch 21, Training Loss: 0.8067674997974844\n",
      "Epoch 22, Training Loss: 0.8065309200567358\n",
      "Epoch 23, Training Loss: 0.8063126026882845\n",
      "Epoch 24, Training Loss: 0.8060401157070609\n",
      "Epoch 25, Training Loss: 0.8055075783589307\n",
      "Epoch 26, Training Loss: 0.8056864542119643\n",
      "Epoch 27, Training Loss: 0.805580859114142\n",
      "Epoch 28, Training Loss: 0.805016965375227\n",
      "Epoch 29, Training Loss: 0.8053204829552595\n",
      "Epoch 30, Training Loss: 0.8040040739143596\n",
      "Epoch 31, Training Loss: 0.804975123054841\n",
      "Epoch 32, Training Loss: 0.8042106080055237\n",
      "Epoch 33, Training Loss: 0.8041369353322422\n",
      "Epoch 34, Training Loss: 0.8037469500653884\n",
      "Epoch 35, Training Loss: 0.803694210543352\n",
      "Epoch 36, Training Loss: 0.8038173426600064\n",
      "Epoch 37, Training Loss: 0.8035979689570034\n",
      "Epoch 38, Training Loss: 0.8029380919652827\n",
      "Epoch 39, Training Loss: 0.8033304577014025\n",
      "Epoch 40, Training Loss: 0.8027930648887859\n",
      "Epoch 41, Training Loss: 0.8030202677670647\n",
      "Epoch 42, Training Loss: 0.802453564195072\n",
      "Epoch 43, Training Loss: 0.8017610552030451\n",
      "Epoch 44, Training Loss: 0.8017281276338241\n",
      "Epoch 45, Training Loss: 0.8018171855281381\n",
      "Epoch 46, Training Loss: 0.8015223519241108\n",
      "Epoch 47, Training Loss: 0.8013840600322275\n",
      "Epoch 48, Training Loss: 0.8016796531396754\n",
      "Epoch 49, Training Loss: 0.8010559601643507\n",
      "Epoch 50, Training Loss: 0.8017258293488446\n",
      "Epoch 51, Training Loss: 0.8014792754369624\n",
      "Epoch 52, Training Loss: 0.8007604995895835\n",
      "Epoch 53, Training Loss: 0.8010024206077351\n",
      "Epoch 54, Training Loss: 0.8007454586029052\n",
      "Epoch 55, Training Loss: 0.8007671759409063\n",
      "Epoch 56, Training Loss: 0.8008922834256116\n",
      "Epoch 57, Training Loss: 0.8006932790840373\n",
      "Epoch 58, Training Loss: 0.8003002945815816\n",
      "Epoch 59, Training Loss: 0.800028517035877\n",
      "Epoch 60, Training Loss: 0.7997235454531277\n",
      "Epoch 61, Training Loss: 0.8003906705098993\n",
      "Epoch 62, Training Loss: 0.800116259911481\n",
      "Epoch 63, Training Loss: 0.7999032995280098\n",
      "Epoch 64, Training Loss: 0.7996730669806985\n",
      "Epoch 65, Training Loss: 0.7994366662642535\n",
      "Epoch 66, Training Loss: 0.7994506462882547\n",
      "Epoch 67, Training Loss: 0.7995854189115412\n",
      "Epoch 68, Training Loss: 0.7990859652266783\n",
      "Epoch 69, Training Loss: 0.7985514363821815\n",
      "Epoch 70, Training Loss: 0.7992104285604813\n",
      "Epoch 71, Training Loss: 0.7990334756234113\n",
      "Epoch 72, Training Loss: 0.7984561638271107\n",
      "Epoch 73, Training Loss: 0.7986554199106553\n",
      "Epoch 74, Training Loss: 0.798392710054622\n",
      "Epoch 75, Training Loss: 0.7982087702610914\n",
      "Epoch 76, Training Loss: 0.7976244427176082\n",
      "Epoch 77, Training Loss: 0.7981148691037122\n",
      "Epoch 78, Training Loss: 0.7978165627928341\n",
      "Epoch 79, Training Loss: 0.7976199107310351\n",
      "Epoch 80, Training Loss: 0.7977560370108661\n",
      "Epoch 81, Training Loss: 0.7975744598753312\n",
      "Epoch 82, Training Loss: 0.7976904919568231\n",
      "Epoch 83, Training Loss: 0.7973322065437541\n",
      "Epoch 84, Training Loss: 0.7977415239810943\n",
      "Epoch 85, Training Loss: 0.7974251289928661\n",
      "Epoch 86, Training Loss: 0.7971624296552995\n",
      "Epoch 87, Training Loss: 0.7974522751219132\n",
      "Epoch 88, Training Loss: 0.7973232326788061\n",
      "Epoch 89, Training Loss: 0.7969749062201557\n",
      "Epoch 90, Training Loss: 0.7968036866889281\n",
      "Epoch 91, Training Loss: 0.7971877638732686\n",
      "Epoch 92, Training Loss: 0.7965854762582218\n",
      "Epoch 93, Training Loss: 0.796852199680665\n",
      "Epoch 94, Training Loss: 0.7968600709999308\n",
      "Epoch 95, Training Loss: 0.7963141554944655\n",
      "Epoch 96, Training Loss: 0.7964944310048048\n",
      "Epoch 97, Training Loss: 0.7963909954183241\n",
      "Epoch 98, Training Loss: 0.7964652371406555\n",
      "Epoch 99, Training Loss: 0.7965036344528198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:04:26,707] Trial 105 finished with value: 0.6385333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.795819528243121\n",
      "Epoch 1, Training Loss: 0.8712848127589506\n",
      "Epoch 2, Training Loss: 0.8237830990903517\n",
      "Epoch 3, Training Loss: 0.8182758817953222\n",
      "Epoch 4, Training Loss: 0.8147464959761676\n",
      "Epoch 5, Training Loss: 0.8116721468112048\n",
      "Epoch 6, Training Loss: 0.8099450286696939\n",
      "Epoch 7, Training Loss: 0.8084587948462543\n",
      "Epoch 8, Training Loss: 0.8064534934829263\n",
      "Epoch 9, Training Loss: 0.8055289977438309\n",
      "Epoch 10, Training Loss: 0.8048890224625083\n",
      "Epoch 11, Training Loss: 0.804133373078178\n",
      "Epoch 12, Training Loss: 0.8035633866225972\n",
      "Epoch 13, Training Loss: 0.8032982702816234\n",
      "Epoch 14, Training Loss: 0.8027499400166904\n",
      "Epoch 15, Training Loss: 0.8026973459299873\n",
      "Epoch 16, Training Loss: 0.8015702077921699\n",
      "Epoch 17, Training Loss: 0.8016874672384823\n",
      "Epoch 18, Training Loss: 0.8007679076054517\n",
      "Epoch 19, Training Loss: 0.8007636912430034\n",
      "Epoch 20, Training Loss: 0.8001758092291215\n",
      "Epoch 21, Training Loss: 0.800046179855571\n",
      "Epoch 22, Training Loss: 0.7994712786113515\n",
      "Epoch 23, Training Loss: 0.7998161252105938\n",
      "Epoch 24, Training Loss: 0.7998225838296553\n",
      "Epoch 25, Training Loss: 0.7988413083553314\n",
      "Epoch 26, Training Loss: 0.7985148232123431\n",
      "Epoch 27, Training Loss: 0.7980103980793672\n",
      "Epoch 28, Training Loss: 0.798430079572341\n",
      "Epoch 29, Training Loss: 0.7979946031289942\n",
      "Epoch 30, Training Loss: 0.7978279566764832\n",
      "Epoch 31, Training Loss: 0.7970297841464772\n",
      "Epoch 32, Training Loss: 0.7978767130655401\n",
      "Epoch 33, Training Loss: 0.7972927765986498\n",
      "Epoch 34, Training Loss: 0.7969810406600728\n",
      "Epoch 35, Training Loss: 0.7969285407487083\n",
      "Epoch 36, Training Loss: 0.796607122631634\n",
      "Epoch 37, Training Loss: 0.7966638398871703\n",
      "Epoch 38, Training Loss: 0.7960214912891388\n",
      "Epoch 39, Training Loss: 0.7964979850544649\n",
      "Epoch 40, Training Loss: 0.7966402613415438\n",
      "Epoch 41, Training Loss: 0.7960432891284718\n",
      "Epoch 42, Training Loss: 0.7966378084351035\n",
      "Epoch 43, Training Loss: 0.7960943689065821\n",
      "Epoch 44, Training Loss: 0.7955971057274762\n",
      "Epoch 45, Training Loss: 0.7959465085057651\n",
      "Epoch 46, Training Loss: 0.7955159714642693\n",
      "Epoch 47, Training Loss: 0.7961517113797805\n",
      "Epoch 48, Training Loss: 0.7966772547890159\n",
      "Epoch 49, Training Loss: 0.795608564895742\n",
      "Epoch 50, Training Loss: 0.7956013201966006\n",
      "Epoch 51, Training Loss: 0.7962559556259828\n",
      "Epoch 52, Training Loss: 0.7959658788933474\n",
      "Epoch 53, Training Loss: 0.795570225575391\n",
      "Epoch 54, Training Loss: 0.7955203561221852\n",
      "Epoch 55, Training Loss: 0.7948607779951656\n",
      "Epoch 56, Training Loss: 0.7953724608000587\n",
      "Epoch 57, Training Loss: 0.794957313116859\n",
      "Epoch 58, Training Loss: 0.7953215070331798\n",
      "Epoch 59, Training Loss: 0.7949356017393224\n",
      "Epoch 60, Training Loss: 0.7955796488593606\n",
      "Epoch 61, Training Loss: 0.7954510002977708\n",
      "Epoch 62, Training Loss: 0.7951425080439624\n",
      "Epoch 63, Training Loss: 0.7954390283191906\n",
      "Epoch 64, Training Loss: 0.7947666649958667\n",
      "Epoch 65, Training Loss: 0.7950246621580684\n",
      "Epoch 66, Training Loss: 0.7947587527948268\n",
      "Epoch 67, Training Loss: 0.7947080258762135\n",
      "Epoch 68, Training Loss: 0.7945148491859436\n",
      "Epoch 69, Training Loss: 0.7953144349771387\n",
      "Epoch 70, Training Loss: 0.7954152821092044\n",
      "Epoch 71, Training Loss: 0.794589930562412\n",
      "Epoch 72, Training Loss: 0.7953391270076527\n",
      "Epoch 73, Training Loss: 0.794649617321351\n",
      "Epoch 74, Training Loss: 0.79472967302098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:06:00,462] Trial 106 finished with value: 0.6326 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7938697516216952\n",
      "Epoch 1, Training Loss: 1.0934926155034232\n",
      "Epoch 2, Training Loss: 1.091056871133692\n",
      "Epoch 3, Training Loss: 1.0908033492985894\n",
      "Epoch 4, Training Loss: 1.0905633540714488\n",
      "Epoch 5, Training Loss: 1.090318215594572\n",
      "Epoch 6, Training Loss: 1.090097984566408\n",
      "Epoch 7, Training Loss: 1.0898719731499167\n",
      "Epoch 8, Training Loss: 1.089626591906828\n",
      "Epoch 9, Training Loss: 1.0893883700931772\n",
      "Epoch 10, Training Loss: 1.089152679864098\n",
      "Epoch 11, Training Loss: 1.0889153236501357\n",
      "Epoch 12, Training Loss: 1.0886620340627782\n",
      "Epoch 13, Training Loss: 1.0883945700701545\n",
      "Epoch 14, Training Loss: 1.0882050326291253\n",
      "Epoch 15, Training Loss: 1.0879397887342117\n",
      "Epoch 16, Training Loss: 1.0876983942705043\n",
      "Epoch 17, Training Loss: 1.087459529427921\n",
      "Epoch 18, Training Loss: 1.0872131119054906\n",
      "Epoch 19, Training Loss: 1.08693958605037\n",
      "Epoch 20, Training Loss: 1.08669329699348\n",
      "Epoch 21, Training Loss: 1.086438223474166\n",
      "Epoch 22, Training Loss: 1.0861841586056877\n",
      "Epoch 23, Training Loss: 1.0859316224210402\n",
      "Epoch 24, Training Loss: 1.0856676546265098\n",
      "Epoch 25, Training Loss: 1.085390490363626\n",
      "Epoch 26, Training Loss: 1.0851245843662936\n",
      "Epoch 27, Training Loss: 1.0848586130142213\n",
      "Epoch 28, Training Loss: 1.084575791499194\n",
      "Epoch 29, Training Loss: 1.0842891562686248\n",
      "Epoch 30, Training Loss: 1.0839960674678577\n",
      "Epoch 31, Training Loss: 1.083712573331945\n",
      "Epoch 32, Training Loss: 1.0834181390089148\n",
      "Epoch 33, Training Loss: 1.0830899528896107\n",
      "Epoch 34, Training Loss: 1.08282667426502\n",
      "Epoch 35, Training Loss: 1.0824863675061394\n",
      "Epoch 36, Training Loss: 1.0821794281286352\n",
      "Epoch 37, Training Loss: 1.0818475303930395\n",
      "Epoch 38, Training Loss: 1.081535631348105\n",
      "Epoch 39, Training Loss: 1.081201396409203\n",
      "Epoch 40, Training Loss: 1.080846231544719\n",
      "Epoch 41, Training Loss: 1.0804965692407944\n",
      "Epoch 42, Training Loss: 1.0801552823010614\n",
      "Epoch 43, Training Loss: 1.0797704916841844\n",
      "Epoch 44, Training Loss: 1.0794019854769987\n",
      "Epoch 45, Training Loss: 1.079038727984709\n",
      "Epoch 46, Training Loss: 1.0786343749831704\n",
      "Epoch 47, Training Loss: 1.0782363389520084\n",
      "Epoch 48, Training Loss: 1.0778530691651738\n",
      "Epoch 49, Training Loss: 1.0774355337199042\n",
      "Epoch 50, Training Loss: 1.0769939400168027\n",
      "Epoch 51, Training Loss: 1.0765700239293716\n",
      "Epoch 52, Training Loss: 1.0761300089780022\n",
      "Epoch 53, Training Loss: 1.0756883855427013\n",
      "Epoch 54, Training Loss: 1.0752058700954212\n",
      "Epoch 55, Training Loss: 1.0747258452808155\n",
      "Epoch 56, Training Loss: 1.0742571384766522\n",
      "Epoch 57, Training Loss: 1.0737491990538204\n",
      "Epoch 58, Training Loss: 1.0732419845637153\n",
      "Epoch 59, Training Loss: 1.072725381009719\n",
      "Epoch 60, Training Loss: 1.0721736710211809\n",
      "Epoch 61, Training Loss: 1.071628492860233\n",
      "Epoch 62, Training Loss: 1.0710789099861593\n",
      "Epoch 63, Training Loss: 1.0704968640383552\n",
      "Epoch 64, Training Loss: 1.069899389743805\n",
      "Epoch 65, Training Loss: 1.06929179724525\n",
      "Epoch 66, Training Loss: 1.0686823937472174\n",
      "Epoch 67, Training Loss: 1.0680376372617835\n",
      "Epoch 68, Training Loss: 1.0673756877113791\n",
      "Epoch 69, Training Loss: 1.066711376134087\n",
      "Epoch 70, Training Loss: 1.0660341986487893\n",
      "Epoch 71, Training Loss: 1.065332013017991\n",
      "Epoch 72, Training Loss: 1.064589090908275\n",
      "Epoch 73, Training Loss: 1.0638657955562367\n",
      "Epoch 74, Training Loss: 1.0631108328875374\n",
      "Epoch 75, Training Loss: 1.062333498281591\n",
      "Epoch 76, Training Loss: 1.0615331212212058\n",
      "Epoch 77, Training Loss: 1.0607121677959666\n",
      "Epoch 78, Training Loss: 1.0598779713406283\n",
      "Epoch 79, Training Loss: 1.059025638103485\n",
      "Epoch 80, Training Loss: 1.0581460121098687\n",
      "Epoch 81, Training Loss: 1.0572491550445557\n",
      "Epoch 82, Training Loss: 1.056319842338562\n",
      "Epoch 83, Training Loss: 1.0553873745132896\n",
      "Epoch 84, Training Loss: 1.054396321773529\n",
      "Epoch 85, Training Loss: 1.053450700114755\n",
      "Epoch 86, Training Loss: 1.0524195856206557\n",
      "Epoch 87, Training Loss: 1.0513946906258078\n",
      "Epoch 88, Training Loss: 1.0503450897160698\n",
      "Epoch 89, Training Loss: 1.0492765308828915\n",
      "Epoch 90, Training Loss: 1.048169168514364\n",
      "Epoch 91, Training Loss: 1.0470369292006774\n",
      "Epoch 92, Training Loss: 1.04589971556383\n",
      "Epoch 93, Training Loss: 1.0447315938332502\n",
      "Epoch 94, Training Loss: 1.0435367710450116\n",
      "Epoch 95, Training Loss: 1.0423300547459546\n",
      "Epoch 96, Training Loss: 1.0410915409817416\n",
      "Epoch 97, Training Loss: 1.0398423943098853\n",
      "Epoch 98, Training Loss: 1.038559212474262\n",
      "Epoch 99, Training Loss: 1.037263065436307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:07:46,333] Trial 107 finished with value: 0.491 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.0359442949295044\n",
      "Epoch 1, Training Loss: 0.9304604182566019\n",
      "Epoch 2, Training Loss: 0.8607490628285516\n",
      "Epoch 3, Training Loss: 0.8234362625537959\n",
      "Epoch 4, Training Loss: 0.8097573498137911\n",
      "Epoch 5, Training Loss: 0.8048928129941897\n",
      "Epoch 6, Training Loss: 0.8020328159619095\n",
      "Epoch 7, Training Loss: 0.8019919835535207\n",
      "Epoch 8, Training Loss: 0.8008888520692524\n",
      "Epoch 9, Training Loss: 0.8005737132595894\n",
      "Epoch 10, Training Loss: 0.7999454674864174\n",
      "Epoch 11, Training Loss: 0.800074411155586\n",
      "Epoch 12, Training Loss: 0.7993335304403664\n",
      "Epoch 13, Training Loss: 0.7992553454592712\n",
      "Epoch 14, Training Loss: 0.799092001305487\n",
      "Epoch 15, Training Loss: 0.7979711060237168\n",
      "Epoch 16, Training Loss: 0.7984002756893187\n",
      "Epoch 17, Training Loss: 0.797925019084959\n",
      "Epoch 18, Training Loss: 0.7973415642752684\n",
      "Epoch 19, Training Loss: 0.797380536749847\n",
      "Epoch 20, Training Loss: 0.7977060927484269\n",
      "Epoch 21, Training Loss: 0.7976086234687862\n",
      "Epoch 22, Training Loss: 0.7968039950930086\n",
      "Epoch 23, Training Loss: 0.7970912829377598\n",
      "Epoch 24, Training Loss: 0.7967337611922645\n",
      "Epoch 25, Training Loss: 0.7965875228544823\n",
      "Epoch 26, Training Loss: 0.7973560551951703\n",
      "Epoch 27, Training Loss: 0.7963594216153138\n",
      "Epoch 28, Training Loss: 0.7969603716878962\n",
      "Epoch 29, Training Loss: 0.7965756997129971\n",
      "Epoch 30, Training Loss: 0.7967828719239486\n",
      "Epoch 31, Training Loss: 0.7954517014044568\n",
      "Epoch 32, Training Loss: 0.7959246115576952\n",
      "Epoch 33, Training Loss: 0.7954811815032385\n",
      "Epoch 34, Training Loss: 0.795380133076718\n",
      "Epoch 35, Training Loss: 0.7954735140155133\n",
      "Epoch 36, Training Loss: 0.795422341321644\n",
      "Epoch 37, Training Loss: 0.7954085013920204\n",
      "Epoch 38, Training Loss: 0.7950749254764471\n",
      "Epoch 39, Training Loss: 0.7958484897936197\n",
      "Epoch 40, Training Loss: 0.7954192368607772\n",
      "Epoch 41, Training Loss: 0.7949292424029873\n",
      "Epoch 42, Training Loss: 0.7947039035029877\n",
      "Epoch 43, Training Loss: 0.7945418420590853\n",
      "Epoch 44, Training Loss: 0.7941728914590707\n",
      "Epoch 45, Training Loss: 0.7942210694901028\n",
      "Epoch 46, Training Loss: 0.794233190654812\n",
      "Epoch 47, Training Loss: 0.7933772891087639\n",
      "Epoch 48, Training Loss: 0.7932973703047387\n",
      "Epoch 49, Training Loss: 0.7928751110134268\n",
      "Epoch 50, Training Loss: 0.7921526014356685\n",
      "Epoch 51, Training Loss: 0.791713997923342\n",
      "Epoch 52, Training Loss: 0.791251023729941\n",
      "Epoch 53, Training Loss: 0.7910639357746095\n",
      "Epoch 54, Training Loss: 0.7911397653414791\n",
      "Epoch 55, Training Loss: 0.7910079066914724\n",
      "Epoch 56, Training Loss: 0.7906565914476724\n",
      "Epoch 57, Training Loss: 0.7903857033951838\n",
      "Epoch 58, Training Loss: 0.7902429149563152\n",
      "Epoch 59, Training Loss: 0.7899488787005718\n",
      "Epoch 60, Training Loss: 0.7901244151861148\n",
      "Epoch 61, Training Loss: 0.7895916956707947\n",
      "Epoch 62, Training Loss: 0.7908247841031928\n",
      "Epoch 63, Training Loss: 0.7891002826224592\n",
      "Epoch 64, Training Loss: 0.789392736531738\n",
      "Epoch 65, Training Loss: 0.7895388311909554\n",
      "Epoch 66, Training Loss: 0.7905740650972926\n",
      "Epoch 67, Training Loss: 0.7893850911828808\n",
      "Epoch 68, Training Loss: 0.7897384061849206\n",
      "Epoch 69, Training Loss: 0.7888917689036606\n",
      "Epoch 70, Training Loss: 0.7899666008196379\n",
      "Epoch 71, Training Loss: 0.789205233136514\n",
      "Epoch 72, Training Loss: 0.7893986208098275\n",
      "Epoch 73, Training Loss: 0.7889701677444286\n",
      "Epoch 74, Training Loss: 0.7887560552224181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:09:00,511] Trial 108 finished with value: 0.6356 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7884718410054544\n",
      "Epoch 1, Training Loss: 1.0302449102261486\n",
      "Epoch 2, Training Loss: 0.945414835915846\n",
      "Epoch 3, Training Loss: 0.9266927351671107\n",
      "Epoch 4, Training Loss: 0.9161285065903383\n",
      "Epoch 5, Training Loss: 0.9059002104927512\n",
      "Epoch 6, Training Loss: 0.8941719251520494\n",
      "Epoch 7, Training Loss: 0.8795232697094189\n",
      "Epoch 8, Training Loss: 0.8613851878222297\n",
      "Epoch 9, Training Loss: 0.8426200753099778\n",
      "Epoch 10, Training Loss: 0.8269276401575874\n",
      "Epoch 11, Training Loss: 0.8165896085430594\n",
      "Epoch 12, Training Loss: 0.8108687766159282\n",
      "Epoch 13, Training Loss: 0.8076996012996225\n",
      "Epoch 14, Training Loss: 0.8062010130461524\n",
      "Epoch 15, Training Loss: 0.8051297903060913\n",
      "Epoch 16, Training Loss: 0.8044354732597575\n",
      "Epoch 17, Training Loss: 0.8035539180390975\n",
      "Epoch 18, Training Loss: 0.8031957858450273\n",
      "Epoch 19, Training Loss: 0.8023616296403548\n",
      "Epoch 20, Training Loss: 0.8020843031827142\n",
      "Epoch 21, Training Loss: 0.8012260897720561\n",
      "Epoch 22, Training Loss: 0.800499955555972\n",
      "Epoch 23, Training Loss: 0.8002672888952143\n",
      "Epoch 24, Training Loss: 0.7993871793326209\n",
      "Epoch 25, Training Loss: 0.7992398052355822\n",
      "Epoch 26, Training Loss: 0.798572190509123\n",
      "Epoch 27, Training Loss: 0.7982350653760574\n",
      "Epoch 28, Training Loss: 0.7978510724796969\n",
      "Epoch 29, Training Loss: 0.7975394858332241\n",
      "Epoch 30, Training Loss: 0.7973521010314717\n",
      "Epoch 31, Training Loss: 0.7967934146348168\n",
      "Epoch 32, Training Loss: 0.7963912491237416\n",
      "Epoch 33, Training Loss: 0.7963960069768569\n",
      "Epoch 34, Training Loss: 0.7962859765221091\n",
      "Epoch 35, Training Loss: 0.7961079095391667\n",
      "Epoch 36, Training Loss: 0.7958265403439017\n",
      "Epoch 37, Training Loss: 0.795405547618866\n",
      "Epoch 38, Training Loss: 0.7953477253633386\n",
      "Epoch 39, Training Loss: 0.7953439045653624\n",
      "Epoch 40, Training Loss: 0.7947585096078761\n",
      "Epoch 41, Training Loss: 0.7946701359047609\n",
      "Epoch 42, Training Loss: 0.7945339600478901\n",
      "Epoch 43, Training Loss: 0.7947318135990816\n",
      "Epoch 44, Training Loss: 0.7941428439056172\n",
      "Epoch 45, Training Loss: 0.7941433717222774\n",
      "Epoch 46, Training Loss: 0.7937602250716266\n",
      "Epoch 47, Training Loss: 0.7935280830018661\n",
      "Epoch 48, Training Loss: 0.7935738934488857\n",
      "Epoch 49, Training Loss: 0.7934359692825991\n",
      "Epoch 50, Training Loss: 0.793062657328213\n",
      "Epoch 51, Training Loss: 0.7930384723579182\n",
      "Epoch 52, Training Loss: 0.7927307806996738\n",
      "Epoch 53, Training Loss: 0.7925679410205168\n",
      "Epoch 54, Training Loss: 0.7923466149498435\n",
      "Epoch 55, Training Loss: 0.792325279712677\n",
      "Epoch 56, Training Loss: 0.7920774433893316\n",
      "Epoch 57, Training Loss: 0.7919318088363199\n",
      "Epoch 58, Training Loss: 0.7917945146560669\n",
      "Epoch 59, Training Loss: 0.7916211029361276\n",
      "Epoch 60, Training Loss: 0.791410927772522\n",
      "Epoch 61, Training Loss: 0.7913470625176149\n",
      "Epoch 62, Training Loss: 0.7911415658277624\n",
      "Epoch 63, Training Loss: 0.7910372383454267\n",
      "Epoch 64, Training Loss: 0.7905550000948064\n",
      "Epoch 65, Training Loss: 0.790616538734997\n",
      "Epoch 66, Training Loss: 0.7905168597838458\n",
      "Epoch 67, Training Loss: 0.7902679828335257\n",
      "Epoch 68, Training Loss: 0.7901409440180834\n",
      "Epoch 69, Training Loss: 0.7898329363149755\n",
      "Epoch 70, Training Loss: 0.7898565063757055\n",
      "Epoch 71, Training Loss: 0.7896194892069873\n",
      "Epoch 72, Training Loss: 0.7895274597757003\n",
      "Epoch 73, Training Loss: 0.7892759964045356\n",
      "Epoch 74, Training Loss: 0.7891375677725848\n",
      "Epoch 75, Training Loss: 0.7889999323031481\n",
      "Epoch 76, Training Loss: 0.7888929986953735\n",
      "Epoch 77, Training Loss: 0.788649396685993\n",
      "Epoch 78, Training Loss: 0.788633257711635\n",
      "Epoch 79, Training Loss: 0.7883936910068288\n",
      "Epoch 80, Training Loss: 0.7881617607789881\n",
      "Epoch 81, Training Loss: 0.7883423721790314\n",
      "Epoch 82, Training Loss: 0.7880304237674264\n",
      "Epoch 83, Training Loss: 0.7879947751409867\n",
      "Epoch 84, Training Loss: 0.7880078858487747\n",
      "Epoch 85, Training Loss: 0.7878110079204335\n",
      "Epoch 86, Training Loss: 0.7875537441758549\n",
      "Epoch 87, Training Loss: 0.7874021816253662\n",
      "Epoch 88, Training Loss: 0.7874410865587347\n",
      "Epoch 89, Training Loss: 0.7873338501593646\n",
      "Epoch 90, Training Loss: 0.7869995680276085\n",
      "Epoch 91, Training Loss: 0.7869392826276667\n",
      "Epoch 92, Training Loss: 0.7868544054031372\n",
      "Epoch 93, Training Loss: 0.7868466371648452\n",
      "Epoch 94, Training Loss: 0.7865028422019061\n",
      "Epoch 95, Training Loss: 0.7866443578635945\n",
      "Epoch 96, Training Loss: 0.7864102606913622\n",
      "Epoch 97, Training Loss: 0.786366998237722\n",
      "Epoch 98, Training Loss: 0.7862683631392087\n",
      "Epoch 99, Training Loss: 0.7861161991427926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:10:46,421] Trial 109 finished with value: 0.6400666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7859577946803149\n",
      "Epoch 1, Training Loss: 1.0956038695223191\n",
      "Epoch 2, Training Loss: 1.0673642420768739\n",
      "Epoch 3, Training Loss: 1.0519442415938658\n",
      "Epoch 4, Training Loss: 1.0391571875880747\n",
      "Epoch 5, Training Loss: 1.0269881649578319\n",
      "Epoch 6, Training Loss: 1.0150776375041288\n",
      "Epoch 7, Training Loss: 1.0034779990420621\n",
      "Epoch 8, Training Loss: 0.9924303187342252\n",
      "Epoch 9, Training Loss: 0.9822583981822519\n",
      "Epoch 10, Training Loss: 0.9732621959377737\n",
      "Epoch 11, Training Loss: 0.965644815529094\n",
      "Epoch 12, Training Loss: 0.9594230258464813\n",
      "Epoch 13, Training Loss: 0.9544765804094427\n",
      "Epoch 14, Training Loss: 0.9506206496322857\n",
      "Epoch 15, Training Loss: 0.9476139595929314\n",
      "Epoch 16, Training Loss: 0.9452425703581642\n",
      "Epoch 17, Training Loss: 0.943306003668729\n",
      "Epoch 18, Training Loss: 0.9416962813629823\n",
      "Epoch 19, Training Loss: 0.9402810607938206\n",
      "Epoch 20, Training Loss: 0.9390156536242541\n",
      "Epoch 21, Training Loss: 0.937839637223412\n",
      "Epoch 22, Training Loss: 0.9367322385311126\n",
      "Epoch 23, Training Loss: 0.9356684181269478\n",
      "Epoch 24, Training Loss: 0.9346445482618668\n",
      "Epoch 25, Training Loss: 0.9336290061473846\n",
      "Epoch 26, Training Loss: 0.9326205736749312\n",
      "Epoch 27, Training Loss: 0.9316198894556831\n",
      "Epoch 28, Training Loss: 0.9306333271194906\n",
      "Epoch 29, Training Loss: 0.9296736294381759\n",
      "Epoch 30, Training Loss: 0.9287327765015995\n",
      "Epoch 31, Training Loss: 0.9278193728362812\n",
      "Epoch 32, Training Loss: 0.9269075204344357\n",
      "Epoch 33, Training Loss: 0.9260139899394092\n",
      "Epoch 34, Training Loss: 0.925126777676975\n",
      "Epoch 35, Training Loss: 0.9242429920505075\n",
      "Epoch 36, Training Loss: 0.9233616308604969\n",
      "Epoch 37, Training Loss: 0.9224932818553027\n",
      "Epoch 38, Training Loss: 0.921623015053132\n",
      "Epoch 39, Training Loss: 0.9207468304914587\n",
      "Epoch 40, Training Loss: 0.9198849955727072\n",
      "Epoch 41, Training Loss: 0.919016713254592\n",
      "Epoch 42, Training Loss: 0.9181608449010288\n",
      "Epoch 43, Training Loss: 0.9173087938392863\n",
      "Epoch 44, Training Loss: 0.9164521806380328\n",
      "Epoch 45, Training Loss: 0.9156074961493997\n",
      "Epoch 46, Training Loss: 0.9147563044463887\n",
      "Epoch 47, Training Loss: 0.9139008451209348\n",
      "Epoch 48, Training Loss: 0.9130486665753758\n",
      "Epoch 49, Training Loss: 0.912190915556515\n",
      "Epoch 50, Training Loss: 0.911325948378619\n",
      "Epoch 51, Training Loss: 0.9104631409925573\n",
      "Epoch 52, Training Loss: 0.9095869773275712\n",
      "Epoch 53, Training Loss: 0.9087130817245035\n",
      "Epoch 54, Training Loss: 0.907822161842795\n",
      "Epoch 55, Training Loss: 0.9069236734334161\n",
      "Epoch 56, Training Loss: 0.9060201271141277\n",
      "Epoch 57, Training Loss: 0.9050982827298781\n",
      "Epoch 58, Training Loss: 0.904157316754846\n",
      "Epoch 59, Training Loss: 0.9032333681863897\n",
      "Epoch 60, Training Loss: 0.9022714177299949\n",
      "Epoch 61, Training Loss: 0.9012828806568595\n",
      "Epoch 62, Training Loss: 0.9002995038032532\n",
      "Epoch 63, Training Loss: 0.8992722319154178\n",
      "Epoch 64, Training Loss: 0.8982496678127962\n",
      "Epoch 65, Training Loss: 0.8971912518669577\n",
      "Epoch 66, Training Loss: 0.8961091024735395\n",
      "Epoch 67, Training Loss: 0.8950073633474462\n",
      "Epoch 68, Training Loss: 0.8938664937019348\n",
      "Epoch 69, Training Loss: 0.8927158722456764\n",
      "Epoch 70, Training Loss: 0.8915360712303835\n",
      "Epoch 71, Training Loss: 0.8903170093368081\n",
      "Epoch 72, Training Loss: 0.8890788984298706\n",
      "Epoch 73, Training Loss: 0.8878098705936881\n",
      "Epoch 74, Training Loss: 0.8865067395743201\n",
      "Epoch 75, Training Loss: 0.885158492046244\n",
      "Epoch 76, Training Loss: 0.8837844204902648\n",
      "Epoch 77, Training Loss: 0.8823918275272145\n",
      "Epoch 78, Training Loss: 0.8809281091129079\n",
      "Epoch 79, Training Loss: 0.8794689672133502\n",
      "Epoch 80, Training Loss: 0.8779398768088397\n",
      "Epoch 81, Training Loss: 0.8764046680927277\n",
      "Epoch 82, Training Loss: 0.874827896637075\n",
      "Epoch 83, Training Loss: 0.8732146995207842\n",
      "Epoch 84, Training Loss: 0.8715926351266748\n",
      "Epoch 85, Training Loss: 0.8698962224231047\n",
      "Epoch 86, Training Loss: 0.8682144331230837\n",
      "Epoch 87, Training Loss: 0.8664702127260321\n",
      "Epoch 88, Training Loss: 0.8647125348624061\n",
      "Epoch 89, Training Loss: 0.8629458075411179\n",
      "Epoch 90, Training Loss: 0.86116722927374\n",
      "Epoch 91, Training Loss: 0.8593453218656427\n",
      "Epoch 92, Training Loss: 0.8575407814278322\n",
      "Epoch 93, Training Loss: 0.855711313976961\n",
      "Epoch 94, Training Loss: 0.853896141613231\n",
      "Epoch 95, Training Loss: 0.8520388762389912\n",
      "Epoch 96, Training Loss: 0.8502123926667606\n",
      "Epoch 97, Training Loss: 0.8483626659477458\n",
      "Epoch 98, Training Loss: 0.8465998005867005\n",
      "Epoch 99, Training Loss: 0.8447974226054024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:12:31,035] Trial 110 finished with value: 0.6055333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8429759177039652\n",
      "Epoch 1, Training Loss: 0.8481653975364857\n",
      "Epoch 2, Training Loss: 0.8123580728258405\n",
      "Epoch 3, Training Loss: 0.8093321573465391\n",
      "Epoch 4, Training Loss: 0.8074171450801362\n",
      "Epoch 5, Training Loss: 0.8058586026492872\n",
      "Epoch 6, Training Loss: 0.8037736151451454\n",
      "Epoch 7, Training Loss: 0.801196176934063\n",
      "Epoch 8, Training Loss: 0.8033275501172346\n",
      "Epoch 9, Training Loss: 0.8017190028850297\n",
      "Epoch 10, Training Loss: 0.8015173819728364\n",
      "Epoch 11, Training Loss: 0.8000304346694086\n",
      "Epoch 12, Training Loss: 0.7997098969337636\n",
      "Epoch 13, Training Loss: 0.7996791095661938\n",
      "Epoch 14, Training Loss: 0.7991729020176077\n",
      "Epoch 15, Training Loss: 0.7972400024421233\n",
      "Epoch 16, Training Loss: 0.7982966913316483\n",
      "Epoch 17, Training Loss: 0.7969443236974846\n",
      "Epoch 18, Training Loss: 0.797595369098778\n",
      "Epoch 19, Training Loss: 0.7956350360597882\n",
      "Epoch 20, Training Loss: 0.795285227244958\n",
      "Epoch 21, Training Loss: 0.7966291493042967\n",
      "Epoch 22, Training Loss: 0.7944727248715279\n",
      "Epoch 23, Training Loss: 0.7950750557999862\n",
      "Epoch 24, Training Loss: 0.7943682191963483\n",
      "Epoch 25, Training Loss: 0.7955947537171213\n",
      "Epoch 26, Training Loss: 0.7936801756234994\n",
      "Epoch 27, Training Loss: 0.7934432231393972\n",
      "Epoch 28, Training Loss: 0.7932039213359804\n",
      "Epoch 29, Training Loss: 0.7929604659403177\n",
      "Epoch 30, Training Loss: 0.7938154797804983\n",
      "Epoch 31, Training Loss: 0.794193456764508\n",
      "Epoch 32, Training Loss: 0.7931642283174328\n",
      "Epoch 33, Training Loss: 0.7934593182757385\n",
      "Epoch 34, Training Loss: 0.7921936225174064\n",
      "Epoch 35, Training Loss: 0.7938511157394352\n",
      "Epoch 36, Training Loss: 0.7920373654903325\n",
      "Epoch 37, Training Loss: 0.7925504357294929\n",
      "Epoch 38, Training Loss: 0.7933119254004687\n",
      "Epoch 39, Training Loss: 0.7935840744721262\n",
      "Epoch 40, Training Loss: 0.7916302412972415\n",
      "Epoch 41, Training Loss: 0.7919102632013478\n",
      "Epoch 42, Training Loss: 0.792265741672731\n",
      "Epoch 43, Training Loss: 0.7922589815648875\n",
      "Epoch 44, Training Loss: 0.7920408606529236\n",
      "Epoch 45, Training Loss: 0.791645848571806\n",
      "Epoch 46, Training Loss: 0.792110952936617\n",
      "Epoch 47, Training Loss: 0.7935828767324749\n",
      "Epoch 48, Training Loss: 0.7930658274127128\n",
      "Epoch 49, Training Loss: 0.7924311496261367\n",
      "Epoch 50, Training Loss: 0.7912772244080565\n",
      "Epoch 51, Training Loss: 0.7912186629790112\n",
      "Epoch 52, Training Loss: 0.7921721532828826\n",
      "Epoch 53, Training Loss: 0.791623552401263\n",
      "Epoch 54, Training Loss: 0.7921201467514039\n",
      "Epoch 55, Training Loss: 0.7921726433854354\n",
      "Epoch 56, Training Loss: 0.7922535155052529\n",
      "Epoch 57, Training Loss: 0.791156820605572\n",
      "Epoch 58, Training Loss: 0.7914364441893155\n",
      "Epoch 59, Training Loss: 0.7927635651782043\n",
      "Epoch 60, Training Loss: 0.7905278679123499\n",
      "Epoch 61, Training Loss: 0.7910970330238343\n",
      "Epoch 62, Training Loss: 0.7910832634545807\n",
      "Epoch 63, Training Loss: 0.79183271540735\n",
      "Epoch 64, Training Loss: 0.7910639775426764\n",
      "Epoch 65, Training Loss: 0.7914275896280332\n",
      "Epoch 66, Training Loss: 0.790510895019187\n",
      "Epoch 67, Training Loss: 0.7906469244257848\n",
      "Epoch 68, Training Loss: 0.7912873220622988\n",
      "Epoch 69, Training Loss: 0.7910924189969113\n",
      "Epoch 70, Training Loss: 0.7914703800265951\n",
      "Epoch 71, Training Loss: 0.7908286970360835\n",
      "Epoch 72, Training Loss: 0.7908879111583967\n",
      "Epoch 73, Training Loss: 0.7909733323226298\n",
      "Epoch 74, Training Loss: 0.7912955843416372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:13:44,896] Trial 111 finished with value: 0.6372 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7915938844358115\n",
      "Epoch 1, Training Loss: 0.9855082075736102\n",
      "Epoch 2, Training Loss: 0.9326536370024961\n",
      "Epoch 3, Training Loss: 0.8855376328440273\n",
      "Epoch 4, Training Loss: 0.8351541281447691\n",
      "Epoch 5, Training Loss: 0.8172802673367893\n",
      "Epoch 6, Training Loss: 0.813144568976234\n",
      "Epoch 7, Training Loss: 0.810747527515187\n",
      "Epoch 8, Training Loss: 0.8099197217296151\n",
      "Epoch 9, Training Loss: 0.8081289729651283\n",
      "Epoch 10, Training Loss: 0.8077020420747645\n",
      "Epoch 11, Training Loss: 0.8057927554495194\n",
      "Epoch 12, Training Loss: 0.8057545004872715\n",
      "Epoch 13, Training Loss: 0.8056491149173063\n",
      "Epoch 14, Training Loss: 0.8049392944924971\n",
      "Epoch 15, Training Loss: 0.8041272396901075\n",
      "Epoch 16, Training Loss: 0.8032935807284187\n",
      "Epoch 17, Training Loss: 0.8028278531046474\n",
      "Epoch 18, Training Loss: 0.8034564481763279\n",
      "Epoch 19, Training Loss: 0.8026528470656451\n",
      "Epoch 20, Training Loss: 0.8022991055600783\n",
      "Epoch 21, Training Loss: 0.8022932030172909\n",
      "Epoch 22, Training Loss: 0.801961141053368\n",
      "Epoch 23, Training Loss: 0.8016431703287012\n",
      "Epoch 24, Training Loss: 0.8013999273496516\n",
      "Epoch 25, Training Loss: 0.8011878773745369\n",
      "Epoch 26, Training Loss: 0.8012559664950651\n",
      "Epoch 27, Training Loss: 0.8006204040611491\n",
      "Epoch 28, Training Loss: 0.8008872513911304\n",
      "Epoch 29, Training Loss: 0.8005003490167506\n",
      "Epoch 30, Training Loss: 0.799832123447867\n",
      "Epoch 31, Training Loss: 0.800488862360225\n",
      "Epoch 32, Training Loss: 0.7997719552937675\n",
      "Epoch 33, Training Loss: 0.7994448784519644\n",
      "Epoch 34, Training Loss: 0.799317182933583\n",
      "Epoch 35, Training Loss: 0.8000682993496165\n",
      "Epoch 36, Training Loss: 0.7987285184158999\n",
      "Epoch 37, Training Loss: 0.7990933303272023\n",
      "Epoch 38, Training Loss: 0.7989048775504617\n",
      "Epoch 39, Training Loss: 0.7983527126732994\n",
      "Epoch 40, Training Loss: 0.7983553207621855\n",
      "Epoch 41, Training Loss: 0.7986846905596116\n",
      "Epoch 42, Training Loss: 0.7986489478980794\n",
      "Epoch 43, Training Loss: 0.7982113634838778\n",
      "Epoch 44, Training Loss: 0.7976138540576486\n",
      "Epoch 45, Training Loss: 0.7977092272393844\n",
      "Epoch 46, Training Loss: 0.7975375563256881\n",
      "Epoch 47, Training Loss: 0.7973472040541032\n",
      "Epoch 48, Training Loss: 0.7976665585181293\n",
      "Epoch 49, Training Loss: 0.7977839052677155\n",
      "Epoch 50, Training Loss: 0.7969525263589972\n",
      "Epoch 51, Training Loss: 0.7972869272793041\n",
      "Epoch 52, Training Loss: 0.7967705500827116\n",
      "Epoch 53, Training Loss: 0.7971905113669002\n",
      "Epoch 54, Training Loss: 0.796850658865536\n",
      "Epoch 55, Training Loss: 0.7968425192552454\n",
      "Epoch 56, Training Loss: 0.796563989204519\n",
      "Epoch 57, Training Loss: 0.79656727531377\n",
      "Epoch 58, Training Loss: 0.7965198348550235\n",
      "Epoch 59, Training Loss: 0.7962056856996873\n",
      "Epoch 60, Training Loss: 0.7963160591966966\n",
      "Epoch 61, Training Loss: 0.7961062571581672\n",
      "Epoch 62, Training Loss: 0.7960779602387372\n",
      "Epoch 63, Training Loss: 0.7959606683955474\n",
      "Epoch 64, Training Loss: 0.7958550511388218\n",
      "Epoch 65, Training Loss: 0.7956680495598737\n",
      "Epoch 66, Training Loss: 0.7953098420535817\n",
      "Epoch 67, Training Loss: 0.7955839908824247\n",
      "Epoch 68, Training Loss: 0.7954013360949124\n",
      "Epoch 69, Training Loss: 0.7954127353079179\n",
      "Epoch 70, Training Loss: 0.7952828038440031\n",
      "Epoch 71, Training Loss: 0.7952203926619361\n",
      "Epoch 72, Training Loss: 0.7944659333369312\n",
      "Epoch 73, Training Loss: 0.7945747834093431\n",
      "Epoch 74, Training Loss: 0.7943652921564439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:15:25,503] Trial 112 finished with value: 0.636 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7947540914311129\n",
      "Epoch 1, Training Loss: 0.8509699140276228\n",
      "Epoch 2, Training Loss: 0.8194570126390098\n",
      "Epoch 3, Training Loss: 0.8151436941067975\n",
      "Epoch 4, Training Loss: 0.812246240798692\n",
      "Epoch 5, Training Loss: 0.8106970755677474\n",
      "Epoch 6, Training Loss: 0.8098130097066549\n",
      "Epoch 7, Training Loss: 0.8089346684011302\n",
      "Epoch 8, Training Loss: 0.8077685366895863\n",
      "Epoch 9, Training Loss: 0.8081674295260494\n",
      "Epoch 10, Training Loss: 0.8071205304081278\n",
      "Epoch 11, Training Loss: 0.8091611543992409\n",
      "Epoch 12, Training Loss: 0.8064155250563657\n",
      "Epoch 13, Training Loss: 0.80588085848586\n",
      "Epoch 14, Training Loss: 0.8061600414433874\n",
      "Epoch 15, Training Loss: 0.8061409740519703\n",
      "Epoch 16, Training Loss: 0.8064575111955629\n",
      "Epoch 17, Training Loss: 0.8050486347729102\n",
      "Epoch 18, Training Loss: 0.8046231907113154\n",
      "Epoch 19, Training Loss: 0.8053323472352852\n",
      "Epoch 20, Training Loss: 0.8055843439317287\n",
      "Epoch 21, Training Loss: 0.8048628315889745\n",
      "Epoch 22, Training Loss: 0.8036637179833606\n",
      "Epoch 23, Training Loss: 0.8038189937297563\n",
      "Epoch 24, Training Loss: 0.803633189111724\n",
      "Epoch 25, Training Loss: 0.8036199150228859\n",
      "Epoch 26, Training Loss: 0.8032243007107784\n",
      "Epoch 27, Training Loss: 0.8038983019671045\n",
      "Epoch 28, Training Loss: 0.803215671930098\n",
      "Epoch 29, Training Loss: 0.8030571040354277\n",
      "Epoch 30, Training Loss: 0.8037119266682101\n",
      "Epoch 31, Training Loss: 0.8038554785843183\n",
      "Epoch 32, Training Loss: 0.8033692304353068\n",
      "Epoch 33, Training Loss: 0.8016178457360519\n",
      "Epoch 34, Training Loss: 0.8018586870422937\n",
      "Epoch 35, Training Loss: 0.8020901596635804\n",
      "Epoch 36, Training Loss: 0.8026242270505518\n",
      "Epoch 37, Training Loss: 0.8027101666407478\n",
      "Epoch 38, Training Loss: 0.801295108544199\n",
      "Epoch 39, Training Loss: 0.8015479045702999\n",
      "Epoch 40, Training Loss: 0.8018743835893789\n",
      "Epoch 41, Training Loss: 0.801965842659312\n",
      "Epoch 42, Training Loss: 0.8013571030215213\n",
      "Epoch 43, Training Loss: 0.8012923764106923\n",
      "Epoch 44, Training Loss: 0.8006958153014793\n",
      "Epoch 45, Training Loss: 0.8010344290195551\n",
      "Epoch 46, Training Loss: 0.8006297812426001\n",
      "Epoch 47, Training Loss: 0.800837692999302\n",
      "Epoch 48, Training Loss: 0.8005883203413253\n",
      "Epoch 49, Training Loss: 0.8008307733033833\n",
      "Epoch 50, Training Loss: 0.8012549145777422\n",
      "Epoch 51, Training Loss: 0.8010553568825686\n",
      "Epoch 52, Training Loss: 0.7992735957740841\n",
      "Epoch 53, Training Loss: 0.7996048639591475\n",
      "Epoch 54, Training Loss: 0.800435535531295\n",
      "Epoch 55, Training Loss: 0.7988671827137022\n",
      "Epoch 56, Training Loss: 0.801287397406155\n",
      "Epoch 57, Training Loss: 0.7990372709761885\n",
      "Epoch 58, Training Loss: 0.8014826585475664\n",
      "Epoch 59, Training Loss: 0.7987864656555922\n",
      "Epoch 60, Training Loss: 0.7995908558816838\n",
      "Epoch 61, Training Loss: 0.798525299344744\n",
      "Epoch 62, Training Loss: 0.7996415972709656\n",
      "Epoch 63, Training Loss: 0.7982873006870872\n",
      "Epoch 64, Training Loss: 0.7980977357778334\n",
      "Epoch 65, Training Loss: 0.798206182978207\n",
      "Epoch 66, Training Loss: 0.7987396398881325\n",
      "Epoch 67, Training Loss: 0.7978248787105532\n",
      "Epoch 68, Training Loss: 0.7976886167562097\n",
      "Epoch 69, Training Loss: 0.797275953319736\n",
      "Epoch 70, Training Loss: 0.7967361617357211\n",
      "Epoch 71, Training Loss: 0.7984754011147004\n",
      "Epoch 72, Training Loss: 0.7978017880504292\n",
      "Epoch 73, Training Loss: 0.7976253977395538\n",
      "Epoch 74, Training Loss: 0.7975363174775489\n",
      "Epoch 75, Training Loss: 0.79727894300805\n",
      "Epoch 76, Training Loss: 0.7971477751445053\n",
      "Epoch 77, Training Loss: 0.7975741440192201\n",
      "Epoch 78, Training Loss: 0.7960756629929506\n",
      "Epoch 79, Training Loss: 0.7970309444836208\n",
      "Epoch 80, Training Loss: 0.7970920362866911\n",
      "Epoch 81, Training Loss: 0.7974197509593534\n",
      "Epoch 82, Training Loss: 0.7956218267741956\n",
      "Epoch 83, Training Loss: 0.7959004344796776\n",
      "Epoch 84, Training Loss: 0.7951836213133389\n",
      "Epoch 85, Training Loss: 0.796581654978874\n",
      "Epoch 86, Training Loss: 0.7965981109698016\n",
      "Epoch 87, Training Loss: 0.7959791789377543\n",
      "Epoch 88, Training Loss: 0.7966589159535286\n",
      "Epoch 89, Training Loss: 0.7966307031480889\n",
      "Epoch 90, Training Loss: 0.7954250201246792\n",
      "Epoch 91, Training Loss: 0.7950178539842592\n",
      "Epoch 92, Training Loss: 0.7952280907702626\n",
      "Epoch 93, Training Loss: 0.7954032166559893\n",
      "Epoch 94, Training Loss: 0.7952863954063645\n",
      "Epoch 95, Training Loss: 0.7939898472083242\n",
      "Epoch 96, Training Loss: 0.7956891412125494\n",
      "Epoch 97, Training Loss: 0.7955762000012219\n",
      "Epoch 98, Training Loss: 0.795227541152696\n",
      "Epoch 99, Training Loss: 0.7953651402229653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:17:04,738] Trial 113 finished with value: 0.6386666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7941767048118706\n",
      "Epoch 1, Training Loss: 0.9807961269428855\n",
      "Epoch 2, Training Loss: 0.9477275671815514\n",
      "Epoch 3, Training Loss: 0.9344996871804833\n",
      "Epoch 4, Training Loss: 0.9178823593863867\n",
      "Epoch 5, Training Loss: 0.8998306087085179\n",
      "Epoch 6, Training Loss: 0.881187665641756\n",
      "Epoch 7, Training Loss: 0.8631318603243147\n",
      "Epoch 8, Training Loss: 0.8477001639237082\n",
      "Epoch 9, Training Loss: 0.8362500093933335\n",
      "Epoch 10, Training Loss: 0.8274428278880012\n",
      "Epoch 11, Training Loss: 0.8219936901465394\n",
      "Epoch 12, Training Loss: 0.8171000063419342\n",
      "Epoch 13, Training Loss: 0.8148845930744831\n",
      "Epoch 14, Training Loss: 0.812797721077625\n",
      "Epoch 15, Training Loss: 0.8120373732165287\n",
      "Epoch 16, Training Loss: 0.810314108554582\n",
      "Epoch 17, Training Loss: 0.809698564396765\n",
      "Epoch 18, Training Loss: 0.8092253817651505\n",
      "Epoch 19, Training Loss: 0.8089392809939564\n",
      "Epoch 20, Training Loss: 0.8087609160215334\n",
      "Epoch 21, Training Loss: 0.8080598531809068\n",
      "Epoch 22, Training Loss: 0.8085669230697746\n",
      "Epoch 23, Training Loss: 0.8079642534255982\n",
      "Epoch 24, Training Loss: 0.8076880014032349\n",
      "Epoch 25, Training Loss: 0.807705277159698\n",
      "Epoch 26, Training Loss: 0.8074843431773938\n",
      "Epoch 27, Training Loss: 0.8069955610691156\n",
      "Epoch 28, Training Loss: 0.8070902220288614\n",
      "Epoch 29, Training Loss: 0.8069656035953895\n",
      "Epoch 30, Training Loss: 0.8067428124578375\n",
      "Epoch 31, Training Loss: 0.8065300061290426\n",
      "Epoch 32, Training Loss: 0.8064230858831477\n",
      "Epoch 33, Training Loss: 0.807119896089224\n",
      "Epoch 34, Training Loss: 0.8066677842821394\n",
      "Epoch 35, Training Loss: 0.806311955846342\n",
      "Epoch 36, Training Loss: 0.8058218575061712\n",
      "Epoch 37, Training Loss: 0.8056931145209119\n",
      "Epoch 38, Training Loss: 0.8060523169381278\n",
      "Epoch 39, Training Loss: 0.8054164824629189\n",
      "Epoch 40, Training Loss: 0.8054549257558091\n",
      "Epoch 41, Training Loss: 0.8051378453584542\n",
      "Epoch 42, Training Loss: 0.805101948483546\n",
      "Epoch 43, Training Loss: 0.8050914032118661\n",
      "Epoch 44, Training Loss: 0.8051142600245942\n",
      "Epoch 45, Training Loss: 0.8047011188098363\n",
      "Epoch 46, Training Loss: 0.804679242650369\n",
      "Epoch 47, Training Loss: 0.8049727749107476\n",
      "Epoch 48, Training Loss: 0.804868751809113\n",
      "Epoch 49, Training Loss: 0.8047111063971555\n",
      "Epoch 50, Training Loss: 0.8049773082697302\n",
      "Epoch 51, Training Loss: 0.8046846579788323\n",
      "Epoch 52, Training Loss: 0.8044527086100184\n",
      "Epoch 53, Training Loss: 0.8039930895755165\n",
      "Epoch 54, Training Loss: 0.8033027784268659\n",
      "Epoch 55, Training Loss: 0.8036182313933409\n",
      "Epoch 56, Training Loss: 0.8036448071773787\n",
      "Epoch 57, Training Loss: 0.8031823726524984\n",
      "Epoch 58, Training Loss: 0.8034176466160251\n",
      "Epoch 59, Training Loss: 0.8030126616470796\n",
      "Epoch 60, Training Loss: 0.8030412279573599\n",
      "Epoch 61, Training Loss: 0.8028475944260905\n",
      "Epoch 62, Training Loss: 0.8029143747530485\n",
      "Epoch 63, Training Loss: 0.8029531507563771\n",
      "Epoch 64, Training Loss: 0.8026855957239194\n",
      "Epoch 65, Training Loss: 0.8026750501833464\n",
      "Epoch 66, Training Loss: 0.8020731744013334\n",
      "Epoch 67, Training Loss: 0.8017884817338528\n",
      "Epoch 68, Training Loss: 0.8029454931280666\n",
      "Epoch 69, Training Loss: 0.8022379692335774\n",
      "Epoch 70, Training Loss: 0.8026038281899646\n",
      "Epoch 71, Training Loss: 0.8019271213309209\n",
      "Epoch 72, Training Loss: 0.8018854130479626\n",
      "Epoch 73, Training Loss: 0.8022463526044573\n",
      "Epoch 74, Training Loss: 0.8019007501745583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:18:15,768] Trial 114 finished with value: 0.6319333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8017458130542497\n",
      "Epoch 1, Training Loss: 0.8998239868528702\n",
      "Epoch 2, Training Loss: 0.8278544693133411\n",
      "Epoch 3, Training Loss: 0.8209592735066134\n",
      "Epoch 4, Training Loss: 0.81479852865724\n",
      "Epoch 5, Training Loss: 0.8124020690777722\n",
      "Epoch 6, Training Loss: 0.8117674144576578\n",
      "Epoch 7, Training Loss: 0.8080572178784539\n",
      "Epoch 8, Training Loss: 0.8088316869735718\n",
      "Epoch 9, Training Loss: 0.8084915891815634\n",
      "Epoch 10, Training Loss: 0.8067488291684319\n",
      "Epoch 11, Training Loss: 0.805456746746512\n",
      "Epoch 12, Training Loss: 0.8060002386569977\n",
      "Epoch 13, Training Loss: 0.8053503678826724\n",
      "Epoch 14, Training Loss: 0.8052678043000838\n",
      "Epoch 15, Training Loss: 0.8052242377926322\n",
      "Epoch 16, Training Loss: 0.80343977914137\n",
      "Epoch 17, Training Loss: 0.8030714029424331\n",
      "Epoch 18, Training Loss: 0.8044896456774543\n",
      "Epoch 19, Training Loss: 0.8035474716915804\n",
      "Epoch 20, Training Loss: 0.802215585638495\n",
      "Epoch 21, Training Loss: 0.8024238422337701\n",
      "Epoch 22, Training Loss: 0.8021879869348862\n",
      "Epoch 23, Training Loss: 0.8021749924912173\n",
      "Epoch 24, Training Loss: 0.8015426866447224\n",
      "Epoch 25, Training Loss: 0.8015077032061184\n",
      "Epoch 26, Training Loss: 0.8025704080918256\n",
      "Epoch 27, Training Loss: 0.801430562804727\n",
      "Epoch 28, Training Loss: 0.8014146586025462\n",
      "Epoch 29, Training Loss: 0.8012463672722088\n",
      "Epoch 30, Training Loss: 0.8011822699098026\n",
      "Epoch 31, Training Loss: 0.8008611878928016\n",
      "Epoch 32, Training Loss: 0.8008136987686157\n",
      "Epoch 33, Training Loss: 0.8014413475289064\n",
      "Epoch 34, Training Loss: 0.8006594484693864\n",
      "Epoch 35, Training Loss: 0.8008912567531361\n",
      "Epoch 36, Training Loss: 0.8003361622024985\n",
      "Epoch 37, Training Loss: 0.8004659621855792\n",
      "Epoch 38, Training Loss: 0.8001909835198346\n",
      "Epoch 39, Training Loss: 0.8004541051387787\n",
      "Epoch 40, Training Loss: 0.7996870180438547\n",
      "Epoch 41, Training Loss: 0.8009448662926169\n",
      "Epoch 42, Training Loss: 0.7998342480378993\n",
      "Epoch 43, Training Loss: 0.7991663279954125\n",
      "Epoch 44, Training Loss: 0.8002874030786402\n",
      "Epoch 45, Training Loss: 0.800542689631967\n",
      "Epoch 46, Training Loss: 0.7999861241789425\n",
      "Epoch 47, Training Loss: 0.8000331582041348\n",
      "Epoch 48, Training Loss: 0.7997340595722199\n",
      "Epoch 49, Training Loss: 0.7998873005193823\n",
      "Epoch 50, Training Loss: 0.8000710810633267\n",
      "Epoch 51, Training Loss: 0.7987171356117024\n",
      "Epoch 52, Training Loss: 0.799680919086232\n",
      "Epoch 53, Training Loss: 0.7997651006894954\n",
      "Epoch 54, Training Loss: 0.7997803251182332\n",
      "Epoch 55, Training Loss: 0.7987253083902247\n",
      "Epoch 56, Training Loss: 0.7995433208521675\n",
      "Epoch 57, Training Loss: 0.7997082356845632\n",
      "Epoch 58, Training Loss: 0.7993898532671087\n",
      "Epoch 59, Training Loss: 0.7992516841607935\n",
      "Epoch 60, Training Loss: 0.7998871208639706\n",
      "Epoch 61, Training Loss: 0.7996931390902575\n",
      "Epoch 62, Training Loss: 0.800477091214236\n",
      "Epoch 63, Training Loss: 0.7994620732700124\n",
      "Epoch 64, Training Loss: 0.7994025489863228\n",
      "Epoch 65, Training Loss: 0.7981396252968732\n",
      "Epoch 66, Training Loss: 0.7987594507021063\n",
      "Epoch 67, Training Loss: 0.7988678033912883\n",
      "Epoch 68, Training Loss: 0.7986973287077511\n",
      "Epoch 69, Training Loss: 0.7989912928553189\n",
      "Epoch 70, Training Loss: 0.7990836929573732\n",
      "Epoch 71, Training Loss: 0.7990988004207611\n",
      "Epoch 72, Training Loss: 0.7994479928297155\n",
      "Epoch 73, Training Loss: 0.7991943752765656\n",
      "Epoch 74, Training Loss: 0.7988333704892326\n",
      "Epoch 75, Training Loss: 0.7989372545130112\n",
      "Epoch 76, Training Loss: 0.7997441550563363\n",
      "Epoch 77, Training Loss: 0.7983276438011843\n",
      "Epoch 78, Training Loss: 0.7987962950678432\n",
      "Epoch 79, Training Loss: 0.7991688274635989\n",
      "Epoch 80, Training Loss: 0.7983091665716733\n",
      "Epoch 81, Training Loss: 0.7981570813936346\n",
      "Epoch 82, Training Loss: 0.79871978114633\n",
      "Epoch 83, Training Loss: 0.7989859314525829\n",
      "Epoch 84, Training Loss: 0.7993958051064435\n",
      "Epoch 85, Training Loss: 0.7991575581185958\n",
      "Epoch 86, Training Loss: 0.7993003592070411\n",
      "Epoch 87, Training Loss: 0.7998394184252795\n",
      "Epoch 88, Training Loss: 0.7992143207437852\n",
      "Epoch 89, Training Loss: 0.7986623424642226\n",
      "Epoch 90, Training Loss: 0.7983494207438301\n",
      "Epoch 91, Training Loss: 0.7985370333755718\n",
      "Epoch 92, Training Loss: 0.7989705714057473\n",
      "Epoch 93, Training Loss: 0.7983398175940795\n",
      "Epoch 94, Training Loss: 0.7983933241227094\n",
      "Epoch 95, Training Loss: 0.798508317751043\n",
      "Epoch 96, Training Loss: 0.7994054743121652\n",
      "Epoch 97, Training Loss: 0.7989537792346056\n",
      "Epoch 98, Training Loss: 0.7986500512151157\n",
      "Epoch 99, Training Loss: 0.7979360335013446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:20:24,521] Trial 115 finished with value: 0.6380666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7987338869010701\n",
      "Epoch 1, Training Loss: 0.9075709766911385\n",
      "Epoch 2, Training Loss: 0.8273511474293874\n",
      "Epoch 3, Training Loss: 0.8179851792808762\n",
      "Epoch 4, Training Loss: 0.8145059776037259\n",
      "Epoch 5, Training Loss: 0.8138096371091398\n",
      "Epoch 6, Training Loss: 0.8123191360244177\n",
      "Epoch 7, Training Loss: 0.8105781645703136\n",
      "Epoch 8, Training Loss: 0.8096362165938643\n",
      "Epoch 9, Training Loss: 0.8087220772764736\n",
      "Epoch 10, Training Loss: 0.8070617620210002\n",
      "Epoch 11, Training Loss: 0.8066949127311993\n",
      "Epoch 12, Training Loss: 0.806165049129859\n",
      "Epoch 13, Training Loss: 0.805467414766326\n",
      "Epoch 14, Training Loss: 0.8056629913193839\n",
      "Epoch 15, Training Loss: 0.8057095408439636\n",
      "Epoch 16, Training Loss: 0.8056632633496048\n",
      "Epoch 17, Training Loss: 0.8038848868438175\n",
      "Epoch 18, Training Loss: 0.8040071557338972\n",
      "Epoch 19, Training Loss: 0.8038385818775435\n",
      "Epoch 20, Training Loss: 0.8040724272118476\n",
      "Epoch 21, Training Loss: 0.8031075490148444\n",
      "Epoch 22, Training Loss: 0.8033172442500752\n",
      "Epoch 23, Training Loss: 0.8028877868688196\n",
      "Epoch 24, Training Loss: 0.8035835720542678\n",
      "Epoch 25, Training Loss: 0.8022759360478336\n",
      "Epoch 26, Training Loss: 0.8028129662786211\n",
      "Epoch 27, Training Loss: 0.8032027368258713\n",
      "Epoch 28, Training Loss: 0.8023787490407327\n",
      "Epoch 29, Training Loss: 0.8022581533381813\n",
      "Epoch 30, Training Loss: 0.8014845528996977\n",
      "Epoch 31, Training Loss: 0.8015353428690057\n",
      "Epoch 32, Training Loss: 0.8015067578258371\n",
      "Epoch 33, Training Loss: 0.8012760257362423\n",
      "Epoch 34, Training Loss: 0.8013585951095237\n",
      "Epoch 35, Training Loss: 0.8008378269080829\n",
      "Epoch 36, Training Loss: 0.8012344925027145\n",
      "Epoch 37, Training Loss: 0.8010476478060385\n",
      "Epoch 38, Training Loss: 0.8004510406264685\n",
      "Epoch 39, Training Loss: 0.8001192883441323\n",
      "Epoch 40, Training Loss: 0.800032673384014\n",
      "Epoch 41, Training Loss: 0.7997419017598145\n",
      "Epoch 42, Training Loss: 0.8006075566872618\n",
      "Epoch 43, Training Loss: 0.7997313068325358\n",
      "Epoch 44, Training Loss: 0.7994812537852983\n",
      "Epoch 45, Training Loss: 0.7996048892351021\n",
      "Epoch 46, Training Loss: 0.7993248062922542\n",
      "Epoch 47, Training Loss: 0.7991399496121514\n",
      "Epoch 48, Training Loss: 0.799431366131718\n",
      "Epoch 49, Training Loss: 0.7990965053551179\n",
      "Epoch 50, Training Loss: 0.799602648996769\n",
      "Epoch 51, Training Loss: 0.7979340198344754\n",
      "Epoch 52, Training Loss: 0.7983442079752011\n",
      "Epoch 53, Training Loss: 0.7983466185125193\n",
      "Epoch 54, Training Loss: 0.7984786861821225\n",
      "Epoch 55, Training Loss: 0.7983334758227929\n",
      "Epoch 56, Training Loss: 0.7979056096614752\n",
      "Epoch 57, Training Loss: 0.7979218045571693\n",
      "Epoch 58, Training Loss: 0.7975238184283551\n",
      "Epoch 59, Training Loss: 0.7975765883474422\n",
      "Epoch 60, Training Loss: 0.7971893038068499\n",
      "Epoch 61, Training Loss: 0.7975063125890001\n",
      "Epoch 62, Training Loss: 0.7969610812968777\n",
      "Epoch 63, Training Loss: 0.7964244979664795\n",
      "Epoch 64, Training Loss: 0.796545974562939\n",
      "Epoch 65, Training Loss: 0.7968898690732799\n",
      "Epoch 66, Training Loss: 0.7965827885426973\n",
      "Epoch 67, Training Loss: 0.7963457129951707\n",
      "Epoch 68, Training Loss: 0.7961067628143426\n",
      "Epoch 69, Training Loss: 0.7956322076625394\n",
      "Epoch 70, Training Loss: 0.7962449475338584\n",
      "Epoch 71, Training Loss: 0.7962370846504555\n",
      "Epoch 72, Training Loss: 0.795401702608381\n",
      "Epoch 73, Training Loss: 0.7957451494116532\n",
      "Epoch 74, Training Loss: 0.7953335257401144\n",
      "Epoch 75, Training Loss: 0.7954656266628352\n",
      "Epoch 76, Training Loss: 0.7957780419435716\n",
      "Epoch 77, Training Loss: 0.7954423995842611\n",
      "Epoch 78, Training Loss: 0.7948099042239942\n",
      "Epoch 79, Training Loss: 0.7950030974875716\n",
      "Epoch 80, Training Loss: 0.7948688255216843\n",
      "Epoch 81, Training Loss: 0.795232175436235\n",
      "Epoch 82, Training Loss: 0.7946797038379468\n",
      "Epoch 83, Training Loss: 0.7944779422050132\n",
      "Epoch 84, Training Loss: 0.794132688753587\n",
      "Epoch 85, Training Loss: 0.7946087137200779\n",
      "Epoch 86, Training Loss: 0.7950937528359262\n",
      "Epoch 87, Training Loss: 0.7950898171367502\n",
      "Epoch 88, Training Loss: 0.7943441751308011\n",
      "Epoch 89, Training Loss: 0.7951628524557989\n",
      "Epoch 90, Training Loss: 0.7940721302104176\n",
      "Epoch 91, Training Loss: 0.7943544850313574\n",
      "Epoch 92, Training Loss: 0.7943939196436028\n",
      "Epoch 93, Training Loss: 0.7936941732141308\n",
      "Epoch 94, Training Loss: 0.7938563853278195\n",
      "Epoch 95, Training Loss: 0.7932640790939331\n",
      "Epoch 96, Training Loss: 0.793575706607417\n",
      "Epoch 97, Training Loss: 0.7936156359830298\n",
      "Epoch 98, Training Loss: 0.7929904578323651\n",
      "Epoch 99, Training Loss: 0.7938196401847036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:21:57,732] Trial 116 finished with value: 0.6229333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.793303056139695\n",
      "Epoch 1, Training Loss: 0.9734938326634859\n",
      "Epoch 2, Training Loss: 0.9438832300049919\n",
      "Epoch 3, Training Loss: 0.9248865905560946\n",
      "Epoch 4, Training Loss: 0.8981608392600726\n",
      "Epoch 5, Training Loss: 0.8649180736756863\n",
      "Epoch 6, Training Loss: 0.8391649228289612\n",
      "Epoch 7, Training Loss: 0.8246109777823427\n",
      "Epoch 8, Training Loss: 0.8170141591165299\n",
      "Epoch 9, Training Loss: 0.8138147490365165\n",
      "Epoch 10, Training Loss: 0.8114371887723306\n",
      "Epoch 11, Training Loss: 0.8096989996451184\n",
      "Epoch 12, Training Loss: 0.8093655883817744\n",
      "Epoch 13, Training Loss: 0.808826033871873\n",
      "Epoch 14, Training Loss: 0.8074761067117964\n",
      "Epoch 15, Training Loss: 0.80784204687391\n",
      "Epoch 16, Training Loss: 0.8056608260126042\n",
      "Epoch 17, Training Loss: 0.8048135191874397\n",
      "Epoch 18, Training Loss: 0.8059974873872627\n",
      "Epoch 19, Training Loss: 0.8035470969694898\n",
      "Epoch 20, Training Loss: 0.8029136714182402\n",
      "Epoch 21, Training Loss: 0.8024084276722786\n",
      "Epoch 22, Training Loss: 0.8022251492156122\n",
      "Epoch 23, Training Loss: 0.8028458592586948\n",
      "Epoch 24, Training Loss: 0.8016198584011622\n",
      "Epoch 25, Training Loss: 0.800928633195117\n",
      "Epoch 26, Training Loss: 0.8006856298984442\n",
      "Epoch 27, Training Loss: 0.8008600318342223\n",
      "Epoch 28, Training Loss: 0.8002483640398298\n",
      "Epoch 29, Training Loss: 0.80027894211891\n",
      "Epoch 30, Training Loss: 0.799845081881473\n",
      "Epoch 31, Training Loss: 0.8002536364964077\n",
      "Epoch 32, Training Loss: 0.7994335378919329\n",
      "Epoch 33, Training Loss: 0.7993594881287195\n",
      "Epoch 34, Training Loss: 0.7992314572621109\n",
      "Epoch 35, Training Loss: 0.7986781771917989\n",
      "Epoch 36, Training Loss: 0.7984674144508247\n",
      "Epoch 37, Training Loss: 0.7998148857202745\n",
      "Epoch 38, Training Loss: 0.7992588191104114\n",
      "Epoch 39, Training Loss: 0.7987248228904896\n",
      "Epoch 40, Training Loss: 0.798490425787474\n",
      "Epoch 41, Training Loss: 0.7982098020109019\n",
      "Epoch 42, Training Loss: 0.7980266497547465\n",
      "Epoch 43, Training Loss: 0.7978486174927618\n",
      "Epoch 44, Training Loss: 0.798362367941921\n",
      "Epoch 45, Training Loss: 0.7987500655023675\n",
      "Epoch 46, Training Loss: 0.7987409935858016\n",
      "Epoch 47, Training Loss: 0.7972068002349452\n",
      "Epoch 48, Training Loss: 0.7980396270751953\n",
      "Epoch 49, Training Loss: 0.7976700849102852\n",
      "Epoch 50, Training Loss: 0.797677831631854\n",
      "Epoch 51, Training Loss: 0.7979283571243286\n",
      "Epoch 52, Training Loss: 0.7980477943456262\n",
      "Epoch 53, Training Loss: 0.7971482339658236\n",
      "Epoch 54, Training Loss: 0.7970680535287785\n",
      "Epoch 55, Training Loss: 0.796424791059996\n",
      "Epoch 56, Training Loss: 0.7974736015599473\n",
      "Epoch 57, Training Loss: 0.7960607548405353\n",
      "Epoch 58, Training Loss: 0.7963885222162519\n",
      "Epoch 59, Training Loss: 0.7966932529793647\n",
      "Epoch 60, Training Loss: 0.7966965853719783\n",
      "Epoch 61, Training Loss: 0.7963399391425283\n",
      "Epoch 62, Training Loss: 0.7969499446395645\n",
      "Epoch 63, Training Loss: 0.7964595231794773\n",
      "Epoch 64, Training Loss: 0.7962648551266892\n",
      "Epoch 65, Training Loss: 0.7962183502383698\n",
      "Epoch 66, Training Loss: 0.7960418656356353\n",
      "Epoch 67, Training Loss: 0.7956837499052062\n",
      "Epoch 68, Training Loss: 0.7957793223230463\n",
      "Epoch 69, Training Loss: 0.7954206043616273\n",
      "Epoch 70, Training Loss: 0.7952547966985775\n",
      "Epoch 71, Training Loss: 0.79538893699646\n",
      "Epoch 72, Training Loss: 0.7952269633909813\n",
      "Epoch 73, Training Loss: 0.7949463802172725\n",
      "Epoch 74, Training Loss: 0.7956043528434925\n",
      "Epoch 75, Training Loss: 0.7954557875045261\n",
      "Epoch 76, Training Loss: 0.7950807536455026\n",
      "Epoch 77, Training Loss: 0.7952041594605697\n",
      "Epoch 78, Training Loss: 0.7953026254374281\n",
      "Epoch 79, Training Loss: 0.7955837245274308\n",
      "Epoch 80, Training Loss: 0.7948178185556168\n",
      "Epoch 81, Training Loss: 0.794207850748435\n",
      "Epoch 82, Training Loss: 0.7950160114388717\n",
      "Epoch 83, Training Loss: 0.7949845714676649\n",
      "Epoch 84, Training Loss: 0.7944087629927729\n",
      "Epoch 85, Training Loss: 0.7951746853670679\n",
      "Epoch 86, Training Loss: 0.7944640426707447\n",
      "Epoch 87, Training Loss: 0.7951295555086064\n",
      "Epoch 88, Training Loss: 0.7939874416903445\n",
      "Epoch 89, Training Loss: 0.7941873357708292\n",
      "Epoch 90, Training Loss: 0.793874434510568\n",
      "Epoch 91, Training Loss: 0.7937594741806948\n",
      "Epoch 92, Training Loss: 0.7934746444673466\n",
      "Epoch 93, Training Loss: 0.7932865669852809\n",
      "Epoch 94, Training Loss: 0.7932615848412191\n",
      "Epoch 95, Training Loss: 0.7930674141510985\n",
      "Epoch 96, Training Loss: 0.7928227729367134\n",
      "Epoch 97, Training Loss: 0.7928262919411624\n",
      "Epoch 98, Training Loss: 0.793048173771765\n",
      "Epoch 99, Training Loss: 0.7923001869280536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:23:30,692] Trial 117 finished with value: 0.6328 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7923363478560197\n",
      "Epoch 1, Training Loss: 0.8474540826969577\n",
      "Epoch 2, Training Loss: 0.8226309356832863\n",
      "Epoch 3, Training Loss: 0.8165463953986204\n",
      "Epoch 4, Training Loss: 0.8147327329879417\n",
      "Epoch 5, Training Loss: 0.8118178732413098\n",
      "Epoch 6, Training Loss: 0.8119832573976732\n",
      "Epoch 7, Training Loss: 0.8115755331247373\n",
      "Epoch 8, Training Loss: 0.8099617544869732\n",
      "Epoch 9, Training Loss: 0.8103170879801413\n",
      "Epoch 10, Training Loss: 0.8095489613991931\n",
      "Epoch 11, Training Loss: 0.8105051921722585\n",
      "Epoch 12, Training Loss: 0.8104579023400643\n",
      "Epoch 13, Training Loss: 0.809659016759772\n",
      "Epoch 14, Training Loss: 0.8076654827684389\n",
      "Epoch 15, Training Loss: 0.809422102458495\n",
      "Epoch 16, Training Loss: 0.8086633865098308\n",
      "Epoch 17, Training Loss: 0.8096512271049328\n",
      "Epoch 18, Training Loss: 0.8089259650474204\n",
      "Epoch 19, Training Loss: 0.8109227925314939\n",
      "Epoch 20, Training Loss: 0.8088599528585162\n",
      "Epoch 21, Training Loss: 0.807063577228919\n",
      "Epoch 22, Training Loss: 0.8074374158579604\n",
      "Epoch 23, Training Loss: 0.8081004717296227\n",
      "Epoch 24, Training Loss: 0.8074598656561142\n",
      "Epoch 25, Training Loss: 0.8079213534082685\n",
      "Epoch 26, Training Loss: 0.8085086638766124\n",
      "Epoch 27, Training Loss: 0.8074692139051911\n",
      "Epoch 28, Training Loss: 0.805222359187621\n",
      "Epoch 29, Training Loss: 0.8066123224738845\n",
      "Epoch 30, Training Loss: 0.8059005693385476\n",
      "Epoch 31, Training Loss: 0.8053675537718866\n",
      "Epoch 32, Training Loss: 0.8038226353494744\n",
      "Epoch 33, Training Loss: 0.8044339390625631\n",
      "Epoch 34, Training Loss: 0.8053668125231463\n",
      "Epoch 35, Training Loss: 0.8040555945912699\n",
      "Epoch 36, Training Loss: 0.8040029127794998\n",
      "Epoch 37, Training Loss: 0.8021970489867648\n",
      "Epoch 38, Training Loss: 0.8062996720012866\n",
      "Epoch 39, Training Loss: 0.8025881601455517\n",
      "Epoch 40, Training Loss: 0.8022491857969671\n",
      "Epoch 41, Training Loss: 0.804158944384496\n",
      "Epoch 42, Training Loss: 0.800773256793058\n",
      "Epoch 43, Training Loss: 0.8030307681040656\n",
      "Epoch 44, Training Loss: 0.8020872320895804\n",
      "Epoch 45, Training Loss: 0.8030511691157979\n",
      "Epoch 46, Training Loss: 0.8008457886545282\n",
      "Epoch 47, Training Loss: 0.8004496448022083\n",
      "Epoch 48, Training Loss: 0.8021275148355871\n",
      "Epoch 49, Training Loss: 0.7999466568007505\n",
      "Epoch 50, Training Loss: 0.8012151144500962\n",
      "Epoch 51, Training Loss: 0.7996882746094152\n",
      "Epoch 52, Training Loss: 0.8006336866912985\n",
      "Epoch 53, Training Loss: 0.8016021460518801\n",
      "Epoch 54, Training Loss: 0.7990962046429627\n",
      "Epoch 55, Training Loss: 0.799626111222389\n",
      "Epoch 56, Training Loss: 0.7988303980432955\n",
      "Epoch 57, Training Loss: 0.799578889420158\n",
      "Epoch 58, Training Loss: 0.7996652727736566\n",
      "Epoch 59, Training Loss: 0.7991645773550622\n",
      "Epoch 60, Training Loss: 0.7993818870164398\n",
      "Epoch 61, Training Loss: 0.8006618100001399\n",
      "Epoch 62, Training Loss: 0.7988999135512158\n",
      "Epoch 63, Training Loss: 0.798589511383745\n",
      "Epoch 64, Training Loss: 0.7994568278018693\n",
      "Epoch 65, Training Loss: 0.7976348653771823\n",
      "Epoch 66, Training Loss: 0.7994847858758797\n",
      "Epoch 67, Training Loss: 0.8001717721609245\n",
      "Epoch 68, Training Loss: 0.7985301005212884\n",
      "Epoch 69, Training Loss: 0.7987636608288701\n",
      "Epoch 70, Training Loss: 0.7995215111657192\n",
      "Epoch 71, Training Loss: 0.7981798480327864\n",
      "Epoch 72, Training Loss: 0.7977533525094054\n",
      "Epoch 73, Training Loss: 0.7985410424999725\n",
      "Epoch 74, Training Loss: 0.7991461076234516\n",
      "Epoch 75, Training Loss: 0.7979929435522036\n",
      "Epoch 76, Training Loss: 0.7970061057492306\n",
      "Epoch 77, Training Loss: 0.7986182591968909\n",
      "Epoch 78, Training Loss: 0.7974450242250486\n",
      "Epoch 79, Training Loss: 0.7985686553151984\n",
      "Epoch 80, Training Loss: 0.7978805679127686\n",
      "Epoch 81, Training Loss: 0.7970577469445709\n",
      "Epoch 82, Training Loss: 0.7977196022980195\n",
      "Epoch 83, Training Loss: 0.7972054259221356\n",
      "Epoch 84, Training Loss: 0.7960253012807745\n",
      "Epoch 85, Training Loss: 0.7981291839054653\n",
      "Epoch 86, Training Loss: 0.7983163462545638\n",
      "Epoch 87, Training Loss: 0.7967204192527255\n",
      "Epoch 88, Training Loss: 0.7988381067613014\n",
      "Epoch 89, Training Loss: 0.7963411872100113\n",
      "Epoch 90, Training Loss: 0.7965225573769189\n",
      "Epoch 91, Training Loss: 0.7973798747349502\n",
      "Epoch 92, Training Loss: 0.7977012445155839\n",
      "Epoch 93, Training Loss: 0.7991785026134405\n",
      "Epoch 94, Training Loss: 0.7967484749349436\n",
      "Epoch 95, Training Loss: 0.7976678129425623\n",
      "Epoch 96, Training Loss: 0.7972252255991885\n",
      "Epoch 97, Training Loss: 0.7975075209051147\n",
      "Epoch 98, Training Loss: 0.7966509848609007\n",
      "Epoch 99, Training Loss: 0.7961538510663169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:25:08,747] Trial 118 finished with value: 0.6309333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7958392934691637\n",
      "Epoch 1, Training Loss: 0.8758837832544083\n",
      "Epoch 2, Training Loss: 0.817250746130047\n",
      "Epoch 3, Training Loss: 0.8129082703948918\n",
      "Epoch 4, Training Loss: 0.8122614886527671\n",
      "Epoch 5, Training Loss: 0.8091324389429021\n",
      "Epoch 6, Training Loss: 0.8095256826035062\n",
      "Epoch 7, Training Loss: 0.8067644071758242\n",
      "Epoch 8, Training Loss: 0.8085364726253021\n",
      "Epoch 9, Training Loss: 0.807832063678512\n",
      "Epoch 10, Training Loss: 0.8065176146371024\n",
      "Epoch 11, Training Loss: 0.8046800184966927\n",
      "Epoch 12, Training Loss: 0.8076807458597914\n",
      "Epoch 13, Training Loss: 0.8055326641950392\n",
      "Epoch 14, Training Loss: 0.8050063927370803\n",
      "Epoch 15, Training Loss: 0.8054919553878612\n",
      "Epoch 16, Training Loss: 0.803398641249291\n",
      "Epoch 17, Training Loss: 0.8040690975977962\n",
      "Epoch 18, Training Loss: 0.8050146685507065\n",
      "Epoch 19, Training Loss: 0.8048018675997741\n",
      "Epoch 20, Training Loss: 0.8048633941134116\n",
      "Epoch 21, Training Loss: 0.8036772022569986\n",
      "Epoch 22, Training Loss: 0.8029220662618938\n",
      "Epoch 23, Training Loss: 0.8036651814790596\n",
      "Epoch 24, Training Loss: 0.8033655973305379\n",
      "Epoch 25, Training Loss: 0.8020957434984078\n",
      "Epoch 26, Training Loss: 0.8022060943725414\n",
      "Epoch 27, Training Loss: 0.8027832946382967\n",
      "Epoch 28, Training Loss: 0.8014302078942607\n",
      "Epoch 29, Training Loss: 0.8002213274625908\n",
      "Epoch 30, Training Loss: 0.8003105162677908\n",
      "Epoch 31, Training Loss: 0.7993416262748546\n",
      "Epoch 32, Training Loss: 0.8005964038963604\n",
      "Epoch 33, Training Loss: 0.7993582228072604\n",
      "Epoch 34, Training Loss: 0.7993872631761364\n",
      "Epoch 35, Training Loss: 0.7987693510557475\n",
      "Epoch 36, Training Loss: 0.7994796933088087\n",
      "Epoch 37, Training Loss: 0.7987609675056055\n",
      "Epoch 38, Training Loss: 0.7970413079387263\n",
      "Epoch 39, Training Loss: 0.7977444590482496\n",
      "Epoch 40, Training Loss: 0.7988018039473914\n",
      "Epoch 41, Training Loss: 0.7962290212624055\n",
      "Epoch 42, Training Loss: 0.7968668149826222\n",
      "Epoch 43, Training Loss: 0.7974573274304096\n",
      "Epoch 44, Training Loss: 0.7959507871391182\n",
      "Epoch 45, Training Loss: 0.7961165592186433\n",
      "Epoch 46, Training Loss: 0.7963330242866861\n",
      "Epoch 47, Training Loss: 0.7950638001126454\n",
      "Epoch 48, Training Loss: 0.7943469228601097\n",
      "Epoch 49, Training Loss: 0.7944597708551507\n",
      "Epoch 50, Training Loss: 0.7941624510108977\n",
      "Epoch 51, Training Loss: 0.7944222296987261\n",
      "Epoch 52, Training Loss: 0.7942646572464391\n",
      "Epoch 53, Training Loss: 0.7946378840539688\n",
      "Epoch 54, Training Loss: 0.7955334990544427\n",
      "Epoch 55, Training Loss: 0.7940667580841179\n",
      "Epoch 56, Training Loss: 0.7960771262197566\n",
      "Epoch 57, Training Loss: 0.793993800206292\n",
      "Epoch 58, Training Loss: 0.7943069777094331\n",
      "Epoch 59, Training Loss: 0.7945758080123959\n",
      "Epoch 60, Training Loss: 0.7942726544867781\n",
      "Epoch 61, Training Loss: 0.7935489193837445\n",
      "Epoch 62, Training Loss: 0.7933687620593193\n",
      "Epoch 63, Training Loss: 0.7934616761100023\n",
      "Epoch 64, Training Loss: 0.7935403098737387\n",
      "Epoch 65, Training Loss: 0.7936253109372648\n",
      "Epoch 66, Training Loss: 0.7930982054624343\n",
      "Epoch 67, Training Loss: 0.7925718242960765\n",
      "Epoch 68, Training Loss: 0.7927148170937273\n",
      "Epoch 69, Training Loss: 0.7921643765349137\n",
      "Epoch 70, Training Loss: 0.7924200438019028\n",
      "Epoch 71, Training Loss: 0.792471394592658\n",
      "Epoch 72, Training Loss: 0.7917494424303672\n",
      "Epoch 73, Training Loss: 0.7931194981237999\n",
      "Epoch 74, Training Loss: 0.7916909590699619\n",
      "Epoch 75, Training Loss: 0.7922876609895463\n",
      "Epoch 76, Training Loss: 0.7919138733605693\n",
      "Epoch 77, Training Loss: 0.7923892700582519\n",
      "Epoch 78, Training Loss: 0.7925254612937009\n",
      "Epoch 79, Training Loss: 0.7928302751447921\n",
      "Epoch 80, Training Loss: 0.7913735778708207\n",
      "Epoch 81, Training Loss: 0.7911738668169294\n",
      "Epoch 82, Training Loss: 0.7917985483219749\n",
      "Epoch 83, Training Loss: 0.7917735112340827\n",
      "Epoch 84, Training Loss: 0.7915249955385251\n",
      "Epoch 85, Training Loss: 0.7919838045772754\n",
      "Epoch 86, Training Loss: 0.7915390619657989\n",
      "Epoch 87, Training Loss: 0.792143774570379\n",
      "Epoch 88, Training Loss: 0.7914152606985623\n",
      "Epoch 89, Training Loss: 0.7915955833922651\n",
      "Epoch 90, Training Loss: 0.7918905354083929\n",
      "Epoch 91, Training Loss: 0.7919824664754079\n",
      "Epoch 92, Training Loss: 0.7911788737863527\n",
      "Epoch 93, Training Loss: 0.7918210827318349\n",
      "Epoch 94, Training Loss: 0.7917280651572952\n",
      "Epoch 95, Training Loss: 0.791499728457372\n",
      "Epoch 96, Training Loss: 0.7918190478382254\n",
      "Epoch 97, Training Loss: 0.7913758393517114\n",
      "Epoch 98, Training Loss: 0.7908547480303542\n",
      "Epoch 99, Training Loss: 0.7907410874850768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:26:48,153] Trial 119 finished with value: 0.6364666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7905516715874349\n",
      "Epoch 1, Training Loss: 0.952867108897159\n",
      "Epoch 2, Training Loss: 0.8679498340850486\n",
      "Epoch 3, Training Loss: 0.8570729313040139\n",
      "Epoch 4, Training Loss: 0.8538017385884336\n",
      "Epoch 5, Training Loss: 0.8508819306703438\n",
      "Epoch 6, Training Loss: 0.8515424544649913\n",
      "Epoch 7, Training Loss: 0.8519452190936956\n",
      "Epoch 8, Training Loss: 0.8503770164977339\n",
      "Epoch 9, Training Loss: 0.8465154477528163\n",
      "Epoch 10, Training Loss: 0.8469057646012844\n",
      "Epoch 11, Training Loss: 0.8473611220381314\n",
      "Epoch 12, Training Loss: 0.8451544439882264\n",
      "Epoch 13, Training Loss: 0.8433372528929459\n",
      "Epoch 14, Training Loss: 0.8442197859735417\n",
      "Epoch 15, Training Loss: 0.8445790159074884\n",
      "Epoch 16, Training Loss: 0.8440877379779529\n",
      "Epoch 17, Training Loss: 0.8440441660414961\n",
      "Epoch 18, Training Loss: 0.8424214672325249\n",
      "Epoch 19, Training Loss: 0.8411380731073538\n",
      "Epoch 20, Training Loss: 0.8413795313440767\n",
      "Epoch 21, Training Loss: 0.8408872088095299\n",
      "Epoch 22, Training Loss: 0.8416711742716624\n",
      "Epoch 23, Training Loss: 0.8415281326250923\n",
      "Epoch 24, Training Loss: 0.8402142758656265\n",
      "Epoch 25, Training Loss: 0.839606515536631\n",
      "Epoch 26, Training Loss: 0.8405822845329916\n",
      "Epoch 27, Training Loss: 0.8397710638835018\n",
      "Epoch 28, Training Loss: 0.8387303504728734\n",
      "Epoch 29, Training Loss: 0.8407725609334787\n",
      "Epoch 30, Training Loss: 0.8401432379744107\n",
      "Epoch 31, Training Loss: 0.8375664722650571\n",
      "Epoch 32, Training Loss: 0.8380268625746993\n",
      "Epoch 33, Training Loss: 0.8395397603063655\n",
      "Epoch 34, Training Loss: 0.8376654558611992\n",
      "Epoch 35, Training Loss: 0.838206696689577\n",
      "Epoch 36, Training Loss: 0.83790997661146\n",
      "Epoch 37, Training Loss: 0.8377212725187603\n",
      "Epoch 38, Training Loss: 0.8359693875886444\n",
      "Epoch 39, Training Loss: 0.8381244901427649\n",
      "Epoch 40, Training Loss: 0.8383772323902389\n",
      "Epoch 41, Training Loss: 0.8375952911556215\n",
      "Epoch 42, Training Loss: 0.8384326023266728\n",
      "Epoch 43, Training Loss: 0.8397319920977255\n",
      "Epoch 44, Training Loss: 0.8361652025602814\n",
      "Epoch 45, Training Loss: 0.8367363260204631\n",
      "Epoch 46, Training Loss: 0.8390279307401269\n",
      "Epoch 47, Training Loss: 0.8376174076159197\n",
      "Epoch 48, Training Loss: 0.8364690904330491\n",
      "Epoch 49, Training Loss: 0.8357343072281744\n",
      "Epoch 50, Training Loss: 0.835705690365985\n",
      "Epoch 51, Training Loss: 0.8367304983891939\n",
      "Epoch 52, Training Loss: 0.8369157523140871\n",
      "Epoch 53, Training Loss: 0.8389562711679845\n",
      "Epoch 54, Training Loss: 0.8348329324471323\n",
      "Epoch 55, Training Loss: 0.8380190556210683\n",
      "Epoch 56, Training Loss: 0.8345618544664598\n",
      "Epoch 57, Training Loss: 0.8341859464358566\n",
      "Epoch 58, Training Loss: 0.8348205915967325\n",
      "Epoch 59, Training Loss: 0.8346309560582154\n",
      "Epoch 60, Training Loss: 0.8330288172664498\n",
      "Epoch 61, Training Loss: 0.8349489603275643\n",
      "Epoch 62, Training Loss: 0.8366307686146041\n",
      "Epoch 63, Training Loss: 0.8338478323212244\n",
      "Epoch 64, Training Loss: 0.8350360970748099\n",
      "Epoch 65, Training Loss: 0.8356410088395714\n",
      "Epoch 66, Training Loss: 0.8353614380485133\n",
      "Epoch 67, Training Loss: 0.8360630497896582\n",
      "Epoch 68, Training Loss: 0.8361605323346933\n",
      "Epoch 69, Training Loss: 0.8363293148521194\n",
      "Epoch 70, Training Loss: 0.8346946199137465\n",
      "Epoch 71, Training Loss: 0.8353851602937942\n",
      "Epoch 72, Training Loss: 0.8357084198105604\n",
      "Epoch 73, Training Loss: 0.833817214087436\n",
      "Epoch 74, Training Loss: 0.8350644899490184\n",
      "Epoch 75, Training Loss: 0.8360530253639795\n",
      "Epoch 76, Training Loss: 0.8348257806964386\n",
      "Epoch 77, Training Loss: 0.8338277244030085\n",
      "Epoch 78, Training Loss: 0.8355207578580183\n",
      "Epoch 79, Training Loss: 0.8347711771054376\n",
      "Epoch 80, Training Loss: 0.8336472025491242\n",
      "Epoch 81, Training Loss: 0.8338078487188296\n",
      "Epoch 82, Training Loss: 0.8343941014512141\n",
      "Epoch 83, Training Loss: 0.8342373489437247\n",
      "Epoch 84, Training Loss: 0.8358117110747144\n",
      "Epoch 85, Training Loss: 0.8371253224243795\n",
      "Epoch 86, Training Loss: 0.835740297719052\n",
      "Epoch 87, Training Loss: 0.8355102968395205\n",
      "Epoch 88, Training Loss: 0.8335325602301977\n",
      "Epoch 89, Training Loss: 0.8356908880678334\n",
      "Epoch 90, Training Loss: 0.8354043351080185\n",
      "Epoch 91, Training Loss: 0.835226475565057\n",
      "Epoch 92, Training Loss: 0.8337801097927237\n",
      "Epoch 93, Training Loss: 0.8353718737910565\n",
      "Epoch 94, Training Loss: 0.8343661013402437\n",
      "Epoch 95, Training Loss: 0.8347875334266434\n",
      "Epoch 96, Training Loss: 0.834019956239184\n",
      "Epoch 97, Training Loss: 0.8337440593798358\n",
      "Epoch 98, Training Loss: 0.8373081642882269\n",
      "Epoch 99, Training Loss: 0.8339795796494734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:28:36,289] Trial 120 finished with value: 0.5866 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8330241852236869\n",
      "Epoch 1, Training Loss: 0.8489629950738491\n",
      "Epoch 2, Training Loss: 0.8154909142874237\n",
      "Epoch 3, Training Loss: 0.8074502589111041\n",
      "Epoch 4, Training Loss: 0.8023020808409927\n",
      "Epoch 5, Training Loss: 0.8019168319558738\n",
      "Epoch 6, Training Loss: 0.8005025133154446\n",
      "Epoch 7, Training Loss: 0.7982744372876963\n",
      "Epoch 8, Training Loss: 0.7969608298817972\n",
      "Epoch 9, Training Loss: 0.7989034598931334\n",
      "Epoch 10, Training Loss: 0.7989125129871799\n",
      "Epoch 11, Training Loss: 0.798650894846235\n",
      "Epoch 12, Training Loss: 0.7979013239530692\n",
      "Epoch 13, Training Loss: 0.7948816050264172\n",
      "Epoch 14, Training Loss: 0.795252331755215\n",
      "Epoch 15, Training Loss: 0.7966928076027031\n",
      "Epoch 16, Training Loss: 0.7958055947059975\n",
      "Epoch 17, Training Loss: 0.79677660510056\n",
      "Epoch 18, Training Loss: 0.7954436214346635\n",
      "Epoch 19, Training Loss: 0.795005836970824\n",
      "Epoch 20, Training Loss: 0.7942487441507498\n",
      "Epoch 21, Training Loss: 0.794358728075386\n",
      "Epoch 22, Training Loss: 0.7935857912651578\n",
      "Epoch 23, Training Loss: 0.7950441688523257\n",
      "Epoch 24, Training Loss: 0.793539179536633\n",
      "Epoch 25, Training Loss: 0.7942279564706902\n",
      "Epoch 26, Training Loss: 0.7936475309214197\n",
      "Epoch 27, Training Loss: 0.7944348703649707\n",
      "Epoch 28, Training Loss: 0.7922607350170164\n",
      "Epoch 29, Training Loss: 0.7938524822543438\n",
      "Epoch 30, Training Loss: 0.7933269302647813\n",
      "Epoch 31, Training Loss: 0.7930018531648736\n",
      "Epoch 32, Training Loss: 0.7928882535238911\n",
      "Epoch 33, Training Loss: 0.7937128421059229\n",
      "Epoch 34, Training Loss: 0.7940303764845196\n",
      "Epoch 35, Training Loss: 0.7932158049784208\n",
      "Epoch 36, Training Loss: 0.7941080400818272\n",
      "Epoch 37, Training Loss: 0.7923003859089729\n",
      "Epoch 38, Training Loss: 0.7928982810866564\n",
      "Epoch 39, Training Loss: 0.7929957621975949\n",
      "Epoch 40, Training Loss: 0.7926137940327924\n",
      "Epoch 41, Training Loss: 0.7916664798456924\n",
      "Epoch 42, Training Loss: 0.7946844599300757\n",
      "Epoch 43, Training Loss: 0.7928306909432089\n",
      "Epoch 44, Training Loss: 0.7927590252761554\n",
      "Epoch 45, Training Loss: 0.792101574481878\n",
      "Epoch 46, Training Loss: 0.7930515431820002\n",
      "Epoch 47, Training Loss: 0.7920970138750578\n",
      "Epoch 48, Training Loss: 0.7924332832035266\n",
      "Epoch 49, Training Loss: 0.7935369560593053\n",
      "Epoch 50, Training Loss: 0.7909527144934002\n",
      "Epoch 51, Training Loss: 0.7934300807185639\n",
      "Epoch 52, Training Loss: 0.7921283055068855\n",
      "Epoch 53, Training Loss: 0.7921524565022691\n",
      "Epoch 54, Training Loss: 0.7917478834776054\n",
      "Epoch 55, Training Loss: 0.7925281409930466\n",
      "Epoch 56, Training Loss: 0.7914571131978716\n",
      "Epoch 57, Training Loss: 0.7930540047193828\n",
      "Epoch 58, Training Loss: 0.7906021619201603\n",
      "Epoch 59, Training Loss: 0.7906292209947916\n",
      "Epoch 60, Training Loss: 0.792048210219333\n",
      "Epoch 61, Training Loss: 0.7918409352015732\n",
      "Epoch 62, Training Loss: 0.7908538642234372\n",
      "Epoch 63, Training Loss: 0.7923533548986105\n",
      "Epoch 64, Training Loss: 0.7911737755725258\n",
      "Epoch 65, Training Loss: 0.7921268645982097\n",
      "Epoch 66, Training Loss: 0.7907247935918937\n",
      "Epoch 67, Training Loss: 0.7918966231937695\n",
      "Epoch 68, Training Loss: 0.7919372581001511\n",
      "Epoch 69, Training Loss: 0.7915366643353512\n",
      "Epoch 70, Training Loss: 0.7906690220187481\n",
      "Epoch 71, Training Loss: 0.7917148062161037\n",
      "Epoch 72, Training Loss: 0.7918938677113755\n",
      "Epoch 73, Training Loss: 0.7938818314918001\n",
      "Epoch 74, Training Loss: 0.7919263811936056\n",
      "Epoch 75, Training Loss: 0.7913563425379588\n",
      "Epoch 76, Training Loss: 0.791364165894071\n",
      "Epoch 77, Training Loss: 0.7932204827330166\n",
      "Epoch 78, Training Loss: 0.7924149553578599\n",
      "Epoch 79, Training Loss: 0.7914183606778769\n",
      "Epoch 80, Training Loss: 0.7917829593321435\n",
      "Epoch 81, Training Loss: 0.7906340156282697\n",
      "Epoch 82, Training Loss: 0.7921753159142975\n",
      "Epoch 83, Training Loss: 0.7913385557052784\n",
      "Epoch 84, Training Loss: 0.7922096728382254\n",
      "Epoch 85, Training Loss: 0.7922162994406277\n",
      "Epoch 86, Training Loss: 0.7910646142816185\n",
      "Epoch 87, Training Loss: 0.7913204463800989\n",
      "Epoch 88, Training Loss: 0.792105640056438\n",
      "Epoch 89, Training Loss: 0.7911896941357089\n",
      "Epoch 90, Training Loss: 0.7902643654579506\n",
      "Epoch 91, Training Loss: 0.7922499721211599\n",
      "Epoch 92, Training Loss: 0.7913748183644804\n",
      "Epoch 93, Training Loss: 0.7904826249395098\n",
      "Epoch 94, Training Loss: 0.7915329877595256\n",
      "Epoch 95, Training Loss: 0.7933949895371172\n",
      "Epoch 96, Training Loss: 0.7917764296209006\n",
      "Epoch 97, Training Loss: 0.7903255998640132\n",
      "Epoch 98, Training Loss: 0.7907468676567078\n",
      "Epoch 99, Training Loss: 0.7923706217816001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:30:30,255] Trial 121 finished with value: 0.6386666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7918879907830317\n",
      "Epoch 1, Training Loss: 1.0538900929286068\n",
      "Epoch 2, Training Loss: 1.000054208766249\n",
      "Epoch 3, Training Loss: 0.9736115476242582\n",
      "Epoch 4, Training Loss: 0.9628876340120358\n",
      "Epoch 5, Training Loss: 0.9585914247018054\n",
      "Epoch 6, Training Loss: 0.9559392283733626\n",
      "Epoch 7, Training Loss: 0.9533364581882505\n",
      "Epoch 8, Training Loss: 0.9513776597223784\n",
      "Epoch 9, Training Loss: 0.9495999562112909\n",
      "Epoch 10, Training Loss: 0.9476664966210386\n",
      "Epoch 11, Training Loss: 0.9452978569762152\n",
      "Epoch 12, Training Loss: 0.942933055870515\n",
      "Epoch 13, Training Loss: 0.9409765443407503\n",
      "Epoch 14, Training Loss: 0.9391991486226706\n",
      "Epoch 15, Training Loss: 0.9361467637513813\n",
      "Epoch 16, Training Loss: 0.934353364499888\n",
      "Epoch 17, Training Loss: 0.9310074903911217\n",
      "Epoch 18, Training Loss: 0.9285066166318449\n",
      "Epoch 19, Training Loss: 0.925349173509985\n",
      "Epoch 20, Training Loss: 0.9221014893144593\n",
      "Epoch 21, Training Loss: 0.9191666668519042\n",
      "Epoch 22, Training Loss: 0.9170445803412818\n",
      "Epoch 23, Training Loss: 0.9132071660873585\n",
      "Epoch 24, Training Loss: 0.9093653695923941\n",
      "Epoch 25, Training Loss: 0.9061736238630195\n",
      "Epoch 26, Training Loss: 0.9029981671419359\n",
      "Epoch 27, Training Loss: 0.8992390018656737\n",
      "Epoch 28, Training Loss: 0.8950145163034138\n",
      "Epoch 29, Training Loss: 0.8912737630363694\n",
      "Epoch 30, Training Loss: 0.88757271972814\n",
      "Epoch 31, Training Loss: 0.8842072044996391\n",
      "Epoch 32, Training Loss: 0.8810920170375279\n",
      "Epoch 33, Training Loss: 0.8768641861757838\n",
      "Epoch 34, Training Loss: 0.8732839709834048\n",
      "Epoch 35, Training Loss: 0.8697590570700796\n",
      "Epoch 36, Training Loss: 0.8664771843673591\n",
      "Epoch 37, Training Loss: 0.8631682429994856\n",
      "Epoch 38, Training Loss: 0.8601642920558614\n",
      "Epoch 39, Training Loss: 0.8571878072014428\n",
      "Epoch 40, Training Loss: 0.8538244345134363\n",
      "Epoch 41, Training Loss: 0.8506217515558229\n",
      "Epoch 42, Training Loss: 0.8485165736729041\n",
      "Epoch 43, Training Loss: 0.8455219342296285\n",
      "Epoch 44, Training Loss: 0.8430341664113497\n",
      "Epoch 45, Training Loss: 0.8408959070542701\n",
      "Epoch 46, Training Loss: 0.8385991461294934\n",
      "Epoch 47, Training Loss: 0.8365136557055596\n",
      "Epoch 48, Training Loss: 0.8346141116063398\n",
      "Epoch 49, Training Loss: 0.832190634164595\n",
      "Epoch 50, Training Loss: 0.8308492967956944\n",
      "Epoch 51, Training Loss: 0.8289633804694154\n",
      "Epoch 52, Training Loss: 0.8276698796372665\n",
      "Epoch 53, Training Loss: 0.8263194827208842\n",
      "Epoch 54, Training Loss: 0.8247650398347611\n",
      "Epoch 55, Training Loss: 0.8238387092611843\n",
      "Epoch 56, Training Loss: 0.8223699530264489\n",
      "Epoch 57, Training Loss: 0.8217770285176155\n",
      "Epoch 58, Training Loss: 0.8204789438642057\n",
      "Epoch 59, Training Loss: 0.8190319164354999\n",
      "Epoch 60, Training Loss: 0.8191845726249809\n",
      "Epoch 61, Training Loss: 0.8184843247098134\n",
      "Epoch 62, Training Loss: 0.817308463189835\n",
      "Epoch 63, Training Loss: 0.8164945149780216\n",
      "Epoch 64, Training Loss: 0.8157460921689084\n",
      "Epoch 65, Training Loss: 0.8154305144360191\n",
      "Epoch 66, Training Loss: 0.8147229879422295\n",
      "Epoch 67, Training Loss: 0.8147252228027\n",
      "Epoch 68, Training Loss: 0.8140011981913918\n",
      "Epoch 69, Training Loss: 0.8137027948422539\n",
      "Epoch 70, Training Loss: 0.8134648323955392\n",
      "Epoch 71, Training Loss: 0.813007682875583\n",
      "Epoch 72, Training Loss: 0.8124955369117565\n",
      "Epoch 73, Training Loss: 0.8122798082523777\n",
      "Epoch 74, Training Loss: 0.8125614270231778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:31:33,117] Trial 122 finished with value: 0.626 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8123066808048047\n",
      "Epoch 1, Training Loss: 0.8682044226281783\n",
      "Epoch 2, Training Loss: 0.8111608934402466\n",
      "Epoch 3, Training Loss: 0.8056767842348884\n",
      "Epoch 4, Training Loss: 0.8030845610534444\n",
      "Epoch 5, Training Loss: 0.800768447413164\n",
      "Epoch 6, Training Loss: 0.7982792956688824\n",
      "Epoch 7, Training Loss: 0.7965686343697941\n",
      "Epoch 8, Training Loss: 0.7949172388806063\n",
      "Epoch 9, Training Loss: 0.7930374644784366\n",
      "Epoch 10, Training Loss: 0.7918062862929176\n",
      "Epoch 11, Training Loss: 0.7915268596480874\n",
      "Epoch 12, Training Loss: 0.7906435979113859\n",
      "Epoch 13, Training Loss: 0.7898369746348437\n",
      "Epoch 14, Training Loss: 0.7888908472481896\n",
      "Epoch 15, Training Loss: 0.7882889261666466\n",
      "Epoch 16, Training Loss: 0.7876476995383992\n",
      "Epoch 17, Training Loss: 0.7870911953729741\n",
      "Epoch 18, Training Loss: 0.7869054158996133\n",
      "Epoch 19, Training Loss: 0.7866737514383653\n",
      "Epoch 20, Training Loss: 0.7861504049160901\n",
      "Epoch 21, Training Loss: 0.7856666383322547\n",
      "Epoch 22, Training Loss: 0.7855686811138601\n",
      "Epoch 23, Training Loss: 0.785000455309363\n",
      "Epoch 24, Training Loss: 0.7847258356739493\n",
      "Epoch 25, Training Loss: 0.7849980587117812\n",
      "Epoch 26, Training Loss: 0.7840858027514289\n",
      "Epoch 27, Training Loss: 0.7839989873942207\n",
      "Epoch 28, Training Loss: 0.7837641403955572\n",
      "Epoch 29, Training Loss: 0.7837458278151119\n",
      "Epoch 30, Training Loss: 0.7834503606487723\n",
      "Epoch 31, Training Loss: 0.7829426332782297\n",
      "Epoch 32, Training Loss: 0.7830065103138194\n",
      "Epoch 33, Training Loss: 0.782794540980283\n",
      "Epoch 34, Training Loss: 0.7827180229916292\n",
      "Epoch 35, Training Loss: 0.7825144401718589\n",
      "Epoch 36, Training Loss: 0.7823691862470964\n",
      "Epoch 37, Training Loss: 0.7820976005582249\n",
      "Epoch 38, Training Loss: 0.7821285893636591\n",
      "Epoch 39, Training Loss: 0.7820600600102369\n",
      "Epoch 40, Training Loss: 0.7814365419920754\n",
      "Epoch 41, Training Loss: 0.781537637500202\n",
      "Epoch 42, Training Loss: 0.7816015979822944\n",
      "Epoch 43, Training Loss: 0.7809567503368153\n",
      "Epoch 44, Training Loss: 0.7811112935402814\n",
      "Epoch 45, Training Loss: 0.7811039545956779\n",
      "Epoch 46, Training Loss: 0.7811051267736099\n",
      "Epoch 47, Training Loss: 0.7809923028945923\n",
      "Epoch 48, Training Loss: 0.7809594286890591\n",
      "Epoch 49, Training Loss: 0.7803518880816067\n",
      "Epoch 50, Training Loss: 0.780813931647469\n",
      "Epoch 51, Training Loss: 0.7807729173407835\n",
      "Epoch 52, Training Loss: 0.7805814633649938\n",
      "Epoch 53, Training Loss: 0.780473771165399\n",
      "Epoch 54, Training Loss: 0.7799881920393775\n",
      "Epoch 55, Training Loss: 0.7804670739173889\n",
      "Epoch 56, Training Loss: 0.7799765084070318\n",
      "Epoch 57, Training Loss: 0.7799840796695036\n",
      "Epoch 58, Training Loss: 0.779892085089403\n",
      "Epoch 59, Training Loss: 0.7798814011321348\n",
      "Epoch 60, Training Loss: 0.7798171765664045\n",
      "Epoch 61, Training Loss: 0.7794825776885538\n",
      "Epoch 62, Training Loss: 0.7795255461159875\n",
      "Epoch 63, Training Loss: 0.7793774223327636\n",
      "Epoch 64, Training Loss: 0.7793087793097776\n",
      "Epoch 65, Training Loss: 0.7793408844050239\n",
      "Epoch 66, Training Loss: 0.7793972347764407\n",
      "Epoch 67, Training Loss: 0.7794856578462264\n",
      "Epoch 68, Training Loss: 0.7792056517741259\n",
      "Epoch 69, Training Loss: 0.7788868720391218\n",
      "Epoch 70, Training Loss: 0.7788372926852283\n",
      "Epoch 71, Training Loss: 0.7790473417674794\n",
      "Epoch 72, Training Loss: 0.779062616334242\n",
      "Epoch 73, Training Loss: 0.7786839360349318\n",
      "Epoch 74, Training Loss: 0.778645252830842\n",
      "Epoch 75, Training Loss: 0.7785222471461577\n",
      "Epoch 76, Training Loss: 0.778618513135349\n",
      "Epoch 77, Training Loss: 0.7787430336194879\n",
      "Epoch 78, Training Loss: 0.7785493754639345\n",
      "Epoch 79, Training Loss: 0.778832481398302\n",
      "Epoch 80, Training Loss: 0.7783651510406943\n",
      "Epoch 81, Training Loss: 0.7782019830451292\n",
      "Epoch 82, Training Loss: 0.7784450693691478\n",
      "Epoch 83, Training Loss: 0.7780756948975955\n",
      "Epoch 84, Training Loss: 0.7785288395601161\n",
      "Epoch 85, Training Loss: 0.7779256270913517\n",
      "Epoch 86, Training Loss: 0.7780315622161417\n",
      "Epoch 87, Training Loss: 0.7780042173581965\n",
      "Epoch 88, Training Loss: 0.7780483167311725\n",
      "Epoch 89, Training Loss: 0.7777509077857523\n",
      "Epoch 90, Training Loss: 0.777573821404401\n",
      "Epoch 91, Training Loss: 0.7777198735405417\n",
      "Epoch 92, Training Loss: 0.7777133613474229\n",
      "Epoch 93, Training Loss: 0.7776029506851645\n",
      "Epoch 94, Training Loss: 0.777620071032468\n",
      "Epoch 95, Training Loss: 0.7774232074793648\n",
      "Epoch 96, Training Loss: 0.7775162191951976\n",
      "Epoch 97, Training Loss: 0.7771901662910686\n",
      "Epoch 98, Training Loss: 0.7774293905145981\n",
      "Epoch 99, Training Loss: 0.7773520613417906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:33:36,165] Trial 123 finished with value: 0.6404 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7773210387370165\n",
      "Epoch 1, Training Loss: 0.8470020766117994\n",
      "Epoch 2, Training Loss: 0.8152008860251483\n",
      "Epoch 3, Training Loss: 0.8134503368069144\n",
      "Epoch 4, Training Loss: 0.8109097700259265\n",
      "Epoch 5, Training Loss: 0.8087671641041251\n",
      "Epoch 6, Training Loss: 0.8077780757932101\n",
      "Epoch 7, Training Loss: 0.8062167634683497\n",
      "Epoch 8, Training Loss: 0.8041937204669504\n",
      "Epoch 9, Training Loss: 0.803523861099692\n",
      "Epoch 10, Training Loss: 0.8023344137388118\n",
      "Epoch 11, Training Loss: 0.8025945691501393\n",
      "Epoch 12, Training Loss: 0.8024124053646536\n",
      "Epoch 13, Training Loss: 0.802200634619769\n",
      "Epoch 14, Training Loss: 0.80082093947074\n",
      "Epoch 15, Training Loss: 0.8009137528784135\n",
      "Epoch 16, Training Loss: 0.801010146631914\n",
      "Epoch 17, Training Loss: 0.8001287145474377\n",
      "Epoch 18, Training Loss: 0.7998589106167064\n",
      "Epoch 19, Training Loss: 0.7998443891721613\n",
      "Epoch 20, Training Loss: 0.7995003172930549\n",
      "Epoch 21, Training Loss: 0.799355502689586\n",
      "Epoch 22, Training Loss: 0.7991508467057172\n",
      "Epoch 23, Training Loss: 0.7988897303272696\n",
      "Epoch 24, Training Loss: 0.7992515671253204\n",
      "Epoch 25, Training Loss: 0.7991467286558712\n",
      "Epoch 26, Training Loss: 0.7978680418519413\n",
      "Epoch 27, Training Loss: 0.7982189227552975\n",
      "Epoch 28, Training Loss: 0.7981675594694474\n",
      "Epoch 29, Training Loss: 0.7981622719063478\n",
      "Epoch 30, Training Loss: 0.7976999712691587\n",
      "Epoch 31, Training Loss: 0.7978111558100757\n",
      "Epoch 32, Training Loss: 0.7982086195665248\n",
      "Epoch 33, Training Loss: 0.7976170283205369\n",
      "Epoch 34, Training Loss: 0.7975873708023744\n",
      "Epoch 35, Training Loss: 0.7974834477901459\n",
      "Epoch 36, Training Loss: 0.7973222938705893\n",
      "Epoch 37, Training Loss: 0.7973795489703908\n",
      "Epoch 38, Training Loss: 0.7969017445339877\n",
      "Epoch 39, Training Loss: 0.7971262897463406\n",
      "Epoch 40, Training Loss: 0.7973690565894632\n",
      "Epoch 41, Training Loss: 0.7972232134903179\n",
      "Epoch 42, Training Loss: 0.7972589503316319\n",
      "Epoch 43, Training Loss: 0.797025679770638\n",
      "Epoch 44, Training Loss: 0.7972111179548151\n",
      "Epoch 45, Training Loss: 0.7966825826027814\n",
      "Epoch 46, Training Loss: 0.7974201015163871\n",
      "Epoch 47, Training Loss: 0.7975897668389713\n",
      "Epoch 48, Training Loss: 0.7965017165155972\n",
      "Epoch 49, Training Loss: 0.7968121981620788\n",
      "Epoch 50, Training Loss: 0.7970957864733303\n",
      "Epoch 51, Training Loss: 0.7964575596416698\n",
      "Epoch 52, Training Loss: 0.7964607950519113\n",
      "Epoch 53, Training Loss: 0.7969072964612175\n",
      "Epoch 54, Training Loss: 0.7963242403899922\n",
      "Epoch 55, Training Loss: 0.7964752918131212\n",
      "Epoch 56, Training Loss: 0.7964458069380592\n",
      "Epoch 57, Training Loss: 0.7962883377075195\n",
      "Epoch 58, Training Loss: 0.7961948052574607\n",
      "Epoch 59, Training Loss: 0.796439268729266\n",
      "Epoch 60, Training Loss: 0.796052065666984\n",
      "Epoch 61, Training Loss: 0.7961999058022219\n",
      "Epoch 62, Training Loss: 0.7961758961397059\n",
      "Epoch 63, Training Loss: 0.7960492659316344\n",
      "Epoch 64, Training Loss: 0.7950025612466476\n",
      "Epoch 65, Training Loss: 0.7957695146167979\n",
      "Epoch 66, Training Loss: 0.7957659647745244\n",
      "Epoch 67, Training Loss: 0.7959091201249291\n",
      "Epoch 68, Training Loss: 0.7954194117994869\n",
      "Epoch 69, Training Loss: 0.794942704930025\n",
      "Epoch 70, Training Loss: 0.7956999568378225\n",
      "Epoch 71, Training Loss: 0.7952698490900152\n",
      "Epoch 72, Training Loss: 0.7955287136751062\n",
      "Epoch 73, Training Loss: 0.7956849154304055\n",
      "Epoch 74, Training Loss: 0.7954017937884611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:35:01,514] Trial 124 finished with value: 0.6341333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7948559916720671\n",
      "Epoch 1, Training Loss: 0.8489308755538043\n",
      "Epoch 2, Training Loss: 0.8213214337124544\n",
      "Epoch 3, Training Loss: 0.8223958520328297\n",
      "Epoch 4, Training Loss: 0.8173266229910009\n",
      "Epoch 5, Training Loss: 0.8137382442109725\n",
      "Epoch 6, Training Loss: 0.8148771181527306\n",
      "Epoch 7, Training Loss: 0.8123414224736831\n",
      "Epoch 8, Training Loss: 0.8126142592991099\n",
      "Epoch 9, Training Loss: 0.8129397730967578\n",
      "Epoch 10, Training Loss: 0.8126465060430414\n",
      "Epoch 11, Training Loss: 0.8119131593143238\n",
      "Epoch 12, Training Loss: 0.8117439631854787\n",
      "Epoch 13, Training Loss: 0.812767903103548\n",
      "Epoch 14, Training Loss: 0.8125671073268441\n",
      "Epoch 15, Training Loss: 0.8117913555397707\n",
      "Epoch 16, Training Loss: 0.8126058599528144\n",
      "Epoch 17, Training Loss: 0.8163223272912643\n",
      "Epoch 18, Training Loss: 0.8106085266085232\n",
      "Epoch 19, Training Loss: 0.8160140601326438\n",
      "Epoch 20, Training Loss: 0.8110043926799999\n",
      "Epoch 21, Training Loss: 0.8125136848057017\n",
      "Epoch 22, Training Loss: 0.8115285250719856\n",
      "Epoch 23, Training Loss: 0.8164865537250743\n",
      "Epoch 24, Training Loss: 0.8181079298608444\n",
      "Epoch 25, Training Loss: 0.8131143646380481\n",
      "Epoch 26, Training Loss: 0.8195033689807443\n",
      "Epoch 27, Training Loss: 0.8137493996760424\n",
      "Epoch 28, Training Loss: 0.8143897885434768\n",
      "Epoch 29, Training Loss: 0.8142031302171595\n",
      "Epoch 30, Training Loss: 0.8147855840009801\n",
      "Epoch 31, Training Loss: 0.8194166028499603\n",
      "Epoch 32, Training Loss: 0.8234260278589586\n",
      "Epoch 33, Training Loss: 0.8249680328369141\n",
      "Epoch 34, Training Loss: 0.8241633550559773\n",
      "Epoch 35, Training Loss: 0.831583509234821\n",
      "Epoch 36, Training Loss: 0.8304480006414301\n",
      "Epoch 37, Training Loss: 0.829966657372082\n",
      "Epoch 38, Training Loss: 0.8371411521995769\n",
      "Epoch 39, Training Loss: 0.8341925564232995\n",
      "Epoch 40, Training Loss: 0.8349767177946428\n",
      "Epoch 41, Training Loss: 0.8361533947551951\n",
      "Epoch 42, Training Loss: 0.8306627555454479\n",
      "Epoch 43, Training Loss: 0.8326696031233843\n",
      "Epoch 44, Training Loss: 0.8347125359843759\n",
      "Epoch 45, Training Loss: 0.8355433860245873\n",
      "Epoch 46, Training Loss: 0.8350817488922793\n",
      "Epoch 47, Training Loss: 0.8323934129406424\n",
      "Epoch 48, Training Loss: 0.8337773055188796\n",
      "Epoch 49, Training Loss: 0.8260551651786355\n",
      "Epoch 50, Training Loss: 0.8293990229859072\n",
      "Epoch 51, Training Loss: 0.8309355823432698\n",
      "Epoch 52, Training Loss: 0.8316603036487804\n",
      "Epoch 53, Training Loss: 0.8327388497661142\n",
      "Epoch 54, Training Loss: 0.8302432682934929\n",
      "Epoch 55, Training Loss: 0.8309510708556456\n",
      "Epoch 56, Training Loss: 0.8318073112123153\n",
      "Epoch 57, Training Loss: 0.8276290285587311\n",
      "Epoch 58, Training Loss: 0.8307708968835719\n",
      "Epoch 59, Training Loss: 0.8333794941621668\n",
      "Epoch 60, Training Loss: 0.8320180775137509\n",
      "Epoch 61, Training Loss: 0.8287861211860881\n",
      "Epoch 62, Training Loss: 0.8305245802682989\n",
      "Epoch 63, Training Loss: 0.8330582766673145\n",
      "Epoch 64, Training Loss: 0.8272712862491608\n",
      "Epoch 65, Training Loss: 0.8306295190839207\n",
      "Epoch 66, Training Loss: 0.8339763959015117\n",
      "Epoch 67, Training Loss: 0.829951740363065\n",
      "Epoch 68, Training Loss: 0.8277849032598383\n",
      "Epoch 69, Training Loss: 0.8270249821158017\n",
      "Epoch 70, Training Loss: 0.8285829729893628\n",
      "Epoch 71, Training Loss: 0.8297678896258859\n",
      "Epoch 72, Training Loss: 0.8244999598054324\n",
      "Epoch 73, Training Loss: 0.8287868534116184\n",
      "Epoch 74, Training Loss: 0.826451117922278\n",
      "Epoch 75, Training Loss: 0.8285699762316311\n",
      "Epoch 76, Training Loss: 0.8327353405952453\n",
      "Epoch 77, Training Loss: 0.8288176608786864\n",
      "Epoch 78, Training Loss: 0.8287749054852654\n",
      "Epoch 79, Training Loss: 0.8307959225598504\n",
      "Epoch 80, Training Loss: 0.8288075098570655\n",
      "Epoch 81, Training Loss: 0.8318125247254091\n",
      "Epoch 82, Training Loss: 0.8320066158911761\n",
      "Epoch 83, Training Loss: 0.828303578601164\n",
      "Epoch 84, Training Loss: 0.8288932757517871\n",
      "Epoch 85, Training Loss: 0.8285787113273845\n",
      "Epoch 86, Training Loss: 0.8305095967825721\n",
      "Epoch 87, Training Loss: 0.8276123529322007\n",
      "Epoch 88, Training Loss: 0.8287193153886234\n",
      "Epoch 89, Training Loss: 0.8305347716808319\n",
      "Epoch 90, Training Loss: 0.8242158532843871\n",
      "Epoch 91, Training Loss: 0.8319587063088136\n",
      "Epoch 92, Training Loss: 0.8311850097600152\n",
      "Epoch 93, Training Loss: 0.8302321117064532\n",
      "Epoch 94, Training Loss: 0.8262332469575545\n",
      "Epoch 95, Training Loss: 0.8280023883370792\n",
      "Epoch 96, Training Loss: 0.8303101964557872\n",
      "Epoch 97, Training Loss: 0.8248759245872498\n",
      "Epoch 98, Training Loss: 0.826775904823752\n",
      "Epoch 99, Training Loss: 0.8303514894317178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:37:16,117] Trial 125 finished with value: 0.6274 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8288537467227263\n",
      "Epoch 1, Training Loss: 0.9595155113621762\n",
      "Epoch 2, Training Loss: 0.8910311826189657\n",
      "Epoch 3, Training Loss: 0.8403867838974286\n",
      "Epoch 4, Training Loss: 0.8195976458097759\n",
      "Epoch 5, Training Loss: 0.8133537585573986\n",
      "Epoch 6, Training Loss: 0.8109136653125735\n",
      "Epoch 7, Training Loss: 0.8092413062439825\n",
      "Epoch 8, Training Loss: 0.8082626939716195\n",
      "Epoch 9, Training Loss: 0.8079296405154063\n",
      "Epoch 10, Training Loss: 0.8074194179441696\n",
      "Epoch 11, Training Loss: 0.8062066551886107\n",
      "Epoch 12, Training Loss: 0.8060552696536358\n",
      "Epoch 13, Training Loss: 0.8055255638029343\n",
      "Epoch 14, Training Loss: 0.8054757779702209\n",
      "Epoch 15, Training Loss: 0.8042031006705492\n",
      "Epoch 16, Training Loss: 0.804767152151667\n",
      "Epoch 17, Training Loss: 0.8041616285653939\n",
      "Epoch 18, Training Loss: 0.8037479197172294\n",
      "Epoch 19, Training Loss: 0.8033937661271346\n",
      "Epoch 20, Training Loss: 0.8035271458159712\n",
      "Epoch 21, Training Loss: 0.8033516321863446\n",
      "Epoch 22, Training Loss: 0.802112649168287\n",
      "Epoch 23, Training Loss: 0.8019694736129359\n",
      "Epoch 24, Training Loss: 0.802538746192043\n",
      "Epoch 25, Training Loss: 0.8023905018218478\n",
      "Epoch 26, Training Loss: 0.8023955784345927\n",
      "Epoch 27, Training Loss: 0.8021318474210295\n",
      "Epoch 28, Training Loss: 0.8016809155170183\n",
      "Epoch 29, Training Loss: 0.8012614519972551\n",
      "Epoch 30, Training Loss: 0.8014707279384584\n",
      "Epoch 31, Training Loss: 0.8012420726001711\n",
      "Epoch 32, Training Loss: 0.8008294296443911\n",
      "Epoch 33, Training Loss: 0.8008273407032616\n",
      "Epoch 34, Training Loss: 0.8015525168942329\n",
      "Epoch 35, Training Loss: 0.8004465250144327\n",
      "Epoch 36, Training Loss: 0.8003969568058961\n",
      "Epoch 37, Training Loss: 0.8001338134134622\n",
      "Epoch 38, Training Loss: 0.8001098291318219\n",
      "Epoch 39, Training Loss: 0.7992741426130883\n",
      "Epoch 40, Training Loss: 0.8007840801898698\n",
      "Epoch 41, Training Loss: 0.7994810863545067\n",
      "Epoch 42, Training Loss: 0.7995073832963643\n",
      "Epoch 43, Training Loss: 0.7994956021022079\n",
      "Epoch 44, Training Loss: 0.7991126249607344\n",
      "Epoch 45, Training Loss: 0.79923232022981\n",
      "Epoch 46, Training Loss: 0.79941493927088\n",
      "Epoch 47, Training Loss: 0.7987109439713614\n",
      "Epoch 48, Training Loss: 0.798372539272882\n",
      "Epoch 49, Training Loss: 0.7985365264397815\n",
      "Epoch 50, Training Loss: 0.7990017928575215\n",
      "Epoch 51, Training Loss: 0.7992955731270008\n",
      "Epoch 52, Training Loss: 0.798567779709522\n",
      "Epoch 53, Training Loss: 0.7983511281192751\n",
      "Epoch 54, Training Loss: 0.7985378242973098\n",
      "Epoch 55, Training Loss: 0.7981303456134365\n",
      "Epoch 56, Training Loss: 0.7982384453142496\n",
      "Epoch 57, Training Loss: 0.7981937320608842\n",
      "Epoch 58, Training Loss: 0.7979569070321276\n",
      "Epoch 59, Training Loss: 0.7984512368539223\n",
      "Epoch 60, Training Loss: 0.7979527368581385\n",
      "Epoch 61, Training Loss: 0.798588894811788\n",
      "Epoch 62, Training Loss: 0.7978938188767971\n",
      "Epoch 63, Training Loss: 0.7975657297256298\n",
      "Epoch 64, Training Loss: 0.797752516789544\n",
      "Epoch 65, Training Loss: 0.7977301984801328\n",
      "Epoch 66, Training Loss: 0.7978598707600644\n",
      "Epoch 67, Training Loss: 0.7975014956373917\n",
      "Epoch 68, Training Loss: 0.7975605040564573\n",
      "Epoch 69, Training Loss: 0.7978264576510379\n",
      "Epoch 70, Training Loss: 0.7978244958067299\n",
      "Epoch 71, Training Loss: 0.7980987601710442\n",
      "Epoch 72, Training Loss: 0.7969886468765431\n",
      "Epoch 73, Training Loss: 0.7980866664334347\n",
      "Epoch 74, Training Loss: 0.7978474470905792\n",
      "Epoch 75, Training Loss: 0.796989947960789\n",
      "Epoch 76, Training Loss: 0.7973174633836387\n",
      "Epoch 77, Training Loss: 0.797470846391262\n",
      "Epoch 78, Training Loss: 0.7971640025762687\n",
      "Epoch 79, Training Loss: 0.7972322624428828\n",
      "Epoch 80, Training Loss: 0.7964252152837309\n",
      "Epoch 81, Training Loss: 0.7964599656431298\n",
      "Epoch 82, Training Loss: 0.796901255711577\n",
      "Epoch 83, Training Loss: 0.7963292895403123\n",
      "Epoch 84, Training Loss: 0.7964945162149301\n",
      "Epoch 85, Training Loss: 0.7968150769857536\n",
      "Epoch 86, Training Loss: 0.7973181540804698\n",
      "Epoch 87, Training Loss: 0.7966342619487218\n",
      "Epoch 88, Training Loss: 0.7967875968244739\n",
      "Epoch 89, Training Loss: 0.7963520464144255\n",
      "Epoch 90, Training Loss: 0.7967026038277418\n",
      "Epoch 91, Training Loss: 0.7966892506843223\n",
      "Epoch 92, Training Loss: 0.7970408228106964\n",
      "Epoch 93, Training Loss: 0.7970059935311625\n",
      "Epoch 94, Training Loss: 0.7968547660605352\n",
      "Epoch 95, Training Loss: 0.7969104073101416\n",
      "Epoch 96, Training Loss: 0.7964629469957567\n",
      "Epoch 97, Training Loss: 0.7963162723340487\n",
      "Epoch 98, Training Loss: 0.7973208393369402\n",
      "Epoch 99, Training Loss: 0.7961317213854395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:38:55,334] Trial 126 finished with value: 0.6352 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7968447274731514\n",
      "Epoch 1, Training Loss: 0.981761049776149\n",
      "Epoch 2, Training Loss: 0.9426646477297733\n",
      "Epoch 3, Training Loss: 0.9216732651667487\n",
      "Epoch 4, Training Loss: 0.8924870439938136\n",
      "Epoch 5, Training Loss: 0.861343925250204\n",
      "Epoch 6, Training Loss: 0.8397819087917643\n",
      "Epoch 7, Training Loss: 0.8278300277272561\n",
      "Epoch 8, Training Loss: 0.8218349451409247\n",
      "Epoch 9, Training Loss: 0.8172807611020884\n",
      "Epoch 10, Training Loss: 0.81392460747769\n",
      "Epoch 11, Training Loss: 0.8111037126161103\n",
      "Epoch 12, Training Loss: 0.8084609690465425\n",
      "Epoch 13, Training Loss: 0.8076270379518208\n",
      "Epoch 14, Training Loss: 0.8064798235893249\n",
      "Epoch 15, Training Loss: 0.8057097563169953\n",
      "Epoch 16, Training Loss: 0.8049094689519782\n",
      "Epoch 17, Training Loss: 0.8044097217402064\n",
      "Epoch 18, Training Loss: 0.8041481880317057\n",
      "Epoch 19, Training Loss: 0.8034084573724216\n",
      "Epoch 20, Training Loss: 0.8035505394290264\n",
      "Epoch 21, Training Loss: 0.8021924753834431\n",
      "Epoch 22, Training Loss: 0.8021068374017127\n",
      "Epoch 23, Training Loss: 0.8015703551751331\n",
      "Epoch 24, Training Loss: 0.8012034951296068\n",
      "Epoch 25, Training Loss: 0.8010364772681903\n",
      "Epoch 26, Training Loss: 0.8009237472276042\n",
      "Epoch 27, Training Loss: 0.8004649131817925\n",
      "Epoch 28, Training Loss: 0.7997243672385251\n",
      "Epoch 29, Training Loss: 0.7998791951882211\n",
      "Epoch 30, Training Loss: 0.7988732790140282\n",
      "Epoch 31, Training Loss: 0.799607268939341\n",
      "Epoch 32, Training Loss: 0.7984940519906525\n",
      "Epoch 33, Training Loss: 0.7990278870539558\n",
      "Epoch 34, Training Loss: 0.7993381087941335\n",
      "Epoch 35, Training Loss: 0.798203004123573\n",
      "Epoch 36, Training Loss: 0.7981430917754209\n",
      "Epoch 37, Training Loss: 0.7982149194057723\n",
      "Epoch 38, Training Loss: 0.7975639109324691\n",
      "Epoch 39, Training Loss: 0.7971974159541882\n",
      "Epoch 40, Training Loss: 0.7972883126789466\n",
      "Epoch 41, Training Loss: 0.7973817213137347\n",
      "Epoch 42, Training Loss: 0.7972721610750471\n",
      "Epoch 43, Training Loss: 0.7968499271493209\n",
      "Epoch 44, Training Loss: 0.7980128247038762\n",
      "Epoch 45, Training Loss: 0.7968879652202577\n",
      "Epoch 46, Training Loss: 0.7967541570950272\n",
      "Epoch 47, Training Loss: 0.7970779903849264\n",
      "Epoch 48, Training Loss: 0.7960717620706199\n",
      "Epoch 49, Training Loss: 0.7965277540056329\n",
      "Epoch 50, Training Loss: 0.7960702962445138\n",
      "Epoch 51, Training Loss: 0.7958565387510715\n",
      "Epoch 52, Training Loss: 0.7962608184133257\n",
      "Epoch 53, Training Loss: 0.7958203694874183\n",
      "Epoch 54, Training Loss: 0.7953116537933063\n",
      "Epoch 55, Training Loss: 0.795190725918103\n",
      "Epoch 56, Training Loss: 0.7951435576704212\n",
      "Epoch 57, Training Loss: 0.7954252916171138\n",
      "Epoch 58, Training Loss: 0.7950042769424898\n",
      "Epoch 59, Training Loss: 0.7954217777216345\n",
      "Epoch 60, Training Loss: 0.7947537926802958\n",
      "Epoch 61, Training Loss: 0.7944793889397069\n",
      "Epoch 62, Training Loss: 0.794270402327516\n",
      "Epoch 63, Training Loss: 0.7943792319835576\n",
      "Epoch 64, Training Loss: 0.7946000812645245\n",
      "Epoch 65, Training Loss: 0.7936384745110246\n",
      "Epoch 66, Training Loss: 0.7938130803574297\n",
      "Epoch 67, Training Loss: 0.793768614933903\n",
      "Epoch 68, Training Loss: 0.793282975917472\n",
      "Epoch 69, Training Loss: 0.7931936255074982\n",
      "Epoch 70, Training Loss: 0.7931518878255571\n",
      "Epoch 71, Training Loss: 0.7929997783854492\n",
      "Epoch 72, Training Loss: 0.7928589289349721\n",
      "Epoch 73, Training Loss: 0.7925015102651782\n",
      "Epoch 74, Training Loss: 0.7924571350104826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:40:04,627] Trial 127 finished with value: 0.6271333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7921003053959151\n",
      "Epoch 1, Training Loss: 0.8718473066064648\n",
      "Epoch 2, Training Loss: 0.8164518279240544\n",
      "Epoch 3, Training Loss: 0.8144470322401004\n",
      "Epoch 4, Training Loss: 0.8119066596927499\n",
      "Epoch 5, Training Loss: 0.8110964043696124\n",
      "Epoch 6, Training Loss: 0.8093585930372539\n",
      "Epoch 7, Training Loss: 0.8087499739532183\n",
      "Epoch 8, Training Loss: 0.806589605037431\n",
      "Epoch 9, Training Loss: 0.8052751948510793\n",
      "Epoch 10, Training Loss: 0.8070332469796776\n",
      "Epoch 11, Training Loss: 0.8061318483567775\n",
      "Epoch 12, Training Loss: 0.8061936378479004\n",
      "Epoch 13, Training Loss: 0.8051774766660275\n",
      "Epoch 14, Training Loss: 0.8054994364430134\n",
      "Epoch 15, Training Loss: 0.8045132743684869\n",
      "Epoch 16, Training Loss: 0.8041042217634674\n",
      "Epoch 17, Training Loss: 0.8042931652606878\n",
      "Epoch 18, Training Loss: 0.8048544241073436\n",
      "Epoch 19, Training Loss: 0.8030534083681895\n",
      "Epoch 20, Training Loss: 0.8028941279963443\n",
      "Epoch 21, Training Loss: 0.8026258604867118\n",
      "Epoch 22, Training Loss: 0.8021969458214322\n",
      "Epoch 23, Training Loss: 0.8017598767029611\n",
      "Epoch 24, Training Loss: 0.8022738397569584\n",
      "Epoch 25, Training Loss: 0.8009801525818674\n",
      "Epoch 26, Training Loss: 0.8028703364214502\n",
      "Epoch 27, Training Loss: 0.8013722649194244\n",
      "Epoch 28, Training Loss: 0.8014395941469006\n",
      "Epoch 29, Training Loss: 0.80133634178262\n",
      "Epoch 30, Training Loss: 0.8013295965983456\n",
      "Epoch 31, Training Loss: 0.8001684311637305\n",
      "Epoch 32, Training Loss: 0.8000564886215038\n",
      "Epoch 33, Training Loss: 0.7996655359304041\n",
      "Epoch 34, Training Loss: 0.7988807385577295\n",
      "Epoch 35, Training Loss: 0.7998118557428059\n",
      "Epoch 36, Training Loss: 0.7990124331381088\n",
      "Epoch 37, Training Loss: 0.7995429275627423\n",
      "Epoch 38, Training Loss: 0.7983975361164352\n",
      "Epoch 39, Training Loss: 0.7983562512505323\n",
      "Epoch 40, Training Loss: 0.7984597390755674\n",
      "Epoch 41, Training Loss: 0.7981211235648707\n",
      "Epoch 42, Training Loss: 0.799035368378001\n",
      "Epoch 43, Training Loss: 0.797188175083103\n",
      "Epoch 44, Training Loss: 0.7976244277523873\n",
      "Epoch 45, Training Loss: 0.7962383727382\n",
      "Epoch 46, Training Loss: 0.796126865982113\n",
      "Epoch 47, Training Loss: 0.79659732499517\n",
      "Epoch 48, Training Loss: 0.7961166115631735\n",
      "Epoch 49, Training Loss: 0.7960249285948904\n",
      "Epoch 50, Training Loss: 0.7957041074458818\n",
      "Epoch 51, Training Loss: 0.7954048144189935\n",
      "Epoch 52, Training Loss: 0.7954153642618567\n",
      "Epoch 53, Training Loss: 0.7950427530403424\n",
      "Epoch 54, Training Loss: 0.794992356730583\n",
      "Epoch 55, Training Loss: 0.7950854877780255\n",
      "Epoch 56, Training Loss: 0.7952994234579847\n",
      "Epoch 57, Training Loss: 0.7952319647136488\n",
      "Epoch 58, Training Loss: 0.7958088098612047\n",
      "Epoch 59, Training Loss: 0.7942654444759053\n",
      "Epoch 60, Training Loss: 0.7933876928530241\n",
      "Epoch 61, Training Loss: 0.793417475546213\n",
      "Epoch 62, Training Loss: 0.7935301563793555\n",
      "Epoch 63, Training Loss: 0.7935388736258772\n",
      "Epoch 64, Training Loss: 0.7947925650983825\n",
      "Epoch 65, Training Loss: 0.79400357922217\n",
      "Epoch 66, Training Loss: 0.7932175336027504\n",
      "Epoch 67, Training Loss: 0.7943488560224834\n",
      "Epoch 68, Training Loss: 0.7932502436458616\n",
      "Epoch 69, Training Loss: 0.7932500087228933\n",
      "Epoch 70, Training Loss: 0.7927545991159023\n",
      "Epoch 71, Training Loss: 0.792269706277919\n",
      "Epoch 72, Training Loss: 0.7921484657696315\n",
      "Epoch 73, Training Loss: 0.7924379646329951\n",
      "Epoch 74, Training Loss: 0.7917986737158066\n",
      "Epoch 75, Training Loss: 0.7920227522240546\n",
      "Epoch 76, Training Loss: 0.7915566815917653\n",
      "Epoch 77, Training Loss: 0.7923482638552672\n",
      "Epoch 78, Training Loss: 0.7913442817845739\n",
      "Epoch 79, Training Loss: 0.7924386531786811\n",
      "Epoch 80, Training Loss: 0.7916652744874022\n",
      "Epoch 81, Training Loss: 0.7929551781568313\n",
      "Epoch 82, Training Loss: 0.7907372432991975\n",
      "Epoch 83, Training Loss: 0.7919218109962636\n",
      "Epoch 84, Training Loss: 0.7911095990274185\n",
      "Epoch 85, Training Loss: 0.7923096921210898\n",
      "Epoch 86, Training Loss: 0.7909213144080083\n",
      "Epoch 87, Training Loss: 0.7925207417710383\n",
      "Epoch 88, Training Loss: 0.7922187294278826\n",
      "Epoch 89, Training Loss: 0.7917777961358092\n",
      "Epoch 90, Training Loss: 0.7924711701565219\n",
      "Epoch 91, Training Loss: 0.791705011962948\n",
      "Epoch 92, Training Loss: 0.7919510470297103\n",
      "Epoch 93, Training Loss: 0.7911539732065416\n",
      "Epoch 94, Training Loss: 0.7917994589733898\n",
      "Epoch 95, Training Loss: 0.7907506240041633\n",
      "Epoch 96, Training Loss: 0.7911275527531043\n",
      "Epoch 97, Training Loss: 0.7907266022567462\n",
      "Epoch 98, Training Loss: 0.7908849732320111\n",
      "Epoch 99, Training Loss: 0.7898875605791135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:41:42,850] Trial 128 finished with value: 0.6365333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7920656863011812\n",
      "Epoch 1, Training Loss: 0.8919632131892039\n",
      "Epoch 2, Training Loss: 0.8165237159657299\n",
      "Epoch 3, Training Loss: 0.8132604957523203\n",
      "Epoch 4, Training Loss: 0.8090269847920066\n",
      "Epoch 5, Training Loss: 0.8089977139817145\n",
      "Epoch 6, Training Loss: 0.8060294397791525\n",
      "Epoch 7, Training Loss: 0.8033519527069608\n",
      "Epoch 8, Training Loss: 0.7997010895184108\n",
      "Epoch 9, Training Loss: 0.7988860430574058\n",
      "Epoch 10, Training Loss: 0.7964834094943857\n",
      "Epoch 11, Training Loss: 0.7955699811304422\n",
      "Epoch 12, Training Loss: 0.7936947443431481\n",
      "Epoch 13, Training Loss: 0.792873397895268\n",
      "Epoch 14, Training Loss: 0.7918607661598607\n",
      "Epoch 15, Training Loss: 0.7921872242949063\n",
      "Epoch 16, Training Loss: 0.7913204294398315\n",
      "Epoch 17, Training Loss: 0.7907468156707018\n",
      "Epoch 18, Training Loss: 0.7895514021242471\n",
      "Epoch 19, Training Loss: 0.7900942220723719\n",
      "Epoch 20, Training Loss: 0.7886604655057864\n",
      "Epoch 21, Training Loss: 0.7889904864748618\n",
      "Epoch 22, Training Loss: 0.7882529785758571\n",
      "Epoch 23, Training Loss: 0.7898604626046087\n",
      "Epoch 24, Training Loss: 0.7876719810908899\n",
      "Epoch 25, Training Loss: 0.7886056834593751\n",
      "Epoch 26, Training Loss: 0.7875230283665478\n",
      "Epoch 27, Training Loss: 0.7874666127047144\n",
      "Epoch 28, Training Loss: 0.7873327034756653\n",
      "Epoch 29, Training Loss: 0.7872310523699997\n",
      "Epoch 30, Training Loss: 0.7874727696404421\n",
      "Epoch 31, Training Loss: 0.7862272152327057\n",
      "Epoch 32, Training Loss: 0.7864317101643498\n",
      "Epoch 33, Training Loss: 0.7862236288257112\n",
      "Epoch 34, Training Loss: 0.7862875837132447\n",
      "Epoch 35, Training Loss: 0.7854308036036958\n",
      "Epoch 36, Training Loss: 0.7855097572606309\n",
      "Epoch 37, Training Loss: 0.7861800457301893\n",
      "Epoch 38, Training Loss: 0.7856251209302056\n",
      "Epoch 39, Training Loss: 0.785474637845405\n",
      "Epoch 40, Training Loss: 0.7858499518910745\n",
      "Epoch 41, Training Loss: 0.7849963525184115\n",
      "Epoch 42, Training Loss: 0.7854073865073068\n",
      "Epoch 43, Training Loss: 0.7849919562949274\n",
      "Epoch 44, Training Loss: 0.784001110102001\n",
      "Epoch 45, Training Loss: 0.7849970984279662\n",
      "Epoch 46, Training Loss: 0.784792319186648\n",
      "Epoch 47, Training Loss: 0.7839892968199307\n",
      "Epoch 48, Training Loss: 0.7848161076244555\n",
      "Epoch 49, Training Loss: 0.7846186973994835\n",
      "Epoch 50, Training Loss: 0.784127292149049\n",
      "Epoch 51, Training Loss: 0.7847044818383411\n",
      "Epoch 52, Training Loss: 0.7846823102549503\n",
      "Epoch 53, Training Loss: 0.7842759688994042\n",
      "Epoch 54, Training Loss: 0.783780554810861\n",
      "Epoch 55, Training Loss: 0.7829060148475762\n",
      "Epoch 56, Training Loss: 0.7829421378616104\n",
      "Epoch 57, Training Loss: 0.7832412746615875\n",
      "Epoch 58, Training Loss: 0.7826064142965733\n",
      "Epoch 59, Training Loss: 0.7825930984396684\n",
      "Epoch 60, Training Loss: 0.7833404395813333\n",
      "Epoch 61, Training Loss: 0.7828856398288468\n",
      "Epoch 62, Training Loss: 0.7824884847142642\n",
      "Epoch 63, Training Loss: 0.7823720674765737\n",
      "Epoch 64, Training Loss: 0.7826271651382734\n",
      "Epoch 65, Training Loss: 0.7825741567109761\n",
      "Epoch 66, Training Loss: 0.782175566558551\n",
      "Epoch 67, Training Loss: 0.7820046923214332\n",
      "Epoch 68, Training Loss: 0.782659609873492\n",
      "Epoch 69, Training Loss: 0.7825539864991841\n",
      "Epoch 70, Training Loss: 0.7819730644835565\n",
      "Epoch 71, Training Loss: 0.7816565707213896\n",
      "Epoch 72, Training Loss: 0.7818855775030036\n",
      "Epoch 73, Training Loss: 0.781453034528216\n",
      "Epoch 74, Training Loss: 0.781264114828038\n",
      "Epoch 75, Training Loss: 0.7813114386752136\n",
      "Epoch 76, Training Loss: 0.7807263033730644\n",
      "Epoch 77, Training Loss: 0.7816020702060901\n",
      "Epoch 78, Training Loss: 0.7821867694532064\n",
      "Epoch 79, Training Loss: 0.7807569717106067\n",
      "Epoch 80, Training Loss: 0.7818074649437926\n",
      "Epoch 81, Training Loss: 0.7816649199428415\n",
      "Epoch 82, Training Loss: 0.7816307023055571\n",
      "Epoch 83, Training Loss: 0.7818675978739459\n",
      "Epoch 84, Training Loss: 0.7810233973919001\n",
      "Epoch 85, Training Loss: 0.7811057478861702\n",
      "Epoch 86, Training Loss: 0.7808698015105455\n",
      "Epoch 87, Training Loss: 0.7810054705555277\n",
      "Epoch 88, Training Loss: 0.780859865461077\n",
      "Epoch 89, Training Loss: 0.779745021888188\n",
      "Epoch 90, Training Loss: 0.7812856491347004\n",
      "Epoch 91, Training Loss: 0.7805432078533603\n",
      "Epoch 92, Training Loss: 0.7804458068725758\n",
      "Epoch 93, Training Loss: 0.7810812831821298\n",
      "Epoch 94, Training Loss: 0.780687577832014\n",
      "Epoch 95, Training Loss: 0.7801809471352656\n",
      "Epoch 96, Training Loss: 0.7797670950566916\n",
      "Epoch 97, Training Loss: 0.7800839562165109\n",
      "Epoch 98, Training Loss: 0.7792983642198089\n",
      "Epoch 99, Training Loss: 0.7793064575446279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:43:37,667] Trial 129 finished with value: 0.6382666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7800573419807548\n",
      "Epoch 1, Training Loss: 1.0989000671788265\n",
      "Epoch 2, Training Loss: 1.0825334111550697\n",
      "Epoch 3, Training Loss: 1.0758107101110588\n",
      "Epoch 4, Training Loss: 1.0695891900170118\n",
      "Epoch 5, Training Loss: 1.0639969816781525\n",
      "Epoch 6, Training Loss: 1.0587816887332084\n",
      "Epoch 7, Training Loss: 1.0539150670058746\n",
      "Epoch 8, Training Loss: 1.0494348024067126\n",
      "Epoch 9, Training Loss: 1.0451384573054492\n",
      "Epoch 10, Training Loss: 1.0411220889342458\n",
      "Epoch 11, Training Loss: 1.037133424801934\n",
      "Epoch 12, Training Loss: 1.0335972100272215\n",
      "Epoch 13, Training Loss: 1.0299489073287276\n",
      "Epoch 14, Training Loss: 1.026600864507202\n",
      "Epoch 15, Training Loss: 1.0235178863195549\n",
      "Epoch 16, Training Loss: 1.020476005758558\n",
      "Epoch 17, Training Loss: 1.0174332007429654\n",
      "Epoch 18, Training Loss: 1.014533551653525\n",
      "Epoch 19, Training Loss: 1.0116998781835227\n",
      "Epoch 20, Training Loss: 1.0093152028277403\n",
      "Epoch 21, Training Loss: 1.0068625899185812\n",
      "Epoch 22, Training Loss: 1.0045689579239465\n",
      "Epoch 23, Training Loss: 1.0021232576298535\n",
      "Epoch 24, Training Loss: 0.9999902187433458\n",
      "Epoch 25, Training Loss: 0.9978047807413832\n",
      "Epoch 26, Training Loss: 0.9957159382956369\n",
      "Epoch 27, Training Loss: 0.9939110585621425\n",
      "Epoch 28, Training Loss: 0.9920153490582803\n",
      "Epoch 29, Training Loss: 0.9905396017813145\n",
      "Epoch 30, Training Loss: 0.988544790099438\n",
      "Epoch 31, Training Loss: 0.9871699748182655\n",
      "Epoch 32, Training Loss: 0.985621730725568\n",
      "Epoch 33, Training Loss: 0.9841418340690153\n",
      "Epoch 34, Training Loss: 0.9830663490116148\n",
      "Epoch 35, Training Loss: 0.9811757072470242\n",
      "Epoch 36, Training Loss: 0.9802714319157421\n",
      "Epoch 37, Training Loss: 0.9789939679597554\n",
      "Epoch 38, Training Loss: 0.97793393753525\n",
      "Epoch 39, Training Loss: 0.9769978798421702\n",
      "Epoch 40, Training Loss: 0.9760837993227449\n",
      "Epoch 41, Training Loss: 0.9749626580933879\n",
      "Epoch 42, Training Loss: 0.9742249871555128\n",
      "Epoch 43, Training Loss: 0.9730341680964133\n",
      "Epoch 44, Training Loss: 0.9725886978601155\n",
      "Epoch 45, Training Loss: 0.9712877384702066\n",
      "Epoch 46, Training Loss: 0.9706515133829046\n",
      "Epoch 47, Training Loss: 0.9699377215894541\n",
      "Epoch 48, Training Loss: 0.969338774143305\n",
      "Epoch 49, Training Loss: 0.9685514920636227\n",
      "Epoch 50, Training Loss: 0.9678643553776849\n",
      "Epoch 51, Training Loss: 0.967374036545144\n",
      "Epoch 52, Training Loss: 0.9670463855105235\n",
      "Epoch 53, Training Loss: 0.9663544003228496\n",
      "Epoch 54, Training Loss: 0.9658867700655658\n",
      "Epoch 55, Training Loss: 0.9655223095327392\n",
      "Epoch 56, Training Loss: 0.9649213855427907\n",
      "Epoch 57, Training Loss: 0.9646872601114718\n",
      "Epoch 58, Training Loss: 0.9642928759854539\n",
      "Epoch 59, Training Loss: 0.96385374338107\n",
      "Epoch 60, Training Loss: 0.963666504218166\n",
      "Epoch 61, Training Loss: 0.9628849589735046\n",
      "Epoch 62, Training Loss: 0.962803724475373\n",
      "Epoch 63, Training Loss: 0.9622933041780515\n",
      "Epoch 64, Training Loss: 0.9623008942245541\n",
      "Epoch 65, Training Loss: 0.9619066643535643\n",
      "Epoch 66, Training Loss: 0.96131409836891\n",
      "Epoch 67, Training Loss: 0.9612399914210901\n",
      "Epoch 68, Training Loss: 0.9612653578134408\n",
      "Epoch 69, Training Loss: 0.9604877204823314\n",
      "Epoch 70, Training Loss: 0.9606027396998011\n",
      "Epoch 71, Training Loss: 0.9605654551570577\n",
      "Epoch 72, Training Loss: 0.9602138190341175\n",
      "Epoch 73, Training Loss: 0.9595213515417916\n",
      "Epoch 74, Training Loss: 0.9596748187129659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:44:40,327] Trial 130 finished with value: 0.5277333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9594854366510435\n",
      "Epoch 1, Training Loss: 1.0110442680523808\n",
      "Epoch 2, Training Loss: 0.9383564677453579\n",
      "Epoch 3, Training Loss: 0.9225870642446934\n",
      "Epoch 4, Training Loss: 0.9095726642393528\n",
      "Epoch 5, Training Loss: 0.891806565639668\n",
      "Epoch 6, Training Loss: 0.8663801541902069\n",
      "Epoch 7, Training Loss: 0.8391766473762972\n",
      "Epoch 8, Training Loss: 0.8202543416417631\n",
      "Epoch 9, Training Loss: 0.8115033964465436\n",
      "Epoch 10, Training Loss: 0.8073149118208347\n",
      "Epoch 11, Training Loss: 0.8054051422535029\n",
      "Epoch 12, Training Loss: 0.804947018533721\n",
      "Epoch 13, Training Loss: 0.8033476471004629\n",
      "Epoch 14, Training Loss: 0.802109776582933\n",
      "Epoch 15, Training Loss: 0.8009407108887694\n",
      "Epoch 16, Training Loss: 0.801105244715411\n",
      "Epoch 17, Training Loss: 0.8000830414599942\n",
      "Epoch 18, Training Loss: 0.8007446738114035\n",
      "Epoch 19, Training Loss: 0.7995086012030006\n",
      "Epoch 20, Training Loss: 0.7984965577161401\n",
      "Epoch 21, Training Loss: 0.7980521052403557\n",
      "Epoch 22, Training Loss: 0.7975794275900475\n",
      "Epoch 23, Training Loss: 0.7974509357509757\n",
      "Epoch 24, Training Loss: 0.7966782405860442\n",
      "Epoch 25, Training Loss: 0.7960755522089793\n",
      "Epoch 26, Training Loss: 0.7966508092736839\n",
      "Epoch 27, Training Loss: 0.7969088095471375\n",
      "Epoch 28, Training Loss: 0.7968444491687574\n",
      "Epoch 29, Training Loss: 0.7952084855925768\n",
      "Epoch 30, Training Loss: 0.7958122599393802\n",
      "Epoch 31, Training Loss: 0.7941976122390059\n",
      "Epoch 32, Training Loss: 0.7943658770475173\n",
      "Epoch 33, Training Loss: 0.7946612528392247\n",
      "Epoch 34, Training Loss: 0.7930782286744369\n",
      "Epoch 35, Training Loss: 0.7932616541260167\n",
      "Epoch 36, Training Loss: 0.7927440365454308\n",
      "Epoch 37, Training Loss: 0.7926054746584785\n",
      "Epoch 38, Training Loss: 0.7922898139272417\n",
      "Epoch 39, Training Loss: 0.7916617695550273\n",
      "Epoch 40, Training Loss: 0.7913998977582257\n",
      "Epoch 41, Training Loss: 0.7907178461999822\n",
      "Epoch 42, Training Loss: 0.7911818194210081\n",
      "Epoch 43, Training Loss: 0.790414960223033\n",
      "Epoch 44, Training Loss: 0.7904408706758256\n",
      "Epoch 45, Training Loss: 0.790157875889226\n",
      "Epoch 46, Training Loss: 0.7896663546562195\n",
      "Epoch 47, Training Loss: 0.7889709353446961\n",
      "Epoch 48, Training Loss: 0.7896781748398802\n",
      "Epoch 49, Training Loss: 0.7884601325020755\n",
      "Epoch 50, Training Loss: 0.7882525393837376\n",
      "Epoch 51, Training Loss: 0.7886814220507342\n",
      "Epoch 52, Training Loss: 0.7875589062396745\n",
      "Epoch 53, Training Loss: 0.7879832168270771\n",
      "Epoch 54, Training Loss: 0.7881755695307165\n",
      "Epoch 55, Training Loss: 0.7878826511533636\n",
      "Epoch 56, Training Loss: 0.7879471564651432\n",
      "Epoch 57, Training Loss: 0.7867748816210525\n",
      "Epoch 58, Training Loss: 0.7871178952374853\n",
      "Epoch 59, Training Loss: 0.7873105123527068\n",
      "Epoch 60, Training Loss: 0.7868674644850251\n",
      "Epoch 61, Training Loss: 0.7866701056186418\n",
      "Epoch 62, Training Loss: 0.7866375050150363\n",
      "Epoch 63, Training Loss: 0.7865668109485081\n",
      "Epoch 64, Training Loss: 0.7862491261690183\n",
      "Epoch 65, Training Loss: 0.7865900420604792\n",
      "Epoch 66, Training Loss: 0.7858155564257973\n",
      "Epoch 67, Training Loss: 0.7856601136967651\n",
      "Epoch 68, Training Loss: 0.7855793176737047\n",
      "Epoch 69, Training Loss: 0.7855315458505674\n",
      "Epoch 70, Training Loss: 0.7858295723011619\n",
      "Epoch 71, Training Loss: 0.7848633299196573\n",
      "Epoch 72, Training Loss: 0.7846004234220748\n",
      "Epoch 73, Training Loss: 0.7844006850755304\n",
      "Epoch 74, Training Loss: 0.7846575729829028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:45:49,987] Trial 131 finished with value: 0.6172666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7848868828070791\n",
      "Epoch 1, Training Loss: 0.870964080767524\n",
      "Epoch 2, Training Loss: 0.8177551098335955\n",
      "Epoch 3, Training Loss: 0.8136170087004067\n",
      "Epoch 4, Training Loss: 0.8105175617942236\n",
      "Epoch 5, Training Loss: 0.8098440425736564\n",
      "Epoch 6, Training Loss: 0.807621803498806\n",
      "Epoch 7, Training Loss: 0.8063059068263921\n",
      "Epoch 8, Training Loss: 0.8058292990340326\n",
      "Epoch 9, Training Loss: 0.8038432212700521\n",
      "Epoch 10, Training Loss: 0.8033937884452648\n",
      "Epoch 11, Training Loss: 0.8031561725121692\n",
      "Epoch 12, Training Loss: 0.8016237560967754\n",
      "Epoch 13, Training Loss: 0.8012261276854609\n",
      "Epoch 14, Training Loss: 0.8012065715359565\n",
      "Epoch 15, Training Loss: 0.8007364263211875\n",
      "Epoch 16, Training Loss: 0.8004478479686536\n",
      "Epoch 17, Training Loss: 0.8001817112578485\n",
      "Epoch 18, Training Loss: 0.8000503606366035\n",
      "Epoch 19, Training Loss: 0.8001478040128722\n",
      "Epoch 20, Training Loss: 0.7989208420416466\n",
      "Epoch 21, Training Loss: 0.7984200388865363\n",
      "Epoch 22, Training Loss: 0.7984963428705258\n",
      "Epoch 23, Training Loss: 0.7974524414628968\n",
      "Epoch 24, Training Loss: 0.79809551696132\n",
      "Epoch 25, Training Loss: 0.7975956315384771\n",
      "Epoch 26, Training Loss: 0.7975579619407653\n",
      "Epoch 27, Training Loss: 0.7969594941999679\n",
      "Epoch 28, Training Loss: 0.7955946242002616\n",
      "Epoch 29, Training Loss: 0.7952629566192627\n",
      "Epoch 30, Training Loss: 0.794966865571818\n",
      "Epoch 31, Training Loss: 0.7942317838955643\n",
      "Epoch 32, Training Loss: 0.7941365782479595\n",
      "Epoch 33, Training Loss: 0.7922391368930501\n",
      "Epoch 34, Training Loss: 0.7921015232129205\n",
      "Epoch 35, Training Loss: 0.7912732155699479\n",
      "Epoch 36, Training Loss: 0.7900961236846178\n",
      "Epoch 37, Training Loss: 0.7891888272493406\n",
      "Epoch 38, Training Loss: 0.7889331984340696\n",
      "Epoch 39, Training Loss: 0.7890820434218959\n",
      "Epoch 40, Training Loss: 0.7886728483931462\n",
      "Epoch 41, Training Loss: 0.7881083981435102\n",
      "Epoch 42, Training Loss: 0.7882943337124989\n",
      "Epoch 43, Training Loss: 0.7875961901550006\n",
      "Epoch 44, Training Loss: 0.7866777945281868\n",
      "Epoch 45, Training Loss: 0.7863443292173228\n",
      "Epoch 46, Training Loss: 0.7865403473825383\n",
      "Epoch 47, Training Loss: 0.7859199888724133\n",
      "Epoch 48, Training Loss: 0.7855362424276825\n",
      "Epoch 49, Training Loss: 0.7854218934711658\n",
      "Epoch 50, Training Loss: 0.7855997025518489\n",
      "Epoch 51, Training Loss: 0.7854346582764073\n",
      "Epoch 52, Training Loss: 0.7846631535013816\n",
      "Epoch 53, Training Loss: 0.7846035898179936\n",
      "Epoch 54, Training Loss: 0.7848360039237746\n",
      "Epoch 55, Training Loss: 0.7838751942143405\n",
      "Epoch 56, Training Loss: 0.783833030291966\n",
      "Epoch 57, Training Loss: 0.7843321225696936\n",
      "Epoch 58, Training Loss: 0.7842039555535281\n",
      "Epoch 59, Training Loss: 0.7837370163515994\n",
      "Epoch 60, Training Loss: 0.7837050307962231\n",
      "Epoch 61, Training Loss: 0.7837604378399096\n",
      "Epoch 62, Training Loss: 0.7833312704150838\n",
      "Epoch 63, Training Loss: 0.7836771317890712\n",
      "Epoch 64, Training Loss: 0.783117707689902\n",
      "Epoch 65, Training Loss: 0.7840192491846874\n",
      "Epoch 66, Training Loss: 0.7828818809717222\n",
      "Epoch 67, Training Loss: 0.7830745455913974\n",
      "Epoch 68, Training Loss: 0.7828327947541287\n",
      "Epoch 69, Training Loss: 0.7832312199406157\n",
      "Epoch 70, Training Loss: 0.7831800992327526\n",
      "Epoch 71, Training Loss: 0.7834110802277586\n",
      "Epoch 72, Training Loss: 0.7831065475492549\n",
      "Epoch 73, Training Loss: 0.78312837548722\n",
      "Epoch 74, Training Loss: 0.7824233338348847\n",
      "Epoch 75, Training Loss: 0.7832626460190106\n",
      "Epoch 76, Training Loss: 0.7828191735690697\n",
      "Epoch 77, Training Loss: 0.7826466044985262\n",
      "Epoch 78, Training Loss: 0.7830758891607585\n",
      "Epoch 79, Training Loss: 0.7821850597410274\n",
      "Epoch 80, Training Loss: 0.7820077599439406\n",
      "Epoch 81, Training Loss: 0.7823806152307897\n",
      "Epoch 82, Training Loss: 0.7824065879771583\n",
      "Epoch 83, Training Loss: 0.7822185695619511\n",
      "Epoch 84, Training Loss: 0.7818371929620441\n",
      "Epoch 85, Training Loss: 0.7829030434888108\n",
      "Epoch 86, Training Loss: 0.7822500589198635\n",
      "Epoch 87, Training Loss: 0.7818939645487563\n",
      "Epoch 88, Training Loss: 0.7820320231573922\n",
      "Epoch 89, Training Loss: 0.7826509127043244\n",
      "Epoch 90, Training Loss: 0.781585255063566\n",
      "Epoch 91, Training Loss: 0.7818347774053874\n",
      "Epoch 92, Training Loss: 0.7821497238668284\n",
      "Epoch 93, Training Loss: 0.7815728185768415\n",
      "Epoch 94, Training Loss: 0.7813546412421348\n",
      "Epoch 95, Training Loss: 0.7823318776331449\n",
      "Epoch 96, Training Loss: 0.781882243317769\n",
      "Epoch 97, Training Loss: 0.7813368519446007\n",
      "Epoch 98, Training Loss: 0.7822944804241783\n",
      "Epoch 99, Training Loss: 0.7811044199574263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:47:40,103] Trial 132 finished with value: 0.6374 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7813137252527969\n",
      "Epoch 1, Training Loss: 0.8392174176608815\n",
      "Epoch 2, Training Loss: 0.8084900223507601\n",
      "Epoch 3, Training Loss: 0.8031422204129837\n",
      "Epoch 4, Training Loss: 0.8019582985429203\n",
      "Epoch 5, Training Loss: 0.7988957194019767\n",
      "Epoch 6, Training Loss: 0.7984986507191377\n",
      "Epoch 7, Training Loss: 0.7979266956974478\n",
      "Epoch 8, Training Loss: 0.796694263640572\n",
      "Epoch 9, Training Loss: 0.7950375489627614\n",
      "Epoch 10, Training Loss: 0.7955441608849694\n",
      "Epoch 11, Training Loss: 0.7952738566959605\n",
      "Epoch 12, Training Loss: 0.7953156132557813\n",
      "Epoch 13, Training Loss: 0.7938909654757556\n",
      "Epoch 14, Training Loss: 0.7938946427317226\n",
      "Epoch 15, Training Loss: 0.7937793128630695\n",
      "Epoch 16, Training Loss: 0.7941918406065772\n",
      "Epoch 17, Training Loss: 0.7936146342754364\n",
      "Epoch 18, Training Loss: 0.7928151515652152\n",
      "Epoch 19, Training Loss: 0.7931428526429569\n",
      "Epoch 20, Training Loss: 0.7923557153870078\n",
      "Epoch 21, Training Loss: 0.7925314459379982\n",
      "Epoch 22, Training Loss: 0.7928794909224791\n",
      "Epoch 23, Training Loss: 0.7928722870349884\n",
      "Epoch 24, Training Loss: 0.791731600410798\n",
      "Epoch 25, Training Loss: 0.7930449742429396\n",
      "Epoch 26, Training Loss: 0.7903536251012017\n",
      "Epoch 27, Training Loss: 0.790766265111811\n",
      "Epoch 28, Training Loss: 0.7911769066137426\n",
      "Epoch 29, Training Loss: 0.7922808247454026\n",
      "Epoch 30, Training Loss: 0.7909160434498507\n",
      "Epoch 31, Training Loss: 0.7909910955148585\n",
      "Epoch 32, Training Loss: 0.7920241278760574\n",
      "Epoch 33, Training Loss: 0.7911321314643411\n",
      "Epoch 34, Training Loss: 0.7898531260209926\n",
      "Epoch 35, Training Loss: 0.7911137077387641\n",
      "Epoch 36, Training Loss: 0.7911455770099864\n",
      "Epoch 37, Training Loss: 0.7908521467096665\n",
      "Epoch 38, Training Loss: 0.790176769915749\n",
      "Epoch 39, Training Loss: 0.790552133251639\n",
      "Epoch 40, Training Loss: 0.7900513193887823\n",
      "Epoch 41, Training Loss: 0.7903714941529667\n",
      "Epoch 42, Training Loss: 0.790035725551493\n",
      "Epoch 43, Training Loss: 0.7900314639596379\n",
      "Epoch 44, Training Loss: 0.7906001480186686\n",
      "Epoch 45, Training Loss: 0.7901308263049406\n",
      "Epoch 46, Training Loss: 0.7899143407625311\n",
      "Epoch 47, Training Loss: 0.7895105503587162\n",
      "Epoch 48, Training Loss: 0.7902000990334679\n",
      "Epoch 49, Training Loss: 0.7893944885450251\n",
      "Epoch 50, Training Loss: 0.7892886632330277\n",
      "Epoch 51, Training Loss: 0.7892079440986409\n",
      "Epoch 52, Training Loss: 0.7900731560763191\n",
      "Epoch 53, Training Loss: 0.7891332802351784\n",
      "Epoch 54, Training Loss: 0.7895063797866597\n",
      "Epoch 55, Training Loss: 0.7889781660192153\n",
      "Epoch 56, Training Loss: 0.7893343789437238\n",
      "Epoch 57, Training Loss: 0.7886157196409562\n",
      "Epoch 58, Training Loss: 0.7890305454590741\n",
      "Epoch 59, Training Loss: 0.789799245595932\n",
      "Epoch 60, Training Loss: 0.788613069337957\n",
      "Epoch 61, Training Loss: 0.7893466560980853\n",
      "Epoch 62, Training Loss: 0.7891115881414974\n",
      "Epoch 63, Training Loss: 0.7885854582225575\n",
      "Epoch 64, Training Loss: 0.7897880645359263\n",
      "Epoch 65, Training Loss: 0.7894506161353168\n",
      "Epoch 66, Training Loss: 0.7884079187757829\n",
      "Epoch 67, Training Loss: 0.7880837421557483\n",
      "Epoch 68, Training Loss: 0.7889969622387606\n",
      "Epoch 69, Training Loss: 0.7885126317248625\n",
      "Epoch 70, Training Loss: 0.7881724097448237\n",
      "Epoch 71, Training Loss: 0.7882422724190881\n",
      "Epoch 72, Training Loss: 0.7892295317790088\n",
      "Epoch 73, Training Loss: 0.7888796052512\n",
      "Epoch 74, Training Loss: 0.7888763977499569\n",
      "Epoch 75, Training Loss: 0.7884144650487339\n",
      "Epoch 76, Training Loss: 0.7881219040646272\n",
      "Epoch 77, Training Loss: 0.7888068231414346\n",
      "Epoch 78, Training Loss: 0.7872418318776523\n",
      "Epoch 79, Training Loss: 0.7885719355414895\n",
      "Epoch 80, Training Loss: 0.7885362393014571\n",
      "Epoch 81, Training Loss: 0.7880307673005497\n",
      "Epoch 82, Training Loss: 0.7881661413697636\n",
      "Epoch 83, Training Loss: 0.7887340715352227\n",
      "Epoch 84, Training Loss: 0.7881838532756357\n",
      "Epoch 85, Training Loss: 0.7867680442333221\n",
      "Epoch 86, Training Loss: 0.7879502788010766\n",
      "Epoch 87, Training Loss: 0.7878844545869266\n",
      "Epoch 88, Training Loss: 0.7873761834817774\n",
      "Epoch 89, Training Loss: 0.7876529005695792\n",
      "Epoch 90, Training Loss: 0.7885561897474177\n",
      "Epoch 91, Training Loss: 0.7882660429617938\n",
      "Epoch 92, Training Loss: 0.7880271354843589\n",
      "Epoch 93, Training Loss: 0.7885340137341443\n",
      "Epoch 94, Training Loss: 0.7883078955902773\n",
      "Epoch 95, Training Loss: 0.787398964657503\n",
      "Epoch 96, Training Loss: 0.7879790587284986\n",
      "Epoch 97, Training Loss: 0.7881514983317431\n",
      "Epoch 98, Training Loss: 0.7877886665568632\n",
      "Epoch 99, Training Loss: 0.7871219827848323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:49:56,008] Trial 133 finished with value: 0.6394 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7871099736410029\n",
      "Epoch 1, Training Loss: 1.0594001021805932\n",
      "Epoch 2, Training Loss: 1.015247434868532\n",
      "Epoch 3, Training Loss: 0.9881926394911373\n",
      "Epoch 4, Training Loss: 0.9726183460740482\n",
      "Epoch 5, Training Loss: 0.9643742698781631\n",
      "Epoch 6, Training Loss: 0.9598143937307245\n",
      "Epoch 7, Training Loss: 0.9573127903657801\n",
      "Epoch 8, Training Loss: 0.9554248588926652\n",
      "Epoch 9, Training Loss: 0.9539128047578475\n",
      "Epoch 10, Training Loss: 0.9525948193494012\n",
      "Epoch 11, Training Loss: 0.9513337702610913\n",
      "Epoch 12, Training Loss: 0.9500754621449639\n",
      "Epoch 13, Training Loss: 0.9487151799482457\n",
      "Epoch 14, Training Loss: 0.947376759262646\n",
      "Epoch 15, Training Loss: 0.9460913024229162\n",
      "Epoch 16, Training Loss: 0.9446772313117981\n",
      "Epoch 17, Training Loss: 0.9432053438354941\n",
      "Epoch 18, Training Loss: 0.9417641257538515\n",
      "Epoch 19, Training Loss: 0.9401678078314837\n",
      "Epoch 20, Training Loss: 0.9385574124139898\n",
      "Epoch 21, Training Loss: 0.9369651814769296\n",
      "Epoch 22, Training Loss: 0.9353941426557653\n",
      "Epoch 23, Training Loss: 0.9336598757435294\n",
      "Epoch 24, Training Loss: 0.9319316899776459\n",
      "Epoch 25, Training Loss: 0.9300876994693981\n",
      "Epoch 26, Training Loss: 0.9282190219795002\n",
      "Epoch 27, Training Loss: 0.9263257877265706\n",
      "Epoch 28, Training Loss: 0.9243757063501021\n",
      "Epoch 29, Training Loss: 0.9223526839648976\n",
      "Epoch 30, Training Loss: 0.9202626006042256\n",
      "Epoch 31, Training Loss: 0.9181859916799209\n",
      "Epoch 32, Training Loss: 0.9159906000950757\n",
      "Epoch 33, Training Loss: 0.9137689555392546\n",
      "Epoch 34, Training Loss: 0.9116250755506403\n",
      "Epoch 35, Training Loss: 0.9092992157094619\n",
      "Epoch 36, Training Loss: 0.9070010249754962\n",
      "Epoch 37, Training Loss: 0.9045998974407421\n",
      "Epoch 38, Training Loss: 0.9021665132045746\n",
      "Epoch 39, Training Loss: 0.8996978757661932\n",
      "Epoch 40, Training Loss: 0.8973342709681567\n",
      "Epoch 41, Training Loss: 0.8949028320172254\n",
      "Epoch 42, Training Loss: 0.8923708265669206\n",
      "Epoch 43, Training Loss: 0.8898903188284706\n",
      "Epoch 44, Training Loss: 0.8873792869904462\n",
      "Epoch 45, Training Loss: 0.884882234545315\n",
      "Epoch 46, Training Loss: 0.8823315116237191\n",
      "Epoch 47, Training Loss: 0.8799275680850533\n",
      "Epoch 48, Training Loss: 0.8773913143662846\n",
      "Epoch 49, Training Loss: 0.8750397459198447\n",
      "Epoch 50, Training Loss: 0.8726032951298882\n",
      "Epoch 51, Training Loss: 0.8701560828265021\n",
      "Epoch 52, Training Loss: 0.8678098803408005\n",
      "Epoch 53, Training Loss: 0.8655081613624798\n",
      "Epoch 54, Training Loss: 0.863260329541038\n",
      "Epoch 55, Training Loss: 0.8609381212907679\n",
      "Epoch 56, Training Loss: 0.8588313436508179\n",
      "Epoch 57, Training Loss: 0.8566770063428317\n",
      "Epoch 58, Training Loss: 0.8545147547301124\n",
      "Epoch 59, Training Loss: 0.8525657054255991\n",
      "Epoch 60, Training Loss: 0.8506088078022003\n",
      "Epoch 61, Training Loss: 0.8486591032673331\n",
      "Epoch 62, Training Loss: 0.8469013638356153\n",
      "Epoch 63, Training Loss: 0.8450834889972911\n",
      "Epoch 64, Training Loss: 0.8433001885694617\n",
      "Epoch 65, Training Loss: 0.8416232204437256\n",
      "Epoch 66, Training Loss: 0.8400119411945343\n",
      "Epoch 67, Training Loss: 0.8384406941778519\n",
      "Epoch 68, Training Loss: 0.8370033365137437\n",
      "Epoch 69, Training Loss: 0.8356346199091743\n",
      "Epoch 70, Training Loss: 0.8342362252403708\n",
      "Epoch 71, Training Loss: 0.8329047866428599\n",
      "Epoch 72, Training Loss: 0.8316353198359995\n",
      "Epoch 73, Training Loss: 0.8304517578377443\n",
      "Epoch 74, Training Loss: 0.8293318066877478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:51:07,079] Trial 134 finished with value: 0.6186 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8283114095295177\n",
      "Epoch 1, Training Loss: 0.9899163404801734\n",
      "Epoch 2, Training Loss: 0.933521244490057\n",
      "Epoch 3, Training Loss: 0.8920027529386649\n",
      "Epoch 4, Training Loss: 0.8434228848693962\n",
      "Epoch 5, Training Loss: 0.8194274768793494\n",
      "Epoch 6, Training Loss: 0.8134270160718071\n",
      "Epoch 7, Training Loss: 0.8112446074198959\n",
      "Epoch 8, Training Loss: 0.8096746616793754\n",
      "Epoch 9, Training Loss: 0.8091809274558734\n",
      "Epoch 10, Training Loss: 0.8079428054336318\n",
      "Epoch 11, Training Loss: 0.8077789459013401\n",
      "Epoch 12, Training Loss: 0.8068246033854951\n",
      "Epoch 13, Training Loss: 0.806935179323182\n",
      "Epoch 14, Training Loss: 0.8051439643802499\n",
      "Epoch 15, Training Loss: 0.8054312827891873\n",
      "Epoch 16, Training Loss: 0.8051986881664821\n",
      "Epoch 17, Training Loss: 0.8034759597670763\n",
      "Epoch 18, Training Loss: 0.8039213534584619\n",
      "Epoch 19, Training Loss: 0.8037040498919953\n",
      "Epoch 20, Training Loss: 0.8029002599250105\n",
      "Epoch 21, Training Loss: 0.8025892017479229\n",
      "Epoch 22, Training Loss: 0.802944198497256\n",
      "Epoch 23, Training Loss: 0.8027305729407117\n",
      "Epoch 24, Training Loss: 0.8030575484261477\n",
      "Epoch 25, Training Loss: 0.8023953625134059\n",
      "Epoch 26, Training Loss: 0.8016871782173788\n",
      "Epoch 27, Training Loss: 0.8021966949441379\n",
      "Epoch 28, Training Loss: 0.8020729881480224\n",
      "Epoch 29, Training Loss: 0.8010205811127684\n",
      "Epoch 30, Training Loss: 0.801495993944039\n",
      "Epoch 31, Training Loss: 0.8006337859576806\n",
      "Epoch 32, Training Loss: 0.8005128406940546\n",
      "Epoch 33, Training Loss: 0.8008956899320273\n",
      "Epoch 34, Training Loss: 0.8002500420226191\n",
      "Epoch 35, Training Loss: 0.800041997791233\n",
      "Epoch 36, Training Loss: 0.8018238319490189\n",
      "Epoch 37, Training Loss: 0.8006479067013677\n",
      "Epoch 38, Training Loss: 0.80144099418382\n",
      "Epoch 39, Training Loss: 0.8003066222470506\n",
      "Epoch 40, Training Loss: 0.7995689774814405\n",
      "Epoch 41, Training Loss: 0.8002986014337468\n",
      "Epoch 42, Training Loss: 0.8000659232749079\n",
      "Epoch 43, Training Loss: 0.8000352332466527\n",
      "Epoch 44, Training Loss: 0.7992155060732276\n",
      "Epoch 45, Training Loss: 0.7991213988540764\n",
      "Epoch 46, Training Loss: 0.799507204661692\n",
      "Epoch 47, Training Loss: 0.7988841472711778\n",
      "Epoch 48, Training Loss: 0.7983306813060789\n",
      "Epoch 49, Training Loss: 0.7988457899344595\n",
      "Epoch 50, Training Loss: 0.7986516620879783\n",
      "Epoch 51, Training Loss: 0.7983505279497993\n",
      "Epoch 52, Training Loss: 0.7984107685268373\n",
      "Epoch 53, Training Loss: 0.7978976431645846\n",
      "Epoch 54, Training Loss: 0.7981116930344947\n",
      "Epoch 55, Training Loss: 0.7975055765388603\n",
      "Epoch 56, Training Loss: 0.7977757973778516\n",
      "Epoch 57, Training Loss: 0.797400206641147\n",
      "Epoch 58, Training Loss: 0.7974364588135168\n",
      "Epoch 59, Training Loss: 0.7973027345381285\n",
      "Epoch 60, Training Loss: 0.7978521615042722\n",
      "Epoch 61, Training Loss: 0.7972247745309557\n",
      "Epoch 62, Training Loss: 0.796957942865845\n",
      "Epoch 63, Training Loss: 0.797700007517535\n",
      "Epoch 64, Training Loss: 0.7968689782278878\n",
      "Epoch 65, Training Loss: 0.7972938459618647\n",
      "Epoch 66, Training Loss: 0.7968025453108594\n",
      "Epoch 67, Training Loss: 0.7970199733748472\n",
      "Epoch 68, Training Loss: 0.7966145442840749\n",
      "Epoch 69, Training Loss: 0.7973582420134007\n",
      "Epoch 70, Training Loss: 0.7971387947412362\n",
      "Epoch 71, Training Loss: 0.7973776013331306\n",
      "Epoch 72, Training Loss: 0.7979795415598647\n",
      "Epoch 73, Training Loss: 0.7963167886088666\n",
      "Epoch 74, Training Loss: 0.796339075637043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:52:35,488] Trial 135 finished with value: 0.6320666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7958914419762174\n",
      "Epoch 1, Training Loss: 1.0945998790568876\n",
      "Epoch 2, Training Loss: 1.09041561883195\n",
      "Epoch 3, Training Loss: 1.0901995866818535\n",
      "Epoch 4, Training Loss: 1.0899872564731683\n",
      "Epoch 5, Training Loss: 1.0897305800502461\n",
      "Epoch 6, Training Loss: 1.089669658725423\n",
      "Epoch 7, Training Loss: 1.0895375950892168\n",
      "Epoch 8, Training Loss: 1.0890354850238426\n",
      "Epoch 9, Training Loss: 1.0889306219000565\n",
      "Epoch 10, Training Loss: 1.0887831306098996\n",
      "Epoch 11, Training Loss: 1.0885180883837822\n",
      "Epoch 12, Training Loss: 1.0882442771940304\n",
      "Epoch 13, Training Loss: 1.088057607636416\n",
      "Epoch 14, Training Loss: 1.087874314121734\n",
      "Epoch 15, Training Loss: 1.0876307548436903\n",
      "Epoch 16, Training Loss: 1.0873542371549105\n",
      "Epoch 17, Training Loss: 1.0872406846598575\n",
      "Epoch 18, Training Loss: 1.087033456429503\n",
      "Epoch 19, Training Loss: 1.0868178758406102\n",
      "Epoch 20, Training Loss: 1.0865203575980393\n",
      "Epoch 21, Training Loss: 1.0863891291439085\n",
      "Epoch 22, Training Loss: 1.0861337918087952\n",
      "Epoch 23, Training Loss: 1.0859756134506455\n",
      "Epoch 24, Training Loss: 1.0857832914008234\n",
      "Epoch 25, Training Loss: 1.0854199097568829\n",
      "Epoch 26, Training Loss: 1.0852872346576892\n",
      "Epoch 27, Training Loss: 1.084912835924249\n",
      "Epoch 28, Training Loss: 1.084912213705536\n",
      "Epoch 29, Training Loss: 1.0845166125691923\n",
      "Epoch 30, Training Loss: 1.0842372623601355\n",
      "Epoch 31, Training Loss: 1.0840365510237844\n",
      "Epoch 32, Training Loss: 1.0838052303271186\n",
      "Epoch 33, Training Loss: 1.0836154104175424\n",
      "Epoch 34, Training Loss: 1.0834498369604124\n",
      "Epoch 35, Training Loss: 1.0831067391804285\n",
      "Epoch 36, Training Loss: 1.0826925489239227\n",
      "Epoch 37, Training Loss: 1.0825382189643113\n",
      "Epoch 38, Training Loss: 1.0823037156485078\n",
      "Epoch 39, Training Loss: 1.082033450083625\n",
      "Epoch 40, Training Loss: 1.0817641717150694\n",
      "Epoch 41, Training Loss: 1.0815077342485127\n",
      "Epoch 42, Training Loss: 1.0811788351015936\n",
      "Epoch 43, Training Loss: 1.0808420005597565\n",
      "Epoch 44, Training Loss: 1.0805779795897634\n",
      "Epoch 45, Training Loss: 1.0804192817300782\n",
      "Epoch 46, Training Loss: 1.0800379159755276\n",
      "Epoch 47, Training Loss: 1.0797173304665357\n",
      "Epoch 48, Training Loss: 1.0793215757025811\n",
      "Epoch 49, Training Loss: 1.079143404064322\n",
      "Epoch 50, Training Loss: 1.0786982842854091\n",
      "Epoch 51, Training Loss: 1.0783068288537794\n",
      "Epoch 52, Training Loss: 1.078152354677817\n",
      "Epoch 53, Training Loss: 1.0777181116261876\n",
      "Epoch 54, Training Loss: 1.077452322235681\n",
      "Epoch 55, Training Loss: 1.0771649850042242\n",
      "Epoch 56, Training Loss: 1.0767485093353386\n",
      "Epoch 57, Training Loss: 1.0762456123093913\n",
      "Epoch 58, Training Loss: 1.0759243445288866\n",
      "Epoch 59, Training Loss: 1.0754761898427978\n",
      "Epoch 60, Training Loss: 1.0751371371118645\n",
      "Epoch 61, Training Loss: 1.0747042699864036\n",
      "Epoch 62, Training Loss: 1.0744455857384474\n",
      "Epoch 63, Training Loss: 1.0739911427175193\n",
      "Epoch 64, Training Loss: 1.073506644614657\n",
      "Epoch 65, Training Loss: 1.073206409475857\n",
      "Epoch 66, Training Loss: 1.0727282744601256\n",
      "Epoch 67, Training Loss: 1.07240481896508\n",
      "Epoch 68, Training Loss: 1.0719220360418908\n",
      "Epoch 69, Training Loss: 1.0713499818529402\n",
      "Epoch 70, Training Loss: 1.0709985691801946\n",
      "Epoch 71, Training Loss: 1.0704795554168243\n",
      "Epoch 72, Training Loss: 1.0699929465028577\n",
      "Epoch 73, Training Loss: 1.0694607792044044\n",
      "Epoch 74, Training Loss: 1.0690085472020887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:53:45,748] Trial 136 finished with value: 0.4454666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 1.0683860556523603\n",
      "Epoch 1, Training Loss: 0.8735469550118411\n",
      "Epoch 2, Training Loss: 0.8176982380393752\n",
      "Epoch 3, Training Loss: 0.8127981971081039\n",
      "Epoch 4, Training Loss: 0.8109958645096399\n",
      "Epoch 5, Training Loss: 0.8097012804863148\n",
      "Epoch 6, Training Loss: 0.8065219750081686\n",
      "Epoch 7, Training Loss: 0.8061752251216343\n",
      "Epoch 8, Training Loss: 0.804326067383128\n",
      "Epoch 9, Training Loss: 0.802678538623609\n",
      "Epoch 10, Training Loss: 0.802782260206409\n",
      "Epoch 11, Training Loss: 0.8028599813468474\n",
      "Epoch 12, Training Loss: 0.8015810389267771\n",
      "Epoch 13, Training Loss: 0.8010141050905213\n",
      "Epoch 14, Training Loss: 0.8007909236097694\n",
      "Epoch 15, Training Loss: 0.8012424791665902\n",
      "Epoch 16, Training Loss: 0.7998906218915953\n",
      "Epoch 17, Training Loss: 0.7997677865781282\n",
      "Epoch 18, Training Loss: 0.7996201153984643\n",
      "Epoch 19, Training Loss: 0.7993430970306683\n",
      "Epoch 20, Training Loss: 0.7988184224394032\n",
      "Epoch 21, Training Loss: 0.7991362266074445\n",
      "Epoch 22, Training Loss: 0.7977885878175721\n",
      "Epoch 23, Training Loss: 0.7972295384209855\n",
      "Epoch 24, Training Loss: 0.7968691318554986\n",
      "Epoch 25, Training Loss: 0.797626547168072\n",
      "Epoch 26, Training Loss: 0.7967989000162684\n",
      "Epoch 27, Training Loss: 0.7960027096863079\n",
      "Epoch 28, Training Loss: 0.7946182449957482\n",
      "Epoch 29, Training Loss: 0.7939166929488791\n",
      "Epoch 30, Training Loss: 0.7937826639727542\n",
      "Epoch 31, Training Loss: 0.7928182003193331\n",
      "Epoch 32, Training Loss: 0.792003465505471\n",
      "Epoch 33, Training Loss: 0.7914251115089073\n",
      "Epoch 34, Training Loss: 0.7908363471353861\n",
      "Epoch 35, Training Loss: 0.7902761592004532\n",
      "Epoch 36, Training Loss: 0.7893630198069981\n",
      "Epoch 37, Training Loss: 0.7894986377622848\n",
      "Epoch 38, Training Loss: 0.7894317799044731\n",
      "Epoch 39, Training Loss: 0.7882342868281487\n",
      "Epoch 40, Training Loss: 0.787580264421334\n",
      "Epoch 41, Training Loss: 0.7870792679320601\n",
      "Epoch 42, Training Loss: 0.7876033447738877\n",
      "Epoch 43, Training Loss: 0.78624424315933\n",
      "Epoch 44, Training Loss: 0.7865958780274356\n",
      "Epoch 45, Training Loss: 0.7863345794211652\n",
      "Epoch 46, Training Loss: 0.7858653361635997\n",
      "Epoch 47, Training Loss: 0.7857285905601387\n",
      "Epoch 48, Training Loss: 0.7854399749210903\n",
      "Epoch 49, Training Loss: 0.7863082929661399\n",
      "Epoch 50, Training Loss: 0.7853253330503192\n",
      "Epoch 51, Training Loss: 0.7851573836534543\n",
      "Epoch 52, Training Loss: 0.7845910413820941\n",
      "Epoch 53, Training Loss: 0.7849457373296408\n",
      "Epoch 54, Training Loss: 0.7847157321478191\n",
      "Epoch 55, Training Loss: 0.7844654318981601\n",
      "Epoch 56, Training Loss: 0.7836919916303534\n",
      "Epoch 57, Training Loss: 0.7838034449215222\n",
      "Epoch 58, Training Loss: 0.7839192967665823\n",
      "Epoch 59, Training Loss: 0.7835226046411614\n",
      "Epoch 60, Training Loss: 0.7835855257242246\n",
      "Epoch 61, Training Loss: 0.7834371442185308\n",
      "Epoch 62, Training Loss: 0.7835072892052787\n",
      "Epoch 63, Training Loss: 0.7827130001738556\n",
      "Epoch 64, Training Loss: 0.7837373202904723\n",
      "Epoch 65, Training Loss: 0.7832959078308335\n",
      "Epoch 66, Training Loss: 0.7843729259376239\n",
      "Epoch 67, Training Loss: 0.7842961935172403\n",
      "Epoch 68, Training Loss: 0.7839640093028993\n",
      "Epoch 69, Training Loss: 0.7835404747410825\n",
      "Epoch 70, Training Loss: 0.7832264924407901\n",
      "Epoch 71, Training Loss: 0.783351563152514\n",
      "Epoch 72, Training Loss: 0.7828614469757653\n",
      "Epoch 73, Training Loss: 0.782713656855705\n",
      "Epoch 74, Training Loss: 0.7833138277656153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:55:08,337] Trial 137 finished with value: 0.6204 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7827325444472464\n",
      "Epoch 1, Training Loss: 0.869674055110243\n",
      "Epoch 2, Training Loss: 0.8275644946815376\n",
      "Epoch 3, Training Loss: 0.8214736962676945\n",
      "Epoch 4, Training Loss: 0.8174589351603859\n",
      "Epoch 5, Training Loss: 0.8145097043281211\n",
      "Epoch 6, Training Loss: 0.812334124038094\n",
      "Epoch 7, Training Loss: 0.8124025035621528\n",
      "Epoch 8, Training Loss: 0.8106676144707472\n",
      "Epoch 9, Training Loss: 0.8103747644818815\n",
      "Epoch 10, Training Loss: 0.8082968542450353\n",
      "Epoch 11, Training Loss: 0.8077789120208052\n",
      "Epoch 12, Training Loss: 0.8072950866885651\n",
      "Epoch 13, Training Loss: 0.8065615213006959\n",
      "Epoch 14, Training Loss: 0.8062379326139177\n",
      "Epoch 15, Training Loss: 0.8066308805817052\n",
      "Epoch 16, Training Loss: 0.8046116607081621\n",
      "Epoch 17, Training Loss: 0.8050611479838091\n",
      "Epoch 18, Training Loss: 0.8056413136030498\n",
      "Epoch 19, Training Loss: 0.8040037728790054\n",
      "Epoch 20, Training Loss: 0.8038484464910693\n",
      "Epoch 21, Training Loss: 0.8029040246081531\n",
      "Epoch 22, Training Loss: 0.804361891746521\n",
      "Epoch 23, Training Loss: 0.8033959071439012\n",
      "Epoch 24, Training Loss: 0.8031120774441196\n",
      "Epoch 25, Training Loss: 0.8031356200239712\n",
      "Epoch 26, Training Loss: 0.8023242762214259\n",
      "Epoch 27, Training Loss: 0.8026765169953941\n",
      "Epoch 28, Training Loss: 0.8019324426364182\n",
      "Epoch 29, Training Loss: 0.8022702458209561\n",
      "Epoch 30, Training Loss: 0.8020505121776036\n",
      "Epoch 31, Training Loss: 0.800797793112303\n",
      "Epoch 32, Training Loss: 0.8015504610269589\n",
      "Epoch 33, Training Loss: 0.8020590819810566\n",
      "Epoch 34, Training Loss: 0.8015927884811745\n",
      "Epoch 35, Training Loss: 0.8020915864105511\n",
      "Epoch 36, Training Loss: 0.8011377957530488\n",
      "Epoch 37, Training Loss: 0.8010701210875261\n",
      "Epoch 38, Training Loss: 0.8005978456117157\n",
      "Epoch 39, Training Loss: 0.8000138999824237\n",
      "Epoch 40, Training Loss: 0.8010074409327113\n",
      "Epoch 41, Training Loss: 0.8010706847771666\n",
      "Epoch 42, Training Loss: 0.8011453656325663\n",
      "Epoch 43, Training Loss: 0.8006044691666625\n",
      "Epoch 44, Training Loss: 0.8002328394050885\n",
      "Epoch 45, Training Loss: 0.8009469353166738\n",
      "Epoch 46, Training Loss: 0.7994857699351203\n",
      "Epoch 47, Training Loss: 0.799819118367102\n",
      "Epoch 48, Training Loss: 0.7996081996681099\n",
      "Epoch 49, Training Loss: 0.799914610923681\n",
      "Epoch 50, Training Loss: 0.7994020011191978\n",
      "Epoch 51, Training Loss: 0.8001337138333715\n",
      "Epoch 52, Training Loss: 0.7991717184396615\n",
      "Epoch 53, Training Loss: 0.7995321387635138\n",
      "Epoch 54, Training Loss: 0.7999634573334142\n",
      "Epoch 55, Training Loss: 0.800882815597649\n",
      "Epoch 56, Training Loss: 0.7998115267968715\n",
      "Epoch 57, Training Loss: 0.7992270647134996\n",
      "Epoch 58, Training Loss: 0.799823908967183\n",
      "Epoch 59, Training Loss: 0.8000116699620297\n",
      "Epoch 60, Training Loss: 0.7987047381867144\n",
      "Epoch 61, Training Loss: 0.7994303138632524\n",
      "Epoch 62, Training Loss: 0.7985892565626848\n",
      "Epoch 63, Training Loss: 0.7988717685964771\n",
      "Epoch 64, Training Loss: 0.8000483598027911\n",
      "Epoch 65, Training Loss: 0.7989491331846195\n",
      "Epoch 66, Training Loss: 0.7989084073475429\n",
      "Epoch 67, Training Loss: 0.7982879420868436\n",
      "Epoch 68, Training Loss: 0.798397559958293\n",
      "Epoch 69, Training Loss: 0.79845185091621\n",
      "Epoch 70, Training Loss: 0.7988786294047994\n",
      "Epoch 71, Training Loss: 0.7991484114998265\n",
      "Epoch 72, Training Loss: 0.7985061326421293\n",
      "Epoch 73, Training Loss: 0.7991198632053863\n",
      "Epoch 74, Training Loss: 0.7983700617811733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:56:19,349] Trial 138 finished with value: 0.5989333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7984260586867655\n",
      "Epoch 1, Training Loss: 0.9784645103870477\n",
      "Epoch 2, Training Loss: 0.9468152506010873\n",
      "Epoch 3, Training Loss: 0.9270456490660073\n",
      "Epoch 4, Training Loss: 0.900399545171207\n",
      "Epoch 5, Training Loss: 0.8670659925704612\n",
      "Epoch 6, Training Loss: 0.8385623221110581\n",
      "Epoch 7, Training Loss: 0.823637126442185\n",
      "Epoch 8, Training Loss: 0.8160084934162914\n",
      "Epoch 9, Training Loss: 0.8123958416451189\n",
      "Epoch 10, Training Loss: 0.8106916383693092\n",
      "Epoch 11, Training Loss: 0.8096128971953142\n",
      "Epoch 12, Training Loss: 0.8084674967859025\n",
      "Epoch 13, Training Loss: 0.8083379199630336\n",
      "Epoch 14, Training Loss: 0.806751735407607\n",
      "Epoch 15, Training Loss: 0.8063090583435575\n",
      "Epoch 16, Training Loss: 0.8059509189505326\n",
      "Epoch 17, Training Loss: 0.8062971480806967\n",
      "Epoch 18, Training Loss: 0.80440094318605\n",
      "Epoch 19, Training Loss: 0.8040148357699688\n",
      "Epoch 20, Training Loss: 0.8040100609449515\n",
      "Epoch 21, Training Loss: 0.8035843874278822\n",
      "Epoch 22, Training Loss: 0.8024889758655003\n",
      "Epoch 23, Training Loss: 0.8023307111030235\n",
      "Epoch 24, Training Loss: 0.801870004037269\n",
      "Epoch 25, Training Loss: 0.8015783158460058\n",
      "Epoch 26, Training Loss: 0.8013573098899727\n",
      "Epoch 27, Training Loss: 0.800847996446423\n",
      "Epoch 28, Training Loss: 0.8006510433397795\n",
      "Epoch 29, Training Loss: 0.8002866671049506\n",
      "Epoch 30, Training Loss: 0.8005699648892969\n",
      "Epoch 31, Training Loss: 0.800050271364083\n",
      "Epoch 32, Training Loss: 0.8000662109009306\n",
      "Epoch 33, Training Loss: 0.7996550211332795\n",
      "Epoch 34, Training Loss: 0.7997124295485647\n",
      "Epoch 35, Training Loss: 0.8000782123185638\n",
      "Epoch 36, Training Loss: 0.7996937636145972\n",
      "Epoch 37, Training Loss: 0.798964614169042\n",
      "Epoch 38, Training Loss: 0.7987750594777272\n",
      "Epoch 39, Training Loss: 0.7985436763082232\n",
      "Epoch 40, Training Loss: 0.798839198915582\n",
      "Epoch 41, Training Loss: 0.7990485332065955\n",
      "Epoch 42, Training Loss: 0.7986713085855757\n",
      "Epoch 43, Training Loss: 0.7990138617673315\n",
      "Epoch 44, Training Loss: 0.7986893963993044\n",
      "Epoch 45, Training Loss: 0.7986817489889332\n",
      "Epoch 46, Training Loss: 0.7982957957382489\n",
      "Epoch 47, Training Loss: 0.7976584387004824\n",
      "Epoch 48, Training Loss: 0.7986415200663689\n",
      "Epoch 49, Training Loss: 0.7980779818126134\n",
      "Epoch 50, Training Loss: 0.798484312322803\n",
      "Epoch 51, Training Loss: 0.7985595791859734\n",
      "Epoch 52, Training Loss: 0.798189535983523\n",
      "Epoch 53, Training Loss: 0.7980843358470086\n",
      "Epoch 54, Training Loss: 0.7975812918261478\n",
      "Epoch 55, Training Loss: 0.7980637574554386\n",
      "Epoch 56, Training Loss: 0.7977200966132315\n",
      "Epoch 57, Training Loss: 0.7974052218566263\n",
      "Epoch 58, Training Loss: 0.7971316889712685\n",
      "Epoch 59, Training Loss: 0.7972557648680264\n",
      "Epoch 60, Training Loss: 0.7974482565894163\n",
      "Epoch 61, Training Loss: 0.7977279698042045\n",
      "Epoch 62, Training Loss: 0.797233976546983\n",
      "Epoch 63, Training Loss: 0.7976097205527743\n",
      "Epoch 64, Training Loss: 0.7976169778888387\n",
      "Epoch 65, Training Loss: 0.7972844196441479\n",
      "Epoch 66, Training Loss: 0.7971120550220174\n",
      "Epoch 67, Training Loss: 0.7963776478193756\n",
      "Epoch 68, Training Loss: 0.7974880873708796\n",
      "Epoch 69, Training Loss: 0.7965957335063389\n",
      "Epoch 70, Training Loss: 0.7967308275681689\n",
      "Epoch 71, Training Loss: 0.7971141104411362\n",
      "Epoch 72, Training Loss: 0.7967225967493272\n",
      "Epoch 73, Training Loss: 0.7976270083197974\n",
      "Epoch 74, Training Loss: 0.7967668855100646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:57:28,462] Trial 139 finished with value: 0.6146666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7961880397975893\n",
      "Epoch 1, Training Loss: 0.8711186679671793\n",
      "Epoch 2, Training Loss: 0.8131945136014153\n",
      "Epoch 3, Training Loss: 0.8068622196421904\n",
      "Epoch 4, Training Loss: 0.802207116169088\n",
      "Epoch 5, Training Loss: 0.8002141855043523\n",
      "Epoch 6, Training Loss: 0.7975985895886141\n",
      "Epoch 7, Training Loss: 0.7953171835927402\n",
      "Epoch 8, Training Loss: 0.7968104657706092\n",
      "Epoch 9, Training Loss: 0.7942727098745458\n",
      "Epoch 10, Training Loss: 0.7931331060914433\n",
      "Epoch 11, Training Loss: 0.7931156581990859\n",
      "Epoch 12, Training Loss: 0.7931682759172776\n",
      "Epoch 13, Training Loss: 0.7919365176032571\n",
      "Epoch 14, Training Loss: 0.7920124114260954\n",
      "Epoch 15, Training Loss: 0.7924422604897443\n",
      "Epoch 16, Training Loss: 0.7918267998975866\n",
      "Epoch 17, Training Loss: 0.7906520786004908\n",
      "Epoch 18, Training Loss: 0.7911276921805214\n",
      "Epoch 19, Training Loss: 0.790957546935362\n",
      "Epoch 20, Training Loss: 0.7904374790191651\n",
      "Epoch 21, Training Loss: 0.7903671338978936\n",
      "Epoch 22, Training Loss: 0.789761520764407\n",
      "Epoch 23, Training Loss: 0.7896548491365769\n",
      "Epoch 24, Training Loss: 0.7892635379118078\n",
      "Epoch 25, Training Loss: 0.7896749863203834\n",
      "Epoch 26, Training Loss: 0.7891053823863758\n",
      "Epoch 27, Training Loss: 0.788688583093531\n",
      "Epoch 28, Training Loss: 0.7886736954660977\n",
      "Epoch 29, Training Loss: 0.788488536231658\n",
      "Epoch 30, Training Loss: 0.7886852396235746\n",
      "Epoch 31, Training Loss: 0.7889789518889259\n",
      "Epoch 32, Training Loss: 0.7879540530373068\n",
      "Epoch 33, Training Loss: 0.787906280335258\n",
      "Epoch 34, Training Loss: 0.7875603170254651\n",
      "Epoch 35, Training Loss: 0.7876332099297467\n",
      "Epoch 36, Training Loss: 0.7864479787209455\n",
      "Epoch 37, Training Loss: 0.786966196789461\n",
      "Epoch 38, Training Loss: 0.7874975979328156\n",
      "Epoch 39, Training Loss: 0.7866008144967697\n",
      "Epoch 40, Training Loss: 0.7876085496650023\n",
      "Epoch 41, Training Loss: 0.7869103754267973\n",
      "Epoch 42, Training Loss: 0.785947763709461\n",
      "Epoch 43, Training Loss: 0.7859024441242218\n",
      "Epoch 44, Training Loss: 0.7879954666249892\n",
      "Epoch 45, Training Loss: 0.78658328091397\n",
      "Epoch 46, Training Loss: 0.7862008990259731\n",
      "Epoch 47, Training Loss: 0.7855596003111671\n",
      "Epoch 48, Training Loss: 0.7866597589324502\n",
      "Epoch 49, Training Loss: 0.7864563100478228\n",
      "Epoch 50, Training Loss: 0.7856344879374785\n",
      "Epoch 51, Training Loss: 0.785663604035097\n",
      "Epoch 52, Training Loss: 0.7853712312614216\n",
      "Epoch 53, Training Loss: 0.7849804229596082\n",
      "Epoch 54, Training Loss: 0.7856160780261544\n",
      "Epoch 55, Training Loss: 0.7850692919422598\n",
      "Epoch 56, Training Loss: 0.7848164380297942\n",
      "Epoch 57, Training Loss: 0.7850932981687434\n",
      "Epoch 58, Training Loss: 0.7850422488240635\n",
      "Epoch 59, Training Loss: 0.784777193700566\n",
      "Epoch 60, Training Loss: 0.7845882936786203\n",
      "Epoch 61, Training Loss: 0.7845262840214897\n",
      "Epoch 62, Training Loss: 0.7848448802443112\n",
      "Epoch 63, Training Loss: 0.7844486721122966\n",
      "Epoch 64, Training Loss: 0.7854397178397459\n",
      "Epoch 65, Training Loss: 0.7843949031128603\n",
      "Epoch 66, Training Loss: 0.7838488521996666\n",
      "Epoch 67, Training Loss: 0.7838456943455865\n",
      "Epoch 68, Training Loss: 0.7840364559958963\n",
      "Epoch 69, Training Loss: 0.7845879335964427\n",
      "Epoch 70, Training Loss: 0.7835039829506594\n",
      "Epoch 71, Training Loss: 0.78440654649454\n",
      "Epoch 72, Training Loss: 0.7834352979940526\n",
      "Epoch 73, Training Loss: 0.7840237335597767\n",
      "Epoch 74, Training Loss: 0.7839508910740123\n",
      "Epoch 75, Training Loss: 0.7843663806775037\n",
      "Epoch 76, Training Loss: 0.783004963608349\n",
      "Epoch 77, Training Loss: 0.783004736549714\n",
      "Epoch 78, Training Loss: 0.7827001074482413\n",
      "Epoch 79, Training Loss: 0.7833598736454459\n",
      "Epoch 80, Training Loss: 0.783358955523547\n",
      "Epoch 81, Training Loss: 0.7829469330871807\n",
      "Epoch 82, Training Loss: 0.7831339971458211\n",
      "Epoch 83, Training Loss: 0.7830113962818595\n",
      "Epoch 84, Training Loss: 0.7826523184776306\n",
      "Epoch 85, Training Loss: 0.782779685609481\n",
      "Epoch 86, Training Loss: 0.7824672776110032\n",
      "Epoch 87, Training Loss: 0.7822048229329727\n",
      "Epoch 88, Training Loss: 0.7822771140407113\n",
      "Epoch 89, Training Loss: 0.7820202377263238\n",
      "Epoch 90, Training Loss: 0.7821816339913537\n",
      "Epoch 91, Training Loss: 0.7823129937228035\n",
      "Epoch 92, Training Loss: 0.7826766270749709\n",
      "Epoch 93, Training Loss: 0.781842303276062\n",
      "Epoch 94, Training Loss: 0.7813644557139453\n",
      "Epoch 95, Training Loss: 0.7820280909538269\n",
      "Epoch 96, Training Loss: 0.7813905646520503\n",
      "Epoch 97, Training Loss: 0.7824721126696643\n",
      "Epoch 98, Training Loss: 0.7826780315006481\n",
      "Epoch 99, Training Loss: 0.7811677556178149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 12:59:42,497] Trial 140 finished with value: 0.6390666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7814904870005215\n",
      "Epoch 1, Training Loss: 0.9907021031660193\n",
      "Epoch 2, Training Loss: 0.9333015202774722\n",
      "Epoch 3, Training Loss: 0.8907758263980641\n",
      "Epoch 4, Training Loss: 0.8403702625106363\n",
      "Epoch 5, Training Loss: 0.8182152784571928\n",
      "Epoch 6, Training Loss: 0.8134397850317113\n",
      "Epoch 7, Training Loss: 0.8107984037960276\n",
      "Epoch 8, Training Loss: 0.8098835055968341\n",
      "Epoch 9, Training Loss: 0.8087983951147865\n",
      "Epoch 10, Training Loss: 0.8083795494191787\n",
      "Epoch 11, Training Loss: 0.8071122101475211\n",
      "Epoch 12, Training Loss: 0.8062399040250218\n",
      "Epoch 13, Training Loss: 0.8052903815578012\n",
      "Epoch 14, Training Loss: 0.8050400928188772\n",
      "Epoch 15, Training Loss: 0.8049618946103488\n",
      "Epoch 16, Training Loss: 0.8036084731186137\n",
      "Epoch 17, Training Loss: 0.8033188847233267\n",
      "Epoch 18, Training Loss: 0.8026626789569855\n",
      "Epoch 19, Training Loss: 0.8029120698395897\n",
      "Epoch 20, Training Loss: 0.802096577462028\n",
      "Epoch 21, Training Loss: 0.8014545914706062\n",
      "Epoch 22, Training Loss: 0.8015527749762815\n",
      "Epoch 23, Training Loss: 0.8012205601439757\n",
      "Epoch 24, Training Loss: 0.8010914649682886\n",
      "Epoch 25, Training Loss: 0.8011057226096883\n",
      "Epoch 26, Training Loss: 0.8007765548369464\n",
      "Epoch 27, Training Loss: 0.8005618082074558\n",
      "Epoch 28, Training Loss: 0.8006258970148423\n",
      "Epoch 29, Training Loss: 0.800616596586564\n",
      "Epoch 30, Training Loss: 0.8004921261002036\n",
      "Epoch 31, Training Loss: 0.8000760066509247\n",
      "Epoch 32, Training Loss: 0.7996944332824034\n",
      "Epoch 33, Training Loss: 0.7999663457449745\n",
      "Epoch 34, Training Loss: 0.7998212637620814\n",
      "Epoch 35, Training Loss: 0.7998371162134058\n",
      "Epoch 36, Training Loss: 0.7994033872379976\n",
      "Epoch 37, Training Loss: 0.7993788875551785\n",
      "Epoch 38, Training Loss: 0.799250508406583\n",
      "Epoch 39, Training Loss: 0.7989535434105817\n",
      "Epoch 40, Training Loss: 0.7985257246915032\n",
      "Epoch 41, Training Loss: 0.7986015866083257\n",
      "Epoch 42, Training Loss: 0.7985666321305668\n",
      "Epoch 43, Training Loss: 0.7987382441408494\n",
      "Epoch 44, Training Loss: 0.7983817118756912\n",
      "Epoch 45, Training Loss: 0.7984709397484274\n",
      "Epoch 46, Training Loss: 0.7979338286904728\n",
      "Epoch 47, Training Loss: 0.7982382064005907\n",
      "Epoch 48, Training Loss: 0.7979659414291382\n",
      "Epoch 49, Training Loss: 0.7977223406118505\n",
      "Epoch 50, Training Loss: 0.7977102982296663\n",
      "Epoch 51, Training Loss: 0.7979020913909464\n",
      "Epoch 52, Training Loss: 0.7973046700393452\n",
      "Epoch 53, Training Loss: 0.7975000676688025\n",
      "Epoch 54, Training Loss: 0.7972753770912395\n",
      "Epoch 55, Training Loss: 0.7974818651816424\n",
      "Epoch 56, Training Loss: 0.7970747069050284\n",
      "Epoch 57, Training Loss: 0.7970362483052646\n",
      "Epoch 58, Training Loss: 0.7969107468689189\n",
      "Epoch 59, Training Loss: 0.7966691072548137\n",
      "Epoch 60, Training Loss: 0.7966711138276493\n",
      "Epoch 61, Training Loss: 0.7968544997888453\n",
      "Epoch 62, Training Loss: 0.7964478417003856\n",
      "Epoch 63, Training Loss: 0.7964827642020057\n",
      "Epoch 64, Training Loss: 0.7969112543498769\n",
      "Epoch 65, Training Loss: 0.7966034952331992\n",
      "Epoch 66, Training Loss: 0.7960789428037756\n",
      "Epoch 67, Training Loss: 0.796384162902832\n",
      "Epoch 68, Training Loss: 0.7960056255845462\n",
      "Epoch 69, Training Loss: 0.7962185320433448\n",
      "Epoch 70, Training Loss: 0.795891426521189\n",
      "Epoch 71, Training Loss: 0.795797316607307\n",
      "Epoch 72, Training Loss: 0.7957849708725424\n",
      "Epoch 73, Training Loss: 0.7959582943074843\n",
      "Epoch 74, Training Loss: 0.795601138717988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:01:22,926] Trial 141 finished with value: 0.6338666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7952532740200267\n",
      "Epoch 1, Training Loss: 0.9785957686883167\n",
      "Epoch 2, Training Loss: 0.9454046848125027\n",
      "Epoch 3, Training Loss: 0.9316300079338533\n",
      "Epoch 4, Training Loss: 0.9168775878454509\n",
      "Epoch 5, Training Loss: 0.9019072593602919\n",
      "Epoch 6, Training Loss: 0.8868546813054192\n",
      "Epoch 7, Training Loss: 0.872251968724387\n",
      "Epoch 8, Training Loss: 0.858226756404217\n",
      "Epoch 9, Training Loss: 0.8467989917088272\n",
      "Epoch 10, Training Loss: 0.836720575694751\n",
      "Epoch 11, Training Loss: 0.8293175177466601\n",
      "Epoch 12, Training Loss: 0.8234538460584512\n",
      "Epoch 13, Training Loss: 0.8201218341526232\n",
      "Epoch 14, Training Loss: 0.8168691758822678\n",
      "Epoch 15, Training Loss: 0.8143253564834595\n",
      "Epoch 16, Training Loss: 0.8121656114893748\n",
      "Epoch 17, Training Loss: 0.8111462816259914\n",
      "Epoch 18, Training Loss: 0.8109716483524867\n",
      "Epoch 19, Training Loss: 0.8093589839182402\n",
      "Epoch 20, Training Loss: 0.8083820629836922\n",
      "Epoch 21, Training Loss: 0.8083231446438266\n",
      "Epoch 22, Training Loss: 0.8079284814067353\n",
      "Epoch 23, Training Loss: 0.8072395847256022\n",
      "Epoch 24, Training Loss: 0.8069403059500501\n",
      "Epoch 25, Training Loss: 0.8070814559334203\n",
      "Epoch 26, Training Loss: 0.8055657533774698\n",
      "Epoch 27, Training Loss: 0.806002272161326\n",
      "Epoch 28, Training Loss: 0.8051930953685502\n",
      "Epoch 29, Training Loss: 0.8053996197263101\n",
      "Epoch 30, Training Loss: 0.8045924529993445\n",
      "Epoch 31, Training Loss: 0.8048746323226986\n",
      "Epoch 32, Training Loss: 0.8041847461148313\n",
      "Epoch 33, Training Loss: 0.8041816489140791\n",
      "Epoch 34, Training Loss: 0.8037908944868504\n",
      "Epoch 35, Training Loss: 0.8040435177939279\n",
      "Epoch 36, Training Loss: 0.8037696502262488\n",
      "Epoch 37, Training Loss: 0.8031362990687664\n",
      "Epoch 38, Training Loss: 0.8027680101251243\n",
      "Epoch 39, Training Loss: 0.8034156041934077\n",
      "Epoch 40, Training Loss: 0.8031862982233664\n",
      "Epoch 41, Training Loss: 0.8026230080683429\n",
      "Epoch 42, Training Loss: 0.8026117588344374\n",
      "Epoch 43, Training Loss: 0.802792833801499\n",
      "Epoch 44, Training Loss: 0.8024829912006407\n",
      "Epoch 45, Training Loss: 0.8018302799167489\n",
      "Epoch 46, Training Loss: 0.8019758497861992\n",
      "Epoch 47, Training Loss: 0.8017265060790499\n",
      "Epoch 48, Training Loss: 0.8015608903160669\n",
      "Epoch 49, Training Loss: 0.8015309839320363\n",
      "Epoch 50, Training Loss: 0.8012253226194167\n",
      "Epoch 51, Training Loss: 0.8012795830131474\n",
      "Epoch 52, Training Loss: 0.8010262509037678\n",
      "Epoch 53, Training Loss: 0.8017637782527092\n",
      "Epoch 54, Training Loss: 0.801342911379678\n",
      "Epoch 55, Training Loss: 0.8011121068681989\n",
      "Epoch 56, Training Loss: 0.8006761205823798\n",
      "Epoch 57, Training Loss: 0.8006458751241067\n",
      "Epoch 58, Training Loss: 0.8005416069711958\n",
      "Epoch 59, Training Loss: 0.8003585225657412\n",
      "Epoch 60, Training Loss: 0.8005537062659299\n",
      "Epoch 61, Training Loss: 0.8010067548966946\n",
      "Epoch 62, Training Loss: 0.8004450165239492\n",
      "Epoch 63, Training Loss: 0.8002277763266312\n",
      "Epoch 64, Training Loss: 0.7996204357398183\n",
      "Epoch 65, Training Loss: 0.8000637282106213\n",
      "Epoch 66, Training Loss: 0.8001601198562106\n",
      "Epoch 67, Training Loss: 0.7997522043106251\n",
      "Epoch 68, Training Loss: 0.8003142554060857\n",
      "Epoch 69, Training Loss: 0.7997357683074205\n",
      "Epoch 70, Training Loss: 0.7997906118407285\n",
      "Epoch 71, Training Loss: 0.7991177558898925\n",
      "Epoch 72, Training Loss: 0.7997156127951199\n",
      "Epoch 73, Training Loss: 0.7994913159456468\n",
      "Epoch 74, Training Loss: 0.7989175044056168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:02:25,302] Trial 142 finished with value: 0.6348 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7996855573546617\n",
      "Epoch 1, Training Loss: 0.8796352269954251\n",
      "Epoch 2, Training Loss: 0.8142338091269472\n",
      "Epoch 3, Training Loss: 0.8067098893617329\n",
      "Epoch 4, Training Loss: 0.8040603356253831\n",
      "Epoch 5, Training Loss: 0.8013466562543596\n",
      "Epoch 6, Training Loss: 0.7994576196025188\n",
      "Epoch 7, Training Loss: 0.7975762630763806\n",
      "Epoch 8, Training Loss: 0.7959731661287466\n",
      "Epoch 9, Training Loss: 0.7947696739569643\n",
      "Epoch 10, Training Loss: 0.792479415764486\n",
      "Epoch 11, Training Loss: 0.7910146023097792\n",
      "Epoch 12, Training Loss: 0.7903936367285879\n",
      "Epoch 13, Training Loss: 0.7900977850856638\n",
      "Epoch 14, Training Loss: 0.7890453609756958\n",
      "Epoch 15, Training Loss: 0.7883846965947545\n",
      "Epoch 16, Training Loss: 0.787709897532499\n",
      "Epoch 17, Training Loss: 0.788218223062673\n",
      "Epoch 18, Training Loss: 0.7869124624065886\n",
      "Epoch 19, Training Loss: 0.7869391858129573\n",
      "Epoch 20, Training Loss: 0.7866773649265891\n",
      "Epoch 21, Training Loss: 0.7863696486430061\n",
      "Epoch 22, Training Loss: 0.7861465347440619\n",
      "Epoch 23, Training Loss: 0.7859593072331937\n",
      "Epoch 24, Training Loss: 0.7853177041935742\n",
      "Epoch 25, Training Loss: 0.7848906596800438\n",
      "Epoch 26, Training Loss: 0.7847046731109906\n",
      "Epoch 27, Training Loss: 0.7846258389322381\n",
      "Epoch 28, Training Loss: 0.7845480010921794\n",
      "Epoch 29, Training Loss: 0.7843281019899182\n",
      "Epoch 30, Training Loss: 0.7841782095737027\n",
      "Epoch 31, Training Loss: 0.783218362815398\n",
      "Epoch 32, Training Loss: 0.7833649396000052\n",
      "Epoch 33, Training Loss: 0.7835511281078024\n",
      "Epoch 34, Training Loss: 0.7829627811460567\n",
      "Epoch 35, Training Loss: 0.783361037781364\n",
      "Epoch 36, Training Loss: 0.7828387313319328\n",
      "Epoch 37, Training Loss: 0.782748658495738\n",
      "Epoch 38, Training Loss: 0.7821705343131732\n",
      "Epoch 39, Training Loss: 0.7823053409282427\n",
      "Epoch 40, Training Loss: 0.7820768723810526\n",
      "Epoch 41, Training Loss: 0.782429437888296\n",
      "Epoch 42, Training Loss: 0.7821084944825424\n",
      "Epoch 43, Training Loss: 0.7815783180688557\n",
      "Epoch 44, Training Loss: 0.7824113230956228\n",
      "Epoch 45, Training Loss: 0.7816606656053012\n",
      "Epoch 46, Training Loss: 0.7817025798604005\n",
      "Epoch 47, Training Loss: 0.7813127345608589\n",
      "Epoch 48, Training Loss: 0.7812959905853845\n",
      "Epoch 49, Training Loss: 0.781142382603839\n",
      "Epoch 50, Training Loss: 0.7812520090798686\n",
      "Epoch 51, Training Loss: 0.7810518676177003\n",
      "Epoch 52, Training Loss: 0.7805295633194141\n",
      "Epoch 53, Training Loss: 0.7800871401801145\n",
      "Epoch 54, Training Loss: 0.78149941926612\n",
      "Epoch 55, Training Loss: 0.7808062776587064\n",
      "Epoch 56, Training Loss: 0.7812313258199763\n",
      "Epoch 57, Training Loss: 0.7804968480777024\n",
      "Epoch 58, Training Loss: 0.7807263650392231\n",
      "Epoch 59, Training Loss: 0.7804139996829785\n",
      "Epoch 60, Training Loss: 0.7798042510685168\n",
      "Epoch 61, Training Loss: 0.7797451350025665\n",
      "Epoch 62, Training Loss: 0.7799086230141776\n",
      "Epoch 63, Training Loss: 0.779677539033101\n",
      "Epoch 64, Training Loss: 0.7796991310621563\n",
      "Epoch 65, Training Loss: 0.7799449350600852\n",
      "Epoch 66, Training Loss: 0.7798668972531656\n",
      "Epoch 67, Training Loss: 0.7803184883935111\n",
      "Epoch 68, Training Loss: 0.7795218139662778\n",
      "Epoch 69, Training Loss: 0.7794593716026249\n",
      "Epoch 70, Training Loss: 0.778993231685538\n",
      "Epoch 71, Training Loss: 0.7791307854473143\n",
      "Epoch 72, Training Loss: 0.7791377574877631\n",
      "Epoch 73, Training Loss: 0.7792688262193723\n",
      "Epoch 74, Training Loss: 0.7791422438800784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:03:47,418] Trial 143 finished with value: 0.6245333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.778849674884538\n",
      "Epoch 1, Training Loss: 0.8904963510877946\n",
      "Epoch 2, Training Loss: 0.8107317371228162\n",
      "Epoch 3, Training Loss: 0.8032843583471635\n",
      "Epoch 4, Training Loss: 0.8013836481290705\n",
      "Epoch 5, Training Loss: 0.7992844314434949\n",
      "Epoch 6, Training Loss: 0.7974327367193559\n",
      "Epoch 7, Training Loss: 0.7962197434902191\n",
      "Epoch 8, Training Loss: 0.7943499008347007\n",
      "Epoch 9, Training Loss: 0.7929532661157496\n",
      "Epoch 10, Training Loss: 0.7915837530528798\n",
      "Epoch 11, Training Loss: 0.790787035928053\n",
      "Epoch 12, Training Loss: 0.7896464568025926\n",
      "Epoch 13, Training Loss: 0.7895874219782212\n",
      "Epoch 14, Training Loss: 0.7888222326951868\n",
      "Epoch 15, Training Loss: 0.7884913215216468\n",
      "Epoch 16, Training Loss: 0.7882455552325529\n",
      "Epoch 17, Training Loss: 0.7873459647683536\n",
      "Epoch 18, Training Loss: 0.7874298718396355\n",
      "Epoch 19, Training Loss: 0.786376190886778\n",
      "Epoch 20, Training Loss: 0.7861922651178697\n",
      "Epoch 21, Training Loss: 0.7858738702185014\n",
      "Epoch 22, Training Loss: 0.7855793626869426\n",
      "Epoch 23, Training Loss: 0.7853872875606313\n",
      "Epoch 24, Training Loss: 0.785243910410825\n",
      "Epoch 25, Training Loss: 0.7844621280361624\n",
      "Epoch 26, Training Loss: 0.7844667815460878\n",
      "Epoch 27, Training Loss: 0.7841706433716942\n",
      "Epoch 28, Training Loss: 0.7842382050963009\n",
      "Epoch 29, Training Loss: 0.7839444739678326\n",
      "Epoch 30, Training Loss: 0.7836593721193426\n",
      "Epoch 31, Training Loss: 0.7833007624570061\n",
      "Epoch 32, Training Loss: 0.7831205619783963\n",
      "Epoch 33, Training Loss: 0.7832141524202684\n",
      "Epoch 34, Training Loss: 0.7826591932072359\n",
      "Epoch 35, Training Loss: 0.7828220425633823\n",
      "Epoch 36, Training Loss: 0.7824789599110098\n",
      "Epoch 37, Training Loss: 0.781850237565882\n",
      "Epoch 38, Training Loss: 0.7821241491682389\n",
      "Epoch 39, Training Loss: 0.7817413428951713\n",
      "Epoch 40, Training Loss: 0.7818491913290585\n",
      "Epoch 41, Training Loss: 0.7816258514628691\n",
      "Epoch 42, Training Loss: 0.7814959275021273\n",
      "Epoch 43, Training Loss: 0.7811879181160646\n",
      "Epoch 44, Training Loss: 0.7812856957491706\n",
      "Epoch 45, Training Loss: 0.7813545351168688\n",
      "Epoch 46, Training Loss: 0.7810404123979456\n",
      "Epoch 47, Training Loss: 0.7809435906129725\n",
      "Epoch 48, Training Loss: 0.7810406256423277\n",
      "Epoch 49, Training Loss: 0.7810578102925244\n",
      "Epoch 50, Training Loss: 0.7805843552421121\n",
      "Epoch 51, Training Loss: 0.7804539107575136\n",
      "Epoch 52, Training Loss: 0.7804960320276373\n",
      "Epoch 53, Training Loss: 0.7800237309231478\n",
      "Epoch 54, Training Loss: 0.7802604919321396\n",
      "Epoch 55, Training Loss: 0.7804075999119703\n",
      "Epoch 56, Training Loss: 0.7799776986066033\n",
      "Epoch 57, Training Loss: 0.779616531975129\n",
      "Epoch 58, Training Loss: 0.7799852456064785\n",
      "Epoch 59, Training Loss: 0.779747404421077\n",
      "Epoch 60, Training Loss: 0.7800058552096871\n",
      "Epoch 61, Training Loss: 0.7795658361210542\n",
      "Epoch 62, Training Loss: 0.7792788886322695\n",
      "Epoch 63, Training Loss: 0.779450503727969\n",
      "Epoch 64, Training Loss: 0.7794240969770095\n",
      "Epoch 65, Training Loss: 0.7792077366744771\n",
      "Epoch 66, Training Loss: 0.7793878220109378\n",
      "Epoch 67, Training Loss: 0.7793163831794964\n",
      "Epoch 68, Training Loss: 0.7792997442273533\n",
      "Epoch 69, Training Loss: 0.7787208894421073\n",
      "Epoch 70, Training Loss: 0.7789990886519937\n",
      "Epoch 71, Training Loss: 0.7791765608507044\n",
      "Epoch 72, Training Loss: 0.7785876061635859\n",
      "Epoch 73, Training Loss: 0.7782031021398657\n",
      "Epoch 74, Training Loss: 0.7782448331047507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:05:28,056] Trial 144 finished with value: 0.6392666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.778442991621354\n",
      "Epoch 1, Training Loss: 1.1063460961510154\n",
      "Epoch 2, Training Loss: 1.0334705988799824\n",
      "Epoch 3, Training Loss: 1.0024229325967677\n",
      "Epoch 4, Training Loss: 0.9859980086018058\n",
      "Epoch 5, Training Loss: 0.9765517989326926\n",
      "Epoch 6, Training Loss: 0.9707585974300609\n",
      "Epoch 7, Training Loss: 0.9670064027870403\n",
      "Epoch 8, Training Loss: 0.9644083342832678\n",
      "Epoch 9, Training Loss: 0.962473571580999\n",
      "Epoch 10, Training Loss: 0.9609318067747004\n",
      "Epoch 11, Training Loss: 0.9596189765369191\n",
      "Epoch 12, Training Loss: 0.9584543709193959\n",
      "Epoch 13, Training Loss: 0.9573767164174248\n",
      "Epoch 14, Training Loss: 0.9563551409104291\n",
      "Epoch 15, Training Loss: 0.9553709349912756\n",
      "Epoch 16, Training Loss: 0.9544116890430451\n",
      "Epoch 17, Training Loss: 0.9534684402802411\n",
      "Epoch 18, Training Loss: 0.9525320255055147\n",
      "Epoch 19, Training Loss: 0.9516102701776168\n",
      "Epoch 20, Training Loss: 0.9506895200645222\n",
      "Epoch 21, Training Loss: 0.9497769524069394\n",
      "Epoch 22, Training Loss: 0.948864078872344\n",
      "Epoch 23, Training Loss: 0.9479567109837251\n",
      "Epoch 24, Training Loss: 0.9470477777368882\n",
      "Epoch 25, Training Loss: 0.9461415723492117\n",
      "Epoch 26, Training Loss: 0.9452363051386441\n",
      "Epoch 27, Training Loss: 0.9443302675555734\n",
      "Epoch 28, Training Loss: 0.9434249895460466\n",
      "Epoch 29, Training Loss: 0.9425193022980409\n",
      "Epoch 30, Training Loss: 0.9416135428933536\n",
      "Epoch 31, Training Loss: 0.9407056338646833\n",
      "Epoch 32, Training Loss: 0.9397958901349236\n",
      "Epoch 33, Training Loss: 0.9388947403430938\n",
      "Epoch 34, Training Loss: 0.9379786627432879\n",
      "Epoch 35, Training Loss: 0.9370683592207292\n",
      "Epoch 36, Training Loss: 0.9361556171669679\n",
      "Epoch 37, Training Loss: 0.9352362040211173\n",
      "Epoch 38, Training Loss: 0.9343200341392965\n",
      "Epoch 39, Training Loss: 0.9333990952547859\n",
      "Epoch 40, Training Loss: 0.9324767612709719\n",
      "Epoch 41, Training Loss: 0.9315488872107337\n",
      "Epoch 42, Training Loss: 0.9306280601725859\n",
      "Epoch 43, Training Loss: 0.9296984438335194\n",
      "Epoch 44, Training Loss: 0.9287653347323922\n",
      "Epoch 45, Training Loss: 0.9278277325630188\n",
      "Epoch 46, Training Loss: 0.9268879826629863\n",
      "Epoch 47, Training Loss: 0.9259479775849511\n",
      "Epoch 48, Training Loss: 0.9250022144177381\n",
      "Epoch 49, Training Loss: 0.9240631330013275\n",
      "Epoch 50, Training Loss: 0.9231095894645243\n",
      "Epoch 51, Training Loss: 0.9221595591657302\n",
      "Epoch 52, Training Loss: 0.9212098268901601\n",
      "Epoch 53, Training Loss: 0.920251468630398\n",
      "Epoch 54, Training Loss: 0.9192900055997512\n",
      "Epoch 55, Training Loss: 0.9183264969376956\n",
      "Epoch 56, Training Loss: 0.9173624719591702\n",
      "Epoch 57, Training Loss: 0.9163933988879709\n",
      "Epoch 58, Training Loss: 0.9154267889611861\n",
      "Epoch 59, Training Loss: 0.9144510633103988\n",
      "Epoch 60, Training Loss: 0.9134727838460137\n",
      "Epoch 61, Training Loss: 0.912502491053413\n",
      "Epoch 62, Training Loss: 0.9115190638514126\n",
      "Epoch 63, Training Loss: 0.9105408913247726\n",
      "Epoch 64, Training Loss: 0.9095516752495485\n",
      "Epoch 65, Training Loss: 0.9085760435637306\n",
      "Epoch 66, Training Loss: 0.9075884330272674\n",
      "Epoch 67, Training Loss: 0.9065954645241008\n",
      "Epoch 68, Training Loss: 0.9056061949449427\n",
      "Epoch 69, Training Loss: 0.9046158258353962\n",
      "Epoch 70, Training Loss: 0.9036169825581943\n",
      "Epoch 71, Training Loss: 0.9026241007973166\n",
      "Epoch 72, Training Loss: 0.9016292038384606\n",
      "Epoch 73, Training Loss: 0.9006459999084473\n",
      "Epoch 74, Training Loss: 0.899645294231527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:06:38,406] Trial 145 finished with value: 0.5757333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8986449951284072\n",
      "Epoch 1, Training Loss: 0.8701006880227258\n",
      "Epoch 2, Training Loss: 0.8258562934398651\n",
      "Epoch 3, Training Loss: 0.817287286309635\n",
      "Epoch 4, Training Loss: 0.8127894856649287\n",
      "Epoch 5, Training Loss: 0.8106001234054565\n",
      "Epoch 6, Training Loss: 0.8088932425835553\n",
      "Epoch 7, Training Loss: 0.8071921458665062\n",
      "Epoch 8, Training Loss: 0.8058627408392289\n",
      "Epoch 9, Training Loss: 0.8047790924941792\n",
      "Epoch 10, Training Loss: 0.8045222656867084\n",
      "Epoch 11, Training Loss: 0.8038569803097669\n",
      "Epoch 12, Training Loss: 0.8034230298855726\n",
      "Epoch 13, Training Loss: 0.8034378865185906\n",
      "Epoch 14, Training Loss: 0.8022212681349586\n",
      "Epoch 15, Training Loss: 0.801771123900133\n",
      "Epoch 16, Training Loss: 0.8021245030795827\n",
      "Epoch 17, Training Loss: 0.8008891785846037\n",
      "Epoch 18, Training Loss: 0.8007728238666759\n",
      "Epoch 19, Training Loss: 0.8003038921075709\n",
      "Epoch 20, Training Loss: 0.7998518681526184\n",
      "Epoch 21, Training Loss: 0.799565642230651\n",
      "Epoch 22, Training Loss: 0.7997576256359324\n",
      "Epoch 23, Training Loss: 0.7998779213428497\n",
      "Epoch 24, Training Loss: 0.7993332926666036\n",
      "Epoch 25, Training Loss: 0.7988348191625931\n",
      "Epoch 26, Training Loss: 0.7993646313162411\n",
      "Epoch 27, Training Loss: 0.7992471541376674\n",
      "Epoch 28, Training Loss: 0.79882423267645\n",
      "Epoch 29, Training Loss: 0.799392305542441\n",
      "Epoch 30, Training Loss: 0.798480986216489\n",
      "Epoch 31, Training Loss: 0.7986150848164278\n",
      "Epoch 32, Training Loss: 0.7980404883272507\n",
      "Epoch 33, Training Loss: 0.7979391295068404\n",
      "Epoch 34, Training Loss: 0.7979637756768395\n",
      "Epoch 35, Training Loss: 0.7984516461456523\n",
      "Epoch 36, Training Loss: 0.7979595301431768\n",
      "Epoch 37, Training Loss: 0.7981687132751241\n",
      "Epoch 38, Training Loss: 0.7973303274547352\n",
      "Epoch 39, Training Loss: 0.7975463517273174\n",
      "Epoch 40, Training Loss: 0.7973562227978426\n",
      "Epoch 41, Training Loss: 0.7970716971509597\n",
      "Epoch 42, Training Loss: 0.796852695310817\n",
      "Epoch 43, Training Loss: 0.7972918818978703\n",
      "Epoch 44, Training Loss: 0.7964035712971407\n",
      "Epoch 45, Training Loss: 0.7970089885767768\n",
      "Epoch 46, Training Loss: 0.7972502970695495\n",
      "Epoch 47, Training Loss: 0.7968318371913012\n",
      "Epoch 48, Training Loss: 0.7974124452647041\n",
      "Epoch 49, Training Loss: 0.7967520204712363\n",
      "Epoch 50, Training Loss: 0.7966703654036802\n",
      "Epoch 51, Training Loss: 0.7974451537693248\n",
      "Epoch 52, Training Loss: 0.7963253756831674\n",
      "Epoch 53, Training Loss: 0.7968945747964522\n",
      "Epoch 54, Training Loss: 0.7965381716980654\n",
      "Epoch 55, Training Loss: 0.7961551997240852\n",
      "Epoch 56, Training Loss: 0.7965209033909966\n",
      "Epoch 57, Training Loss: 0.7963348002293531\n",
      "Epoch 58, Training Loss: 0.7964060277097366\n",
      "Epoch 59, Training Loss: 0.7963598627202652\n",
      "Epoch 60, Training Loss: 0.7964313986722161\n",
      "Epoch 61, Training Loss: 0.7956918391760658\n",
      "Epoch 62, Training Loss: 0.7960726132813623\n",
      "Epoch 63, Training Loss: 0.7963581125175252\n",
      "Epoch 64, Training Loss: 0.7960620286184199\n",
      "Epoch 65, Training Loss: 0.7954217637987698\n",
      "Epoch 66, Training Loss: 0.7954379390267765\n",
      "Epoch 67, Training Loss: 0.7956948954918805\n",
      "Epoch 68, Training Loss: 0.7959164176968967\n",
      "Epoch 69, Training Loss: 0.7960832320241367\n",
      "Epoch 70, Training Loss: 0.7956807073424844\n",
      "Epoch 71, Training Loss: 0.7957023705454433\n",
      "Epoch 72, Training Loss: 0.7952653112131006\n",
      "Epoch 73, Training Loss: 0.7956993577760808\n",
      "Epoch 74, Training Loss: 0.7954809666381163\n",
      "Epoch 75, Training Loss: 0.7958253716020023\n",
      "Epoch 76, Training Loss: 0.7957363614615272\n",
      "Epoch 77, Training Loss: 0.7954873185298023\n",
      "Epoch 78, Training Loss: 0.7956933156883015\n",
      "Epoch 79, Training Loss: 0.795208095031626\n",
      "Epoch 80, Training Loss: 0.7960098211204304\n",
      "Epoch 81, Training Loss: 0.7950071574659908\n",
      "Epoch 82, Training Loss: 0.7952401147870456\n",
      "Epoch 83, Training Loss: 0.7951189731149112\n",
      "Epoch 84, Training Loss: 0.7955109188837164\n",
      "Epoch 85, Training Loss: 0.7949499755045947\n",
      "Epoch 86, Training Loss: 0.7945155051876517\n",
      "Epoch 87, Training Loss: 0.7946185125322903\n",
      "Epoch 88, Training Loss: 0.7960894688437967\n",
      "Epoch 89, Training Loss: 0.7954452496416429\n",
      "Epoch 90, Training Loss: 0.7951466228681452\n",
      "Epoch 91, Training Loss: 0.794564225954168\n",
      "Epoch 92, Training Loss: 0.7956555053767036\n",
      "Epoch 93, Training Loss: 0.7947839923465954\n",
      "Epoch 94, Training Loss: 0.795085422221352\n",
      "Epoch 95, Training Loss: 0.7953296249053058\n",
      "Epoch 96, Training Loss: 0.7949639398210189\n",
      "Epoch 97, Training Loss: 0.7952991224036497\n",
      "Epoch 98, Training Loss: 0.7949215550282422\n",
      "Epoch 99, Training Loss: 0.7953027069568634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:08:44,886] Trial 146 finished with value: 0.6257333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7955083058160894\n",
      "Epoch 1, Training Loss: 0.970946170692157\n",
      "Epoch 2, Training Loss: 0.9463067389968642\n",
      "Epoch 3, Training Loss: 0.933989513637428\n",
      "Epoch 4, Training Loss: 0.921774264833981\n",
      "Epoch 5, Training Loss: 0.9082603259194166\n",
      "Epoch 6, Training Loss: 0.8940542290981551\n",
      "Epoch 7, Training Loss: 0.8800352650477474\n",
      "Epoch 8, Training Loss: 0.8669113689795472\n",
      "Epoch 9, Training Loss: 0.8543946062711845\n",
      "Epoch 10, Training Loss: 0.8441735575969954\n",
      "Epoch 11, Training Loss: 0.8354130788853295\n",
      "Epoch 12, Training Loss: 0.828773922489998\n",
      "Epoch 13, Training Loss: 0.8245617095689128\n",
      "Epoch 14, Training Loss: 0.8205608486232902\n",
      "Epoch 15, Training Loss: 0.817516863614993\n",
      "Epoch 16, Training Loss: 0.8154213713524037\n",
      "Epoch 17, Training Loss: 0.8141875924920677\n",
      "Epoch 18, Training Loss: 0.8124315718062838\n",
      "Epoch 19, Training Loss: 0.8114254213813552\n",
      "Epoch 20, Training Loss: 0.8108192145376277\n",
      "Epoch 21, Training Loss: 0.811064598972636\n",
      "Epoch 22, Training Loss: 0.8100372367335441\n",
      "Epoch 23, Training Loss: 0.809448594347875\n",
      "Epoch 24, Training Loss: 0.8086992599910363\n",
      "Epoch 25, Training Loss: 0.8099124705881104\n",
      "Epoch 26, Training Loss: 0.8078022194984263\n",
      "Epoch 27, Training Loss: 0.808170503601992\n",
      "Epoch 28, Training Loss: 0.8071161604465399\n",
      "Epoch 29, Training Loss: 0.8063125542231968\n",
      "Epoch 30, Training Loss: 0.8063255153204265\n",
      "Epoch 31, Training Loss: 0.8060214031907849\n",
      "Epoch 32, Training Loss: 0.8056280141486261\n",
      "Epoch 33, Training Loss: 0.8054608544908968\n",
      "Epoch 34, Training Loss: 0.8053467986278964\n",
      "Epoch 35, Training Loss: 0.8046203917130492\n",
      "Epoch 36, Training Loss: 0.8045228949166778\n",
      "Epoch 37, Training Loss: 0.8046474653975408\n",
      "Epoch 38, Training Loss: 0.8040887324433578\n",
      "Epoch 39, Training Loss: 0.8037707431872089\n",
      "Epoch 40, Training Loss: 0.8041722810358033\n",
      "Epoch 41, Training Loss: 0.8036696905480292\n",
      "Epoch 42, Training Loss: 0.8030937813278428\n",
      "Epoch 43, Training Loss: 0.8027427510211342\n",
      "Epoch 44, Training Loss: 0.8031771762030465\n",
      "Epoch 45, Training Loss: 0.8028787133388949\n",
      "Epoch 46, Training Loss: 0.8026533404687294\n",
      "Epoch 47, Training Loss: 0.8028746255358359\n",
      "Epoch 48, Training Loss: 0.8017079470749188\n",
      "Epoch 49, Training Loss: 0.8025794662031016\n",
      "Epoch 50, Training Loss: 0.8017538575301493\n",
      "Epoch 51, Training Loss: 0.8015720690103402\n",
      "Epoch 52, Training Loss: 0.8014975160584414\n",
      "Epoch 53, Training Loss: 0.8019517713919618\n",
      "Epoch 54, Training Loss: 0.8011138962623768\n",
      "Epoch 55, Training Loss: 0.8011097086999649\n",
      "Epoch 56, Training Loss: 0.8013174988273392\n",
      "Epoch 57, Training Loss: 0.8012116661645416\n",
      "Epoch 58, Training Loss: 0.8009178806068306\n",
      "Epoch 59, Training Loss: 0.8009473135596827\n",
      "Epoch 60, Training Loss: 0.8006291107127541\n",
      "Epoch 61, Training Loss: 0.8003293306307685\n",
      "Epoch 62, Training Loss: 0.800399981315871\n",
      "Epoch 63, Training Loss: 0.8000804476271894\n",
      "Epoch 64, Training Loss: 0.8002535138811384\n",
      "Epoch 65, Training Loss: 0.8009108056699423\n",
      "Epoch 66, Training Loss: 0.8000415337713142\n",
      "Epoch 67, Training Loss: 0.7999198903714804\n",
      "Epoch 68, Training Loss: 0.800527540543922\n",
      "Epoch 69, Training Loss: 0.7996784381400374\n",
      "Epoch 70, Training Loss: 0.8001919516943451\n",
      "Epoch 71, Training Loss: 0.7993029173155476\n",
      "Epoch 72, Training Loss: 0.7996081885538603\n",
      "Epoch 73, Training Loss: 0.8003026216549981\n",
      "Epoch 74, Training Loss: 0.7997789405342332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:09:47,258] Trial 147 finished with value: 0.6323333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7990388812875389\n",
      "Epoch 1, Training Loss: 0.8461487690140219\n",
      "Epoch 2, Training Loss: 0.8130550078784718\n",
      "Epoch 3, Training Loss: 0.8062593673257267\n",
      "Epoch 4, Training Loss: 0.8061556311214672\n",
      "Epoch 5, Training Loss: 0.8019942852328805\n",
      "Epoch 6, Training Loss: 0.8019294333457947\n",
      "Epoch 7, Training Loss: 0.8013008159048417\n",
      "Epoch 8, Training Loss: 0.7996197018202613\n",
      "Epoch 9, Training Loss: 0.7993772151890923\n",
      "Epoch 10, Training Loss: 0.7983144112895517\n",
      "Epoch 11, Training Loss: 0.7998699465218713\n",
      "Epoch 12, Training Loss: 0.7991216968087589\n",
      "Epoch 13, Training Loss: 0.7989806253769819\n",
      "Epoch 14, Training Loss: 0.7981135978418238\n",
      "Epoch 15, Training Loss: 0.7985998250456418\n",
      "Epoch 16, Training Loss: 0.7986332443882437\n",
      "Epoch 17, Training Loss: 0.7975211684142842\n",
      "Epoch 18, Training Loss: 0.7982703928386464\n",
      "Epoch 19, Training Loss: 0.7961100806208218\n",
      "Epoch 20, Training Loss: 0.7973075922096476\n",
      "Epoch 21, Training Loss: 0.796392985161613\n",
      "Epoch 22, Training Loss: 0.7986874556541443\n",
      "Epoch 23, Training Loss: 0.7954594852643855\n",
      "Epoch 24, Training Loss: 0.795533102610532\n",
      "Epoch 25, Training Loss: 0.7958546561353347\n",
      "Epoch 26, Training Loss: 0.7965630344082327\n",
      "Epoch 27, Training Loss: 0.7954088875125436\n",
      "Epoch 28, Training Loss: 0.7976920707085553\n",
      "Epoch 29, Training Loss: 0.794479639600305\n",
      "Epoch 30, Training Loss: 0.7955172983337851\n",
      "Epoch 31, Training Loss: 0.7950101238839766\n",
      "Epoch 32, Training Loss: 0.7964878479172202\n",
      "Epoch 33, Training Loss: 0.7960569520557628\n",
      "Epoch 34, Training Loss: 0.7943119043462417\n",
      "Epoch 35, Training Loss: 0.7937875027516309\n",
      "Epoch 36, Training Loss: 0.7945359958620632\n",
      "Epoch 37, Training Loss: 0.7950748778090757\n",
      "Epoch 38, Training Loss: 0.7948603443538441\n",
      "Epoch 39, Training Loss: 0.796353388744242\n",
      "Epoch 40, Training Loss: 0.7948439586863798\n",
      "Epoch 41, Training Loss: 0.7939775133132935\n",
      "Epoch 42, Training Loss: 0.7943062386793249\n",
      "Epoch 43, Training Loss: 0.7954062641368193\n",
      "Epoch 44, Training Loss: 0.7952497722120846\n",
      "Epoch 45, Training Loss: 0.7939345485322615\n",
      "Epoch 46, Training Loss: 0.7931721832471735\n",
      "Epoch 47, Training Loss: 0.7934339427246767\n",
      "Epoch 48, Training Loss: 0.7940906169835259\n",
      "Epoch 49, Training Loss: 0.7940483933336595\n",
      "Epoch 50, Training Loss: 0.7927496048282174\n",
      "Epoch 51, Training Loss: 0.7956583330911748\n",
      "Epoch 52, Training Loss: 0.7933403549474828\n",
      "Epoch 53, Training Loss: 0.7938291328093585\n",
      "Epoch 54, Training Loss: 0.7952504207807428\n",
      "Epoch 55, Training Loss: 0.7938387797159308\n",
      "Epoch 56, Training Loss: 0.7931956974899068\n",
      "Epoch 57, Training Loss: 0.7936791411568137\n",
      "Epoch 58, Training Loss: 0.7934615305592032\n",
      "Epoch 59, Training Loss: 0.7947273487904493\n",
      "Epoch 60, Training Loss: 0.7940166502139148\n",
      "Epoch 61, Training Loss: 0.7934526779371149\n",
      "Epoch 62, Training Loss: 0.7948263411662158\n",
      "Epoch 63, Training Loss: 0.7927646996694453\n",
      "Epoch 64, Training Loss: 0.7944610085908105\n",
      "Epoch 65, Training Loss: 0.7934315337854273\n",
      "Epoch 66, Training Loss: 0.794106139084872\n",
      "Epoch 67, Training Loss: 0.7931852971105015\n",
      "Epoch 68, Training Loss: 0.7932443304622875\n",
      "Epoch 69, Training Loss: 0.794088322765687\n",
      "Epoch 70, Training Loss: 0.7938046309527229\n",
      "Epoch 71, Training Loss: 0.7941047199333415\n",
      "Epoch 72, Training Loss: 0.7932092081097996\n",
      "Epoch 73, Training Loss: 0.7925882521797629\n",
      "Epoch 74, Training Loss: 0.7932398383056416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:11:29,733] Trial 148 finished with value: 0.6288666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7935079544432023\n",
      "Epoch 1, Training Loss: 0.9497604617498872\n",
      "Epoch 2, Training Loss: 0.8772762681308546\n",
      "Epoch 3, Training Loss: 0.8311780041321776\n",
      "Epoch 4, Training Loss: 0.8161610138147397\n",
      "Epoch 5, Training Loss: 0.8110759744966837\n",
      "Epoch 6, Training Loss: 0.8098429014808253\n",
      "Epoch 7, Training Loss: 0.808507017623213\n",
      "Epoch 8, Training Loss: 0.8077861876416027\n",
      "Epoch 9, Training Loss: 0.807137672076548\n",
      "Epoch 10, Training Loss: 0.8072393436180918\n",
      "Epoch 11, Training Loss: 0.8067093107933389\n",
      "Epoch 12, Training Loss: 0.8081433855501333\n",
      "Epoch 13, Training Loss: 0.8067715937033632\n",
      "Epoch 14, Training Loss: 0.8055294299484196\n",
      "Epoch 15, Training Loss: 0.8056748023606781\n",
      "Epoch 16, Training Loss: 0.8052041597832414\n",
      "Epoch 17, Training Loss: 0.8046403088964018\n",
      "Epoch 18, Training Loss: 0.8042539995415766\n",
      "Epoch 19, Training Loss: 0.8041575541173606\n",
      "Epoch 20, Training Loss: 0.8043231003266528\n",
      "Epoch 21, Training Loss: 0.803745935106636\n",
      "Epoch 22, Training Loss: 0.8039406640189034\n",
      "Epoch 23, Training Loss: 0.8030725565171779\n",
      "Epoch 24, Training Loss: 0.8028119815919632\n",
      "Epoch 25, Training Loss: 0.8028589711153418\n",
      "Epoch 26, Training Loss: 0.8024347374761911\n",
      "Epoch 27, Training Loss: 0.8021027290731444\n",
      "Epoch 28, Training Loss: 0.8020979555925929\n",
      "Epoch 29, Training Loss: 0.803768067252367\n",
      "Epoch 30, Training Loss: 0.8017742663397824\n",
      "Epoch 31, Training Loss: 0.8016475880056395\n",
      "Epoch 32, Training Loss: 0.8015568036782114\n",
      "Epoch 33, Training Loss: 0.8016703445212285\n",
      "Epoch 34, Training Loss: 0.8015726957106053\n",
      "Epoch 35, Training Loss: 0.8006771436311249\n",
      "Epoch 36, Training Loss: 0.8009139076211399\n",
      "Epoch 37, Training Loss: 0.8000008218270496\n",
      "Epoch 38, Training Loss: 0.8008528289042021\n",
      "Epoch 39, Training Loss: 0.800562069828349\n",
      "Epoch 40, Training Loss: 0.8001129592271675\n",
      "Epoch 41, Training Loss: 0.7997688635847622\n",
      "Epoch 42, Training Loss: 0.8005519840950356\n",
      "Epoch 43, Training Loss: 0.8004477636258405\n",
      "Epoch 44, Training Loss: 0.7998015197596156\n",
      "Epoch 45, Training Loss: 0.7989551443802683\n",
      "Epoch 46, Training Loss: 0.8002587801531742\n",
      "Epoch 47, Training Loss: 0.7991599514072103\n",
      "Epoch 48, Training Loss: 0.7992695127214704\n",
      "Epoch 49, Training Loss: 0.7998757112295107\n",
      "Epoch 50, Training Loss: 0.7991719752326047\n",
      "Epoch 51, Training Loss: 0.7987506465804308\n",
      "Epoch 52, Training Loss: 0.7981811080660138\n",
      "Epoch 53, Training Loss: 0.7985608477341501\n",
      "Epoch 54, Training Loss: 0.7981163020420792\n",
      "Epoch 55, Training Loss: 0.79875951288338\n",
      "Epoch 56, Training Loss: 0.7985048146176159\n",
      "Epoch 57, Training Loss: 0.7988086442302044\n",
      "Epoch 58, Training Loss: 0.7977626319218399\n",
      "Epoch 59, Training Loss: 0.7976351065743238\n",
      "Epoch 60, Training Loss: 0.7980314620455405\n",
      "Epoch 61, Training Loss: 0.798736185866191\n",
      "Epoch 62, Training Loss: 0.7980403521007164\n",
      "Epoch 63, Training Loss: 0.7988789242909367\n",
      "Epoch 64, Training Loss: 0.7983167845503728\n",
      "Epoch 65, Training Loss: 0.7976892465935614\n",
      "Epoch 66, Training Loss: 0.797531489411691\n",
      "Epoch 67, Training Loss: 0.7975231471366452\n",
      "Epoch 68, Training Loss: 0.7971925365297418\n",
      "Epoch 69, Training Loss: 0.7975947619380808\n",
      "Epoch 70, Training Loss: 0.7975662879477766\n",
      "Epoch 71, Training Loss: 0.7986468121521455\n",
      "Epoch 72, Training Loss: 0.7973256660583324\n",
      "Epoch 73, Training Loss: 0.7976329105240958\n",
      "Epoch 74, Training Loss: 0.7972350628752457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:12:44,335] Trial 149 finished with value: 0.6341333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7970151813406693\n",
      "Epoch 1, Training Loss: 0.8478619360222536\n",
      "Epoch 2, Training Loss: 0.8140110394533943\n",
      "Epoch 3, Training Loss: 0.8112394050289603\n",
      "Epoch 4, Training Loss: 0.8079645219269921\n",
      "Epoch 5, Training Loss: 0.8057881684163037\n",
      "Epoch 6, Training Loss: 0.8037025772122776\n",
      "Epoch 7, Training Loss: 0.8052878583880032\n",
      "Epoch 8, Training Loss: 0.8051534673746894\n",
      "Epoch 9, Training Loss: 0.8035124937927022\n",
      "Epoch 10, Training Loss: 0.8030803056324229\n",
      "Epoch 11, Training Loss: 0.8010420946513905\n",
      "Epoch 12, Training Loss: 0.8015834293645971\n",
      "Epoch 13, Training Loss: 0.8001520268356099\n",
      "Epoch 14, Training Loss: 0.8009989160649916\n",
      "Epoch 15, Training Loss: 0.8004263997779173\n",
      "Epoch 16, Training Loss: 0.7999299529720755\n",
      "Epoch 17, Training Loss: 0.800052255672567\n",
      "Epoch 18, Training Loss: 0.7997085453482236\n",
      "Epoch 19, Training Loss: 0.8001232192796819\n",
      "Epoch 20, Training Loss: 0.7983232946956859\n",
      "Epoch 21, Training Loss: 0.8001402679611654\n",
      "Epoch 22, Training Loss: 0.7989209917012383\n",
      "Epoch 23, Training Loss: 0.7994247717717115\n",
      "Epoch 24, Training Loss: 0.7989736529658823\n",
      "Epoch 25, Training Loss: 0.7980115634553573\n",
      "Epoch 26, Training Loss: 0.7980481347616981\n",
      "Epoch 27, Training Loss: 0.7972005083981683\n",
      "Epoch 28, Training Loss: 0.7977784710070667\n",
      "Epoch 29, Training Loss: 0.7981342924342436\n",
      "Epoch 30, Training Loss: 0.798734301258536\n",
      "Epoch 31, Training Loss: 0.7993958766320173\n",
      "Epoch 32, Training Loss: 0.7972880554199219\n",
      "Epoch 33, Training Loss: 0.7965011314784779\n",
      "Epoch 34, Training Loss: 0.7969630637589623\n",
      "Epoch 35, Training Loss: 0.7971296012401581\n",
      "Epoch 36, Training Loss: 0.7985543300123775\n",
      "Epoch 37, Training Loss: 0.7990055359812344\n",
      "Epoch 38, Training Loss: 0.7979028609920951\n",
      "Epoch 39, Training Loss: 0.7996027301339542\n",
      "Epoch 40, Training Loss: 0.7978172870243296\n",
      "Epoch 41, Training Loss: 0.7980631412478054\n",
      "Epoch 42, Training Loss: 0.79652620343601\n",
      "Epoch 43, Training Loss: 0.7977057938715991\n",
      "Epoch 44, Training Loss: 0.7977910414162804\n",
      "Epoch 45, Training Loss: 0.7971709203720093\n",
      "Epoch 46, Training Loss: 0.7964306632210226\n",
      "Epoch 47, Training Loss: 0.7971076029188493\n",
      "Epoch 48, Training Loss: 0.7973589425227221\n",
      "Epoch 49, Training Loss: 0.7968820348206689\n",
      "Epoch 50, Training Loss: 0.7978623477851643\n",
      "Epoch 51, Training Loss: 0.7986341509398293\n",
      "Epoch 52, Training Loss: 0.7967364776835723\n",
      "Epoch 53, Training Loss: 0.7976839414764854\n",
      "Epoch 54, Training Loss: 0.7972392824116875\n",
      "Epoch 55, Training Loss: 0.7970261160766378\n",
      "Epoch 56, Training Loss: 0.7970777916908264\n",
      "Epoch 57, Training Loss: 0.7964228951931\n",
      "Epoch 58, Training Loss: 0.7977573294499342\n",
      "Epoch 59, Training Loss: 0.7962819909348208\n",
      "Epoch 60, Training Loss: 0.797803163949181\n",
      "Epoch 61, Training Loss: 0.797934882851208\n",
      "Epoch 62, Training Loss: 0.7967232694345362\n",
      "Epoch 63, Training Loss: 0.7964812023499432\n",
      "Epoch 64, Training Loss: 0.7969529149812811\n",
      "Epoch 65, Training Loss: 0.7961121275144465\n",
      "Epoch 66, Training Loss: 0.7961686489862554\n",
      "Epoch 67, Training Loss: 0.7970317226998946\n",
      "Epoch 68, Training Loss: 0.7959429193945492\n",
      "Epoch 69, Training Loss: 0.7974990619631375\n",
      "Epoch 70, Training Loss: 0.7969270875173456\n",
      "Epoch 71, Training Loss: 0.7972806852004107\n",
      "Epoch 72, Training Loss: 0.7965938882266774\n",
      "Epoch 73, Training Loss: 0.7973954775052912\n",
      "Epoch 74, Training Loss: 0.7972209558767431\n",
      "Epoch 75, Training Loss: 0.7970715241572436\n",
      "Epoch 76, Training Loss: 0.7963879524259007\n",
      "Epoch 77, Training Loss: 0.7963433868043563\n",
      "Epoch 78, Training Loss: 0.7965812685910393\n",
      "Epoch 79, Training Loss: 0.7969882648131427\n",
      "Epoch 80, Training Loss: 0.7953074585690217\n",
      "Epoch 81, Training Loss: 0.7955656662408044\n",
      "Epoch 82, Training Loss: 0.7946434990097495\n",
      "Epoch 83, Training Loss: 0.7958515592883615\n",
      "Epoch 84, Training Loss: 0.7965995458995595\n",
      "Epoch 85, Training Loss: 0.7951678714331458\n",
      "Epoch 86, Training Loss: 0.7963031877489651\n",
      "Epoch 87, Training Loss: 0.7966563818034004\n",
      "Epoch 88, Training Loss: 0.7959020156720106\n",
      "Epoch 89, Training Loss: 0.796214137357824\n",
      "Epoch 90, Training Loss: 0.7961966652028701\n",
      "Epoch 91, Training Loss: 0.7974965207015767\n",
      "Epoch 92, Training Loss: 0.7965791384612813\n",
      "Epoch 93, Training Loss: 0.7964915035752689\n",
      "Epoch 94, Training Loss: 0.7970458012468674\n",
      "Epoch 95, Training Loss: 0.7960349603961496\n",
      "Epoch 96, Training Loss: 0.796136522363214\n",
      "Epoch 97, Training Loss: 0.7952380514144898\n",
      "Epoch 98, Training Loss: 0.7963954872243545\n",
      "Epoch 99, Training Loss: 0.7965517544746399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:23:41,032] Trial 150 finished with value: 0.6258 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7959060591108659\n",
      "Epoch 1, Training Loss: 1.0898844523537428\n",
      "Epoch 2, Training Loss: 1.0843026598593346\n",
      "Epoch 3, Training Loss: 1.0797021206160238\n",
      "Epoch 4, Training Loss: 1.0752824423008396\n",
      "Epoch 5, Training Loss: 1.0712225182612138\n",
      "Epoch 6, Training Loss: 1.0671378440426704\n",
      "Epoch 7, Training Loss: 1.0632453827033366\n",
      "Epoch 8, Training Loss: 1.0594099950073357\n",
      "Epoch 9, Training Loss: 1.0557845357665443\n",
      "Epoch 10, Training Loss: 1.0524021012442453\n",
      "Epoch 11, Training Loss: 1.0490604499228915\n",
      "Epoch 12, Training Loss: 1.0458368408948855\n",
      "Epoch 13, Training Loss: 1.0425131266278431\n",
      "Epoch 14, Training Loss: 1.039524719858528\n",
      "Epoch 15, Training Loss: 1.0363082586374497\n",
      "Epoch 16, Training Loss: 1.033362441134632\n",
      "Epoch 17, Training Loss: 1.0307355258697855\n",
      "Epoch 18, Training Loss: 1.0278423869520201\n",
      "Epoch 19, Training Loss: 1.02488645548211\n",
      "Epoch 20, Training Loss: 1.0223512017637266\n",
      "Epoch 21, Training Loss: 1.0198583931851208\n",
      "Epoch 22, Training Loss: 1.0174886426531282\n",
      "Epoch 23, Training Loss: 1.0151985411357163\n",
      "Epoch 24, Training Loss: 1.0127762228922736\n",
      "Epoch 25, Training Loss: 1.010313748506675\n",
      "Epoch 26, Training Loss: 1.0081352609440797\n",
      "Epoch 27, Training Loss: 1.0061192742863991\n",
      "Epoch 28, Training Loss: 1.0041616396796433\n",
      "Epoch 29, Training Loss: 1.0021752206006445\n",
      "Epoch 30, Training Loss: 0.9999550817604351\n",
      "Epoch 31, Training Loss: 0.9982314038097411\n",
      "Epoch 32, Training Loss: 0.9961945348216179\n",
      "Epoch 33, Training Loss: 0.9946515624684499\n",
      "Epoch 34, Training Loss: 0.9928759748774364\n",
      "Epoch 35, Training Loss: 0.9915261187051472\n",
      "Epoch 36, Training Loss: 0.9898212372808528\n",
      "Epoch 37, Training Loss: 0.9883293547128377\n",
      "Epoch 38, Training Loss: 0.9869489054034527\n",
      "Epoch 39, Training Loss: 0.9853660605903855\n",
      "Epoch 40, Training Loss: 0.984261076880577\n",
      "Epoch 41, Training Loss: 0.9831592803610895\n",
      "Epoch 42, Training Loss: 0.9816268310511023\n",
      "Epoch 43, Training Loss: 0.9805876542751054\n",
      "Epoch 44, Training Loss: 0.9794118793387162\n",
      "Epoch 45, Training Loss: 0.978364430423966\n",
      "Epoch 46, Training Loss: 0.9775365128553003\n",
      "Epoch 47, Training Loss: 0.9762662640191558\n",
      "Epoch 48, Training Loss: 0.9756848093262293\n",
      "Epoch 49, Training Loss: 0.9747794072430833\n",
      "Epoch 50, Training Loss: 0.9738527116918922\n",
      "Epoch 51, Training Loss: 0.9730700202454302\n",
      "Epoch 52, Training Loss: 0.972411881084729\n",
      "Epoch 53, Training Loss: 0.9713216977011888\n",
      "Epoch 54, Training Loss: 0.9709739930647656\n",
      "Epoch 55, Training Loss: 0.9700856379996565\n",
      "Epoch 56, Training Loss: 0.9691588902831973\n",
      "Epoch 57, Training Loss: 0.9690203337741078\n",
      "Epoch 58, Training Loss: 0.968097508401799\n",
      "Epoch 59, Training Loss: 0.9674377013866167\n",
      "Epoch 60, Training Loss: 0.9672138890825717\n",
      "Epoch 61, Training Loss: 0.9666660894128613\n",
      "Epoch 62, Training Loss: 0.9661835639100326\n",
      "Epoch 63, Training Loss: 0.9655102468971023\n",
      "Epoch 64, Training Loss: 0.9653242810328204\n",
      "Epoch 65, Training Loss: 0.9650457252237133\n",
      "Epoch 66, Training Loss: 0.9646307559837972\n",
      "Epoch 67, Training Loss: 0.9639103214543565\n",
      "Epoch 68, Training Loss: 0.9637607891756789\n",
      "Epoch 69, Training Loss: 0.9630746621834604\n",
      "Epoch 70, Training Loss: 0.9632719044398544\n",
      "Epoch 71, Training Loss: 0.9625735167273901\n",
      "Epoch 72, Training Loss: 0.962726630752248\n",
      "Epoch 73, Training Loss: 0.9622406059638002\n",
      "Epoch 74, Training Loss: 0.9616586406428115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:24:43,362] Trial 151 finished with value: 0.5267333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9616479996451758\n",
      "Epoch 1, Training Loss: 1.0895735398460837\n",
      "Epoch 2, Training Loss: 1.083508555468391\n",
      "Epoch 3, Training Loss: 1.0755254724446466\n",
      "Epoch 4, Training Loss: 1.062782181852004\n",
      "Epoch 5, Training Loss: 1.0425264446174398\n",
      "Epoch 6, Training Loss: 1.016527615855722\n",
      "Epoch 7, Training Loss: 0.992637748297523\n",
      "Epoch 8, Training Loss: 0.9767382098646725\n",
      "Epoch 9, Training Loss: 0.9674279849669513\n",
      "Epoch 10, Training Loss: 0.961973156858893\n",
      "Epoch 11, Training Loss: 0.9586849315026227\n",
      "Epoch 12, Training Loss: 0.9564326235126046\n",
      "Epoch 13, Training Loss: 0.9545285764161279\n",
      "Epoch 14, Training Loss: 0.9527582175591413\n",
      "Epoch 15, Training Loss: 0.951106117402806\n",
      "Epoch 16, Training Loss: 0.9491972145613502\n",
      "Epoch 17, Training Loss: 0.9471455389611861\n",
      "Epoch 18, Training Loss: 0.9451616594370673\n",
      "Epoch 19, Training Loss: 0.9428334923351512\n",
      "Epoch 20, Training Loss: 0.9407357721468982\n",
      "Epoch 21, Training Loss: 0.9380835746316348\n",
      "Epoch 22, Training Loss: 0.9352609244514914\n",
      "Epoch 23, Training Loss: 0.9320940352888668\n",
      "Epoch 24, Training Loss: 0.9288381084273843\n",
      "Epoch 25, Training Loss: 0.9250315741230459\n",
      "Epoch 26, Training Loss: 0.9209004256304573\n",
      "Epoch 27, Training Loss: 0.9162970451046438\n",
      "Epoch 28, Training Loss: 0.9112971563198987\n",
      "Epoch 29, Training Loss: 0.9059205034901114\n",
      "Epoch 30, Training Loss: 0.8999443511401906\n",
      "Epoch 31, Training Loss: 0.893706907454659\n",
      "Epoch 32, Training Loss: 0.8873547226541183\n",
      "Epoch 33, Training Loss: 0.8808186918847701\n",
      "Epoch 34, Training Loss: 0.8743916756966534\n",
      "Epoch 35, Training Loss: 0.8680012324978323\n",
      "Epoch 36, Training Loss: 0.8622350992875941\n",
      "Epoch 37, Training Loss: 0.856811827210819\n",
      "Epoch 38, Training Loss: 0.8522126552637885\n",
      "Epoch 39, Training Loss: 0.8479683571002062\n",
      "Epoch 40, Training Loss: 0.8442050019432517\n",
      "Epoch 41, Training Loss: 0.8410321745451759\n",
      "Epoch 42, Training Loss: 0.8382871775066152\n",
      "Epoch 43, Training Loss: 0.8360694103381213\n",
      "Epoch 44, Training Loss: 0.8337604347397299\n",
      "Epoch 45, Training Loss: 0.8320893363391652\n",
      "Epoch 46, Training Loss: 0.8303607899301192\n",
      "Epoch 47, Training Loss: 0.8289569997086245\n",
      "Epoch 48, Training Loss: 0.8278657554177677\n",
      "Epoch 49, Training Loss: 0.8264485335350037\n",
      "Epoch 50, Training Loss: 0.8252588147275588\n",
      "Epoch 51, Training Loss: 0.8242443578383502\n",
      "Epoch 52, Training Loss: 0.8232359296434066\n",
      "Epoch 53, Training Loss: 0.8224034769394818\n",
      "Epoch 54, Training Loss: 0.8215041354123284\n",
      "Epoch 55, Training Loss: 0.8205891110616572\n",
      "Epoch 56, Training Loss: 0.8197865820632262\n",
      "Epoch 57, Training Loss: 0.8190590826202842\n",
      "Epoch 58, Training Loss: 0.8182884385305292\n",
      "Epoch 59, Training Loss: 0.8176468668965733\n",
      "Epoch 60, Training Loss: 0.8170196999521816\n",
      "Epoch 61, Training Loss: 0.8165048156065099\n",
      "Epoch 62, Training Loss: 0.8156640413228203\n",
      "Epoch 63, Training Loss: 0.8152180972520043\n",
      "Epoch 64, Training Loss: 0.8145442879199982\n",
      "Epoch 65, Training Loss: 0.8139294542985804\n",
      "Epoch 66, Training Loss: 0.813901179117315\n",
      "Epoch 67, Training Loss: 0.813225723364774\n",
      "Epoch 68, Training Loss: 0.812972381255206\n",
      "Epoch 69, Training Loss: 0.8125556462652543\n",
      "Epoch 70, Training Loss: 0.812380187441321\n",
      "Epoch 71, Training Loss: 0.811741937048295\n",
      "Epoch 72, Training Loss: 0.8113546808326946\n",
      "Epoch 73, Training Loss: 0.8112351444889517\n",
      "Epoch 74, Training Loss: 0.811050197166555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:26:04,957] Trial 152 finished with value: 0.6269333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8108828719223247\n",
      "Epoch 1, Training Loss: 0.8988493004239592\n",
      "Epoch 2, Training Loss: 0.8285712176695802\n",
      "Epoch 3, Training Loss: 0.8188853978214408\n",
      "Epoch 4, Training Loss: 0.814746083800954\n",
      "Epoch 5, Training Loss: 0.8146023424944483\n",
      "Epoch 6, Training Loss: 0.808670873659894\n",
      "Epoch 7, Training Loss: 0.8096836725571999\n",
      "Epoch 8, Training Loss: 0.8075356304197383\n",
      "Epoch 9, Training Loss: 0.8062777458276964\n",
      "Epoch 10, Training Loss: 0.8058389829513722\n",
      "Epoch 11, Training Loss: 0.804217186906284\n",
      "Epoch 12, Training Loss: 0.8037498457987505\n",
      "Epoch 13, Training Loss: 0.8036324090527412\n",
      "Epoch 14, Training Loss: 0.8034266740755928\n",
      "Epoch 15, Training Loss: 0.8030070867753567\n",
      "Epoch 16, Training Loss: 0.8020552116229122\n",
      "Epoch 17, Training Loss: 0.8024514767460357\n",
      "Epoch 18, Training Loss: 0.8016183968773462\n",
      "Epoch 19, Training Loss: 0.8018308287276361\n",
      "Epoch 20, Training Loss: 0.8005348989838048\n",
      "Epoch 21, Training Loss: 0.8009022328190337\n",
      "Epoch 22, Training Loss: 0.8006541007443478\n",
      "Epoch 23, Training Loss: 0.8002262199731698\n",
      "Epoch 24, Training Loss: 0.8006176904628152\n",
      "Epoch 25, Training Loss: 0.7995468663093739\n",
      "Epoch 26, Training Loss: 0.799539838249522\n",
      "Epoch 27, Training Loss: 0.8002061650269013\n",
      "Epoch 28, Training Loss: 0.8000714308337161\n",
      "Epoch 29, Training Loss: 0.7997331696345393\n",
      "Epoch 30, Training Loss: 0.7996528292060795\n",
      "Epoch 31, Training Loss: 0.7989243323641612\n",
      "Epoch 32, Training Loss: 0.7996002003662568\n",
      "Epoch 33, Training Loss: 0.7993664167877427\n",
      "Epoch 34, Training Loss: 0.8000901779734102\n",
      "Epoch 35, Training Loss: 0.7990488595532296\n",
      "Epoch 36, Training Loss: 0.7991931278006474\n",
      "Epoch 37, Training Loss: 0.7984782941359326\n",
      "Epoch 38, Training Loss: 0.7987657482462718\n",
      "Epoch 39, Training Loss: 0.7987875376428877\n",
      "Epoch 40, Training Loss: 0.7992116746149565\n",
      "Epoch 41, Training Loss: 0.7986834893549295\n",
      "Epoch 42, Training Loss: 0.7993136612096228\n",
      "Epoch 43, Training Loss: 0.7981311849185398\n",
      "Epoch 44, Training Loss: 0.798007825890878\n",
      "Epoch 45, Training Loss: 0.7983493133595115\n",
      "Epoch 46, Training Loss: 0.7978647714270685\n",
      "Epoch 47, Training Loss: 0.7984194747487405\n",
      "Epoch 48, Training Loss: 0.7978045390960865\n",
      "Epoch 49, Training Loss: 0.7988537036386648\n",
      "Epoch 50, Training Loss: 0.7979831595169871\n",
      "Epoch 51, Training Loss: 0.7990458949167926\n",
      "Epoch 52, Training Loss: 0.7987103414714785\n",
      "Epoch 53, Training Loss: 0.7981038772970214\n",
      "Epoch 54, Training Loss: 0.7987921697752817\n",
      "Epoch 55, Training Loss: 0.7975312141547526\n",
      "Epoch 56, Training Loss: 0.7974432581349423\n",
      "Epoch 57, Training Loss: 0.7980605516218602\n",
      "Epoch 58, Training Loss: 0.7978932793875386\n",
      "Epoch 59, Training Loss: 0.7986266187259129\n",
      "Epoch 60, Training Loss: 0.7970528154444874\n",
      "Epoch 61, Training Loss: 0.7980107173883826\n",
      "Epoch 62, Training Loss: 0.7982168473695453\n",
      "Epoch 63, Training Loss: 0.7976782672387317\n",
      "Epoch 64, Training Loss: 0.7972993067332677\n",
      "Epoch 65, Training Loss: 0.7969322159774321\n",
      "Epoch 66, Training Loss: 0.7966176930674933\n",
      "Epoch 67, Training Loss: 0.7994812336183132\n",
      "Epoch 68, Training Loss: 0.7979185940627765\n",
      "Epoch 69, Training Loss: 0.7975385811992157\n",
      "Epoch 70, Training Loss: 0.7975512413153971\n",
      "Epoch 71, Training Loss: 0.7971793060015915\n",
      "Epoch 72, Training Loss: 0.7972495630271452\n",
      "Epoch 73, Training Loss: 0.7967190157201953\n",
      "Epoch 74, Training Loss: 0.7983977551747086\n",
      "Epoch 75, Training Loss: 0.7979542506368537\n",
      "Epoch 76, Training Loss: 0.7967139746013441\n",
      "Epoch 77, Training Loss: 0.7968688270203153\n",
      "Epoch 78, Training Loss: 0.7978877062188056\n",
      "Epoch 79, Training Loss: 0.7965998862022744\n",
      "Epoch 80, Training Loss: 0.7976998066543637\n",
      "Epoch 81, Training Loss: 0.7976714098363891\n",
      "Epoch 82, Training Loss: 0.796516725219282\n",
      "Epoch 83, Training Loss: 0.7973868623712009\n",
      "Epoch 84, Training Loss: 0.7978012498160054\n",
      "Epoch 85, Training Loss: 0.7982555240616762\n",
      "Epoch 86, Training Loss: 0.7972741956101325\n",
      "Epoch 87, Training Loss: 0.7978716836836105\n",
      "Epoch 88, Training Loss: 0.7970982179605871\n",
      "Epoch 89, Training Loss: 0.7964203907134838\n",
      "Epoch 90, Training Loss: 0.7967763829052\n",
      "Epoch 91, Training Loss: 0.7977105293955121\n",
      "Epoch 92, Training Loss: 0.7976590345676681\n",
      "Epoch 93, Training Loss: 0.79713121554009\n",
      "Epoch 94, Training Loss: 0.7974013803596783\n",
      "Epoch 95, Training Loss: 0.7975846010939519\n",
      "Epoch 96, Training Loss: 0.797390766072094\n",
      "Epoch 97, Training Loss: 0.7974151193647456\n",
      "Epoch 98, Training Loss: 0.7967636900736873\n",
      "Epoch 99, Training Loss: 0.7966934260569121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:27:55,945] Trial 153 finished with value: 0.5551333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7971993805770587\n",
      "Epoch 1, Training Loss: 0.8549773588216394\n",
      "Epoch 2, Training Loss: 0.813695948912685\n",
      "Epoch 3, Training Loss: 0.8093105120766432\n",
      "Epoch 4, Training Loss: 0.8055054508653798\n",
      "Epoch 5, Training Loss: 0.8023533601509897\n",
      "Epoch 6, Training Loss: 0.8013925315742206\n",
      "Epoch 7, Training Loss: 0.7990936786608589\n",
      "Epoch 8, Training Loss: 0.7990307839293229\n",
      "Epoch 9, Training Loss: 0.797869098365755\n",
      "Epoch 10, Training Loss: 0.7974574490597374\n",
      "Epoch 11, Training Loss: 0.7967614504627716\n",
      "Epoch 12, Training Loss: 0.796565924730516\n",
      "Epoch 13, Training Loss: 0.7961386729003792\n",
      "Epoch 14, Training Loss: 0.7949801327590655\n",
      "Epoch 15, Training Loss: 0.7946462280768201\n",
      "Epoch 16, Training Loss: 0.7944336842773552\n",
      "Epoch 17, Training Loss: 0.7941126789365496\n",
      "Epoch 18, Training Loss: 0.794673610540261\n",
      "Epoch 19, Training Loss: 0.7936874666608366\n",
      "Epoch 20, Training Loss: 0.7935192578717282\n",
      "Epoch 21, Training Loss: 0.7931916820375543\n",
      "Epoch 22, Training Loss: 0.7932139562484913\n",
      "Epoch 23, Training Loss: 0.7934817518506732\n",
      "Epoch 24, Training Loss: 0.7918758449697854\n",
      "Epoch 25, Training Loss: 0.7926225707046968\n",
      "Epoch 26, Training Loss: 0.7928051927932223\n",
      "Epoch 27, Training Loss: 0.792601568268654\n",
      "Epoch 28, Training Loss: 0.7920227868216378\n",
      "Epoch 29, Training Loss: 0.7918119611596702\n",
      "Epoch 30, Training Loss: 0.7932359267894487\n",
      "Epoch 31, Training Loss: 0.791958192775124\n",
      "Epoch 32, Training Loss: 0.7920843325163188\n",
      "Epoch 33, Training Loss: 0.7921869588973827\n",
      "Epoch 34, Training Loss: 0.7924571261370092\n",
      "Epoch 35, Training Loss: 0.7920330276166586\n",
      "Epoch 36, Training Loss: 0.791108835370917\n",
      "Epoch 37, Training Loss: 0.7904517748302087\n",
      "Epoch 38, Training Loss: 0.7913278937339783\n",
      "Epoch 39, Training Loss: 0.791364682437782\n",
      "Epoch 40, Training Loss: 0.7906936048565054\n",
      "Epoch 41, Training Loss: 0.7912689516418858\n",
      "Epoch 42, Training Loss: 0.7903913353618822\n",
      "Epoch 43, Training Loss: 0.7909706311118334\n",
      "Epoch 44, Training Loss: 0.7901374177825182\n",
      "Epoch 45, Training Loss: 0.7908509417584068\n",
      "Epoch 46, Training Loss: 0.7897302188371357\n",
      "Epoch 47, Training Loss: 0.7898862260624878\n",
      "Epoch 48, Training Loss: 0.7903026676715765\n",
      "Epoch 49, Training Loss: 0.7894197790246261\n",
      "Epoch 50, Training Loss: 0.7901581697894219\n",
      "Epoch 51, Training Loss: 0.7893712316240583\n",
      "Epoch 52, Training Loss: 0.7894964534537237\n",
      "Epoch 53, Training Loss: 0.7896618103622494\n",
      "Epoch 54, Training Loss: 0.7898036187752745\n",
      "Epoch 55, Training Loss: 0.7899865663141237\n",
      "Epoch 56, Training Loss: 0.7901981412916255\n",
      "Epoch 57, Training Loss: 0.789399654703929\n",
      "Epoch 58, Training Loss: 0.789442069100258\n",
      "Epoch 59, Training Loss: 0.7893643212497683\n",
      "Epoch 60, Training Loss: 0.789904258663493\n",
      "Epoch 61, Training Loss: 0.7887743141418113\n",
      "Epoch 62, Training Loss: 0.788995771659048\n",
      "Epoch 63, Training Loss: 0.7891467679712109\n",
      "Epoch 64, Training Loss: 0.7893511673561613\n",
      "Epoch 65, Training Loss: 0.7895962613865846\n",
      "Epoch 66, Training Loss: 0.787798456814056\n",
      "Epoch 67, Training Loss: 0.7892871256161453\n",
      "Epoch 68, Training Loss: 0.789635141451556\n",
      "Epoch 69, Training Loss: 0.7889105520750347\n",
      "Epoch 70, Training Loss: 0.7890615244557087\n",
      "Epoch 71, Training Loss: 0.7881851861351414\n",
      "Epoch 72, Training Loss: 0.7890747306938458\n",
      "Epoch 73, Training Loss: 0.7890060258987255\n",
      "Epoch 74, Training Loss: 0.7886750146858674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:29:21,467] Trial 154 finished with value: 0.6118 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7882172184779231\n",
      "Epoch 1, Training Loss: 0.9792090237588811\n",
      "Epoch 2, Training Loss: 0.9493057922313087\n",
      "Epoch 3, Training Loss: 0.9347432798012755\n",
      "Epoch 4, Training Loss: 0.9173547923116756\n",
      "Epoch 5, Training Loss: 0.8977843100863292\n",
      "Epoch 6, Training Loss: 0.876439096336078\n",
      "Epoch 7, Training Loss: 0.8581257072606481\n",
      "Epoch 8, Training Loss: 0.8428175071128329\n",
      "Epoch 9, Training Loss: 0.8325283610731139\n",
      "Epoch 10, Training Loss: 0.8251508231449844\n",
      "Epoch 11, Training Loss: 0.8207408907718228\n",
      "Epoch 12, Training Loss: 0.8164812269963716\n",
      "Epoch 13, Training Loss: 0.8151979413247646\n",
      "Epoch 14, Training Loss: 0.8135528571623608\n",
      "Epoch 15, Training Loss: 0.8128725695430784\n",
      "Epoch 16, Training Loss: 0.8114131777806389\n",
      "Epoch 17, Training Loss: 0.8109845348766872\n",
      "Epoch 18, Training Loss: 0.8102978268064054\n",
      "Epoch 19, Training Loss: 0.8095761541137122\n",
      "Epoch 20, Training Loss: 0.809598762200291\n",
      "Epoch 21, Training Loss: 0.8090431216964148\n",
      "Epoch 22, Training Loss: 0.808240616007855\n",
      "Epoch 23, Training Loss: 0.8083929449992072\n",
      "Epoch 24, Training Loss: 0.8077635480945272\n",
      "Epoch 25, Training Loss: 0.8081222389873706\n",
      "Epoch 26, Training Loss: 0.80731435978323\n",
      "Epoch 27, Training Loss: 0.8075995272263549\n",
      "Epoch 28, Training Loss: 0.8069404681822411\n",
      "Epoch 29, Training Loss: 0.8070706705401715\n",
      "Epoch 30, Training Loss: 0.8065715705541739\n",
      "Epoch 31, Training Loss: 0.806431806177125\n",
      "Epoch 32, Training Loss: 0.8057605172010293\n",
      "Epoch 33, Training Loss: 0.8053767044741408\n",
      "Epoch 34, Training Loss: 0.8057203425500626\n",
      "Epoch 35, Training Loss: 0.8051358975862202\n",
      "Epoch 36, Training Loss: 0.8052014042560319\n",
      "Epoch 37, Training Loss: 0.8056068311060282\n",
      "Epoch 38, Training Loss: 0.8050852396434411\n",
      "Epoch 39, Training Loss: 0.8051839322972119\n",
      "Epoch 40, Training Loss: 0.8049741062006556\n",
      "Epoch 41, Training Loss: 0.8041811659820097\n",
      "Epoch 42, Training Loss: 0.8047064107163509\n",
      "Epoch 43, Training Loss: 0.8040896455148109\n",
      "Epoch 44, Training Loss: 0.8037412514363913\n",
      "Epoch 45, Training Loss: 0.8040527168969462\n",
      "Epoch 46, Training Loss: 0.803411951369809\n",
      "Epoch 47, Training Loss: 0.8035236720751999\n",
      "Epoch 48, Training Loss: 0.8034003555326533\n",
      "Epoch 49, Training Loss: 0.8037383747280092\n",
      "Epoch 50, Training Loss: 0.8032664594793678\n",
      "Epoch 51, Training Loss: 0.803402799054196\n",
      "Epoch 52, Training Loss: 0.8024481785924811\n",
      "Epoch 53, Training Loss: 0.8032221321772812\n",
      "Epoch 54, Training Loss: 0.802873016210427\n",
      "Epoch 55, Training Loss: 0.8024345975173147\n",
      "Epoch 56, Training Loss: 0.803951510809418\n",
      "Epoch 57, Training Loss: 0.80225304185896\n",
      "Epoch 58, Training Loss: 0.8019582682982423\n",
      "Epoch 59, Training Loss: 0.8025569723961049\n",
      "Epoch 60, Training Loss: 0.8022091136839157\n",
      "Epoch 61, Training Loss: 0.8015625803094161\n",
      "Epoch 62, Training Loss: 0.8023184965427657\n",
      "Epoch 63, Training Loss: 0.8011247401846979\n",
      "Epoch 64, Training Loss: 0.8014040492531053\n",
      "Epoch 65, Training Loss: 0.8012609877980741\n",
      "Epoch 66, Training Loss: 0.8015346233109782\n",
      "Epoch 67, Training Loss: 0.8013794008054231\n",
      "Epoch 68, Training Loss: 0.8011720256697863\n",
      "Epoch 69, Training Loss: 0.8008510589599609\n",
      "Epoch 70, Training Loss: 0.8006273992079541\n",
      "Epoch 71, Training Loss: 0.8008955968053717\n",
      "Epoch 72, Training Loss: 0.8007987827286684\n",
      "Epoch 73, Training Loss: 0.8004950779721253\n",
      "Epoch 74, Training Loss: 0.8005370323819325\n",
      "Epoch 75, Training Loss: 0.8003470759642751\n",
      "Epoch 76, Training Loss: 0.8008228272423709\n",
      "Epoch 77, Training Loss: 0.8001295528017489\n",
      "Epoch 78, Training Loss: 0.8005298174413523\n",
      "Epoch 79, Training Loss: 0.8001351525909022\n",
      "Epoch 80, Training Loss: 0.8007121218774552\n",
      "Epoch 81, Training Loss: 0.800014849533712\n",
      "Epoch 82, Training Loss: 0.8002268103728617\n",
      "Epoch 83, Training Loss: 0.7999382087162563\n",
      "Epoch 84, Training Loss: 0.7998183372325467\n",
      "Epoch 85, Training Loss: 0.7999502628369439\n",
      "Epoch 86, Training Loss: 0.8001290083827829\n",
      "Epoch 87, Training Loss: 0.8000199544698672\n",
      "Epoch 88, Training Loss: 0.7997023607555188\n",
      "Epoch 89, Training Loss: 0.7993066938299882\n",
      "Epoch 90, Training Loss: 0.7999206272282995\n",
      "Epoch 91, Training Loss: 0.7999405272921225\n",
      "Epoch 92, Training Loss: 0.7997191005183342\n",
      "Epoch 93, Training Loss: 0.7993185276375677\n",
      "Epoch 94, Training Loss: 0.7996303565520093\n",
      "Epoch 95, Training Loss: 0.7997128142450088\n",
      "Epoch 96, Training Loss: 0.7990810939243862\n",
      "Epoch 97, Training Loss: 0.7995450844442038\n",
      "Epoch 98, Training Loss: 0.7996738920534464\n",
      "Epoch 99, Training Loss: 0.799213883482424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:30:54,541] Trial 155 finished with value: 0.6288666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.799242032918715\n",
      "Epoch 1, Training Loss: 0.9336226281366851\n",
      "Epoch 2, Training Loss: 0.8683583778546269\n",
      "Epoch 3, Training Loss: 0.8317723458870909\n",
      "Epoch 4, Training Loss: 0.8181545794458318\n",
      "Epoch 5, Training Loss: 0.8133210023542992\n",
      "Epoch 6, Training Loss: 0.8103152154979849\n",
      "Epoch 7, Training Loss: 0.8089386437620435\n",
      "Epoch 8, Training Loss: 0.8084449593285868\n",
      "Epoch 9, Training Loss: 0.8081028906026281\n",
      "Epoch 10, Training Loss: 0.8073972844539729\n",
      "Epoch 11, Training Loss: 0.8068643326149847\n",
      "Epoch 12, Training Loss: 0.8064754901972032\n",
      "Epoch 13, Training Loss: 0.8067401401978687\n",
      "Epoch 14, Training Loss: 0.805941341066719\n",
      "Epoch 15, Training Loss: 0.8052488134319621\n",
      "Epoch 16, Training Loss: 0.8046296398442491\n",
      "Epoch 17, Training Loss: 0.8049977670934864\n",
      "Epoch 18, Training Loss: 0.8042853819696527\n",
      "Epoch 19, Training Loss: 0.804664723407057\n",
      "Epoch 20, Training Loss: 0.8034445587405585\n",
      "Epoch 21, Training Loss: 0.8038548067996376\n",
      "Epoch 22, Training Loss: 0.8031131286370127\n",
      "Epoch 23, Training Loss: 0.8033890775271825\n",
      "Epoch 24, Training Loss: 0.8032817201506822\n",
      "Epoch 25, Training Loss: 0.802747697220709\n",
      "Epoch 26, Training Loss: 0.8025095928880505\n",
      "Epoch 27, Training Loss: 0.8021833841962026\n",
      "Epoch 28, Training Loss: 0.8017630274134471\n",
      "Epoch 29, Training Loss: 0.8023249786599238\n",
      "Epoch 30, Training Loss: 0.8019636892734614\n",
      "Epoch 31, Training Loss: 0.8010617539398652\n",
      "Epoch 32, Training Loss: 0.8013821796367043\n",
      "Epoch 33, Training Loss: 0.8014918548720223\n",
      "Epoch 34, Training Loss: 0.8005810415834412\n",
      "Epoch 35, Training Loss: 0.8012981524144797\n",
      "Epoch 36, Training Loss: 0.8015297729269902\n",
      "Epoch 37, Training Loss: 0.8010769457745373\n",
      "Epoch 38, Training Loss: 0.8006175246453823\n",
      "Epoch 39, Training Loss: 0.8004725503742247\n",
      "Epoch 40, Training Loss: 0.8000136345849002\n",
      "Epoch 41, Training Loss: 0.8000209801179126\n",
      "Epoch 42, Training Loss: 0.7999158532099616\n",
      "Epoch 43, Training Loss: 0.8002387083562693\n",
      "Epoch 44, Training Loss: 0.7993561921263099\n",
      "Epoch 45, Training Loss: 0.7997145669800895\n",
      "Epoch 46, Training Loss: 0.8000123739242554\n",
      "Epoch 47, Training Loss: 0.7992591507452771\n",
      "Epoch 48, Training Loss: 0.7989123208182198\n",
      "Epoch 49, Training Loss: 0.7994996596099739\n",
      "Epoch 50, Training Loss: 0.7990270137786866\n",
      "Epoch 51, Training Loss: 0.7991215974764716\n",
      "Epoch 52, Training Loss: 0.7994285749313527\n",
      "Epoch 53, Training Loss: 0.7990092035523034\n",
      "Epoch 54, Training Loss: 0.7986089661605376\n",
      "Epoch 55, Training Loss: 0.7987058588436672\n",
      "Epoch 56, Training Loss: 0.7989746809902047\n",
      "Epoch 57, Training Loss: 0.7984809191603409\n",
      "Epoch 58, Training Loss: 0.7985699341709452\n",
      "Epoch 59, Training Loss: 0.7985717523366885\n",
      "Epoch 60, Training Loss: 0.7986629823993023\n",
      "Epoch 61, Training Loss: 0.7990532620508868\n",
      "Epoch 62, Training Loss: 0.7983081781774535\n",
      "Epoch 63, Training Loss: 0.7982408898217338\n",
      "Epoch 64, Training Loss: 0.7985878567946585\n",
      "Epoch 65, Training Loss: 0.7979012701744423\n",
      "Epoch 66, Training Loss: 0.7977026704558753\n",
      "Epoch 67, Training Loss: 0.7980347739126449\n",
      "Epoch 68, Training Loss: 0.7979274202110176\n",
      "Epoch 69, Training Loss: 0.7982146429836302\n",
      "Epoch 70, Training Loss: 0.7980248342779346\n",
      "Epoch 71, Training Loss: 0.7986116742729245\n",
      "Epoch 72, Training Loss: 0.7976881298803745\n",
      "Epoch 73, Training Loss: 0.7982574576722052\n",
      "Epoch 74, Training Loss: 0.7975633672305515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:32:04,564] Trial 156 finished with value: 0.6212666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7974990342792712\n",
      "Epoch 1, Training Loss: 1.0881888591541964\n",
      "Epoch 2, Training Loss: 1.081732354444616\n",
      "Epoch 3, Training Loss: 1.0728938045221217\n",
      "Epoch 4, Training Loss: 1.0591234016418456\n",
      "Epoch 5, Training Loss: 1.0380509640188778\n",
      "Epoch 6, Training Loss: 1.0107522500963773\n",
      "Epoch 7, Training Loss: 0.985751675788094\n",
      "Epoch 8, Training Loss: 0.9703565870313083\n",
      "Epoch 9, Training Loss: 0.9630315742773168\n",
      "Epoch 10, Training Loss: 0.9595203779725467\n",
      "Epoch 11, Training Loss: 0.9571657790155972\n",
      "Epoch 12, Training Loss: 0.9554573851473191\n",
      "Epoch 13, Training Loss: 0.9535931439960704\n",
      "Epoch 14, Training Loss: 0.9519163994228139\n",
      "Epoch 15, Training Loss: 0.9501858769444859\n",
      "Epoch 16, Training Loss: 0.9483149263438057\n",
      "Epoch 17, Training Loss: 0.9461982599426718\n",
      "Epoch 18, Training Loss: 0.9440527970650617\n",
      "Epoch 19, Training Loss: 0.9418574792497298\n",
      "Epoch 20, Training Loss: 0.9393599242322586\n",
      "Epoch 21, Training Loss: 0.9364894655872794\n",
      "Epoch 22, Training Loss: 0.9334333789348602\n",
      "Epoch 23, Training Loss: 0.930255067208234\n",
      "Epoch 24, Training Loss: 0.9265625882849974\n",
      "Epoch 25, Training Loss: 0.9224232243089114\n",
      "Epoch 26, Training Loss: 0.9180525913659264\n",
      "Epoch 27, Training Loss: 0.9132581625966465\n",
      "Epoch 28, Training Loss: 0.9078032778291141\n",
      "Epoch 29, Training Loss: 0.9019764353247249\n",
      "Epoch 30, Training Loss: 0.8955095143879162\n",
      "Epoch 31, Training Loss: 0.8888426409749424\n",
      "Epoch 32, Training Loss: 0.8816606514594134\n",
      "Epoch 33, Training Loss: 0.8744686650528627\n",
      "Epoch 34, Training Loss: 0.867326228688745\n",
      "Epoch 35, Training Loss: 0.8601948128728305\n",
      "Epoch 36, Training Loss: 0.8535435662550085\n",
      "Epoch 37, Training Loss: 0.8474358771127813\n",
      "Epoch 38, Training Loss: 0.8419454599829281\n",
      "Epoch 39, Training Loss: 0.8367173678734723\n",
      "Epoch 40, Training Loss: 0.8325752435011022\n",
      "Epoch 41, Training Loss: 0.8291768236020032\n",
      "Epoch 42, Training Loss: 0.8260551821484285\n",
      "Epoch 43, Training Loss: 0.8236482056449441\n",
      "Epoch 44, Training Loss: 0.8214369945666369\n",
      "Epoch 45, Training Loss: 0.8198867027198568\n",
      "Epoch 46, Training Loss: 0.8183954496944652\n",
      "Epoch 47, Training Loss: 0.8171018446193022\n",
      "Epoch 48, Training Loss: 0.8160753123900469\n",
      "Epoch 49, Training Loss: 0.8153196857256048\n",
      "Epoch 50, Training Loss: 0.8142495418296141\n",
      "Epoch 51, Training Loss: 0.8139866122077493\n",
      "Epoch 52, Training Loss: 0.8134812183941111\n",
      "Epoch 53, Training Loss: 0.8127502780100878\n",
      "Epoch 54, Training Loss: 0.8121349899909076\n",
      "Epoch 55, Training Loss: 0.8118638711817124\n",
      "Epoch 56, Training Loss: 0.811595053322175\n",
      "Epoch 57, Training Loss: 0.8111944993103252\n",
      "Epoch 58, Training Loss: 0.8108908437981325\n",
      "Epoch 59, Training Loss: 0.8105450856685639\n",
      "Epoch 60, Training Loss: 0.8103702430865344\n",
      "Epoch 61, Training Loss: 0.8101066364260281\n",
      "Epoch 62, Training Loss: 0.8098932990607093\n",
      "Epoch 63, Training Loss: 0.809652844246696\n",
      "Epoch 64, Training Loss: 0.809503030426362\n",
      "Epoch 65, Training Loss: 0.809211837123422\n",
      "Epoch 66, Training Loss: 0.8091393184661865\n",
      "Epoch 67, Training Loss: 0.8089031473328085\n",
      "Epoch 68, Training Loss: 0.8086628340973574\n",
      "Epoch 69, Training Loss: 0.8086625164396622\n",
      "Epoch 70, Training Loss: 0.8083849560513215\n",
      "Epoch 71, Training Loss: 0.8083091317906099\n",
      "Epoch 72, Training Loss: 0.8081729957636665\n",
      "Epoch 73, Training Loss: 0.8078708067361047\n",
      "Epoch 74, Training Loss: 0.807862608573016\n",
      "Epoch 75, Training Loss: 0.8076767191466163\n",
      "Epoch 76, Training Loss: 0.8076446851562051\n",
      "Epoch 77, Training Loss: 0.8073431241512299\n",
      "Epoch 78, Training Loss: 0.8072484680484323\n",
      "Epoch 79, Training Loss: 0.8071740799791672\n",
      "Epoch 80, Training Loss: 0.807039361000061\n",
      "Epoch 81, Training Loss: 0.8068195396311143\n",
      "Epoch 82, Training Loss: 0.8068877872999977\n",
      "Epoch 83, Training Loss: 0.8064378447392407\n",
      "Epoch 84, Training Loss: 0.8065643091061536\n",
      "Epoch 85, Training Loss: 0.8065179685985341\n",
      "Epoch 86, Training Loss: 0.8062754994280198\n",
      "Epoch 87, Training Loss: 0.8060658390381757\n",
      "Epoch 88, Training Loss: 0.8060647314436296\n",
      "Epoch 89, Training Loss: 0.8058660166403827\n",
      "Epoch 90, Training Loss: 0.8057456320874832\n",
      "Epoch 91, Training Loss: 0.805715110021479\n",
      "Epoch 92, Training Loss: 0.8054580452862908\n",
      "Epoch 93, Training Loss: 0.8054110059317421\n",
      "Epoch 94, Training Loss: 0.8053071310940911\n",
      "Epoch 95, Training Loss: 0.8054302385274101\n",
      "Epoch 96, Training Loss: 0.8051412247208988\n",
      "Epoch 97, Training Loss: 0.8049983046335333\n",
      "Epoch 98, Training Loss: 0.8049547436658074\n",
      "Epoch 99, Training Loss: 0.8048972669769736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:33:52,242] Trial 157 finished with value: 0.6312666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8048424375758452\n",
      "Epoch 1, Training Loss: 0.8640738167482264\n",
      "Epoch 2, Training Loss: 0.8272322556551766\n",
      "Epoch 3, Training Loss: 0.8213451636538786\n",
      "Epoch 4, Training Loss: 0.8171544870208292\n",
      "Epoch 5, Training Loss: 0.8143981494623072\n",
      "Epoch 6, Training Loss: 0.8127980059034684\n",
      "Epoch 7, Training Loss: 0.8108709394230562\n",
      "Epoch 8, Training Loss: 0.8113592691281263\n",
      "Epoch 9, Training Loss: 0.8096643460498136\n",
      "Epoch 10, Training Loss: 0.8089790673115674\n",
      "Epoch 11, Training Loss: 0.8081634184192209\n",
      "Epoch 12, Training Loss: 0.8076721845654881\n",
      "Epoch 13, Training Loss: 0.806941429306479\n",
      "Epoch 14, Training Loss: 0.8069020828780006\n",
      "Epoch 15, Training Loss: 0.8067134660833022\n",
      "Epoch 16, Training Loss: 0.8056102911163779\n",
      "Epoch 17, Training Loss: 0.805239748323665\n",
      "Epoch 18, Training Loss: 0.8057252449147841\n",
      "Epoch 19, Training Loss: 0.8049990247978883\n",
      "Epoch 20, Training Loss: 0.8047521492312936\n",
      "Epoch 21, Training Loss: 0.8043442719824174\n",
      "Epoch 22, Training Loss: 0.8040508478529313\n",
      "Epoch 23, Training Loss: 0.8038720743095173\n",
      "Epoch 24, Training Loss: 0.8044237632611219\n",
      "Epoch 25, Training Loss: 0.8041903997870052\n",
      "Epoch 26, Training Loss: 0.8037077534198761\n",
      "Epoch 27, Training Loss: 0.803517022413366\n",
      "Epoch 28, Training Loss: 0.803440589203554\n",
      "Epoch 29, Training Loss: 0.8029326067251318\n",
      "Epoch 30, Training Loss: 0.803073944905225\n",
      "Epoch 31, Training Loss: 0.8036099892504075\n",
      "Epoch 32, Training Loss: 0.8028592777252197\n",
      "Epoch 33, Training Loss: 0.8023754363200244\n",
      "Epoch 34, Training Loss: 0.8024274185124566\n",
      "Epoch 35, Training Loss: 0.8021919279939989\n",
      "Epoch 36, Training Loss: 0.8022386143488043\n",
      "Epoch 37, Training Loss: 0.8021743679747863\n",
      "Epoch 38, Training Loss: 0.8023009421544917\n",
      "Epoch 39, Training Loss: 0.802165344322429\n",
      "Epoch 40, Training Loss: 0.8021203661666197\n",
      "Epoch 41, Training Loss: 0.8019485477138968\n",
      "Epoch 42, Training Loss: 0.8020498642500709\n",
      "Epoch 43, Training Loss: 0.8014963957842659\n",
      "Epoch 44, Training Loss: 0.8015488624572754\n",
      "Epoch 45, Training Loss: 0.802144370990641\n",
      "Epoch 46, Training Loss: 0.8016293044651256\n",
      "Epoch 47, Training Loss: 0.8013587333875544\n",
      "Epoch 48, Training Loss: 0.8012439417137819\n",
      "Epoch 49, Training Loss: 0.8005162966952605\n",
      "Epoch 50, Training Loss: 0.8009385882405674\n",
      "Epoch 51, Training Loss: 0.8010396758949055\n",
      "Epoch 52, Training Loss: 0.8008327487637015\n",
      "Epoch 53, Training Loss: 0.8009141633791083\n",
      "Epoch 54, Training Loss: 0.80039827374851\n",
      "Epoch 55, Training Loss: 0.800505696885726\n",
      "Epoch 56, Training Loss: 0.8003898372369654\n",
      "Epoch 57, Training Loss: 0.8009624968556797\n",
      "Epoch 58, Training Loss: 0.8010196638808531\n",
      "Epoch 59, Training Loss: 0.8009561425096848\n",
      "Epoch 60, Training Loss: 0.8004691090303309\n",
      "Epoch 61, Training Loss: 0.800629777207094\n",
      "Epoch 62, Training Loss: 0.8000964430500479\n",
      "Epoch 63, Training Loss: 0.8000362184468438\n",
      "Epoch 64, Training Loss: 0.8000127756595612\n",
      "Epoch 65, Training Loss: 0.7999937093959135\n",
      "Epoch 66, Training Loss: 0.8002401509705712\n",
      "Epoch 67, Training Loss: 0.8001432127111099\n",
      "Epoch 68, Training Loss: 0.8001168276983149\n",
      "Epoch 69, Training Loss: 0.8005994452448453\n",
      "Epoch 70, Training Loss: 0.7999524652957917\n",
      "Epoch 71, Training Loss: 0.7998460860112134\n",
      "Epoch 72, Training Loss: 0.8000344555518206\n",
      "Epoch 73, Training Loss: 0.8002394638342016\n",
      "Epoch 74, Training Loss: 0.7997639121728785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:35:15,758] Trial 158 finished with value: 0.6349333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.80077721118927\n",
      "Epoch 1, Training Loss: 0.928745843522689\n",
      "Epoch 2, Training Loss: 0.8634278765846701\n",
      "Epoch 3, Training Loss: 0.82771367521847\n",
      "Epoch 4, Training Loss: 0.8163850482071148\n",
      "Epoch 5, Training Loss: 0.8126062400902019\n",
      "Epoch 6, Training Loss: 0.8105849953258739\n",
      "Epoch 7, Training Loss: 0.8095324699317707\n",
      "Epoch 8, Training Loss: 0.8086597120761871\n",
      "Epoch 9, Training Loss: 0.8082345607000239\n",
      "Epoch 10, Training Loss: 0.8077354396090788\n",
      "Epoch 11, Training Loss: 0.8070607496009153\n",
      "Epoch 12, Training Loss: 0.8063348500868853\n",
      "Epoch 13, Training Loss: 0.8063253590639899\n",
      "Epoch 14, Training Loss: 0.8061314955178429\n",
      "Epoch 15, Training Loss: 0.8056508440129897\n",
      "Epoch 16, Training Loss: 0.8052108512205236\n",
      "Epoch 17, Training Loss: 0.8048123086901272\n",
      "Epoch 18, Training Loss: 0.8048590447622187\n",
      "Epoch 19, Training Loss: 0.8045613092534682\n",
      "Epoch 20, Training Loss: 0.8042076526669895\n",
      "Epoch 21, Training Loss: 0.8038525833803065\n",
      "Epoch 22, Training Loss: 0.803489102265414\n",
      "Epoch 23, Training Loss: 0.8034875875360825\n",
      "Epoch 24, Training Loss: 0.8031877784168019\n",
      "Epoch 25, Training Loss: 0.8030304732743432\n",
      "Epoch 26, Training Loss: 0.8026909166925094\n",
      "Epoch 27, Training Loss: 0.8025147437348086\n",
      "Epoch 28, Training Loss: 0.8022016727223116\n",
      "Epoch 29, Training Loss: 0.8020067299814785\n",
      "Epoch 30, Training Loss: 0.8017602251557743\n",
      "Epoch 31, Training Loss: 0.8014186957303215\n",
      "Epoch 32, Training Loss: 0.8013533104868497\n",
      "Epoch 33, Training Loss: 0.8013461925702936\n",
      "Epoch 34, Training Loss: 0.8012094817442053\n",
      "Epoch 35, Training Loss: 0.8009655878123115\n",
      "Epoch 36, Training Loss: 0.8007044285185196\n",
      "Epoch 37, Training Loss: 0.8002362526164335\n",
      "Epoch 38, Training Loss: 0.8004799761491663\n",
      "Epoch 39, Training Loss: 0.8004364408464992\n",
      "Epoch 40, Training Loss: 0.800149306900361\n",
      "Epoch 41, Training Loss: 0.7998610274230733\n",
      "Epoch 42, Training Loss: 0.7998752500730403\n",
      "Epoch 43, Training Loss: 0.7998814777065726\n",
      "Epoch 44, Training Loss: 0.7996358548192417\n",
      "Epoch 45, Training Loss: 0.7994628132090849\n",
      "Epoch 46, Training Loss: 0.7989594251268051\n",
      "Epoch 47, Training Loss: 0.7992357380249921\n",
      "Epoch 48, Training Loss: 0.7993164248326246\n",
      "Epoch 49, Training Loss: 0.7989954400763792\n",
      "Epoch 50, Training Loss: 0.7988395211977117\n",
      "Epoch 51, Training Loss: 0.7986279763894922\n",
      "Epoch 52, Training Loss: 0.7988988979423748\n",
      "Epoch 53, Training Loss: 0.7986929858432097\n",
      "Epoch 54, Training Loss: 0.7987462665052975\n",
      "Epoch 55, Training Loss: 0.7986487652974971\n",
      "Epoch 56, Training Loss: 0.7984402289811303\n",
      "Epoch 57, Training Loss: 0.798270539816688\n",
      "Epoch 58, Training Loss: 0.798364591107649\n",
      "Epoch 59, Training Loss: 0.7982377294933095\n",
      "Epoch 60, Training Loss: 0.7982913970246035\n",
      "Epoch 61, Training Loss: 0.798340594698401\n",
      "Epoch 62, Training Loss: 0.7981261436378254\n",
      "Epoch 63, Training Loss: 0.7981254665991839\n",
      "Epoch 64, Training Loss: 0.7980434435255387\n",
      "Epoch 65, Training Loss: 0.7978505434008205\n",
      "Epoch 66, Training Loss: 0.7980362814314225\n",
      "Epoch 67, Training Loss: 0.7978937020021326\n",
      "Epoch 68, Training Loss: 0.7977081995150622\n",
      "Epoch 69, Training Loss: 0.7977894206608043\n",
      "Epoch 70, Training Loss: 0.7977818367761724\n",
      "Epoch 71, Training Loss: 0.7979323325437658\n",
      "Epoch 72, Training Loss: 0.7976986753940583\n",
      "Epoch 73, Training Loss: 0.7977284934240229\n",
      "Epoch 74, Training Loss: 0.7977515373510473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:36:39,809] Trial 159 finished with value: 0.6351333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7973348872100605\n",
      "Epoch 1, Training Loss: 0.9824948659516815\n",
      "Epoch 2, Training Loss: 0.9437304608803943\n",
      "Epoch 3, Training Loss: 0.9214399031230381\n",
      "Epoch 4, Training Loss: 0.8873590341187958\n",
      "Epoch 5, Training Loss: 0.8497182775260811\n",
      "Epoch 6, Training Loss: 0.8252678267937854\n",
      "Epoch 7, Training Loss: 0.8163548132530729\n",
      "Epoch 8, Training Loss: 0.8139900448627042\n",
      "Epoch 9, Training Loss: 0.8127670180528683\n",
      "Epoch 10, Training Loss: 0.8115159511566162\n",
      "Epoch 11, Training Loss: 0.8105463412471283\n",
      "Epoch 12, Training Loss: 0.810361227057034\n",
      "Epoch 13, Training Loss: 0.8085457431642633\n",
      "Epoch 14, Training Loss: 0.8086953031389337\n",
      "Epoch 15, Training Loss: 0.8080392275537763\n",
      "Epoch 16, Training Loss: 0.8071792100605212\n",
      "Epoch 17, Training Loss: 0.8063009529185474\n",
      "Epoch 18, Training Loss: 0.8058392893999142\n",
      "Epoch 19, Training Loss: 0.8056555505085709\n",
      "Epoch 20, Training Loss: 0.8049245735756436\n",
      "Epoch 21, Training Loss: 0.8052237468554562\n",
      "Epoch 22, Training Loss: 0.8040952956766114\n",
      "Epoch 23, Training Loss: 0.8035288092785312\n",
      "Epoch 24, Training Loss: 0.8028289662267929\n",
      "Epoch 25, Training Loss: 0.8031351896156942\n",
      "Epoch 26, Training Loss: 0.8023024514653629\n",
      "Epoch 27, Training Loss: 0.8024444238583844\n",
      "Epoch 28, Training Loss: 0.8024948280556757\n",
      "Epoch 29, Training Loss: 0.8024097229305066\n",
      "Epoch 30, Training Loss: 0.8022198311368326\n",
      "Epoch 31, Training Loss: 0.8019635969534853\n",
      "Epoch 32, Training Loss: 0.8018750133370994\n",
      "Epoch 33, Training Loss: 0.8016094892544854\n",
      "Epoch 34, Training Loss: 0.8011797814440906\n",
      "Epoch 35, Training Loss: 0.8014324172098833\n",
      "Epoch 36, Training Loss: 0.8012967346306133\n",
      "Epoch 37, Training Loss: 0.8009674550895404\n",
      "Epoch 38, Training Loss: 0.8016474802691237\n",
      "Epoch 39, Training Loss: 0.801134601392244\n",
      "Epoch 40, Training Loss: 0.8009274985557212\n",
      "Epoch 41, Training Loss: 0.8001292452328187\n",
      "Epoch 42, Training Loss: 0.8007177952536964\n",
      "Epoch 43, Training Loss: 0.8004208054757656\n",
      "Epoch 44, Training Loss: 0.7996389299407041\n",
      "Epoch 45, Training Loss: 0.7998341575601047\n",
      "Epoch 46, Training Loss: 0.7994184304897051\n",
      "Epoch 47, Training Loss: 0.7995800854568195\n",
      "Epoch 48, Training Loss: 0.799337737631977\n",
      "Epoch 49, Training Loss: 0.7996909161259357\n",
      "Epoch 50, Training Loss: 0.7989289131379665\n",
      "Epoch 51, Training Loss: 0.799207874079396\n",
      "Epoch 52, Training Loss: 0.798661710624408\n",
      "Epoch 53, Training Loss: 0.7989593002132903\n",
      "Epoch 54, Training Loss: 0.7988155291492778\n",
      "Epoch 55, Training Loss: 0.7991169256375248\n",
      "Epoch 56, Training Loss: 0.7983908495508638\n",
      "Epoch 57, Training Loss: 0.798163584390081\n",
      "Epoch 58, Training Loss: 0.7984386546271188\n",
      "Epoch 59, Training Loss: 0.798328899799433\n",
      "Epoch 60, Training Loss: 0.7991367089120965\n",
      "Epoch 61, Training Loss: 0.7980366254211368\n",
      "Epoch 62, Training Loss: 0.7977469238123499\n",
      "Epoch 63, Training Loss: 0.7979339011629721\n",
      "Epoch 64, Training Loss: 0.7975380770245889\n",
      "Epoch 65, Training Loss: 0.7975210820821891\n",
      "Epoch 66, Training Loss: 0.7973905287290874\n",
      "Epoch 67, Training Loss: 0.7972078992908163\n",
      "Epoch 68, Training Loss: 0.7972165301330107\n",
      "Epoch 69, Training Loss: 0.7974286941657389\n",
      "Epoch 70, Training Loss: 0.7970382490552458\n",
      "Epoch 71, Training Loss: 0.7975217672218954\n",
      "Epoch 72, Training Loss: 0.797028610760108\n",
      "Epoch 73, Training Loss: 0.7972203192854286\n",
      "Epoch 74, Training Loss: 0.7973380032338594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:38:02,724] Trial 160 finished with value: 0.6262666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7965758500690747\n",
      "Epoch 1, Training Loss: 1.0899600829797633\n",
      "Epoch 2, Training Loss: 1.0872248981980717\n",
      "Epoch 3, Training Loss: 1.084522720785702\n",
      "Epoch 4, Training Loss: 1.081429128226112\n",
      "Epoch 5, Training Loss: 1.0776911936086766\n",
      "Epoch 6, Training Loss: 1.0730830694647395\n",
      "Epoch 7, Training Loss: 1.0672168195948881\n",
      "Epoch 8, Training Loss: 1.0598368521297679\n",
      "Epoch 9, Training Loss: 1.0505431612098919\n",
      "Epoch 10, Training Loss: 1.0392038599182578\n",
      "Epoch 11, Training Loss: 1.0261733204476973\n",
      "Epoch 12, Training Loss: 1.0122495627403258\n",
      "Epoch 13, Training Loss: 0.9988488254126381\n",
      "Epoch 14, Training Loss: 0.9871826078611262\n",
      "Epoch 15, Training Loss: 0.978143243859796\n",
      "Epoch 16, Training Loss: 0.9715924755965962\n",
      "Epoch 17, Training Loss: 0.9667697468925925\n",
      "Epoch 18, Training Loss: 0.9633996602366952\n",
      "Epoch 19, Training Loss: 0.9609887098564821\n",
      "Epoch 20, Training Loss: 0.9591065831044141\n",
      "Epoch 21, Training Loss: 0.9577282325660481\n",
      "Epoch 22, Training Loss: 0.9564659887902877\n",
      "Epoch 23, Training Loss: 0.9555233870534335\n",
      "Epoch 24, Training Loss: 0.9545878521835103\n",
      "Epoch 25, Training Loss: 0.9536874919077929\n",
      "Epoch 26, Training Loss: 0.9528124335232903\n",
      "Epoch 27, Training Loss: 0.951978606967365\n",
      "Epoch 28, Training Loss: 0.9510913630794077\n",
      "Epoch 29, Training Loss: 0.9501671526011298\n",
      "Epoch 30, Training Loss: 0.949282186942942\n",
      "Epoch 31, Training Loss: 0.9482837672794566\n",
      "Epoch 32, Training Loss: 0.9472708779924056\n",
      "Epoch 33, Training Loss: 0.9462593443954692\n",
      "Epoch 34, Training Loss: 0.9451846861138063\n",
      "Epoch 35, Training Loss: 0.9441264956137714\n",
      "Epoch 36, Training Loss: 0.9429872355040382\n",
      "Epoch 37, Training Loss: 0.9418786015230066\n",
      "Epoch 38, Training Loss: 0.9406588359440075\n",
      "Epoch 39, Training Loss: 0.9393352509246153\n",
      "Epoch 40, Training Loss: 0.9380693697929382\n",
      "Epoch 41, Training Loss: 0.9366148221492767\n",
      "Epoch 42, Training Loss: 0.9351940206920399\n",
      "Epoch 43, Training Loss: 0.9337395184180316\n",
      "Epoch 44, Training Loss: 0.9320434946172378\n",
      "Epoch 45, Training Loss: 0.9304342490084031\n",
      "Epoch 46, Training Loss: 0.9287121968409594\n",
      "Epoch 47, Training Loss: 0.9268919817840352\n",
      "Epoch 48, Training Loss: 0.9248985195159912\n",
      "Epoch 49, Training Loss: 0.922892284673803\n",
      "Epoch 50, Training Loss: 0.920790534510332\n",
      "Epoch 51, Training Loss: 0.918490131111706\n",
      "Epoch 52, Training Loss: 0.9161724485369289\n",
      "Epoch 53, Training Loss: 0.9136833843764137\n",
      "Epoch 54, Training Loss: 0.9111102458308725\n",
      "Epoch 55, Training Loss: 0.9083861987029805\n",
      "Epoch 56, Training Loss: 0.9055642569065094\n",
      "Epoch 57, Training Loss: 0.9025493005444022\n",
      "Epoch 58, Training Loss: 0.8995245962984422\n",
      "Epoch 59, Training Loss: 0.8962589331935433\n",
      "Epoch 60, Training Loss: 0.8928835532244515\n",
      "Epoch 61, Training Loss: 0.8894507298750036\n",
      "Epoch 62, Training Loss: 0.8860176533110001\n",
      "Epoch 63, Training Loss: 0.8824716091156006\n",
      "Epoch 64, Training Loss: 0.8788096234377692\n",
      "Epoch 65, Training Loss: 0.8752119406531839\n",
      "Epoch 66, Training Loss: 0.8714798418914571\n",
      "Epoch 67, Training Loss: 0.8678927592670216\n",
      "Epoch 68, Training Loss: 0.8643420090394861\n",
      "Epoch 69, Training Loss: 0.8607753096608555\n",
      "Epoch 70, Training Loss: 0.8574038986598744\n",
      "Epoch 71, Training Loss: 0.8540613267000984\n",
      "Epoch 72, Training Loss: 0.8508903622627259\n",
      "Epoch 73, Training Loss: 0.8478640949726105\n",
      "Epoch 74, Training Loss: 0.8450611821342917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:39:23,624] Trial 161 finished with value: 0.6071333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8422788631916046\n",
      "Epoch 1, Training Loss: 0.8430223321213441\n",
      "Epoch 2, Training Loss: 0.813686744185055\n",
      "Epoch 3, Training Loss: 0.8096703636646271\n",
      "Epoch 4, Training Loss: 0.8089142841451308\n",
      "Epoch 5, Training Loss: 0.8061845186878653\n",
      "Epoch 6, Training Loss: 0.8049842680903042\n",
      "Epoch 7, Training Loss: 0.8037181941901936\n",
      "Epoch 8, Training Loss: 0.8042959310727961\n",
      "Epoch 9, Training Loss: 0.803468450868831\n",
      "Epoch 10, Training Loss: 0.8022204623502843\n",
      "Epoch 11, Training Loss: 0.8033886370238136\n",
      "Epoch 12, Training Loss: 0.8010551445624408\n",
      "Epoch 13, Training Loss: 0.8013210816243116\n",
      "Epoch 14, Training Loss: 0.7991245619689717\n",
      "Epoch 15, Training Loss: 0.8006044326810275\n",
      "Epoch 16, Training Loss: 0.7996763165558086\n",
      "Epoch 17, Training Loss: 0.8008932035109576\n",
      "Epoch 18, Training Loss: 0.7992294590613421\n",
      "Epoch 19, Training Loss: 0.7996498251662535\n",
      "Epoch 20, Training Loss: 0.798373779409072\n",
      "Epoch 21, Training Loss: 0.8011636353941525\n",
      "Epoch 22, Training Loss: 0.7975461274736068\n",
      "Epoch 23, Training Loss: 0.7995230831819422\n",
      "Epoch 24, Training Loss: 0.7992873243724599\n",
      "Epoch 25, Training Loss: 0.7989679821799783\n",
      "Epoch 26, Training Loss: 0.7982605620692758\n",
      "Epoch 27, Training Loss: 0.7989807957761428\n",
      "Epoch 28, Training Loss: 0.8002505622190588\n",
      "Epoch 29, Training Loss: 0.7980979935561909\n",
      "Epoch 30, Training Loss: 0.7992925139034496\n",
      "Epoch 31, Training Loss: 0.7996889399780946\n",
      "Epoch 32, Training Loss: 0.7976927410855013\n",
      "Epoch 33, Training Loss: 0.7973041204143972\n",
      "Epoch 34, Training Loss: 0.798526437072193\n",
      "Epoch 35, Training Loss: 0.7978991249729606\n",
      "Epoch 36, Training Loss: 0.7985509293920854\n",
      "Epoch 37, Training Loss: 0.7976854876910939\n",
      "Epoch 38, Training Loss: 0.7976557347353767\n",
      "Epoch 39, Training Loss: 0.7985500602862414\n",
      "Epoch 40, Training Loss: 0.7979348196702845\n",
      "Epoch 41, Training Loss: 0.796399322607938\n",
      "Epoch 42, Training Loss: 0.7980843940903158\n",
      "Epoch 43, Training Loss: 0.7986482894420623\n",
      "Epoch 44, Training Loss: 0.7981371499510372\n",
      "Epoch 45, Training Loss: 0.8000149404301363\n",
      "Epoch 46, Training Loss: 0.7977441222527448\n",
      "Epoch 47, Training Loss: 0.7975263852231643\n",
      "Epoch 48, Training Loss: 0.7979091954932493\n",
      "Epoch 49, Training Loss: 0.7977983785376829\n",
      "Epoch 50, Training Loss: 0.7979119290323818\n",
      "Epoch 51, Training Loss: 0.7984275916744681\n",
      "Epoch 52, Training Loss: 0.7980473738558153\n",
      "Epoch 53, Training Loss: 0.7984612080630135\n",
      "Epoch 54, Training Loss: 0.7983026399331934\n",
      "Epoch 55, Training Loss: 0.7978615068688112\n",
      "Epoch 56, Training Loss: 0.7979451100966509\n",
      "Epoch 57, Training Loss: 0.7960893431130578\n",
      "Epoch 58, Training Loss: 0.7975018687107984\n",
      "Epoch 59, Training Loss: 0.7978753373903387\n",
      "Epoch 60, Training Loss: 0.7973376379293554\n",
      "Epoch 61, Training Loss: 0.797727312410579\n",
      "Epoch 62, Training Loss: 0.7963485193252563\n",
      "Epoch 63, Training Loss: 0.7986031100329231\n",
      "Epoch 64, Training Loss: 0.7966512925484601\n",
      "Epoch 65, Training Loss: 0.7986227593001197\n",
      "Epoch 66, Training Loss: 0.796983451422523\n",
      "Epoch 67, Training Loss: 0.7983057290666243\n",
      "Epoch 68, Training Loss: 0.7965250058735118\n",
      "Epoch 69, Training Loss: 0.7964475246737985\n",
      "Epoch 70, Training Loss: 0.7972626386670505\n",
      "Epoch 71, Training Loss: 0.7982165985247668\n",
      "Epoch 72, Training Loss: 0.7960716038591722\n",
      "Epoch 73, Training Loss: 0.7971887231574339\n",
      "Epoch 74, Training Loss: 0.7962672225868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:41:04,980] Trial 162 finished with value: 0.6356 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7975927534524132\n",
      "Epoch 1, Training Loss: 0.8441882106594574\n",
      "Epoch 2, Training Loss: 0.8110447844168297\n",
      "Epoch 3, Training Loss: 0.8100673865554924\n",
      "Epoch 4, Training Loss: 0.8069503293001562\n",
      "Epoch 5, Training Loss: 0.807717874354886\n",
      "Epoch 6, Training Loss: 0.8042935489711905\n",
      "Epoch 7, Training Loss: 0.8029257316338388\n",
      "Epoch 8, Training Loss: 0.8029232197238091\n",
      "Epoch 9, Training Loss: 0.8018385330537208\n",
      "Epoch 10, Training Loss: 0.799944371836526\n",
      "Epoch 11, Training Loss: 0.7992730157715934\n",
      "Epoch 12, Training Loss: 0.7993823150046786\n",
      "Epoch 13, Training Loss: 0.7984190474775501\n",
      "Epoch 14, Training Loss: 0.7992708596968113\n",
      "Epoch 15, Training Loss: 0.7975434869751894\n",
      "Epoch 16, Training Loss: 0.7967284038550871\n",
      "Epoch 17, Training Loss: 0.7983951288058345\n",
      "Epoch 18, Training Loss: 0.7969807704588524\n",
      "Epoch 19, Training Loss: 0.797199581350599\n",
      "Epoch 20, Training Loss: 0.7958868569897529\n",
      "Epoch 21, Training Loss: 0.7963097968495878\n",
      "Epoch 22, Training Loss: 0.7954659509927706\n",
      "Epoch 23, Training Loss: 0.7955033256595296\n",
      "Epoch 24, Training Loss: 0.794995565432355\n",
      "Epoch 25, Training Loss: 0.7955676561011408\n",
      "Epoch 26, Training Loss: 0.7957242294361717\n",
      "Epoch 27, Training Loss: 0.7941866476732985\n",
      "Epoch 28, Training Loss: 0.7951975507843764\n",
      "Epoch 29, Training Loss: 0.7935981201946287\n",
      "Epoch 30, Training Loss: 0.7947090824295704\n",
      "Epoch 31, Training Loss: 0.7959974264740047\n",
      "Epoch 32, Training Loss: 0.7941102406136076\n",
      "Epoch 33, Training Loss: 0.793177960958696\n",
      "Epoch 34, Training Loss: 0.794226858848916\n",
      "Epoch 35, Training Loss: 0.7942806488589237\n",
      "Epoch 36, Training Loss: 0.7943859971555552\n",
      "Epoch 37, Training Loss: 0.7931306616704267\n",
      "Epoch 38, Training Loss: 0.7933028834206718\n",
      "Epoch 39, Training Loss: 0.7948451059205192\n",
      "Epoch 40, Training Loss: 0.7929760097561026\n",
      "Epoch 41, Training Loss: 0.794187863697683\n",
      "Epoch 42, Training Loss: 0.7937195478525377\n",
      "Epoch 43, Training Loss: 0.7932403047281996\n",
      "Epoch 44, Training Loss: 0.7932406382453173\n",
      "Epoch 45, Training Loss: 0.7932306355103514\n",
      "Epoch 46, Training Loss: 0.7944418393579641\n",
      "Epoch 47, Training Loss: 0.7930080229178407\n",
      "Epoch 48, Training Loss: 0.7916848327880516\n",
      "Epoch 49, Training Loss: 0.7919331050456915\n",
      "Epoch 50, Training Loss: 0.7932559686495846\n",
      "Epoch 51, Training Loss: 0.7927859418374256\n",
      "Epoch 52, Training Loss: 0.7912827614554785\n",
      "Epoch 53, Training Loss: 0.7927570396796205\n",
      "Epoch 54, Training Loss: 0.7912210857957825\n",
      "Epoch 55, Training Loss: 0.7924547892764099\n",
      "Epoch 56, Training Loss: 0.7928900066174959\n",
      "Epoch 57, Training Loss: 0.7928404626093413\n",
      "Epoch 58, Training Loss: 0.793492431300027\n",
      "Epoch 59, Training Loss: 0.7916630609591204\n",
      "Epoch 60, Training Loss: 0.7918260430931149\n",
      "Epoch 61, Training Loss: 0.792093855545933\n",
      "Epoch 62, Training Loss: 0.7906130541536145\n",
      "Epoch 63, Training Loss: 0.7912333042101752\n",
      "Epoch 64, Training Loss: 0.7911692537759479\n",
      "Epoch 65, Training Loss: 0.7919145969519937\n",
      "Epoch 66, Training Loss: 0.7924810540407223\n",
      "Epoch 67, Training Loss: 0.7929979031247304\n",
      "Epoch 68, Training Loss: 0.7908082180453423\n",
      "Epoch 69, Training Loss: 0.7918966388343869\n",
      "Epoch 70, Training Loss: 0.7915584782012424\n",
      "Epoch 71, Training Loss: 0.7917100601626518\n",
      "Epoch 72, Training Loss: 0.7917033092419904\n",
      "Epoch 73, Training Loss: 0.7923086605573956\n",
      "Epoch 74, Training Loss: 0.7912218192466219\n",
      "Epoch 75, Training Loss: 0.7911662213784412\n",
      "Epoch 76, Training Loss: 0.790623616724086\n",
      "Epoch 77, Training Loss: 0.7918159295741777\n",
      "Epoch 78, Training Loss: 0.7910550660656807\n",
      "Epoch 79, Training Loss: 0.7917511498121391\n",
      "Epoch 80, Training Loss: 0.790438467398622\n",
      "Epoch 81, Training Loss: 0.790642413788272\n",
      "Epoch 82, Training Loss: 0.7908406041618576\n",
      "Epoch 83, Training Loss: 0.7909544994956569\n",
      "Epoch 84, Training Loss: 0.7913416201906993\n",
      "Epoch 85, Training Loss: 0.7909476335783651\n",
      "Epoch 86, Training Loss: 0.7906620301698384\n",
      "Epoch 87, Training Loss: 0.7913581691290202\n",
      "Epoch 88, Training Loss: 0.7910988570155953\n",
      "Epoch 89, Training Loss: 0.7900313767275415\n",
      "Epoch 90, Training Loss: 0.7912283235922792\n",
      "Epoch 91, Training Loss: 0.7915579299281414\n",
      "Epoch 92, Training Loss: 0.7931483396910187\n",
      "Epoch 93, Training Loss: 0.7914811534989149\n",
      "Epoch 94, Training Loss: 0.7902856757766322\n",
      "Epoch 95, Training Loss: 0.7906322092938244\n",
      "Epoch 96, Training Loss: 0.7907190371276741\n",
      "Epoch 97, Training Loss: 0.7904848351066274\n",
      "Epoch 98, Training Loss: 0.7898141140328314\n",
      "Epoch 99, Training Loss: 0.7901284289539309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:42:43,557] Trial 163 finished with value: 0.6401333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.790965321458372\n",
      "Epoch 1, Training Loss: 0.8854259239103561\n",
      "Epoch 2, Training Loss: 0.8216414712425462\n",
      "Epoch 3, Training Loss: 0.8103503116987701\n",
      "Epoch 4, Training Loss: 0.805890070675011\n",
      "Epoch 5, Training Loss: 0.8026490316355139\n",
      "Epoch 6, Training Loss: 0.79976745386769\n",
      "Epoch 7, Training Loss: 0.7989851168223789\n",
      "Epoch 8, Training Loss: 0.7969853519048906\n",
      "Epoch 9, Training Loss: 0.7966763031213804\n",
      "Epoch 10, Training Loss: 0.7964655247846044\n",
      "Epoch 11, Training Loss: 0.7951742551380531\n",
      "Epoch 12, Training Loss: 0.7945621553220247\n",
      "Epoch 13, Training Loss: 0.7950977853366307\n",
      "Epoch 14, Training Loss: 0.7933115044034513\n",
      "Epoch 15, Training Loss: 0.7933508931246019\n",
      "Epoch 16, Training Loss: 0.7920359947627649\n",
      "Epoch 17, Training Loss: 0.7919095907892499\n",
      "Epoch 18, Training Loss: 0.7919097436998124\n",
      "Epoch 19, Training Loss: 0.7914671326938428\n",
      "Epoch 20, Training Loss: 0.7913447102209679\n",
      "Epoch 21, Training Loss: 0.7906773718676172\n",
      "Epoch 22, Training Loss: 0.7910890793441829\n",
      "Epoch 23, Training Loss: 0.7905567752687555\n",
      "Epoch 24, Training Loss: 0.7897180759368982\n",
      "Epoch 25, Training Loss: 0.7892490461356658\n",
      "Epoch 26, Training Loss: 0.7900669321081693\n",
      "Epoch 27, Training Loss: 0.7896056063193128\n",
      "Epoch 28, Training Loss: 0.7896750215300941\n",
      "Epoch 29, Training Loss: 0.7892968462822133\n",
      "Epoch 30, Training Loss: 0.7897948256112579\n",
      "Epoch 31, Training Loss: 0.7881614016410999\n",
      "Epoch 32, Training Loss: 0.7893401553756312\n",
      "Epoch 33, Training Loss: 0.7888148744303481\n",
      "Epoch 34, Training Loss: 0.7880536018457628\n",
      "Epoch 35, Training Loss: 0.7878944821823809\n",
      "Epoch 36, Training Loss: 0.7881772422252741\n",
      "Epoch 37, Training Loss: 0.7880682288255907\n",
      "Epoch 38, Training Loss: 0.7875972793514567\n",
      "Epoch 39, Training Loss: 0.7876295214308832\n",
      "Epoch 40, Training Loss: 0.7872824210869639\n",
      "Epoch 41, Training Loss: 0.7873362726286838\n",
      "Epoch 42, Training Loss: 0.7872006518500192\n",
      "Epoch 43, Training Loss: 0.7875906255908478\n",
      "Epoch 44, Training Loss: 0.7871414850977131\n",
      "Epoch 45, Training Loss: 0.7869012479495285\n",
      "Epoch 46, Training Loss: 0.7867506672565202\n",
      "Epoch 47, Training Loss: 0.7873045662291964\n",
      "Epoch 48, Training Loss: 0.7881567901238463\n",
      "Epoch 49, Training Loss: 0.7869204052408835\n",
      "Epoch 50, Training Loss: 0.7870862725982093\n",
      "Epoch 51, Training Loss: 0.7872299921243711\n",
      "Epoch 52, Training Loss: 0.7862295866908884\n",
      "Epoch 53, Training Loss: 0.786849143899473\n",
      "Epoch 54, Training Loss: 0.7870730893056196\n",
      "Epoch 55, Training Loss: 0.7861690163612366\n",
      "Epoch 56, Training Loss: 0.7866738377657152\n",
      "Epoch 57, Training Loss: 0.7863675750287852\n",
      "Epoch 58, Training Loss: 0.785556471527071\n",
      "Epoch 59, Training Loss: 0.7858160830081854\n",
      "Epoch 60, Training Loss: 0.7857373464600484\n",
      "Epoch 61, Training Loss: 0.7859502208860297\n",
      "Epoch 62, Training Loss: 0.7853192144766786\n",
      "Epoch 63, Training Loss: 0.7862435647419521\n",
      "Epoch 64, Training Loss: 0.78567166870698\n",
      "Epoch 65, Training Loss: 0.7858106242982965\n",
      "Epoch 66, Training Loss: 0.7853402949813614\n",
      "Epoch 67, Training Loss: 0.7859435924910065\n",
      "Epoch 68, Training Loss: 0.7852133336820101\n",
      "Epoch 69, Training Loss: 0.7860344197517051\n",
      "Epoch 70, Training Loss: 0.7859674614174922\n",
      "Epoch 71, Training Loss: 0.7860358761665517\n",
      "Epoch 72, Training Loss: 0.7857820158614252\n",
      "Epoch 73, Training Loss: 0.7856808674962897\n",
      "Epoch 74, Training Loss: 0.7851493712206532\n",
      "Epoch 75, Training Loss: 0.7855324276407859\n",
      "Epoch 76, Training Loss: 0.7855456618438089\n",
      "Epoch 77, Training Loss: 0.7860375915254866\n",
      "Epoch 78, Training Loss: 0.7853500439708394\n",
      "Epoch 79, Training Loss: 0.7843029092129011\n",
      "Epoch 80, Training Loss: 0.7850838824322349\n",
      "Epoch 81, Training Loss: 0.7860324903538353\n",
      "Epoch 82, Training Loss: 0.785845971286745\n",
      "Epoch 83, Training Loss: 0.7860929495409915\n",
      "Epoch 84, Training Loss: 0.7857715412190086\n",
      "Epoch 85, Training Loss: 0.7849262749342094\n",
      "Epoch 86, Training Loss: 0.7852896409823482\n",
      "Epoch 87, Training Loss: 0.7849515396849553\n",
      "Epoch 88, Training Loss: 0.7858690754811567\n",
      "Epoch 89, Training Loss: 0.7854926372829236\n",
      "Epoch 90, Training Loss: 0.7853967521423684\n",
      "Epoch 91, Training Loss: 0.7861225509105768\n",
      "Epoch 92, Training Loss: 0.7849172245290943\n",
      "Epoch 93, Training Loss: 0.785105838452963\n",
      "Epoch 94, Training Loss: 0.7851984890780055\n",
      "Epoch 95, Training Loss: 0.785152168291852\n",
      "Epoch 96, Training Loss: 0.7857696016031996\n",
      "Epoch 97, Training Loss: 0.7850286113588434\n",
      "Epoch 98, Training Loss: 0.7857678282529788\n",
      "Epoch 99, Training Loss: 0.7846140886607923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:44:33,386] Trial 164 finished with value: 0.6282666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7850308499838177\n",
      "Epoch 1, Training Loss: 1.0088927418665778\n",
      "Epoch 2, Training Loss: 0.9356123829246463\n",
      "Epoch 3, Training Loss: 0.9197396427168882\n",
      "Epoch 4, Training Loss: 0.9066514769891151\n",
      "Epoch 5, Training Loss: 0.8889212208582943\n",
      "Epoch 6, Training Loss: 0.8648075625412446\n",
      "Epoch 7, Training Loss: 0.8381792126741624\n",
      "Epoch 8, Training Loss: 0.8205891923796862\n",
      "Epoch 9, Training Loss: 0.8119439240685082\n",
      "Epoch 10, Training Loss: 0.8099674821796273\n",
      "Epoch 11, Training Loss: 0.807904775250227\n",
      "Epoch 12, Training Loss: 0.8055288715470106\n",
      "Epoch 13, Training Loss: 0.8050977102795938\n",
      "Epoch 14, Training Loss: 0.8033032967631978\n",
      "Epoch 15, Training Loss: 0.8032332829962996\n",
      "Epoch 16, Training Loss: 0.802006611250397\n",
      "Epoch 17, Training Loss: 0.8015367035578964\n",
      "Epoch 18, Training Loss: 0.8016226617913497\n",
      "Epoch 19, Training Loss: 0.7996566691792997\n",
      "Epoch 20, Training Loss: 0.7993307183559676\n",
      "Epoch 21, Training Loss: 0.7988949550721879\n",
      "Epoch 22, Training Loss: 0.7985680709207864\n",
      "Epoch 23, Training Loss: 0.7985976409195061\n",
      "Epoch 24, Training Loss: 0.7980760510702779\n",
      "Epoch 25, Training Loss: 0.7961875472749983\n",
      "Epoch 26, Training Loss: 0.7968703307603535\n",
      "Epoch 27, Training Loss: 0.7957699801688803\n",
      "Epoch 28, Training Loss: 0.7957737222649998\n",
      "Epoch 29, Training Loss: 0.7956352644396903\n",
      "Epoch 30, Training Loss: 0.7951276092601002\n",
      "Epoch 31, Training Loss: 0.7950468230964546\n",
      "Epoch 32, Training Loss: 0.7943931888816949\n",
      "Epoch 33, Training Loss: 0.7946673444339207\n",
      "Epoch 34, Training Loss: 0.7935491927584312\n",
      "Epoch 35, Training Loss: 0.7938408449180144\n",
      "Epoch 36, Training Loss: 0.7927732585964347\n",
      "Epoch 37, Training Loss: 0.7928631549491022\n",
      "Epoch 38, Training Loss: 0.793080226640056\n",
      "Epoch 39, Training Loss: 0.7920494411224709\n",
      "Epoch 40, Training Loss: 0.7916569716948315\n",
      "Epoch 41, Training Loss: 0.7910058929507894\n",
      "Epoch 42, Training Loss: 0.7913340473533573\n",
      "Epoch 43, Training Loss: 0.7907371630345968\n",
      "Epoch 44, Training Loss: 0.7904496181280093\n",
      "Epoch 45, Training Loss: 0.7905556287084307\n",
      "Epoch 46, Training Loss: 0.7903567208383316\n",
      "Epoch 47, Training Loss: 0.789812966307303\n",
      "Epoch 48, Training Loss: 0.7905175089836121\n",
      "Epoch 49, Training Loss: 0.7898009202534095\n",
      "Epoch 50, Training Loss: 0.788926000792281\n",
      "Epoch 51, Training Loss: 0.789060594354357\n",
      "Epoch 52, Training Loss: 0.788841598464134\n",
      "Epoch 53, Training Loss: 0.7883429029830417\n",
      "Epoch 54, Training Loss: 0.7885095127543112\n",
      "Epoch 55, Training Loss: 0.7875940219800275\n",
      "Epoch 56, Training Loss: 0.7879448171845056\n",
      "Epoch 57, Training Loss: 0.787793885496326\n",
      "Epoch 58, Training Loss: 0.7868682415861833\n",
      "Epoch 59, Training Loss: 0.7872187643122852\n",
      "Epoch 60, Training Loss: 0.7863242282903284\n",
      "Epoch 61, Training Loss: 0.7867505314654873\n",
      "Epoch 62, Training Loss: 0.7864056866868098\n",
      "Epoch 63, Training Loss: 0.7873093854215808\n",
      "Epoch 64, Training Loss: 0.7866232999285361\n",
      "Epoch 65, Training Loss: 0.7861303414617266\n",
      "Epoch 66, Training Loss: 0.7864458932912439\n",
      "Epoch 67, Training Loss: 0.7863566079534086\n",
      "Epoch 68, Training Loss: 0.7853480325605636\n",
      "Epoch 69, Training Loss: 0.7858290684850593\n",
      "Epoch 70, Training Loss: 0.7862890084883324\n",
      "Epoch 71, Training Loss: 0.7850883591892128\n",
      "Epoch 72, Training Loss: 0.7850233432941867\n",
      "Epoch 73, Training Loss: 0.7851225167288816\n",
      "Epoch 74, Training Loss: 0.7852452017310867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:45:41,382] Trial 165 finished with value: 0.6376666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7850900860657369\n",
      "Epoch 1, Training Loss: 0.9898972119303311\n",
      "Epoch 2, Training Loss: 0.9481098694661084\n",
      "Epoch 3, Training Loss: 0.934716999390546\n",
      "Epoch 4, Training Loss: 0.9199745502191431\n",
      "Epoch 5, Training Loss: 0.9025008157421561\n",
      "Epoch 6, Training Loss: 0.8822101101454567\n",
      "Epoch 7, Training Loss: 0.8610523250523735\n",
      "Epoch 8, Training Loss: 0.8424835667890661\n",
      "Epoch 9, Training Loss: 0.8293528214622946\n",
      "Epoch 10, Training Loss: 0.8213635350676144\n",
      "Epoch 11, Training Loss: 0.8170879858381608\n",
      "Epoch 12, Training Loss: 0.8148505255755256\n",
      "Epoch 13, Training Loss: 0.8130579848850474\n",
      "Epoch 14, Training Loss: 0.8122056349586038\n",
      "Epoch 15, Training Loss: 0.8112828328328974\n",
      "Epoch 16, Training Loss: 0.8108602763624753\n",
      "Epoch 17, Training Loss: 0.8101268967460183\n",
      "Epoch 18, Training Loss: 0.809452267183977\n",
      "Epoch 19, Training Loss: 0.8088202619552612\n",
      "Epoch 20, Training Loss: 0.8082187544598299\n",
      "Epoch 21, Training Loss: 0.8075090403416577\n",
      "Epoch 22, Training Loss: 0.8073833134595085\n",
      "Epoch 23, Training Loss: 0.8067735335406135\n",
      "Epoch 24, Training Loss: 0.805975743602304\n",
      "Epoch 25, Training Loss: 0.8057024474704967\n",
      "Epoch 26, Training Loss: 0.8052958433067098\n",
      "Epoch 27, Training Loss: 0.8047164600736955\n",
      "Epoch 28, Training Loss: 0.8043321462238536\n",
      "Epoch 29, Training Loss: 0.8036685888907489\n",
      "Epoch 30, Training Loss: 0.8033602943841149\n",
      "Epoch 31, Training Loss: 0.8031038155275233\n",
      "Epoch 32, Training Loss: 0.8025953405744889\n",
      "Epoch 33, Training Loss: 0.8024037961398854\n",
      "Epoch 34, Training Loss: 0.8020898057432736\n",
      "Epoch 35, Training Loss: 0.8017106520428378\n",
      "Epoch 36, Training Loss: 0.8014403681895312\n",
      "Epoch 37, Training Loss: 0.801165861452327\n",
      "Epoch 38, Training Loss: 0.8011085125278025\n",
      "Epoch 39, Training Loss: 0.8007714240691242\n",
      "Epoch 40, Training Loss: 0.8003150361425736\n",
      "Epoch 41, Training Loss: 0.8002718531384188\n",
      "Epoch 42, Training Loss: 0.8000569852660684\n",
      "Epoch 43, Training Loss: 0.8000848461599911\n",
      "Epoch 44, Training Loss: 0.799745198207743\n",
      "Epoch 45, Training Loss: 0.7996903138300951\n",
      "Epoch 46, Training Loss: 0.7995154281223521\n",
      "Epoch 47, Training Loss: 0.799321176444783\n",
      "Epoch 48, Training Loss: 0.7992595826177036\n",
      "Epoch 49, Training Loss: 0.799247058840359\n",
      "Epoch 50, Training Loss: 0.7990929309059592\n",
      "Epoch 51, Training Loss: 0.7989669388182024\n",
      "Epoch 52, Training Loss: 0.7985436384116902\n",
      "Epoch 53, Training Loss: 0.7987504889684565\n",
      "Epoch 54, Training Loss: 0.7985396787699531\n",
      "Epoch 55, Training Loss: 0.7984287998255561\n",
      "Epoch 56, Training Loss: 0.7984245833929847\n",
      "Epoch 57, Training Loss: 0.7981441622621873\n",
      "Epoch 58, Training Loss: 0.798092740984524\n",
      "Epoch 59, Training Loss: 0.7980801438584048\n",
      "Epoch 60, Training Loss: 0.7979444730281829\n",
      "Epoch 61, Training Loss: 0.797801232548321\n",
      "Epoch 62, Training Loss: 0.7978109535750221\n",
      "Epoch 63, Training Loss: 0.7977373008868274\n",
      "Epoch 64, Training Loss: 0.7976269212891074\n",
      "Epoch 65, Training Loss: 0.7975116151921889\n",
      "Epoch 66, Training Loss: 0.7976183664798736\n",
      "Epoch 67, Training Loss: 0.7975450684743769\n",
      "Epoch 68, Training Loss: 0.797491788373274\n",
      "Epoch 69, Training Loss: 0.7974012781591976\n",
      "Epoch 70, Training Loss: 0.7973112633649041\n",
      "Epoch 71, Training Loss: 0.7971488627265482\n",
      "Epoch 72, Training Loss: 0.7969633671816657\n",
      "Epoch 73, Training Loss: 0.7973110245957095\n",
      "Epoch 74, Training Loss: 0.7970485983876621\n",
      "Epoch 75, Training Loss: 0.796940477735856\n",
      "Epoch 76, Training Loss: 0.7967612049860113\n",
      "Epoch 77, Training Loss: 0.7968986867455875\n",
      "Epoch 78, Training Loss: 0.796972376388662\n",
      "Epoch 79, Training Loss: 0.7966668067258947\n",
      "Epoch 80, Training Loss: 0.7964409096100751\n",
      "Epoch 81, Training Loss: 0.7965563480994281\n",
      "Epoch 82, Training Loss: 0.7966462613554561\n",
      "Epoch 83, Training Loss: 0.7965936302437502\n",
      "Epoch 84, Training Loss: 0.7964195888182696\n",
      "Epoch 85, Training Loss: 0.7962889148908503\n",
      "Epoch 86, Training Loss: 0.7961748820893905\n",
      "Epoch 87, Training Loss: 0.7960067912410287\n",
      "Epoch 88, Training Loss: 0.7963906490802765\n",
      "Epoch 89, Training Loss: 0.7962113374822279\n",
      "Epoch 90, Training Loss: 0.79612147268127\n",
      "Epoch 91, Training Loss: 0.796032875846414\n",
      "Epoch 92, Training Loss: 0.7960234157478108\n",
      "Epoch 93, Training Loss: 0.7958855023103601\n",
      "Epoch 94, Training Loss: 0.7958716596575345\n",
      "Epoch 95, Training Loss: 0.7956922005204593\n",
      "Epoch 96, Training Loss: 0.7955468771738164\n",
      "Epoch 97, Training Loss: 0.7956462346105014\n",
      "Epoch 98, Training Loss: 0.7956730505298165\n",
      "Epoch 99, Training Loss: 0.7955640714308795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:47:31,249] Trial 166 finished with value: 0.6339333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7954667368355919\n",
      "Epoch 1, Training Loss: 0.9731523011860095\n",
      "Epoch 2, Training Loss: 0.9448778351446739\n",
      "Epoch 3, Training Loss: 0.9297905281970376\n",
      "Epoch 4, Training Loss: 0.9139999844077834\n",
      "Epoch 5, Training Loss: 0.8982559462238972\n",
      "Epoch 6, Training Loss: 0.8827033939218163\n",
      "Epoch 7, Training Loss: 0.8678697448027761\n",
      "Epoch 8, Training Loss: 0.8551244281288376\n",
      "Epoch 9, Training Loss: 0.8444140035406987\n",
      "Epoch 10, Training Loss: 0.8356894089763326\n",
      "Epoch 11, Training Loss: 0.8296083727277311\n",
      "Epoch 12, Training Loss: 0.824607269208234\n",
      "Epoch 13, Training Loss: 0.8212710721152169\n",
      "Epoch 14, Training Loss: 0.8187068314480602\n",
      "Epoch 15, Training Loss: 0.8163281765199245\n",
      "Epoch 16, Training Loss: 0.8152556304644821\n",
      "Epoch 17, Training Loss: 0.813848774773734\n",
      "Epoch 18, Training Loss: 0.813155216023438\n",
      "Epoch 19, Training Loss: 0.8124425970522084\n",
      "Epoch 20, Training Loss: 0.8113499299924176\n",
      "Epoch 21, Training Loss: 0.8112759729973356\n",
      "Epoch 22, Training Loss: 0.810563476013958\n",
      "Epoch 23, Training Loss: 0.8101857162059698\n",
      "Epoch 24, Training Loss: 0.8097964755574564\n",
      "Epoch 25, Training Loss: 0.8091712312590807\n",
      "Epoch 26, Training Loss: 0.8090784027164144\n",
      "Epoch 27, Training Loss: 0.808970836858104\n",
      "Epoch 28, Training Loss: 0.8086268325497333\n",
      "Epoch 29, Training Loss: 0.808683929407507\n",
      "Epoch 30, Training Loss: 0.8079670200670572\n",
      "Epoch 31, Training Loss: 0.8072910485411049\n",
      "Epoch 32, Training Loss: 0.806672203540802\n",
      "Epoch 33, Training Loss: 0.8070257820581135\n",
      "Epoch 34, Training Loss: 0.8066251201737196\n",
      "Epoch 35, Training Loss: 0.806276183648217\n",
      "Epoch 36, Training Loss: 0.8055016592032928\n",
      "Epoch 37, Training Loss: 0.8053791338339784\n",
      "Epoch 38, Training Loss: 0.8056602939627224\n",
      "Epoch 39, Training Loss: 0.8051318785301724\n",
      "Epoch 40, Training Loss: 0.8045438468904423\n",
      "Epoch 41, Training Loss: 0.8040513612273941\n",
      "Epoch 42, Training Loss: 0.8047394363503707\n",
      "Epoch 43, Training Loss: 0.804494527587317\n",
      "Epoch 44, Training Loss: 0.8034262178535748\n",
      "Epoch 45, Training Loss: 0.803321251474825\n",
      "Epoch 46, Training Loss: 0.8032029492514474\n",
      "Epoch 47, Training Loss: 0.802365497538918\n",
      "Epoch 48, Training Loss: 0.8019783553324248\n",
      "Epoch 49, Training Loss: 0.8020804973473227\n",
      "Epoch 50, Training Loss: 0.8019247162610965\n",
      "Epoch 51, Training Loss: 0.8015538083879571\n",
      "Epoch 52, Training Loss: 0.802214515209198\n",
      "Epoch 53, Training Loss: 0.8024017178922668\n",
      "Epoch 54, Training Loss: 0.8017962298895184\n",
      "Epoch 55, Training Loss: 0.8009526259917066\n",
      "Epoch 56, Training Loss: 0.8007635829143954\n",
      "Epoch 57, Training Loss: 0.8008851528167724\n",
      "Epoch 58, Training Loss: 0.8003878789736812\n",
      "Epoch 59, Training Loss: 0.8007414790920745\n",
      "Epoch 60, Training Loss: 0.8008510683712207\n",
      "Epoch 61, Training Loss: 0.8008947228130542\n",
      "Epoch 62, Training Loss: 0.7998943441792539\n",
      "Epoch 63, Training Loss: 0.8003357578937272\n",
      "Epoch 64, Training Loss: 0.8005406050753773\n",
      "Epoch 65, Training Loss: 0.7996259523065467\n",
      "Epoch 66, Training Loss: 0.7995495452916712\n",
      "Epoch 67, Training Loss: 0.7999449204681511\n",
      "Epoch 68, Training Loss: 0.7996601060817116\n",
      "Epoch 69, Training Loss: 0.7995952553318856\n",
      "Epoch 70, Training Loss: 0.7993626878673868\n",
      "Epoch 71, Training Loss: 0.7990908151282403\n",
      "Epoch 72, Training Loss: 0.7996424943880928\n",
      "Epoch 73, Training Loss: 0.7996939438626283\n",
      "Epoch 74, Training Loss: 0.7991947518255478\n",
      "Epoch 75, Training Loss: 0.7988612013651912\n",
      "Epoch 76, Training Loss: 0.7986204250414569\n",
      "Epoch 77, Training Loss: 0.7991638864789691\n",
      "Epoch 78, Training Loss: 0.7989278761964095\n",
      "Epoch 79, Training Loss: 0.7985464228723282\n",
      "Epoch 80, Training Loss: 0.7985110389558893\n",
      "Epoch 81, Training Loss: 0.7985281824169302\n",
      "Epoch 82, Training Loss: 0.7986947442355908\n",
      "Epoch 83, Training Loss: 0.7986082068959574\n",
      "Epoch 84, Training Loss: 0.7988125780471286\n",
      "Epoch 85, Training Loss: 0.79843721775184\n",
      "Epoch 86, Training Loss: 0.7983662823985394\n",
      "Epoch 87, Training Loss: 0.7985576473680654\n",
      "Epoch 88, Training Loss: 0.7981783010009537\n",
      "Epoch 89, Training Loss: 0.7985063649657974\n",
      "Epoch 90, Training Loss: 0.7981386204411213\n",
      "Epoch 91, Training Loss: 0.7985480426845694\n",
      "Epoch 92, Training Loss: 0.7981102494368876\n",
      "Epoch 93, Training Loss: 0.7981220339473926\n",
      "Epoch 94, Training Loss: 0.7979984744150835\n",
      "Epoch 95, Training Loss: 0.7990688332937714\n",
      "Epoch 96, Training Loss: 0.7978870576485655\n",
      "Epoch 97, Training Loss: 0.797850683577975\n",
      "Epoch 98, Training Loss: 0.797908670023868\n",
      "Epoch 99, Training Loss: 0.7975350816447035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:48:55,515] Trial 167 finished with value: 0.6317333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7975139319448543\n",
      "Epoch 1, Training Loss: 0.8506977098328726\n",
      "Epoch 2, Training Loss: 0.813475201183692\n",
      "Epoch 3, Training Loss: 0.8088648631160421\n",
      "Epoch 4, Training Loss: 0.803427425183748\n",
      "Epoch 5, Training Loss: 0.8016594732614388\n",
      "Epoch 6, Training Loss: 0.798092752381375\n",
      "Epoch 7, Training Loss: 0.7989518961512057\n",
      "Epoch 8, Training Loss: 0.7988673689670133\n",
      "Epoch 9, Training Loss: 0.7990012818709352\n",
      "Epoch 10, Training Loss: 0.7971152659645654\n",
      "Epoch 11, Training Loss: 0.7971768349633181\n",
      "Epoch 12, Training Loss: 0.7968121193405381\n",
      "Epoch 13, Training Loss: 0.7966227974210467\n",
      "Epoch 14, Training Loss: 0.797400785209541\n",
      "Epoch 15, Training Loss: 0.7947645127325129\n",
      "Epoch 16, Training Loss: 0.7960719742273029\n",
      "Epoch 17, Training Loss: 0.7953852360409902\n",
      "Epoch 18, Training Loss: 0.7971094714967828\n",
      "Epoch 19, Training Loss: 0.7959052140551403\n",
      "Epoch 20, Training Loss: 0.79509044939414\n",
      "Epoch 21, Training Loss: 0.7954586736689833\n",
      "Epoch 22, Training Loss: 0.7937569258804608\n",
      "Epoch 23, Training Loss: 0.7950745837132733\n",
      "Epoch 24, Training Loss: 0.7938986480684209\n",
      "Epoch 25, Training Loss: 0.7943493042673383\n",
      "Epoch 26, Training Loss: 0.7949920168496613\n",
      "Epoch 27, Training Loss: 0.792921227082274\n",
      "Epoch 28, Training Loss: 0.7946115077886367\n",
      "Epoch 29, Training Loss: 0.7931137294697582\n",
      "Epoch 30, Training Loss: 0.7940310687947094\n",
      "Epoch 31, Training Loss: 0.794907491906245\n",
      "Epoch 32, Training Loss: 0.7938064622699766\n",
      "Epoch 33, Training Loss: 0.793423034553241\n",
      "Epoch 34, Training Loss: 0.7947789214607468\n",
      "Epoch 35, Training Loss: 0.7937918357383039\n",
      "Epoch 36, Training Loss: 0.791264898346779\n",
      "Epoch 37, Training Loss: 0.7928868065203043\n",
      "Epoch 38, Training Loss: 0.7935887351071924\n",
      "Epoch 39, Training Loss: 0.792700369555251\n",
      "Epoch 40, Training Loss: 0.7923070774042517\n",
      "Epoch 41, Training Loss: 0.7917770372297531\n",
      "Epoch 42, Training Loss: 0.7942304066249303\n",
      "Epoch 43, Training Loss: 0.7921172717460117\n",
      "Epoch 44, Training Loss: 0.7918748452251119\n",
      "Epoch 45, Training Loss: 0.7941099839102953\n",
      "Epoch 46, Training Loss: 0.7919929723990591\n",
      "Epoch 47, Training Loss: 0.7934293035277746\n",
      "Epoch 48, Training Loss: 0.7916339334688689\n",
      "Epoch 49, Training Loss: 0.7923019964892165\n",
      "Epoch 50, Training Loss: 0.7920826144684526\n",
      "Epoch 51, Training Loss: 0.792606214383491\n",
      "Epoch 52, Training Loss: 0.7904028195411639\n",
      "Epoch 53, Training Loss: 0.7922900759187856\n",
      "Epoch 54, Training Loss: 0.7932243622335277\n",
      "Epoch 55, Training Loss: 0.7912989037377494\n",
      "Epoch 56, Training Loss: 0.7919925731823857\n",
      "Epoch 57, Training Loss: 0.7925952719566517\n",
      "Epoch 58, Training Loss: 0.7914527994349487\n",
      "Epoch 59, Training Loss: 0.7914763102854104\n",
      "Epoch 60, Training Loss: 0.7916989957479607\n",
      "Epoch 61, Training Loss: 0.7916818117736873\n",
      "Epoch 62, Training Loss: 0.7920894213188859\n",
      "Epoch 63, Training Loss: 0.7927404839293402\n",
      "Epoch 64, Training Loss: 0.7918187499942636\n",
      "Epoch 65, Training Loss: 0.7923583455551836\n",
      "Epoch 66, Training Loss: 0.7916045041012585\n",
      "Epoch 67, Training Loss: 0.7921161217797071\n",
      "Epoch 68, Training Loss: 0.7907651877941045\n",
      "Epoch 69, Training Loss: 0.7921198878073155\n",
      "Epoch 70, Training Loss: 0.7916974929938639\n",
      "Epoch 71, Training Loss: 0.7918758130611334\n",
      "Epoch 72, Training Loss: 0.7919568384500374\n",
      "Epoch 73, Training Loss: 0.7920836178879989\n",
      "Epoch 74, Training Loss: 0.7907092548402629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:50:23,408] Trial 168 finished with value: 0.6388666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7907052991085483\n",
      "Epoch 1, Training Loss: 0.8482217215953913\n",
      "Epoch 2, Training Loss: 0.8171748141597088\n",
      "Epoch 3, Training Loss: 0.8124051990813779\n",
      "Epoch 4, Training Loss: 0.8093590091045638\n",
      "Epoch 5, Training Loss: 0.8087148003112105\n",
      "Epoch 6, Training Loss: 0.8066109850890655\n",
      "Epoch 7, Training Loss: 0.8054967221909\n",
      "Epoch 8, Training Loss: 0.8050323645871384\n",
      "Epoch 9, Training Loss: 0.8034553838851757\n",
      "Epoch 10, Training Loss: 0.8035088484448598\n",
      "Epoch 11, Training Loss: 0.8029409195247449\n",
      "Epoch 12, Training Loss: 0.8012060259966026\n",
      "Epoch 13, Training Loss: 0.8013955454181011\n",
      "Epoch 14, Training Loss: 0.8004970603419426\n",
      "Epoch 15, Training Loss: 0.8010172108062228\n",
      "Epoch 16, Training Loss: 0.8007565720637042\n",
      "Epoch 17, Training Loss: 0.8001807241511524\n",
      "Epoch 18, Training Loss: 0.7999099112094793\n",
      "Epoch 19, Training Loss: 0.7995895388431119\n",
      "Epoch 20, Training Loss: 0.7990967909196266\n",
      "Epoch 21, Training Loss: 0.7989972382559812\n",
      "Epoch 22, Training Loss: 0.7982297620827095\n",
      "Epoch 23, Training Loss: 0.7990999655616015\n",
      "Epoch 24, Training Loss: 0.7978260984994415\n",
      "Epoch 25, Training Loss: 0.7985900897728769\n",
      "Epoch 26, Training Loss: 0.7979455392163499\n",
      "Epoch 27, Training Loss: 0.7977654314578924\n",
      "Epoch 28, Training Loss: 0.797587603823583\n",
      "Epoch 29, Training Loss: 0.797099957340642\n",
      "Epoch 30, Training Loss: 0.7966961226068942\n",
      "Epoch 31, Training Loss: 0.7964121607909526\n",
      "Epoch 32, Training Loss: 0.7970789772227295\n",
      "Epoch 33, Training Loss: 0.7966028397244619\n",
      "Epoch 34, Training Loss: 0.7960281891034062\n",
      "Epoch 35, Training Loss: 0.7962750588144575\n",
      "Epoch 36, Training Loss: 0.7960137787618136\n",
      "Epoch 37, Training Loss: 0.7955087785434006\n",
      "Epoch 38, Training Loss: 0.7961940637208466\n",
      "Epoch 39, Training Loss: 0.7958952664880824\n",
      "Epoch 40, Training Loss: 0.794999617293365\n",
      "Epoch 41, Training Loss: 0.7949397769189419\n",
      "Epoch 42, Training Loss: 0.7957142071616381\n",
      "Epoch 43, Training Loss: 0.7959018920597277\n",
      "Epoch 44, Training Loss: 0.7954956287728217\n",
      "Epoch 45, Training Loss: 0.7948683450992843\n",
      "Epoch 46, Training Loss: 0.7945235167230879\n",
      "Epoch 47, Training Loss: 0.7948936419379442\n",
      "Epoch 48, Training Loss: 0.7950131985477935\n",
      "Epoch 49, Training Loss: 0.7943333571118519\n",
      "Epoch 50, Training Loss: 0.7951168560443964\n",
      "Epoch 51, Training Loss: 0.7942762373085309\n",
      "Epoch 52, Training Loss: 0.7944590372250493\n",
      "Epoch 53, Training Loss: 0.7938194348847956\n",
      "Epoch 54, Training Loss: 0.7945886212184017\n",
      "Epoch 55, Training Loss: 0.7945658347660438\n",
      "Epoch 56, Training Loss: 0.7944239372597601\n",
      "Epoch 57, Training Loss: 0.7935025538716998\n",
      "Epoch 58, Training Loss: 0.7942040265950941\n",
      "Epoch 59, Training Loss: 0.7939285220060134\n",
      "Epoch 60, Training Loss: 0.7942521382095222\n",
      "Epoch 61, Training Loss: 0.7935198826897413\n",
      "Epoch 62, Training Loss: 0.7929356356312458\n",
      "Epoch 63, Training Loss: 0.7934300351411776\n",
      "Epoch 64, Training Loss: 0.7930855428365836\n",
      "Epoch 65, Training Loss: 0.7935334829459513\n",
      "Epoch 66, Training Loss: 0.7933829779911759\n",
      "Epoch 67, Training Loss: 0.7939679217517824\n",
      "Epoch 68, Training Loss: 0.7937320202813113\n",
      "Epoch 69, Training Loss: 0.7938057661056519\n",
      "Epoch 70, Training Loss: 0.7934552549419547\n",
      "Epoch 71, Training Loss: 0.7936454311349338\n",
      "Epoch 72, Training Loss: 0.7939956476813869\n",
      "Epoch 73, Training Loss: 0.7939348106097458\n",
      "Epoch 74, Training Loss: 0.7924451960656876\n",
      "Epoch 75, Training Loss: 0.7930088883952091\n",
      "Epoch 76, Training Loss: 0.793460083814492\n",
      "Epoch 77, Training Loss: 0.7929134415504627\n",
      "Epoch 78, Training Loss: 0.792861429013704\n",
      "Epoch 79, Training Loss: 0.7931208079918883\n",
      "Epoch 80, Training Loss: 0.792086827037926\n",
      "Epoch 81, Training Loss: 0.7924203878058527\n",
      "Epoch 82, Training Loss: 0.7929777580096309\n",
      "Epoch 83, Training Loss: 0.7922946906627569\n",
      "Epoch 84, Training Loss: 0.7923293229332544\n",
      "Epoch 85, Training Loss: 0.7925292018660925\n",
      "Epoch 86, Training Loss: 0.7913989931120908\n",
      "Epoch 87, Training Loss: 0.7919887976538866\n",
      "Epoch 88, Training Loss: 0.7926045125588439\n",
      "Epoch 89, Training Loss: 0.7922365444943421\n",
      "Epoch 90, Training Loss: 0.7922679484338688\n",
      "Epoch 91, Training Loss: 0.7915772270439263\n",
      "Epoch 92, Training Loss: 0.7922078481294159\n",
      "Epoch 93, Training Loss: 0.7919740787126068\n",
      "Epoch 94, Training Loss: 0.791832223064021\n",
      "Epoch 95, Training Loss: 0.7923700737773924\n",
      "Epoch 96, Training Loss: 0.7925434125097175\n",
      "Epoch 97, Training Loss: 0.7926698295693648\n",
      "Epoch 98, Training Loss: 0.7915006927081517\n",
      "Epoch 99, Training Loss: 0.7926906317696536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:51:57,503] Trial 169 finished with value: 0.6114666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7924984551910171\n",
      "Epoch 1, Training Loss: 0.84683474575772\n",
      "Epoch 2, Training Loss: 0.8138844389073989\n",
      "Epoch 3, Training Loss: 0.8109483938357409\n",
      "Epoch 4, Training Loss: 0.8037098252072054\n",
      "Epoch 5, Training Loss: 0.803291004265056\n",
      "Epoch 6, Training Loss: 0.8014739730077631\n",
      "Epoch 7, Training Loss: 0.8001416795394\n",
      "Epoch 8, Training Loss: 0.8020231781286352\n",
      "Epoch 9, Training Loss: 0.7988754606246948\n",
      "Epoch 10, Training Loss: 0.7997308217076694\n",
      "Epoch 11, Training Loss: 0.7985741840390598\n",
      "Epoch 12, Training Loss: 0.7973884393888361\n",
      "Epoch 13, Training Loss: 0.7978649179374471\n",
      "Epoch 14, Training Loss: 0.79779738685664\n",
      "Epoch 15, Training Loss: 0.7964776103636798\n",
      "Epoch 16, Training Loss: 0.7968299298426684\n",
      "Epoch 17, Training Loss: 0.796854505258448\n",
      "Epoch 18, Training Loss: 0.7954256085788503\n",
      "Epoch 19, Training Loss: 0.7946612078302047\n",
      "Epoch 20, Training Loss: 0.7972595701498144\n",
      "Epoch 21, Training Loss: 0.7955987490625942\n",
      "Epoch 22, Training Loss: 0.7964123549180873\n",
      "Epoch 23, Training Loss: 0.7946668137522305\n",
      "Epoch 24, Training Loss: 0.7970284538409289\n",
      "Epoch 25, Training Loss: 0.7956158305616939\n",
      "Epoch 26, Training Loss: 0.7962741941564223\n",
      "Epoch 27, Training Loss: 0.7940845615022323\n",
      "Epoch 28, Training Loss: 0.7969968129606808\n",
      "Epoch 29, Training Loss: 0.79408269405365\n",
      "Epoch 30, Training Loss: 0.7945581851987278\n",
      "Epoch 31, Training Loss: 0.7942083128760843\n",
      "Epoch 32, Training Loss: 0.7943143140568453\n",
      "Epoch 33, Training Loss: 0.7945132508698631\n",
      "Epoch 34, Training Loss: 0.7942719841003418\n",
      "Epoch 35, Training Loss: 0.7943917576004477\n",
      "Epoch 36, Training Loss: 0.794848596348482\n",
      "Epoch 37, Training Loss: 0.7943167157734141\n",
      "Epoch 38, Training Loss: 0.7941629880316117\n",
      "Epoch 39, Training Loss: 0.7934373570890988\n",
      "Epoch 40, Training Loss: 0.7944742931337918\n",
      "Epoch 41, Training Loss: 0.7930891993466546\n",
      "Epoch 42, Training Loss: 0.7940880311236662\n",
      "Epoch 43, Training Loss: 0.7927855892742381\n",
      "Epoch 44, Training Loss: 0.7944002006334417\n",
      "Epoch 45, Training Loss: 0.7942476444384631\n",
      "Epoch 46, Training Loss: 0.7941357642061571\n",
      "Epoch 47, Training Loss: 0.7927111572377822\n",
      "Epoch 48, Training Loss: 0.7925084991314831\n",
      "Epoch 49, Training Loss: 0.793305132669561\n",
      "Epoch 50, Training Loss: 0.7941268990320318\n",
      "Epoch 51, Training Loss: 0.7924578399517956\n",
      "Epoch 52, Training Loss: 0.7929305295383229\n",
      "Epoch 53, Training Loss: 0.7935663321438957\n",
      "Epoch 54, Training Loss: 0.794385623441023\n",
      "Epoch 55, Training Loss: 0.7926870746472302\n",
      "Epoch 56, Training Loss: 0.7930886378709008\n",
      "Epoch 57, Training Loss: 0.7926824212775511\n",
      "Epoch 58, Training Loss: 0.7933414856826558\n",
      "Epoch 59, Training Loss: 0.792373258857166\n",
      "Epoch 60, Training Loss: 0.7935901348029866\n",
      "Epoch 61, Training Loss: 0.7915598803408006\n",
      "Epoch 62, Training Loss: 0.7923423072169808\n",
      "Epoch 63, Training Loss: 0.7936248967226813\n",
      "Epoch 64, Training Loss: 0.7938319697099574\n",
      "Epoch 65, Training Loss: 0.7939312089891994\n",
      "Epoch 66, Training Loss: 0.7929232548264896\n",
      "Epoch 67, Training Loss: 0.7923522774612203\n",
      "Epoch 68, Training Loss: 0.7929820823669433\n",
      "Epoch 69, Training Loss: 0.7918844260888941\n",
      "Epoch 70, Training Loss: 0.7921212570807513\n",
      "Epoch 71, Training Loss: 0.7923054512809304\n",
      "Epoch 72, Training Loss: 0.7932122594468733\n",
      "Epoch 73, Training Loss: 0.7919966746779049\n",
      "Epoch 74, Training Loss: 0.7916775422236498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:53:38,406] Trial 170 finished with value: 0.6374666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7921582355920006\n",
      "Epoch 1, Training Loss: 1.0921605248200266\n",
      "Epoch 2, Training Loss: 1.0897751892419687\n",
      "Epoch 3, Training Loss: 1.0880320380504866\n",
      "Epoch 4, Training Loss: 1.0860305117485218\n",
      "Epoch 5, Training Loss: 1.0839179076646503\n",
      "Epoch 6, Training Loss: 1.081350240671545\n",
      "Epoch 7, Training Loss: 1.0782769047228018\n",
      "Epoch 8, Training Loss: 1.0745062924865494\n",
      "Epoch 9, Training Loss: 1.0704145750605074\n",
      "Epoch 10, Training Loss: 1.0649424744727916\n",
      "Epoch 11, Training Loss: 1.0584954686630936\n",
      "Epoch 12, Training Loss: 1.0506963315762972\n",
      "Epoch 13, Training Loss: 1.041712466666573\n",
      "Epoch 14, Training Loss: 1.031537525994437\n",
      "Epoch 15, Training Loss: 1.0210027069077456\n",
      "Epoch 16, Training Loss: 1.0103307864719764\n",
      "Epoch 17, Training Loss: 1.00103344370548\n",
      "Epoch 18, Training Loss: 0.9927213001968269\n",
      "Epoch 19, Training Loss: 0.9861613546995293\n",
      "Epoch 20, Training Loss: 0.9807561294476789\n",
      "Epoch 21, Training Loss: 0.9763834700548559\n",
      "Epoch 22, Training Loss: 0.9724825778401884\n",
      "Epoch 23, Training Loss: 0.9694164289567704\n",
      "Epoch 24, Training Loss: 0.9670677773934558\n",
      "Epoch 25, Training Loss: 0.9648736366652009\n",
      "Epoch 26, Training Loss: 0.9625043253253277\n",
      "Epoch 27, Training Loss: 0.9611046516805664\n",
      "Epoch 28, Training Loss: 0.9604906635176866\n",
      "Epoch 29, Training Loss: 0.9587561714021783\n",
      "Epoch 30, Training Loss: 0.9576657158987862\n",
      "Epoch 31, Training Loss: 0.9573825926708995\n",
      "Epoch 32, Training Loss: 0.9560911499468008\n",
      "Epoch 33, Training Loss: 0.9552518997873579\n",
      "Epoch 34, Training Loss: 0.9545101160393622\n",
      "Epoch 35, Training Loss: 0.9542921121855428\n",
      "Epoch 36, Training Loss: 0.9535777100943085\n",
      "Epoch 37, Training Loss: 0.9525616480891866\n",
      "Epoch 38, Training Loss: 0.9521554131256906\n",
      "Epoch 39, Training Loss: 0.9515408855631835\n",
      "Epoch 40, Training Loss: 0.9508383426451146\n",
      "Epoch 41, Training Loss: 0.9503414967006311\n",
      "Epoch 42, Training Loss: 0.9491929329427561\n",
      "Epoch 43, Training Loss: 0.9487261517603595\n",
      "Epoch 44, Training Loss: 0.947803673260194\n",
      "Epoch 45, Training Loss: 0.9466601905069854\n",
      "Epoch 46, Training Loss: 0.945821956405066\n",
      "Epoch 47, Training Loss: 0.9451831272670201\n",
      "Epoch 48, Training Loss: 0.945121681510954\n",
      "Epoch 49, Training Loss: 0.943764631371749\n",
      "Epoch 50, Training Loss: 0.9426151372436294\n",
      "Epoch 51, Training Loss: 0.9420346868665594\n",
      "Epoch 52, Training Loss: 0.9409341282414314\n",
      "Epoch 53, Training Loss: 0.9396671121281789\n",
      "Epoch 54, Training Loss: 0.93863133245841\n",
      "Epoch 55, Training Loss: 0.9375565262665426\n",
      "Epoch 56, Training Loss: 0.9367926731145472\n",
      "Epoch 57, Training Loss: 0.9351454080495619\n",
      "Epoch 58, Training Loss: 0.9340598151199799\n",
      "Epoch 59, Training Loss: 0.9334850920770401\n",
      "Epoch 60, Training Loss: 0.9322324189028346\n",
      "Epoch 61, Training Loss: 0.9303282056535993\n",
      "Epoch 62, Training Loss: 0.9292300859788307\n",
      "Epoch 63, Training Loss: 0.927356791675539\n",
      "Epoch 64, Training Loss: 0.9265003870304366\n",
      "Epoch 65, Training Loss: 0.9247009683372382\n",
      "Epoch 66, Training Loss: 0.9229278235507191\n",
      "Epoch 67, Training Loss: 0.9211587873616613\n",
      "Epoch 68, Training Loss: 0.9197848729621199\n",
      "Epoch 69, Training Loss: 0.9173222311457297\n",
      "Epoch 70, Training Loss: 0.9155775318468423\n",
      "Epoch 71, Training Loss: 0.9135677386943559\n",
      "Epoch 72, Training Loss: 0.9115802652853772\n",
      "Epoch 73, Training Loss: 0.9094893627596977\n",
      "Epoch 74, Training Loss: 0.9074165671391594\n",
      "Epoch 75, Training Loss: 0.9047478174804745\n",
      "Epoch 76, Training Loss: 0.902291763187351\n",
      "Epoch 77, Training Loss: 0.900230238043276\n",
      "Epoch 78, Training Loss: 0.8975672859894602\n",
      "Epoch 79, Training Loss: 0.8947935193104851\n",
      "Epoch 80, Training Loss: 0.8921930114129433\n",
      "Epoch 81, Training Loss: 0.8895581923929372\n",
      "Epoch 82, Training Loss: 0.8866244240810996\n",
      "Epoch 83, Training Loss: 0.884578548069287\n",
      "Epoch 84, Training Loss: 0.8813410278549768\n",
      "Epoch 85, Training Loss: 0.8787089234007929\n",
      "Epoch 86, Training Loss: 0.875834550176348\n",
      "Epoch 87, Training Loss: 0.8737846655056889\n",
      "Epoch 88, Training Loss: 0.8710130258610375\n",
      "Epoch 89, Training Loss: 0.8682946716932426\n",
      "Epoch 90, Training Loss: 0.8652655722503375\n",
      "Epoch 91, Training Loss: 0.8630903338131152\n",
      "Epoch 92, Training Loss: 0.8602892307410562\n",
      "Epoch 93, Training Loss: 0.8585732390109758\n",
      "Epoch 94, Training Loss: 0.8558211277302047\n",
      "Epoch 95, Training Loss: 0.8536305834476213\n",
      "Epoch 96, Training Loss: 0.8518354383626379\n",
      "Epoch 97, Training Loss: 0.8494622601602311\n",
      "Epoch 98, Training Loss: 0.8474489479136647\n",
      "Epoch 99, Training Loss: 0.8459404889802288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:55:12,076] Trial 171 finished with value: 0.6066 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8439830844563649\n",
      "Epoch 1, Training Loss: 0.8406941175460816\n",
      "Epoch 2, Training Loss: 0.8178585936252336\n",
      "Epoch 3, Training Loss: 0.8125685221270511\n",
      "Epoch 4, Training Loss: 0.8097175173293379\n",
      "Epoch 5, Training Loss: 0.8101633276258197\n",
      "Epoch 6, Training Loss: 0.8089012321672941\n",
      "Epoch 7, Training Loss: 0.8081258574822792\n",
      "Epoch 8, Training Loss: 0.8055815130248105\n",
      "Epoch 9, Training Loss: 0.8067776325053738\n",
      "Epoch 10, Training Loss: 0.8060450689685076\n",
      "Epoch 11, Training Loss: 0.8039716394324051\n",
      "Epoch 12, Training Loss: 0.8045905581094268\n",
      "Epoch 13, Training Loss: 0.803823426970862\n",
      "Epoch 14, Training Loss: 0.804311450173084\n",
      "Epoch 15, Training Loss: 0.8043702911613579\n",
      "Epoch 16, Training Loss: 0.8031397815933801\n",
      "Epoch 17, Training Loss: 0.8042046079958292\n",
      "Epoch 18, Training Loss: 0.8042263956894552\n",
      "Epoch 19, Training Loss: 0.8029115576493112\n",
      "Epoch 20, Training Loss: 0.8028935218215885\n",
      "Epoch 21, Training Loss: 0.8041490484897356\n",
      "Epoch 22, Training Loss: 0.802641314940345\n",
      "Epoch 23, Training Loss: 0.8027355330330985\n",
      "Epoch 24, Training Loss: 0.8027288369666364\n",
      "Epoch 25, Training Loss: 0.8022544025478506\n",
      "Epoch 26, Training Loss: 0.80161925639425\n",
      "Epoch 27, Training Loss: 0.80023123698127\n",
      "Epoch 28, Training Loss: 0.8006311205096711\n",
      "Epoch 29, Training Loss: 0.8024341286573194\n",
      "Epoch 30, Training Loss: 0.8007607599846402\n",
      "Epoch 31, Training Loss: 0.7996226329552499\n",
      "Epoch 32, Training Loss: 0.8007749195385696\n",
      "Epoch 33, Training Loss: 0.7987157220230963\n",
      "Epoch 34, Training Loss: 0.8011194891499397\n",
      "Epoch 35, Training Loss: 0.799588053567069\n",
      "Epoch 36, Training Loss: 0.7998606979398799\n",
      "Epoch 37, Training Loss: 0.8004765633353613\n",
      "Epoch 38, Training Loss: 0.7981119601350082\n",
      "Epoch 39, Training Loss: 0.7991131660633517\n",
      "Epoch 40, Training Loss: 0.7989763863104626\n",
      "Epoch 41, Training Loss: 0.7991134819231536\n",
      "Epoch 42, Training Loss: 0.7982785409554503\n",
      "Epoch 43, Training Loss: 0.8012844781230267\n",
      "Epoch 44, Training Loss: 0.7996561175898502\n",
      "Epoch 45, Training Loss: 0.7997174524723139\n",
      "Epoch 46, Training Loss: 0.7985343465231415\n",
      "Epoch 47, Training Loss: 0.7991496368458396\n",
      "Epoch 48, Training Loss: 0.7993896076553746\n",
      "Epoch 49, Training Loss: 0.7985716367126408\n",
      "Epoch 50, Training Loss: 0.7984697591093249\n",
      "Epoch 51, Training Loss: 0.7981970891020351\n",
      "Epoch 52, Training Loss: 0.7985156161444528\n",
      "Epoch 53, Training Loss: 0.7994523380035744\n",
      "Epoch 54, Training Loss: 0.7997354675056343\n",
      "Epoch 55, Training Loss: 0.8000435087913857\n",
      "Epoch 56, Training Loss: 0.7986123966095143\n",
      "Epoch 57, Training Loss: 0.7987869389971396\n",
      "Epoch 58, Training Loss: 0.7977681618884094\n",
      "Epoch 59, Training Loss: 0.7981416912007152\n",
      "Epoch 60, Training Loss: 0.7978808318761955\n",
      "Epoch 61, Training Loss: 0.7977744873305013\n",
      "Epoch 62, Training Loss: 0.7988230476702066\n",
      "Epoch 63, Training Loss: 0.7987860866955349\n",
      "Epoch 64, Training Loss: 0.7980623788403389\n",
      "Epoch 65, Training Loss: 0.7972489678770079\n",
      "Epoch 66, Training Loss: 0.7988207812596084\n",
      "Epoch 67, Training Loss: 0.7984244939976168\n",
      "Epoch 68, Training Loss: 0.7972617834134209\n",
      "Epoch 69, Training Loss: 0.7973377484127991\n",
      "Epoch 70, Training Loss: 0.797291358341848\n",
      "Epoch 71, Training Loss: 0.7977062367855158\n",
      "Epoch 72, Training Loss: 0.797823874618774\n",
      "Epoch 73, Training Loss: 0.7976087107694239\n",
      "Epoch 74, Training Loss: 0.7978757975693036\n",
      "Epoch 75, Training Loss: 0.7985291713162472\n",
      "Epoch 76, Training Loss: 0.7972518622427058\n",
      "Epoch 77, Training Loss: 0.7969041829718683\n",
      "Epoch 78, Training Loss: 0.7968742140253684\n",
      "Epoch 79, Training Loss: 0.7981897891912245\n",
      "Epoch 80, Training Loss: 0.7974304261960481\n",
      "Epoch 81, Training Loss: 0.7975752324089969\n",
      "Epoch 82, Training Loss: 0.7965898754005145\n",
      "Epoch 83, Training Loss: 0.7978841849735805\n",
      "Epoch 84, Training Loss: 0.7965712722979094\n",
      "Epoch 85, Training Loss: 0.7981300938398318\n",
      "Epoch 86, Training Loss: 0.7963374293836436\n",
      "Epoch 87, Training Loss: 0.7981523006482232\n",
      "Epoch 88, Training Loss: 0.7983926089186417\n",
      "Epoch 89, Training Loss: 0.797149292060307\n",
      "Epoch 90, Training Loss: 0.7971209770754764\n",
      "Epoch 91, Training Loss: 0.797013709330021\n",
      "Epoch 92, Training Loss: 0.7968052531543531\n",
      "Epoch 93, Training Loss: 0.7975889120783125\n",
      "Epoch 94, Training Loss: 0.7980734999018504\n",
      "Epoch 95, Training Loss: 0.7965855956077575\n",
      "Epoch 96, Training Loss: 0.7973536430444933\n",
      "Epoch 97, Training Loss: 0.7977394667783178\n",
      "Epoch 98, Training Loss: 0.7963230540877895\n",
      "Epoch 99, Training Loss: 0.7970691664774615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:56:53,769] Trial 172 finished with value: 0.6240666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.796930958543505\n",
      "Epoch 1, Training Loss: 0.9297559233536398\n",
      "Epoch 2, Training Loss: 0.8683486394416121\n",
      "Epoch 3, Training Loss: 0.83212590961528\n",
      "Epoch 4, Training Loss: 0.8189556027713575\n",
      "Epoch 5, Training Loss: 0.8142429758731584\n",
      "Epoch 6, Training Loss: 0.810675680413282\n",
      "Epoch 7, Training Loss: 0.8102239960118344\n",
      "Epoch 8, Training Loss: 0.8093802504969719\n",
      "Epoch 9, Training Loss: 0.8089084579532307\n",
      "Epoch 10, Training Loss: 0.8080172018000954\n",
      "Epoch 11, Training Loss: 0.8073620714639362\n",
      "Epoch 12, Training Loss: 0.8068070918097532\n",
      "Epoch 13, Training Loss: 0.8065547961041444\n",
      "Epoch 14, Training Loss: 0.8065680750330588\n",
      "Epoch 15, Training Loss: 0.805816122015616\n",
      "Epoch 16, Training Loss: 0.8057736007790817\n",
      "Epoch 17, Training Loss: 0.8054378641279121\n",
      "Epoch 18, Training Loss: 0.8054284053637569\n",
      "Epoch 19, Training Loss: 0.8045726429250903\n",
      "Epoch 20, Training Loss: 0.8051272262307935\n",
      "Epoch 21, Training Loss: 0.8039827066256587\n",
      "Epoch 22, Training Loss: 0.8037254951950302\n",
      "Epoch 23, Training Loss: 0.8033346962659879\n",
      "Epoch 24, Training Loss: 0.8039403073769763\n",
      "Epoch 25, Training Loss: 0.8037078107210031\n",
      "Epoch 26, Training Loss: 0.802978099288797\n",
      "Epoch 27, Training Loss: 0.8028599102694289\n",
      "Epoch 28, Training Loss: 0.8023176438826367\n",
      "Epoch 29, Training Loss: 0.8025412571161313\n",
      "Epoch 30, Training Loss: 0.8022873182045785\n",
      "Epoch 31, Training Loss: 0.8023117754692421\n",
      "Epoch 32, Training Loss: 0.801458728223815\n",
      "Epoch 33, Training Loss: 0.8017850582760976\n",
      "Epoch 34, Training Loss: 0.8012188003475504\n",
      "Epoch 35, Training Loss: 0.8014624759666902\n",
      "Epoch 36, Training Loss: 0.8008887855630172\n",
      "Epoch 37, Training Loss: 0.8014237132287563\n",
      "Epoch 38, Training Loss: 0.8005073733795854\n",
      "Epoch 39, Training Loss: 0.8005822549188943\n",
      "Epoch 40, Training Loss: 0.8008583791273877\n",
      "Epoch 41, Training Loss: 0.8005798353288407\n",
      "Epoch 42, Training Loss: 0.8010547915795692\n",
      "Epoch 43, Training Loss: 0.8000344755058002\n",
      "Epoch 44, Training Loss: 0.8000568737661032\n",
      "Epoch 45, Training Loss: 0.7997683065278189\n",
      "Epoch 46, Training Loss: 0.8003851316028968\n",
      "Epoch 47, Training Loss: 0.7993763726456721\n",
      "Epoch 48, Training Loss: 0.7996548307569403\n",
      "Epoch 49, Training Loss: 0.7990871772730261\n",
      "Epoch 50, Training Loss: 0.7989995570111096\n",
      "Epoch 51, Training Loss: 0.799731795680254\n",
      "Epoch 52, Training Loss: 0.7994110963398353\n",
      "Epoch 53, Training Loss: 0.7992641542190896\n",
      "Epoch 54, Training Loss: 0.7994165292359833\n",
      "Epoch 55, Training Loss: 0.7989317864403689\n",
      "Epoch 56, Training Loss: 0.7992134770952669\n",
      "Epoch 57, Training Loss: 0.7986475780494231\n",
      "Epoch 58, Training Loss: 0.7994525400319494\n",
      "Epoch 59, Training Loss: 0.798860838628353\n",
      "Epoch 60, Training Loss: 0.7988464503359974\n",
      "Epoch 61, Training Loss: 0.7981537831905193\n",
      "Epoch 62, Training Loss: 0.7991831093802488\n",
      "Epoch 63, Training Loss: 0.7987504958210135\n",
      "Epoch 64, Training Loss: 0.7984986579507813\n",
      "Epoch 65, Training Loss: 0.7991099690136156\n",
      "Epoch 66, Training Loss: 0.7985132910255203\n",
      "Epoch 67, Training Loss: 0.7980354094863834\n",
      "Epoch 68, Training Loss: 0.7980438660858269\n",
      "Epoch 69, Training Loss: 0.7977035085061439\n",
      "Epoch 70, Training Loss: 0.7981954136289152\n",
      "Epoch 71, Training Loss: 0.7983985978857915\n",
      "Epoch 72, Training Loss: 0.7979120180122834\n",
      "Epoch 73, Training Loss: 0.7973717457369754\n",
      "Epoch 74, Training Loss: 0.7981026099140482\n",
      "Epoch 75, Training Loss: 0.798182867792316\n",
      "Epoch 76, Training Loss: 0.7980070098002154\n",
      "Epoch 77, Training Loss: 0.798150038181391\n",
      "Epoch 78, Training Loss: 0.7973557798486007\n",
      "Epoch 79, Training Loss: 0.7985509420696058\n",
      "Epoch 80, Training Loss: 0.7973848169011281\n",
      "Epoch 81, Training Loss: 0.797606529060163\n",
      "Epoch 82, Training Loss: 0.7976389711960814\n",
      "Epoch 83, Training Loss: 0.7968781644240358\n",
      "Epoch 84, Training Loss: 0.7975067295526204\n",
      "Epoch 85, Training Loss: 0.7985003316312804\n",
      "Epoch 86, Training Loss: 0.7979142525142296\n",
      "Epoch 87, Training Loss: 0.797431052537789\n",
      "Epoch 88, Training Loss: 0.7973966085821166\n",
      "Epoch 89, Training Loss: 0.7970632122871572\n",
      "Epoch 90, Training Loss: 0.7970288982964996\n",
      "Epoch 91, Training Loss: 0.7976885064203937\n",
      "Epoch 92, Training Loss: 0.7972516687292802\n",
      "Epoch 93, Training Loss: 0.7975272587367467\n",
      "Epoch 94, Training Loss: 0.7971166331068914\n",
      "Epoch 95, Training Loss: 0.7970730144278447\n",
      "Epoch 96, Training Loss: 0.7970966824911591\n",
      "Epoch 97, Training Loss: 0.7970512356973232\n",
      "Epoch 98, Training Loss: 0.7972410973749663\n",
      "Epoch 99, Training Loss: 0.7978290167966283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:58:29,057] Trial 173 finished with value: 0.6246666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7968978839709346\n",
      "Epoch 1, Training Loss: 1.0844816419415009\n",
      "Epoch 2, Training Loss: 1.0422918187944512\n",
      "Epoch 3, Training Loss: 1.0203614592552186\n",
      "Epoch 4, Training Loss: 1.0042596142094835\n",
      "Epoch 5, Training Loss: 0.991969423007248\n",
      "Epoch 6, Training Loss: 0.9827844327553771\n",
      "Epoch 7, Training Loss: 0.975725643796132\n",
      "Epoch 8, Training Loss: 0.9697078544394414\n",
      "Epoch 9, Training Loss: 0.9648011731025868\n",
      "Epoch 10, Training Loss: 0.9617847229305067\n",
      "Epoch 11, Training Loss: 0.9584297610404796\n",
      "Epoch 12, Training Loss: 0.9558762573658075\n",
      "Epoch 13, Training Loss: 0.9536719072133975\n",
      "Epoch 14, Training Loss: 0.9521184187186392\n",
      "Epoch 15, Training Loss: 0.950875076673981\n",
      "Epoch 16, Training Loss: 0.9491654492858658\n",
      "Epoch 17, Training Loss: 0.9475471256370831\n",
      "Epoch 18, Training Loss: 0.9462506737027849\n",
      "Epoch 19, Training Loss: 0.9455913509641375\n",
      "Epoch 20, Training Loss: 0.9442213571161255\n",
      "Epoch 21, Training Loss: 0.9437968546286561\n",
      "Epoch 22, Training Loss: 0.9428426216419478\n",
      "Epoch 23, Training Loss: 0.9426419867608781\n",
      "Epoch 24, Training Loss: 0.941030898667816\n",
      "Epoch 25, Training Loss: 0.9401920917338895\n",
      "Epoch 26, Training Loss: 0.9398124502117472\n",
      "Epoch 27, Training Loss: 0.9393844776583794\n",
      "Epoch 28, Training Loss: 0.9387451713246511\n",
      "Epoch 29, Training Loss: 0.9379410970479922\n",
      "Epoch 30, Training Loss: 0.9373231394846636\n",
      "Epoch 31, Training Loss: 0.9366515354106301\n",
      "Epoch 32, Training Loss: 0.9358349256945732\n",
      "Epoch 33, Training Loss: 0.9356104700188888\n",
      "Epoch 34, Training Loss: 0.9347747451380679\n",
      "Epoch 35, Training Loss: 0.9347101284148998\n",
      "Epoch 36, Training Loss: 0.9344778260790316\n",
      "Epoch 37, Training Loss: 0.9334710777254033\n",
      "Epoch 38, Training Loss: 0.9332265264109562\n",
      "Epoch 39, Training Loss: 0.932527760814007\n",
      "Epoch 40, Training Loss: 0.9318358844384215\n",
      "Epoch 41, Training Loss: 0.9310072159408627\n",
      "Epoch 42, Training Loss: 0.9305028563155268\n",
      "Epoch 43, Training Loss: 0.9306357606013018\n",
      "Epoch 44, Training Loss: 0.9301105674944425\n",
      "Epoch 45, Training Loss: 0.9292845074395488\n",
      "Epoch 46, Training Loss: 0.929060112981868\n",
      "Epoch 47, Training Loss: 0.9283969569026975\n",
      "Epoch 48, Training Loss: 0.9274681124472081\n",
      "Epoch 49, Training Loss: 0.9275175675413663\n",
      "Epoch 50, Training Loss: 0.9271594319128452\n",
      "Epoch 51, Training Loss: 0.926875406638124\n",
      "Epoch 52, Training Loss: 0.9262887411547783\n",
      "Epoch 53, Training Loss: 0.9257627687059847\n",
      "Epoch 54, Training Loss: 0.9254617438280492\n",
      "Epoch 55, Training Loss: 0.9246899894305638\n",
      "Epoch 56, Training Loss: 0.9244838602560803\n",
      "Epoch 57, Training Loss: 0.9241395594482135\n",
      "Epoch 58, Training Loss: 0.9238022068389377\n",
      "Epoch 59, Training Loss: 0.9229638376630338\n",
      "Epoch 60, Training Loss: 0.9231245425410737\n",
      "Epoch 61, Training Loss: 0.9221051687584784\n",
      "Epoch 62, Training Loss: 0.9219681354393636\n",
      "Epoch 63, Training Loss: 0.9209989226850351\n",
      "Epoch 64, Training Loss: 0.9208906179980227\n",
      "Epoch 65, Training Loss: 0.9206765839928075\n",
      "Epoch 66, Training Loss: 0.9196837493351527\n",
      "Epoch 67, Training Loss: 0.9195939476328685\n",
      "Epoch 68, Training Loss: 0.9188810780532378\n",
      "Epoch 69, Training Loss: 0.9190489448102793\n",
      "Epoch 70, Training Loss: 0.9181909216974015\n",
      "Epoch 71, Training Loss: 0.9176233853612628\n",
      "Epoch 72, Training Loss: 0.9174318993001952\n",
      "Epoch 73, Training Loss: 0.9169462776721868\n",
      "Epoch 74, Training Loss: 0.9165552611637833\n",
      "Epoch 75, Training Loss: 0.9163145503603426\n",
      "Epoch 76, Training Loss: 0.9157323908088799\n",
      "Epoch 77, Training Loss: 0.9154497544568284\n",
      "Epoch 78, Training Loss: 0.9146009961465248\n",
      "Epoch 79, Training Loss: 0.9141218293878369\n",
      "Epoch 80, Training Loss: 0.9136261704272793\n",
      "Epoch 81, Training Loss: 0.9134119710527865\n",
      "Epoch 82, Training Loss: 0.9128593160693806\n",
      "Epoch 83, Training Loss: 0.9125065965760023\n",
      "Epoch 84, Training Loss: 0.9118313306256345\n",
      "Epoch 85, Training Loss: 0.9115368677261181\n",
      "Epoch 86, Training Loss: 0.9110407978968513\n",
      "Epoch 87, Training Loss: 0.9104863171290635\n",
      "Epoch 88, Training Loss: 0.9103113990080984\n",
      "Epoch 89, Training Loss: 0.9094429640841664\n",
      "Epoch 90, Training Loss: 0.9095142885258324\n",
      "Epoch 91, Training Loss: 0.9085117308717024\n",
      "Epoch 92, Training Loss: 0.9080839045065686\n",
      "Epoch 93, Training Loss: 0.9078593734511755\n",
      "Epoch 94, Training Loss: 0.9069086004020577\n",
      "Epoch 95, Training Loss: 0.907056174600931\n",
      "Epoch 96, Training Loss: 0.9064461755573301\n",
      "Epoch 97, Training Loss: 0.9055460319483191\n",
      "Epoch 98, Training Loss: 0.9055149816032639\n",
      "Epoch 99, Training Loss: 0.9050853680847283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 13:59:52,106] Trial 174 finished with value: 0.5691333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.9044079726799986\n",
      "Epoch 1, Training Loss: 0.8809498864008968\n",
      "Epoch 2, Training Loss: 0.8256299114765081\n",
      "Epoch 3, Training Loss: 0.8186110513550895\n",
      "Epoch 4, Training Loss: 0.8146279023105937\n",
      "Epoch 5, Training Loss: 0.8133008172637538\n",
      "Epoch 6, Training Loss: 0.8100837969242182\n",
      "Epoch 7, Training Loss: 0.8098707908078244\n",
      "Epoch 8, Training Loss: 0.8071130504285483\n",
      "Epoch 9, Training Loss: 0.8071321994738472\n",
      "Epoch 10, Training Loss: 0.8059318541584158\n",
      "Epoch 11, Training Loss: 0.8045311161449977\n",
      "Epoch 12, Training Loss: 0.8049558386766821\n",
      "Epoch 13, Training Loss: 0.8030122965798342\n",
      "Epoch 14, Training Loss: 0.8038192354198685\n",
      "Epoch 15, Training Loss: 0.8030724306751911\n",
      "Epoch 16, Training Loss: 0.802393556896009\n",
      "Epoch 17, Training Loss: 0.8004843552309767\n",
      "Epoch 18, Training Loss: 0.80184082357507\n",
      "Epoch 19, Training Loss: 0.8011754696530508\n",
      "Epoch 20, Training Loss: 0.8012829902476835\n",
      "Epoch 21, Training Loss: 0.8008122097280689\n",
      "Epoch 22, Training Loss: 0.800173618918971\n",
      "Epoch 23, Training Loss: 0.7999097437786876\n",
      "Epoch 24, Training Loss: 0.79947590836905\n",
      "Epoch 25, Training Loss: 0.7997194600284547\n",
      "Epoch 26, Training Loss: 0.8018010801838753\n",
      "Epoch 27, Training Loss: 0.7990009683415406\n",
      "Epoch 28, Training Loss: 0.7986025306515228\n",
      "Epoch 29, Training Loss: 0.7994489742400951\n",
      "Epoch 30, Training Loss: 0.7994304090514219\n",
      "Epoch 31, Training Loss: 0.797660291732702\n",
      "Epoch 32, Training Loss: 0.7978916999092676\n",
      "Epoch 33, Training Loss: 0.7974468392537053\n",
      "Epoch 34, Training Loss: 0.7983316768380933\n",
      "Epoch 35, Training Loss: 0.7977576343636764\n",
      "Epoch 36, Training Loss: 0.7991657997432507\n",
      "Epoch 37, Training Loss: 0.7965947009567032\n",
      "Epoch 38, Training Loss: 0.7984813606828676\n",
      "Epoch 39, Training Loss: 0.7971053051769286\n",
      "Epoch 40, Training Loss: 0.7977097877882477\n",
      "Epoch 41, Training Loss: 0.7983469531948405\n",
      "Epoch 42, Training Loss: 0.7974930601012438\n",
      "Epoch 43, Training Loss: 0.796913098482261\n",
      "Epoch 44, Training Loss: 0.795610132880677\n",
      "Epoch 45, Training Loss: 0.7962557172416744\n",
      "Epoch 46, Training Loss: 0.7965059182697669\n",
      "Epoch 47, Training Loss: 0.7979209517177782\n",
      "Epoch 48, Training Loss: 0.7966320291497654\n",
      "Epoch 49, Training Loss: 0.7980825307673978\n",
      "Epoch 50, Training Loss: 0.7977189413586954\n",
      "Epoch 51, Training Loss: 0.7960961516638447\n",
      "Epoch 52, Training Loss: 0.7962498299161295\n",
      "Epoch 53, Training Loss: 0.7953070760669565\n",
      "Epoch 54, Training Loss: 0.7968383942331586\n",
      "Epoch 55, Training Loss: 0.7968195352339207\n",
      "Epoch 56, Training Loss: 0.7968461929407334\n",
      "Epoch 57, Training Loss: 0.797826189564583\n",
      "Epoch 58, Training Loss: 0.7966098293325955\n",
      "Epoch 59, Training Loss: 0.7959657415411526\n",
      "Epoch 60, Training Loss: 0.7958140140189264\n",
      "Epoch 61, Training Loss: 0.7965372200299027\n",
      "Epoch 62, Training Loss: 0.7961797168380336\n",
      "Epoch 63, Training Loss: 0.7958017102757791\n",
      "Epoch 64, Training Loss: 0.7954929017930998\n",
      "Epoch 65, Training Loss: 0.796519592740482\n",
      "Epoch 66, Training Loss: 0.7960942532783164\n",
      "Epoch 67, Training Loss: 0.795403069750707\n",
      "Epoch 68, Training Loss: 0.795617339306308\n",
      "Epoch 69, Training Loss: 0.7955788457304015\n",
      "Epoch 70, Training Loss: 0.7954113506733027\n",
      "Epoch 71, Training Loss: 0.7964665543764158\n",
      "Epoch 72, Training Loss: 0.7954718197198739\n",
      "Epoch 73, Training Loss: 0.7960062580001085\n",
      "Epoch 74, Training Loss: 0.7950674489924782\n",
      "Epoch 75, Training Loss: 0.7957714384659789\n",
      "Epoch 76, Training Loss: 0.7962720205909327\n",
      "Epoch 77, Training Loss: 0.7947632255410789\n",
      "Epoch 78, Training Loss: 0.795626547730955\n",
      "Epoch 79, Training Loss: 0.7950768007371659\n",
      "Epoch 80, Training Loss: 0.7950729408658537\n",
      "Epoch 81, Training Loss: 0.7962229625623028\n",
      "Epoch 82, Training Loss: 0.7950107087766317\n",
      "Epoch 83, Training Loss: 0.7949265896825862\n",
      "Epoch 84, Training Loss: 0.7951322618283724\n",
      "Epoch 85, Training Loss: 0.794855604091085\n",
      "Epoch 86, Training Loss: 0.795073181794102\n",
      "Epoch 87, Training Loss: 0.7955534704645774\n",
      "Epoch 88, Training Loss: 0.7948567428086933\n",
      "Epoch 89, Training Loss: 0.7954086938298734\n",
      "Epoch 90, Training Loss: 0.795263333517806\n",
      "Epoch 91, Training Loss: 0.7954585121986562\n",
      "Epoch 92, Training Loss: 0.7954235202387759\n",
      "Epoch 93, Training Loss: 0.7945143163652348\n",
      "Epoch 94, Training Loss: 0.7949257756534376\n",
      "Epoch 95, Training Loss: 0.7952069465379069\n",
      "Epoch 96, Training Loss: 0.7962625283943979\n",
      "Epoch 97, Training Loss: 0.7949970582374056\n",
      "Epoch 98, Training Loss: 0.7951557600408569\n",
      "Epoch 99, Training Loss: 0.7944015616761114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:01:43,315] Trial 175 finished with value: 0.6254666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7950720441072507\n",
      "Epoch 1, Training Loss: 1.0403164466520898\n",
      "Epoch 2, Training Loss: 0.960313672409918\n",
      "Epoch 3, Training Loss: 0.9355547056162268\n",
      "Epoch 4, Training Loss: 0.9275583193714457\n",
      "Epoch 5, Training Loss: 0.9220367623451061\n",
      "Epoch 6, Training Loss: 0.9160492344010145\n",
      "Epoch 7, Training Loss: 0.9096280656362835\n",
      "Epoch 8, Training Loss: 0.9028383074846483\n",
      "Epoch 9, Training Loss: 0.894827764016345\n",
      "Epoch 10, Training Loss: 0.8858707358066301\n",
      "Epoch 11, Training Loss: 0.8741002340962116\n",
      "Epoch 12, Training Loss: 0.8617187797575069\n",
      "Epoch 13, Training Loss: 0.8480531325913909\n",
      "Epoch 14, Training Loss: 0.8350614817518937\n",
      "Epoch 15, Training Loss: 0.8244332200602481\n",
      "Epoch 16, Training Loss: 0.8172483971244411\n",
      "Epoch 17, Training Loss: 0.8121418765613011\n",
      "Epoch 18, Training Loss: 0.8090539166801854\n",
      "Epoch 19, Training Loss: 0.8072036951108086\n",
      "Epoch 20, Training Loss: 0.8057425380649423\n",
      "Epoch 21, Training Loss: 0.804879187193132\n",
      "Epoch 22, Training Loss: 0.8042757788098844\n",
      "Epoch 23, Training Loss: 0.8033399393683985\n",
      "Epoch 24, Training Loss: 0.8026047470874356\n",
      "Epoch 25, Training Loss: 0.8022482456121229\n",
      "Epoch 26, Training Loss: 0.8018603107086698\n",
      "Epoch 27, Training Loss: 0.8011156529412233\n",
      "Epoch 28, Training Loss: 0.8005662086314724\n",
      "Epoch 29, Training Loss: 0.7999902400755344\n",
      "Epoch 30, Training Loss: 0.8001691880082725\n",
      "Epoch 31, Training Loss: 0.7999564257779516\n",
      "Epoch 32, Training Loss: 0.7991442753856344\n",
      "Epoch 33, Training Loss: 0.7982818099789153\n",
      "Epoch 34, Training Loss: 0.7983994033103599\n",
      "Epoch 35, Training Loss: 0.7980498139123271\n",
      "Epoch 36, Training Loss: 0.7977175924114714\n",
      "Epoch 37, Training Loss: 0.7980531912997253\n",
      "Epoch 38, Training Loss: 0.7969765118190221\n",
      "Epoch 39, Training Loss: 0.7963397160956734\n",
      "Epoch 40, Training Loss: 0.7965093447749776\n",
      "Epoch 41, Training Loss: 0.7969470688274929\n",
      "Epoch 42, Training Loss: 0.796342675040539\n",
      "Epoch 43, Training Loss: 0.7965017761502947\n",
      "Epoch 44, Training Loss: 0.7957959121331236\n",
      "Epoch 45, Training Loss: 0.7959809664496802\n",
      "Epoch 46, Training Loss: 0.7957150188603795\n",
      "Epoch 47, Training Loss: 0.7953901725604122\n",
      "Epoch 48, Training Loss: 0.7951957669473232\n",
      "Epoch 49, Training Loss: 0.7951322603046446\n",
      "Epoch 50, Training Loss: 0.7947494280965705\n",
      "Epoch 51, Training Loss: 0.7952814472349067\n",
      "Epoch 52, Training Loss: 0.7949355206991496\n",
      "Epoch 53, Training Loss: 0.7944548436573573\n",
      "Epoch 54, Training Loss: 0.7948824826039766\n",
      "Epoch 55, Training Loss: 0.7941925884189462\n",
      "Epoch 56, Training Loss: 0.7941642953040905\n",
      "Epoch 57, Training Loss: 0.7938249224110654\n",
      "Epoch 58, Training Loss: 0.7937720396464929\n",
      "Epoch 59, Training Loss: 0.7940071108645963\n",
      "Epoch 60, Training Loss: 0.7933565769876753\n",
      "Epoch 61, Training Loss: 0.7940396925560513\n",
      "Epoch 62, Training Loss: 0.7929727670841648\n",
      "Epoch 63, Training Loss: 0.7928871135066327\n",
      "Epoch 64, Training Loss: 0.7929188453165212\n",
      "Epoch 65, Training Loss: 0.7935251264643849\n",
      "Epoch 66, Training Loss: 0.7930718086715928\n",
      "Epoch 67, Training Loss: 0.7923077190729012\n",
      "Epoch 68, Training Loss: 0.7936018704471731\n",
      "Epoch 69, Training Loss: 0.7926783044535415\n",
      "Epoch 70, Training Loss: 0.7923730123311954\n",
      "Epoch 71, Training Loss: 0.7921826072205278\n",
      "Epoch 72, Training Loss: 0.7922865340584203\n",
      "Epoch 73, Training Loss: 0.7920986379895891\n",
      "Epoch 74, Training Loss: 0.791914695366881\n",
      "Epoch 75, Training Loss: 0.7916117180558971\n",
      "Epoch 76, Training Loss: 0.7915892402032264\n",
      "Epoch 77, Training Loss: 0.7915866658203584\n",
      "Epoch 78, Training Loss: 0.7911273962573001\n",
      "Epoch 79, Training Loss: 0.7914372585769883\n",
      "Epoch 80, Training Loss: 0.7915982811970819\n",
      "Epoch 81, Training Loss: 0.7907259223156405\n",
      "Epoch 82, Training Loss: 0.7906790561245797\n",
      "Epoch 83, Training Loss: 0.7909529514778826\n",
      "Epoch 84, Training Loss: 0.791770381049106\n",
      "Epoch 85, Training Loss: 0.790735451977952\n",
      "Epoch 86, Training Loss: 0.7904490257564344\n",
      "Epoch 87, Training Loss: 0.7901395143422866\n",
      "Epoch 88, Training Loss: 0.7905363384942363\n",
      "Epoch 89, Training Loss: 0.7899688404305537\n",
      "Epoch 90, Training Loss: 0.7904026772742881\n",
      "Epoch 91, Training Loss: 0.789490388748341\n",
      "Epoch 92, Training Loss: 0.7894270602921795\n",
      "Epoch 93, Training Loss: 0.7896211900209126\n",
      "Epoch 94, Training Loss: 0.7893774896636045\n",
      "Epoch 95, Training Loss: 0.7893611442773862\n",
      "Epoch 96, Training Loss: 0.7902585082484367\n",
      "Epoch 97, Training Loss: 0.7894442212312741\n",
      "Epoch 98, Training Loss: 0.789094863170968\n",
      "Epoch 99, Training Loss: 0.7884585495281936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:03:15,883] Trial 176 finished with value: 0.6367333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7882350684108591\n",
      "Epoch 1, Training Loss: 1.0590057258044971\n",
      "Epoch 2, Training Loss: 0.9967188740477843\n",
      "Epoch 3, Training Loss: 0.9699086608606227\n",
      "Epoch 4, Training Loss: 0.9607164119271671\n",
      "Epoch 5, Training Loss: 0.9567640578045564\n",
      "Epoch 6, Training Loss: 0.9540195082215702\n",
      "Epoch 7, Training Loss: 0.9515404230706832\n",
      "Epoch 8, Training Loss: 0.9492960507028243\n",
      "Epoch 9, Training Loss: 0.94667256453458\n",
      "Epoch 10, Training Loss: 0.944066113023197\n",
      "Epoch 11, Training Loss: 0.9411344089227565\n",
      "Epoch 12, Training Loss: 0.9383908728291007\n",
      "Epoch 13, Training Loss: 0.935422537467059\n",
      "Epoch 14, Training Loss: 0.9322020879212548\n",
      "Epoch 15, Training Loss: 0.9288992594270146\n",
      "Epoch 16, Training Loss: 0.9254296938110801\n",
      "Epoch 17, Training Loss: 0.9217253154165604\n",
      "Epoch 18, Training Loss: 0.9179013524335974\n",
      "Epoch 19, Training Loss: 0.9139087571817286\n",
      "Epoch 20, Training Loss: 0.9097618456447826\n",
      "Epoch 21, Training Loss: 0.9056162067721871\n",
      "Epoch 22, Training Loss: 0.9013897698065814\n",
      "Epoch 23, Training Loss: 0.8968873718906851\n",
      "Epoch 24, Training Loss: 0.8924303714668049\n",
      "Epoch 25, Training Loss: 0.8880323910713196\n",
      "Epoch 26, Training Loss: 0.8836720783570233\n",
      "Epoch 27, Training Loss: 0.8792901978773229\n",
      "Epoch 28, Training Loss: 0.8749972830099219\n",
      "Epoch 29, Training Loss: 0.8708030664920807\n",
      "Epoch 30, Training Loss: 0.8666465387624853\n",
      "Epoch 31, Training Loss: 0.8627865557109609\n",
      "Epoch 32, Training Loss: 0.8588469278812408\n",
      "Epoch 33, Training Loss: 0.8550701398709241\n",
      "Epoch 34, Training Loss: 0.8515078813188216\n",
      "Epoch 35, Training Loss: 0.8481345110781052\n",
      "Epoch 36, Training Loss: 0.8449185456949122\n",
      "Epoch 37, Training Loss: 0.8420234966278076\n",
      "Epoch 38, Training Loss: 0.8391821922975428\n",
      "Epoch 39, Training Loss: 0.8364696389787337\n",
      "Epoch 40, Training Loss: 0.8340242058389327\n",
      "Epoch 41, Training Loss: 0.8317843482774847\n",
      "Epoch 42, Training Loss: 0.8295069580218372\n",
      "Epoch 43, Training Loss: 0.8277923182880177\n",
      "Epoch 44, Training Loss: 0.8259748856460347\n",
      "Epoch 45, Training Loss: 0.8244720917589524\n",
      "Epoch 46, Training Loss: 0.8230791217439315\n",
      "Epoch 47, Training Loss: 0.8217715414131389\n",
      "Epoch 48, Training Loss: 0.8206796777949613\n",
      "Epoch 49, Training Loss: 0.8197240221500397\n",
      "Epoch 50, Training Loss: 0.8188152123900021\n",
      "Epoch 51, Training Loss: 0.8180216101337882\n",
      "Epoch 52, Training Loss: 0.8173556216324077\n",
      "Epoch 53, Training Loss: 0.816690139910754\n",
      "Epoch 54, Training Loss: 0.8160601139068604\n",
      "Epoch 55, Training Loss: 0.8153252950135399\n",
      "Epoch 56, Training Loss: 0.8150394669701071\n",
      "Epoch 57, Training Loss: 0.8147549700035769\n",
      "Epoch 58, Training Loss: 0.8143627084002776\n",
      "Epoch 59, Training Loss: 0.8138650632605833\n",
      "Epoch 60, Training Loss: 0.8135570656552035\n",
      "Epoch 61, Training Loss: 0.8132391292908613\n",
      "Epoch 62, Training Loss: 0.8129304739306955\n",
      "Epoch 63, Training Loss: 0.8127933468538172\n",
      "Epoch 64, Training Loss: 0.8125641666440403\n",
      "Epoch 65, Training Loss: 0.8122598098306095\n",
      "Epoch 66, Training Loss: 0.8120776414871216\n",
      "Epoch 67, Training Loss: 0.8119766215717091\n",
      "Epoch 68, Training Loss: 0.8118077349662781\n",
      "Epoch 69, Training Loss: 0.8115120166890761\n",
      "Epoch 70, Training Loss: 0.8113333812881919\n",
      "Epoch 71, Training Loss: 0.8112338991726146\n",
      "Epoch 72, Training Loss: 0.811125503988827\n",
      "Epoch 73, Training Loss: 0.8109620338327744\n",
      "Epoch 74, Training Loss: 0.8108658259055194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:04:25,387] Trial 177 finished with value: 0.6283333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8105721673544716\n",
      "Epoch 1, Training Loss: 0.9055658075146209\n",
      "Epoch 2, Training Loss: 0.8170106834038756\n",
      "Epoch 3, Training Loss: 0.8047873266657493\n",
      "Epoch 4, Training Loss: 0.8008809273404286\n",
      "Epoch 5, Training Loss: 0.8000414291718849\n",
      "Epoch 6, Training Loss: 0.7982987558035026\n",
      "Epoch 7, Training Loss: 0.7973300214996911\n",
      "Epoch 8, Training Loss: 0.7956505852534359\n",
      "Epoch 9, Training Loss: 0.7942719088461166\n",
      "Epoch 10, Training Loss: 0.7928053354858455\n",
      "Epoch 11, Training Loss: 0.7922804781368801\n",
      "Epoch 12, Training Loss: 0.7910987516991178\n",
      "Epoch 13, Training Loss: 0.7898224395916874\n",
      "Epoch 14, Training Loss: 0.7888845615369037\n",
      "Epoch 15, Training Loss: 0.7885627646195261\n",
      "Epoch 16, Training Loss: 0.7887244651192113\n",
      "Epoch 17, Training Loss: 0.7879242843255064\n",
      "Epoch 18, Training Loss: 0.7878011125370972\n",
      "Epoch 19, Training Loss: 0.7872935348883607\n",
      "Epoch 20, Training Loss: 0.7872488286262168\n",
      "Epoch 21, Training Loss: 0.7854708879065693\n",
      "Epoch 22, Training Loss: 0.7866099519837172\n",
      "Epoch 23, Training Loss: 0.7861409542255832\n",
      "Epoch 24, Training Loss: 0.7849921702442313\n",
      "Epoch 25, Training Loss: 0.7854514686684859\n",
      "Epoch 26, Training Loss: 0.7860592285493263\n",
      "Epoch 27, Training Loss: 0.785422980875001\n",
      "Epoch 28, Training Loss: 0.7844096160472784\n",
      "Epoch 29, Training Loss: 0.7847766372494231\n",
      "Epoch 30, Training Loss: 0.7836625254691991\n",
      "Epoch 31, Training Loss: 0.7831557002282681\n",
      "Epoch 32, Training Loss: 0.783060768313874\n",
      "Epoch 33, Training Loss: 0.7841404632518166\n",
      "Epoch 34, Training Loss: 0.7838556403504279\n",
      "Epoch 35, Training Loss: 0.7824649275245523\n",
      "Epoch 36, Training Loss: 0.7835248226509954\n",
      "Epoch 37, Training Loss: 0.7830897575034235\n",
      "Epoch 38, Training Loss: 0.7827573231288365\n",
      "Epoch 39, Training Loss: 0.7822643431505762\n",
      "Epoch 40, Training Loss: 0.7829691218254261\n",
      "Epoch 41, Training Loss: 0.7832566900360853\n",
      "Epoch 42, Training Loss: 0.7819933591032386\n",
      "Epoch 43, Training Loss: 0.7813946551846382\n",
      "Epoch 44, Training Loss: 0.7824220819580824\n",
      "Epoch 45, Training Loss: 0.780702685503135\n",
      "Epoch 46, Training Loss: 0.7811545708125696\n",
      "Epoch 47, Training Loss: 0.7810576645055212\n",
      "Epoch 48, Training Loss: 0.7808213775319265\n",
      "Epoch 49, Training Loss: 0.7809412997468074\n",
      "Epoch 50, Training Loss: 0.7812108912862333\n",
      "Epoch 51, Training Loss: 0.7802577097613113\n",
      "Epoch 52, Training Loss: 0.7805968843008343\n",
      "Epoch 53, Training Loss: 0.7802613907290581\n",
      "Epoch 54, Training Loss: 0.7803901197318744\n",
      "Epoch 55, Training Loss: 0.7806525729652635\n",
      "Epoch 56, Training Loss: 0.7797675778991298\n",
      "Epoch 57, Training Loss: 0.7801121720694062\n",
      "Epoch 58, Training Loss: 0.7799118698091435\n",
      "Epoch 59, Training Loss: 0.779618517617534\n",
      "Epoch 60, Training Loss: 0.7798018597122421\n",
      "Epoch 61, Training Loss: 0.7800303564932113\n",
      "Epoch 62, Training Loss: 0.7795653985855274\n",
      "Epoch 63, Training Loss: 0.7793603698113807\n",
      "Epoch 64, Training Loss: 0.7791282477235435\n",
      "Epoch 65, Training Loss: 0.7794056402113204\n",
      "Epoch 66, Training Loss: 0.7793558639691288\n",
      "Epoch 67, Training Loss: 0.7792446650956807\n",
      "Epoch 68, Training Loss: 0.7788494573499923\n",
      "Epoch 69, Training Loss: 0.7789270543514337\n",
      "Epoch 70, Training Loss: 0.7795025334322363\n",
      "Epoch 71, Training Loss: 0.7789587518326322\n",
      "Epoch 72, Training Loss: 0.7784815896722607\n",
      "Epoch 73, Training Loss: 0.7786461181210396\n",
      "Epoch 74, Training Loss: 0.7794111057331687\n",
      "Epoch 75, Training Loss: 0.778479462279413\n",
      "Epoch 76, Training Loss: 0.7777134138838689\n",
      "Epoch 77, Training Loss: 0.7784610685549285\n",
      "Epoch 78, Training Loss: 0.7788886491517375\n",
      "Epoch 79, Training Loss: 0.7780200192802831\n",
      "Epoch 80, Training Loss: 0.7788551032095027\n",
      "Epoch 81, Training Loss: 0.7779820884080758\n",
      "Epoch 82, Training Loss: 0.7780267563977636\n",
      "Epoch 83, Training Loss: 0.7780492122011974\n",
      "Epoch 84, Training Loss: 0.7782907000161652\n",
      "Epoch 85, Training Loss: 0.7782591436142312\n",
      "Epoch 86, Training Loss: 0.7779410922437682\n",
      "Epoch 87, Training Loss: 0.7771335770313005\n",
      "Epoch 88, Training Loss: 0.778661304577849\n",
      "Epoch 89, Training Loss: 0.7781383532330506\n",
      "Epoch 90, Training Loss: 0.7781199060884634\n",
      "Epoch 91, Training Loss: 0.7775515474771199\n",
      "Epoch 92, Training Loss: 0.7771930250906407\n",
      "Epoch 93, Training Loss: 0.7780772064861499\n",
      "Epoch 94, Training Loss: 0.7780605307199004\n",
      "Epoch 95, Training Loss: 0.7784464458773906\n",
      "Epoch 96, Training Loss: 0.7777291527368072\n",
      "Epoch 97, Training Loss: 0.7778093410613841\n",
      "Epoch 98, Training Loss: 0.777524034959033\n",
      "Epoch 99, Training Loss: 0.7772314394327035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:06:22,183] Trial 178 finished with value: 0.6393333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7773927434046466\n",
      "Epoch 1, Training Loss: 0.8505390373398276\n",
      "Epoch 2, Training Loss: 0.8193872251931359\n",
      "Epoch 3, Training Loss: 0.8159647260693943\n",
      "Epoch 4, Training Loss: 0.8133532453985776\n",
      "Epoch 5, Training Loss: 0.8116340555162991\n",
      "Epoch 6, Training Loss: 0.8112941233550801\n",
      "Epoch 7, Training Loss: 0.8096975261323592\n",
      "Epoch 8, Training Loss: 0.8083283596880295\n",
      "Epoch 9, Training Loss: 0.808984144855948\n",
      "Epoch 10, Training Loss: 0.808533622938044\n",
      "Epoch 11, Training Loss: 0.806817621904261\n",
      "Epoch 12, Training Loss: 0.8072469493922065\n",
      "Epoch 13, Training Loss: 0.8067971827703364\n",
      "Epoch 14, Training Loss: 0.8059903316638048\n",
      "Epoch 15, Training Loss: 0.8058990709220661\n",
      "Epoch 16, Training Loss: 0.8053759222872117\n",
      "Epoch 17, Training Loss: 0.8056507011020885\n",
      "Epoch 18, Training Loss: 0.8057697539469775\n",
      "Epoch 19, Training Loss: 0.8043172512334936\n",
      "Epoch 20, Training Loss: 0.8051378731166615\n",
      "Epoch 21, Training Loss: 0.8043356326047112\n",
      "Epoch 22, Training Loss: 0.8041792442518122\n",
      "Epoch 23, Training Loss: 0.8056331809128032\n",
      "Epoch 24, Training Loss: 0.8054422867999357\n",
      "Epoch 25, Training Loss: 0.8047454484771279\n",
      "Epoch 26, Training Loss: 0.8048968646806829\n",
      "Epoch 27, Training Loss: 0.803256188841427\n",
      "Epoch 28, Training Loss: 0.8034283241804908\n",
      "Epoch 29, Training Loss: 0.8036623037562651\n",
      "Epoch 30, Training Loss: 0.8046193175456103\n",
      "Epoch 31, Training Loss: 0.8038381885781007\n",
      "Epoch 32, Training Loss: 0.8036652818848105\n",
      "Epoch 33, Training Loss: 0.8031907711309545\n",
      "Epoch 34, Training Loss: 0.8025407044326558\n",
      "Epoch 35, Training Loss: 0.8034025368269752\n",
      "Epoch 36, Training Loss: 0.8029848275465123\n",
      "Epoch 37, Training Loss: 0.8028641043691074\n",
      "Epoch 38, Training Loss: 0.8030940717108109\n",
      "Epoch 39, Training Loss: 0.8025040352344512\n",
      "Epoch 40, Training Loss: 0.8021244269258836\n",
      "Epoch 41, Training Loss: 0.8015905568880194\n",
      "Epoch 42, Training Loss: 0.802801967087914\n",
      "Epoch 43, Training Loss: 0.8019673890226028\n",
      "Epoch 44, Training Loss: 0.8021691607727724\n",
      "Epoch 45, Training Loss: 0.8005124674825107\n",
      "Epoch 46, Training Loss: 0.8008544633669011\n",
      "Epoch 47, Training Loss: 0.8010524238558376\n",
      "Epoch 48, Training Loss: 0.8008051717281341\n",
      "Epoch 49, Training Loss: 0.8015903222560883\n",
      "Epoch 50, Training Loss: 0.8001008807210361\n",
      "Epoch 51, Training Loss: 0.7998077802097097\n",
      "Epoch 52, Training Loss: 0.79892269078423\n",
      "Epoch 53, Training Loss: 0.8003261347378001\n",
      "Epoch 54, Training Loss: 0.7996671168944415\n",
      "Epoch 55, Training Loss: 0.7991090548739713\n",
      "Epoch 56, Training Loss: 0.7994863989773918\n",
      "Epoch 57, Training Loss: 0.7997394357709323\n",
      "Epoch 58, Training Loss: 0.7988297770303838\n",
      "Epoch 59, Training Loss: 0.7988335133300108\n",
      "Epoch 60, Training Loss: 0.7985154716407552\n",
      "Epoch 61, Training Loss: 0.7984185451619765\n",
      "Epoch 62, Training Loss: 0.7986596695815815\n",
      "Epoch 63, Training Loss: 0.7983698513227351\n",
      "Epoch 64, Training Loss: 0.7985278241774615\n",
      "Epoch 65, Training Loss: 0.799166952231351\n",
      "Epoch 66, Training Loss: 0.7984361933960634\n",
      "Epoch 67, Training Loss: 0.7979270883167491\n",
      "Epoch 68, Training Loss: 0.7975520780507256\n",
      "Epoch 69, Training Loss: 0.7980857480273528\n",
      "Epoch 70, Training Loss: 0.7979605568857754\n",
      "Epoch 71, Training Loss: 0.797264021915548\n",
      "Epoch 72, Training Loss: 0.7969635241171893\n",
      "Epoch 73, Training Loss: 0.7975709732841043\n",
      "Epoch 74, Training Loss: 0.7963815210847294\n",
      "Epoch 75, Training Loss: 0.7971054382183973\n",
      "Epoch 76, Training Loss: 0.797270520995645\n",
      "Epoch 77, Training Loss: 0.7966042913408841\n",
      "Epoch 78, Training Loss: 0.796282219395918\n",
      "Epoch 79, Training Loss: 0.7973689462858088\n",
      "Epoch 80, Training Loss: 0.79671967828975\n",
      "Epoch 81, Training Loss: 0.7960688163953669\n",
      "Epoch 82, Training Loss: 0.7959034229026122\n",
      "Epoch 83, Training Loss: 0.7950842355279362\n",
      "Epoch 84, Training Loss: 0.7963686818235061\n",
      "Epoch 85, Training Loss: 0.7960828765700845\n",
      "Epoch 86, Training Loss: 0.7960748092567219\n",
      "Epoch 87, Training Loss: 0.7956018931725446\n",
      "Epoch 88, Training Loss: 0.7952210282578188\n",
      "Epoch 89, Training Loss: 0.7959190790092244\n",
      "Epoch 90, Training Loss: 0.7946428868349861\n",
      "Epoch 91, Training Loss: 0.7960962623708389\n",
      "Epoch 92, Training Loss: 0.796096931625815\n",
      "Epoch 93, Training Loss: 0.7951549051789677\n",
      "Epoch 94, Training Loss: 0.7957927982246175\n",
      "Epoch 95, Training Loss: 0.7944808558155508\n",
      "Epoch 96, Training Loss: 0.7942013112937703\n",
      "Epoch 97, Training Loss: 0.7952722559956943\n",
      "Epoch 98, Training Loss: 0.7952655683545505\n",
      "Epoch 99, Training Loss: 0.7947676106060253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:08:19,596] Trial 179 finished with value: 0.6334 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7954592312083525\n",
      "Epoch 1, Training Loss: 1.0994020323192373\n",
      "Epoch 2, Training Loss: 1.0893552861494176\n",
      "Epoch 3, Training Loss: 1.0812149758899914\n",
      "Epoch 4, Training Loss: 1.0737420610820545\n",
      "Epoch 5, Training Loss: 1.0668021033791935\n",
      "Epoch 6, Training Loss: 1.060345830636866\n",
      "Epoch 7, Training Loss: 1.0542976822572596\n",
      "Epoch 8, Training Loss: 1.0486306207320268\n",
      "Epoch 9, Training Loss: 1.0432717507727005\n",
      "Epoch 10, Training Loss: 1.0382070660591125\n",
      "Epoch 11, Training Loss: 1.0334617461176479\n",
      "Epoch 12, Training Loss: 1.028938308323131\n",
      "Epoch 13, Training Loss: 1.0246556110943066\n",
      "Epoch 14, Training Loss: 1.0205963743434232\n",
      "Epoch 15, Training Loss: 1.0167682867891648\n",
      "Epoch 16, Training Loss: 1.013125177341349\n",
      "Epoch 17, Training Loss: 1.009707685989492\n",
      "Epoch 18, Training Loss: 1.0064559099253487\n",
      "Epoch 19, Training Loss: 1.0033819719623116\n",
      "Epoch 20, Training Loss: 1.000477193032994\n",
      "Epoch 21, Training Loss: 0.9977470997501822\n",
      "Epoch 22, Training Loss: 0.9951365108349745\n",
      "Epoch 23, Training Loss: 0.9927115814826067\n",
      "Epoch 24, Training Loss: 0.9904262940322651\n",
      "Epoch 25, Training Loss: 0.9882550470268026\n",
      "Epoch 26, Training Loss: 0.9862227578022901\n",
      "Epoch 27, Training Loss: 0.9843161984050975\n",
      "Epoch 28, Training Loss: 0.9825141826096703\n",
      "Epoch 29, Training Loss: 0.9808148329398211\n",
      "Epoch 30, Training Loss: 0.9792624183963327\n",
      "Epoch 31, Training Loss: 0.9777677313019247\n",
      "Epoch 32, Training Loss: 0.9763762906018425\n",
      "Epoch 33, Training Loss: 0.9750745054553537\n",
      "Epoch 34, Training Loss: 0.9738637060277602\n",
      "Epoch 35, Training Loss: 0.9727196641529308\n",
      "Epoch 36, Training Loss: 0.971645633543239\n",
      "Epoch 37, Training Loss: 0.9706503226476557\n",
      "Epoch 38, Training Loss: 0.9696979212059694\n",
      "Epoch 39, Training Loss: 0.9688289936149822\n",
      "Epoch 40, Training Loss: 0.9680016372484319\n",
      "Epoch 41, Training Loss: 0.9672263628595016\n",
      "Epoch 42, Training Loss: 0.9665035340365241\n",
      "Epoch 43, Training Loss: 0.9658429218741025\n",
      "Epoch 44, Training Loss: 0.9651856827034669\n",
      "Epoch 45, Training Loss: 0.9645970502320458\n",
      "Epoch 46, Training Loss: 0.9640435115730062\n",
      "Epoch 47, Training Loss: 0.9635127645380357\n",
      "Epoch 48, Training Loss: 0.9630128422204186\n",
      "Epoch 49, Training Loss: 0.9625648374417249\n",
      "Epoch 50, Training Loss: 0.9621266942865708\n",
      "Epoch 51, Training Loss: 0.9617177061473622\n",
      "Epoch 52, Training Loss: 0.9613320988066056\n",
      "Epoch 53, Training Loss: 0.9609608030319214\n",
      "Epoch 54, Training Loss: 0.9606185975495507\n",
      "Epoch 55, Training Loss: 0.9602881090080037\n",
      "Epoch 56, Training Loss: 0.9599785318795373\n",
      "Epoch 57, Training Loss: 0.9596954068716834\n",
      "Epoch 58, Training Loss: 0.959411253157784\n",
      "Epoch 59, Training Loss: 0.9591438846728381\n",
      "Epoch 60, Training Loss: 0.9588947577336255\n",
      "Epoch 61, Training Loss: 0.9586377583531772\n",
      "Epoch 62, Training Loss: 0.9584120617193335\n",
      "Epoch 63, Training Loss: 0.9581986716915579\n",
      "Epoch 64, Training Loss: 0.9579856833289652\n",
      "Epoch 65, Training Loss: 0.9577739439992343\n",
      "Epoch 66, Training Loss: 0.9575752376107609\n",
      "Epoch 67, Training Loss: 0.9573897767066956\n",
      "Epoch 68, Training Loss: 0.9572093390015994\n",
      "Epoch 69, Training Loss: 0.9570349469605615\n",
      "Epoch 70, Training Loss: 0.9568651742794935\n",
      "Epoch 71, Training Loss: 0.9567004452032202\n",
      "Epoch 72, Training Loss: 0.9565341556773467\n",
      "Epoch 73, Training Loss: 0.9563795323231641\n",
      "Epoch 74, Training Loss: 0.9562262668329127\n",
      "Epoch 75, Training Loss: 0.9560750394007739\n",
      "Epoch 76, Training Loss: 0.9559052385302151\n",
      "Epoch 77, Training Loss: 0.9557699671913595\n",
      "Epoch 78, Training Loss: 0.9556410119814032\n",
      "Epoch 79, Training Loss: 0.9555001110890332\n",
      "Epoch 80, Training Loss: 0.9553276093567119\n",
      "Epoch 81, Training Loss: 0.955216816663742\n",
      "Epoch 82, Training Loss: 0.9550830652433283\n",
      "Epoch 83, Training Loss: 0.9549713185254265\n",
      "Epoch 84, Training Loss: 0.9548354916712817\n",
      "Epoch 85, Training Loss: 0.9546929902890149\n",
      "Epoch 86, Training Loss: 0.9545906996025758\n",
      "Epoch 87, Training Loss: 0.9544376875372494\n",
      "Epoch 88, Training Loss: 0.9543335818543154\n",
      "Epoch 89, Training Loss: 0.954209359183031\n",
      "Epoch 90, Training Loss: 0.9540853707930621\n",
      "Epoch 91, Training Loss: 0.9539619219303132\n",
      "Epoch 92, Training Loss: 0.9538391236697926\n",
      "Epoch 93, Training Loss: 0.953715664989808\n",
      "Epoch 94, Training Loss: 0.9535966629841749\n",
      "Epoch 95, Training Loss: 0.9534780697261586\n",
      "Epoch 96, Training Loss: 0.9533644276506761\n",
      "Epoch 97, Training Loss: 0.9532423990614274\n",
      "Epoch 98, Training Loss: 0.9531192954147564\n",
      "Epoch 99, Training Loss: 0.9530053721455967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:09:53,023] Trial 180 finished with value: 0.5328 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.9528818582787233\n",
      "Epoch 1, Training Loss: 1.0940291431370903\n",
      "Epoch 2, Training Loss: 1.0906589049451492\n",
      "Epoch 3, Training Loss: 1.0903027189479155\n",
      "Epoch 4, Training Loss: 1.0899979967229507\n",
      "Epoch 5, Training Loss: 1.089648376492893\n",
      "Epoch 6, Training Loss: 1.0893353936251473\n",
      "Epoch 7, Training Loss: 1.088994175125571\n",
      "Epoch 8, Training Loss: 1.0886613718201132\n",
      "Epoch 9, Training Loss: 1.0883347048478968\n",
      "Epoch 10, Training Loss: 1.0879973107225756\n",
      "Epoch 11, Training Loss: 1.087656789667466\n",
      "Epoch 12, Training Loss: 1.0873134310105268\n",
      "Epoch 13, Training Loss: 1.086970386505127\n",
      "Epoch 14, Training Loss: 1.0866315022636863\n",
      "Epoch 15, Training Loss: 1.086249490625718\n",
      "Epoch 16, Training Loss: 1.0859153541396647\n",
      "Epoch 17, Training Loss: 1.0855534110349767\n",
      "Epoch 18, Training Loss: 1.085179471689112\n",
      "Epoch 19, Training Loss: 1.0848035325723535\n",
      "Epoch 20, Training Loss: 1.084433298110962\n",
      "Epoch 21, Training Loss: 1.084051524330588\n",
      "Epoch 22, Training Loss: 1.0836586142988767\n",
      "Epoch 23, Training Loss: 1.0832530258683597\n",
      "Epoch 24, Training Loss: 1.0828308550049277\n",
      "Epoch 25, Training Loss: 1.0824504382470075\n",
      "Epoch 26, Training Loss: 1.082012343266431\n",
      "Epoch 27, Training Loss: 1.0815933871269225\n",
      "Epoch 28, Training Loss: 1.0811621297107024\n",
      "Epoch 29, Training Loss: 1.080705815483542\n",
      "Epoch 30, Training Loss: 1.0802512771943036\n",
      "Epoch 31, Training Loss: 1.0797807480307187\n",
      "Epoch 32, Training Loss: 1.0793147218928618\n",
      "Epoch 33, Training Loss: 1.0788094341053682\n",
      "Epoch 34, Training Loss: 1.0783328429390402\n",
      "Epoch 35, Training Loss: 1.0778014744029325\n",
      "Epoch 36, Training Loss: 1.0772907067747677\n",
      "Epoch 37, Training Loss: 1.0767406425756567\n",
      "Epoch 38, Training Loss: 1.0762002532622394\n",
      "Epoch 39, Training Loss: 1.075639176789452\n",
      "Epoch 40, Training Loss: 1.075045064757852\n",
      "Epoch 41, Training Loss: 1.074458934138803\n",
      "Epoch 42, Training Loss: 1.0738636974727407\n",
      "Epoch 43, Training Loss: 1.0732384262365453\n",
      "Epoch 44, Training Loss: 1.072591893672943\n",
      "Epoch 45, Training Loss: 1.0719288497812607\n",
      "Epoch 46, Training Loss: 1.0712793306743398\n",
      "Epoch 47, Training Loss: 1.0705895332729116\n",
      "Epoch 48, Training Loss: 1.0698705223027398\n",
      "Epoch 49, Training Loss: 1.0691651857600493\n",
      "Epoch 50, Training Loss: 1.068411900436177\n",
      "Epoch 51, Training Loss: 1.0676633095741273\n",
      "Epoch 52, Training Loss: 1.0668911600112916\n",
      "Epoch 53, Training Loss: 1.066087656021118\n",
      "Epoch 54, Training Loss: 1.0652684343562406\n",
      "Epoch 55, Training Loss: 1.0644372724084292\n",
      "Epoch 56, Training Loss: 1.0635738044626573\n",
      "Epoch 57, Training Loss: 1.0627018136136672\n",
      "Epoch 58, Training Loss: 1.0618006783373215\n",
      "Epoch 59, Training Loss: 1.0608677730840794\n",
      "Epoch 60, Training Loss: 1.0599021942475262\n",
      "Epoch 61, Training Loss: 1.058985690649818\n",
      "Epoch 62, Training Loss: 1.0579955122050118\n",
      "Epoch 63, Training Loss: 1.0569800720495337\n",
      "Epoch 64, Training Loss: 1.0559638402041267\n",
      "Epoch 65, Training Loss: 1.054896953386419\n",
      "Epoch 66, Training Loss: 1.0538333796052373\n",
      "Epoch 67, Training Loss: 1.0527462280497832\n",
      "Epoch 68, Training Loss: 1.0516251744943506\n",
      "Epoch 69, Training Loss: 1.0505095581447377\n",
      "Epoch 70, Training Loss: 1.0493397238675286\n",
      "Epoch 71, Training Loss: 1.048166874997756\n",
      "Epoch 72, Training Loss: 1.0469751696025624\n",
      "Epoch 73, Training Loss: 1.0457657038464265\n",
      "Epoch 74, Training Loss: 1.0445257310306324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:11:14,194] Trial 181 finished with value: 0.4818 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 1.0432814264998715\n",
      "Epoch 1, Training Loss: 1.048762553795836\n",
      "Epoch 2, Training Loss: 0.9740648959812365\n",
      "Epoch 3, Training Loss: 0.9434853386161919\n",
      "Epoch 4, Training Loss: 0.9338896780085743\n",
      "Epoch 5, Training Loss: 0.9262815818750769\n",
      "Epoch 6, Training Loss: 0.9193515507798445\n",
      "Epoch 7, Training Loss: 0.9125249058680427\n",
      "Epoch 8, Training Loss: 0.9059479224950747\n",
      "Epoch 9, Training Loss: 0.8969625241774365\n",
      "Epoch 10, Training Loss: 0.8871111954065194\n",
      "Epoch 11, Training Loss: 0.87535521096753\n",
      "Epoch 12, Training Loss: 0.8612036629727012\n",
      "Epoch 13, Training Loss: 0.8468967854528499\n",
      "Epoch 14, Training Loss: 0.8337783237148945\n",
      "Epoch 15, Training Loss: 0.8239048960513639\n",
      "Epoch 16, Training Loss: 0.8170460191884436\n",
      "Epoch 17, Training Loss: 0.8121017972329505\n",
      "Epoch 18, Training Loss: 0.8091889857349539\n",
      "Epoch 19, Training Loss: 0.8071941305820207\n",
      "Epoch 20, Training Loss: 0.8065810144395756\n",
      "Epoch 21, Training Loss: 0.8054176907790335\n",
      "Epoch 22, Training Loss: 0.8051233669868986\n",
      "Epoch 23, Training Loss: 0.8041012263835821\n",
      "Epoch 24, Training Loss: 0.8030502941375388\n",
      "Epoch 25, Training Loss: 0.8023312422117792\n",
      "Epoch 26, Training Loss: 0.8030553469980569\n",
      "Epoch 27, Training Loss: 0.8017511956673816\n",
      "Epoch 28, Training Loss: 0.8007626953877901\n",
      "Epoch 29, Training Loss: 0.8008614879801758\n",
      "Epoch 30, Training Loss: 0.8003721785724611\n",
      "Epoch 31, Training Loss: 0.7992616814778264\n",
      "Epoch 32, Training Loss: 0.7996741423033233\n",
      "Epoch 33, Training Loss: 0.7986450110163007\n",
      "Epoch 34, Training Loss: 0.7982456028909611\n",
      "Epoch 35, Training Loss: 0.7977933548446885\n",
      "Epoch 36, Training Loss: 0.7979410226183726\n",
      "Epoch 37, Training Loss: 0.7974868884660248\n",
      "Epoch 38, Training Loss: 0.797750318946695\n",
      "Epoch 39, Training Loss: 0.7965467229821628\n",
      "Epoch 40, Training Loss: 0.7972621068022305\n",
      "Epoch 41, Training Loss: 0.7963071285333848\n",
      "Epoch 42, Training Loss: 0.7964351934597904\n",
      "Epoch 43, Training Loss: 0.7960364628555183\n",
      "Epoch 44, Training Loss: 0.7961096062696069\n",
      "Epoch 45, Training Loss: 0.7955471706569643\n",
      "Epoch 46, Training Loss: 0.7959153332208332\n",
      "Epoch 47, Training Loss: 0.7954656894045665\n",
      "Epoch 48, Training Loss: 0.7949917222324171\n",
      "Epoch 49, Training Loss: 0.7957192687163676\n",
      "Epoch 50, Training Loss: 0.795406120612209\n",
      "Epoch 51, Training Loss: 0.7944969133326881\n",
      "Epoch 52, Training Loss: 0.7946875814208411\n",
      "Epoch 53, Training Loss: 0.7941843169076103\n",
      "Epoch 54, Training Loss: 0.7945378568835725\n",
      "Epoch 55, Training Loss: 0.7941190406792146\n",
      "Epoch 56, Training Loss: 0.7940119856282284\n",
      "Epoch 57, Training Loss: 0.794322825822615\n",
      "Epoch 58, Training Loss: 0.7938047341834333\n",
      "Epoch 59, Training Loss: 0.7931675992513958\n",
      "Epoch 60, Training Loss: 0.7932756605901217\n",
      "Epoch 61, Training Loss: 0.7931385064483585\n",
      "Epoch 62, Training Loss: 0.7934149607679898\n",
      "Epoch 63, Training Loss: 0.7931047795410443\n",
      "Epoch 64, Training Loss: 0.7923526537149472\n",
      "Epoch 65, Training Loss: 0.7923253208174741\n",
      "Epoch 66, Training Loss: 0.7924519356032064\n",
      "Epoch 67, Training Loss: 0.7929152766564735\n",
      "Epoch 68, Training Loss: 0.7926081473666026\n",
      "Epoch 69, Training Loss: 0.7919583179000625\n",
      "Epoch 70, Training Loss: 0.7926309641142537\n",
      "Epoch 71, Training Loss: 0.7918915677787666\n",
      "Epoch 72, Training Loss: 0.7915354260824676\n",
      "Epoch 73, Training Loss: 0.7915545535266848\n",
      "Epoch 74, Training Loss: 0.7913230513271533\n",
      "Epoch 75, Training Loss: 0.791873890654485\n",
      "Epoch 76, Training Loss: 0.7915733294379442\n",
      "Epoch 77, Training Loss: 0.7914715706853939\n",
      "Epoch 78, Training Loss: 0.7911400306493717\n",
      "Epoch 79, Training Loss: 0.7907216441362424\n",
      "Epoch 80, Training Loss: 0.7904305895916501\n",
      "Epoch 81, Training Loss: 0.7911064821078365\n",
      "Epoch 82, Training Loss: 0.7913571344282394\n",
      "Epoch 83, Training Loss: 0.7909325646278553\n",
      "Epoch 84, Training Loss: 0.7907651748872341\n",
      "Epoch 85, Training Loss: 0.7900604322440642\n",
      "Epoch 86, Training Loss: 0.7909325837192679\n",
      "Epoch 87, Training Loss: 0.7903235104747285\n",
      "Epoch 88, Training Loss: 0.7910603521461773\n",
      "Epoch 89, Training Loss: 0.7894977634114431\n",
      "Epoch 90, Training Loss: 0.7902440952179127\n",
      "Epoch 91, Training Loss: 0.7898426517508084\n",
      "Epoch 92, Training Loss: 0.789770675781078\n",
      "Epoch 93, Training Loss: 0.7893798465119268\n",
      "Epoch 94, Training Loss: 0.7897012905966967\n",
      "Epoch 95, Training Loss: 0.7899732059105895\n",
      "Epoch 96, Training Loss: 0.7893188663891384\n",
      "Epoch 97, Training Loss: 0.7889066264593512\n",
      "Epoch 98, Training Loss: 0.7893001248065691\n",
      "Epoch 99, Training Loss: 0.7893925927635422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:12:46,701] Trial 182 finished with value: 0.639 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7889937367654385\n",
      "Epoch 1, Training Loss: 0.8414028136169209\n",
      "Epoch 2, Training Loss: 0.814622599587721\n",
      "Epoch 3, Training Loss: 0.8074357564309064\n",
      "Epoch 4, Training Loss: 0.8102497571356156\n",
      "Epoch 5, Training Loss: 0.8073139063049766\n",
      "Epoch 6, Training Loss: 0.8060243256653057\n",
      "Epoch 7, Training Loss: 0.8037880394739263\n",
      "Epoch 8, Training Loss: 0.8046684427822337\n",
      "Epoch 9, Training Loss: 0.8031410598053652\n",
      "Epoch 10, Training Loss: 0.799677865785711\n",
      "Epoch 11, Training Loss: 0.8017980646385866\n",
      "Epoch 12, Training Loss: 0.801214526611216\n",
      "Epoch 13, Training Loss: 0.8009800331031575\n",
      "Epoch 14, Training Loss: 0.7999612438678741\n",
      "Epoch 15, Training Loss: 0.7998002468838411\n",
      "Epoch 16, Training Loss: 0.8001247675278608\n",
      "Epoch 17, Training Loss: 0.799270298130372\n",
      "Epoch 18, Training Loss: 0.8005177328867071\n",
      "Epoch 19, Training Loss: 0.79926325377296\n",
      "Epoch 20, Training Loss: 0.7994453790608574\n",
      "Epoch 21, Training Loss: 0.7995631776837742\n",
      "Epoch 22, Training Loss: 0.7997449345448437\n",
      "Epoch 23, Training Loss: 0.7988513764213113\n",
      "Epoch 24, Training Loss: 0.799230892237495\n",
      "Epoch 25, Training Loss: 0.7992100618867313\n",
      "Epoch 26, Training Loss: 0.7998110881272484\n",
      "Epoch 27, Training Loss: 0.7988710974945742\n",
      "Epoch 28, Training Loss: 0.8004344909331378\n",
      "Epoch 29, Training Loss: 0.7982267173598795\n",
      "Epoch 30, Training Loss: 0.7992984620262594\n",
      "Epoch 31, Training Loss: 0.7973844496642842\n",
      "Epoch 32, Training Loss: 0.7973187407325296\n",
      "Epoch 33, Training Loss: 0.7985493551282322\n",
      "Epoch 34, Training Loss: 0.7971739266900455\n",
      "Epoch 35, Training Loss: 0.7975905142812167\n",
      "Epoch 36, Training Loss: 0.797976126390345\n",
      "Epoch 37, Training Loss: 0.7979276567346909\n",
      "Epoch 38, Training Loss: 0.7970816231475157\n",
      "Epoch 39, Training Loss: 0.796991815847509\n",
      "Epoch 40, Training Loss: 0.7971134916473838\n",
      "Epoch 41, Training Loss: 0.7973122290302725\n",
      "Epoch 42, Training Loss: 0.797484356024686\n",
      "Epoch 43, Training Loss: 0.7966753777335672\n",
      "Epoch 44, Training Loss: 0.7972087297720067\n",
      "Epoch 45, Training Loss: 0.7973031266296611\n",
      "Epoch 46, Training Loss: 0.7969456894958721\n",
      "Epoch 47, Training Loss: 0.7965112959637362\n",
      "Epoch 48, Training Loss: 0.795815639145234\n",
      "Epoch 49, Training Loss: 0.796203418339\n",
      "Epoch 50, Training Loss: 0.7967946525181041\n",
      "Epoch 51, Training Loss: 0.797228236619164\n",
      "Epoch 52, Training Loss: 0.7971387462054982\n",
      "Epoch 53, Training Loss: 0.7962784362540526\n",
      "Epoch 54, Training Loss: 0.7964471863297855\n",
      "Epoch 55, Training Loss: 0.7977172253412359\n",
      "Epoch 56, Training Loss: 0.797427679089939\n",
      "Epoch 57, Training Loss: 0.7966717419904821\n",
      "Epoch 58, Training Loss: 0.7973431402795454\n",
      "Epoch 59, Training Loss: 0.7975986566263087\n",
      "Epoch 60, Training Loss: 0.7972077345146852\n",
      "Epoch 61, Training Loss: 0.7964494630869697\n",
      "Epoch 62, Training Loss: 0.7967233752503114\n",
      "Epoch 63, Training Loss: 0.7967091652225046\n",
      "Epoch 64, Training Loss: 0.7971158854400411\n",
      "Epoch 65, Training Loss: 0.7953265658546896\n",
      "Epoch 66, Training Loss: 0.7967262959480286\n",
      "Epoch 67, Training Loss: 0.795816733416389\n",
      "Epoch 68, Training Loss: 0.7963528665374308\n",
      "Epoch 69, Training Loss: 0.7963785679901347\n",
      "Epoch 70, Training Loss: 0.796021304060431\n",
      "Epoch 71, Training Loss: 0.7967618499783908\n",
      "Epoch 72, Training Loss: 0.795370353530435\n",
      "Epoch 73, Training Loss: 0.7970425232718973\n",
      "Epoch 74, Training Loss: 0.7963870995886185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:14:28,766] Trial 183 finished with value: 0.6350666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7960169633697061\n",
      "Epoch 1, Training Loss: 1.0388108200596688\n",
      "Epoch 2, Training Loss: 0.9897002925550131\n",
      "Epoch 3, Training Loss: 0.9684703284636476\n",
      "Epoch 4, Training Loss: 0.960051961680104\n",
      "Epoch 5, Training Loss: 0.9562538328027367\n",
      "Epoch 6, Training Loss: 0.954518068822703\n",
      "Epoch 7, Training Loss: 0.9527954016413007\n",
      "Epoch 8, Training Loss: 0.950875345029329\n",
      "Epoch 9, Training Loss: 0.9491950068258701\n",
      "Epoch 10, Training Loss: 0.9469808305116524\n",
      "Epoch 11, Training Loss: 0.9452902329595465\n",
      "Epoch 12, Training Loss: 0.9434470528050473\n",
      "Epoch 13, Training Loss: 0.9407071494518366\n",
      "Epoch 14, Training Loss: 0.9393133091747312\n",
      "Epoch 15, Training Loss: 0.9371950499993518\n",
      "Epoch 16, Training Loss: 0.9341916209773014\n",
      "Epoch 17, Training Loss: 0.9320833135368233\n",
      "Epoch 18, Training Loss: 0.9292466259540472\n",
      "Epoch 19, Training Loss: 0.9270441225596837\n",
      "Epoch 20, Training Loss: 0.9241469678125883\n",
      "Epoch 21, Training Loss: 0.9209467677245463\n",
      "Epoch 22, Training Loss: 0.9180090595008735\n",
      "Epoch 23, Training Loss: 0.9153157167864922\n",
      "Epoch 24, Training Loss: 0.9114066809639895\n",
      "Epoch 25, Training Loss: 0.9079517015837189\n",
      "Epoch 26, Training Loss: 0.9045314805848258\n",
      "Epoch 27, Training Loss: 0.9010819244205504\n",
      "Epoch 28, Training Loss: 0.8979009960827075\n",
      "Epoch 29, Training Loss: 0.8937477310797326\n",
      "Epoch 30, Training Loss: 0.8896885003362384\n",
      "Epoch 31, Training Loss: 0.8858657366351077\n",
      "Epoch 32, Training Loss: 0.882036833117779\n",
      "Epoch 33, Training Loss: 0.8781495616848307\n",
      "Epoch 34, Training Loss: 0.8740188144203416\n",
      "Epoch 35, Training Loss: 0.8704262277237454\n",
      "Epoch 36, Training Loss: 0.8673324885224938\n",
      "Epoch 37, Training Loss: 0.8632407472545939\n",
      "Epoch 38, Training Loss: 0.860025583891044\n",
      "Epoch 39, Training Loss: 0.8564517536557706\n",
      "Epoch 40, Training Loss: 0.8532500453461381\n",
      "Epoch 41, Training Loss: 0.8501248211788952\n",
      "Epoch 42, Training Loss: 0.8473901875037\n",
      "Epoch 43, Training Loss: 0.8446040879514881\n",
      "Epoch 44, Training Loss: 0.8416426854922359\n",
      "Epoch 45, Training Loss: 0.8389094412774968\n",
      "Epoch 46, Training Loss: 0.8369175891230877\n",
      "Epoch 47, Training Loss: 0.8345150098764806\n",
      "Epoch 48, Training Loss: 0.8323807676035658\n",
      "Epoch 49, Training Loss: 0.8304304666985246\n",
      "Epoch 50, Training Loss: 0.8288974541470521\n",
      "Epoch 51, Training Loss: 0.8277119954725853\n",
      "Epoch 52, Training Loss: 0.825760182760712\n",
      "Epoch 53, Training Loss: 0.8243096650094914\n",
      "Epoch 54, Training Loss: 0.8233967044299706\n",
      "Epoch 55, Training Loss: 0.8216562924528481\n",
      "Epoch 56, Training Loss: 0.820975611353279\n",
      "Epoch 57, Training Loss: 0.8197327083214782\n",
      "Epoch 58, Training Loss: 0.819049076299022\n",
      "Epoch 59, Training Loss: 0.8181185215935671\n",
      "Epoch 60, Training Loss: 0.8176020659898456\n",
      "Epoch 61, Training Loss: 0.8162734120411981\n",
      "Epoch 62, Training Loss: 0.8165305376052856\n",
      "Epoch 63, Training Loss: 0.815610176548922\n",
      "Epoch 64, Training Loss: 0.8150678322727519\n",
      "Epoch 65, Training Loss: 0.8150319896246258\n",
      "Epoch 66, Training Loss: 0.8145129922637366\n",
      "Epoch 67, Training Loss: 0.8141527846343535\n",
      "Epoch 68, Training Loss: 0.8134269245585105\n",
      "Epoch 69, Training Loss: 0.8127400936936974\n",
      "Epoch 70, Training Loss: 0.8125438252786048\n",
      "Epoch 71, Training Loss: 0.8123352472943471\n",
      "Epoch 72, Training Loss: 0.812636857642267\n",
      "Epoch 73, Training Loss: 0.8117026105859226\n",
      "Epoch 74, Training Loss: 0.8117638280517177\n",
      "Epoch 75, Training Loss: 0.8113740415501415\n",
      "Epoch 76, Training Loss: 0.811551704980377\n",
      "Epoch 77, Training Loss: 0.8110590191712057\n",
      "Epoch 78, Training Loss: 0.8109368212240979\n",
      "Epoch 79, Training Loss: 0.810929634965452\n",
      "Epoch 80, Training Loss: 0.8103040927334836\n",
      "Epoch 81, Training Loss: 0.8109610281492534\n",
      "Epoch 82, Training Loss: 0.8108371537430842\n",
      "Epoch 83, Training Loss: 0.8100136989041379\n",
      "Epoch 84, Training Loss: 0.8099575155659726\n",
      "Epoch 85, Training Loss: 0.8099298530951479\n",
      "Epoch 86, Training Loss: 0.8102999761588592\n",
      "Epoch 87, Training Loss: 0.8094330014142775\n",
      "Epoch 88, Training Loss: 0.8088819022465469\n",
      "Epoch 89, Training Loss: 0.8099201775134954\n",
      "Epoch 90, Training Loss: 0.810037893012054\n",
      "Epoch 91, Training Loss: 0.8092092716604247\n",
      "Epoch 92, Training Loss: 0.808887423339643\n",
      "Epoch 93, Training Loss: 0.8085960470196\n",
      "Epoch 94, Training Loss: 0.8088820733522114\n",
      "Epoch 95, Training Loss: 0.8092195077050001\n",
      "Epoch 96, Training Loss: 0.8091441406343216\n",
      "Epoch 97, Training Loss: 0.8081595562454453\n",
      "Epoch 98, Training Loss: 0.8082052175263713\n",
      "Epoch 99, Training Loss: 0.8083310388980951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:15:51,252] Trial 184 finished with value: 0.6309333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8088527850638655\n",
      "Epoch 1, Training Loss: 0.9000069189071656\n",
      "Epoch 2, Training Loss: 0.8168514763607698\n",
      "Epoch 3, Training Loss: 0.8114424092629376\n",
      "Epoch 4, Training Loss: 0.8090574287666994\n",
      "Epoch 5, Training Loss: 0.8071885993901421\n",
      "Epoch 6, Training Loss: 0.804458163205315\n",
      "Epoch 7, Training Loss: 0.8032636253974017\n",
      "Epoch 8, Training Loss: 0.8024882008047665\n",
      "Epoch 9, Training Loss: 0.8019152663735782\n",
      "Epoch 10, Training Loss: 0.8011745799990261\n",
      "Epoch 11, Training Loss: 0.8011617536404554\n",
      "Epoch 12, Training Loss: 0.8000312944019542\n",
      "Epoch 13, Training Loss: 0.8005524213875042\n",
      "Epoch 14, Training Loss: 0.7994928071078132\n",
      "Epoch 15, Training Loss: 0.7991103173003478\n",
      "Epoch 16, Training Loss: 0.7984694177964154\n",
      "Epoch 17, Training Loss: 0.7976066332704881\n",
      "Epoch 18, Training Loss: 0.7976874796081992\n",
      "Epoch 19, Training Loss: 0.7966905537773581\n",
      "Epoch 20, Training Loss: 0.795871911609874\n",
      "Epoch 21, Training Loss: 0.7962708631683798\n",
      "Epoch 22, Training Loss: 0.795857640504837\n",
      "Epoch 23, Training Loss: 0.7948322332606597\n",
      "Epoch 24, Training Loss: 0.7942224716438967\n",
      "Epoch 25, Training Loss: 0.793919260782354\n",
      "Epoch 26, Training Loss: 0.7927931172006271\n",
      "Epoch 27, Training Loss: 0.7924086965532864\n",
      "Epoch 28, Training Loss: 0.7917999461117913\n",
      "Epoch 29, Training Loss: 0.7905073245833901\n",
      "Epoch 30, Training Loss: 0.7908610407043906\n",
      "Epoch 31, Training Loss: 0.789741380705553\n",
      "Epoch 32, Training Loss: 0.7888758497378405\n",
      "Epoch 33, Training Loss: 0.7888938081965727\n",
      "Epoch 34, Training Loss: 0.7883965084833258\n",
      "Epoch 35, Training Loss: 0.787468958532109\n",
      "Epoch 36, Training Loss: 0.7872920585379881\n",
      "Epoch 37, Training Loss: 0.786854002195246\n",
      "Epoch 38, Training Loss: 0.7869661455294665\n",
      "Epoch 39, Training Loss: 0.7861356698064244\n",
      "Epoch 40, Training Loss: 0.7861772408906151\n",
      "Epoch 41, Training Loss: 0.7859166816402884\n",
      "Epoch 42, Training Loss: 0.7861038518653196\n",
      "Epoch 43, Training Loss: 0.7856414332810571\n",
      "Epoch 44, Training Loss: 0.7851646537640515\n",
      "Epoch 45, Training Loss: 0.7852114899018231\n",
      "Epoch 46, Training Loss: 0.7845421039356905\n",
      "Epoch 47, Training Loss: 0.784657008858288\n",
      "Epoch 48, Training Loss: 0.7848911035060883\n",
      "Epoch 49, Training Loss: 0.7843207141932319\n",
      "Epoch 50, Training Loss: 0.7841571632553549\n",
      "Epoch 51, Training Loss: 0.7844329252663781\n",
      "Epoch 52, Training Loss: 0.7840920120828292\n",
      "Epoch 53, Training Loss: 0.784262630518745\n",
      "Epoch 54, Training Loss: 0.7836221453722786\n",
      "Epoch 55, Training Loss: 0.7839728742487291\n",
      "Epoch 56, Training Loss: 0.7839060396306655\n",
      "Epoch 57, Training Loss: 0.783391727770076\n",
      "Epoch 58, Training Loss: 0.7838852511433994\n",
      "Epoch 59, Training Loss: 0.7834478928762324\n",
      "Epoch 60, Training Loss: 0.7837264604428236\n",
      "Epoch 61, Training Loss: 0.7834334194660186\n",
      "Epoch 62, Training Loss: 0.782704865581849\n",
      "Epoch 63, Training Loss: 0.7834806973092696\n",
      "Epoch 64, Training Loss: 0.7834478789217332\n",
      "Epoch 65, Training Loss: 0.7830802219755509\n",
      "Epoch 66, Training Loss: 0.7829683610972236\n",
      "Epoch 67, Training Loss: 0.7830932408220628\n",
      "Epoch 68, Training Loss: 0.7829592290345361\n",
      "Epoch 69, Training Loss: 0.7829465730751262\n",
      "Epoch 70, Training Loss: 0.7829742636400111\n",
      "Epoch 71, Training Loss: 0.7827071748060339\n",
      "Epoch 72, Training Loss: 0.7825179993405061\n",
      "Epoch 73, Training Loss: 0.7828719095622791\n",
      "Epoch 74, Training Loss: 0.7824685312018675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:17:32,417] Trial 185 finished with value: 0.6399333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.782517309960197\n",
      "Epoch 1, Training Loss: 1.0019864507983713\n",
      "Epoch 2, Training Loss: 0.9341736050914315\n",
      "Epoch 3, Training Loss: 0.9135232297813192\n",
      "Epoch 4, Training Loss: 0.8917582733490887\n",
      "Epoch 5, Training Loss: 0.8602005657027749\n",
      "Epoch 6, Training Loss: 0.8281100165142733\n",
      "Epoch 7, Training Loss: 0.8125553308514988\n",
      "Epoch 8, Training Loss: 0.8077087833600886\n",
      "Epoch 9, Training Loss: 0.8053660659229054\n",
      "Epoch 10, Training Loss: 0.8037804215094623\n",
      "Epoch 11, Training Loss: 0.8022597105362836\n",
      "Epoch 12, Training Loss: 0.8014011048569398\n",
      "Epoch 13, Training Loss: 0.800216980751823\n",
      "Epoch 14, Training Loss: 0.799313595715691\n",
      "Epoch 15, Training Loss: 0.7987122365306405\n",
      "Epoch 16, Training Loss: 0.7985070363213034\n",
      "Epoch 17, Training Loss: 0.7976540639821221\n",
      "Epoch 18, Training Loss: 0.7973164582252502\n",
      "Epoch 19, Training Loss: 0.7967713872825398\n",
      "Epoch 20, Training Loss: 0.796297433867174\n",
      "Epoch 21, Training Loss: 0.7958504422973184\n",
      "Epoch 22, Training Loss: 0.7951089846386629\n",
      "Epoch 23, Training Loss: 0.7947944581508637\n",
      "Epoch 24, Training Loss: 0.7946082698597627\n",
      "Epoch 25, Training Loss: 0.7940706424853381\n",
      "Epoch 26, Training Loss: 0.7932955067999222\n",
      "Epoch 27, Training Loss: 0.7930997110114378\n",
      "Epoch 28, Training Loss: 0.7925749160261715\n",
      "Epoch 29, Training Loss: 0.7925646864666658\n",
      "Epoch 30, Training Loss: 0.7919847669320949\n",
      "Epoch 31, Training Loss: 0.7916061190296622\n",
      "Epoch 32, Training Loss: 0.7912032240979812\n",
      "Epoch 33, Training Loss: 0.7908897463013144\n",
      "Epoch 34, Training Loss: 0.7906834906690261\n",
      "Epoch 35, Training Loss: 0.7905377012841842\n",
      "Epoch 36, Training Loss: 0.7899356004771064\n",
      "Epoch 37, Training Loss: 0.7897140122862423\n",
      "Epoch 38, Training Loss: 0.7895062487966874\n",
      "Epoch 39, Training Loss: 0.7890451873050016\n",
      "Epoch 40, Training Loss: 0.7888774374653311\n",
      "Epoch 41, Training Loss: 0.7887582621153664\n",
      "Epoch 42, Training Loss: 0.7883334775531993\n",
      "Epoch 43, Training Loss: 0.7882687231372384\n",
      "Epoch 44, Training Loss: 0.787904979761909\n",
      "Epoch 45, Training Loss: 0.787853061171139\n",
      "Epoch 46, Training Loss: 0.7875246268160203\n",
      "Epoch 47, Training Loss: 0.7872918696964488\n",
      "Epoch 48, Training Loss: 0.7868282072684344\n",
      "Epoch 49, Training Loss: 0.7869059677685009\n",
      "Epoch 50, Training Loss: 0.7864968637157889\n",
      "Epoch 51, Training Loss: 0.7866877215750078\n",
      "Epoch 52, Training Loss: 0.7861816660796894\n",
      "Epoch 53, Training Loss: 0.7858483790650087\n",
      "Epoch 54, Training Loss: 0.7861743884226855\n",
      "Epoch 55, Training Loss: 0.7857609980947831\n",
      "Epoch 56, Training Loss: 0.7855958921067855\n",
      "Epoch 57, Training Loss: 0.7853736312950359\n",
      "Epoch 58, Training Loss: 0.7854546998528873\n",
      "Epoch 59, Training Loss: 0.78506760281675\n",
      "Epoch 60, Training Loss: 0.7850715848978828\n",
      "Epoch 61, Training Loss: 0.7846919882297516\n",
      "Epoch 62, Training Loss: 0.7847470781382393\n",
      "Epoch 63, Training Loss: 0.7846588036593269\n",
      "Epoch 64, Training Loss: 0.7845783353553099\n",
      "Epoch 65, Training Loss: 0.7844052209573633\n",
      "Epoch 66, Training Loss: 0.7843934362776139\n",
      "Epoch 67, Training Loss: 0.7846408443591174\n",
      "Epoch 68, Training Loss: 0.784080272772733\n",
      "Epoch 69, Training Loss: 0.7840366156662212\n",
      "Epoch 70, Training Loss: 0.7842043687315549\n",
      "Epoch 71, Training Loss: 0.78379438365207\n",
      "Epoch 72, Training Loss: 0.7837710593728459\n",
      "Epoch 73, Training Loss: 0.7837084540900062\n",
      "Epoch 74, Training Loss: 0.7837161658090703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:18:54,529] Trial 186 finished with value: 0.6392666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7835513985157013\n",
      "Epoch 1, Training Loss: 0.8924220626515553\n",
      "Epoch 2, Training Loss: 0.8295322057896091\n",
      "Epoch 3, Training Loss: 0.8211015473631091\n",
      "Epoch 4, Training Loss: 0.8175132790902504\n",
      "Epoch 5, Training Loss: 0.8140644696422089\n",
      "Epoch 6, Training Loss: 0.8122782299393102\n",
      "Epoch 7, Training Loss: 0.8101058713027409\n",
      "Epoch 8, Training Loss: 0.8084775095595453\n",
      "Epoch 9, Training Loss: 0.8082152196339198\n",
      "Epoch 10, Training Loss: 0.8072512419600236\n",
      "Epoch 11, Training Loss: 0.807632933613053\n",
      "Epoch 12, Training Loss: 0.806227165236509\n",
      "Epoch 13, Training Loss: 0.805174475953095\n",
      "Epoch 14, Training Loss: 0.8039813607258904\n",
      "Epoch 15, Training Loss: 0.8044933672238114\n",
      "Epoch 16, Training Loss: 0.8031034951819513\n",
      "Epoch 17, Training Loss: 0.8032758848111432\n",
      "Epoch 18, Training Loss: 0.8026548386516428\n",
      "Epoch 19, Training Loss: 0.8022985696792603\n",
      "Epoch 20, Training Loss: 0.8028629110271769\n",
      "Epoch 21, Training Loss: 0.8019108176231384\n",
      "Epoch 22, Training Loss: 0.8009802891795796\n",
      "Epoch 23, Training Loss: 0.8020411455541625\n",
      "Epoch 24, Training Loss: 0.8020559888137014\n",
      "Epoch 25, Training Loss: 0.8000589198635933\n",
      "Epoch 26, Training Loss: 0.8011201007025582\n",
      "Epoch 27, Training Loss: 0.8006820792542364\n",
      "Epoch 28, Training Loss: 0.7984952001643361\n",
      "Epoch 29, Training Loss: 0.8000239659969072\n",
      "Epoch 30, Training Loss: 0.7991416330624344\n",
      "Epoch 31, Training Loss: 0.8002092168743449\n",
      "Epoch 32, Training Loss: 0.7983881154454741\n",
      "Epoch 33, Training Loss: 0.7979140434050023\n",
      "Epoch 34, Training Loss: 0.7989105897738521\n",
      "Epoch 35, Training Loss: 0.7991611488779685\n",
      "Epoch 36, Training Loss: 0.8000924478796192\n",
      "Epoch 37, Training Loss: 0.7988450014501586\n",
      "Epoch 38, Training Loss: 0.7986862850368471\n",
      "Epoch 39, Training Loss: 0.7975591930231654\n",
      "Epoch 40, Training Loss: 0.798588870790668\n",
      "Epoch 41, Training Loss: 0.797596403978821\n",
      "Epoch 42, Training Loss: 0.7977095408547193\n",
      "Epoch 43, Training Loss: 0.7981191702355119\n",
      "Epoch 44, Training Loss: 0.798956468768586\n",
      "Epoch 45, Training Loss: 0.7980263663413829\n",
      "Epoch 46, Training Loss: 0.7972451671621853\n",
      "Epoch 47, Training Loss: 0.7984281262060753\n",
      "Epoch 48, Training Loss: 0.7977882888980378\n",
      "Epoch 49, Training Loss: 0.7970929176287543\n",
      "Epoch 50, Training Loss: 0.7967823121780739\n",
      "Epoch 51, Training Loss: 0.7981098091691956\n",
      "Epoch 52, Training Loss: 0.796420089956513\n",
      "Epoch 53, Training Loss: 0.7971108824686897\n",
      "Epoch 54, Training Loss: 0.7968133074000365\n",
      "Epoch 55, Training Loss: 0.7972558166747703\n",
      "Epoch 56, Training Loss: 0.797293309968217\n",
      "Epoch 57, Training Loss: 0.7956696756800314\n",
      "Epoch 58, Training Loss: 0.7963174193425286\n",
      "Epoch 59, Training Loss: 0.796937332207099\n",
      "Epoch 60, Training Loss: 0.7961713417132098\n",
      "Epoch 61, Training Loss: 0.7976295193335168\n",
      "Epoch 62, Training Loss: 0.7960573822035826\n",
      "Epoch 63, Training Loss: 0.795992572594406\n",
      "Epoch 64, Training Loss: 0.7960375949852448\n",
      "Epoch 65, Training Loss: 0.7961760757561017\n",
      "Epoch 66, Training Loss: 0.7961947826514567\n",
      "Epoch 67, Training Loss: 0.7960490639944722\n",
      "Epoch 68, Training Loss: 0.7957327079504056\n",
      "Epoch 69, Training Loss: 0.7955591817547504\n",
      "Epoch 70, Training Loss: 0.7960588340472458\n",
      "Epoch 71, Training Loss: 0.7970679098502138\n",
      "Epoch 72, Training Loss: 0.7954941191171345\n",
      "Epoch 73, Training Loss: 0.7955583157395958\n",
      "Epoch 74, Training Loss: 0.7953388170192116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:20:18,337] Trial 187 finished with value: 0.5788666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7960682732718332\n",
      "Epoch 1, Training Loss: 1.0500081816113982\n",
      "Epoch 2, Training Loss: 0.9679704318369241\n",
      "Epoch 3, Training Loss: 0.9390033077476616\n",
      "Epoch 4, Training Loss: 0.9282505367931567\n",
      "Epoch 5, Training Loss: 0.9195967954800541\n",
      "Epoch 6, Training Loss: 0.9104817866382743\n",
      "Epoch 7, Training Loss: 0.9020483467811928\n",
      "Epoch 8, Training Loss: 0.891847152369363\n",
      "Epoch 9, Training Loss: 0.8791259779069657\n",
      "Epoch 10, Training Loss: 0.8651786510209392\n",
      "Epoch 11, Training Loss: 0.8500776844813411\n",
      "Epoch 12, Training Loss: 0.8356359667347786\n",
      "Epoch 13, Training Loss: 0.8242237874439784\n",
      "Epoch 14, Training Loss: 0.8171478709780184\n",
      "Epoch 15, Training Loss: 0.8123043686823738\n",
      "Epoch 16, Training Loss: 0.8092343130506071\n",
      "Epoch 17, Training Loss: 0.8070228636712956\n",
      "Epoch 18, Training Loss: 0.8056430651729268\n",
      "Epoch 19, Training Loss: 0.804307978000856\n",
      "Epoch 20, Training Loss: 0.8041071709833647\n",
      "Epoch 21, Training Loss: 0.8037654494880734\n",
      "Epoch 22, Training Loss: 0.8033698641267935\n",
      "Epoch 23, Training Loss: 0.802897075871776\n",
      "Epoch 24, Training Loss: 0.8027809765105857\n",
      "Epoch 25, Training Loss: 0.8021626878501777\n",
      "Epoch 26, Training Loss: 0.8012637228894054\n",
      "Epoch 27, Training Loss: 0.8005043803301073\n",
      "Epoch 28, Training Loss: 0.8002072371038279\n",
      "Epoch 29, Training Loss: 0.7995769519106786\n",
      "Epoch 30, Training Loss: 0.7997066033513922\n",
      "Epoch 31, Training Loss: 0.7994933572030605\n",
      "Epoch 32, Training Loss: 0.7987192962402688\n",
      "Epoch 33, Training Loss: 0.7985128393746856\n",
      "Epoch 34, Training Loss: 0.7987477238016917\n",
      "Epoch 35, Training Loss: 0.7977424477276049\n",
      "Epoch 36, Training Loss: 0.7981182156648851\n",
      "Epoch 37, Training Loss: 0.7972733510167975\n",
      "Epoch 38, Training Loss: 0.7977895414918885\n",
      "Epoch 39, Training Loss: 0.7974952710302252\n",
      "Epoch 40, Training Loss: 0.7970749070769862\n",
      "Epoch 41, Training Loss: 0.7969484428714092\n",
      "Epoch 42, Training Loss: 0.7964925473794005\n",
      "Epoch 43, Training Loss: 0.7966702122437327\n",
      "Epoch 44, Training Loss: 0.7963431472168829\n",
      "Epoch 45, Training Loss: 0.7957446013178144\n",
      "Epoch 46, Training Loss: 0.7959557218659193\n",
      "Epoch 47, Training Loss: 0.7962286133515207\n",
      "Epoch 48, Training Loss: 0.7957668733776064\n",
      "Epoch 49, Training Loss: 0.7961023584344333\n",
      "Epoch 50, Training Loss: 0.7955520487369452\n",
      "Epoch 51, Training Loss: 0.795440115247454\n",
      "Epoch 52, Training Loss: 0.7945972932908768\n",
      "Epoch 53, Training Loss: 0.7947732732708293\n",
      "Epoch 54, Training Loss: 0.7948190344007392\n",
      "Epoch 55, Training Loss: 0.7950013440354426\n",
      "Epoch 56, Training Loss: 0.7946656697674801\n",
      "Epoch 57, Training Loss: 0.7945507413462589\n",
      "Epoch 58, Training Loss: 0.7941165312788541\n",
      "Epoch 59, Training Loss: 0.7942144192251047\n",
      "Epoch 60, Training Loss: 0.7937818741439877\n",
      "Epoch 61, Training Loss: 0.7940766722636116\n",
      "Epoch 62, Training Loss: 0.7939346956131154\n",
      "Epoch 63, Training Loss: 0.7943513477655282\n",
      "Epoch 64, Training Loss: 0.7935520596073983\n",
      "Epoch 65, Training Loss: 0.7931121652287648\n",
      "Epoch 66, Training Loss: 0.7928734207511845\n",
      "Epoch 67, Training Loss: 0.7930437840913471\n",
      "Epoch 68, Training Loss: 0.7928457156159824\n",
      "Epoch 69, Training Loss: 0.7925850240807785\n",
      "Epoch 70, Training Loss: 0.7925432566413306\n",
      "Epoch 71, Training Loss: 0.7925857822697862\n",
      "Epoch 72, Training Loss: 0.7923247172420186\n",
      "Epoch 73, Training Loss: 0.7926109596302635\n",
      "Epoch 74, Training Loss: 0.7922946281002876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:21:27,671] Trial 188 finished with value: 0.6384 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7923459062002656\n",
      "Epoch 1, Training Loss: 0.9783888658186547\n",
      "Epoch 2, Training Loss: 0.9338999221199438\n",
      "Epoch 3, Training Loss: 0.9216359758735599\n",
      "Epoch 4, Training Loss: 0.9104373972218736\n",
      "Epoch 5, Training Loss: 0.9000102113960381\n",
      "Epoch 6, Training Loss: 0.8884531605512576\n",
      "Epoch 7, Training Loss: 0.8755830968232979\n",
      "Epoch 8, Training Loss: 0.8619411213057382\n",
      "Epoch 9, Training Loss: 0.8502666936781174\n",
      "Epoch 10, Training Loss: 0.8390696469106173\n",
      "Epoch 11, Training Loss: 0.8295873929683427\n",
      "Epoch 12, Training Loss: 0.823354947387724\n",
      "Epoch 13, Training Loss: 0.8183663568102327\n",
      "Epoch 14, Training Loss: 0.8138442737715584\n",
      "Epoch 15, Training Loss: 0.8113204398549589\n",
      "Epoch 16, Training Loss: 0.809624903901179\n",
      "Epoch 17, Training Loss: 0.8076652785889188\n",
      "Epoch 18, Training Loss: 0.8065190259675334\n",
      "Epoch 19, Training Loss: 0.8059654710884381\n",
      "Epoch 20, Training Loss: 0.8052099297817489\n",
      "Epoch 21, Training Loss: 0.8047234329962193\n",
      "Epoch 22, Training Loss: 0.8039268348450052\n",
      "Epoch 23, Training Loss: 0.8033974513971716\n",
      "Epoch 24, Training Loss: 0.8032962983712217\n",
      "Epoch 25, Training Loss: 0.8037289304840833\n",
      "Epoch 26, Training Loss: 0.8026339897535797\n",
      "Epoch 27, Training Loss: 0.8025404928322125\n",
      "Epoch 28, Training Loss: 0.8026381207588024\n",
      "Epoch 29, Training Loss: 0.8018639715094316\n",
      "Epoch 30, Training Loss: 0.8011146350462633\n",
      "Epoch 31, Training Loss: 0.801293399011282\n",
      "Epoch 32, Training Loss: 0.8010192273254682\n",
      "Epoch 33, Training Loss: 0.8011312174617796\n",
      "Epoch 34, Training Loss: 0.8008446178041903\n",
      "Epoch 35, Training Loss: 0.8008419371189032\n",
      "Epoch 36, Training Loss: 0.8006460647833975\n",
      "Epoch 37, Training Loss: 0.8000439402752353\n",
      "Epoch 38, Training Loss: 0.800310664696801\n",
      "Epoch 39, Training Loss: 0.8005132081813382\n",
      "Epoch 40, Training Loss: 0.7996641539989557\n",
      "Epoch 41, Training Loss: 0.799491476564479\n",
      "Epoch 42, Training Loss: 0.7995368041490254\n",
      "Epoch 43, Training Loss: 0.8000728543539692\n",
      "Epoch 44, Training Loss: 0.7991683791454574\n",
      "Epoch 45, Training Loss: 0.7990571788379124\n",
      "Epoch 46, Training Loss: 0.7987625634759888\n",
      "Epoch 47, Training Loss: 0.7990475838345693\n",
      "Epoch 48, Training Loss: 0.7980002494682943\n",
      "Epoch 49, Training Loss: 0.798298134032945\n",
      "Epoch 50, Training Loss: 0.7973673579388095\n",
      "Epoch 51, Training Loss: 0.7974312998298415\n",
      "Epoch 52, Training Loss: 0.7976844566208976\n",
      "Epoch 53, Training Loss: 0.7970040031841823\n",
      "Epoch 54, Training Loss: 0.7968000655783747\n",
      "Epoch 55, Training Loss: 0.7966851759673957\n",
      "Epoch 56, Training Loss: 0.796262198193629\n",
      "Epoch 57, Training Loss: 0.7963006541244966\n",
      "Epoch 58, Training Loss: 0.7972449359140898\n",
      "Epoch 59, Training Loss: 0.7963636682445842\n",
      "Epoch 60, Training Loss: 0.7961764256757005\n",
      "Epoch 61, Training Loss: 0.795742490685972\n",
      "Epoch 62, Training Loss: 0.7955827793680635\n",
      "Epoch 63, Training Loss: 0.7965593408821221\n",
      "Epoch 64, Training Loss: 0.795762714766022\n",
      "Epoch 65, Training Loss: 0.7956985572226962\n",
      "Epoch 66, Training Loss: 0.795788982667421\n",
      "Epoch 67, Training Loss: 0.7955135990802507\n",
      "Epoch 68, Training Loss: 0.795227657224899\n",
      "Epoch 69, Training Loss: 0.7958065039233158\n",
      "Epoch 70, Training Loss: 0.7947808358006011\n",
      "Epoch 71, Training Loss: 0.7947439973515675\n",
      "Epoch 72, Training Loss: 0.7945228365130891\n",
      "Epoch 73, Training Loss: 0.7945048837733448\n",
      "Epoch 74, Training Loss: 0.7949641938496353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:22:29,573] Trial 189 finished with value: 0.6273333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7956535856526598\n",
      "Epoch 1, Training Loss: 1.0433766172913943\n",
      "Epoch 2, Training Loss: 0.9567253576306736\n",
      "Epoch 3, Training Loss: 0.929879262938219\n",
      "Epoch 4, Training Loss: 0.9190272553528056\n",
      "Epoch 5, Training Loss: 0.9095523430319393\n",
      "Epoch 6, Training Loss: 0.8996234826480641\n",
      "Epoch 7, Training Loss: 0.8877899659381193\n",
      "Epoch 8, Training Loss: 0.872683979693581\n",
      "Epoch 9, Training Loss: 0.8545985252716962\n",
      "Epoch 10, Training Loss: 0.8363809501423555\n",
      "Epoch 11, Training Loss: 0.8221562106469098\n",
      "Epoch 12, Training Loss: 0.8135995219034308\n",
      "Epoch 13, Training Loss: 0.809062020147548\n",
      "Epoch 14, Training Loss: 0.806606990589815\n",
      "Epoch 15, Training Loss: 0.8052137781591976\n",
      "Epoch 16, Training Loss: 0.8042731236009036\n",
      "Epoch 17, Training Loss: 0.8037609951636371\n",
      "Epoch 18, Training Loss: 0.8031807884749245\n",
      "Epoch 19, Training Loss: 0.8025937689051909\n",
      "Epoch 20, Training Loss: 0.8020747333414414\n",
      "Epoch 21, Training Loss: 0.8017325674786288\n",
      "Epoch 22, Training Loss: 0.8013852427987491\n",
      "Epoch 23, Training Loss: 0.8009531584206749\n",
      "Epoch 24, Training Loss: 0.8007029940801508\n",
      "Epoch 25, Training Loss: 0.8002944255576414\n",
      "Epoch 26, Training Loss: 0.8000048747483421\n",
      "Epoch 27, Training Loss: 0.7997357490483452\n",
      "Epoch 28, Training Loss: 0.7997728063078487\n",
      "Epoch 29, Training Loss: 0.799157397887286\n",
      "Epoch 30, Training Loss: 0.799202162027359\n",
      "Epoch 31, Training Loss: 0.7987909028109382\n",
      "Epoch 32, Training Loss: 0.7986947813454797\n",
      "Epoch 33, Training Loss: 0.7984851546848521\n",
      "Epoch 34, Training Loss: 0.7982309846317067\n",
      "Epoch 35, Training Loss: 0.7979375529990477\n",
      "Epoch 36, Training Loss: 0.7978189862475675\n",
      "Epoch 37, Training Loss: 0.7977324580445009\n",
      "Epoch 38, Training Loss: 0.7973409422706155\n",
      "Epoch 39, Training Loss: 0.7969824351983912\n",
      "Epoch 40, Training Loss: 0.7968632768883425\n",
      "Epoch 41, Training Loss: 0.7965850329399109\n",
      "Epoch 42, Training Loss: 0.7964341627850252\n",
      "Epoch 43, Training Loss: 0.7964507745995241\n",
      "Epoch 44, Training Loss: 0.7963200608421774\n",
      "Epoch 45, Training Loss: 0.7959545298884897\n",
      "Epoch 46, Training Loss: 0.795904486319598\n",
      "Epoch 47, Training Loss: 0.7955276515203363\n",
      "Epoch 48, Training Loss: 0.7955168975100798\n",
      "Epoch 49, Training Loss: 0.7951501844911014\n",
      "Epoch 50, Training Loss: 0.7952694417448605\n",
      "Epoch 51, Training Loss: 0.7949531132333418\n",
      "Epoch 52, Training Loss: 0.7950515693776747\n",
      "Epoch 53, Training Loss: 0.7946641370829414\n",
      "Epoch 54, Training Loss: 0.7945278535169714\n",
      "Epoch 55, Training Loss: 0.7943316494717317\n",
      "Epoch 56, Training Loss: 0.7941747629642486\n",
      "Epoch 57, Training Loss: 0.7939336176479564\n",
      "Epoch 58, Training Loss: 0.7935557117181665\n",
      "Epoch 59, Training Loss: 0.7934360600920285\n",
      "Epoch 60, Training Loss: 0.7933607404372272\n",
      "Epoch 61, Training Loss: 0.7931930142991683\n",
      "Epoch 62, Training Loss: 0.7929871967259575\n",
      "Epoch 63, Training Loss: 0.7927200061433456\n",
      "Epoch 64, Training Loss: 0.792692378689261\n",
      "Epoch 65, Training Loss: 0.7926398649636437\n",
      "Epoch 66, Training Loss: 0.7923645635212169\n",
      "Epoch 67, Training Loss: 0.7922819457334631\n",
      "Epoch 68, Training Loss: 0.7920422724415275\n",
      "Epoch 69, Training Loss: 0.7917365347637849\n",
      "Epoch 70, Training Loss: 0.7917413299223955\n",
      "Epoch 71, Training Loss: 0.7914588005402509\n",
      "Epoch 72, Training Loss: 0.7913797673758338\n",
      "Epoch 73, Training Loss: 0.7912461583053364\n",
      "Epoch 74, Training Loss: 0.7911307335601133\n",
      "Epoch 75, Training Loss: 0.7907087189309737\n",
      "Epoch 76, Training Loss: 0.7905110977677738\n",
      "Epoch 77, Training Loss: 0.7904539224680732\n",
      "Epoch 78, Training Loss: 0.7902494160568013\n",
      "Epoch 79, Training Loss: 0.7901246780507705\n",
      "Epoch 80, Training Loss: 0.7900001495725968\n",
      "Epoch 81, Training Loss: 0.7899218390268438\n",
      "Epoch 82, Training Loss: 0.7897061933489407\n",
      "Epoch 83, Training Loss: 0.7895078520915088\n",
      "Epoch 84, Training Loss: 0.7894142719577341\n",
      "Epoch 85, Training Loss: 0.7893841366206898\n",
      "Epoch 86, Training Loss: 0.7889371785696815\n",
      "Epoch 87, Training Loss: 0.7888854351464439\n",
      "Epoch 88, Training Loss: 0.7890169878566966\n",
      "Epoch 89, Training Loss: 0.7887901455514571\n",
      "Epoch 90, Training Loss: 0.788752171011532\n",
      "Epoch 91, Training Loss: 0.7885645069094265\n",
      "Epoch 92, Training Loss: 0.788355152536841\n",
      "Epoch 93, Training Loss: 0.7882832612710841\n",
      "Epoch 94, Training Loss: 0.7881568554569693\n",
      "Epoch 95, Training Loss: 0.7882290145228891\n",
      "Epoch 96, Training Loss: 0.7880688117532169\n",
      "Epoch 97, Training Loss: 0.7878689129212323\n",
      "Epoch 98, Training Loss: 0.7878581211146186\n",
      "Epoch 99, Training Loss: 0.7875765287174898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:24:16,078] Trial 190 finished with value: 0.638 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7875096172444961\n",
      "Epoch 1, Training Loss: 0.8606080868664909\n",
      "Epoch 2, Training Loss: 0.8174753637874828\n",
      "Epoch 3, Training Loss: 0.8138010775341707\n",
      "Epoch 4, Training Loss: 0.8103313519674189\n",
      "Epoch 5, Training Loss: 0.8079344813262715\n",
      "Epoch 6, Training Loss: 0.8065844202742857\n",
      "Epoch 7, Training Loss: 0.8048644081284018\n",
      "Epoch 8, Training Loss: 0.8036390064744389\n",
      "Epoch 9, Training Loss: 0.8029762826246374\n",
      "Epoch 10, Training Loss: 0.8024126378928914\n",
      "Epoch 11, Training Loss: 0.8022433412776274\n",
      "Epoch 12, Training Loss: 0.801518958456376\n",
      "Epoch 13, Training Loss: 0.8004954464295331\n",
      "Epoch 14, Training Loss: 0.8007834696769714\n",
      "Epoch 15, Training Loss: 0.7998277439089383\n",
      "Epoch 16, Training Loss: 0.7989372198721941\n",
      "Epoch 17, Training Loss: 0.798504097181208\n",
      "Epoch 18, Training Loss: 0.7981543595650616\n",
      "Epoch 19, Training Loss: 0.7971613423263325\n",
      "Epoch 20, Training Loss: 0.7963860779650072\n",
      "Epoch 21, Training Loss: 0.795072650418562\n",
      "Epoch 22, Training Loss: 0.7941138290657717\n",
      "Epoch 23, Training Loss: 0.7930696509164923\n",
      "Epoch 24, Training Loss: 0.792301795202143\n",
      "Epoch 25, Training Loss: 0.7912885190458858\n",
      "Epoch 26, Training Loss: 0.7902653443112093\n",
      "Epoch 27, Training Loss: 0.7898496418139513\n",
      "Epoch 28, Training Loss: 0.7889967761320227\n",
      "Epoch 29, Training Loss: 0.7883464664571426\n",
      "Epoch 30, Training Loss: 0.7879901580249562\n",
      "Epoch 31, Training Loss: 0.7873666210034315\n",
      "Epoch 32, Training Loss: 0.7871659201734206\n",
      "Epoch 33, Training Loss: 0.7863894069194793\n",
      "Epoch 34, Training Loss: 0.7863146145203534\n",
      "Epoch 35, Training Loss: 0.7861610547935262\n",
      "Epoch 36, Training Loss: 0.7856254076957703\n",
      "Epoch 37, Training Loss: 0.7856871637176065\n",
      "Epoch 38, Training Loss: 0.7856562979782329\n",
      "Epoch 39, Training Loss: 0.785427395105362\n",
      "Epoch 40, Training Loss: 0.7855303281195023\n",
      "Epoch 41, Training Loss: 0.7851258624301237\n",
      "Epoch 42, Training Loss: 0.7850907662335564\n",
      "Epoch 43, Training Loss: 0.7852799143510706\n",
      "Epoch 44, Training Loss: 0.784415693844066\n",
      "Epoch 45, Training Loss: 0.7845775251528796\n",
      "Epoch 46, Training Loss: 0.7844145060988034\n",
      "Epoch 47, Training Loss: 0.7844064373128554\n",
      "Epoch 48, Training Loss: 0.7843188900807324\n",
      "Epoch 49, Training Loss: 0.7843911718620974\n",
      "Epoch 50, Training Loss: 0.7839479369976942\n",
      "Epoch 51, Training Loss: 0.7839440023197847\n",
      "Epoch 52, Training Loss: 0.7836402075430926\n",
      "Epoch 53, Training Loss: 0.7834218755189111\n",
      "Epoch 54, Training Loss: 0.7837108220773584\n",
      "Epoch 55, Training Loss: 0.7835096816455617\n",
      "Epoch 56, Training Loss: 0.7832380525504842\n",
      "Epoch 57, Training Loss: 0.7832753885493559\n",
      "Epoch 58, Training Loss: 0.7831848004986258\n",
      "Epoch 59, Training Loss: 0.7830151928172392\n",
      "Epoch 60, Training Loss: 0.7829805483537562\n",
      "Epoch 61, Training Loss: 0.7826061531375437\n",
      "Epoch 62, Training Loss: 0.7827913487658781\n",
      "Epoch 63, Training Loss: 0.7829055174659281\n",
      "Epoch 64, Training Loss: 0.7826249426252702\n",
      "Epoch 65, Training Loss: 0.7825590405744665\n",
      "Epoch 66, Training Loss: 0.7824929027697619\n",
      "Epoch 67, Training Loss: 0.7824534999623018\n",
      "Epoch 68, Training Loss: 0.7824465704665464\n",
      "Epoch 69, Training Loss: 0.782396162888583\n",
      "Epoch 70, Training Loss: 0.7821554784915027\n",
      "Epoch 71, Training Loss: 0.7823751735687255\n",
      "Epoch 72, Training Loss: 0.7822917842163759\n",
      "Epoch 73, Training Loss: 0.7821881235347075\n",
      "Epoch 74, Training Loss: 0.7821316201546613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:25:53,958] Trial 191 finished with value: 0.6388666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7820947061566745\n",
      "Epoch 1, Training Loss: 0.8767933886191425\n",
      "Epoch 2, Training Loss: 0.828129731837441\n",
      "Epoch 3, Training Loss: 0.8180404767569374\n",
      "Epoch 4, Training Loss: 0.815119766628041\n",
      "Epoch 5, Training Loss: 0.8133694810727063\n",
      "Epoch 6, Training Loss: 0.8116582370505614\n",
      "Epoch 7, Training Loss: 0.8105931751868304\n",
      "Epoch 8, Training Loss: 0.8080992359273574\n",
      "Epoch 9, Training Loss: 0.8070249313466689\n",
      "Epoch 10, Training Loss: 0.8066127214712255\n",
      "Epoch 11, Training Loss: 0.8062688003568088\n",
      "Epoch 12, Training Loss: 0.8047232017096351\n",
      "Epoch 13, Training Loss: 0.8038655585401199\n",
      "Epoch 14, Training Loss: 0.8047762442336363\n",
      "Epoch 15, Training Loss: 0.8031587644184337\n",
      "Epoch 16, Training Loss: 0.8030596188236685\n",
      "Epoch 17, Training Loss: 0.802786625623703\n",
      "Epoch 18, Training Loss: 0.8027272403240204\n",
      "Epoch 19, Training Loss: 0.8024576523724725\n",
      "Epoch 20, Training Loss: 0.8018867356636945\n",
      "Epoch 21, Training Loss: 0.8026093629528495\n",
      "Epoch 22, Training Loss: 0.8009884721391342\n",
      "Epoch 23, Training Loss: 0.8014640080227572\n",
      "Epoch 24, Training Loss: 0.8004710313853095\n",
      "Epoch 25, Training Loss: 0.8004068641802844\n",
      "Epoch 26, Training Loss: 0.8006454976165995\n",
      "Epoch 27, Training Loss: 0.8007999537972843\n",
      "Epoch 28, Training Loss: 0.8001159283694099\n",
      "Epoch 29, Training Loss: 0.800686967162525\n",
      "Epoch 30, Training Loss: 0.8004222109037287\n",
      "Epoch 31, Training Loss: 0.8000010893625371\n",
      "Epoch 32, Training Loss: 0.7997350096702576\n",
      "Epoch 33, Training Loss: 0.7988604453731986\n",
      "Epoch 34, Training Loss: 0.7991611248605391\n",
      "Epoch 35, Training Loss: 0.7993698974917917\n",
      "Epoch 36, Training Loss: 0.799453583535026\n",
      "Epoch 37, Training Loss: 0.7989816071005429\n",
      "Epoch 38, Training Loss: 0.7992916370840634\n",
      "Epoch 39, Training Loss: 0.7987308500794803\n",
      "Epoch 40, Training Loss: 0.7991047343085794\n",
      "Epoch 41, Training Loss: 0.799216689081753\n",
      "Epoch 42, Training Loss: 0.7982542110891904\n",
      "Epoch 43, Training Loss: 0.7987928159096662\n",
      "Epoch 44, Training Loss: 0.7984896601648892\n",
      "Epoch 45, Training Loss: 0.7983342240838444\n",
      "Epoch 46, Training Loss: 0.7985451825226054\n",
      "Epoch 47, Training Loss: 0.7986695469126982\n",
      "Epoch 48, Training Loss: 0.7979831952908459\n",
      "Epoch 49, Training Loss: 0.7978595355679007\n",
      "Epoch 50, Training Loss: 0.7976207730349373\n",
      "Epoch 51, Training Loss: 0.7980947221727932\n",
      "Epoch 52, Training Loss: 0.7976846287531011\n",
      "Epoch 53, Training Loss: 0.7983286053994123\n",
      "Epoch 54, Training Loss: 0.7974445249052609\n",
      "Epoch 55, Training Loss: 0.7979597900895511\n",
      "Epoch 56, Training Loss: 0.7972491694197935\n",
      "Epoch 57, Training Loss: 0.7977150625341078\n",
      "Epoch 58, Training Loss: 0.7980884457335753\n",
      "Epoch 59, Training Loss: 0.7976306713328642\n",
      "Epoch 60, Training Loss: 0.7969366025924682\n",
      "Epoch 61, Training Loss: 0.7971646449145149\n",
      "Epoch 62, Training Loss: 0.797767631446614\n",
      "Epoch 63, Training Loss: 0.7975467296908884\n",
      "Epoch 64, Training Loss: 0.7971958793612087\n",
      "Epoch 65, Training Loss: 0.7973559190245235\n",
      "Epoch 66, Training Loss: 0.7967700029120726\n",
      "Epoch 67, Training Loss: 0.7974509979696834\n",
      "Epoch 68, Training Loss: 0.7971956942361944\n",
      "Epoch 69, Training Loss: 0.7968403457192814\n",
      "Epoch 70, Training Loss: 0.7962840723290163\n",
      "Epoch 71, Training Loss: 0.7969843739621779\n",
      "Epoch 72, Training Loss: 0.7966493380069732\n",
      "Epoch 73, Training Loss: 0.7968476219037\n",
      "Epoch 74, Training Loss: 0.7969484761182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:27:31,135] Trial 192 finished with value: 0.6356666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7957904258896323\n",
      "Epoch 1, Training Loss: 1.0675591212466247\n",
      "Epoch 2, Training Loss: 1.029287740341703\n",
      "Epoch 3, Training Loss: 1.0077555600861858\n",
      "Epoch 4, Training Loss: 0.993838079800283\n",
      "Epoch 5, Training Loss: 0.9836620542339812\n",
      "Epoch 6, Training Loss: 0.9769359351997089\n",
      "Epoch 7, Training Loss: 0.9716840757463211\n",
      "Epoch 8, Training Loss: 0.9675677784403464\n",
      "Epoch 9, Training Loss: 0.9649582459514302\n",
      "Epoch 10, Training Loss: 0.9626863534289195\n",
      "Epoch 11, Training Loss: 0.9607177891229328\n",
      "Epoch 12, Training Loss: 0.959743244755537\n",
      "Epoch 13, Training Loss: 0.9583706105562081\n",
      "Epoch 14, Training Loss: 0.9568467564152595\n",
      "Epoch 15, Training Loss: 0.9559331814149269\n",
      "Epoch 16, Training Loss: 0.9553916914122446\n",
      "Epoch 17, Training Loss: 0.9546987147259532\n",
      "Epoch 18, Training Loss: 0.9537014857270664\n",
      "Epoch 19, Training Loss: 0.9534137255267093\n",
      "Epoch 20, Training Loss: 0.952337488285581\n",
      "Epoch 21, Training Loss: 0.9522046489823134\n",
      "Epoch 22, Training Loss: 0.9511280011413689\n",
      "Epoch 23, Training Loss: 0.9506128591702396\n",
      "Epoch 24, Training Loss: 0.9504450965644722\n",
      "Epoch 25, Training Loss: 0.9491875694210368\n",
      "Epoch 26, Training Loss: 0.9487045751478439\n",
      "Epoch 27, Training Loss: 0.948188655089615\n",
      "Epoch 28, Training Loss: 0.9473322887169687\n",
      "Epoch 29, Training Loss: 0.9469038833352856\n",
      "Epoch 30, Training Loss: 0.9468919800636464\n",
      "Epoch 31, Training Loss: 0.9455523650449021\n",
      "Epoch 32, Training Loss: 0.9456488488312055\n",
      "Epoch 33, Training Loss: 0.9447959855086822\n",
      "Epoch 34, Training Loss: 0.9439594485705957\n",
      "Epoch 35, Training Loss: 0.9433623892920358\n",
      "Epoch 36, Training Loss: 0.9426326342095109\n",
      "Epoch 37, Training Loss: 0.9417013716876955\n",
      "Epoch 38, Training Loss: 0.9419835763766353\n",
      "Epoch 39, Training Loss: 0.940842964864315\n",
      "Epoch 40, Training Loss: 0.9402123818720194\n",
      "Epoch 41, Training Loss: 0.9400276395611297\n",
      "Epoch 42, Training Loss: 0.9391286521029651\n",
      "Epoch 43, Training Loss: 0.9387547795933888\n",
      "Epoch 44, Training Loss: 0.9383807921768131\n",
      "Epoch 45, Training Loss: 0.9377667021034355\n",
      "Epoch 46, Training Loss: 0.9372302293777466\n",
      "Epoch 47, Training Loss: 0.9362501093319484\n",
      "Epoch 48, Training Loss: 0.9354636473763258\n",
      "Epoch 49, Training Loss: 0.9353412013304861\n",
      "Epoch 50, Training Loss: 0.9347366564255908\n",
      "Epoch 51, Training Loss: 0.934025288076329\n",
      "Epoch 52, Training Loss: 0.9328001117347775\n",
      "Epoch 53, Training Loss: 0.9326672644543469\n",
      "Epoch 54, Training Loss: 0.9320071794932946\n",
      "Epoch 55, Training Loss: 0.931568593996808\n",
      "Epoch 56, Training Loss: 0.9306575541209458\n",
      "Epoch 57, Training Loss: 0.9303312769509796\n",
      "Epoch 58, Training Loss: 0.9297481202541438\n",
      "Epoch 59, Training Loss: 0.9295049877095043\n",
      "Epoch 60, Training Loss: 0.9282482620468713\n",
      "Epoch 61, Training Loss: 0.9282074575137375\n",
      "Epoch 62, Training Loss: 0.9271204324593222\n",
      "Epoch 63, Training Loss: 0.9266032569390491\n",
      "Epoch 64, Training Loss: 0.9260192768914359\n",
      "Epoch 65, Training Loss: 0.9254380166082454\n",
      "Epoch 66, Training Loss: 0.9245286866238243\n",
      "Epoch 67, Training Loss: 0.9238927018373533\n",
      "Epoch 68, Training Loss: 0.9232833858719446\n",
      "Epoch 69, Training Loss: 0.9230282534333997\n",
      "Epoch 70, Training Loss: 0.922184413626678\n",
      "Epoch 71, Training Loss: 0.9218942987291436\n",
      "Epoch 72, Training Loss: 0.9211304150129619\n",
      "Epoch 73, Training Loss: 0.9205561045417212\n",
      "Epoch 74, Training Loss: 0.9193984471765676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:28:33,528] Trial 193 finished with value: 0.5610666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9187135876569533\n",
      "Epoch 1, Training Loss: 0.8643692244264416\n",
      "Epoch 2, Training Loss: 0.8280937415316589\n",
      "Epoch 3, Training Loss: 0.8215075428324534\n",
      "Epoch 4, Training Loss: 0.8193134196718833\n",
      "Epoch 5, Training Loss: 0.8149463357782005\n",
      "Epoch 6, Training Loss: 0.8135439989262058\n",
      "Epoch 7, Training Loss: 0.8120557561852878\n",
      "Epoch 8, Training Loss: 0.8114552492485907\n",
      "Epoch 9, Training Loss: 0.8114439683749264\n",
      "Epoch 10, Training Loss: 0.8096284347369258\n",
      "Epoch 11, Training Loss: 0.8092080863794886\n",
      "Epoch 12, Training Loss: 0.8083093936281993\n",
      "Epoch 13, Training Loss: 0.8082195233581657\n",
      "Epoch 14, Training Loss: 0.8072275456181146\n",
      "Epoch 15, Training Loss: 0.8082327062922313\n",
      "Epoch 16, Training Loss: 0.8069180240308432\n",
      "Epoch 17, Training Loss: 0.8070398315451199\n",
      "Epoch 18, Training Loss: 0.806669711080709\n",
      "Epoch 19, Training Loss: 0.8072589782843912\n",
      "Epoch 20, Training Loss: 0.8065408773888323\n",
      "Epoch 21, Training Loss: 0.8068722488288592\n",
      "Epoch 22, Training Loss: 0.8076415906275125\n",
      "Epoch 23, Training Loss: 0.8063774877921083\n",
      "Epoch 24, Training Loss: 0.8060678368224237\n",
      "Epoch 25, Training Loss: 0.8056221039671647\n",
      "Epoch 26, Training Loss: 0.805980804809054\n",
      "Epoch 27, Training Loss: 0.8052949491300081\n",
      "Epoch 28, Training Loss: 0.806081083215269\n",
      "Epoch 29, Training Loss: 0.8053117571916796\n",
      "Epoch 30, Training Loss: 0.8061126965329163\n",
      "Epoch 31, Training Loss: 0.8061730113244594\n",
      "Epoch 32, Training Loss: 0.8063255005313041\n",
      "Epoch 33, Training Loss: 0.8050399775791885\n",
      "Epoch 34, Training Loss: 0.8051414548902583\n",
      "Epoch 35, Training Loss: 0.804506508479441\n",
      "Epoch 36, Training Loss: 0.805555810695304\n",
      "Epoch 37, Training Loss: 0.804519650452119\n",
      "Epoch 38, Training Loss: 0.8046192684567961\n",
      "Epoch 39, Training Loss: 0.8045983327958817\n",
      "Epoch 40, Training Loss: 0.8048440201838214\n",
      "Epoch 41, Training Loss: 0.8048664009660706\n",
      "Epoch 42, Training Loss: 0.804693471399465\n",
      "Epoch 43, Training Loss: 0.8046660664386319\n",
      "Epoch 44, Training Loss: 0.8048426665757832\n",
      "Epoch 45, Training Loss: 0.8044619824653282\n",
      "Epoch 46, Training Loss: 0.8042517016704818\n",
      "Epoch 47, Training Loss: 0.8037795431631848\n",
      "Epoch 48, Training Loss: 0.8036237191436882\n",
      "Epoch 49, Training Loss: 0.8031235023548728\n",
      "Epoch 50, Training Loss: 0.8035627967432926\n",
      "Epoch 51, Training Loss: 0.8037853810123932\n",
      "Epoch 52, Training Loss: 0.8032124843812527\n",
      "Epoch 53, Training Loss: 0.8031459998367424\n",
      "Epoch 54, Training Loss: 0.8020383232966402\n",
      "Epoch 55, Training Loss: 0.8030574145173668\n",
      "Epoch 56, Training Loss: 0.8032593365898706\n",
      "Epoch 57, Training Loss: 0.8029887305166489\n",
      "Epoch 58, Training Loss: 0.8024758724341715\n",
      "Epoch 59, Training Loss: 0.8016310922185281\n",
      "Epoch 60, Training Loss: 0.8016181806872662\n",
      "Epoch 61, Training Loss: 0.8012583574854342\n",
      "Epoch 62, Training Loss: 0.8020476662126699\n",
      "Epoch 63, Training Loss: 0.80160249005583\n",
      "Epoch 64, Training Loss: 0.8012497417012552\n",
      "Epoch 65, Training Loss: 0.8008616213511703\n",
      "Epoch 66, Training Loss: 0.8014096338946121\n",
      "Epoch 67, Training Loss: 0.7997987079441099\n",
      "Epoch 68, Training Loss: 0.7999876104351273\n",
      "Epoch 69, Training Loss: 0.8002626728294487\n",
      "Epoch 70, Training Loss: 0.8009912368946506\n",
      "Epoch 71, Training Loss: 0.7998126192648608\n",
      "Epoch 72, Training Loss: 0.8002050920536644\n",
      "Epoch 73, Training Loss: 0.799580292594164\n",
      "Epoch 74, Training Loss: 0.7995353125988093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:29:44,963] Trial 194 finished with value: 0.6192 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8002688338882045\n",
      "Epoch 1, Training Loss: 0.8822836379359539\n",
      "Epoch 2, Training Loss: 0.8186081815482978\n",
      "Epoch 3, Training Loss: 0.8106037199049068\n",
      "Epoch 4, Training Loss: 0.8081245814947258\n",
      "Epoch 5, Training Loss: 0.8056229449752578\n",
      "Epoch 6, Training Loss: 0.8041822029235668\n",
      "Epoch 7, Training Loss: 0.8025187620543\n",
      "Epoch 8, Training Loss: 0.8000122487993169\n",
      "Epoch 9, Training Loss: 0.7989825213762154\n",
      "Epoch 10, Training Loss: 0.7963468840247706\n",
      "Epoch 11, Training Loss: 0.7956240559879102\n",
      "Epoch 12, Training Loss: 0.793653953254671\n",
      "Epoch 13, Training Loss: 0.7914785262337305\n",
      "Epoch 14, Training Loss: 0.7912306627832857\n",
      "Epoch 15, Training Loss: 0.790939936781288\n",
      "Epoch 16, Training Loss: 0.7901205637401207\n",
      "Epoch 17, Training Loss: 0.7901426799315259\n",
      "Epoch 18, Training Loss: 0.7895242481303394\n",
      "Epoch 19, Training Loss: 0.7901727035529631\n",
      "Epoch 20, Training Loss: 0.787807187729312\n",
      "Epoch 21, Training Loss: 0.7882544210978917\n",
      "Epoch 22, Training Loss: 0.7880555592981496\n",
      "Epoch 23, Training Loss: 0.788875188684105\n",
      "Epoch 24, Training Loss: 0.7878289600512138\n",
      "Epoch 25, Training Loss: 0.7866488169458575\n",
      "Epoch 26, Training Loss: 0.7872995651754221\n",
      "Epoch 27, Training Loss: 0.7872937984932634\n",
      "Epoch 28, Training Loss: 0.7871699714122858\n",
      "Epoch 29, Training Loss: 0.7869037903341135\n",
      "Epoch 30, Training Loss: 0.7878164741329681\n",
      "Epoch 31, Training Loss: 0.7866392888520893\n",
      "Epoch 32, Training Loss: 0.786007397067278\n",
      "Epoch 33, Training Loss: 0.7861023140132876\n",
      "Epoch 34, Training Loss: 0.7858695208578181\n",
      "Epoch 35, Training Loss: 0.7860848058435254\n",
      "Epoch 36, Training Loss: 0.7848786278774864\n",
      "Epoch 37, Training Loss: 0.7851303258336576\n",
      "Epoch 38, Training Loss: 0.785159931415902\n",
      "Epoch 39, Training Loss: 0.7859723980265453\n",
      "Epoch 40, Training Loss: 0.7843281569337486\n",
      "Epoch 41, Training Loss: 0.7847381637508708\n",
      "Epoch 42, Training Loss: 0.7845445404375406\n",
      "Epoch 43, Training Loss: 0.7855752467212821\n",
      "Epoch 44, Training Loss: 0.7848021663221202\n",
      "Epoch 45, Training Loss: 0.7848972297252569\n",
      "Epoch 46, Training Loss: 0.7849483053487046\n",
      "Epoch 47, Training Loss: 0.7844732116935844\n",
      "Epoch 48, Training Loss: 0.7841343642177439\n",
      "Epoch 49, Training Loss: 0.7844927198008487\n",
      "Epoch 50, Training Loss: 0.7845666615586532\n",
      "Epoch 51, Training Loss: 0.7841304776363803\n",
      "Epoch 52, Training Loss: 0.7840149069190921\n",
      "Epoch 53, Training Loss: 0.7831085310842758\n",
      "Epoch 54, Training Loss: 0.7829749793934643\n",
      "Epoch 55, Training Loss: 0.7831240991900738\n",
      "Epoch 56, Training Loss: 0.7832067386548321\n",
      "Epoch 57, Training Loss: 0.7835670274899418\n",
      "Epoch 58, Training Loss: 0.7832165999520094\n",
      "Epoch 59, Training Loss: 0.7837714335972206\n",
      "Epoch 60, Training Loss: 0.7823522505007292\n",
      "Epoch 61, Training Loss: 0.7818671081299172\n",
      "Epoch 62, Training Loss: 0.7822544243998993\n",
      "Epoch 63, Training Loss: 0.7826601956123697\n",
      "Epoch 64, Training Loss: 0.7822081323853113\n",
      "Epoch 65, Training Loss: 0.7823914984115085\n",
      "Epoch 66, Training Loss: 0.7818685002793047\n",
      "Epoch 67, Training Loss: 0.7822424683355748\n",
      "Epoch 68, Training Loss: 0.7808996361897405\n",
      "Epoch 69, Training Loss: 0.7823465847431269\n",
      "Epoch 70, Training Loss: 0.7818580546773466\n",
      "Epoch 71, Training Loss: 0.7817047070739861\n",
      "Epoch 72, Training Loss: 0.7816756340794098\n",
      "Epoch 73, Training Loss: 0.7806857041846541\n",
      "Epoch 74, Training Loss: 0.7809147668960399\n",
      "Epoch 75, Training Loss: 0.7817810423392102\n",
      "Epoch 76, Training Loss: 0.7810369730892038\n",
      "Epoch 77, Training Loss: 0.7807861133625633\n",
      "Epoch 78, Training Loss: 0.7796387713206442\n",
      "Epoch 79, Training Loss: 0.7811272075301723\n",
      "Epoch 80, Training Loss: 0.7804172112529439\n",
      "Epoch 81, Training Loss: 0.7810419387387154\n",
      "Epoch 82, Training Loss: 0.780937342401734\n",
      "Epoch 83, Training Loss: 0.7804032451228091\n",
      "Epoch 84, Training Loss: 0.7801739455165719\n",
      "Epoch 85, Training Loss: 0.7797271744649213\n",
      "Epoch 86, Training Loss: 0.7795487921040757\n",
      "Epoch 87, Training Loss: 0.781224805340731\n",
      "Epoch 88, Training Loss: 0.7807045353086371\n",
      "Epoch 89, Training Loss: 0.779604517158709\n",
      "Epoch 90, Training Loss: 0.779804866116746\n",
      "Epoch 91, Training Loss: 0.7796270506722587\n",
      "Epoch 92, Training Loss: 0.7800025170010731\n",
      "Epoch 93, Training Loss: 0.7797301052208233\n",
      "Epoch 94, Training Loss: 0.7803792214931402\n",
      "Epoch 95, Training Loss: 0.7798870186160382\n",
      "Epoch 96, Training Loss: 0.7790246686541048\n",
      "Epoch 97, Training Loss: 0.7796009194582029\n",
      "Epoch 98, Training Loss: 0.7791764713767776\n",
      "Epoch 99, Training Loss: 0.7789133569351713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:31:43,285] Trial 195 finished with value: 0.6373333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.779165554405155\n",
      "Epoch 1, Training Loss: 0.9050952053607855\n",
      "Epoch 2, Training Loss: 0.8258405566215515\n",
      "Epoch 3, Training Loss: 0.8182072581205153\n",
      "Epoch 4, Training Loss: 0.8166866145635906\n",
      "Epoch 5, Training Loss: 0.8140847909719424\n",
      "Epoch 6, Training Loss: 0.8120320810411209\n",
      "Epoch 7, Training Loss: 0.8104179407420912\n",
      "Epoch 8, Training Loss: 0.809760528489163\n",
      "Epoch 9, Training Loss: 0.8088016346881264\n",
      "Epoch 10, Training Loss: 0.8075702766726788\n",
      "Epoch 11, Training Loss: 0.8072749863889881\n",
      "Epoch 12, Training Loss: 0.8064855355069154\n",
      "Epoch 13, Training Loss: 0.8057098319655971\n",
      "Epoch 14, Training Loss: 0.8057024849088569\n",
      "Epoch 15, Training Loss: 0.8053721115105135\n",
      "Epoch 16, Training Loss: 0.8045997174162614\n",
      "Epoch 17, Training Loss: 0.8045398238906287\n",
      "Epoch 18, Training Loss: 0.8046269875720031\n",
      "Epoch 19, Training Loss: 0.803840210204734\n",
      "Epoch 20, Training Loss: 0.8034181939031845\n",
      "Epoch 21, Training Loss: 0.8030856534054405\n",
      "Epoch 22, Training Loss: 0.8033197829597875\n",
      "Epoch 23, Training Loss: 0.8031582125147483\n",
      "Epoch 24, Training Loss: 0.8026759265060711\n",
      "Epoch 25, Training Loss: 0.8026240275318461\n",
      "Epoch 26, Training Loss: 0.8021831852152832\n",
      "Epoch 27, Training Loss: 0.8024251024525865\n",
      "Epoch 28, Training Loss: 0.8020596377831652\n",
      "Epoch 29, Training Loss: 0.8014972363199506\n",
      "Epoch 30, Training Loss: 0.8012164376732102\n",
      "Epoch 31, Training Loss: 0.8008492580033783\n",
      "Epoch 32, Training Loss: 0.8014108686518848\n",
      "Epoch 33, Training Loss: 0.8007477300507682\n",
      "Epoch 34, Training Loss: 0.8011884182019341\n",
      "Epoch 35, Training Loss: 0.8017733961119687\n",
      "Epoch 36, Training Loss: 0.8005403606067026\n",
      "Epoch 37, Training Loss: 0.8011361115857174\n",
      "Epoch 38, Training Loss: 0.8004436723271707\n",
      "Epoch 39, Training Loss: 0.800856698186774\n",
      "Epoch 40, Training Loss: 0.8008367594919706\n",
      "Epoch 41, Training Loss: 0.80057087195547\n",
      "Epoch 42, Training Loss: 0.8004358952206777\n",
      "Epoch 43, Training Loss: 0.7994488145175733\n",
      "Epoch 44, Training Loss: 0.7993441743061955\n",
      "Epoch 45, Training Loss: 0.800497539509508\n",
      "Epoch 46, Training Loss: 0.8006993505291473\n",
      "Epoch 47, Training Loss: 0.7995903850498056\n",
      "Epoch 48, Training Loss: 0.7988644928860485\n",
      "Epoch 49, Training Loss: 0.7984661433033478\n",
      "Epoch 50, Training Loss: 0.7985835248366334\n",
      "Epoch 51, Training Loss: 0.7987066179289853\n",
      "Epoch 52, Training Loss: 0.7985633021906803\n",
      "Epoch 53, Training Loss: 0.7977997739960376\n",
      "Epoch 54, Training Loss: 0.7981592302931879\n",
      "Epoch 55, Training Loss: 0.7976409255113817\n",
      "Epoch 56, Training Loss: 0.7974274294716971\n",
      "Epoch 57, Training Loss: 0.7976349815390164\n",
      "Epoch 58, Training Loss: 0.7967271253578645\n",
      "Epoch 59, Training Loss: 0.797599775002415\n",
      "Epoch 60, Training Loss: 0.7968440952157616\n",
      "Epoch 61, Training Loss: 0.797385921603755\n",
      "Epoch 62, Training Loss: 0.7971604872466926\n",
      "Epoch 63, Training Loss: 0.7970163601681702\n",
      "Epoch 64, Training Loss: 0.7968432148596398\n",
      "Epoch 65, Training Loss: 0.7963368583442574\n",
      "Epoch 66, Training Loss: 0.7968956273301203\n",
      "Epoch 67, Training Loss: 0.7963532906725891\n",
      "Epoch 68, Training Loss: 0.7967379973347025\n",
      "Epoch 69, Training Loss: 0.7972336154235037\n",
      "Epoch 70, Training Loss: 0.7955806242792229\n",
      "Epoch 71, Training Loss: 0.7961553459776971\n",
      "Epoch 72, Training Loss: 0.7956798859108659\n",
      "Epoch 73, Training Loss: 0.7960910450246997\n",
      "Epoch 74, Training Loss: 0.7955181306466124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:32:54,266] Trial 196 finished with value: 0.6208 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7957429011961571\n",
      "Epoch 1, Training Loss: 1.074656146988833\n",
      "Epoch 2, Training Loss: 1.03931795286953\n",
      "Epoch 3, Training Loss: 1.0142792142423471\n",
      "Epoch 4, Training Loss: 0.9959572382439348\n",
      "Epoch 5, Training Loss: 0.9821919163366906\n",
      "Epoch 6, Training Loss: 0.9727511930286437\n",
      "Epoch 7, Training Loss: 0.9652908536724578\n",
      "Epoch 8, Training Loss: 0.9603563018311235\n",
      "Epoch 9, Training Loss: 0.9572111098389876\n",
      "Epoch 10, Training Loss: 0.9546935745647975\n",
      "Epoch 11, Training Loss: 0.9525639185331818\n",
      "Epoch 12, Training Loss: 0.9515311214260589\n",
      "Epoch 13, Training Loss: 0.950467486399457\n",
      "Epoch 14, Training Loss: 0.9493474340976629\n",
      "Epoch 15, Training Loss: 0.9489825393920555\n",
      "Epoch 16, Training Loss: 0.9474146188649916\n",
      "Epoch 17, Training Loss: 0.9472551588725326\n",
      "Epoch 18, Training Loss: 0.9463409670313498\n",
      "Epoch 19, Training Loss: 0.9459671100279442\n",
      "Epoch 20, Training Loss: 0.9449566461089859\n",
      "Epoch 21, Training Loss: 0.9445740180804317\n",
      "Epoch 22, Training Loss: 0.943529691104602\n",
      "Epoch 23, Training Loss: 0.9430126913508078\n",
      "Epoch 24, Training Loss: 0.9420823644874687\n",
      "Epoch 25, Training Loss: 0.9408566556478801\n",
      "Epoch 26, Training Loss: 0.9405030462078582\n",
      "Epoch 27, Training Loss: 0.9398295009046569\n",
      "Epoch 28, Training Loss: 0.9390013072723733\n",
      "Epoch 29, Training Loss: 0.9381629545885818\n",
      "Epoch 30, Training Loss: 0.9374227834823436\n",
      "Epoch 31, Training Loss: 0.9368513732028186\n",
      "Epoch 32, Training Loss: 0.9353490319467128\n",
      "Epoch 33, Training Loss: 0.9350579448212358\n",
      "Epoch 34, Training Loss: 0.9338015953400978\n",
      "Epoch 35, Training Loss: 0.9328887729716481\n",
      "Epoch 36, Training Loss: 0.9316156788876182\n",
      "Epoch 37, Training Loss: 0.9312199761096697\n",
      "Epoch 38, Training Loss: 0.9300383484453187\n",
      "Epoch 39, Training Loss: 0.9292833623133208\n",
      "Epoch 40, Training Loss: 0.9280511254654791\n",
      "Epoch 41, Training Loss: 0.9271479796646233\n",
      "Epoch 42, Training Loss: 0.9258189715837177\n",
      "Epoch 43, Training Loss: 0.9252267092690432\n",
      "Epoch 44, Training Loss: 0.9240203281094257\n",
      "Epoch 45, Training Loss: 0.9226112364826345\n",
      "Epoch 46, Training Loss: 0.9219781485715307\n",
      "Epoch 47, Training Loss: 0.9209242349280451\n",
      "Epoch 48, Training Loss: 0.919100056375776\n",
      "Epoch 49, Training Loss: 0.9186089520167587\n",
      "Epoch 50, Training Loss: 0.9174360471560543\n",
      "Epoch 51, Training Loss: 0.9161879542178677\n",
      "Epoch 52, Training Loss: 0.9152326541735714\n",
      "Epoch 53, Training Loss: 0.9131158509648832\n",
      "Epoch 54, Training Loss: 0.9120919008900349\n",
      "Epoch 55, Training Loss: 0.9106376072517911\n",
      "Epoch 56, Training Loss: 0.9092844064970662\n",
      "Epoch 57, Training Loss: 0.9084822966640157\n",
      "Epoch 58, Training Loss: 0.9065147154313281\n",
      "Epoch 59, Training Loss: 0.9056179658810896\n",
      "Epoch 60, Training Loss: 0.9036025071502628\n",
      "Epoch 61, Training Loss: 0.9020393701424276\n",
      "Epoch 62, Training Loss: 0.9008209445422753\n",
      "Epoch 63, Training Loss: 0.8986271932609099\n",
      "Epoch 64, Training Loss: 0.8975191137844458\n",
      "Epoch 65, Training Loss: 0.8959224403352666\n",
      "Epoch 66, Training Loss: 0.8946105301828313\n",
      "Epoch 67, Training Loss: 0.8924577406474522\n",
      "Epoch 68, Training Loss: 0.8914471644207947\n",
      "Epoch 69, Training Loss: 0.8892581458378556\n",
      "Epoch 70, Training Loss: 0.8871969936485578\n",
      "Epoch 71, Training Loss: 0.8860578668744941\n",
      "Epoch 72, Training Loss: 0.8836298192354073\n",
      "Epoch 73, Training Loss: 0.8823486542343197\n",
      "Epoch 74, Training Loss: 0.880767049646019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:34:05,308] Trial 197 finished with value: 0.5883333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8786878503354868\n",
      "Epoch 1, Training Loss: 0.9434675558169086\n",
      "Epoch 2, Training Loss: 0.8880659664483895\n",
      "Epoch 3, Training Loss: 0.8506575137152708\n",
      "Epoch 4, Training Loss: 0.8246589425811194\n",
      "Epoch 5, Training Loss: 0.8123742452241425\n",
      "Epoch 6, Training Loss: 0.8078801724247466\n",
      "Epoch 7, Training Loss: 0.8050542281982594\n",
      "Epoch 8, Training Loss: 0.803643224382759\n",
      "Epoch 9, Training Loss: 0.8028768233786848\n",
      "Epoch 10, Training Loss: 0.8016810694135221\n",
      "Epoch 11, Training Loss: 0.8008070074525991\n",
      "Epoch 12, Training Loss: 0.8014213305666931\n",
      "Epoch 13, Training Loss: 0.8008708074576872\n",
      "Epoch 14, Training Loss: 0.800520894760476\n",
      "Epoch 15, Training Loss: 0.8001867878705935\n",
      "Epoch 16, Training Loss: 0.7998099444504071\n",
      "Epoch 17, Training Loss: 0.7984145514947131\n",
      "Epoch 18, Training Loss: 0.7987215992203333\n",
      "Epoch 19, Training Loss: 0.7990243545152191\n",
      "Epoch 20, Training Loss: 0.7981018597022035\n",
      "Epoch 21, Training Loss: 0.7981547678323616\n",
      "Epoch 22, Training Loss: 0.7980470426996847\n",
      "Epoch 23, Training Loss: 0.7978485647000765\n",
      "Epoch 24, Training Loss: 0.7979179875294965\n",
      "Epoch 25, Training Loss: 0.7978305181166283\n",
      "Epoch 26, Training Loss: 0.7980648638610554\n",
      "Epoch 27, Training Loss: 0.7974181657446955\n",
      "Epoch 28, Training Loss: 0.7972354635260159\n",
      "Epoch 29, Training Loss: 0.7971956222577202\n",
      "Epoch 30, Training Loss: 0.7965833088509122\n",
      "Epoch 31, Training Loss: 0.7965090518607233\n",
      "Epoch 32, Training Loss: 0.7966874229280572\n",
      "Epoch 33, Training Loss: 0.7965155740429585\n",
      "Epoch 34, Training Loss: 0.7953597277626956\n",
      "Epoch 35, Training Loss: 0.7956740985239359\n",
      "Epoch 36, Training Loss: 0.7958028563879487\n",
      "Epoch 37, Training Loss: 0.7957116758913025\n",
      "Epoch 38, Training Loss: 0.7955160142783831\n",
      "Epoch 39, Training Loss: 0.7950352446477216\n",
      "Epoch 40, Training Loss: 0.7951007400240216\n",
      "Epoch 41, Training Loss: 0.7942442399218567\n",
      "Epoch 42, Training Loss: 0.7939201645385053\n",
      "Epoch 43, Training Loss: 0.7930205481392997\n",
      "Epoch 44, Training Loss: 0.7936097334202071\n",
      "Epoch 45, Training Loss: 0.7933071489620925\n",
      "Epoch 46, Training Loss: 0.793027236766385\n",
      "Epoch 47, Training Loss: 0.7925904013160476\n",
      "Epoch 48, Training Loss: 0.7920188768913872\n",
      "Epoch 49, Training Loss: 0.79322506162457\n",
      "Epoch 50, Training Loss: 0.7919718766570988\n",
      "Epoch 51, Training Loss: 0.7919262721126241\n",
      "Epoch 52, Training Loss: 0.7919906019268179\n",
      "Epoch 53, Training Loss: 0.7916847788301625\n",
      "Epoch 54, Training Loss: 0.791798050062997\n",
      "Epoch 55, Training Loss: 0.7917821648425626\n",
      "Epoch 56, Training Loss: 0.7906705026339768\n",
      "Epoch 57, Training Loss: 0.7907006806448886\n",
      "Epoch 58, Training Loss: 0.790869448955794\n",
      "Epoch 59, Training Loss: 0.7912164802838089\n",
      "Epoch 60, Training Loss: 0.7901805413396735\n",
      "Epoch 61, Training Loss: 0.7900426733762698\n",
      "Epoch 62, Training Loss: 0.7900514901132512\n",
      "Epoch 63, Training Loss: 0.7909598882036998\n",
      "Epoch 64, Training Loss: 0.7902340603054018\n",
      "Epoch 65, Training Loss: 0.7905262092898663\n",
      "Epoch 66, Training Loss: 0.7907921104502857\n",
      "Epoch 67, Training Loss: 0.7898650981429824\n",
      "Epoch 68, Training Loss: 0.7896115672319455\n",
      "Epoch 69, Training Loss: 0.7901240187480038\n",
      "Epoch 70, Training Loss: 0.7894473920191141\n",
      "Epoch 71, Training Loss: 0.7895022855665451\n",
      "Epoch 72, Training Loss: 0.7900395111930102\n",
      "Epoch 73, Training Loss: 0.7892212921515444\n",
      "Epoch 74, Training Loss: 0.7896677356017263\n",
      "Epoch 75, Training Loss: 0.7893864314358934\n",
      "Epoch 76, Training Loss: 0.7896162135260446\n",
      "Epoch 77, Training Loss: 0.789141379352799\n",
      "Epoch 78, Training Loss: 0.7892826392238301\n",
      "Epoch 79, Training Loss: 0.7897267626640492\n",
      "Epoch 80, Training Loss: 0.789123561149253\n",
      "Epoch 81, Training Loss: 0.78938607255319\n",
      "Epoch 82, Training Loss: 0.788952086384135\n",
      "Epoch 83, Training Loss: 0.7887080732144808\n",
      "Epoch 84, Training Loss: 0.7887969851493836\n",
      "Epoch 85, Training Loss: 0.7890222093216459\n",
      "Epoch 86, Training Loss: 0.7889305483129688\n",
      "Epoch 87, Training Loss: 0.7889128037861415\n",
      "Epoch 88, Training Loss: 0.7895851628224653\n",
      "Epoch 89, Training Loss: 0.7881967911146637\n",
      "Epoch 90, Training Loss: 0.7891008472084102\n",
      "Epoch 91, Training Loss: 0.7889243682524315\n",
      "Epoch 92, Training Loss: 0.7879609381345878\n",
      "Epoch 93, Training Loss: 0.7889547390149052\n",
      "Epoch 94, Training Loss: 0.7885598806510294\n",
      "Epoch 95, Training Loss: 0.7884473707442893\n",
      "Epoch 96, Training Loss: 0.788319624725141\n",
      "Epoch 97, Training Loss: 0.7884155387268927\n",
      "Epoch 98, Training Loss: 0.7875865768669243\n",
      "Epoch 99, Training Loss: 0.7882016821911461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:35:44,103] Trial 198 finished with value: 0.6349333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7884670063965302\n",
      "Epoch 1, Training Loss: 1.0699596899397232\n",
      "Epoch 2, Training Loss: 1.0227717184319216\n",
      "Epoch 3, Training Loss: 0.9931410297926735\n",
      "Epoch 4, Training Loss: 0.9751652242155636\n",
      "Epoch 5, Training Loss: 0.9652245536271263\n",
      "Epoch 6, Training Loss: 0.9598502964833203\n",
      "Epoch 7, Training Loss: 0.9568724021490882\n",
      "Epoch 8, Training Loss: 0.9549600818577935\n",
      "Epoch 9, Training Loss: 0.9535152980860542\n",
      "Epoch 10, Training Loss: 0.952369502362083\n",
      "Epoch 11, Training Loss: 0.9512009453072268\n",
      "Epoch 12, Training Loss: 0.9500959004374111\n",
      "Epoch 13, Training Loss: 0.948944909362232\n",
      "Epoch 14, Training Loss: 0.9477662919549381\n",
      "Epoch 15, Training Loss: 0.9465623237104976\n",
      "Epoch 16, Training Loss: 0.9454030029212728\n",
      "Epoch 17, Training Loss: 0.9441455374044531\n",
      "Epoch 18, Training Loss: 0.9429473200966331\n",
      "Epoch 19, Training Loss: 0.9416701415005853\n",
      "Epoch 20, Training Loss: 0.9402633477659786\n",
      "Epoch 21, Training Loss: 0.9389355055724873\n",
      "Epoch 22, Training Loss: 0.9374978888736052\n",
      "Epoch 23, Training Loss: 0.935930855624816\n",
      "Epoch 24, Training Loss: 0.9345383043148938\n",
      "Epoch 25, Training Loss: 0.9329556930065155\n",
      "Epoch 26, Training Loss: 0.9312148768761579\n",
      "Epoch 27, Training Loss: 0.9296749272767235\n",
      "Epoch 28, Training Loss: 0.9280077484074761\n",
      "Epoch 29, Training Loss: 0.926198434899835\n",
      "Epoch 30, Training Loss: 0.9243488894490635\n",
      "Epoch 31, Training Loss: 0.9225859423244701\n",
      "Epoch 32, Training Loss: 0.9205499620297376\n",
      "Epoch 33, Training Loss: 0.9186561538892634\n",
      "Epoch 34, Training Loss: 0.9165878107968499\n",
      "Epoch 35, Training Loss: 0.9144928984782275\n",
      "Epoch 36, Training Loss: 0.9124002616545733\n",
      "Epoch 37, Training Loss: 0.910275285875096\n",
      "Epoch 38, Training Loss: 0.9080577360181248\n",
      "Epoch 39, Training Loss: 0.9057764946011936\n",
      "Epoch 40, Training Loss: 0.9035134574245004\n",
      "Epoch 41, Training Loss: 0.9011896202844732\n",
      "Epoch 42, Training Loss: 0.8988279714303858\n",
      "Epoch 43, Training Loss: 0.8964124983198503\n",
      "Epoch 44, Training Loss: 0.8940916181311888\n",
      "Epoch 45, Training Loss: 0.891657295998405\n",
      "Epoch 46, Training Loss: 0.8891363725241492\n",
      "Epoch 47, Training Loss: 0.8866881014319027\n",
      "Epoch 48, Training Loss: 0.8842796409831327\n",
      "Epoch 49, Training Loss: 0.8817476567100077\n",
      "Epoch 50, Training Loss: 0.8793574543560252\n",
      "Epoch 51, Training Loss: 0.8769080901145935\n",
      "Epoch 52, Training Loss: 0.8745336219843696\n",
      "Epoch 53, Training Loss: 0.8720921985542073\n",
      "Epoch 54, Training Loss: 0.8696808117978713\n",
      "Epoch 55, Training Loss: 0.8673639653009527\n",
      "Epoch 56, Training Loss: 0.8650631199163549\n",
      "Epoch 57, Training Loss: 0.8628153887215783\n",
      "Epoch 58, Training Loss: 0.8605455289167516\n",
      "Epoch 59, Training Loss: 0.8583832449071548\n",
      "Epoch 60, Training Loss: 0.8561882202064289\n",
      "Epoch 61, Training Loss: 0.8541112989537856\n",
      "Epoch 62, Training Loss: 0.852107489529778\n",
      "Epoch 63, Training Loss: 0.8501737641586977\n",
      "Epoch 64, Training Loss: 0.8481866357607\n",
      "Epoch 65, Training Loss: 0.8463205836800968\n",
      "Epoch 66, Training Loss: 0.8445494070473839\n",
      "Epoch 67, Training Loss: 0.8428415851733264\n",
      "Epoch 68, Training Loss: 0.8411425775640151\n",
      "Epoch 69, Training Loss: 0.8395380293621736\n",
      "Epoch 70, Training Loss: 0.8378791206724503\n",
      "Epoch 71, Training Loss: 0.8364542860844556\n",
      "Epoch 72, Training Loss: 0.8350700067071354\n",
      "Epoch 73, Training Loss: 0.8337335493985344\n",
      "Epoch 74, Training Loss: 0.8323420123492971\n",
      "Epoch 75, Training Loss: 0.8311540109269759\n",
      "Epoch 76, Training Loss: 0.8299704162513508\n",
      "Epoch 77, Training Loss: 0.8287834227786345\n",
      "Epoch 78, Training Loss: 0.8277818167209625\n",
      "Epoch 79, Training Loss: 0.8267344143811394\n",
      "Epoch 80, Training Loss: 0.8257816643574658\n",
      "Epoch 81, Training Loss: 0.8248174765530755\n",
      "Epoch 82, Training Loss: 0.8239799656587489\n",
      "Epoch 83, Training Loss: 0.8232206551467671\n",
      "Epoch 84, Training Loss: 0.8223493283636429\n",
      "Epoch 85, Training Loss: 0.8216084600196165\n",
      "Epoch 86, Training Loss: 0.8209084186133216\n",
      "Epoch 87, Training Loss: 0.82011861948406\n",
      "Epoch 88, Training Loss: 0.8195961068658267\n",
      "Epoch 89, Training Loss: 0.8190174368549795\n",
      "Epoch 90, Training Loss: 0.8184047370097216\n",
      "Epoch 91, Training Loss: 0.817921413603951\n",
      "Epoch 92, Training Loss: 0.8173733853592592\n",
      "Epoch 93, Training Loss: 0.8169356189755832\n",
      "Epoch 94, Training Loss: 0.8164516585013446\n",
      "Epoch 95, Training Loss: 0.8160144673375522\n",
      "Epoch 96, Training Loss: 0.8157209214743446\n",
      "Epoch 97, Training Loss: 0.8152642213597017\n",
      "Epoch 98, Training Loss: 0.8149343894509709\n",
      "Epoch 99, Training Loss: 0.8146010001266704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:37:17,272] Trial 199 finished with value: 0.6267333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8143087299431071\n",
      "Epoch 1, Training Loss: 0.993142423138899\n",
      "Epoch 2, Training Loss: 0.9411944868284113\n",
      "Epoch 3, Training Loss: 0.9295808773181018\n",
      "Epoch 4, Training Loss: 0.9212180634105906\n",
      "Epoch 5, Training Loss: 0.9138726982649635\n",
      "Epoch 6, Training Loss: 0.9067331381405102\n",
      "Epoch 7, Training Loss: 0.8995165674125447\n",
      "Epoch 8, Training Loss: 0.8920078046882853\n",
      "Epoch 9, Training Loss: 0.8840082663648269\n",
      "Epoch 10, Training Loss: 0.875829934723237\n",
      "Epoch 11, Training Loss: 0.8674042092351353\n",
      "Epoch 12, Training Loss: 0.8592092860446257\n",
      "Epoch 13, Training Loss: 0.8511761164665222\n",
      "Epoch 14, Training Loss: 0.8438654381387374\n",
      "Epoch 15, Training Loss: 0.8373210066907546\n",
      "Epoch 16, Training Loss: 0.8315593051209169\n",
      "Epoch 17, Training Loss: 0.8265828768646016\n",
      "Epoch 18, Training Loss: 0.8223991018884322\n",
      "Epoch 19, Training Loss: 0.8188395324174096\n",
      "Epoch 20, Training Loss: 0.8159674915145425\n",
      "Epoch 21, Training Loss: 0.8134348615478066\n",
      "Epoch 22, Training Loss: 0.8114895569576936\n",
      "Epoch 23, Training Loss: 0.8098802431190715\n",
      "Epoch 24, Training Loss: 0.8085771583809572\n",
      "Epoch 25, Training Loss: 0.8074170546671924\n",
      "Epoch 26, Training Loss: 0.8064447468168595\n",
      "Epoch 27, Training Loss: 0.8056949426146115\n",
      "Epoch 28, Training Loss: 0.8050240346964668\n",
      "Epoch 29, Training Loss: 0.8043579109276042\n",
      "Epoch 30, Training Loss: 0.8037163580866421\n",
      "Epoch 31, Training Loss: 0.803274232710109\n",
      "Epoch 32, Training Loss: 0.8027895097171559\n",
      "Epoch 33, Training Loss: 0.8024273373098935\n",
      "Epoch 34, Training Loss: 0.8020944717351128\n",
      "Epoch 35, Training Loss: 0.8017545256895178\n",
      "Epoch 36, Training Loss: 0.801354906278498\n",
      "Epoch 37, Training Loss: 0.8012263527337242\n",
      "Epoch 38, Training Loss: 0.8009754473321578\n",
      "Epoch 39, Training Loss: 0.8006840251473819\n",
      "Epoch 40, Training Loss: 0.8004732183147879\n",
      "Epoch 41, Training Loss: 0.8002269316420836\n",
      "Epoch 42, Training Loss: 0.8000828381145701\n",
      "Epoch 43, Training Loss: 0.7999860867332009\n",
      "Epoch 44, Training Loss: 0.7998377593825845\n",
      "Epoch 45, Training Loss: 0.7996483538431279\n",
      "Epoch 46, Training Loss: 0.7994444206882926\n",
      "Epoch 47, Training Loss: 0.7994150494126713\n",
      "Epoch 48, Training Loss: 0.799218261382159\n",
      "Epoch 49, Training Loss: 0.7989662917922525\n",
      "Epoch 50, Training Loss: 0.7989252009812523\n",
      "Epoch 51, Training Loss: 0.7987312555313111\n",
      "Epoch 52, Training Loss: 0.7986525693360497\n",
      "Epoch 53, Training Loss: 0.7983917446697459\n",
      "Epoch 54, Training Loss: 0.7984637751298792\n",
      "Epoch 55, Training Loss: 0.7983031340907601\n",
      "Epoch 56, Training Loss: 0.7981100907045252\n",
      "Epoch 57, Training Loss: 0.797988948541529\n",
      "Epoch 58, Training Loss: 0.7978628971296198\n",
      "Epoch 59, Training Loss: 0.7978793528500725\n",
      "Epoch 60, Training Loss: 0.7977516668684342\n",
      "Epoch 61, Training Loss: 0.7976701987490934\n",
      "Epoch 62, Training Loss: 0.7975181802581338\n",
      "Epoch 63, Training Loss: 0.7974399241980384\n",
      "Epoch 64, Training Loss: 0.7974104718601003\n",
      "Epoch 65, Training Loss: 0.7972537037905525\n",
      "Epoch 66, Training Loss: 0.7971997237906736\n",
      "Epoch 67, Training Loss: 0.7969295745737413\n",
      "Epoch 68, Training Loss: 0.7970660683687996\n",
      "Epoch 69, Training Loss: 0.7968233394622802\n",
      "Epoch 70, Training Loss: 0.796778754416634\n",
      "Epoch 71, Training Loss: 0.7967103638368495\n",
      "Epoch 72, Training Loss: 0.796503694267834\n",
      "Epoch 73, Training Loss: 0.7965237863624797\n",
      "Epoch 74, Training Loss: 0.7963965193664326\n",
      "Epoch 75, Training Loss: 0.7962237252207364\n",
      "Epoch 76, Training Loss: 0.7961751167914447\n",
      "Epoch 77, Training Loss: 0.7959498601100025\n",
      "Epoch 78, Training Loss: 0.7960173951177036\n",
      "Epoch 79, Training Loss: 0.7959863216035507\n",
      "Epoch 80, Training Loss: 0.7959425224977381\n",
      "Epoch 81, Training Loss: 0.7959037859299604\n",
      "Epoch 82, Training Loss: 0.795688255183837\n",
      "Epoch 83, Training Loss: 0.7956440770626068\n",
      "Epoch 84, Training Loss: 0.7956180155277253\n",
      "Epoch 85, Training Loss: 0.7955084834379308\n",
      "Epoch 86, Training Loss: 0.7954375062269323\n",
      "Epoch 87, Training Loss: 0.7954097486243529\n",
      "Epoch 88, Training Loss: 0.7953954645465402\n",
      "Epoch 89, Training Loss: 0.7954112575334661\n",
      "Epoch 90, Training Loss: 0.7952067260882434\n",
      "Epoch 91, Training Loss: 0.7952320652849534\n",
      "Epoch 92, Training Loss: 0.7951440621123594\n",
      "Epoch 93, Training Loss: 0.7950289143534268\n",
      "Epoch 94, Training Loss: 0.7949951661334318\n",
      "Epoch 95, Training Loss: 0.7948961392570945\n",
      "Epoch 96, Training Loss: 0.7949075450616724\n",
      "Epoch 97, Training Loss: 0.7947405554967768\n",
      "Epoch 98, Training Loss: 0.7948199601033155\n",
      "Epoch 99, Training Loss: 0.79477891311926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:38:50,212] Trial 200 finished with value: 0.6362 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7947227710134843\n",
      "Epoch 1, Training Loss: 1.0916157319966484\n",
      "Epoch 2, Training Loss: 1.0863705419091618\n",
      "Epoch 3, Training Loss: 1.0827120827226078\n",
      "Epoch 4, Training Loss: 1.078357656142291\n",
      "Epoch 5, Training Loss: 1.0729163775724524\n",
      "Epoch 6, Training Loss: 1.066171875\n",
      "Epoch 7, Training Loss: 1.0575062360483056\n",
      "Epoch 8, Training Loss: 1.0467999740207896\n",
      "Epoch 9, Training Loss: 1.0343283360845903\n",
      "Epoch 10, Training Loss: 1.020885205970091\n",
      "Epoch 11, Training Loss: 1.007907858876621\n",
      "Epoch 12, Training Loss: 0.9969660762478324\n",
      "Epoch 13, Training Loss: 0.9885167168869692\n",
      "Epoch 14, Training Loss: 0.982170318365097\n",
      "Epoch 15, Training Loss: 0.9773219572796541\n",
      "Epoch 16, Training Loss: 0.9731909086423761\n",
      "Epoch 17, Training Loss: 0.9696678717697368\n",
      "Epoch 18, Training Loss: 0.9664439386479995\n",
      "Epoch 19, Training Loss: 0.963644453497494\n",
      "Epoch 20, Training Loss: 0.9611912833241856\n",
      "Epoch 21, Training Loss: 0.959038927414838\n",
      "Epoch 22, Training Loss: 0.9573168415181778\n",
      "Epoch 23, Training Loss: 0.9557673626086292\n",
      "Epoch 24, Training Loss: 0.9544829600698808\n",
      "Epoch 25, Training Loss: 0.9533561299127691\n",
      "Epoch 26, Training Loss: 0.9523484209705801\n",
      "Epoch 27, Training Loss: 0.9512419938339907\n",
      "Epoch 28, Training Loss: 0.9502792976884281\n",
      "Epoch 29, Training Loss: 0.9493127924554489\n",
      "Epoch 30, Training Loss: 0.9482668056207545\n",
      "Epoch 31, Training Loss: 0.9473276163549984\n",
      "Epoch 32, Training Loss: 0.9462629243906806\n",
      "Epoch 33, Training Loss: 0.9451736631814172\n",
      "Epoch 34, Training Loss: 0.9440183675289154\n",
      "Epoch 35, Training Loss: 0.9428203623435076\n",
      "Epoch 36, Training Loss: 0.9416477983839372\n",
      "Epoch 37, Training Loss: 0.9403486498664407\n",
      "Epoch 38, Training Loss: 0.9390302532560685\n",
      "Epoch 39, Training Loss: 0.9375844923888936\n",
      "Epoch 40, Training Loss: 0.9361288553125718\n",
      "Epoch 41, Training Loss: 0.934650543717777\n",
      "Epoch 42, Training Loss: 0.9330325510221369\n",
      "Epoch 43, Training Loss: 0.9313471792024725\n",
      "Epoch 44, Training Loss: 0.9295728965366588\n",
      "Epoch 45, Training Loss: 0.9277260837134192\n",
      "Epoch 46, Training Loss: 0.9256763184070587\n",
      "Epoch 47, Training Loss: 0.9235779585557825\n",
      "Epoch 48, Training Loss: 0.9214254262868096\n",
      "Epoch 49, Training Loss: 0.9191452499698191\n",
      "Epoch 50, Training Loss: 0.9167925042264602\n",
      "Epoch 51, Training Loss: 0.9141225807105794\n",
      "Epoch 52, Training Loss: 0.9115082432943232\n",
      "Epoch 53, Training Loss: 0.9087428705832538\n",
      "Epoch 54, Training Loss: 0.9058792673139011\n",
      "Epoch 55, Training Loss: 0.9029042085479287\n",
      "Epoch 56, Training Loss: 0.8997744588992175\n",
      "Epoch 57, Training Loss: 0.8965267812504488\n",
      "Epoch 58, Training Loss: 0.8933220699955435\n",
      "Epoch 59, Training Loss: 0.8898889308817246\n",
      "Epoch 60, Training Loss: 0.8866862718498005\n",
      "Epoch 61, Training Loss: 0.8832702211071464\n",
      "Epoch 62, Training Loss: 0.880038868188858\n",
      "Epoch 63, Training Loss: 0.876618497021058\n",
      "Epoch 64, Training Loss: 0.8733002608663896\n",
      "Epoch 65, Training Loss: 0.8701167644472683\n",
      "Epoch 66, Training Loss: 0.8669272623342626\n",
      "Epoch 67, Training Loss: 0.863955570599612\n",
      "Epoch 68, Training Loss: 0.861153269515318\n",
      "Epoch 69, Training Loss: 0.8583330443326165\n",
      "Epoch 70, Training Loss: 0.855711583740571\n",
      "Epoch 71, Training Loss: 0.8532923412322998\n",
      "Epoch 72, Training Loss: 0.850973289644017\n",
      "Epoch 73, Training Loss: 0.8488987141496995\n",
      "Epoch 74, Training Loss: 0.8467679504787221\n",
      "Epoch 75, Training Loss: 0.8448921755482168\n",
      "Epoch 76, Training Loss: 0.8431771287497352\n",
      "Epoch 77, Training Loss: 0.8414894163608551\n",
      "Epoch 78, Training Loss: 0.8399994083713083\n",
      "Epoch 79, Training Loss: 0.838519536116544\n",
      "Epoch 80, Training Loss: 0.8373295795917511\n",
      "Epoch 81, Training Loss: 0.8361761950745302\n",
      "Epoch 82, Training Loss: 0.834926068572437\n",
      "Epoch 83, Training Loss: 0.833887951444177\n",
      "Epoch 84, Training Loss: 0.8328685595708735\n",
      "Epoch 85, Training Loss: 0.8320244251980501\n",
      "Epoch 86, Training Loss: 0.831000104301116\n",
      "Epoch 87, Training Loss: 0.830208184929455\n",
      "Epoch 88, Training Loss: 0.8294459875892191\n",
      "Epoch 89, Training Loss: 0.8286988991849563\n",
      "Epoch 90, Training Loss: 0.827832283412709\n",
      "Epoch 91, Training Loss: 0.8272153444851146\n",
      "Epoch 92, Training Loss: 0.8264578294052797\n",
      "Epoch 93, Training Loss: 0.8259017740277683\n",
      "Epoch 94, Training Loss: 0.8252842919966754\n",
      "Epoch 95, Training Loss: 0.8246971093205845\n",
      "Epoch 96, Training Loss: 0.8241211712360382\n",
      "Epoch 97, Training Loss: 0.8235045505972469\n",
      "Epoch 98, Training Loss: 0.822982060137917\n",
      "Epoch 99, Training Loss: 0.8223508691086489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:40:38,277] Trial 201 finished with value: 0.6213333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8218748610861161\n",
      "Epoch 1, Training Loss: 1.0796345689242943\n",
      "Epoch 2, Training Loss: 1.044920777916012\n",
      "Epoch 3, Training Loss: 1.0212220389143865\n",
      "Epoch 4, Training Loss: 1.003413943061255\n",
      "Epoch 5, Training Loss: 0.9894047863501355\n",
      "Epoch 6, Training Loss: 0.9792669105350523\n",
      "Epoch 7, Training Loss: 0.971730369284637\n",
      "Epoch 8, Training Loss: 0.9660566793348556\n",
      "Epoch 9, Training Loss: 0.9624184850463293\n",
      "Epoch 10, Training Loss: 0.9592440501191563\n",
      "Epoch 11, Training Loss: 0.9576798084983252\n",
      "Epoch 12, Training Loss: 0.9556318031217819\n",
      "Epoch 13, Training Loss: 0.9543915379316287\n",
      "Epoch 14, Training Loss: 0.9529827127779337\n",
      "Epoch 15, Training Loss: 0.9517515219243845\n",
      "Epoch 16, Training Loss: 0.950733500763886\n",
      "Epoch 17, Training Loss: 0.949696894158098\n",
      "Epoch 18, Training Loss: 0.9489582836179805\n",
      "Epoch 19, Training Loss: 0.9474958953104521\n",
      "Epoch 20, Training Loss: 0.9467109921283292\n",
      "Epoch 21, Training Loss: 0.9458340361602324\n",
      "Epoch 22, Training Loss: 0.9444601540278671\n",
      "Epoch 23, Training Loss: 0.9436789699963161\n",
      "Epoch 24, Training Loss: 0.943062649006234\n",
      "Epoch 25, Training Loss: 0.9414262323451221\n",
      "Epoch 26, Training Loss: 0.9403132107024802\n",
      "Epoch 27, Training Loss: 0.9397597763771401\n",
      "Epoch 28, Training Loss: 0.9382441832606954\n",
      "Epoch 29, Training Loss: 0.9375323050900509\n",
      "Epoch 30, Training Loss: 0.9361237706098341\n",
      "Epoch 31, Training Loss: 0.9351918968939243\n",
      "Epoch 32, Training Loss: 0.9341617651451799\n",
      "Epoch 33, Training Loss: 0.932904179024517\n",
      "Epoch 34, Training Loss: 0.9317239795412336\n",
      "Epoch 35, Training Loss: 0.9306042474911626\n",
      "Epoch 36, Training Loss: 0.9295238451850145\n",
      "Epoch 37, Training Loss: 0.9284319299504273\n",
      "Epoch 38, Training Loss: 0.9270187929160613\n",
      "Epoch 39, Training Loss: 0.9257892860505814\n",
      "Epoch 40, Training Loss: 0.9246082536259989\n",
      "Epoch 41, Training Loss: 0.9229520106674137\n",
      "Epoch 42, Training Loss: 0.9221581290539046\n",
      "Epoch 43, Training Loss: 0.9202712830744292\n",
      "Epoch 44, Training Loss: 0.9189045091320698\n",
      "Epoch 45, Training Loss: 0.9175539645037256\n",
      "Epoch 46, Training Loss: 0.91642041475253\n",
      "Epoch 47, Training Loss: 0.9145675947791652\n",
      "Epoch 48, Training Loss: 0.9132341522023194\n",
      "Epoch 49, Training Loss: 0.9119040180865984\n",
      "Epoch 50, Training Loss: 0.910400873019283\n",
      "Epoch 51, Training Loss: 0.9086214075411173\n",
      "Epoch 52, Training Loss: 0.9074770106408829\n",
      "Epoch 53, Training Loss: 0.9055505890595286\n",
      "Epoch 54, Training Loss: 0.903570198295708\n",
      "Epoch 55, Training Loss: 0.90239172537524\n",
      "Epoch 56, Training Loss: 0.9003612657238667\n",
      "Epoch 57, Training Loss: 0.8987566408358122\n",
      "Epoch 58, Training Loss: 0.8973610946110316\n",
      "Epoch 59, Training Loss: 0.8951116275070305\n",
      "Epoch 60, Training Loss: 0.8935937617954455\n",
      "Epoch 61, Training Loss: 0.8918445161410741\n",
      "Epoch 62, Training Loss: 0.889807335057653\n",
      "Epoch 63, Training Loss: 0.8880324568067278\n",
      "Epoch 64, Training Loss: 0.8863632849284581\n",
      "Epoch 65, Training Loss: 0.8842369404054226\n",
      "Epoch 66, Training Loss: 0.8821694296105463\n",
      "Epoch 67, Training Loss: 0.880648057891014\n",
      "Epoch 68, Training Loss: 0.8785496886511495\n",
      "Epoch 69, Training Loss: 0.8767919840669274\n",
      "Epoch 70, Training Loss: 0.8746277227437586\n",
      "Epoch 71, Training Loss: 0.8731918610128245\n",
      "Epoch 72, Training Loss: 0.8715904172201802\n",
      "Epoch 73, Training Loss: 0.8690601854396046\n",
      "Epoch 74, Training Loss: 0.8676173820531458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:41:47,766] Trial 202 finished with value: 0.5974 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8657691606901642\n",
      "Epoch 1, Training Loss: 1.0678686607152896\n",
      "Epoch 2, Training Loss: 1.027942757230056\n",
      "Epoch 3, Training Loss: 1.002512284239432\n",
      "Epoch 4, Training Loss: 0.9855985719458501\n",
      "Epoch 5, Training Loss: 0.9750617271975467\n",
      "Epoch 6, Training Loss: 0.9681315439984315\n",
      "Epoch 7, Training Loss: 0.9641429042457638\n",
      "Epoch 8, Training Loss: 0.9614643046730443\n",
      "Epoch 9, Training Loss: 0.9600023366454848\n",
      "Epoch 10, Training Loss: 0.9585828004026772\n",
      "Epoch 11, Training Loss: 0.9577454840330253\n",
      "Epoch 12, Training Loss: 0.9563558092690948\n",
      "Epoch 13, Training Loss: 0.9552801764101014\n",
      "Epoch 14, Training Loss: 0.9541051593041958\n",
      "Epoch 15, Training Loss: 0.9537486388271016\n",
      "Epoch 16, Training Loss: 0.9530104942787859\n",
      "Epoch 17, Training Loss: 0.9517676404544285\n",
      "Epoch 18, Training Loss: 0.9507763793593959\n",
      "Epoch 19, Training Loss: 0.9500066630822376\n",
      "Epoch 20, Training Loss: 0.948738503904271\n",
      "Epoch 21, Training Loss: 0.947949060730468\n",
      "Epoch 22, Training Loss: 0.947167131954566\n",
      "Epoch 23, Training Loss: 0.945832003417768\n",
      "Epoch 24, Training Loss: 0.9449839964845127\n",
      "Epoch 25, Training Loss: 0.9437351634627894\n",
      "Epoch 26, Training Loss: 0.9428664311430508\n",
      "Epoch 27, Training Loss: 0.9419175421384941\n",
      "Epoch 28, Training Loss: 0.9408149547146675\n",
      "Epoch 29, Training Loss: 0.9396654271541681\n",
      "Epoch 30, Training Loss: 0.9380916004790399\n",
      "Epoch 31, Training Loss: 0.9374205785586421\n",
      "Epoch 32, Training Loss: 0.9358564862631318\n",
      "Epoch 33, Training Loss: 0.9347608814562174\n",
      "Epoch 34, Training Loss: 0.9334168058588989\n",
      "Epoch 35, Training Loss: 0.9319200992584229\n",
      "Epoch 36, Training Loss: 0.930658961866135\n",
      "Epoch 37, Training Loss: 0.9294806503711787\n",
      "Epoch 38, Training Loss: 0.927830597959963\n",
      "Epoch 39, Training Loss: 0.926887657767848\n",
      "Epoch 40, Training Loss: 0.9254327018458144\n",
      "Epoch 41, Training Loss: 0.9236967418426858\n",
      "Epoch 42, Training Loss: 0.9219803125338447\n",
      "Epoch 43, Training Loss: 0.9203719969082595\n",
      "Epoch 44, Training Loss: 0.9191010683999026\n",
      "Epoch 45, Training Loss: 0.9172625904692743\n",
      "Epoch 46, Training Loss: 0.9160276691716417\n",
      "Epoch 47, Training Loss: 0.9143721387798625\n",
      "Epoch 48, Training Loss: 0.9121801075182463\n",
      "Epoch 49, Training Loss: 0.9115864229381533\n",
      "Epoch 50, Training Loss: 0.9093422506088601\n",
      "Epoch 51, Training Loss: 0.9073013945629722\n",
      "Epoch 52, Training Loss: 0.9059947562396975\n",
      "Epoch 53, Training Loss: 0.904841899602933\n",
      "Epoch 54, Training Loss: 0.9025487361994005\n",
      "Epoch 55, Training Loss: 0.9006780109907452\n",
      "Epoch 56, Training Loss: 0.8988779113704997\n",
      "Epoch 57, Training Loss: 0.897257659847575\n",
      "Epoch 58, Training Loss: 0.8952853017283562\n",
      "Epoch 59, Training Loss: 0.8935931804484891\n",
      "Epoch 60, Training Loss: 0.8915157423879867\n",
      "Epoch 61, Training Loss: 0.8899743746994133\n",
      "Epoch 62, Training Loss: 0.888511235731885\n",
      "Epoch 63, Training Loss: 0.8866316979989073\n",
      "Epoch 64, Training Loss: 0.8844926486337992\n",
      "Epoch 65, Training Loss: 0.8828050119536264\n",
      "Epoch 66, Training Loss: 0.8812503470514054\n",
      "Epoch 67, Training Loss: 0.8796944232811605\n",
      "Epoch 68, Training Loss: 0.8776616666550027\n",
      "Epoch 69, Training Loss: 0.8758948319836667\n",
      "Epoch 70, Training Loss: 0.8747586249408865\n",
      "Epoch 71, Training Loss: 0.8735132575931406\n",
      "Epoch 72, Training Loss: 0.8710830733292085\n",
      "Epoch 73, Training Loss: 0.8694764754825965\n",
      "Epoch 74, Training Loss: 0.8680920299730803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:42:50,369] Trial 203 finished with value: 0.5938666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8662117695449887\n",
      "Epoch 1, Training Loss: 1.0764735144780095\n",
      "Epoch 2, Training Loss: 1.038362251098891\n",
      "Epoch 3, Training Loss: 1.0186127682377522\n",
      "Epoch 4, Training Loss: 1.0041561007499695\n",
      "Epoch 5, Training Loss: 0.9931606665589756\n",
      "Epoch 6, Training Loss: 0.9841502146613329\n",
      "Epoch 7, Training Loss: 0.9771408980950377\n",
      "Epoch 8, Training Loss: 0.971255498273032\n",
      "Epoch 9, Training Loss: 0.9663063724238173\n",
      "Epoch 10, Training Loss: 0.9628265021438885\n",
      "Epoch 11, Training Loss: 0.9592815264723354\n",
      "Epoch 12, Training Loss: 0.9566539255299963\n",
      "Epoch 13, Training Loss: 0.9542832709792861\n",
      "Epoch 14, Training Loss: 0.9522457590676788\n",
      "Epoch 15, Training Loss: 0.9505810293936192\n",
      "Epoch 16, Training Loss: 0.9491220260921277\n",
      "Epoch 17, Training Loss: 0.9471135872647278\n",
      "Epoch 18, Training Loss: 0.9458860480695739\n",
      "Epoch 19, Training Loss: 0.9449817577699073\n",
      "Epoch 20, Training Loss: 0.9436047433910514\n",
      "Epoch 21, Training Loss: 0.9422365706666072\n",
      "Epoch 22, Training Loss: 0.941491561007679\n",
      "Epoch 23, Training Loss: 0.9405347788244262\n",
      "Epoch 24, Training Loss: 0.9401802797962848\n",
      "Epoch 25, Training Loss: 0.9385725181801875\n",
      "Epoch 26, Training Loss: 0.9378020644187928\n",
      "Epoch 27, Training Loss: 0.9367922225392851\n",
      "Epoch 28, Training Loss: 0.9364323323830626\n",
      "Epoch 29, Training Loss: 0.9354877116984891\n",
      "Epoch 30, Training Loss: 0.934722688233942\n",
      "Epoch 31, Training Loss: 0.9342746659329063\n",
      "Epoch 32, Training Loss: 0.9332143402637396\n",
      "Epoch 33, Training Loss: 0.9328203492594841\n",
      "Epoch 34, Training Loss: 0.9320101676130653\n",
      "Epoch 35, Training Loss: 0.9314781396908868\n",
      "Epoch 36, Training Loss: 0.9303103953375852\n",
      "Epoch 37, Training Loss: 0.929929680752575\n",
      "Epoch 38, Training Loss: 0.92907027839718\n",
      "Epoch 39, Training Loss: 0.9290527573205475\n",
      "Epoch 40, Training Loss: 0.9279998296185543\n",
      "Epoch 41, Training Loss: 0.9278470161265897\n",
      "Epoch 42, Training Loss: 0.9271595811485348\n",
      "Epoch 43, Training Loss: 0.9263023993126431\n",
      "Epoch 44, Training Loss: 0.9257525608951884\n",
      "Epoch 45, Training Loss: 0.925238994010409\n",
      "Epoch 46, Training Loss: 0.92434722624327\n",
      "Epoch 47, Training Loss: 0.9239742151776651\n",
      "Epoch 48, Training Loss: 0.923359683850654\n",
      "Epoch 49, Training Loss: 0.9232254731027704\n",
      "Epoch 50, Training Loss: 0.9222084207642347\n",
      "Epoch 51, Training Loss: 0.9220436285312911\n",
      "Epoch 52, Training Loss: 0.9217648940875118\n",
      "Epoch 53, Training Loss: 0.9212970227227175\n",
      "Epoch 54, Training Loss: 0.9209903014333625\n",
      "Epoch 55, Training Loss: 0.9198227621558913\n",
      "Epoch 56, Training Loss: 0.9195121515962414\n",
      "Epoch 57, Training Loss: 0.9188074137035169\n",
      "Epoch 58, Training Loss: 0.9180374891238106\n",
      "Epoch 59, Training Loss: 0.9177664659973374\n",
      "Epoch 60, Training Loss: 0.9170543991533437\n",
      "Epoch 61, Training Loss: 0.9167659947746678\n",
      "Epoch 62, Training Loss: 0.916164137187757\n",
      "Epoch 63, Training Loss: 0.9160337909719998\n",
      "Epoch 64, Training Loss: 0.9155409775282207\n",
      "Epoch 65, Training Loss: 0.9152822481958489\n",
      "Epoch 66, Training Loss: 0.9146743777999304\n",
      "Epoch 67, Training Loss: 0.9135059197146194\n",
      "Epoch 68, Training Loss: 0.9128945457307915\n",
      "Epoch 69, Training Loss: 0.9130226078786348\n",
      "Epoch 70, Training Loss: 0.9121669042379337\n",
      "Epoch 71, Training Loss: 0.9118826267414524\n",
      "Epoch 72, Training Loss: 0.911605657760362\n",
      "Epoch 73, Training Loss: 0.910841535087815\n",
      "Epoch 74, Training Loss: 0.910379924092974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:43:53,400] Trial 204 finished with value: 0.5653333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9097627750016692\n",
      "Epoch 1, Training Loss: 0.8605858973094396\n",
      "Epoch 2, Training Loss: 0.815211314097383\n",
      "Epoch 3, Training Loss: 0.8094923397652188\n",
      "Epoch 4, Training Loss: 0.8046516764432864\n",
      "Epoch 5, Training Loss: 0.8036381365661335\n",
      "Epoch 6, Training Loss: 0.8004280504427458\n",
      "Epoch 7, Training Loss: 0.8003365865327362\n",
      "Epoch 8, Training Loss: 0.8001010307692047\n",
      "Epoch 9, Training Loss: 0.7974397510514224\n",
      "Epoch 10, Training Loss: 0.7980259965236922\n",
      "Epoch 11, Training Loss: 0.7976550246539869\n",
      "Epoch 12, Training Loss: 0.7962443795419277\n",
      "Epoch 13, Training Loss: 0.7963562284197127\n",
      "Epoch 14, Training Loss: 0.795539821359448\n",
      "Epoch 15, Training Loss: 0.7947762342323934\n",
      "Epoch 16, Training Loss: 0.7950246166465874\n",
      "Epoch 17, Training Loss: 0.794294952360311\n",
      "Epoch 18, Training Loss: 0.7946180458355667\n",
      "Epoch 19, Training Loss: 0.7941800423134538\n",
      "Epoch 20, Training Loss: 0.7942330677706496\n",
      "Epoch 21, Training Loss: 0.7931078183023553\n",
      "Epoch 22, Training Loss: 0.7931903381096689\n",
      "Epoch 23, Training Loss: 0.7932321697249448\n",
      "Epoch 24, Training Loss: 0.7927303477337486\n",
      "Epoch 25, Training Loss: 0.7924325521727253\n",
      "Epoch 26, Training Loss: 0.7932224141924005\n",
      "Epoch 27, Training Loss: 0.7933973859127302\n",
      "Epoch 28, Training Loss: 0.7917286966080056\n",
      "Epoch 29, Training Loss: 0.7924328097723481\n",
      "Epoch 30, Training Loss: 0.7923258400501165\n",
      "Epoch 31, Training Loss: 0.7922545317420386\n",
      "Epoch 32, Training Loss: 0.7922172156491674\n",
      "Epoch 33, Training Loss: 0.7924502120878464\n",
      "Epoch 34, Training Loss: 0.7920044294873575\n",
      "Epoch 35, Training Loss: 0.7910576853088866\n",
      "Epoch 36, Training Loss: 0.7909737207835779\n",
      "Epoch 37, Training Loss: 0.7913153573982697\n",
      "Epoch 38, Training Loss: 0.791235303609891\n",
      "Epoch 39, Training Loss: 0.7911276958042518\n",
      "Epoch 40, Training Loss: 0.7918353649010336\n",
      "Epoch 41, Training Loss: 0.7919333767173882\n",
      "Epoch 42, Training Loss: 0.7902109315968994\n",
      "Epoch 43, Training Loss: 0.7905216211663153\n",
      "Epoch 44, Training Loss: 0.7902416485592835\n",
      "Epoch 45, Training Loss: 0.7897078646304913\n",
      "Epoch 46, Training Loss: 0.7908201494611296\n",
      "Epoch 47, Training Loss: 0.7903848561129175\n",
      "Epoch 48, Training Loss: 0.7913227116254936\n",
      "Epoch 49, Training Loss: 0.7906207505025362\n",
      "Epoch 50, Training Loss: 0.7903696941253834\n",
      "Epoch 51, Training Loss: 0.7901481973497491\n",
      "Epoch 52, Training Loss: 0.7906610514884604\n",
      "Epoch 53, Training Loss: 0.7903260931036526\n",
      "Epoch 54, Training Loss: 0.78948175306607\n",
      "Epoch 55, Training Loss: 0.7892818065962397\n",
      "Epoch 56, Training Loss: 0.7896615865535306\n",
      "Epoch 57, Training Loss: 0.788972717702837\n",
      "Epoch 58, Training Loss: 0.7899494549385587\n",
      "Epoch 59, Training Loss: 0.7906935621024971\n",
      "Epoch 60, Training Loss: 0.789560233919244\n",
      "Epoch 61, Training Loss: 0.7895176419638154\n",
      "Epoch 62, Training Loss: 0.7898430187003057\n",
      "Epoch 63, Training Loss: 0.7897970144013713\n",
      "Epoch 64, Training Loss: 0.7903690641087697\n",
      "Epoch 65, Training Loss: 0.7902498555362673\n",
      "Epoch 66, Training Loss: 0.789777237938759\n",
      "Epoch 67, Training Loss: 0.7889467498413603\n",
      "Epoch 68, Training Loss: 0.7888912399012343\n",
      "Epoch 69, Training Loss: 0.7900288321918114\n",
      "Epoch 70, Training Loss: 0.7897904396953439\n",
      "Epoch 71, Training Loss: 0.789598928896108\n",
      "Epoch 72, Training Loss: 0.7904749513568735\n",
      "Epoch 73, Training Loss: 0.7893919139876402\n",
      "Epoch 74, Training Loss: 0.7888275602706393\n",
      "Epoch 75, Training Loss: 0.7888893783540654\n",
      "Epoch 76, Training Loss: 0.7903070971481783\n",
      "Epoch 77, Training Loss: 0.7886968272072928\n",
      "Epoch 78, Training Loss: 0.7895377033635189\n",
      "Epoch 79, Training Loss: 0.7893359473773411\n",
      "Epoch 80, Training Loss: 0.7895986447656961\n",
      "Epoch 81, Training Loss: 0.7893403527431918\n",
      "Epoch 82, Training Loss: 0.7892095520980376\n",
      "Epoch 83, Training Loss: 0.7893645490022531\n",
      "Epoch 84, Training Loss: 0.7887866040817777\n",
      "Epoch 85, Training Loss: 0.7896724706305597\n",
      "Epoch 86, Training Loss: 0.7897229857910845\n",
      "Epoch 87, Training Loss: 0.7898679694735018\n",
      "Epoch 88, Training Loss: 0.7886483092953388\n",
      "Epoch 89, Training Loss: 0.7892143020952554\n",
      "Epoch 90, Training Loss: 0.78948820874207\n",
      "Epoch 91, Training Loss: 0.7895638951681611\n",
      "Epoch 92, Training Loss: 0.7890811066878469\n",
      "Epoch 93, Training Loss: 0.7880964814272142\n",
      "Epoch 94, Training Loss: 0.7884674710438664\n",
      "Epoch 95, Training Loss: 0.7892399909801053\n",
      "Epoch 96, Training Loss: 0.7887060669131745\n",
      "Epoch 97, Training Loss: 0.7895247208444696\n",
      "Epoch 98, Training Loss: 0.7899021300158107\n",
      "Epoch 99, Training Loss: 0.7891043365449834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:45:48,022] Trial 205 finished with value: 0.6296 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7881720390534939\n",
      "Epoch 1, Training Loss: 1.0401880195266322\n",
      "Epoch 2, Training Loss: 0.9876112392074183\n",
      "Epoch 3, Training Loss: 0.9665038883237911\n",
      "Epoch 4, Training Loss: 0.9589158557411424\n",
      "Epoch 5, Training Loss: 0.9557900478069047\n",
      "Epoch 6, Training Loss: 0.9535210661422041\n",
      "Epoch 7, Training Loss: 0.9515663598713122\n",
      "Epoch 8, Training Loss: 0.9496195846930482\n",
      "Epoch 9, Training Loss: 0.9471703233575463\n",
      "Epoch 10, Training Loss: 0.9452079586516645\n",
      "Epoch 11, Training Loss: 0.9435156898390978\n",
      "Epoch 12, Training Loss: 0.9410068364967977\n",
      "Epoch 13, Training Loss: 0.938583498969114\n",
      "Epoch 14, Training Loss: 0.9364009776509794\n",
      "Epoch 15, Training Loss: 0.9338498568176327\n",
      "Epoch 16, Training Loss: 0.9307592287995762\n",
      "Epoch 17, Training Loss: 0.9280664591860951\n",
      "Epoch 18, Training Loss: 0.925336309841701\n",
      "Epoch 19, Training Loss: 0.9223590625856156\n",
      "Epoch 20, Training Loss: 0.9197902725155193\n",
      "Epoch 21, Training Loss: 0.9157830082384267\n",
      "Epoch 22, Training Loss: 0.9127519240952973\n",
      "Epoch 23, Training Loss: 0.9094436293257806\n",
      "Epoch 24, Training Loss: 0.9063313820308312\n",
      "Epoch 25, Training Loss: 0.9026268129061935\n",
      "Epoch 26, Training Loss: 0.8986949273518153\n",
      "Epoch 27, Training Loss: 0.895263823889252\n",
      "Epoch 28, Training Loss: 0.8916712665020075\n",
      "Epoch 29, Training Loss: 0.8880831270289601\n",
      "Epoch 30, Training Loss: 0.8839188616078599\n",
      "Epoch 31, Training Loss: 0.8806979096025452\n",
      "Epoch 32, Training Loss: 0.8771957674420866\n",
      "Epoch 33, Training Loss: 0.8731610497137657\n",
      "Epoch 34, Training Loss: 0.8695109427423405\n",
      "Epoch 35, Training Loss: 0.8664374546000831\n",
      "Epoch 36, Training Loss: 0.8632060102054051\n",
      "Epoch 37, Training Loss: 0.859666615500486\n",
      "Epoch 38, Training Loss: 0.8565682789436857\n",
      "Epoch 39, Training Loss: 0.8533250917169385\n",
      "Epoch 40, Training Loss: 0.8505319054861714\n",
      "Epoch 41, Training Loss: 0.8480557740182805\n",
      "Epoch 42, Training Loss: 0.8456034352008561\n",
      "Epoch 43, Training Loss: 0.8429552979935381\n",
      "Epoch 44, Training Loss: 0.8401647206535913\n",
      "Epoch 45, Training Loss: 0.8379573424059645\n",
      "Epoch 46, Training Loss: 0.8363988034707263\n",
      "Epoch 47, Training Loss: 0.8338374175523456\n",
      "Epoch 48, Training Loss: 0.8321203195959106\n",
      "Epoch 49, Training Loss: 0.8299052069061681\n",
      "Epoch 50, Training Loss: 0.8285520256910109\n",
      "Epoch 51, Training Loss: 0.8264253117984399\n",
      "Epoch 52, Training Loss: 0.8258151711377882\n",
      "Epoch 53, Training Loss: 0.8241755317924614\n",
      "Epoch 54, Training Loss: 0.8227487871521397\n",
      "Epoch 55, Training Loss: 0.8216142922415769\n",
      "Epoch 56, Training Loss: 0.8203641696980125\n",
      "Epoch 57, Training Loss: 0.8202964619586343\n",
      "Epoch 58, Training Loss: 0.8187679004848452\n",
      "Epoch 59, Training Loss: 0.8177854129246303\n",
      "Epoch 60, Training Loss: 0.817401945859866\n",
      "Epoch 61, Training Loss: 0.816314455172173\n",
      "Epoch 62, Training Loss: 0.8161039457285315\n",
      "Epoch 63, Training Loss: 0.8155288245444907\n",
      "Epoch 64, Training Loss: 0.8144945239662228\n",
      "Epoch 65, Training Loss: 0.8143526909046603\n",
      "Epoch 66, Training Loss: 0.8139911028675567\n",
      "Epoch 67, Training Loss: 0.8136586278901065\n",
      "Epoch 68, Training Loss: 0.8129822123319583\n",
      "Epoch 69, Training Loss: 0.8128012787130542\n",
      "Epoch 70, Training Loss: 0.8125409881871446\n",
      "Epoch 71, Training Loss: 0.8120857736221829\n",
      "Epoch 72, Training Loss: 0.8116654222172902\n",
      "Epoch 73, Training Loss: 0.8112785351007504\n",
      "Epoch 74, Training Loss: 0.8114688939617989\n",
      "Epoch 75, Training Loss: 0.8109014930581688\n",
      "Epoch 76, Training Loss: 0.8108764172496652\n",
      "Epoch 77, Training Loss: 0.8108199053240898\n",
      "Epoch 78, Training Loss: 0.8109327428322985\n",
      "Epoch 79, Training Loss: 0.8105994395743635\n",
      "Epoch 80, Training Loss: 0.8104808469464008\n",
      "Epoch 81, Training Loss: 0.8105505216390567\n",
      "Epoch 82, Training Loss: 0.8096731285403546\n",
      "Epoch 83, Training Loss: 0.8098013064915076\n",
      "Epoch 84, Training Loss: 0.809862346577465\n",
      "Epoch 85, Training Loss: 0.8092754455437338\n",
      "Epoch 86, Training Loss: 0.8098897577228402\n",
      "Epoch 87, Training Loss: 0.8098355657175967\n",
      "Epoch 88, Training Loss: 0.8086816385724491\n",
      "Epoch 89, Training Loss: 0.8093083999210731\n",
      "Epoch 90, Training Loss: 0.8090735925767655\n",
      "Epoch 91, Training Loss: 0.8089484999054357\n",
      "Epoch 92, Training Loss: 0.8086509234026859\n",
      "Epoch 93, Training Loss: 0.8085010947141432\n",
      "Epoch 94, Training Loss: 0.8085916427741373\n",
      "Epoch 95, Training Loss: 0.8085530646761557\n",
      "Epoch 96, Training Loss: 0.8085100735040536\n",
      "Epoch 97, Training Loss: 0.8090018405053848\n",
      "Epoch 98, Training Loss: 0.8083366570616127\n",
      "Epoch 99, Training Loss: 0.8081484252348878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:47:11,690] Trial 206 finished with value: 0.6304666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8082972370592275\n",
      "Epoch 1, Training Loss: 1.091438409260341\n",
      "Epoch 2, Training Loss: 1.088424221017307\n",
      "Epoch 3, Training Loss: 1.0860275648590316\n",
      "Epoch 4, Training Loss: 1.0831677884983837\n",
      "Epoch 5, Training Loss: 1.0801540247479775\n",
      "Epoch 6, Training Loss: 1.076737250242018\n",
      "Epoch 7, Training Loss: 1.0724738079802434\n",
      "Epoch 8, Training Loss: 1.067606728596795\n",
      "Epoch 9, Training Loss: 1.0613679767551278\n",
      "Epoch 10, Training Loss: 1.0543576263843621\n",
      "Epoch 11, Training Loss: 1.04570984320533\n",
      "Epoch 12, Training Loss: 1.035931856829421\n",
      "Epoch 13, Training Loss: 1.0251674624313984\n",
      "Epoch 14, Training Loss: 1.0140673181168118\n",
      "Epoch 15, Training Loss: 1.0030871474653258\n",
      "Epoch 16, Training Loss: 0.9934571391657779\n",
      "Epoch 17, Training Loss: 0.985299421521954\n",
      "Epoch 18, Training Loss: 0.9785870993047728\n",
      "Epoch 19, Training Loss: 0.9735229021624515\n",
      "Epoch 20, Training Loss: 0.9699528873414922\n",
      "Epoch 21, Training Loss: 0.966866600065303\n",
      "Epoch 22, Training Loss: 0.9642877519578862\n",
      "Epoch 23, Training Loss: 0.9625646377864637\n",
      "Epoch 24, Training Loss: 0.9613816841204363\n",
      "Epoch 25, Training Loss: 0.9598103347577547\n",
      "Epoch 26, Training Loss: 0.9585272761215841\n",
      "Epoch 27, Training Loss: 0.9576906888108504\n",
      "Epoch 28, Training Loss: 0.9569423812672608\n",
      "Epoch 29, Training Loss: 0.9567265826060359\n",
      "Epoch 30, Training Loss: 0.955694109784033\n",
      "Epoch 31, Training Loss: 0.954696367916308\n",
      "Epoch 32, Training Loss: 0.9545240725789751\n",
      "Epoch 33, Training Loss: 0.9535424000338504\n",
      "Epoch 34, Training Loss: 0.9533225547102161\n",
      "Epoch 35, Training Loss: 0.9526370412424991\n",
      "Epoch 36, Training Loss: 0.9517086513060377\n",
      "Epoch 37, Training Loss: 0.9508927025293049\n",
      "Epoch 38, Training Loss: 0.9504232996388485\n",
      "Epoch 39, Training Loss: 0.949593965092996\n",
      "Epoch 40, Training Loss: 0.9491735184999337\n",
      "Epoch 41, Training Loss: 0.9480148447187323\n",
      "Epoch 42, Training Loss: 0.9478752096792809\n",
      "Epoch 43, Training Loss: 0.9467334627208853\n",
      "Epoch 44, Training Loss: 0.9463666009723692\n",
      "Epoch 45, Training Loss: 0.9456025232946066\n",
      "Epoch 46, Training Loss: 0.9450005096600468\n",
      "Epoch 47, Training Loss: 0.9435014535609941\n",
      "Epoch 48, Training Loss: 0.942755253064005\n",
      "Epoch 49, Training Loss: 0.9417844411125756\n",
      "Epoch 50, Training Loss: 0.9410025122470426\n",
      "Epoch 51, Training Loss: 0.9405110955238343\n",
      "Epoch 52, Training Loss: 0.9391123526974728\n",
      "Epoch 53, Training Loss: 0.9377146683241192\n",
      "Epoch 54, Training Loss: 0.9372178371687581\n",
      "Epoch 55, Training Loss: 0.9361803317428532\n",
      "Epoch 56, Training Loss: 0.9346298405102321\n",
      "Epoch 57, Training Loss: 0.9336747246577327\n",
      "Epoch 58, Training Loss: 0.9320443303961503\n",
      "Epoch 59, Training Loss: 0.9309062964037845\n",
      "Epoch 60, Training Loss: 0.9300551822311\n",
      "Epoch 61, Training Loss: 0.9281073055769268\n",
      "Epoch 62, Training Loss: 0.9269758597352451\n",
      "Epoch 63, Training Loss: 0.9252674252467048\n",
      "Epoch 64, Training Loss: 0.9238166895127834\n",
      "Epoch 65, Training Loss: 0.9220081621542909\n",
      "Epoch 66, Training Loss: 0.9207370941800282\n",
      "Epoch 67, Training Loss: 0.9190636199219783\n",
      "Epoch 68, Training Loss: 0.9172427082420291\n",
      "Epoch 69, Training Loss: 0.9153650665641727\n",
      "Epoch 70, Training Loss: 0.9135360285751801\n",
      "Epoch 71, Training Loss: 0.9113845899589079\n",
      "Epoch 72, Training Loss: 0.9094084312144975\n",
      "Epoch 73, Training Loss: 0.907386190909192\n",
      "Epoch 74, Training Loss: 0.9048250236905607\n",
      "Epoch 75, Training Loss: 0.902817321988873\n",
      "Epoch 76, Training Loss: 0.8997803768717256\n",
      "Epoch 77, Training Loss: 0.8979931153749164\n",
      "Epoch 78, Training Loss: 0.8950120344197839\n",
      "Epoch 79, Training Loss: 0.8930657263088944\n",
      "Epoch 80, Training Loss: 0.8897041206969354\n",
      "Epoch 81, Training Loss: 0.8872734440896745\n",
      "Epoch 82, Training Loss: 0.8842787540944895\n",
      "Epoch 83, Training Loss: 0.8820215547891488\n",
      "Epoch 84, Training Loss: 0.8791794337724385\n",
      "Epoch 85, Training Loss: 0.8766497709697351\n",
      "Epoch 86, Training Loss: 0.8738070245972254\n",
      "Epoch 87, Training Loss: 0.8709934174566341\n",
      "Epoch 88, Training Loss: 0.8681657936339988\n",
      "Epoch 89, Training Loss: 0.8654724019810669\n",
      "Epoch 90, Training Loss: 0.8629791715987642\n",
      "Epoch 91, Training Loss: 0.8602734031533836\n",
      "Epoch 92, Training Loss: 0.8577754880252637\n",
      "Epoch 93, Training Loss: 0.8552489130120529\n",
      "Epoch 94, Training Loss: 0.8528484590071485\n",
      "Epoch 95, Training Loss: 0.8503599484164016\n",
      "Epoch 96, Training Loss: 0.8479343938648253\n",
      "Epoch 97, Training Loss: 0.8460683317112744\n",
      "Epoch 98, Training Loss: 0.8439143976770845\n",
      "Epoch 99, Training Loss: 0.8419916627998639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:48:44,110] Trial 207 finished with value: 0.6055333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8400395826289528\n",
      "Epoch 1, Training Loss: 0.9955984741225279\n",
      "Epoch 2, Training Loss: 0.9509251862540281\n",
      "Epoch 3, Training Loss: 0.9397263620132791\n",
      "Epoch 4, Training Loss: 0.9289848797303394\n",
      "Epoch 5, Training Loss: 0.9155833624359361\n",
      "Epoch 6, Training Loss: 0.9004246128232856\n",
      "Epoch 7, Training Loss: 0.8830641477627862\n",
      "Epoch 8, Training Loss: 0.8652749186171624\n",
      "Epoch 9, Training Loss: 0.8496013673624598\n",
      "Epoch 10, Training Loss: 0.8381781907009899\n",
      "Epoch 11, Training Loss: 0.8294960004046448\n",
      "Epoch 12, Training Loss: 0.8236753889940736\n",
      "Epoch 13, Training Loss: 0.8213282581558801\n",
      "Epoch 14, Training Loss: 0.8174707541788431\n",
      "Epoch 15, Training Loss: 0.8157536752241895\n",
      "Epoch 16, Training Loss: 0.8141826911080152\n",
      "Epoch 17, Training Loss: 0.8132444671222142\n",
      "Epoch 18, Training Loss: 0.8121953639769016\n",
      "Epoch 19, Training Loss: 0.8115668744969189\n",
      "Epoch 20, Training Loss: 0.8104423784671869\n",
      "Epoch 21, Training Loss: 0.8097608465001099\n",
      "Epoch 22, Training Loss: 0.8092318608348531\n",
      "Epoch 23, Training Loss: 0.8081737632141974\n",
      "Epoch 24, Training Loss: 0.8081812679319453\n",
      "Epoch 25, Training Loss: 0.8075574981538873\n",
      "Epoch 26, Training Loss: 0.8068097546584624\n",
      "Epoch 27, Training Loss: 0.806559013244801\n",
      "Epoch 28, Training Loss: 0.8061165502196864\n",
      "Epoch 29, Training Loss: 0.806221866518035\n",
      "Epoch 30, Training Loss: 0.8056776400795557\n",
      "Epoch 31, Training Loss: 0.8049674101341936\n",
      "Epoch 32, Training Loss: 0.8045419174029415\n",
      "Epoch 33, Training Loss: 0.8040457604522991\n",
      "Epoch 34, Training Loss: 0.8032754517139349\n",
      "Epoch 35, Training Loss: 0.8033354380076989\n",
      "Epoch 36, Training Loss: 0.8032458885271746\n",
      "Epoch 37, Training Loss: 0.8025485337228704\n",
      "Epoch 38, Training Loss: 0.8023687008628272\n",
      "Epoch 39, Training Loss: 0.801872536203915\n",
      "Epoch 40, Training Loss: 0.8018751861457538\n",
      "Epoch 41, Training Loss: 0.8020883987720747\n",
      "Epoch 42, Training Loss: 0.8013470683779035\n",
      "Epoch 43, Training Loss: 0.8008433004967253\n",
      "Epoch 44, Training Loss: 0.8008120872920617\n",
      "Epoch 45, Training Loss: 0.8000767910390868\n",
      "Epoch 46, Training Loss: 0.800336255883812\n",
      "Epoch 47, Training Loss: 0.8003867683554055\n",
      "Epoch 48, Training Loss: 0.8004578443398153\n",
      "Epoch 49, Training Loss: 0.7994950423563333\n",
      "Epoch 50, Training Loss: 0.7998193264007568\n",
      "Epoch 51, Training Loss: 0.7991713880596304\n",
      "Epoch 52, Training Loss: 0.799685852240799\n",
      "Epoch 53, Training Loss: 0.79915822529255\n",
      "Epoch 54, Training Loss: 0.7986850699087731\n",
      "Epoch 55, Training Loss: 0.7986648487865476\n",
      "Epoch 56, Training Loss: 0.7985847411299111\n",
      "Epoch 57, Training Loss: 0.7984991364909294\n",
      "Epoch 58, Training Loss: 0.798027593569648\n",
      "Epoch 59, Training Loss: 0.7982482556113624\n",
      "Epoch 60, Training Loss: 0.7989648130603303\n",
      "Epoch 61, Training Loss: 0.7981703575392415\n",
      "Epoch 62, Training Loss: 0.7979194903732243\n",
      "Epoch 63, Training Loss: 0.7984537033210123\n",
      "Epoch 64, Training Loss: 0.7981585551025276\n",
      "Epoch 65, Training Loss: 0.798104557148496\n",
      "Epoch 66, Training Loss: 0.7984419597718949\n",
      "Epoch 67, Training Loss: 0.7978934802507099\n",
      "Epoch 68, Training Loss: 0.7976042086916758\n",
      "Epoch 69, Training Loss: 0.7982786989749823\n",
      "Epoch 70, Training Loss: 0.7973020750777166\n",
      "Epoch 71, Training Loss: 0.7974877990278086\n",
      "Epoch 72, Training Loss: 0.7973038961116533\n",
      "Epoch 73, Training Loss: 0.7971204307742585\n",
      "Epoch 74, Training Loss: 0.7971760907567533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:49:53,846] Trial 208 finished with value: 0.6294666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7969539336692122\n",
      "Epoch 1, Training Loss: 0.8521793465754565\n",
      "Epoch 2, Training Loss: 0.8134473980174345\n",
      "Epoch 3, Training Loss: 0.807770851429771\n",
      "Epoch 4, Training Loss: 0.8049689522911521\n",
      "Epoch 5, Training Loss: 0.8026009863965652\n",
      "Epoch 6, Training Loss: 0.8013893812544206\n",
      "Epoch 7, Training Loss: 0.8001400492471807\n",
      "Epoch 8, Training Loss: 0.798547297225279\n",
      "Epoch 9, Training Loss: 0.7981161319508272\n",
      "Epoch 10, Training Loss: 0.7967732007363263\n",
      "Epoch 11, Training Loss: 0.7970672504340901\n",
      "Epoch 12, Training Loss: 0.7969046105356777\n",
      "Epoch 13, Training Loss: 0.7958261239528656\n",
      "Epoch 14, Training Loss: 0.7950016071515925\n",
      "Epoch 15, Training Loss: 0.7942844734472387\n",
      "Epoch 16, Training Loss: 0.7949254137628219\n",
      "Epoch 17, Training Loss: 0.7942356930059545\n",
      "Epoch 18, Training Loss: 0.7939511262669283\n",
      "Epoch 19, Training Loss: 0.793811155908248\n",
      "Epoch 20, Training Loss: 0.794546855617972\n",
      "Epoch 21, Training Loss: 0.7932413927246542\n",
      "Epoch 22, Training Loss: 0.7936442807842703\n",
      "Epoch 23, Training Loss: 0.7930440449714661\n",
      "Epoch 24, Training Loss: 0.7934723997116089\n",
      "Epoch 25, Training Loss: 0.7927342548089868\n",
      "Epoch 26, Training Loss: 0.793030395858428\n",
      "Epoch 27, Training Loss: 0.7920933269051944\n",
      "Epoch 28, Training Loss: 0.7929641244691961\n",
      "Epoch 29, Training Loss: 0.7923978992069469\n",
      "Epoch 30, Training Loss: 0.7926197041483486\n",
      "Epoch 31, Training Loss: 0.7925827273901771\n",
      "Epoch 32, Training Loss: 0.7927053895417382\n",
      "Epoch 33, Training Loss: 0.7923728919730467\n",
      "Epoch 34, Training Loss: 0.7921502920459299\n",
      "Epoch 35, Training Loss: 0.7925960260980269\n",
      "Epoch 36, Training Loss: 0.7924930291316089\n",
      "Epoch 37, Training Loss: 0.7921673594502842\n",
      "Epoch 38, Training Loss: 0.7919943595633787\n",
      "Epoch 39, Training Loss: 0.792768026379978\n",
      "Epoch 40, Training Loss: 0.7916961213420419\n",
      "Epoch 41, Training Loss: 0.7927829019462361\n",
      "Epoch 42, Training Loss: 0.7928132343993467\n",
      "Epoch 43, Training Loss: 0.7921463996522566\n",
      "Epoch 44, Training Loss: 0.7918897362316356\n",
      "Epoch 45, Training Loss: 0.791705725403393\n",
      "Epoch 46, Training Loss: 0.7915030675074634\n",
      "Epoch 47, Training Loss: 0.7922768562681535\n",
      "Epoch 48, Training Loss: 0.7915692967527053\n",
      "Epoch 49, Training Loss: 0.790808529292836\n",
      "Epoch 50, Training Loss: 0.7913183938054478\n",
      "Epoch 51, Training Loss: 0.7914810787930208\n",
      "Epoch 52, Training Loss: 0.7916978419528288\n",
      "Epoch 53, Training Loss: 0.7914298854154699\n",
      "Epoch 54, Training Loss: 0.7914074944748598\n",
      "Epoch 55, Training Loss: 0.7921285618052764\n",
      "Epoch 56, Training Loss: 0.7912060759348027\n",
      "Epoch 57, Training Loss: 0.7918895663233364\n",
      "Epoch 58, Training Loss: 0.791126490691129\n",
      "Epoch 59, Training Loss: 0.7907452821731568\n",
      "Epoch 60, Training Loss: 0.7916023313297945\n",
      "Epoch 61, Training Loss: 0.7910942975212546\n",
      "Epoch 62, Training Loss: 0.7909339545754825\n",
      "Epoch 63, Training Loss: 0.7910948001637178\n",
      "Epoch 64, Training Loss: 0.7910201313215144\n",
      "Epoch 65, Training Loss: 0.7915400078717401\n",
      "Epoch 66, Training Loss: 0.7910891502745011\n",
      "Epoch 67, Training Loss: 0.7910983459388509\n",
      "Epoch 68, Training Loss: 0.7905979463633369\n",
      "Epoch 69, Training Loss: 0.7909605143350713\n",
      "Epoch 70, Training Loss: 0.7909539626626407\n",
      "Epoch 71, Training Loss: 0.7907427568295423\n",
      "Epoch 72, Training Loss: 0.7907159090042114\n",
      "Epoch 73, Training Loss: 0.7914088226767148\n",
      "Epoch 74, Training Loss: 0.7909800227950601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:51:30,902] Trial 209 finished with value: 0.6384 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7905654520848218\n",
      "Epoch 1, Training Loss: 1.0461085016587202\n",
      "Epoch 2, Training Loss: 1.0004079498964198\n",
      "Epoch 3, Training Loss: 0.9810012236763449\n",
      "Epoch 4, Training Loss: 0.9705247982109294\n",
      "Epoch 5, Training Loss: 0.9643411807452931\n",
      "Epoch 6, Training Loss: 0.9604142008809482\n",
      "Epoch 7, Training Loss: 0.9577141901324777\n",
      "Epoch 8, Training Loss: 0.955703087273766\n",
      "Epoch 9, Training Loss: 0.9540780889286714\n",
      "Epoch 10, Training Loss: 0.9526842099778793\n",
      "Epoch 11, Training Loss: 0.9514210043233984\n",
      "Epoch 12, Training Loss: 0.9502349960803985\n",
      "Epoch 13, Training Loss: 0.9491044863532572\n",
      "Epoch 14, Training Loss: 0.9480056262016296\n",
      "Epoch 15, Training Loss: 0.9469230219897102\n",
      "Epoch 16, Training Loss: 0.9458589807678671\n",
      "Epoch 17, Training Loss: 0.9448101718986736\n",
      "Epoch 18, Training Loss: 0.943761896946851\n",
      "Epoch 19, Training Loss: 0.9427230489253998\n",
      "Epoch 20, Training Loss: 0.9416933116492103\n",
      "Epoch 21, Training Loss: 0.9406652675656711\n",
      "Epoch 22, Training Loss: 0.9396341295102063\n",
      "Epoch 23, Training Loss: 0.9386292027024662\n",
      "Epoch 24, Training Loss: 0.9376167784718906\n",
      "Epoch 25, Training Loss: 0.9366027790658614\n",
      "Epoch 26, Training Loss: 0.9355944712021772\n",
      "Epoch 27, Training Loss: 0.9345841118167428\n",
      "Epoch 28, Training Loss: 0.9335894737524145\n",
      "Epoch 29, Training Loss: 0.9325835497239057\n",
      "Epoch 30, Training Loss: 0.9315925265760983\n",
      "Epoch 31, Training Loss: 0.9305980769325705\n",
      "Epoch 32, Training Loss: 0.9295988685944501\n",
      "Epoch 33, Training Loss: 0.9286084125322455\n",
      "Epoch 34, Training Loss: 0.9276178506542655\n",
      "Epoch 35, Training Loss: 0.9266294825077057\n",
      "Epoch 36, Training Loss: 0.9256399459698621\n",
      "Epoch 37, Training Loss: 0.9246528503474067\n",
      "Epoch 38, Training Loss: 0.9236648952960969\n",
      "Epoch 39, Training Loss: 0.9226765845803654\n",
      "Epoch 40, Training Loss: 0.9216943031900069\n",
      "Epoch 41, Training Loss: 0.9207063868466545\n",
      "Epoch 42, Training Loss: 0.9197230257707484\n",
      "Epoch 43, Training Loss: 0.918731127206017\n",
      "Epoch 44, Training Loss: 0.917751325228635\n",
      "Epoch 45, Training Loss: 0.9167719622920542\n",
      "Epoch 46, Training Loss: 0.9157837542365579\n",
      "Epoch 47, Training Loss: 0.9147964079239789\n",
      "Epoch 48, Training Loss: 0.9138085504840402\n",
      "Epoch 49, Training Loss: 0.9128327248376958\n",
      "Epoch 50, Training Loss: 0.9118431040819953\n",
      "Epoch 51, Training Loss: 0.910860373693354\n",
      "Epoch 52, Training Loss: 0.9098720956549925\n",
      "Epoch 53, Training Loss: 0.9088909723478205\n",
      "Epoch 54, Training Loss: 0.907900351005442\n",
      "Epoch 55, Training Loss: 0.9069163621874416\n",
      "Epoch 56, Training Loss: 0.9059292479823617\n",
      "Epoch 57, Training Loss: 0.9049350224522983\n",
      "Epoch 58, Training Loss: 0.9039553856148439\n",
      "Epoch 59, Training Loss: 0.9029681878230151\n",
      "Epoch 60, Training Loss: 0.9019809786712422\n",
      "Epoch 61, Training Loss: 0.900982347726822\n",
      "Epoch 62, Training Loss: 0.9000040309569415\n",
      "Epoch 63, Training Loss: 0.8990195993816151\n",
      "Epoch 64, Training Loss: 0.898031605832717\n",
      "Epoch 65, Training Loss: 0.8970434030364541\n",
      "Epoch 66, Training Loss: 0.896061045702766\n",
      "Epoch 67, Training Loss: 0.8950729399568894\n",
      "Epoch 68, Training Loss: 0.8940835269058451\n",
      "Epoch 69, Training Loss: 0.8930942230364856\n",
      "Epoch 70, Training Loss: 0.8921259960006265\n",
      "Epoch 71, Training Loss: 0.8911370581037859\n",
      "Epoch 72, Training Loss: 0.8901531417930827\n",
      "Epoch 73, Training Loss: 0.8891802608966828\n",
      "Epoch 74, Training Loss: 0.8882074045433718\n",
      "Epoch 75, Training Loss: 0.8872303261476404\n",
      "Epoch 76, Training Loss: 0.8862578784718232\n",
      "Epoch 77, Training Loss: 0.8852822890702416\n",
      "Epoch 78, Training Loss: 0.8843131773612078\n",
      "Epoch 79, Training Loss: 0.8833483920377844\n",
      "Epoch 80, Training Loss: 0.8823880238392774\n",
      "Epoch 81, Training Loss: 0.8814095345665427\n",
      "Epoch 82, Training Loss: 0.8804739251557518\n",
      "Epoch 83, Training Loss: 0.8795114979323219\n",
      "Epoch 84, Training Loss: 0.8785687651353724\n",
      "Epoch 85, Training Loss: 0.8776129557805903\n",
      "Epoch 86, Training Loss: 0.8766710930010851\n",
      "Epoch 87, Training Loss: 0.8757387715227464\n",
      "Epoch 88, Training Loss: 0.8748023147442762\n",
      "Epoch 89, Training Loss: 0.8738797798577477\n",
      "Epoch 90, Training Loss: 0.8729592006346759\n",
      "Epoch 91, Training Loss: 0.8720389922927407\n",
      "Epoch 92, Training Loss: 0.8711233762432548\n",
      "Epoch 93, Training Loss: 0.8702115781166975\n",
      "Epoch 94, Training Loss: 0.8693076604955337\n",
      "Epoch 95, Training Loss: 0.8684064584619858\n",
      "Epoch 96, Training Loss: 0.8675300889856675\n",
      "Epoch 97, Training Loss: 0.8666409450418809\n",
      "Epoch 98, Training Loss: 0.8657599627270418\n",
      "Epoch 99, Training Loss: 0.8648839917603661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:53:03,726] Trial 210 finished with value: 0.5968 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8640127314539516\n",
      "Epoch 1, Training Loss: 1.0074819344326966\n",
      "Epoch 2, Training Loss: 0.9439569419487974\n",
      "Epoch 3, Training Loss: 0.9166239137040045\n",
      "Epoch 4, Training Loss: 0.8702011766290306\n",
      "Epoch 5, Training Loss: 0.8348958067427901\n",
      "Epoch 6, Training Loss: 0.819672537567024\n",
      "Epoch 7, Training Loss: 0.8138751271075773\n",
      "Epoch 8, Training Loss: 0.8114911354574046\n",
      "Epoch 9, Training Loss: 0.8105783825530145\n",
      "Epoch 10, Training Loss: 0.8101228644973353\n",
      "Epoch 11, Training Loss: 0.8085037200074446\n",
      "Epoch 12, Training Loss: 0.8084446846094346\n",
      "Epoch 13, Training Loss: 0.8066538576792953\n",
      "Epoch 14, Training Loss: 0.8061933765734048\n",
      "Epoch 15, Training Loss: 0.8057911819085143\n",
      "Epoch 16, Training Loss: 0.8054545706376097\n",
      "Epoch 17, Training Loss: 0.8051469835123621\n",
      "Epoch 18, Training Loss: 0.8053030700611888\n",
      "Epoch 19, Training Loss: 0.8035260177196417\n",
      "Epoch 20, Training Loss: 0.804440488761529\n",
      "Epoch 21, Training Loss: 0.8039276878636582\n",
      "Epoch 22, Training Loss: 0.8033616701463111\n",
      "Epoch 23, Training Loss: 0.8028804305800818\n",
      "Epoch 24, Training Loss: 0.8015844118774386\n",
      "Epoch 25, Training Loss: 0.8039215319138721\n",
      "Epoch 26, Training Loss: 0.8019401241065864\n",
      "Epoch 27, Training Loss: 0.8028540177452833\n",
      "Epoch 28, Training Loss: 0.8014449822275262\n",
      "Epoch 29, Training Loss: 0.8017999830102562\n",
      "Epoch 30, Training Loss: 0.8012871415095222\n",
      "Epoch 31, Training Loss: 0.8020418161736396\n",
      "Epoch 32, Training Loss: 0.8005965825758482\n",
      "Epoch 33, Training Loss: 0.8010064066800856\n",
      "Epoch 34, Training Loss: 0.8009035831107233\n",
      "Epoch 35, Training Loss: 0.8012140217580294\n",
      "Epoch 36, Training Loss: 0.8005800890743284\n",
      "Epoch 37, Training Loss: 0.8001708960174618\n",
      "Epoch 38, Training Loss: 0.8009094229318146\n",
      "Epoch 39, Training Loss: 0.7998392963319793\n",
      "Epoch 40, Training Loss: 0.8001103025630004\n",
      "Epoch 41, Training Loss: 0.8003546235256626\n",
      "Epoch 42, Training Loss: 0.7999572461709044\n",
      "Epoch 43, Training Loss: 0.7997509904373857\n",
      "Epoch 44, Training Loss: 0.8003843464349446\n",
      "Epoch 45, Training Loss: 0.7994656055493462\n",
      "Epoch 46, Training Loss: 0.7992279848658053\n",
      "Epoch 47, Training Loss: 0.7990188163922245\n",
      "Epoch 48, Training Loss: 0.7987052989185305\n",
      "Epoch 49, Training Loss: 0.799780381442909\n",
      "Epoch 50, Training Loss: 0.7993603615832509\n",
      "Epoch 51, Training Loss: 0.7986936990479777\n",
      "Epoch 52, Training Loss: 0.7986229266439165\n",
      "Epoch 53, Training Loss: 0.7994456872904211\n",
      "Epoch 54, Training Loss: 0.7988838269298238\n",
      "Epoch 55, Training Loss: 0.799096972063968\n",
      "Epoch 56, Training Loss: 0.7987160621729112\n",
      "Epoch 57, Training Loss: 0.7981834718159266\n",
      "Epoch 58, Training Loss: 0.7986646170902969\n",
      "Epoch 59, Training Loss: 0.7980532656038614\n",
      "Epoch 60, Training Loss: 0.7980848318652103\n",
      "Epoch 61, Training Loss: 0.7977758778665299\n",
      "Epoch 62, Training Loss: 0.7976095567072244\n",
      "Epoch 63, Training Loss: 0.797474563838844\n",
      "Epoch 64, Training Loss: 0.7974194537428089\n",
      "Epoch 65, Training Loss: 0.7990804983260936\n",
      "Epoch 66, Training Loss: 0.7976905137076413\n",
      "Epoch 67, Training Loss: 0.7971661082784036\n",
      "Epoch 68, Training Loss: 0.7973607616317003\n",
      "Epoch 69, Training Loss: 0.7966050970823245\n",
      "Epoch 70, Training Loss: 0.7971865631583938\n",
      "Epoch 71, Training Loss: 0.797665220006068\n",
      "Epoch 72, Training Loss: 0.7964726229359332\n",
      "Epoch 73, Training Loss: 0.7965312366198777\n",
      "Epoch 74, Training Loss: 0.7967354291363766\n",
      "Epoch 75, Training Loss: 0.7975015005670992\n",
      "Epoch 76, Training Loss: 0.7965136702795674\n",
      "Epoch 77, Training Loss: 0.7967641130425877\n",
      "Epoch 78, Training Loss: 0.7971588136558246\n",
      "Epoch 79, Training Loss: 0.7968903941319401\n",
      "Epoch 80, Training Loss: 0.7967161581032258\n",
      "Epoch 81, Training Loss: 0.7958066387284071\n",
      "Epoch 82, Training Loss: 0.7970540892808957\n",
      "Epoch 83, Training Loss: 0.7960792954703023\n",
      "Epoch 84, Training Loss: 0.796738796933253\n",
      "Epoch 85, Training Loss: 0.796132427050655\n",
      "Epoch 86, Training Loss: 0.7955042286026747\n",
      "Epoch 87, Training Loss: 0.7965629904790033\n",
      "Epoch 88, Training Loss: 0.7954830672507895\n",
      "Epoch 89, Training Loss: 0.7959890670346138\n",
      "Epoch 90, Training Loss: 0.7951646755512496\n",
      "Epoch 91, Training Loss: 0.7958929109394102\n",
      "Epoch 92, Training Loss: 0.7950710735822979\n",
      "Epoch 93, Training Loss: 0.7948402011304869\n",
      "Epoch 94, Training Loss: 0.7956318607007651\n",
      "Epoch 95, Training Loss: 0.7954232293860357\n",
      "Epoch 96, Training Loss: 0.7964230839471171\n",
      "Epoch 97, Training Loss: 0.7950491223120152\n",
      "Epoch 98, Training Loss: 0.7952276441387665\n",
      "Epoch 99, Training Loss: 0.7946918838902524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:55:01,316] Trial 211 finished with value: 0.6357333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7948141761292192\n",
      "Epoch 1, Training Loss: 1.0664668663810282\n",
      "Epoch 2, Training Loss: 1.0266750616185805\n",
      "Epoch 3, Training Loss: 0.99921550673597\n",
      "Epoch 4, Training Loss: 0.9807692839117611\n",
      "Epoch 5, Training Loss: 0.9698569419804741\n",
      "Epoch 6, Training Loss: 0.9636113450807684\n",
      "Epoch 7, Training Loss: 0.9601739480916192\n",
      "Epoch 8, Training Loss: 0.9579570151777829\n",
      "Epoch 9, Training Loss: 0.956412949071211\n",
      "Epoch 10, Training Loss: 0.9550422704219819\n",
      "Epoch 11, Training Loss: 0.9537352893633001\n",
      "Epoch 12, Training Loss: 0.9525802580048056\n",
      "Epoch 13, Training Loss: 0.951322389350218\n",
      "Epoch 14, Training Loss: 0.9500635529265684\n",
      "Epoch 15, Training Loss: 0.9487514889941496\n",
      "Epoch 16, Training Loss: 0.9475675036626704\n",
      "Epoch 17, Training Loss: 0.9462210669938256\n",
      "Epoch 18, Training Loss: 0.9448809538168066\n",
      "Epoch 19, Training Loss: 0.9435101851295022\n",
      "Epoch 20, Training Loss: 0.9419442408225116\n",
      "Epoch 21, Training Loss: 0.9404730338910047\n",
      "Epoch 22, Training Loss: 0.9390836917652803\n",
      "Epoch 23, Training Loss: 0.9375165372736314\n",
      "Epoch 24, Training Loss: 0.9359135258899015\n",
      "Epoch 25, Training Loss: 0.9342203349926892\n",
      "Epoch 26, Training Loss: 0.9325885864566354\n",
      "Epoch 27, Training Loss: 0.9307804876916549\n",
      "Epoch 28, Training Loss: 0.928985269280041\n",
      "Epoch 29, Training Loss: 0.9270875750569736\n",
      "Epoch 30, Training Loss: 0.925225864017711\n",
      "Epoch 31, Training Loss: 0.9232206510095036\n",
      "Epoch 32, Training Loss: 0.9212666479279014\n",
      "Epoch 33, Training Loss: 0.9191472021271201\n",
      "Epoch 34, Training Loss: 0.9170496336852803\n",
      "Epoch 35, Training Loss: 0.9149239298876594\n",
      "Epoch 36, Training Loss: 0.9126722943081576\n",
      "Epoch 37, Training Loss: 0.91044409254018\n",
      "Epoch 38, Training Loss: 0.9081038639124702\n",
      "Epoch 39, Training Loss: 0.9058040796307957\n",
      "Epoch 40, Training Loss: 0.9034322370501126\n",
      "Epoch 41, Training Loss: 0.9009167453821968\n",
      "Epoch 42, Training Loss: 0.8985833245165208\n",
      "Epoch 43, Training Loss: 0.8961165769661175\n",
      "Epoch 44, Training Loss: 0.8936559463248533\n",
      "Epoch 45, Training Loss: 0.8911401261301601\n",
      "Epoch 46, Training Loss: 0.8886954785094542\n",
      "Epoch 47, Training Loss: 0.8861964877212749\n",
      "Epoch 48, Training Loss: 0.8836745241810294\n",
      "Epoch 49, Training Loss: 0.8812399015005897\n",
      "Epoch 50, Training Loss: 0.878736322627348\n",
      "Epoch 51, Training Loss: 0.8762349440770991\n",
      "Epoch 52, Training Loss: 0.8738488532515133\n",
      "Epoch 53, Training Loss: 0.8713639114183538\n",
      "Epoch 54, Training Loss: 0.8690928108551923\n",
      "Epoch 55, Training Loss: 0.8666293476609622\n",
      "Epoch 56, Training Loss: 0.8643499220118803\n",
      "Epoch 57, Training Loss: 0.8621317527574651\n",
      "Epoch 58, Training Loss: 0.8598837644913617\n",
      "Epoch 59, Training Loss: 0.8577235034634085\n",
      "Epoch 60, Training Loss: 0.855556029922822\n",
      "Epoch 61, Training Loss: 0.8535603082881255\n",
      "Epoch 62, Training Loss: 0.8515203211588018\n",
      "Epoch 63, Training Loss: 0.8496500783808091\n",
      "Epoch 64, Training Loss: 0.8477520407648648\n",
      "Epoch 65, Training Loss: 0.8458704454057356\n",
      "Epoch 66, Training Loss: 0.8442681546772227\n",
      "Epoch 67, Training Loss: 0.8424106862264521\n",
      "Epoch 68, Training Loss: 0.8409249927717096\n",
      "Epoch 69, Training Loss: 0.8393340396881104\n",
      "Epoch 70, Training Loss: 0.8378035561477437\n",
      "Epoch 71, Training Loss: 0.8364365206746494\n",
      "Epoch 72, Training Loss: 0.8349815510301029\n",
      "Epoch 73, Training Loss: 0.8336495586703805\n",
      "Epoch 74, Training Loss: 0.8323753569406621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:56:12,588] Trial 212 finished with value: 0.6182 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8312567999082453\n",
      "Epoch 1, Training Loss: 0.8415903662934022\n",
      "Epoch 2, Training Loss: 0.8162654145324931\n",
      "Epoch 3, Training Loss: 0.8164544672825758\n",
      "Epoch 4, Training Loss: 0.8132818316010868\n",
      "Epoch 5, Training Loss: 0.8109234404563904\n",
      "Epoch 6, Training Loss: 0.8099124915459577\n",
      "Epoch 7, Training Loss: 0.8084513665648068\n",
      "Epoch 8, Training Loss: 0.8073143874897676\n",
      "Epoch 9, Training Loss: 0.8059520096638624\n",
      "Epoch 10, Training Loss: 0.8058682404546177\n",
      "Epoch 11, Training Loss: 0.8043728682574104\n",
      "Epoch 12, Training Loss: 0.8033703073333291\n",
      "Epoch 13, Training Loss: 0.8039759201162001\n",
      "Epoch 14, Training Loss: 0.8030934480358572\n",
      "Epoch 15, Training Loss: 0.8028213063408347\n",
      "Epoch 16, Training Loss: 0.8040611834385816\n",
      "Epoch 17, Training Loss: 0.8016329553548027\n",
      "Epoch 18, Training Loss: 0.8036222740481882\n",
      "Epoch 19, Training Loss: 0.801291617084952\n",
      "Epoch 20, Training Loss: 0.8029369414553923\n",
      "Epoch 21, Training Loss: 0.8006988512067233\n",
      "Epoch 22, Training Loss: 0.8028160561533535\n",
      "Epoch 23, Training Loss: 0.8012147914662081\n",
      "Epoch 24, Training Loss: 0.8012330003345713\n",
      "Epoch 25, Training Loss: 0.8008554346421186\n",
      "Epoch 26, Training Loss: 0.801241972095826\n",
      "Epoch 27, Training Loss: 0.8007334458827973\n",
      "Epoch 28, Training Loss: 0.8004762029647827\n",
      "Epoch 29, Training Loss: 0.8005273524452658\n",
      "Epoch 30, Training Loss: 0.8006349594452802\n",
      "Epoch 31, Training Loss: 0.7993027321030112\n",
      "Epoch 32, Training Loss: 0.7997859523576849\n",
      "Epoch 33, Training Loss: 0.7999396714042215\n",
      "Epoch 34, Training Loss: 0.7999179564504062\n",
      "Epoch 35, Training Loss: 0.7995380480149213\n",
      "Epoch 36, Training Loss: 0.8002248745806076\n",
      "Epoch 37, Training Loss: 0.7993182607959298\n",
      "Epoch 38, Training Loss: 0.7997892176403719\n",
      "Epoch 39, Training Loss: 0.800713257228627\n",
      "Epoch 40, Training Loss: 0.7994853444660411\n",
      "Epoch 41, Training Loss: 0.7993975262782153\n",
      "Epoch 42, Training Loss: 0.7994001561052659\n",
      "Epoch 43, Training Loss: 0.7989812032615438\n",
      "Epoch 44, Training Loss: 0.800165113771663\n",
      "Epoch 45, Training Loss: 0.7996099821258994\n",
      "Epoch 46, Training Loss: 0.7992423019689672\n",
      "Epoch 47, Training Loss: 0.7985434705369613\n",
      "Epoch 48, Training Loss: 0.7995598335827098\n",
      "Epoch 49, Training Loss: 0.798730514680638\n",
      "Epoch 50, Training Loss: 0.8006703598359052\n",
      "Epoch 51, Training Loss: 0.7982052989567028\n",
      "Epoch 52, Training Loss: 0.7987976360321045\n",
      "Epoch 53, Training Loss: 0.7992221433274886\n",
      "Epoch 54, Training Loss: 0.7995878636135775\n",
      "Epoch 55, Training Loss: 0.7989127298663644\n",
      "Epoch 56, Training Loss: 0.7983861224791583\n",
      "Epoch 57, Training Loss: 0.7979283855241888\n",
      "Epoch 58, Training Loss: 0.7979530044162975\n",
      "Epoch 59, Training Loss: 0.7981966161026675\n",
      "Epoch 60, Training Loss: 0.7979330484306111\n",
      "Epoch 61, Training Loss: 0.7973685563311858\n",
      "Epoch 62, Training Loss: 0.7986378178175758\n",
      "Epoch 63, Training Loss: 0.7972258468936472\n",
      "Epoch 64, Training Loss: 0.7983924689713646\n",
      "Epoch 65, Training Loss: 0.7990361860920401\n",
      "Epoch 66, Training Loss: 0.7980838308614843\n",
      "Epoch 67, Training Loss: 0.798582751119838\n",
      "Epoch 68, Training Loss: 0.797876701074488\n",
      "Epoch 69, Training Loss: 0.7981198164294748\n",
      "Epoch 70, Training Loss: 0.7982565827229444\n",
      "Epoch 71, Training Loss: 0.7976334359365351\n",
      "Epoch 72, Training Loss: 0.7976591374593622\n",
      "Epoch 73, Training Loss: 0.7984368448397693\n",
      "Epoch 74, Training Loss: 0.7989411107231589\n",
      "Epoch 75, Training Loss: 0.7984241124461678\n",
      "Epoch 76, Training Loss: 0.7977431192117579\n",
      "Epoch 77, Training Loss: 0.7981343148736393\n",
      "Epoch 78, Training Loss: 0.7974510019667008\n",
      "Epoch 79, Training Loss: 0.7982438637929804\n",
      "Epoch 80, Training Loss: 0.7975420980593737\n",
      "Epoch 81, Training Loss: 0.7975951667392955\n",
      "Epoch 82, Training Loss: 0.7979351820665247\n",
      "Epoch 83, Training Loss: 0.7978058692988227\n",
      "Epoch 84, Training Loss: 0.7964147162437439\n",
      "Epoch 85, Training Loss: 0.7983684056646684\n",
      "Epoch 86, Training Loss: 0.7974758183956147\n",
      "Epoch 87, Training Loss: 0.7975041507272159\n",
      "Epoch 88, Training Loss: 0.7974522779268377\n",
      "Epoch 89, Training Loss: 0.7983906302031348\n",
      "Epoch 90, Training Loss: 0.7965014945759493\n",
      "Epoch 91, Training Loss: 0.7976168949463788\n",
      "Epoch 92, Training Loss: 0.7967884793702293\n",
      "Epoch 93, Training Loss: 0.7976509009389317\n",
      "Epoch 94, Training Loss: 0.7974079594892614\n",
      "Epoch 95, Training Loss: 0.7964272578323589\n",
      "Epoch 96, Training Loss: 0.7977843985838049\n",
      "Epoch 97, Training Loss: 0.7977749799279605\n",
      "Epoch 98, Training Loss: 0.797505728076486\n",
      "Epoch 99, Training Loss: 0.7981963975289289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 14:58:11,026] Trial 213 finished with value: 0.6302666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7981524219933678\n",
      "Epoch 1, Training Loss: 0.8672209474619698\n",
      "Epoch 2, Training Loss: 0.812936563351575\n",
      "Epoch 3, Training Loss: 0.8076096218473771\n",
      "Epoch 4, Training Loss: 0.8031760740280152\n",
      "Epoch 5, Training Loss: 0.8004823005900663\n",
      "Epoch 6, Training Loss: 0.7991083240509034\n",
      "Epoch 7, Training Loss: 0.7972147878478555\n",
      "Epoch 8, Training Loss: 0.7958726298809051\n",
      "Epoch 9, Training Loss: 0.7953388074566337\n",
      "Epoch 10, Training Loss: 0.7947916737724753\n",
      "Epoch 11, Training Loss: 0.7940212103198556\n",
      "Epoch 12, Training Loss: 0.794045549771365\n",
      "Epoch 13, Training Loss: 0.793721775377498\n",
      "Epoch 14, Training Loss: 0.7937468323988073\n",
      "Epoch 15, Training Loss: 0.7918111533277175\n",
      "Epoch 16, Training Loss: 0.7918375898108763\n",
      "Epoch 17, Training Loss: 0.7919573737593258\n",
      "Epoch 18, Training Loss: 0.7916417015300078\n",
      "Epoch 19, Training Loss: 0.79139000415802\n",
      "Epoch 20, Training Loss: 0.7910431507755729\n",
      "Epoch 21, Training Loss: 0.7907327070656944\n",
      "Epoch 22, Training Loss: 0.7897350910130669\n",
      "Epoch 23, Training Loss: 0.7899061010865605\n",
      "Epoch 24, Training Loss: 0.7895867044785444\n",
      "Epoch 25, Training Loss: 0.7893044278902166\n",
      "Epoch 26, Training Loss: 0.78829838514328\n",
      "Epoch 27, Training Loss: 0.7898890815061681\n",
      "Epoch 28, Training Loss: 0.7889038178500007\n",
      "Epoch 29, Training Loss: 0.788516432537752\n",
      "Epoch 30, Training Loss: 0.7874860070733463\n",
      "Epoch 31, Training Loss: 0.7890053113067851\n",
      "Epoch 32, Training Loss: 0.7876626241908354\n",
      "Epoch 33, Training Loss: 0.7875845860733706\n",
      "Epoch 34, Training Loss: 0.7881637970138998\n",
      "Epoch 35, Training Loss: 0.7874479638127719\n",
      "Epoch 36, Training Loss: 0.7875295896389906\n",
      "Epoch 37, Training Loss: 0.7875694892686956\n",
      "Epoch 38, Training Loss: 0.7872950735512901\n",
      "Epoch 39, Training Loss: 0.7865007627711577\n",
      "Epoch 40, Training Loss: 0.786253072794746\n",
      "Epoch 41, Training Loss: 0.7864244516456829\n",
      "Epoch 42, Training Loss: 0.7872594343213474\n",
      "Epoch 43, Training Loss: 0.787169466229046\n",
      "Epoch 44, Training Loss: 0.786241661941304\n",
      "Epoch 45, Training Loss: 0.7864781151098363\n",
      "Epoch 46, Training Loss: 0.7869796281001147\n",
      "Epoch 47, Training Loss: 0.7858513593673706\n",
      "Epoch 48, Training Loss: 0.7855585830351886\n",
      "Epoch 49, Training Loss: 0.7857807570345261\n",
      "Epoch 50, Training Loss: 0.7852762617784388\n",
      "Epoch 51, Training Loss: 0.7852424721858081\n",
      "Epoch 52, Training Loss: 0.785063080296797\n",
      "Epoch 53, Training Loss: 0.7846437756454243\n",
      "Epoch 54, Training Loss: 0.7854762677585377\n",
      "Epoch 55, Training Loss: 0.7855861738148857\n",
      "Epoch 56, Training Loss: 0.784739096585442\n",
      "Epoch 57, Training Loss: 0.7847228407158571\n",
      "Epoch 58, Training Loss: 0.7848921116660623\n",
      "Epoch 59, Training Loss: 0.7849356350478004\n",
      "Epoch 60, Training Loss: 0.784958756460863\n",
      "Epoch 61, Training Loss: 0.784695102817872\n",
      "Epoch 62, Training Loss: 0.7846511908839731\n",
      "Epoch 63, Training Loss: 0.7844443403272068\n",
      "Epoch 64, Training Loss: 0.7839671741513645\n",
      "Epoch 65, Training Loss: 0.7839711128964144\n",
      "Epoch 66, Training Loss: 0.7840984918790705\n",
      "Epoch 67, Training Loss: 0.7837291162154254\n",
      "Epoch 68, Training Loss: 0.7841061932199141\n",
      "Epoch 69, Training Loss: 0.7837069275098688\n",
      "Epoch 70, Training Loss: 0.7836092495918274\n",
      "Epoch 71, Training Loss: 0.7841323340640348\n",
      "Epoch 72, Training Loss: 0.7839206628238453\n",
      "Epoch 73, Training Loss: 0.7834898548967698\n",
      "Epoch 74, Training Loss: 0.7831707342933206\n",
      "Epoch 75, Training Loss: 0.7835138127383063\n",
      "Epoch 76, Training Loss: 0.7835239970684051\n",
      "Epoch 77, Training Loss: 0.7843738709477818\n",
      "Epoch 78, Training Loss: 0.782679430807338\n",
      "Epoch 79, Training Loss: 0.7839977711789748\n",
      "Epoch 80, Training Loss: 0.7833943599111893\n",
      "Epoch 81, Training Loss: 0.7828834137495826\n",
      "Epoch 82, Training Loss: 0.7825830548651078\n",
      "Epoch 83, Training Loss: 0.7825623083815855\n",
      "Epoch 84, Training Loss: 0.7836937808990478\n",
      "Epoch 85, Training Loss: 0.7829604500181535\n",
      "Epoch 86, Training Loss: 0.7829685591950136\n",
      "Epoch 87, Training Loss: 0.782708067893982\n",
      "Epoch 88, Training Loss: 0.783197701327941\n",
      "Epoch 89, Training Loss: 0.7834328886340646\n",
      "Epoch 90, Training Loss: 0.7827253413901609\n",
      "Epoch 91, Training Loss: 0.782043215527254\n",
      "Epoch 92, Training Loss: 0.7823881245360655\n",
      "Epoch 93, Training Loss: 0.7820074204837575\n",
      "Epoch 94, Training Loss: 0.7823611469829783\n",
      "Epoch 95, Training Loss: 0.7817585484420552\n",
      "Epoch 96, Training Loss: 0.7818540573821349\n",
      "Epoch 97, Training Loss: 0.7822738437792834\n",
      "Epoch 98, Training Loss: 0.7813397262376898\n",
      "Epoch 99, Training Loss: 0.7811101900128757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:00:27,350] Trial 214 finished with value: 0.6382666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7821005023928249\n",
      "Epoch 1, Training Loss: 0.9770967045224699\n",
      "Epoch 2, Training Loss: 0.9370638893062907\n",
      "Epoch 3, Training Loss: 0.9034243478810877\n",
      "Epoch 4, Training Loss: 0.8638992316740796\n",
      "Epoch 5, Training Loss: 0.8321275597228144\n",
      "Epoch 6, Training Loss: 0.8181846985243317\n",
      "Epoch 7, Training Loss: 0.8139727600535056\n",
      "Epoch 8, Training Loss: 0.8128622055053711\n",
      "Epoch 9, Training Loss: 0.8115829969707288\n",
      "Epoch 10, Training Loss: 0.8103705341654612\n",
      "Epoch 11, Training Loss: 0.8101921876570336\n",
      "Epoch 12, Training Loss: 0.8090486205610118\n",
      "Epoch 13, Training Loss: 0.8084836979557697\n",
      "Epoch 14, Training Loss: 0.8074443224229311\n",
      "Epoch 15, Training Loss: 0.8068679558603387\n",
      "Epoch 16, Training Loss: 0.8059126593116531\n",
      "Epoch 17, Training Loss: 0.8057282869977163\n",
      "Epoch 18, Training Loss: 0.8059163746080901\n",
      "Epoch 19, Training Loss: 0.8051455228848565\n",
      "Epoch 20, Training Loss: 0.8057948297127745\n",
      "Epoch 21, Training Loss: 0.8037899380339716\n",
      "Epoch 22, Training Loss: 0.8042187255127986\n",
      "Epoch 23, Training Loss: 0.8029303128557994\n",
      "Epoch 24, Training Loss: 0.8033112497258007\n",
      "Epoch 25, Training Loss: 0.8023619915309705\n",
      "Epoch 26, Training Loss: 0.8027667243677871\n",
      "Epoch 27, Training Loss: 0.8020276252488444\n",
      "Epoch 28, Training Loss: 0.8023084365335622\n",
      "Epoch 29, Training Loss: 0.8022601731737754\n",
      "Epoch 30, Training Loss: 0.8019621528180918\n",
      "Epoch 31, Training Loss: 0.8020501611824322\n",
      "Epoch 32, Training Loss: 0.8012297560397844\n",
      "Epoch 33, Training Loss: 0.8023499841080572\n",
      "Epoch 34, Training Loss: 0.801538320233051\n",
      "Epoch 35, Training Loss: 0.8007065319477167\n",
      "Epoch 36, Training Loss: 0.8010147926502658\n",
      "Epoch 37, Training Loss: 0.8006643638126832\n",
      "Epoch 38, Training Loss: 0.8014992251432032\n",
      "Epoch 39, Training Loss: 0.8006770652039606\n",
      "Epoch 40, Training Loss: 0.8002236923777071\n",
      "Epoch 41, Training Loss: 0.80033509668551\n",
      "Epoch 42, Training Loss: 0.8004001228432907\n",
      "Epoch 43, Training Loss: 0.801277834580357\n",
      "Epoch 44, Training Loss: 0.7996050702001816\n",
      "Epoch 45, Training Loss: 0.8000700471096469\n",
      "Epoch 46, Training Loss: 0.7994729192633377\n",
      "Epoch 47, Training Loss: 0.8000986426396477\n",
      "Epoch 48, Training Loss: 0.799552227321424\n",
      "Epoch 49, Training Loss: 0.7991848527937007\n",
      "Epoch 50, Training Loss: 0.7993007166045053\n",
      "Epoch 51, Training Loss: 0.799039624060007\n",
      "Epoch 52, Training Loss: 0.7989559659384247\n",
      "Epoch 53, Training Loss: 0.7991999327688288\n",
      "Epoch 54, Training Loss: 0.8000955060012358\n",
      "Epoch 55, Training Loss: 0.799045205295534\n",
      "Epoch 56, Training Loss: 0.7986192575074677\n",
      "Epoch 57, Training Loss: 0.7993304163889777\n",
      "Epoch 58, Training Loss: 0.7988995881008922\n",
      "Epoch 59, Training Loss: 0.7987591576755495\n",
      "Epoch 60, Training Loss: 0.7990763366670537\n",
      "Epoch 61, Training Loss: 0.7985862539226848\n",
      "Epoch 62, Training Loss: 0.7980765852713048\n",
      "Epoch 63, Training Loss: 0.7982479355389014\n",
      "Epoch 64, Training Loss: 0.7976116670701737\n",
      "Epoch 65, Training Loss: 0.7987374744020906\n",
      "Epoch 66, Training Loss: 0.7977966952144652\n",
      "Epoch 67, Training Loss: 0.797928626017463\n",
      "Epoch 68, Training Loss: 0.7972679139976214\n",
      "Epoch 69, Training Loss: 0.7974859821169\n",
      "Epoch 70, Training Loss: 0.7975655901700931\n",
      "Epoch 71, Training Loss: 0.7974018591687195\n",
      "Epoch 72, Training Loss: 0.7970837988351521\n",
      "Epoch 73, Training Loss: 0.7968121841437834\n",
      "Epoch 74, Training Loss: 0.7967859089822698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:01:51,611] Trial 215 finished with value: 0.6173333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.797020399480834\n",
      "Epoch 1, Training Loss: 0.8976159442636303\n",
      "Epoch 2, Training Loss: 0.8234165414831692\n",
      "Epoch 3, Training Loss: 0.8167592763900757\n",
      "Epoch 4, Training Loss: 0.8113871706159491\n",
      "Epoch 5, Training Loss: 0.806443602967083\n",
      "Epoch 6, Training Loss: 0.8041736812519847\n",
      "Epoch 7, Training Loss: 0.8019813950796773\n",
      "Epoch 8, Training Loss: 0.7997569658702477\n",
      "Epoch 9, Training Loss: 0.7973843153258016\n",
      "Epoch 10, Training Loss: 0.7966465450767287\n",
      "Epoch 11, Training Loss: 0.7948454386309574\n",
      "Epoch 12, Training Loss: 0.7943464381354196\n",
      "Epoch 13, Training Loss: 0.7937320413446067\n",
      "Epoch 14, Training Loss: 0.792613296759756\n",
      "Epoch 15, Training Loss: 0.791771161914768\n",
      "Epoch 16, Training Loss: 0.791069023232711\n",
      "Epoch 17, Training Loss: 0.7914169055178649\n",
      "Epoch 18, Training Loss: 0.7907621304791673\n",
      "Epoch 19, Training Loss: 0.790599719026035\n",
      "Epoch 20, Training Loss: 0.7900536130245467\n",
      "Epoch 21, Training Loss: 0.7890681847593838\n",
      "Epoch 22, Training Loss: 0.788931915993081\n",
      "Epoch 23, Training Loss: 0.7887681412965731\n",
      "Epoch 24, Training Loss: 0.7888405787317376\n",
      "Epoch 25, Training Loss: 0.7889490806070486\n",
      "Epoch 26, Training Loss: 0.7882489321823407\n",
      "Epoch 27, Training Loss: 0.7884718903025291\n",
      "Epoch 28, Training Loss: 0.7878685357875393\n",
      "Epoch 29, Training Loss: 0.7873225645015114\n",
      "Epoch 30, Training Loss: 0.7883948683738708\n",
      "Epoch 31, Training Loss: 0.7875937960201637\n",
      "Epoch 32, Training Loss: 0.7875327471503638\n",
      "Epoch 33, Training Loss: 0.787387031899359\n",
      "Epoch 34, Training Loss: 0.7866453898580451\n",
      "Epoch 35, Training Loss: 0.7866449916273132\n",
      "Epoch 36, Training Loss: 0.7872133152825492\n",
      "Epoch 37, Training Loss: 0.7868866566428565\n",
      "Epoch 38, Training Loss: 0.7861771838109296\n",
      "Epoch 39, Training Loss: 0.7867327174746004\n",
      "Epoch 40, Training Loss: 0.7865086372633626\n",
      "Epoch 41, Training Loss: 0.7854414498895631\n",
      "Epoch 42, Training Loss: 0.7866248563716286\n",
      "Epoch 43, Training Loss: 0.7855424396077493\n",
      "Epoch 44, Training Loss: 0.7853041391623647\n",
      "Epoch 45, Training Loss: 0.7856455314428287\n",
      "Epoch 46, Training Loss: 0.7848354247279633\n",
      "Epoch 47, Training Loss: 0.7847202730358095\n",
      "Epoch 48, Training Loss: 0.7843935882686672\n",
      "Epoch 49, Training Loss: 0.784880762978604\n",
      "Epoch 50, Training Loss: 0.7842774027272275\n",
      "Epoch 51, Training Loss: 0.7847686107893636\n",
      "Epoch 52, Training Loss: 0.7845479534980946\n",
      "Epoch 53, Training Loss: 0.7835438649905355\n",
      "Epoch 54, Training Loss: 0.7850502537605458\n",
      "Epoch 55, Training Loss: 0.7843778954412705\n",
      "Epoch 56, Training Loss: 0.7839361084134955\n",
      "Epoch 57, Training Loss: 0.7839848158951093\n",
      "Epoch 58, Training Loss: 0.7841946182394386\n",
      "Epoch 59, Training Loss: 0.7838009106485467\n",
      "Epoch 60, Training Loss: 0.783236679367553\n",
      "Epoch 61, Training Loss: 0.7837096118389215\n",
      "Epoch 62, Training Loss: 0.7835767304986939\n",
      "Epoch 63, Training Loss: 0.7830788600713687\n",
      "Epoch 64, Training Loss: 0.7835933910276657\n",
      "Epoch 65, Training Loss: 0.78330635414984\n",
      "Epoch 66, Training Loss: 0.7835421087150287\n",
      "Epoch 67, Training Loss: 0.7830146245490339\n",
      "Epoch 68, Training Loss: 0.7830366328246612\n",
      "Epoch 69, Training Loss: 0.7826620892474526\n",
      "Epoch 70, Training Loss: 0.7828931464288468\n",
      "Epoch 71, Training Loss: 0.7831840782237233\n",
      "Epoch 72, Training Loss: 0.7831983797532275\n",
      "Epoch 73, Training Loss: 0.7830073235626508\n",
      "Epoch 74, Training Loss: 0.7826617008761356\n",
      "Epoch 75, Training Loss: 0.7836569928585139\n",
      "Epoch 76, Training Loss: 0.7826476567669919\n",
      "Epoch 77, Training Loss: 0.7829863974026271\n",
      "Epoch 78, Training Loss: 0.7831226212637765\n",
      "Epoch 79, Training Loss: 0.7821416689937276\n",
      "Epoch 80, Training Loss: 0.7820664163818933\n",
      "Epoch 81, Training Loss: 0.7825590952894741\n",
      "Epoch 82, Training Loss: 0.7828738591724769\n",
      "Epoch 83, Training Loss: 0.7829435031216844\n",
      "Epoch 84, Training Loss: 0.7825199959869672\n",
      "Epoch 85, Training Loss: 0.7825845616204398\n",
      "Epoch 86, Training Loss: 0.7830683066432638\n",
      "Epoch 87, Training Loss: 0.7817496404611974\n",
      "Epoch 88, Training Loss: 0.782819680522259\n",
      "Epoch 89, Training Loss: 0.7826326708148297\n",
      "Epoch 90, Training Loss: 0.7826333115871688\n",
      "Epoch 91, Training Loss: 0.7825851714700685\n",
      "Epoch 92, Training Loss: 0.7826490531290384\n",
      "Epoch 93, Training Loss: 0.7814388890911762\n",
      "Epoch 94, Training Loss: 0.7819326112144872\n",
      "Epoch 95, Training Loss: 0.7815937917931636\n",
      "Epoch 96, Training Loss: 0.7811495364608622\n",
      "Epoch 97, Training Loss: 0.7821134418473208\n",
      "Epoch 98, Training Loss: 0.782485684774872\n",
      "Epoch 99, Training Loss: 0.7819345052977253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:03:41,980] Trial 216 finished with value: 0.622 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.783386845158455\n",
      "Epoch 1, Training Loss: 0.839271022572237\n",
      "Epoch 2, Training Loss: 0.8072729882773231\n",
      "Epoch 3, Training Loss: 0.8019743892725776\n",
      "Epoch 4, Training Loss: 0.8019532153886907\n",
      "Epoch 5, Training Loss: 0.7992028151540195\n",
      "Epoch 6, Training Loss: 0.7977755110404071\n",
      "Epoch 7, Training Loss: 0.7982437891819898\n",
      "Epoch 8, Training Loss: 0.7970210956825929\n",
      "Epoch 9, Training Loss: 0.7963245743863723\n",
      "Epoch 10, Training Loss: 0.7947344092060538\n",
      "Epoch 11, Training Loss: 0.794234144337037\n",
      "Epoch 12, Training Loss: 0.7950203173300799\n",
      "Epoch 13, Training Loss: 0.7939860726805295\n",
      "Epoch 14, Training Loss: 0.7948044011873358\n",
      "Epoch 15, Training Loss: 0.7934886551604552\n",
      "Epoch 16, Training Loss: 0.7939762686280644\n",
      "Epoch 17, Training Loss: 0.7933766192548415\n",
      "Epoch 18, Training Loss: 0.7921491832592908\n",
      "Epoch 19, Training Loss: 0.7918716822652255\n",
      "Epoch 20, Training Loss: 0.7925636214368483\n",
      "Epoch 21, Training Loss: 0.7918689498480629\n",
      "Epoch 22, Training Loss: 0.7917864165586583\n",
      "Epoch 23, Training Loss: 0.7923335782920613\n",
      "Epoch 24, Training Loss: 0.7932898348219255\n",
      "Epoch 25, Training Loss: 0.7926165392819573\n",
      "Epoch 26, Training Loss: 0.7911422613789053\n",
      "Epoch 27, Training Loss: 0.7917694625433753\n",
      "Epoch 28, Training Loss: 0.7916009427519406\n",
      "Epoch 29, Training Loss: 0.7929042051820194\n",
      "Epoch 30, Training Loss: 0.791267607422436\n",
      "Epoch 31, Training Loss: 0.7906483942620894\n",
      "Epoch 32, Training Loss: 0.7899859903840458\n",
      "Epoch 33, Training Loss: 0.7909348055194406\n",
      "Epoch 34, Training Loss: 0.7893726318724015\n",
      "Epoch 35, Training Loss: 0.7900544531205121\n",
      "Epoch 36, Training Loss: 0.7904065748523263\n",
      "Epoch 37, Training Loss: 0.7908493246751673\n",
      "Epoch 38, Training Loss: 0.7904422525097342\n",
      "Epoch 39, Training Loss: 0.7904335348045125\n",
      "Epoch 40, Training Loss: 0.7901225543723387\n",
      "Epoch 41, Training Loss: 0.7899452077641207\n",
      "Epoch 42, Training Loss: 0.7898264298719518\n",
      "Epoch 43, Training Loss: 0.7902238372494192\n",
      "Epoch 44, Training Loss: 0.7896950312922982\n",
      "Epoch 45, Training Loss: 0.7896522685359506\n",
      "Epoch 46, Training Loss: 0.7896463105257819\n",
      "Epoch 47, Training Loss: 0.78979527277105\n",
      "Epoch 48, Training Loss: 0.7897224530051736\n",
      "Epoch 49, Training Loss: 0.7896551166562473\n",
      "Epoch 50, Training Loss: 0.790125675201416\n",
      "Epoch 51, Training Loss: 0.7890160871253294\n",
      "Epoch 52, Training Loss: 0.7895672703490538\n",
      "Epoch 53, Training Loss: 0.7893430803803837\n",
      "Epoch 54, Training Loss: 0.7900125361190122\n",
      "Epoch 55, Training Loss: 0.7885061906365787\n",
      "Epoch 56, Training Loss: 0.7888516698164099\n",
      "Epoch 57, Training Loss: 0.7884582649960238\n",
      "Epoch 58, Training Loss: 0.7885118177357842\n",
      "Epoch 59, Training Loss: 0.7891417298597448\n",
      "Epoch 60, Training Loss: 0.7890348311732797\n",
      "Epoch 61, Training Loss: 0.7888391451975879\n",
      "Epoch 62, Training Loss: 0.7883842748754165\n",
      "Epoch 63, Training Loss: 0.789121071100235\n",
      "Epoch 64, Training Loss: 0.7890648575390087\n",
      "Epoch 65, Training Loss: 0.7878530795433942\n",
      "Epoch 66, Training Loss: 0.7885262783134684\n",
      "Epoch 67, Training Loss: 0.7887697102041805\n",
      "Epoch 68, Training Loss: 0.788469186039532\n",
      "Epoch 69, Training Loss: 0.7886252938298618\n",
      "Epoch 70, Training Loss: 0.7890322624935824\n",
      "Epoch 71, Training Loss: 0.7888860513883479\n",
      "Epoch 72, Training Loss: 0.7887781211207895\n",
      "Epoch 73, Training Loss: 0.7881224884005154\n",
      "Epoch 74, Training Loss: 0.7885129689469057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:05:24,662] Trial 217 finished with value: 0.63 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7883899143162896\n",
      "Epoch 1, Training Loss: 0.9113210011931027\n",
      "Epoch 2, Training Loss: 0.8540412256296943\n",
      "Epoch 3, Training Loss: 0.822557264285929\n",
      "Epoch 4, Training Loss: 0.8107907751027276\n",
      "Epoch 5, Training Loss: 0.806352562343373\n",
      "Epoch 6, Training Loss: 0.8044160739814534\n",
      "Epoch 7, Training Loss: 0.8032349517064936\n",
      "Epoch 8, Training Loss: 0.8026142025695128\n",
      "Epoch 9, Training Loss: 0.8019909053690293\n",
      "Epoch 10, Training Loss: 0.8013108003840727\n",
      "Epoch 11, Training Loss: 0.8009723694184248\n",
      "Epoch 12, Training Loss: 0.8005632807927973\n",
      "Epoch 13, Training Loss: 0.799875502726611\n",
      "Epoch 14, Training Loss: 0.799701383043738\n",
      "Epoch 15, Training Loss: 0.7992198463748483\n",
      "Epoch 16, Training Loss: 0.7991192666923299\n",
      "Epoch 17, Training Loss: 0.7987913077719071\n",
      "Epoch 18, Training Loss: 0.798209844827652\n",
      "Epoch 19, Training Loss: 0.7980256056785584\n",
      "Epoch 20, Training Loss: 0.7980575144290925\n",
      "Epoch 21, Training Loss: 0.7976279736266416\n",
      "Epoch 22, Training Loss: 0.7974001566101523\n",
      "Epoch 23, Training Loss: 0.7970045431922463\n",
      "Epoch 24, Training Loss: 0.796649993167204\n",
      "Epoch 25, Training Loss: 0.7961396739763372\n",
      "Epoch 26, Training Loss: 0.7957559427093057\n",
      "Epoch 27, Training Loss: 0.7956221887644599\n",
      "Epoch 28, Training Loss: 0.7952685100892011\n",
      "Epoch 29, Training Loss: 0.7949620521769805\n",
      "Epoch 30, Training Loss: 0.7948306383104885\n",
      "Epoch 31, Training Loss: 0.7941682968420141\n",
      "Epoch 32, Training Loss: 0.7942034048192641\n",
      "Epoch 33, Training Loss: 0.7933816107581644\n",
      "Epoch 34, Training Loss: 0.7935038575004129\n",
      "Epoch 35, Training Loss: 0.7934006499542909\n",
      "Epoch 36, Training Loss: 0.7931060105211595\n",
      "Epoch 37, Training Loss: 0.7930469581660102\n",
      "Epoch 38, Training Loss: 0.7926731364166035\n",
      "Epoch 39, Training Loss: 0.7922702578236075\n",
      "Epoch 40, Training Loss: 0.7921690951375401\n",
      "Epoch 41, Training Loss: 0.7923360847725588\n",
      "Epoch 42, Training Loss: 0.792089885262882\n",
      "Epoch 43, Training Loss: 0.791839305302676\n",
      "Epoch 44, Training Loss: 0.7918859572270337\n",
      "Epoch 45, Training Loss: 0.7917235178807203\n",
      "Epoch 46, Training Loss: 0.791779380125158\n",
      "Epoch 47, Training Loss: 0.7914290674994974\n",
      "Epoch 48, Training Loss: 0.7912515927763546\n",
      "Epoch 49, Training Loss: 0.791169769413331\n",
      "Epoch 50, Training Loss: 0.7914046957212336\n",
      "Epoch 51, Training Loss: 0.791188379385892\n",
      "Epoch 52, Training Loss: 0.7910335199271932\n",
      "Epoch 53, Training Loss: 0.7910883230321547\n",
      "Epoch 54, Training Loss: 0.79082202371429\n",
      "Epoch 55, Training Loss: 0.790711190069423\n",
      "Epoch 56, Training Loss: 0.790649594559389\n",
      "Epoch 57, Training Loss: 0.7907024934011347\n",
      "Epoch 58, Training Loss: 0.7908207691417021\n",
      "Epoch 59, Training Loss: 0.790693578229231\n",
      "Epoch 60, Training Loss: 0.7908033877260545\n",
      "Epoch 61, Training Loss: 0.7905028739396264\n",
      "Epoch 62, Training Loss: 0.7905563623063705\n",
      "Epoch 63, Training Loss: 0.7905725389368394\n",
      "Epoch 64, Training Loss: 0.7904322144564461\n",
      "Epoch 65, Training Loss: 0.7901733891402973\n",
      "Epoch 66, Training Loss: 0.7902427372511696\n",
      "Epoch 67, Training Loss: 0.7904155548179851\n",
      "Epoch 68, Training Loss: 0.7901056708307828\n",
      "Epoch 69, Training Loss: 0.7900935173034668\n",
      "Epoch 70, Training Loss: 0.7899860065824845\n",
      "Epoch 71, Training Loss: 0.7900464581040775\n",
      "Epoch 72, Training Loss: 0.7898461808877832\n",
      "Epoch 73, Training Loss: 0.7898341884332545\n",
      "Epoch 74, Training Loss: 0.7897243358107174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:06:48,693] Trial 218 finished with value: 0.6394666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7894415299331441\n",
      "Epoch 1, Training Loss: 0.8636278531605139\n",
      "Epoch 2, Training Loss: 0.8284491495978563\n",
      "Epoch 3, Training Loss: 0.8216548083420087\n",
      "Epoch 4, Training Loss: 0.8193093348266487\n",
      "Epoch 5, Training Loss: 0.8162602383391302\n",
      "Epoch 6, Training Loss: 0.8138976772028701\n",
      "Epoch 7, Training Loss: 0.8137999949598671\n",
      "Epoch 8, Training Loss: 0.8108227681396599\n",
      "Epoch 9, Training Loss: 0.8104768540626182\n",
      "Epoch 10, Training Loss: 0.810289960965178\n",
      "Epoch 11, Training Loss: 0.8087770211069207\n",
      "Epoch 12, Training Loss: 0.8084530018326035\n",
      "Epoch 13, Training Loss: 0.8085999446703975\n",
      "Epoch 14, Training Loss: 0.8077068548005326\n",
      "Epoch 15, Training Loss: 0.8071196029060765\n",
      "Epoch 16, Training Loss: 0.8077578473808174\n",
      "Epoch 17, Training Loss: 0.80811132219501\n",
      "Epoch 18, Training Loss: 0.8077199500306208\n",
      "Epoch 19, Training Loss: 0.8070268907941374\n",
      "Epoch 20, Training Loss: 0.8062005207054597\n",
      "Epoch 21, Training Loss: 0.8059113115296328\n",
      "Epoch 22, Training Loss: 0.8064949951673809\n",
      "Epoch 23, Training Loss: 0.8056503834581017\n",
      "Epoch 24, Training Loss: 0.8057597081464036\n",
      "Epoch 25, Training Loss: 0.8055038374169429\n",
      "Epoch 26, Training Loss: 0.8052857014469634\n",
      "Epoch 27, Training Loss: 0.8048693596868587\n",
      "Epoch 28, Training Loss: 0.8052531018292993\n",
      "Epoch 29, Training Loss: 0.8058313170770057\n",
      "Epoch 30, Training Loss: 0.8047452537637008\n",
      "Epoch 31, Training Loss: 0.8041883799366485\n",
      "Epoch 32, Training Loss: 0.8048911584051032\n",
      "Epoch 33, Training Loss: 0.8047385660329259\n",
      "Epoch 34, Training Loss: 0.8058292071622117\n",
      "Epoch 35, Training Loss: 0.8054211011506561\n",
      "Epoch 36, Training Loss: 0.8044206413111292\n",
      "Epoch 37, Training Loss: 0.8056153174629785\n",
      "Epoch 38, Training Loss: 0.8047324394821225\n",
      "Epoch 39, Training Loss: 0.8043786629698331\n",
      "Epoch 40, Training Loss: 0.8040055389691116\n",
      "Epoch 41, Training Loss: 0.803591805920565\n",
      "Epoch 42, Training Loss: 0.8029788855323218\n",
      "Epoch 43, Training Loss: 0.8040437389137154\n",
      "Epoch 44, Training Loss: 0.8043276605749489\n",
      "Epoch 45, Training Loss: 0.8026671272471435\n",
      "Epoch 46, Training Loss: 0.80284519912605\n",
      "Epoch 47, Training Loss: 0.8021521393517802\n",
      "Epoch 48, Training Loss: 0.8031982126092552\n",
      "Epoch 49, Training Loss: 0.8024716703515303\n",
      "Epoch 50, Training Loss: 0.8031961370231514\n",
      "Epoch 51, Training Loss: 0.8026330299843523\n",
      "Epoch 52, Training Loss: 0.801724182214952\n",
      "Epoch 53, Training Loss: 0.8020257818967776\n",
      "Epoch 54, Training Loss: 0.8012202514741654\n",
      "Epoch 55, Training Loss: 0.8023281816253088\n",
      "Epoch 56, Training Loss: 0.80258891752788\n",
      "Epoch 57, Training Loss: 0.8013473270530987\n",
      "Epoch 58, Training Loss: 0.8015074219918789\n",
      "Epoch 59, Training Loss: 0.8006442424049951\n",
      "Epoch 60, Training Loss: 0.8002767532391656\n",
      "Epoch 61, Training Loss: 0.8010088143492103\n",
      "Epoch 62, Training Loss: 0.8009052564326982\n",
      "Epoch 63, Training Loss: 0.8005910816049218\n",
      "Epoch 64, Training Loss: 0.8016335378015848\n",
      "Epoch 65, Training Loss: 0.8001187700078003\n",
      "Epoch 66, Training Loss: 0.8004659288807919\n",
      "Epoch 67, Training Loss: 0.7996687656058404\n",
      "Epoch 68, Training Loss: 0.7994706374362\n",
      "Epoch 69, Training Loss: 0.7995309753973682\n",
      "Epoch 70, Training Loss: 0.799354926894482\n",
      "Epoch 71, Training Loss: 0.7996562511401069\n",
      "Epoch 72, Training Loss: 0.798798204006109\n",
      "Epoch 73, Training Loss: 0.8000981483244358\n",
      "Epoch 74, Training Loss: 0.7995140911941242\n",
      "Epoch 75, Training Loss: 0.7998161069432596\n",
      "Epoch 76, Training Loss: 0.8002142841654613\n",
      "Epoch 77, Training Loss: 0.7990191092168478\n",
      "Epoch 78, Training Loss: 0.7982009046956112\n",
      "Epoch 79, Training Loss: 0.7988008546649962\n",
      "Epoch 80, Training Loss: 0.7993580941867111\n",
      "Epoch 81, Training Loss: 0.7992073787782425\n",
      "Epoch 82, Training Loss: 0.7981159886919467\n",
      "Epoch 83, Training Loss: 0.7985739206909237\n",
      "Epoch 84, Training Loss: 0.7985866914118143\n",
      "Epoch 85, Training Loss: 0.7979560279308405\n",
      "Epoch 86, Training Loss: 0.79749376926207\n",
      "Epoch 87, Training Loss: 0.7973659923202113\n",
      "Epoch 88, Training Loss: 0.7976010052781356\n",
      "Epoch 89, Training Loss: 0.7970294411021067\n",
      "Epoch 90, Training Loss: 0.7974410917525901\n",
      "Epoch 91, Training Loss: 0.796417906006476\n",
      "Epoch 92, Training Loss: 0.7965153947808689\n",
      "Epoch 93, Training Loss: 0.7970999554583901\n",
      "Epoch 94, Training Loss: 0.796864548482393\n",
      "Epoch 95, Training Loss: 0.7968205113159983\n",
      "Epoch 96, Training Loss: 0.7961695418321997\n",
      "Epoch 97, Training Loss: 0.7970074338124211\n",
      "Epoch 98, Training Loss: 0.7965023119646804\n",
      "Epoch 99, Training Loss: 0.796588609362007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:08:23,296] Trial 219 finished with value: 0.6320666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.795425808250456\n",
      "Epoch 1, Training Loss: 0.8492069865675533\n",
      "Epoch 2, Training Loss: 0.8228198252004736\n",
      "Epoch 3, Training Loss: 0.8177080913151011\n",
      "Epoch 4, Training Loss: 0.816175973555621\n",
      "Epoch 5, Training Loss: 0.814293029238196\n",
      "Epoch 6, Training Loss: 0.8131775262075313\n",
      "Epoch 7, Training Loss: 0.8133973802538479\n",
      "Epoch 8, Training Loss: 0.8127010159632739\n",
      "Epoch 9, Training Loss: 0.8139514195919036\n",
      "Epoch 10, Training Loss: 0.81090675536324\n",
      "Epoch 11, Training Loss: 0.8110867984154645\n",
      "Epoch 12, Training Loss: 0.8115543509230895\n",
      "Epoch 13, Training Loss: 0.8111024318723118\n",
      "Epoch 14, Training Loss: 0.8117099268997416\n",
      "Epoch 15, Training Loss: 0.8097945409662584\n",
      "Epoch 16, Training Loss: 0.8094625710739809\n",
      "Epoch 17, Training Loss: 0.8093309505546794\n",
      "Epoch 18, Training Loss: 0.8093623236347647\n",
      "Epoch 19, Training Loss: 0.8071676610497868\n",
      "Epoch 20, Training Loss: 0.8077600505071528\n",
      "Epoch 21, Training Loss: 0.8089998140054591\n",
      "Epoch 22, Training Loss: 0.8084058514763327\n",
      "Epoch 23, Training Loss: 0.8065260068809285\n",
      "Epoch 24, Training Loss: 0.8074352006351246\n",
      "Epoch 25, Training Loss: 0.8064523472505457\n",
      "Epoch 26, Training Loss: 0.8066630029678344\n",
      "Epoch 27, Training Loss: 0.8061185071748845\n",
      "Epoch 28, Training Loss: 0.8055333971276003\n",
      "Epoch 29, Training Loss: 0.8050975807274089\n",
      "Epoch 30, Training Loss: 0.8045375442504883\n",
      "Epoch 31, Training Loss: 0.8030031340262469\n",
      "Epoch 32, Training Loss: 0.8062078170916613\n",
      "Epoch 33, Training Loss: 0.8023894027401419\n",
      "Epoch 34, Training Loss: 0.8040011240454281\n",
      "Epoch 35, Training Loss: 0.8034703626352198\n",
      "Epoch 36, Training Loss: 0.8040862007702099\n",
      "Epoch 37, Training Loss: 0.8027354574904723\n",
      "Epoch 38, Training Loss: 0.8026469924169428\n",
      "Epoch 39, Training Loss: 0.8021930724031785\n",
      "Epoch 40, Training Loss: 0.8016067027344423\n",
      "Epoch 41, Training Loss: 0.8041695765186758\n",
      "Epoch 42, Training Loss: 0.8011977750413558\n",
      "Epoch 43, Training Loss: 0.8042685888094061\n",
      "Epoch 44, Training Loss: 0.7997914932054632\n",
      "Epoch 45, Training Loss: 0.8019089324334089\n",
      "Epoch 46, Training Loss: 0.8018318229563096\n",
      "Epoch 47, Training Loss: 0.7998722729963414\n",
      "Epoch 48, Training Loss: 0.8021480284017675\n",
      "Epoch 49, Training Loss: 0.8015539711363175\n",
      "Epoch 50, Training Loss: 0.8010742348783156\n",
      "Epoch 51, Training Loss: 0.8009059800119961\n",
      "Epoch 52, Training Loss: 0.8001080833463108\n",
      "Epoch 53, Training Loss: 0.7996236553612878\n",
      "Epoch 54, Training Loss: 0.8010858542077681\n",
      "Epoch 55, Training Loss: 0.8003645989474129\n",
      "Epoch 56, Training Loss: 0.8003643272203558\n",
      "Epoch 57, Training Loss: 0.8016416987952064\n",
      "Epoch 58, Training Loss: 0.8005236213347491\n",
      "Epoch 59, Training Loss: 0.7997316437609056\n",
      "Epoch 60, Training Loss: 0.8000914957242854\n",
      "Epoch 61, Training Loss: 0.800496409850962\n",
      "Epoch 62, Training Loss: 0.8012053438495187\n",
      "Epoch 63, Training Loss: 0.8004343257932102\n",
      "Epoch 64, Training Loss: 0.7990237224803252\n",
      "Epoch 65, Training Loss: 0.8011474718065823\n",
      "Epoch 66, Training Loss: 0.7982856588503894\n",
      "Epoch 67, Training Loss: 0.8000062079289381\n",
      "Epoch 68, Training Loss: 0.7988722466020023\n",
      "Epoch 69, Training Loss: 0.798845321781495\n",
      "Epoch 70, Training Loss: 0.8005014829775866\n",
      "Epoch 71, Training Loss: 0.7996609225693871\n",
      "Epoch 72, Training Loss: 0.7996421738231884\n",
      "Epoch 73, Training Loss: 0.7993858495179345\n",
      "Epoch 74, Training Loss: 0.7994798633631538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:09:51,784] Trial 220 finished with value: 0.6318666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7975776997734518\n",
      "Epoch 1, Training Loss: 0.8946755860084877\n",
      "Epoch 2, Training Loss: 0.8238034833642773\n",
      "Epoch 3, Training Loss: 0.8150495061300751\n",
      "Epoch 4, Training Loss: 0.808836088861738\n",
      "Epoch 5, Training Loss: 0.8043946998459952\n",
      "Epoch 6, Training Loss: 0.8010142112136783\n",
      "Epoch 7, Training Loss: 0.7997986828474174\n",
      "Epoch 8, Training Loss: 0.7989545466308307\n",
      "Epoch 9, Training Loss: 0.7972496190465482\n",
      "Epoch 10, Training Loss: 0.7967395170290668\n",
      "Epoch 11, Training Loss: 0.7954064032637087\n",
      "Epoch 12, Training Loss: 0.7947316280881265\n",
      "Epoch 13, Training Loss: 0.7936140435978882\n",
      "Epoch 14, Training Loss: 0.7936862747920187\n",
      "Epoch 15, Training Loss: 0.793494188785553\n",
      "Epoch 16, Training Loss: 0.7929518878011775\n",
      "Epoch 17, Training Loss: 0.7920651556853961\n",
      "Epoch 18, Training Loss: 0.7928986171134432\n",
      "Epoch 19, Training Loss: 0.7924966870393968\n",
      "Epoch 20, Training Loss: 0.792016195085712\n",
      "Epoch 21, Training Loss: 0.7917926317767093\n",
      "Epoch 22, Training Loss: 0.7917725480588755\n",
      "Epoch 23, Training Loss: 0.7905538835023579\n",
      "Epoch 24, Training Loss: 0.7904321029222101\n",
      "Epoch 25, Training Loss: 0.7910815435244625\n",
      "Epoch 26, Training Loss: 0.7904692321791684\n",
      "Epoch 27, Training Loss: 0.7906301376514865\n",
      "Epoch 28, Training Loss: 0.7905697536647768\n",
      "Epoch 29, Training Loss: 0.7896923126134657\n",
      "Epoch 30, Training Loss: 0.7897385506701649\n",
      "Epoch 31, Training Loss: 0.7890393357527884\n",
      "Epoch 32, Training Loss: 0.7907874510700541\n",
      "Epoch 33, Training Loss: 0.7894753241001216\n",
      "Epoch 34, Training Loss: 0.7891811841412595\n",
      "Epoch 35, Training Loss: 0.789512648797573\n",
      "Epoch 36, Training Loss: 0.7884582971271715\n",
      "Epoch 37, Training Loss: 0.7888978397039542\n",
      "Epoch 38, Training Loss: 0.7889634796551296\n",
      "Epoch 39, Training Loss: 0.7883056896969788\n",
      "Epoch 40, Training Loss: 0.7886584003168837\n",
      "Epoch 41, Training Loss: 0.7888177151966812\n",
      "Epoch 42, Training Loss: 0.788429623887055\n",
      "Epoch 43, Training Loss: 0.7882888424665409\n",
      "Epoch 44, Training Loss: 0.7877209796045059\n",
      "Epoch 45, Training Loss: 0.7878038693191414\n",
      "Epoch 46, Training Loss: 0.7878161686703675\n",
      "Epoch 47, Training Loss: 0.7876347407362515\n",
      "Epoch 48, Training Loss: 0.7882675888843106\n",
      "Epoch 49, Training Loss: 0.7876739858684684\n",
      "Epoch 50, Training Loss: 0.7875310170023064\n",
      "Epoch 51, Training Loss: 0.7882048787927269\n",
      "Epoch 52, Training Loss: 0.7881799777647607\n",
      "Epoch 53, Training Loss: 0.787313507642961\n",
      "Epoch 54, Training Loss: 0.7874404027049703\n",
      "Epoch 55, Training Loss: 0.787894302382505\n",
      "Epoch 56, Training Loss: 0.787982506052892\n",
      "Epoch 57, Training Loss: 0.7874204950225084\n",
      "Epoch 58, Training Loss: 0.786801630751531\n",
      "Epoch 59, Training Loss: 0.7869792637968422\n",
      "Epoch 60, Training Loss: 0.7879805047709243\n",
      "Epoch 61, Training Loss: 0.7874920798423595\n",
      "Epoch 62, Training Loss: 0.7883174715185524\n",
      "Epoch 63, Training Loss: 0.787393946217415\n",
      "Epoch 64, Training Loss: 0.7869905326599466\n",
      "Epoch 65, Training Loss: 0.7874805661520563\n",
      "Epoch 66, Training Loss: 0.7878799095189661\n",
      "Epoch 67, Training Loss: 0.787633006196273\n",
      "Epoch 68, Training Loss: 0.7879565347406201\n",
      "Epoch 69, Training Loss: 0.7872716001101903\n",
      "Epoch 70, Training Loss: 0.7876997561383068\n",
      "Epoch 71, Training Loss: 0.7873659736231754\n",
      "Epoch 72, Training Loss: 0.7876036081098973\n",
      "Epoch 73, Training Loss: 0.7871140641377384\n",
      "Epoch 74, Training Loss: 0.7869732669421605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:11:15,707] Trial 221 finished with value: 0.6125333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7877362003003744\n",
      "Epoch 1, Training Loss: 0.9016377834449137\n",
      "Epoch 2, Training Loss: 0.8279706277345357\n",
      "Epoch 3, Training Loss: 0.8208243317173836\n",
      "Epoch 4, Training Loss: 0.8161068066618496\n",
      "Epoch 5, Training Loss: 0.8149104509138523\n",
      "Epoch 6, Training Loss: 0.8125133836179748\n",
      "Epoch 7, Training Loss: 0.8115211547765516\n",
      "Epoch 8, Training Loss: 0.8108849501251278\n",
      "Epoch 9, Training Loss: 0.8089514891904099\n",
      "Epoch 10, Training Loss: 0.8072407166760667\n",
      "Epoch 11, Training Loss: 0.8070233416736574\n",
      "Epoch 12, Training Loss: 0.8058311041136433\n",
      "Epoch 13, Training Loss: 0.8058878126000999\n",
      "Epoch 14, Training Loss: 0.8051717643450974\n",
      "Epoch 15, Training Loss: 0.8044823910060682\n",
      "Epoch 16, Training Loss: 0.8045101661431162\n",
      "Epoch 17, Training Loss: 0.8047064886953598\n",
      "Epoch 18, Training Loss: 0.8034667915867684\n",
      "Epoch 19, Training Loss: 0.8036850273161006\n",
      "Epoch 20, Training Loss: 0.8037487959503231\n",
      "Epoch 21, Training Loss: 0.8042253813349215\n",
      "Epoch 22, Training Loss: 0.8031799147899886\n",
      "Epoch 23, Training Loss: 0.803139389009404\n",
      "Epoch 24, Training Loss: 0.8023666631010242\n",
      "Epoch 25, Training Loss: 0.8025646451720618\n",
      "Epoch 26, Training Loss: 0.8021925821340173\n",
      "Epoch 27, Training Loss: 0.8034127074973028\n",
      "Epoch 28, Training Loss: 0.8015084320441225\n",
      "Epoch 29, Training Loss: 0.8013681160776238\n",
      "Epoch 30, Training Loss: 0.8011404543890989\n",
      "Epoch 31, Training Loss: 0.8007894437564047\n",
      "Epoch 32, Training Loss: 0.800687403338296\n",
      "Epoch 33, Training Loss: 0.8008438914342034\n",
      "Epoch 34, Training Loss: 0.8015962239494897\n",
      "Epoch 35, Training Loss: 0.8009015097653955\n",
      "Epoch 36, Training Loss: 0.7999959070879714\n",
      "Epoch 37, Training Loss: 0.7997688913703861\n",
      "Epoch 38, Training Loss: 0.8000123965112786\n",
      "Epoch 39, Training Loss: 0.7993858877877543\n",
      "Epoch 40, Training Loss: 0.8002007707617337\n",
      "Epoch 41, Training Loss: 0.7998070887156895\n",
      "Epoch 42, Training Loss: 0.79926544024532\n",
      "Epoch 43, Training Loss: 0.7984021591960936\n",
      "Epoch 44, Training Loss: 0.7992498278617859\n",
      "Epoch 45, Training Loss: 0.7988911895823658\n",
      "Epoch 46, Training Loss: 0.7986139557415382\n",
      "Epoch 47, Training Loss: 0.7989561096169895\n",
      "Epoch 48, Training Loss: 0.7985747423387112\n",
      "Epoch 49, Training Loss: 0.797785567923596\n",
      "Epoch 50, Training Loss: 0.7973765399671139\n",
      "Epoch 51, Training Loss: 0.797689706221559\n",
      "Epoch 52, Training Loss: 0.7980380197216693\n",
      "Epoch 53, Training Loss: 0.7980209360445353\n",
      "Epoch 54, Training Loss: 0.7984361501564657\n",
      "Epoch 55, Training Loss: 0.7975196173316554\n",
      "Epoch 56, Training Loss: 0.7974240227749473\n",
      "Epoch 57, Training Loss: 0.7966296500281284\n",
      "Epoch 58, Training Loss: 0.7973680423614674\n",
      "Epoch 59, Training Loss: 0.7973064963978932\n",
      "Epoch 60, Training Loss: 0.7967625821443428\n",
      "Epoch 61, Training Loss: 0.7964774892742472\n",
      "Epoch 62, Training Loss: 0.7967048571522074\n",
      "Epoch 63, Training Loss: 0.796312770000974\n",
      "Epoch 64, Training Loss: 0.795886058377144\n",
      "Epoch 65, Training Loss: 0.7965021196164583\n",
      "Epoch 66, Training Loss: 0.7960605892023646\n",
      "Epoch 67, Training Loss: 0.7959045661123175\n",
      "Epoch 68, Training Loss: 0.7960460621611516\n",
      "Epoch 69, Training Loss: 0.7954979786299226\n",
      "Epoch 70, Training Loss: 0.7956767632100815\n",
      "Epoch 71, Training Loss: 0.7959497436544949\n",
      "Epoch 72, Training Loss: 0.7958042994477695\n",
      "Epoch 73, Training Loss: 0.7967786013631892\n",
      "Epoch 74, Training Loss: 0.7956202183450971\n",
      "Epoch 75, Training Loss: 0.7956856593153531\n",
      "Epoch 76, Training Loss: 0.7951476513891291\n",
      "Epoch 77, Training Loss: 0.7947315717998303\n",
      "Epoch 78, Training Loss: 0.7948968792320195\n",
      "Epoch 79, Training Loss: 0.7952385766165597\n",
      "Epoch 80, Training Loss: 0.7950930028929746\n",
      "Epoch 81, Training Loss: 0.7949902634871634\n",
      "Epoch 82, Training Loss: 0.7945066651903597\n",
      "Epoch 83, Training Loss: 0.7943081214015645\n",
      "Epoch 84, Training Loss: 0.7945283855710711\n",
      "Epoch 85, Training Loss: 0.7941628399648164\n",
      "Epoch 86, Training Loss: 0.7942107707934272\n",
      "Epoch 87, Training Loss: 0.7943041634738893\n",
      "Epoch 88, Training Loss: 0.7944360848656274\n",
      "Epoch 89, Training Loss: 0.7940106418796051\n",
      "Epoch 90, Training Loss: 0.7940618468406505\n",
      "Epoch 91, Training Loss: 0.7939350661478545\n",
      "Epoch 92, Training Loss: 0.7936737397559603\n",
      "Epoch 93, Training Loss: 0.7934124577314334\n",
      "Epoch 94, Training Loss: 0.7933377818953722\n",
      "Epoch 95, Training Loss: 0.793208748864052\n",
      "Epoch 96, Training Loss: 0.7935931912042145\n",
      "Epoch 97, Training Loss: 0.7936568540738042\n",
      "Epoch 98, Training Loss: 0.7924726293499308\n",
      "Epoch 99, Training Loss: 0.793915191270355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:12:50,327] Trial 222 finished with value: 0.6119333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7933213709888601\n",
      "Epoch 1, Training Loss: 0.949293153357685\n",
      "Epoch 2, Training Loss: 0.87490652342488\n",
      "Epoch 3, Training Loss: 0.8308548066849099\n",
      "Epoch 4, Training Loss: 0.8158118222889147\n",
      "Epoch 5, Training Loss: 0.8113461590351019\n",
      "Epoch 6, Training Loss: 0.8099622195824645\n",
      "Epoch 7, Training Loss: 0.808847231076176\n",
      "Epoch 8, Training Loss: 0.8080067486691296\n",
      "Epoch 9, Training Loss: 0.8069137108057065\n",
      "Epoch 10, Training Loss: 0.8070561119488308\n",
      "Epoch 11, Training Loss: 0.8062032911114226\n",
      "Epoch 12, Training Loss: 0.8056014140745751\n",
      "Epoch 13, Training Loss: 0.8054302791007479\n",
      "Epoch 14, Training Loss: 0.8052618766189518\n",
      "Epoch 15, Training Loss: 0.8046516684661235\n",
      "Epoch 16, Training Loss: 0.8050053136689322\n",
      "Epoch 17, Training Loss: 0.803455597789664\n",
      "Epoch 18, Training Loss: 0.8037787797755764\n",
      "Epoch 19, Training Loss: 0.8033157411374544\n",
      "Epoch 20, Training Loss: 0.8029558065242337\n",
      "Epoch 21, Training Loss: 0.8032330983563474\n",
      "Epoch 22, Training Loss: 0.8028861652639575\n",
      "Epoch 23, Training Loss: 0.8027862253045677\n",
      "Epoch 24, Training Loss: 0.8026026334081378\n",
      "Epoch 25, Training Loss: 0.8020428246125243\n",
      "Epoch 26, Training Loss: 0.8017957235637464\n",
      "Epoch 27, Training Loss: 0.8020447112563858\n",
      "Epoch 28, Training Loss: 0.8009805094030567\n",
      "Epoch 29, Training Loss: 0.8012452165883287\n",
      "Epoch 30, Training Loss: 0.801479010474413\n",
      "Epoch 31, Training Loss: 0.8004949589420979\n",
      "Epoch 32, Training Loss: 0.8005886806581254\n",
      "Epoch 33, Training Loss: 0.8008148029334563\n",
      "Epoch 34, Training Loss: 0.8003630776154368\n",
      "Epoch 35, Training Loss: 0.8004485196637031\n",
      "Epoch 36, Training Loss: 0.801340835703943\n",
      "Epoch 37, Training Loss: 0.7997583744221164\n",
      "Epoch 38, Training Loss: 0.7996404637967733\n",
      "Epoch 39, Training Loss: 0.7994011859248455\n",
      "Epoch 40, Training Loss: 0.7995892321256767\n",
      "Epoch 41, Training Loss: 0.7994765456457783\n",
      "Epoch 42, Training Loss: 0.8000603328970142\n",
      "Epoch 43, Training Loss: 0.7991734158723874\n",
      "Epoch 44, Training Loss: 0.7992131648207069\n",
      "Epoch 45, Training Loss: 0.7987812243009869\n",
      "Epoch 46, Training Loss: 0.7991226917818973\n",
      "Epoch 47, Training Loss: 0.7989278364898567\n",
      "Epoch 48, Training Loss: 0.7981129156915765\n",
      "Epoch 49, Training Loss: 0.7989075523570068\n",
      "Epoch 50, Training Loss: 0.7986890202178094\n",
      "Epoch 51, Training Loss: 0.798916208564787\n",
      "Epoch 52, Training Loss: 0.7984648523474098\n",
      "Epoch 53, Training Loss: 0.7977552628158626\n",
      "Epoch 54, Training Loss: 0.7984153291336575\n",
      "Epoch 55, Training Loss: 0.7978157157288458\n",
      "Epoch 56, Training Loss: 0.798430748003766\n",
      "Epoch 57, Training Loss: 0.7982046482258274\n",
      "Epoch 58, Training Loss: 0.7983374246081015\n",
      "Epoch 59, Training Loss: 0.7977626283365981\n",
      "Epoch 60, Training Loss: 0.798314937164909\n",
      "Epoch 61, Training Loss: 0.7972796711706578\n",
      "Epoch 62, Training Loss: 0.7977789690620021\n",
      "Epoch 63, Training Loss: 0.7979895403510646\n",
      "Epoch 64, Training Loss: 0.7980872902655064\n",
      "Epoch 65, Training Loss: 0.798581802486477\n",
      "Epoch 66, Training Loss: 0.7977809260662337\n",
      "Epoch 67, Training Loss: 0.7973753192370996\n",
      "Epoch 68, Training Loss: 0.797461398024308\n",
      "Epoch 69, Training Loss: 0.7976378209608838\n",
      "Epoch 70, Training Loss: 0.7975149737264877\n",
      "Epoch 71, Training Loss: 0.7985513957819544\n",
      "Epoch 72, Training Loss: 0.7979496168911009\n",
      "Epoch 73, Training Loss: 0.7979916359248914\n",
      "Epoch 74, Training Loss: 0.7973268050896494\n",
      "Epoch 75, Training Loss: 0.7974634851728167\n",
      "Epoch 76, Training Loss: 0.7971173452255421\n",
      "Epoch 77, Training Loss: 0.7967507319342821\n",
      "Epoch 78, Training Loss: 0.7974507271795345\n",
      "Epoch 79, Training Loss: 0.7972398809920577\n",
      "Epoch 80, Training Loss: 0.7969980204015746\n",
      "Epoch 81, Training Loss: 0.7972053539484067\n",
      "Epoch 82, Training Loss: 0.7969632332486317\n",
      "Epoch 83, Training Loss: 0.796969837532904\n",
      "Epoch 84, Training Loss: 0.7969494324877746\n",
      "Epoch 85, Training Loss: 0.7969160142697786\n",
      "Epoch 86, Training Loss: 0.797097959285392\n",
      "Epoch 87, Training Loss: 0.7971207217166298\n",
      "Epoch 88, Training Loss: 0.7979803717226014\n",
      "Epoch 89, Training Loss: 0.7965501020725508\n",
      "Epoch 90, Training Loss: 0.7972410193959573\n",
      "Epoch 91, Training Loss: 0.7970783508809886\n",
      "Epoch 92, Training Loss: 0.7975615308697063\n",
      "Epoch 93, Training Loss: 0.7966657293470283\n",
      "Epoch 94, Training Loss: 0.7971118157967589\n",
      "Epoch 95, Training Loss: 0.7968477545824266\n",
      "Epoch 96, Training Loss: 0.7968155661023649\n",
      "Epoch 97, Training Loss: 0.7970182911793988\n",
      "Epoch 98, Training Loss: 0.7971417755112612\n",
      "Epoch 99, Training Loss: 0.7961848255835081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:14:29,665] Trial 223 finished with value: 0.6333333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7967197644979434\n",
      "Epoch 1, Training Loss: 0.8440350954693959\n",
      "Epoch 2, Training Loss: 0.8113894346065091\n",
      "Epoch 3, Training Loss: 0.8091963584261729\n",
      "Epoch 4, Training Loss: 0.8064038093825032\n",
      "Epoch 5, Training Loss: 0.8049713810583703\n",
      "Epoch 6, Training Loss: 0.8060510190805994\n",
      "Epoch 7, Training Loss: 0.8024548457529311\n",
      "Epoch 8, Training Loss: 0.8010669580079559\n",
      "Epoch 9, Training Loss: 0.8018486290946043\n",
      "Epoch 10, Training Loss: 0.8014004044066695\n",
      "Epoch 11, Training Loss: 0.801532224604958\n",
      "Epoch 12, Training Loss: 0.8011244504971612\n",
      "Epoch 13, Training Loss: 0.7977429739514688\n",
      "Epoch 14, Training Loss: 0.8009550979262904\n",
      "Epoch 15, Training Loss: 0.8004914671854866\n",
      "Epoch 16, Training Loss: 0.8013753864998208\n",
      "Epoch 17, Training Loss: 0.797477864860592\n",
      "Epoch 18, Training Loss: 0.7992594382368532\n",
      "Epoch 19, Training Loss: 0.798202379484822\n",
      "Epoch 20, Training Loss: 0.7983074341501508\n",
      "Epoch 21, Training Loss: 0.7999845105006282\n",
      "Epoch 22, Training Loss: 0.7974517199329864\n",
      "Epoch 23, Training Loss: 0.7965895861163175\n",
      "Epoch 24, Training Loss: 0.7979334290762593\n",
      "Epoch 25, Training Loss: 0.7979249238071585\n",
      "Epoch 26, Training Loss: 0.7986264975447404\n",
      "Epoch 27, Training Loss: 0.7975230479598941\n",
      "Epoch 28, Training Loss: 0.7969172466966442\n",
      "Epoch 29, Training Loss: 0.7980144382419443\n",
      "Epoch 30, Training Loss: 0.7979417732783727\n",
      "Epoch 31, Training Loss: 0.7973511253084455\n",
      "Epoch 32, Training Loss: 0.796490934916905\n",
      "Epoch 33, Training Loss: 0.797641927048676\n",
      "Epoch 34, Training Loss: 0.7965281348927576\n",
      "Epoch 35, Training Loss: 0.7977011276367015\n",
      "Epoch 36, Training Loss: 0.7964773575166114\n",
      "Epoch 37, Training Loss: 0.7967001969652965\n",
      "Epoch 38, Training Loss: 0.7973260734762464\n",
      "Epoch 39, Training Loss: 0.7946898528955932\n",
      "Epoch 40, Training Loss: 0.7964181153397811\n",
      "Epoch 41, Training Loss: 0.7952499078628712\n",
      "Epoch 42, Training Loss: 0.7960799011072718\n",
      "Epoch 43, Training Loss: 0.7959506764447779\n",
      "Epoch 44, Training Loss: 0.7975259749512923\n",
      "Epoch 45, Training Loss: 0.7953267810936261\n",
      "Epoch 46, Training Loss: 0.7973362522913997\n",
      "Epoch 47, Training Loss: 0.7967213421835935\n",
      "Epoch 48, Training Loss: 0.7967867476599557\n",
      "Epoch 49, Training Loss: 0.7952061272205266\n",
      "Epoch 50, Training Loss: 0.7944365335586375\n",
      "Epoch 51, Training Loss: 0.7948915084501854\n",
      "Epoch 52, Training Loss: 0.7958842980234246\n",
      "Epoch 53, Training Loss: 0.7942501225865873\n",
      "Epoch 54, Training Loss: 0.7962541133837593\n",
      "Epoch 55, Training Loss: 0.7954039932193613\n",
      "Epoch 56, Training Loss: 0.7947818134064065\n",
      "Epoch 57, Training Loss: 0.7952794124309281\n",
      "Epoch 58, Training Loss: 0.7948795665475659\n",
      "Epoch 59, Training Loss: 0.7962199866323543\n",
      "Epoch 60, Training Loss: 0.795347605791307\n",
      "Epoch 61, Training Loss: 0.7952364603379616\n",
      "Epoch 62, Training Loss: 0.7950824806564732\n",
      "Epoch 63, Training Loss: 0.7953157318265814\n",
      "Epoch 64, Training Loss: 0.7949956402742773\n",
      "Epoch 65, Training Loss: 0.7947556060955937\n",
      "Epoch 66, Training Loss: 0.796189762775163\n",
      "Epoch 67, Training Loss: 0.7943853456274907\n",
      "Epoch 68, Training Loss: 0.7944831934190334\n",
      "Epoch 69, Training Loss: 0.7934625266189862\n",
      "Epoch 70, Training Loss: 0.795032857414475\n",
      "Epoch 71, Training Loss: 0.7957185715661013\n",
      "Epoch 72, Training Loss: 0.7935427611035512\n",
      "Epoch 73, Training Loss: 0.7942411966789934\n",
      "Epoch 74, Training Loss: 0.795403602248744\n",
      "Epoch 75, Training Loss: 0.7939298547300181\n",
      "Epoch 76, Training Loss: 0.7949686241329165\n",
      "Epoch 77, Training Loss: 0.7946395360437551\n",
      "Epoch 78, Training Loss: 0.7944221336142461\n",
      "Epoch 79, Training Loss: 0.7944861524087146\n",
      "Epoch 80, Training Loss: 0.795745183202557\n",
      "Epoch 81, Training Loss: 0.7951768879603622\n",
      "Epoch 82, Training Loss: 0.7955884042539094\n",
      "Epoch 83, Training Loss: 0.7956819431226057\n",
      "Epoch 84, Training Loss: 0.7952295833064201\n",
      "Epoch 85, Training Loss: 0.7949883093511252\n",
      "Epoch 86, Training Loss: 0.7952998211509303\n",
      "Epoch 87, Training Loss: 0.7939038247990429\n",
      "Epoch 88, Training Loss: 0.7939051332330345\n",
      "Epoch 89, Training Loss: 0.7943056534107467\n",
      "Epoch 90, Training Loss: 0.7947769668765534\n",
      "Epoch 91, Training Loss: 0.7962309414282778\n",
      "Epoch 92, Training Loss: 0.7954268040513633\n",
      "Epoch 93, Training Loss: 0.7932798097904463\n",
      "Epoch 94, Training Loss: 0.7941199898719787\n",
      "Epoch 95, Training Loss: 0.7952069763850449\n",
      "Epoch 96, Training Loss: 0.7947934960960445\n",
      "Epoch 97, Training Loss: 0.7931342250422427\n",
      "Epoch 98, Training Loss: 0.7941746563839733\n",
      "Epoch 99, Training Loss: 0.7944374336335892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:16:26,190] Trial 224 finished with value: 0.6345333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7942624587761729\n",
      "Epoch 1, Training Loss: 0.9428369825727799\n",
      "Epoch 2, Training Loss: 0.871301552548128\n",
      "Epoch 3, Training Loss: 0.8619072733907138\n",
      "Epoch 4, Training Loss: 0.8568855521258186\n",
      "Epoch 5, Training Loss: 0.8536983680023866\n",
      "Epoch 6, Training Loss: 0.8541890491457547\n",
      "Epoch 7, Training Loss: 0.8558437850194819\n",
      "Epoch 8, Training Loss: 0.8519843585350935\n",
      "Epoch 9, Training Loss: 0.8519836581454557\n",
      "Epoch 10, Training Loss: 0.8520992863178253\n",
      "Epoch 11, Training Loss: 0.8526275566044976\n",
      "Epoch 12, Training Loss: 0.8511013009267695\n",
      "Epoch 13, Training Loss: 0.8495118160107556\n",
      "Epoch 14, Training Loss: 0.8474405813918394\n",
      "Epoch 15, Training Loss: 0.8472340979295618\n",
      "Epoch 16, Training Loss: 0.8483093049245722\n",
      "Epoch 17, Training Loss: 0.8481790928980883\n",
      "Epoch 18, Training Loss: 0.8487423440989326\n",
      "Epoch 19, Training Loss: 0.8475285110052894\n",
      "Epoch 20, Training Loss: 0.8468731722410987\n",
      "Epoch 21, Training Loss: 0.8481249136083266\n",
      "Epoch 22, Training Loss: 0.8473706898268532\n",
      "Epoch 23, Training Loss: 0.8455792955090018\n",
      "Epoch 24, Training Loss: 0.8454173599271213\n",
      "Epoch 25, Training Loss: 0.8442895421561073\n",
      "Epoch 26, Training Loss: 0.8466642745102153\n",
      "Epoch 27, Training Loss: 0.8465684314335093\n",
      "Epoch 28, Training Loss: 0.8447888682870304\n",
      "Epoch 29, Training Loss: 0.8429734859045814\n",
      "Epoch 30, Training Loss: 0.844853405531715\n",
      "Epoch 31, Training Loss: 0.845217640470056\n",
      "Epoch 32, Training Loss: 0.8451039664184345\n",
      "Epoch 33, Training Loss: 0.8454492712722105\n",
      "Epoch 34, Training Loss: 0.8413179081327775\n",
      "Epoch 35, Training Loss: 0.8424021912322325\n",
      "Epoch 36, Training Loss: 0.8422413847726934\n",
      "Epoch 37, Training Loss: 0.8442886880566092\n",
      "Epoch 38, Training Loss: 0.8418786424047807\n",
      "Epoch 39, Training Loss: 0.8415850269093234\n",
      "Epoch 40, Training Loss: 0.8394973257709952\n",
      "Epoch 41, Training Loss: 0.8409080468205845\n",
      "Epoch 42, Training Loss: 0.8463257058928995\n",
      "Epoch 43, Training Loss: 0.8445556144153371\n",
      "Epoch 44, Training Loss: 0.8433578257700977\n",
      "Epoch 45, Training Loss: 0.8431610377395854\n",
      "Epoch 46, Training Loss: 0.8419196758550757\n",
      "Epoch 47, Training Loss: 0.8407622974059161\n",
      "Epoch 48, Training Loss: 0.8404946853132809\n",
      "Epoch 49, Training Loss: 0.8426509700101965\n",
      "Epoch 50, Training Loss: 0.8428171704095953\n",
      "Epoch 51, Training Loss: 0.8403427682904636\n",
      "Epoch 52, Training Loss: 0.8440282348324271\n",
      "Epoch 53, Training Loss: 0.8425157108026392\n",
      "Epoch 54, Training Loss: 0.8423221251543831\n",
      "Epoch 55, Training Loss: 0.842925595816444\n",
      "Epoch 56, Training Loss: 0.8448523329987245\n",
      "Epoch 57, Training Loss: 0.8416846736038432\n",
      "Epoch 58, Training Loss: 0.8424584567546844\n",
      "Epoch 59, Training Loss: 0.8420168800914989\n",
      "Epoch 60, Training Loss: 0.8432218892434064\n",
      "Epoch 61, Training Loss: 0.8410981996620402\n",
      "Epoch 62, Training Loss: 0.8421855379553402\n",
      "Epoch 63, Training Loss: 0.8401116026149077\n",
      "Epoch 64, Training Loss: 0.8394867446142085\n",
      "Epoch 65, Training Loss: 0.8413435599383186\n",
      "Epoch 66, Training Loss: 0.8397508532159469\n",
      "Epoch 67, Training Loss: 0.8385691536875333\n",
      "Epoch 68, Training Loss: 0.838934450640398\n",
      "Epoch 69, Training Loss: 0.8385411435015061\n",
      "Epoch 70, Training Loss: 0.8402378845916075\n",
      "Epoch 71, Training Loss: 0.8398006138380836\n",
      "Epoch 72, Training Loss: 0.8380896482748144\n",
      "Epoch 73, Training Loss: 0.840958844423294\n",
      "Epoch 74, Training Loss: 0.8410169941537521\n",
      "Epoch 75, Training Loss: 0.8403616419960471\n",
      "Epoch 76, Training Loss: 0.8417636709353503\n",
      "Epoch 77, Training Loss: 0.841422807819703\n",
      "Epoch 78, Training Loss: 0.8397598027481752\n",
      "Epoch 79, Training Loss: 0.8383236451709972\n",
      "Epoch 80, Training Loss: 0.8401108901640948\n",
      "Epoch 81, Training Loss: 0.8387108508979573\n",
      "Epoch 82, Training Loss: 0.8389127569338855\n",
      "Epoch 83, Training Loss: 0.8375408833167132\n",
      "Epoch 84, Training Loss: 0.8385135755819433\n",
      "Epoch 85, Training Loss: 0.8391568968576544\n",
      "Epoch 86, Training Loss: 0.8415356857636396\n",
      "Epoch 87, Training Loss: 0.8389099374939414\n",
      "Epoch 88, Training Loss: 0.8378113083979662\n",
      "Epoch 89, Training Loss: 0.8412727576844833\n",
      "Epoch 90, Training Loss: 0.8404724581802593\n",
      "Epoch 91, Training Loss: 0.8403045737743378\n",
      "Epoch 92, Training Loss: 0.837289159858928\n",
      "Epoch 93, Training Loss: 0.841567609240027\n",
      "Epoch 94, Training Loss: 0.8407314305445727\n",
      "Epoch 95, Training Loss: 0.8388245318216436\n",
      "Epoch 96, Training Loss: 0.8405142741343554\n",
      "Epoch 97, Training Loss: 0.8394083619818968\n",
      "Epoch 98, Training Loss: 0.8378499528940986\n",
      "Epoch 99, Training Loss: 0.8374574091153987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:18:34,222] Trial 225 finished with value: 0.6152 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8381947724258199\n",
      "Epoch 1, Training Loss: 0.8950598161919673\n",
      "Epoch 2, Training Loss: 0.8239707448428735\n",
      "Epoch 3, Training Loss: 0.8178751581593564\n",
      "Epoch 4, Training Loss: 0.8132410153410489\n",
      "Epoch 5, Training Loss: 0.8124762612177914\n",
      "Epoch 6, Training Loss: 0.8107008778959288\n",
      "Epoch 7, Training Loss: 0.8083317569323949\n",
      "Epoch 8, Training Loss: 0.8069366612828763\n",
      "Epoch 9, Training Loss: 0.8070018640138152\n",
      "Epoch 10, Training Loss: 0.8051171409456354\n",
      "Epoch 11, Training Loss: 0.8042127186194399\n",
      "Epoch 12, Training Loss: 0.8047134022963675\n",
      "Epoch 13, Training Loss: 0.8032254398317266\n",
      "Epoch 14, Training Loss: 0.8020137422963193\n",
      "Epoch 15, Training Loss: 0.8030249024692334\n",
      "Epoch 16, Training Loss: 0.8021607239443557\n",
      "Epoch 17, Training Loss: 0.8019228752394368\n",
      "Epoch 18, Training Loss: 0.8026054815242165\n",
      "Epoch 19, Training Loss: 0.8014953899204282\n",
      "Epoch 20, Training Loss: 0.8008145313513907\n",
      "Epoch 21, Training Loss: 0.8014438111979262\n",
      "Epoch 22, Training Loss: 0.8014279096646416\n",
      "Epoch 23, Training Loss: 0.8001668522232457\n",
      "Epoch 24, Training Loss: 0.8008189713148246\n",
      "Epoch 25, Training Loss: 0.8008928555294983\n",
      "Epoch 26, Training Loss: 0.8003532362163515\n",
      "Epoch 27, Training Loss: 0.7989906962652852\n",
      "Epoch 28, Training Loss: 0.7997857238117018\n",
      "Epoch 29, Training Loss: 0.80028451501875\n",
      "Epoch 30, Training Loss: 0.8004004473076727\n",
      "Epoch 31, Training Loss: 0.7995538776082204\n",
      "Epoch 32, Training Loss: 0.8001012017852381\n",
      "Epoch 33, Training Loss: 0.7998264784203436\n",
      "Epoch 34, Training Loss: 0.7988834083528447\n",
      "Epoch 35, Training Loss: 0.8005284304009345\n",
      "Epoch 36, Training Loss: 0.7992687248645869\n",
      "Epoch 37, Training Loss: 0.7992879516200015\n",
      "Epoch 38, Training Loss: 0.7991626310169249\n",
      "Epoch 39, Training Loss: 0.7989752384953033\n",
      "Epoch 40, Training Loss: 0.7996054264835846\n",
      "Epoch 41, Training Loss: 0.7997292231796379\n",
      "Epoch 42, Training Loss: 0.7989995921464791\n",
      "Epoch 43, Training Loss: 0.7983679303549286\n",
      "Epoch 44, Training Loss: 0.7993817078439813\n",
      "Epoch 45, Training Loss: 0.798671683601867\n",
      "Epoch 46, Training Loss: 0.7991439545961251\n",
      "Epoch 47, Training Loss: 0.7986654317468629\n",
      "Epoch 48, Training Loss: 0.7987157066961876\n",
      "Epoch 49, Training Loss: 0.7973832928148428\n",
      "Epoch 50, Training Loss: 0.7982572039267174\n",
      "Epoch 51, Training Loss: 0.7975626879168632\n",
      "Epoch 52, Training Loss: 0.7993485169303148\n",
      "Epoch 53, Training Loss: 0.7985658186718934\n",
      "Epoch 54, Training Loss: 0.7978726637094541\n",
      "Epoch 55, Training Loss: 0.7997426707045476\n",
      "Epoch 56, Training Loss: 0.7990058812879979\n",
      "Epoch 57, Training Loss: 0.797654460336929\n",
      "Epoch 58, Training Loss: 0.7978018379749212\n",
      "Epoch 59, Training Loss: 0.7983978248180303\n",
      "Epoch 60, Training Loss: 0.7979590764619354\n",
      "Epoch 61, Training Loss: 0.7985381782503056\n",
      "Epoch 62, Training Loss: 0.7989512614737776\n",
      "Epoch 63, Training Loss: 0.7981832784817631\n",
      "Epoch 64, Training Loss: 0.7974681848870184\n",
      "Epoch 65, Training Loss: 0.7981388688087463\n",
      "Epoch 66, Training Loss: 0.7981150928296541\n",
      "Epoch 67, Training Loss: 0.7977269763337043\n",
      "Epoch 68, Training Loss: 0.7983773440346682\n",
      "Epoch 69, Training Loss: 0.7978925908418526\n",
      "Epoch 70, Training Loss: 0.7975235342083121\n",
      "Epoch 71, Training Loss: 0.7971785897599127\n",
      "Epoch 72, Training Loss: 0.7975895754376748\n",
      "Epoch 73, Training Loss: 0.7973879786362326\n",
      "Epoch 74, Training Loss: 0.7980988979339599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:19:59,084] Trial 226 finished with value: 0.6115333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7967629809576766\n",
      "Epoch 1, Training Loss: 1.076276561909152\n",
      "Epoch 2, Training Loss: 1.0198052999668552\n",
      "Epoch 3, Training Loss: 0.9949358346766995\n",
      "Epoch 4, Training Loss: 0.9816743445575685\n",
      "Epoch 5, Training Loss: 0.973520146635242\n",
      "Epoch 6, Training Loss: 0.9686675994019759\n",
      "Epoch 7, Training Loss: 0.9652399484376262\n",
      "Epoch 8, Training Loss: 0.9629913107793134\n",
      "Epoch 9, Training Loss: 0.9614320415303224\n",
      "Epoch 10, Training Loss: 0.9599010678162252\n",
      "Epoch 11, Training Loss: 0.958834821747658\n",
      "Epoch 12, Training Loss: 0.9583670559682345\n",
      "Epoch 13, Training Loss: 0.9576155405295522\n",
      "Epoch 14, Training Loss: 0.9566540597973013\n",
      "Epoch 15, Training Loss: 0.9558325032542523\n",
      "Epoch 16, Training Loss: 0.9550039044000153\n",
      "Epoch 17, Training Loss: 0.9544541517594703\n",
      "Epoch 18, Training Loss: 0.9535232818216309\n",
      "Epoch 19, Training Loss: 0.9531903192512972\n",
      "Epoch 20, Training Loss: 0.9524559501418494\n",
      "Epoch 21, Training Loss: 0.9522608501570565\n",
      "Epoch 22, Training Loss: 0.9508954292849491\n",
      "Epoch 23, Training Loss: 0.9503112590402589\n",
      "Epoch 24, Training Loss: 0.9500885775214747\n",
      "Epoch 25, Training Loss: 0.949720767655767\n",
      "Epoch 26, Training Loss: 0.9486101434643107\n",
      "Epoch 27, Training Loss: 0.9476753298501323\n",
      "Epoch 28, Training Loss: 0.9469145720166371\n",
      "Epoch 29, Training Loss: 0.9462567808036517\n",
      "Epoch 30, Training Loss: 0.9460418015494383\n",
      "Epoch 31, Training Loss: 0.9452162712140191\n",
      "Epoch 32, Training Loss: 0.9444300134379164\n",
      "Epoch 33, Training Loss: 0.9443760623609213\n",
      "Epoch 34, Training Loss: 0.9434265508687586\n",
      "Epoch 35, Training Loss: 0.943083277501558\n",
      "Epoch 36, Training Loss: 0.9422236318875076\n",
      "Epoch 37, Training Loss: 0.941346692321892\n",
      "Epoch 38, Training Loss: 0.9407904261933233\n",
      "Epoch 39, Training Loss: 0.9400445212098889\n",
      "Epoch 40, Training Loss: 0.9388981128097477\n",
      "Epoch 41, Training Loss: 0.9389053769577714\n",
      "Epoch 42, Training Loss: 0.9385290121673642\n",
      "Epoch 43, Training Loss: 0.9370671149483301\n",
      "Epoch 44, Training Loss: 0.9366984152256098\n",
      "Epoch 45, Training Loss: 0.9364533027311913\n",
      "Epoch 46, Training Loss: 0.9353472378917207\n",
      "Epoch 47, Training Loss: 0.9347104851464579\n",
      "Epoch 48, Training Loss: 0.9346270837281879\n",
      "Epoch 49, Training Loss: 0.9334507839124006\n",
      "Epoch 50, Training Loss: 0.9331581320081438\n",
      "Epoch 51, Training Loss: 0.9325586102062599\n",
      "Epoch 52, Training Loss: 0.9316107095632338\n",
      "Epoch 53, Training Loss: 0.9306083384313082\n",
      "Epoch 54, Training Loss: 0.9297037910698052\n",
      "Epoch 55, Training Loss: 0.9290910225165517\n",
      "Epoch 56, Training Loss: 0.9287209249080572\n",
      "Epoch 57, Training Loss: 0.9280420920902626\n",
      "Epoch 58, Training Loss: 0.9273991252246656\n",
      "Epoch 59, Training Loss: 0.9262259843654203\n",
      "Epoch 60, Training Loss: 0.9256850417395284\n",
      "Epoch 61, Training Loss: 0.9250053740085515\n",
      "Epoch 62, Training Loss: 0.9247976370323869\n",
      "Epoch 63, Training Loss: 0.9240938925205316\n",
      "Epoch 64, Training Loss: 0.9229319053484981\n",
      "Epoch 65, Training Loss: 0.9219287197392686\n",
      "Epoch 66, Training Loss: 0.9215560703349293\n",
      "Epoch 67, Training Loss: 0.921077465204368\n",
      "Epoch 68, Training Loss: 0.9198355561808536\n",
      "Epoch 69, Training Loss: 0.9190541695831413\n",
      "Epoch 70, Training Loss: 0.9184055528246371\n",
      "Epoch 71, Training Loss: 0.9175104110760797\n",
      "Epoch 72, Training Loss: 0.916852171080453\n",
      "Epoch 73, Training Loss: 0.9162807307745281\n",
      "Epoch 74, Training Loss: 0.9154217952176145\n",
      "Epoch 75, Training Loss: 0.9148404885951738\n",
      "Epoch 76, Training Loss: 0.9136509614779537\n",
      "Epoch 77, Training Loss: 0.912957393316398\n",
      "Epoch 78, Training Loss: 0.9122812593789925\n",
      "Epoch 79, Training Loss: 0.9113882331023538\n",
      "Epoch 80, Training Loss: 0.910716354129906\n",
      "Epoch 81, Training Loss: 0.9100144083338573\n",
      "Epoch 82, Training Loss: 0.9092628633169303\n",
      "Epoch 83, Training Loss: 0.908559342882687\n",
      "Epoch 84, Training Loss: 0.9073242921578256\n",
      "Epoch 85, Training Loss: 0.9071193959479942\n",
      "Epoch 86, Training Loss: 0.9058965979662157\n",
      "Epoch 87, Training Loss: 0.9061562764913516\n",
      "Epoch 88, Training Loss: 0.9047138573531818\n",
      "Epoch 89, Training Loss: 0.9039099056917922\n",
      "Epoch 90, Training Loss: 0.9027611188422469\n",
      "Epoch 91, Training Loss: 0.9025077383321031\n",
      "Epoch 92, Training Loss: 0.901428424863887\n",
      "Epoch 93, Training Loss: 0.9004621846335275\n",
      "Epoch 94, Training Loss: 0.8993959392820086\n",
      "Epoch 95, Training Loss: 0.8989387854597622\n",
      "Epoch 96, Training Loss: 0.8983382907128872\n",
      "Epoch 97, Training Loss: 0.8979058977356531\n",
      "Epoch 98, Training Loss: 0.896448974949973\n",
      "Epoch 99, Training Loss: 0.8961179931360976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:21:22,493] Trial 227 finished with value: 0.5786666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8948713723878216\n",
      "Epoch 1, Training Loss: 0.8721907319040859\n",
      "Epoch 2, Training Loss: 0.81746357700404\n",
      "Epoch 3, Training Loss: 0.8139138057652642\n",
      "Epoch 4, Training Loss: 0.8099160010674421\n",
      "Epoch 5, Training Loss: 0.8096544666150037\n",
      "Epoch 6, Training Loss: 0.8079607472700231\n",
      "Epoch 7, Training Loss: 0.8067893892176011\n",
      "Epoch 8, Training Loss: 0.8064238079155193\n",
      "Epoch 9, Training Loss: 0.8067491323807661\n",
      "Epoch 10, Training Loss: 0.8057911497705122\n",
      "Epoch 11, Training Loss: 0.80562520440887\n",
      "Epoch 12, Training Loss: 0.8052173463737263\n",
      "Epoch 13, Training Loss: 0.8055024188406327\n",
      "Epoch 14, Training Loss: 0.8049860813337214\n",
      "Epoch 15, Training Loss: 0.8043067469316371\n",
      "Epoch 16, Training Loss: 0.8034678633072797\n",
      "Epoch 17, Training Loss: 0.8037340413121616\n",
      "Epoch 18, Training Loss: 0.8030020897528705\n",
      "Epoch 19, Training Loss: 0.8023817021005294\n",
      "Epoch 20, Training Loss: 0.8022080673189724\n",
      "Epoch 21, Training Loss: 0.8020336095024557\n",
      "Epoch 22, Training Loss: 0.8019716149919174\n",
      "Epoch 23, Training Loss: 0.8027079050681171\n",
      "Epoch 24, Training Loss: 0.8006938867007984\n",
      "Epoch 25, Training Loss: 0.8012251095210805\n",
      "Epoch 26, Training Loss: 0.8008152099216685\n",
      "Epoch 27, Training Loss: 0.8012843121500576\n",
      "Epoch 28, Training Loss: 0.801446420024423\n",
      "Epoch 29, Training Loss: 0.799281578134088\n",
      "Epoch 30, Training Loss: 0.8001409445089452\n",
      "Epoch 31, Training Loss: 0.7995833906005411\n",
      "Epoch 32, Training Loss: 0.7980921332275166\n",
      "Epoch 33, Training Loss: 0.7980573084775139\n",
      "Epoch 34, Training Loss: 0.7982537642647238\n",
      "Epoch 35, Training Loss: 0.797761530455421\n",
      "Epoch 36, Training Loss: 0.7975415546753827\n",
      "Epoch 37, Training Loss: 0.7978565565978779\n",
      "Epoch 38, Training Loss: 0.7973781906155979\n",
      "Epoch 39, Training Loss: 0.7973128629432005\n",
      "Epoch 40, Training Loss: 0.7964557475202224\n",
      "Epoch 41, Training Loss: 0.7966496783845565\n",
      "Epoch 42, Training Loss: 0.7951931272534764\n",
      "Epoch 43, Training Loss: 0.7952106970198014\n",
      "Epoch 44, Training Loss: 0.7960416310675004\n",
      "Epoch 45, Training Loss: 0.796246729037341\n",
      "Epoch 46, Training Loss: 0.7950191895400777\n",
      "Epoch 47, Training Loss: 0.7946578491435331\n",
      "Epoch 48, Training Loss: 0.7949320509153254\n",
      "Epoch 49, Training Loss: 0.7943311089627882\n",
      "Epoch 50, Training Loss: 0.7942601493526907\n",
      "Epoch 51, Training Loss: 0.7941988006760092\n",
      "Epoch 52, Training Loss: 0.794001337219687\n",
      "Epoch 53, Training Loss: 0.7944736163055196\n",
      "Epoch 54, Training Loss: 0.7938370098787195\n",
      "Epoch 55, Training Loss: 0.793856074108797\n",
      "Epoch 56, Training Loss: 0.7930072440119351\n",
      "Epoch 57, Training Loss: 0.794182248396032\n",
      "Epoch 58, Training Loss: 0.7934186151448418\n",
      "Epoch 59, Training Loss: 0.7931417411215165\n",
      "Epoch 60, Training Loss: 0.7943131359885721\n",
      "Epoch 61, Training Loss: 0.7932690766979666\n",
      "Epoch 62, Training Loss: 0.7934271945672877\n",
      "Epoch 63, Training Loss: 0.7932014609785641\n",
      "Epoch 64, Training Loss: 0.7930151794237249\n",
      "Epoch 65, Training Loss: 0.7930794931860531\n",
      "Epoch 66, Training Loss: 0.7924754175017862\n",
      "Epoch 67, Training Loss: 0.7924703069995431\n",
      "Epoch 68, Training Loss: 0.7921293580532074\n",
      "Epoch 69, Training Loss: 0.7922691816442153\n",
      "Epoch 70, Training Loss: 0.7927089428901672\n",
      "Epoch 71, Training Loss: 0.7926561464982874\n",
      "Epoch 72, Training Loss: 0.7917940091385561\n",
      "Epoch 73, Training Loss: 0.7920504752327414\n",
      "Epoch 74, Training Loss: 0.7916396267975078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:22:51,182] Trial 228 finished with value: 0.6353333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.792353118798312\n",
      "Epoch 1, Training Loss: 0.9698878511961768\n",
      "Epoch 2, Training Loss: 0.9360449872998631\n",
      "Epoch 3, Training Loss: 0.9149317927220288\n",
      "Epoch 4, Training Loss: 0.8900062193590051\n",
      "Epoch 5, Training Loss: 0.8660219885321224\n",
      "Epoch 6, Training Loss: 0.8466148333689746\n",
      "Epoch 7, Training Loss: 0.833092428445816\n",
      "Epoch 8, Training Loss: 0.8246931712767657\n",
      "Epoch 9, Training Loss: 0.8192085855848649\n",
      "Epoch 10, Training Loss: 0.8161264497392318\n",
      "Epoch 11, Training Loss: 0.81415131891475\n",
      "Epoch 12, Training Loss: 0.8127566876130946\n",
      "Epoch 13, Training Loss: 0.8120361984477323\n",
      "Epoch 14, Training Loss: 0.8110963252712698\n",
      "Epoch 15, Training Loss: 0.81066433541915\n",
      "Epoch 16, Training Loss: 0.8100368489237393\n",
      "Epoch 17, Training Loss: 0.8095854548846975\n",
      "Epoch 18, Training Loss: 0.809410818604862\n",
      "Epoch 19, Training Loss: 0.8087726432435652\n",
      "Epoch 20, Training Loss: 0.8085526427801918\n",
      "Epoch 21, Training Loss: 0.8083128055404214\n",
      "Epoch 22, Training Loss: 0.8080681634650511\n",
      "Epoch 23, Training Loss: 0.80788571582121\n",
      "Epoch 24, Training Loss: 0.8077160962890176\n",
      "Epoch 25, Training Loss: 0.8073182579349069\n",
      "Epoch 26, Training Loss: 0.807303576259052\n",
      "Epoch 27, Training Loss: 0.8071057037044974\n",
      "Epoch 28, Training Loss: 0.8070697740947499\n",
      "Epoch 29, Training Loss: 0.8068793312942281\n",
      "Epoch 30, Training Loss: 0.80662676579812\n",
      "Epoch 31, Training Loss: 0.806299405448577\n",
      "Epoch 32, Training Loss: 0.8062131132799036\n",
      "Epoch 33, Training Loss: 0.8061101599300609\n",
      "Epoch 34, Training Loss: 0.8059594364727245\n",
      "Epoch 35, Training Loss: 0.8057746275733499\n",
      "Epoch 36, Training Loss: 0.8056570435972775\n",
      "Epoch 37, Training Loss: 0.8054996898595025\n",
      "Epoch 38, Training Loss: 0.8055019828852485\n",
      "Epoch 39, Training Loss: 0.8050971768182866\n",
      "Epoch 40, Training Loss: 0.8051038426511428\n",
      "Epoch 41, Training Loss: 0.8046916848771712\n",
      "Epoch 42, Training Loss: 0.8048604645448573\n",
      "Epoch 43, Training Loss: 0.8047135546628167\n",
      "Epoch 44, Training Loss: 0.8047643594882068\n",
      "Epoch 45, Training Loss: 0.8043698543660781\n",
      "Epoch 46, Training Loss: 0.804441726067487\n",
      "Epoch 47, Training Loss: 0.8040173179261825\n",
      "Epoch 48, Training Loss: 0.803954916491228\n",
      "Epoch 49, Training Loss: 0.803824894989238\n",
      "Epoch 50, Training Loss: 0.80368649076013\n",
      "Epoch 51, Training Loss: 0.8037338815717137\n",
      "Epoch 52, Training Loss: 0.8035188708585851\n",
      "Epoch 53, Training Loss: 0.8036783114601584\n",
      "Epoch 54, Training Loss: 0.8032885055682238\n",
      "Epoch 55, Training Loss: 0.8030754719762241\n",
      "Epoch 56, Training Loss: 0.8031000635203194\n",
      "Epoch 57, Training Loss: 0.8029349000313702\n",
      "Epoch 58, Training Loss: 0.8028471091214349\n",
      "Epoch 59, Training Loss: 0.8026895393343533\n",
      "Epoch 60, Training Loss: 0.8026011030112996\n",
      "Epoch 61, Training Loss: 0.8023949471641989\n",
      "Epoch 62, Training Loss: 0.8023344675933614\n",
      "Epoch 63, Training Loss: 0.8021166206808651\n",
      "Epoch 64, Training Loss: 0.8021320663480198\n",
      "Epoch 65, Training Loss: 0.8020806396007538\n",
      "Epoch 66, Training Loss: 0.8020394243913538\n",
      "Epoch 67, Training Loss: 0.8018899325763478\n",
      "Epoch 68, Training Loss: 0.801842487489476\n",
      "Epoch 69, Training Loss: 0.8016801015068503\n",
      "Epoch 70, Training Loss: 0.8016323645675884\n",
      "Epoch 71, Training Loss: 0.8014587148498087\n",
      "Epoch 72, Training Loss: 0.8011649681540096\n",
      "Epoch 73, Training Loss: 0.8012806679220761\n",
      "Epoch 74, Training Loss: 0.8011834141787361\n",
      "Epoch 75, Training Loss: 0.8009834680136513\n",
      "Epoch 76, Training Loss: 0.8007489637066336\n",
      "Epoch 77, Training Loss: 0.8009529690882738\n",
      "Epoch 78, Training Loss: 0.8008029115200043\n",
      "Epoch 79, Training Loss: 0.800600694137461\n",
      "Epoch 80, Training Loss: 0.8007597531991847\n",
      "Epoch 81, Training Loss: 0.8004309358316309\n",
      "Epoch 82, Training Loss: 0.8004932150420021\n",
      "Epoch 83, Training Loss: 0.8003230592082529\n",
      "Epoch 84, Training Loss: 0.8003077592569239\n",
      "Epoch 85, Training Loss: 0.8001638573057511\n",
      "Epoch 86, Training Loss: 0.8004010732033674\n",
      "Epoch 87, Training Loss: 0.8000914995109334\n",
      "Epoch 88, Training Loss: 0.7999899139825035\n",
      "Epoch 89, Training Loss: 0.7999643399434931\n",
      "Epoch 90, Training Loss: 0.7998621399963604\n",
      "Epoch 91, Training Loss: 0.7999120856032652\n",
      "Epoch 92, Training Loss: 0.7997234207742354\n",
      "Epoch 93, Training Loss: 0.7996953618526459\n",
      "Epoch 94, Training Loss: 0.799593409650466\n",
      "Epoch 95, Training Loss: 0.7998221022241255\n",
      "Epoch 96, Training Loss: 0.799407821893692\n",
      "Epoch 97, Training Loss: 0.7995086517053492\n",
      "Epoch 98, Training Loss: 0.7993979827796711\n",
      "Epoch 99, Training Loss: 0.7992370665774626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:24:43,221] Trial 229 finished with value: 0.6337333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.799235504655277\n",
      "Epoch 1, Training Loss: 0.9684111836377313\n",
      "Epoch 2, Training Loss: 0.8840616464614868\n",
      "Epoch 3, Training Loss: 0.876171881872065\n",
      "Epoch 4, Training Loss: 0.8709685073880589\n",
      "Epoch 5, Training Loss: 0.8697530686154085\n",
      "Epoch 6, Training Loss: 0.8665992614802193\n",
      "Epoch 7, Training Loss: 0.8637904406996334\n",
      "Epoch 8, Training Loss: 0.8642797913270838\n",
      "Epoch 9, Training Loss: 0.8633951873639051\n",
      "Epoch 10, Training Loss: 0.8639682361658881\n",
      "Epoch 11, Training Loss: 0.8627460719557369\n",
      "Epoch 12, Training Loss: 0.8587263760145972\n",
      "Epoch 13, Training Loss: 0.8572213961096371\n",
      "Epoch 14, Training Loss: 0.8585364258289337\n",
      "Epoch 15, Training Loss: 0.8584761471608106\n",
      "Epoch 16, Training Loss: 0.859721773371977\n",
      "Epoch 17, Training Loss: 0.8591136989873999\n",
      "Epoch 18, Training Loss: 0.856829069922952\n",
      "Epoch 19, Training Loss: 0.8578862400615916\n",
      "Epoch 20, Training Loss: 0.856590942214517\n",
      "Epoch 21, Training Loss: 0.8576095130163081\n",
      "Epoch 22, Training Loss: 0.8574987936019898\n",
      "Epoch 23, Training Loss: 0.8586472770045785\n",
      "Epoch 24, Training Loss: 0.857980497304131\n",
      "Epoch 25, Training Loss: 0.8553111652065726\n",
      "Epoch 26, Training Loss: 0.858146822312299\n",
      "Epoch 27, Training Loss: 0.8553293625747457\n",
      "Epoch 28, Training Loss: 0.8557517491368687\n",
      "Epoch 29, Training Loss: 0.8557275413765627\n",
      "Epoch 30, Training Loss: 0.8588517684796277\n",
      "Epoch 31, Training Loss: 0.8546394688241622\n",
      "Epoch 32, Training Loss: 0.8546042830803815\n",
      "Epoch 33, Training Loss: 0.853829590923646\n",
      "Epoch 34, Training Loss: 0.855002587332445\n",
      "Epoch 35, Training Loss: 0.8535667885752285\n",
      "Epoch 36, Training Loss: 0.8520688199295717\n",
      "Epoch 37, Training Loss: 0.8529933359342463\n",
      "Epoch 38, Training Loss: 0.8526951674854054\n",
      "Epoch 39, Training Loss: 0.8537016310411341\n",
      "Epoch 40, Training Loss: 0.8519843386201298\n",
      "Epoch 41, Training Loss: 0.8541526170337901\n",
      "Epoch 42, Training Loss: 0.851884883852566\n",
      "Epoch 43, Training Loss: 0.8524454803326551\n",
      "Epoch 44, Training Loss: 0.8523290500921361\n",
      "Epoch 45, Training Loss: 0.850993307688657\n",
      "Epoch 46, Training Loss: 0.8526429440694697\n",
      "Epoch 47, Training Loss: 0.8546221780776978\n",
      "Epoch 48, Training Loss: 0.8522496970962076\n",
      "Epoch 49, Training Loss: 0.8509245811490451\n",
      "Epoch 50, Training Loss: 0.8526384085066179\n",
      "Epoch 51, Training Loss: 0.8526244439798243\n",
      "Epoch 52, Training Loss: 0.8532640233460594\n",
      "Epoch 53, Training Loss: 0.8518292185138253\n",
      "Epoch 54, Training Loss: 0.8528701474386103\n",
      "Epoch 55, Training Loss: 0.8536139608130735\n",
      "Epoch 56, Training Loss: 0.8505576790781582\n",
      "Epoch 57, Training Loss: 0.8500657420298633\n",
      "Epoch 58, Training Loss: 0.8534826296918533\n",
      "Epoch 59, Training Loss: 0.8513272833824158\n",
      "Epoch 60, Training Loss: 0.8517938890877892\n",
      "Epoch 61, Training Loss: 0.8542545041617225\n",
      "Epoch 62, Training Loss: 0.8519925529816571\n",
      "Epoch 63, Training Loss: 0.851633337315391\n",
      "Epoch 64, Training Loss: 0.8535219449155471\n",
      "Epoch 65, Training Loss: 0.8519308015879463\n",
      "Epoch 66, Training Loss: 0.8526600247972151\n",
      "Epoch 67, Training Loss: 0.8526066072548137\n",
      "Epoch 68, Training Loss: 0.8523822601402508\n",
      "Epoch 69, Training Loss: 0.8505480778918547\n",
      "Epoch 70, Training Loss: 0.8533839440345764\n",
      "Epoch 71, Training Loss: 0.8516334591893588\n",
      "Epoch 72, Training Loss: 0.8524780378622168\n",
      "Epoch 73, Training Loss: 0.8563545070676243\n",
      "Epoch 74, Training Loss: 0.8512211306656108\n",
      "Epoch 75, Training Loss: 0.8514019387609818\n",
      "Epoch 76, Training Loss: 0.8501396552254172\n",
      "Epoch 77, Training Loss: 0.8526543242791119\n",
      "Epoch 78, Training Loss: 0.8526423879931955\n",
      "Epoch 79, Training Loss: 0.8522035608572118\n",
      "Epoch 80, Training Loss: 0.852924712265239\n",
      "Epoch 81, Training Loss: 0.8514528981377096\n",
      "Epoch 82, Training Loss: 0.8517179615357343\n",
      "Epoch 83, Training Loss: 0.8510774125071133\n",
      "Epoch 84, Training Loss: 0.8521052873134614\n",
      "Epoch 85, Training Loss: 0.8515594518184662\n",
      "Epoch 86, Training Loss: 0.8504240688155679\n",
      "Epoch 87, Training Loss: 0.850651658633176\n",
      "Epoch 88, Training Loss: 0.849792439586976\n",
      "Epoch 89, Training Loss: 0.8503437687368954\n",
      "Epoch 90, Training Loss: 0.8518263408716987\n",
      "Epoch 91, Training Loss: 0.8505929615217097\n",
      "Epoch 92, Training Loss: 0.8482335332561942\n",
      "Epoch 93, Training Loss: 0.8505463971109951\n",
      "Epoch 94, Training Loss: 0.8476845450962291\n",
      "Epoch 95, Training Loss: 0.8482194807950189\n",
      "Epoch 96, Training Loss: 0.8494468949822819\n",
      "Epoch 97, Training Loss: 0.8523611645137562\n",
      "Epoch 98, Training Loss: 0.8503446002567515\n",
      "Epoch 99, Training Loss: 0.8513251690303578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:26:54,874] Trial 230 finished with value: 0.6178666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8505749917030334\n",
      "Epoch 1, Training Loss: 0.9738372237542097\n",
      "Epoch 2, Training Loss: 0.9299674407874836\n",
      "Epoch 3, Training Loss: 0.8900573602844687\n",
      "Epoch 4, Training Loss: 0.8476005573132459\n",
      "Epoch 5, Training Loss: 0.8236099905126235\n",
      "Epoch 6, Training Loss: 0.8157965601191801\n",
      "Epoch 7, Training Loss: 0.8127573376543382\n",
      "Epoch 8, Training Loss: 0.8116978979110718\n",
      "Epoch 9, Training Loss: 0.8103860389485079\n",
      "Epoch 10, Training Loss: 0.8098496662869173\n",
      "Epoch 11, Training Loss: 0.8086158216700834\n",
      "Epoch 12, Training Loss: 0.8076932466731352\n",
      "Epoch 13, Training Loss: 0.8071301654507133\n",
      "Epoch 14, Training Loss: 0.8065481742690591\n",
      "Epoch 15, Training Loss: 0.8061869921403773\n",
      "Epoch 16, Training Loss: 0.8052685441690333\n",
      "Epoch 17, Training Loss: 0.8050444214484271\n",
      "Epoch 18, Training Loss: 0.8043915104164797\n",
      "Epoch 19, Training Loss: 0.804171070842182\n",
      "Epoch 20, Training Loss: 0.8041042719869053\n",
      "Epoch 21, Training Loss: 0.8034159475214341\n",
      "Epoch 22, Training Loss: 0.8032422985048855\n",
      "Epoch 23, Training Loss: 0.8029191526244669\n",
      "Epoch 24, Training Loss: 0.8031446995454676\n",
      "Epoch 25, Training Loss: 0.802446806010078\n",
      "Epoch 26, Training Loss: 0.8023656049896689\n",
      "Epoch 27, Training Loss: 0.8019854342937469\n",
      "Epoch 28, Training Loss: 0.8021433998556698\n",
      "Epoch 29, Training Loss: 0.8016459858417511\n",
      "Epoch 30, Training Loss: 0.8019032063203699\n",
      "Epoch 31, Training Loss: 0.8011606355975656\n",
      "Epoch 32, Training Loss: 0.801179772194694\n",
      "Epoch 33, Training Loss: 0.8013232208700741\n",
      "Epoch 34, Training Loss: 0.800904507076039\n",
      "Epoch 35, Training Loss: 0.8009041979733635\n",
      "Epoch 36, Training Loss: 0.8006362245363348\n",
      "Epoch 37, Training Loss: 0.8005532019278583\n",
      "Epoch 38, Training Loss: 0.8003813892953536\n",
      "Epoch 39, Training Loss: 0.8001168202652651\n",
      "Epoch 40, Training Loss: 0.7999849688305575\n",
      "Epoch 41, Training Loss: 0.79970510931576\n",
      "Epoch 42, Training Loss: 0.8000029919427983\n",
      "Epoch 43, Training Loss: 0.7997722557011773\n",
      "Epoch 44, Training Loss: 0.7994890884792103\n",
      "Epoch 45, Training Loss: 0.7996328216440537\n",
      "Epoch 46, Training Loss: 0.7991366054030026\n",
      "Epoch 47, Training Loss: 0.7989995098815245\n",
      "Epoch 48, Training Loss: 0.798622644999448\n",
      "Epoch 49, Training Loss: 0.7985589033715865\n",
      "Epoch 50, Training Loss: 0.7985577461298774\n",
      "Epoch 51, Training Loss: 0.7986562113200917\n",
      "Epoch 52, Training Loss: 0.7982733570828158\n",
      "Epoch 53, Training Loss: 0.798300549843732\n",
      "Epoch 54, Training Loss: 0.797904644573436\n",
      "Epoch 55, Training Loss: 0.7981239487844355\n",
      "Epoch 56, Training Loss: 0.7980767295641058\n",
      "Epoch 57, Training Loss: 0.7981293049279381\n",
      "Epoch 58, Training Loss: 0.7976886620942284\n",
      "Epoch 59, Training Loss: 0.7978108728633208\n",
      "Epoch 60, Training Loss: 0.79764729254386\n",
      "Epoch 61, Training Loss: 0.7976346871432136\n",
      "Epoch 62, Training Loss: 0.7973451940452352\n",
      "Epoch 63, Training Loss: 0.7972863381049212\n",
      "Epoch 64, Training Loss: 0.7972329319925869\n",
      "Epoch 65, Training Loss: 0.796730047604617\n",
      "Epoch 66, Training Loss: 0.7970375058230231\n",
      "Epoch 67, Training Loss: 0.7965412759079653\n",
      "Epoch 68, Training Loss: 0.7968689947268542\n",
      "Epoch 69, Training Loss: 0.7965408144978916\n",
      "Epoch 70, Training Loss: 0.7967261151706471\n",
      "Epoch 71, Training Loss: 0.7962158587399651\n",
      "Epoch 72, Training Loss: 0.7961800238665413\n",
      "Epoch 73, Training Loss: 0.79607306915171\n",
      "Epoch 74, Training Loss: 0.7959014675897711\n",
      "Epoch 75, Training Loss: 0.7954951241437127\n",
      "Epoch 76, Training Loss: 0.7957547754399916\n",
      "Epoch 77, Training Loss: 0.7955087582504048\n",
      "Epoch 78, Training Loss: 0.795508317456526\n",
      "Epoch 79, Training Loss: 0.7951976631669437\n",
      "Epoch 80, Training Loss: 0.7951195303131552\n",
      "Epoch 81, Training Loss: 0.7948924272901872\n",
      "Epoch 82, Training Loss: 0.7949009363791522\n",
      "Epoch 83, Training Loss: 0.7944092100508073\n",
      "Epoch 84, Training Loss: 0.7944273618389578\n",
      "Epoch 85, Training Loss: 0.7946144112418679\n",
      "Epoch 86, Training Loss: 0.7945298164732316\n",
      "Epoch 87, Training Loss: 0.7939849694336162\n",
      "Epoch 88, Training Loss: 0.7939132933756884\n",
      "Epoch 89, Training Loss: 0.7934220066491295\n",
      "Epoch 90, Training Loss: 0.7934312569393831\n",
      "Epoch 91, Training Loss: 0.7932134441768421\n",
      "Epoch 92, Training Loss: 0.793081231397741\n",
      "Epoch 93, Training Loss: 0.7930596470131593\n",
      "Epoch 94, Training Loss: 0.7926804392478045\n",
      "Epoch 95, Training Loss: 0.7924536716938019\n",
      "Epoch 96, Training Loss: 0.7923813789031084\n",
      "Epoch 97, Training Loss: 0.7923382072588977\n",
      "Epoch 98, Training Loss: 0.7918794389332042\n",
      "Epoch 99, Training Loss: 0.7919319553936229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:29:05,003] Trial 231 finished with value: 0.6338666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7914328506413628\n",
      "Epoch 1, Training Loss: 0.8533103186943952\n",
      "Epoch 2, Training Loss: 0.8224106412775376\n",
      "Epoch 3, Training Loss: 0.8163985688546125\n",
      "Epoch 4, Training Loss: 0.8135175200069652\n",
      "Epoch 5, Training Loss: 0.8123694096593296\n",
      "Epoch 6, Training Loss: 0.8088164412975312\n",
      "Epoch 7, Training Loss: 0.8080437790646272\n",
      "Epoch 8, Training Loss: 0.8094730598786298\n",
      "Epoch 9, Training Loss: 0.8081657688056721\n",
      "Epoch 10, Training Loss: 0.8071003157251021\n",
      "Epoch 11, Training Loss: 0.8063985375797047\n",
      "Epoch 12, Training Loss: 0.8063022618434008\n",
      "Epoch 13, Training Loss: 0.8060296204510857\n",
      "Epoch 14, Training Loss: 0.8066442932100857\n",
      "Epoch 15, Training Loss: 0.8065309039284201\n",
      "Epoch 16, Training Loss: 0.8081268182221581\n",
      "Epoch 17, Training Loss: 0.8051677228422726\n",
      "Epoch 18, Training Loss: 0.8047733205907485\n",
      "Epoch 19, Training Loss: 0.805507840128506\n",
      "Epoch 20, Training Loss: 0.8049104649179122\n",
      "Epoch 21, Training Loss: 0.8059147684013143\n",
      "Epoch 22, Training Loss: 0.8055179375760696\n",
      "Epoch 23, Training Loss: 0.8048281716599184\n",
      "Epoch 24, Training Loss: 0.8038297481396619\n",
      "Epoch 25, Training Loss: 0.8042022838312037\n",
      "Epoch 26, Training Loss: 0.804420230178272\n",
      "Epoch 27, Training Loss: 0.8036130956341239\n",
      "Epoch 28, Training Loss: 0.8045035267577452\n",
      "Epoch 29, Training Loss: 0.8032057520922492\n",
      "Epoch 30, Training Loss: 0.8028951790753532\n",
      "Epoch 31, Training Loss: 0.8037326485970441\n",
      "Epoch 32, Training Loss: 0.8030070725609274\n",
      "Epoch 33, Training Loss: 0.8029869245781618\n",
      "Epoch 34, Training Loss: 0.8036446079085855\n",
      "Epoch 35, Training Loss: 0.8025848204949323\n",
      "Epoch 36, Training Loss: 0.8022393005735734\n",
      "Epoch 37, Training Loss: 0.8016419255032259\n",
      "Epoch 38, Training Loss: 0.8026761662258821\n",
      "Epoch 39, Training Loss: 0.8011102128730101\n",
      "Epoch 40, Training Loss: 0.8017991453759811\n",
      "Epoch 41, Training Loss: 0.8010698674706852\n",
      "Epoch 42, Training Loss: 0.8018215834393221\n",
      "Epoch 43, Training Loss: 0.8009806655435001\n",
      "Epoch 44, Training Loss: 0.8007219542475308\n",
      "Epoch 45, Training Loss: 0.8005994920169606\n",
      "Epoch 46, Training Loss: 0.8000044177560245\n",
      "Epoch 47, Training Loss: 0.8004459336925955\n",
      "Epoch 48, Training Loss: 0.8001903904185576\n",
      "Epoch 49, Training Loss: 0.7997419201626497\n",
      "Epoch 50, Training Loss: 0.7998709593099707\n",
      "Epoch 51, Training Loss: 0.8003001295819002\n",
      "Epoch 52, Training Loss: 0.8000368909976062\n",
      "Epoch 53, Training Loss: 0.8000425049136667\n",
      "Epoch 54, Training Loss: 0.7993067681088167\n",
      "Epoch 55, Training Loss: 0.7989796911267674\n",
      "Epoch 56, Training Loss: 0.7984204593125511\n",
      "Epoch 57, Training Loss: 0.7980428377319785\n",
      "Epoch 58, Training Loss: 0.7978713884774377\n",
      "Epoch 59, Training Loss: 0.7984878657144658\n",
      "Epoch 60, Training Loss: 0.7978600067250868\n",
      "Epoch 61, Training Loss: 0.7984627603082096\n",
      "Epoch 62, Training Loss: 0.7974870396361632\n",
      "Epoch 63, Training Loss: 0.7980079674720764\n",
      "Epoch 64, Training Loss: 0.7969261171537287\n",
      "Epoch 65, Training Loss: 0.7968668515541975\n",
      "Epoch 66, Training Loss: 0.7974679702870986\n",
      "Epoch 67, Training Loss: 0.7971029987755944\n",
      "Epoch 68, Training Loss: 0.7971680996698491\n",
      "Epoch 69, Training Loss: 0.7969911069028518\n",
      "Epoch 70, Training Loss: 0.7965482762981864\n",
      "Epoch 71, Training Loss: 0.7958519561851726\n",
      "Epoch 72, Training Loss: 0.7965342474684995\n",
      "Epoch 73, Training Loss: 0.7964220673196456\n",
      "Epoch 74, Training Loss: 0.797254610342138\n",
      "Epoch 75, Training Loss: 0.7962876955901875\n",
      "Epoch 76, Training Loss: 0.797268396615982\n",
      "Epoch 77, Training Loss: 0.7964797822166891\n",
      "Epoch 78, Training Loss: 0.7953744883397046\n",
      "Epoch 79, Training Loss: 0.7967258610444911\n",
      "Epoch 80, Training Loss: 0.7949313490531024\n",
      "Epoch 81, Training Loss: 0.7950670453380136\n",
      "Epoch 82, Training Loss: 0.7953236039947061\n",
      "Epoch 83, Training Loss: 0.7950431707326103\n",
      "Epoch 84, Training Loss: 0.794609271217795\n",
      "Epoch 85, Training Loss: 0.7960508002253139\n",
      "Epoch 86, Training Loss: 0.795226182937622\n",
      "Epoch 87, Training Loss: 0.7952108655957615\n",
      "Epoch 88, Training Loss: 0.7952663149553186\n",
      "Epoch 89, Training Loss: 0.7945527816520018\n",
      "Epoch 90, Training Loss: 0.7950822929073783\n",
      "Epoch 91, Training Loss: 0.7958766180627487\n",
      "Epoch 92, Training Loss: 0.7949749238350812\n",
      "Epoch 93, Training Loss: 0.7940446710586548\n",
      "Epoch 94, Training Loss: 0.7946407895929674\n",
      "Epoch 95, Training Loss: 0.7949318961536183\n",
      "Epoch 96, Training Loss: 0.793982434202643\n",
      "Epoch 97, Training Loss: 0.793583322132335\n",
      "Epoch 98, Training Loss: 0.7940545230753282\n",
      "Epoch 99, Training Loss: 0.7944069666021011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:31:03,165] Trial 232 finished with value: 0.6328 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7939915831650005\n",
      "Epoch 1, Training Loss: 0.9696360802650452\n",
      "Epoch 2, Training Loss: 0.9374327563538271\n",
      "Epoch 3, Training Loss: 0.906366579322254\n",
      "Epoch 4, Training Loss: 0.8673231902543236\n",
      "Epoch 5, Training Loss: 0.8393699415291057\n",
      "Epoch 6, Training Loss: 0.8265305010711446\n",
      "Epoch 7, Training Loss: 0.8195589135674869\n",
      "Epoch 8, Training Loss: 0.8154428234521081\n",
      "Epoch 9, Training Loss: 0.8130549223983989\n",
      "Epoch 10, Training Loss: 0.8113378293374005\n",
      "Epoch 11, Training Loss: 0.8101707363128662\n",
      "Epoch 12, Training Loss: 0.8095217603094438\n",
      "Epoch 13, Training Loss: 0.8085248632290783\n",
      "Epoch 14, Training Loss: 0.807474570905461\n",
      "Epoch 15, Training Loss: 0.807126278035781\n",
      "Epoch 16, Training Loss: 0.8065502685659072\n",
      "Epoch 17, Training Loss: 0.8058032825413872\n",
      "Epoch 18, Training Loss: 0.8050665221494787\n",
      "Epoch 19, Training Loss: 0.8047753697984359\n",
      "Epoch 20, Training Loss: 0.8043421067911036\n",
      "Epoch 21, Training Loss: 0.8035785807581509\n",
      "Epoch 22, Training Loss: 0.8029510747685152\n",
      "Epoch 23, Training Loss: 0.8025198011538561\n",
      "Epoch 24, Training Loss: 0.802374352497213\n",
      "Epoch 25, Training Loss: 0.8020917194731095\n",
      "Epoch 26, Training Loss: 0.8016293677161722\n",
      "Epoch 27, Training Loss: 0.8012600125284756\n",
      "Epoch 28, Training Loss: 0.8013656385505901\n",
      "Epoch 29, Training Loss: 0.8010187634299784\n",
      "Epoch 30, Training Loss: 0.800714615653543\n",
      "Epoch 31, Training Loss: 0.8004463566752041\n",
      "Epoch 32, Training Loss: 0.8003033514583812\n",
      "Epoch 33, Training Loss: 0.8001056537207435\n",
      "Epoch 34, Training Loss: 0.7996760165691376\n",
      "Epoch 35, Training Loss: 0.7997332349945517\n",
      "Epoch 36, Training Loss: 0.7997916549794815\n",
      "Epoch 37, Training Loss: 0.7994015501527225\n",
      "Epoch 38, Training Loss: 0.799380233147565\n",
      "Epoch 39, Training Loss: 0.7991449427604675\n",
      "Epoch 40, Training Loss: 0.7989577021318324\n",
      "Epoch 41, Training Loss: 0.7988511486614451\n",
      "Epoch 42, Training Loss: 0.7989054691090304\n",
      "Epoch 43, Training Loss: 0.7986766102033503\n",
      "Epoch 44, Training Loss: 0.7985366820587831\n",
      "Epoch 45, Training Loss: 0.7982462019780103\n",
      "Epoch 46, Training Loss: 0.7978849680984721\n",
      "Epoch 47, Training Loss: 0.7983655127357034\n",
      "Epoch 48, Training Loss: 0.7976994229765499\n",
      "Epoch 49, Training Loss: 0.7980722341116737\n",
      "Epoch 50, Training Loss: 0.7978785822672002\n",
      "Epoch 51, Training Loss: 0.7978324125093572\n",
      "Epoch 52, Training Loss: 0.7974438782299266\n",
      "Epoch 53, Training Loss: 0.7972533616598915\n",
      "Epoch 54, Training Loss: 0.7972495185627657\n",
      "Epoch 55, Training Loss: 0.7971940705355476\n",
      "Epoch 56, Training Loss: 0.796922933844959\n",
      "Epoch 57, Training Loss: 0.7967799495248233\n",
      "Epoch 58, Training Loss: 0.7966519533886629\n",
      "Epoch 59, Training Loss: 0.7965319928702186\n",
      "Epoch 60, Training Loss: 0.7964602980894201\n",
      "Epoch 61, Training Loss: 0.7960515479480519\n",
      "Epoch 62, Training Loss: 0.7963380703505347\n",
      "Epoch 63, Training Loss: 0.7962040419438307\n",
      "Epoch 64, Training Loss: 0.7958021882001092\n",
      "Epoch 65, Training Loss: 0.7957792805924135\n",
      "Epoch 66, Training Loss: 0.7954295965503244\n",
      "Epoch 67, Training Loss: 0.7951298412154703\n",
      "Epoch 68, Training Loss: 0.795117980522268\n",
      "Epoch 69, Training Loss: 0.7948892089899848\n",
      "Epoch 70, Training Loss: 0.7947609060652115\n",
      "Epoch 71, Training Loss: 0.7947147175844977\n",
      "Epoch 72, Training Loss: 0.7943525846565471\n",
      "Epoch 73, Training Loss: 0.7943792747750001\n",
      "Epoch 74, Training Loss: 0.7940172863006592\n",
      "Epoch 75, Training Loss: 0.7937628393313464\n",
      "Epoch 76, Training Loss: 0.7936725779140696\n",
      "Epoch 77, Training Loss: 0.7935019696460051\n",
      "Epoch 78, Training Loss: 0.7928831225283006\n",
      "Epoch 79, Training Loss: 0.7928955728166244\n",
      "Epoch 80, Training Loss: 0.7925796435861027\n",
      "Epoch 81, Training Loss: 0.7923969594170065\n",
      "Epoch 82, Training Loss: 0.7921446650168475\n",
      "Epoch 83, Training Loss: 0.791750816527535\n",
      "Epoch 84, Training Loss: 0.7913505893595079\n",
      "Epoch 85, Training Loss: 0.7913508124211255\n",
      "Epoch 86, Training Loss: 0.7910461676120758\n",
      "Epoch 87, Training Loss: 0.7911149355243234\n",
      "Epoch 88, Training Loss: 0.7901213596147649\n",
      "Epoch 89, Training Loss: 0.7901703007782207\n",
      "Epoch 90, Training Loss: 0.7902085776890025\n",
      "Epoch 91, Training Loss: 0.7898575375360601\n",
      "Epoch 92, Training Loss: 0.7896926478778614\n",
      "Epoch 93, Training Loss: 0.7892864318454966\n",
      "Epoch 94, Training Loss: 0.7891282083707697\n",
      "Epoch 95, Training Loss: 0.7886904413559858\n",
      "Epoch 96, Training Loss: 0.7886560858698453\n",
      "Epoch 97, Training Loss: 0.7884414235984578\n",
      "Epoch 98, Training Loss: 0.7882928806192735\n",
      "Epoch 99, Training Loss: 0.7879636303116293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:32:53,465] Trial 233 finished with value: 0.6359333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7875810988510357\n",
      "Epoch 1, Training Loss: 0.9047721275161295\n",
      "Epoch 2, Training Loss: 0.8282448908160714\n",
      "Epoch 3, Training Loss: 0.8179077114077176\n",
      "Epoch 4, Training Loss: 0.8156196910493514\n",
      "Epoch 5, Training Loss: 0.812807043019463\n",
      "Epoch 6, Training Loss: 0.810862710686291\n",
      "Epoch 7, Training Loss: 0.8098284485760857\n",
      "Epoch 8, Training Loss: 0.8074491108165068\n",
      "Epoch 9, Training Loss: 0.8073932540416717\n",
      "Epoch 10, Training Loss: 0.8072083910773782\n",
      "Epoch 11, Training Loss: 0.8060466322478126\n",
      "Epoch 12, Training Loss: 0.8054753581916585\n",
      "Epoch 13, Training Loss: 0.8056738868881674\n",
      "Epoch 14, Training Loss: 0.8047620802065906\n",
      "Epoch 15, Training Loss: 0.8049082314266878\n",
      "Epoch 16, Training Loss: 0.8041336382837857\n",
      "Epoch 17, Training Loss: 0.8024584243578069\n",
      "Epoch 18, Training Loss: 0.8019677071711596\n",
      "Epoch 19, Training Loss: 0.8032833548153148\n",
      "Epoch 20, Training Loss: 0.8027729741966023\n",
      "Epoch 21, Training Loss: 0.8033078750442056\n",
      "Epoch 22, Training Loss: 0.8027955153409172\n",
      "Epoch 23, Training Loss: 0.8024478959336\n",
      "Epoch 24, Training Loss: 0.8022484054284937\n",
      "Epoch 25, Training Loss: 0.8016666561715743\n",
      "Epoch 26, Training Loss: 0.8023915540470796\n",
      "Epoch 27, Training Loss: 0.8016395730832043\n",
      "Epoch 28, Training Loss: 0.8016130389886744\n",
      "Epoch 29, Training Loss: 0.8015852138575386\n",
      "Epoch 30, Training Loss: 0.8019083649971905\n",
      "Epoch 31, Training Loss: 0.8017164024885963\n",
      "Epoch 32, Training Loss: 0.8015285302610958\n",
      "Epoch 33, Training Loss: 0.8023489660375258\n",
      "Epoch 34, Training Loss: 0.8013452460485346\n",
      "Epoch 35, Training Loss: 0.8015843413857853\n",
      "Epoch 36, Training Loss: 0.8013923967585844\n",
      "Epoch 37, Training Loss: 0.8013528159786674\n",
      "Epoch 38, Training Loss: 0.8022173718143912\n",
      "Epoch 39, Training Loss: 0.8007938971940209\n",
      "Epoch 40, Training Loss: 0.8012727805446176\n",
      "Epoch 41, Training Loss: 0.8008401761335485\n",
      "Epoch 42, Training Loss: 0.8011557736817528\n",
      "Epoch 43, Training Loss: 0.8006865123440238\n",
      "Epoch 44, Training Loss: 0.800665554299074\n",
      "Epoch 45, Training Loss: 0.8009481798901278\n",
      "Epoch 46, Training Loss: 0.8010166560201084\n",
      "Epoch 47, Training Loss: 0.8005080141039456\n",
      "Epoch 48, Training Loss: 0.8010916348765879\n",
      "Epoch 49, Training Loss: 0.8016862323704887\n",
      "Epoch 50, Training Loss: 0.8003848713285783\n",
      "Epoch 51, Training Loss: 0.8011464128073524\n",
      "Epoch 52, Training Loss: 0.8012866484417634\n",
      "Epoch 53, Training Loss: 0.8005038716512568\n",
      "Epoch 54, Training Loss: 0.8002588900397806\n",
      "Epoch 55, Training Loss: 0.8007478673317853\n",
      "Epoch 56, Training Loss: 0.8003941879552954\n",
      "Epoch 57, Training Loss: 0.8007364992534413\n",
      "Epoch 58, Training Loss: 0.8001965231053969\n",
      "Epoch 59, Training Loss: 0.8006922483444214\n",
      "Epoch 60, Training Loss: 0.8008185900660122\n",
      "Epoch 61, Training Loss: 0.8013980613035314\n",
      "Epoch 62, Training Loss: 0.8007833940842572\n",
      "Epoch 63, Training Loss: 0.8001507407076218\n",
      "Epoch 64, Training Loss: 0.7996352068816914\n",
      "Epoch 65, Training Loss: 0.8000783003779018\n",
      "Epoch 66, Training Loss: 0.8004512821225559\n",
      "Epoch 67, Training Loss: 0.8000727544812595\n",
      "Epoch 68, Training Loss: 0.8004556623627158\n",
      "Epoch 69, Training Loss: 0.7999472446301404\n",
      "Epoch 70, Training Loss: 0.7994943334074581\n",
      "Epoch 71, Training Loss: 0.799950495747959\n",
      "Epoch 72, Training Loss: 0.8003386648262248\n",
      "Epoch 73, Training Loss: 0.8008445020282969\n",
      "Epoch 74, Training Loss: 0.7997274195446687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:34:31,733] Trial 234 finished with value: 0.6356666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8000798969409045\n",
      "Epoch 1, Training Loss: 0.8915974393823093\n",
      "Epoch 2, Training Loss: 0.8398814665643792\n",
      "Epoch 3, Training Loss: 0.8315298352026401\n",
      "Epoch 4, Training Loss: 0.8268318603809615\n",
      "Epoch 5, Training Loss: 0.8244155568287785\n",
      "Epoch 6, Training Loss: 0.8218549664755513\n",
      "Epoch 7, Training Loss: 0.8207283129369406\n",
      "Epoch 8, Training Loss: 0.8200563342051399\n",
      "Epoch 9, Training Loss: 0.8197248241955176\n",
      "Epoch 10, Training Loss: 0.8182824438675902\n",
      "Epoch 11, Training Loss: 0.8159808953901879\n",
      "Epoch 12, Training Loss: 0.817769305956991\n",
      "Epoch 13, Training Loss: 0.816270439427598\n",
      "Epoch 14, Training Loss: 0.8174348620543802\n",
      "Epoch 15, Training Loss: 0.8142605808892645\n",
      "Epoch 16, Training Loss: 0.8166171114247545\n",
      "Epoch 17, Training Loss: 0.8135278483082478\n",
      "Epoch 18, Training Loss: 0.8173694020823429\n",
      "Epoch 19, Training Loss: 0.8167020980576823\n",
      "Epoch 20, Training Loss: 0.8141272750115932\n",
      "Epoch 21, Training Loss: 0.8139381124560995\n",
      "Epoch 22, Training Loss: 0.8144788050113764\n",
      "Epoch 23, Training Loss: 0.8160356786018027\n",
      "Epoch 24, Training Loss: 0.8135353217447611\n",
      "Epoch 25, Training Loss: 0.8175193452297297\n",
      "Epoch 26, Training Loss: 0.8135721354556263\n",
      "Epoch 27, Training Loss: 0.8153409725741336\n",
      "Epoch 28, Training Loss: 0.8124140292182005\n",
      "Epoch 29, Training Loss: 0.8147803682133667\n",
      "Epoch 30, Training Loss: 0.8150958565841043\n",
      "Epoch 31, Training Loss: 0.8127809330036766\n",
      "Epoch 32, Training Loss: 0.8140336733115346\n",
      "Epoch 33, Training Loss: 0.8126881043713792\n",
      "Epoch 34, Training Loss: 0.8116281304144322\n",
      "Epoch 35, Training Loss: 0.8124003611112895\n",
      "Epoch 36, Training Loss: 0.8125863463358771\n",
      "Epoch 37, Training Loss: 0.8100516414283809\n",
      "Epoch 38, Training Loss: 0.8112765147273702\n",
      "Epoch 39, Training Loss: 0.8113213541812466\n",
      "Epoch 40, Training Loss: 0.8103048741817475\n",
      "Epoch 41, Training Loss: 0.8096170760635146\n",
      "Epoch 42, Training Loss: 0.8098533876408311\n",
      "Epoch 43, Training Loss: 0.8099177391009224\n",
      "Epoch 44, Training Loss: 0.8089361290286358\n",
      "Epoch 45, Training Loss: 0.8078317681649574\n",
      "Epoch 46, Training Loss: 0.8092215036987362\n",
      "Epoch 47, Training Loss: 0.8082380463306169\n",
      "Epoch 48, Training Loss: 0.8082056858485802\n",
      "Epoch 49, Training Loss: 0.8069002124599944\n",
      "Epoch 50, Training Loss: 0.809799219253368\n",
      "Epoch 51, Training Loss: 0.8093792494974639\n",
      "Epoch 52, Training Loss: 0.8093537645680564\n",
      "Epoch 53, Training Loss: 0.8071573249379494\n",
      "Epoch 54, Training Loss: 0.806796275493794\n",
      "Epoch 55, Training Loss: 0.8067040393227025\n",
      "Epoch 56, Training Loss: 0.8066138945127789\n",
      "Epoch 57, Training Loss: 0.8093305568946035\n",
      "Epoch 58, Training Loss: 0.8064917409330382\n",
      "Epoch 59, Training Loss: 0.806597868392342\n",
      "Epoch 60, Training Loss: 0.8090483587487299\n",
      "Epoch 61, Training Loss: 0.8054729490351856\n",
      "Epoch 62, Training Loss: 0.8074267144490005\n",
      "Epoch 63, Training Loss: 0.807463394430347\n",
      "Epoch 64, Training Loss: 0.8059887355431579\n",
      "Epoch 65, Training Loss: 0.8067102161565222\n",
      "Epoch 66, Training Loss: 0.8065744144575936\n",
      "Epoch 67, Training Loss: 0.8069687630897178\n",
      "Epoch 68, Training Loss: 0.8063846851201882\n",
      "Epoch 69, Training Loss: 0.8072097234259871\n",
      "Epoch 70, Training Loss: 0.8070026630745795\n",
      "Epoch 71, Training Loss: 0.8054368575712791\n",
      "Epoch 72, Training Loss: 0.8067563514064129\n",
      "Epoch 73, Training Loss: 0.805032661893314\n",
      "Epoch 74, Training Loss: 0.8060359896573805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:35:42,934] Trial 235 finished with value: 0.6155333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8059060197127493\n",
      "Epoch 1, Training Loss: 0.840258401001201\n",
      "Epoch 2, Training Loss: 0.8125432651183184\n",
      "Epoch 3, Training Loss: 0.8061199617385865\n",
      "Epoch 4, Training Loss: 0.8039701702314265\n",
      "Epoch 5, Training Loss: 0.8025983281696544\n",
      "Epoch 6, Training Loss: 0.8024085103764254\n",
      "Epoch 7, Training Loss: 0.8018366456031799\n",
      "Epoch 8, Training Loss: 0.8006877369039199\n",
      "Epoch 9, Training Loss: 0.8006542754874509\n",
      "Epoch 10, Training Loss: 0.798359913335127\n",
      "Epoch 11, Training Loss: 0.7991104785835041\n",
      "Epoch 12, Training Loss: 0.7973929367345922\n",
      "Epoch 13, Training Loss: 0.7976462947621065\n",
      "Epoch 14, Training Loss: 0.7980446712409749\n",
      "Epoch 15, Training Loss: 0.7971219533331254\n",
      "Epoch 16, Training Loss: 0.796519878541722\n",
      "Epoch 17, Training Loss: 0.7964816279972301\n",
      "Epoch 18, Training Loss: 0.7968470975230721\n",
      "Epoch 19, Training Loss: 0.7950252707565532\n",
      "Epoch 20, Training Loss: 0.7966862854536841\n",
      "Epoch 21, Training Loss: 0.7965709187002743\n",
      "Epoch 22, Training Loss: 0.7972050226435942\n",
      "Epoch 23, Training Loss: 0.7964250876623041\n",
      "Epoch 24, Training Loss: 0.7967662078492782\n",
      "Epoch 25, Training Loss: 0.7955831728963291\n",
      "Epoch 26, Training Loss: 0.7963485188343946\n",
      "Epoch 27, Training Loss: 0.7963629941379323\n",
      "Epoch 28, Training Loss: 0.7953068480772131\n",
      "Epoch 29, Training Loss: 0.795310134747449\n",
      "Epoch 30, Training Loss: 0.7962756262106054\n",
      "Epoch 31, Training Loss: 0.7950052225589752\n",
      "Epoch 32, Training Loss: 0.7956486429186428\n",
      "Epoch 33, Training Loss: 0.7953129032780143\n",
      "Epoch 34, Training Loss: 0.7955032678912668\n",
      "Epoch 35, Training Loss: 0.795215646238888\n",
      "Epoch 36, Training Loss: 0.7946861443098854\n",
      "Epoch 37, Training Loss: 0.794185478476917\n",
      "Epoch 38, Training Loss: 0.7939370749277227\n",
      "Epoch 39, Training Loss: 0.7940047809656928\n",
      "Epoch 40, Training Loss: 0.7957314473039964\n",
      "Epoch 41, Training Loss: 0.7931516274283914\n",
      "Epoch 42, Training Loss: 0.794591458124273\n",
      "Epoch 43, Training Loss: 0.7947317794491263\n",
      "Epoch 44, Training Loss: 0.7945096868627212\n",
      "Epoch 45, Training Loss: 0.7939894916029537\n",
      "Epoch 46, Training Loss: 0.7927019626954023\n",
      "Epoch 47, Training Loss: 0.7943055412348579\n",
      "Epoch 48, Training Loss: 0.794692180928062\n",
      "Epoch 49, Training Loss: 0.7943838089353898\n",
      "Epoch 50, Training Loss: 0.793428793444353\n",
      "Epoch 51, Training Loss: 0.7933855039933149\n",
      "Epoch 52, Training Loss: 0.7939841522889979\n",
      "Epoch 53, Training Loss: 0.7935338835856494\n",
      "Epoch 54, Training Loss: 0.7923584794998169\n",
      "Epoch 55, Training Loss: 0.7932170597244711\n",
      "Epoch 56, Training Loss: 0.7929449218862197\n",
      "Epoch 57, Training Loss: 0.7923722316237057\n",
      "Epoch 58, Training Loss: 0.7942449923823861\n",
      "Epoch 59, Training Loss: 0.7924197308456197\n",
      "Epoch 60, Training Loss: 0.7934274441354415\n",
      "Epoch 61, Training Loss: 0.7931379923399757\n",
      "Epoch 62, Training Loss: 0.7943018653112299\n",
      "Epoch 63, Training Loss: 0.7933707925852608\n",
      "Epoch 64, Training Loss: 0.79347402733915\n",
      "Epoch 65, Training Loss: 0.7933358937852523\n",
      "Epoch 66, Training Loss: 0.794034457767711\n",
      "Epoch 67, Training Loss: 0.7928958336745991\n",
      "Epoch 68, Training Loss: 0.7930371489244349\n",
      "Epoch 69, Training Loss: 0.7936657881035524\n",
      "Epoch 70, Training Loss: 0.7928769034497878\n",
      "Epoch 71, Training Loss: 0.7926989605847528\n",
      "Epoch 72, Training Loss: 0.7924756553593804\n",
      "Epoch 73, Training Loss: 0.7930555063135484\n",
      "Epoch 74, Training Loss: 0.7920839865067426\n",
      "Epoch 75, Training Loss: 0.7931747191793779\n",
      "Epoch 76, Training Loss: 0.7921839584322536\n",
      "Epoch 77, Training Loss: 0.7921898528407602\n",
      "Epoch 78, Training Loss: 0.7932145094871521\n",
      "Epoch 79, Training Loss: 0.7919700512465309\n",
      "Epoch 80, Training Loss: 0.7937735320540036\n",
      "Epoch 81, Training Loss: 0.7926753933289472\n",
      "Epoch 82, Training Loss: 0.7930105409902685\n",
      "Epoch 83, Training Loss: 0.7932735001339631\n",
      "Epoch 84, Training Loss: 0.7925936032042784\n",
      "Epoch 85, Training Loss: 0.7933340375563678\n",
      "Epoch 86, Training Loss: 0.7924479051898508\n",
      "Epoch 87, Training Loss: 0.7938514297148761\n",
      "Epoch 88, Training Loss: 0.792196097724578\n",
      "Epoch 89, Training Loss: 0.7936373209252077\n",
      "Epoch 90, Training Loss: 0.7927474649513468\n",
      "Epoch 91, Training Loss: 0.7931540253583123\n",
      "Epoch 92, Training Loss: 0.7924843753786648\n",
      "Epoch 93, Training Loss: 0.7921757771688349\n",
      "Epoch 94, Training Loss: 0.7918888411101173\n",
      "Epoch 95, Training Loss: 0.7931242378319011\n",
      "Epoch 96, Training Loss: 0.7925596536608304\n",
      "Epoch 97, Training Loss: 0.7927488195896149\n",
      "Epoch 98, Training Loss: 0.7932421701094684\n",
      "Epoch 99, Training Loss: 0.7928860818638521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:37:58,693] Trial 236 finished with value: 0.6346666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7929333796220667\n",
      "Epoch 1, Training Loss: 0.8917911436324729\n",
      "Epoch 2, Training Loss: 0.8171968692227414\n",
      "Epoch 3, Training Loss: 0.8113885892961258\n",
      "Epoch 4, Training Loss: 0.8094159688268389\n",
      "Epoch 5, Training Loss: 0.8085661703482606\n",
      "Epoch 6, Training Loss: 0.8056124723943553\n",
      "Epoch 7, Training Loss: 0.804251197675117\n",
      "Epoch 8, Training Loss: 0.802903481623284\n",
      "Epoch 9, Training Loss: 0.801775410390438\n",
      "Epoch 10, Training Loss: 0.801365329717335\n",
      "Epoch 11, Training Loss: 0.8006309949365774\n",
      "Epoch 12, Training Loss: 0.8010402114767777\n",
      "Epoch 13, Training Loss: 0.7995168024435976\n",
      "Epoch 14, Training Loss: 0.799669481309733\n",
      "Epoch 15, Training Loss: 0.7984744555071781\n",
      "Epoch 16, Training Loss: 0.7993738759729199\n",
      "Epoch 17, Training Loss: 0.7972600310368645\n",
      "Epoch 18, Training Loss: 0.7971958474109048\n",
      "Epoch 19, Training Loss: 0.7963709629119787\n",
      "Epoch 20, Training Loss: 0.7961771550035118\n",
      "Epoch 21, Training Loss: 0.7962868969243272\n",
      "Epoch 22, Training Loss: 0.7952411008956737\n",
      "Epoch 23, Training Loss: 0.7955200520673192\n",
      "Epoch 24, Training Loss: 0.7949909259502153\n",
      "Epoch 25, Training Loss: 0.7937582517028752\n",
      "Epoch 26, Training Loss: 0.7937690526022947\n",
      "Epoch 27, Training Loss: 0.793198641708919\n",
      "Epoch 28, Training Loss: 0.7926111979592115\n",
      "Epoch 29, Training Loss: 0.792014621612721\n",
      "Epoch 30, Training Loss: 0.7919316766853619\n",
      "Epoch 31, Training Loss: 0.7903209798318103\n",
      "Epoch 32, Training Loss: 0.7898360820641195\n",
      "Epoch 33, Training Loss: 0.7893969427374072\n",
      "Epoch 34, Training Loss: 0.7895726941581955\n",
      "Epoch 35, Training Loss: 0.7894936303447063\n",
      "Epoch 36, Training Loss: 0.788424891816046\n",
      "Epoch 37, Training Loss: 0.787693560930123\n",
      "Epoch 38, Training Loss: 0.7881419237394979\n",
      "Epoch 39, Training Loss: 0.7879678276248444\n",
      "Epoch 40, Training Loss: 0.7866937989579108\n",
      "Epoch 41, Training Loss: 0.7864303126370996\n",
      "Epoch 42, Training Loss: 0.7862870303311742\n",
      "Epoch 43, Training Loss: 0.7858545306929968\n",
      "Epoch 44, Training Loss: 0.7857849144397822\n",
      "Epoch 45, Training Loss: 0.7857772830733679\n",
      "Epoch 46, Training Loss: 0.7854382891404001\n",
      "Epoch 47, Training Loss: 0.7850368735485507\n",
      "Epoch 48, Training Loss: 0.7856289984588336\n",
      "Epoch 49, Training Loss: 0.7854802010650922\n",
      "Epoch 50, Training Loss: 0.7851973599061034\n",
      "Epoch 51, Training Loss: 0.7844614072849876\n",
      "Epoch 52, Training Loss: 0.783695079599108\n",
      "Epoch 53, Training Loss: 0.7845360289838977\n",
      "Epoch 54, Training Loss: 0.7845396443417197\n",
      "Epoch 55, Training Loss: 0.7839472874662929\n",
      "Epoch 56, Training Loss: 0.7841401293761748\n",
      "Epoch 57, Training Loss: 0.7843218163440102\n",
      "Epoch 58, Training Loss: 0.7833190558548261\n",
      "Epoch 59, Training Loss: 0.7830178683861754\n",
      "Epoch 60, Training Loss: 0.7837621178842129\n",
      "Epoch 61, Training Loss: 0.7829101057877218\n",
      "Epoch 62, Training Loss: 0.7831642204657533\n",
      "Epoch 63, Training Loss: 0.7832298345135567\n",
      "Epoch 64, Training Loss: 0.7831721485557412\n",
      "Epoch 65, Training Loss: 0.7822690060264186\n",
      "Epoch 66, Training Loss: 0.7831591867862787\n",
      "Epoch 67, Training Loss: 0.7829960477083249\n",
      "Epoch 68, Training Loss: 0.782387231884146\n",
      "Epoch 69, Training Loss: 0.7831248686725932\n",
      "Epoch 70, Training Loss: 0.7825967362948827\n",
      "Epoch 71, Training Loss: 0.7826744621857664\n",
      "Epoch 72, Training Loss: 0.7827089901257278\n",
      "Epoch 73, Training Loss: 0.7827671957195254\n",
      "Epoch 74, Training Loss: 0.7827782460621425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:39:26,494] Trial 237 finished with value: 0.6390666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7819868172918047\n",
      "Epoch 1, Training Loss: 0.9798334726713653\n",
      "Epoch 2, Training Loss: 0.8858922512907731\n",
      "Epoch 3, Training Loss: 0.8717932148087294\n",
      "Epoch 4, Training Loss: 0.8680918830677979\n",
      "Epoch 5, Training Loss: 0.8655933494854691\n",
      "Epoch 6, Training Loss: 0.8601406297289339\n",
      "Epoch 7, Training Loss: 0.8581870772784814\n",
      "Epoch 8, Training Loss: 0.8569816616692938\n",
      "Epoch 9, Training Loss: 0.8565653642317406\n",
      "Epoch 10, Training Loss: 0.8550521124574475\n",
      "Epoch 11, Training Loss: 0.85317599603108\n",
      "Epoch 12, Training Loss: 0.856923138467889\n",
      "Epoch 13, Training Loss: 0.8549874169485909\n",
      "Epoch 14, Training Loss: 0.8532573272411088\n",
      "Epoch 15, Training Loss: 0.8545056531303807\n",
      "Epoch 16, Training Loss: 0.8531582817995459\n",
      "Epoch 17, Training Loss: 0.8522052198424375\n",
      "Epoch 18, Training Loss: 0.8523608998248452\n",
      "Epoch 19, Training Loss: 0.8518935643640676\n",
      "Epoch 20, Training Loss: 0.8515965816669895\n",
      "Epoch 21, Training Loss: 0.8489113497554808\n",
      "Epoch 22, Training Loss: 0.8484725646506575\n",
      "Epoch 23, Training Loss: 0.8497809241588851\n",
      "Epoch 24, Training Loss: 0.8514305009877772\n",
      "Epoch 25, Training Loss: 0.8505406251527313\n",
      "Epoch 26, Training Loss: 0.8522360362504658\n",
      "Epoch 27, Training Loss: 0.8513832746591783\n",
      "Epoch 28, Training Loss: 0.8494614876750717\n",
      "Epoch 29, Training Loss: 0.8473483752487297\n",
      "Epoch 30, Training Loss: 0.8472425673241005\n",
      "Epoch 31, Training Loss: 0.8483613621919674\n",
      "Epoch 32, Training Loss: 0.8507840943515749\n",
      "Epoch 33, Training Loss: 0.8530024768714618\n",
      "Epoch 34, Training Loss: 0.851048245196952\n",
      "Epoch 35, Training Loss: 0.8470891930106887\n",
      "Epoch 36, Training Loss: 0.8482774588398467\n",
      "Epoch 37, Training Loss: 0.8497214145230171\n",
      "Epoch 38, Training Loss: 0.8490338559437515\n",
      "Epoch 39, Training Loss: 0.8489679579447983\n",
      "Epoch 40, Training Loss: 0.8480303246275823\n",
      "Epoch 41, Training Loss: 0.8476449393688288\n",
      "Epoch 42, Training Loss: 0.8486104858549017\n",
      "Epoch 43, Training Loss: 0.8470023415142433\n",
      "Epoch 44, Training Loss: 0.8453287837200595\n",
      "Epoch 45, Training Loss: 0.8458990091668036\n",
      "Epoch 46, Training Loss: 0.8477569615034233\n",
      "Epoch 47, Training Loss: 0.8446568135928391\n",
      "Epoch 48, Training Loss: 0.8467498163531597\n",
      "Epoch 49, Training Loss: 0.8467825679850758\n",
      "Epoch 50, Training Loss: 0.846020173488703\n",
      "Epoch 51, Training Loss: 0.8445092296241817\n",
      "Epoch 52, Training Loss: 0.8472223823231863\n",
      "Epoch 53, Training Loss: 0.8446677103078455\n",
      "Epoch 54, Training Loss: 0.8466076555108666\n",
      "Epoch 55, Training Loss: 0.8488397083784405\n",
      "Epoch 56, Training Loss: 0.8468895834191401\n",
      "Epoch 57, Training Loss: 0.8450540515713225\n",
      "Epoch 58, Training Loss: 0.8464033263966553\n",
      "Epoch 59, Training Loss: 0.8468908145911711\n",
      "Epoch 60, Training Loss: 0.8468955357271926\n",
      "Epoch 61, Training Loss: 0.844965168049461\n",
      "Epoch 62, Training Loss: 0.8429320793402822\n",
      "Epoch 63, Training Loss: 0.8430669396443474\n",
      "Epoch 64, Training Loss: 0.8425007305647197\n",
      "Epoch 65, Training Loss: 0.8432677934044286\n",
      "Epoch 66, Training Loss: 0.8444816950568579\n",
      "Epoch 67, Training Loss: 0.8440024016495038\n",
      "Epoch 68, Training Loss: 0.847018321564323\n",
      "Epoch 69, Training Loss: 0.8442736686620497\n",
      "Epoch 70, Training Loss: 0.8425667480418556\n",
      "Epoch 71, Training Loss: 0.8431597895192025\n",
      "Epoch 72, Training Loss: 0.844622890931323\n",
      "Epoch 73, Training Loss: 0.8441164367181018\n",
      "Epoch 74, Training Loss: 0.8466455568048291\n",
      "Epoch 75, Training Loss: 0.8449050664005423\n",
      "Epoch 76, Training Loss: 0.8454518942008341\n",
      "Epoch 77, Training Loss: 0.8460778572505578\n",
      "Epoch 78, Training Loss: 0.8470326953364494\n",
      "Epoch 79, Training Loss: 0.8447313735359594\n",
      "Epoch 80, Training Loss: 0.8415724617197997\n",
      "Epoch 81, Training Loss: 0.843254932604338\n",
      "Epoch 82, Training Loss: 0.8426414392048255\n",
      "Epoch 83, Training Loss: 0.8410541105091124\n",
      "Epoch 84, Training Loss: 0.841505349668345\n",
      "Epoch 85, Training Loss: 0.843967312529571\n",
      "Epoch 86, Training Loss: 0.8427088762584486\n",
      "Epoch 87, Training Loss: 0.8455669075026548\n",
      "Epoch 88, Training Loss: 0.8426580638813793\n",
      "Epoch 89, Training Loss: 0.8449713364579624\n",
      "Epoch 90, Training Loss: 0.8431999513977453\n",
      "Epoch 91, Training Loss: 0.8410659307824042\n",
      "Epoch 92, Training Loss: 0.8438453672523786\n",
      "Epoch 93, Training Loss: 0.8445780237814537\n",
      "Epoch 94, Training Loss: 0.8431053152657989\n",
      "Epoch 95, Training Loss: 0.8439378912735702\n",
      "Epoch 96, Training Loss: 0.8437638808013801\n",
      "Epoch 97, Training Loss: 0.8423307217153392\n",
      "Epoch 98, Training Loss: 0.8467524301736875\n",
      "Epoch 99, Training Loss: 0.8462891320536907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:41:20,243] Trial 238 finished with value: 0.39666666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8464312829469379\n",
      "Epoch 1, Training Loss: 0.874147731827614\n",
      "Epoch 2, Training Loss: 0.818454625014972\n",
      "Epoch 3, Training Loss: 0.8141740084590768\n",
      "Epoch 4, Training Loss: 0.8114364614164022\n",
      "Epoch 5, Training Loss: 0.8095198905557618\n",
      "Epoch 6, Training Loss: 0.8075896190521412\n",
      "Epoch 7, Training Loss: 0.8049571714006869\n",
      "Epoch 8, Training Loss: 0.8046208612004617\n",
      "Epoch 9, Training Loss: 0.8047368221713188\n",
      "Epoch 10, Training Loss: 0.8035892823584994\n",
      "Epoch 11, Training Loss: 0.8026726275458371\n",
      "Epoch 12, Training Loss: 0.8017412474280909\n",
      "Epoch 13, Training Loss: 0.801035956242927\n",
      "Epoch 14, Training Loss: 0.8005558520331418\n",
      "Epoch 15, Training Loss: 0.8004988483916547\n",
      "Epoch 16, Training Loss: 0.7993947948728289\n",
      "Epoch 17, Training Loss: 0.7992559595215589\n",
      "Epoch 18, Training Loss: 0.7992398339106624\n",
      "Epoch 19, Training Loss: 0.7983525814866661\n",
      "Epoch 20, Training Loss: 0.7974315372624792\n",
      "Epoch 21, Training Loss: 0.7976069519394322\n",
      "Epoch 22, Training Loss: 0.7973815755736559\n",
      "Epoch 23, Training Loss: 0.7962148516697991\n",
      "Epoch 24, Training Loss: 0.7948194609548812\n",
      "Epoch 25, Training Loss: 0.7951143006633099\n",
      "Epoch 26, Training Loss: 0.7932633552784311\n",
      "Epoch 27, Training Loss: 0.7941212968718737\n",
      "Epoch 28, Training Loss: 0.792926776229887\n",
      "Epoch 29, Training Loss: 0.7919906855525827\n",
      "Epoch 30, Training Loss: 0.7915032632368848\n",
      "Epoch 31, Training Loss: 0.7914173744674912\n",
      "Epoch 32, Training Loss: 0.7910019562656718\n",
      "Epoch 33, Training Loss: 0.7899058102665091\n",
      "Epoch 34, Training Loss: 0.7892114890249152\n",
      "Epoch 35, Training Loss: 0.7890099389212472\n",
      "Epoch 36, Training Loss: 0.7892454608042437\n",
      "Epoch 37, Training Loss: 0.7892568382105433\n",
      "Epoch 38, Training Loss: 0.7877946837504107\n",
      "Epoch 39, Training Loss: 0.7874110536915916\n",
      "Epoch 40, Training Loss: 0.7867063103761888\n",
      "Epoch 41, Training Loss: 0.7864553617355519\n",
      "Epoch 42, Training Loss: 0.7867468421620534\n",
      "Epoch 43, Training Loss: 0.786416332703784\n",
      "Epoch 44, Training Loss: 0.7864061481074283\n",
      "Epoch 45, Training Loss: 0.786069817560956\n",
      "Epoch 46, Training Loss: 0.7856749736276785\n",
      "Epoch 47, Training Loss: 0.7852345710410211\n",
      "Epoch 48, Training Loss: 0.7851883666855949\n",
      "Epoch 49, Training Loss: 0.7850639421240728\n",
      "Epoch 50, Training Loss: 0.7848490085816922\n",
      "Epoch 51, Training Loss: 0.7848619313168347\n",
      "Epoch 52, Training Loss: 0.7851931962751805\n",
      "Epoch 53, Training Loss: 0.7851919859871829\n",
      "Epoch 54, Training Loss: 0.7841828052262615\n",
      "Epoch 55, Training Loss: 0.7845384444509234\n",
      "Epoch 56, Training Loss: 0.7841332500142263\n",
      "Epoch 57, Training Loss: 0.7837949068922746\n",
      "Epoch 58, Training Loss: 0.7844747097868668\n",
      "Epoch 59, Training Loss: 0.7834152853578553\n",
      "Epoch 60, Training Loss: 0.7838496455572601\n",
      "Epoch 61, Training Loss: 0.7832532064358991\n",
      "Epoch 62, Training Loss: 0.783651574303333\n",
      "Epoch 63, Training Loss: 0.7832535129740722\n",
      "Epoch 64, Training Loss: 0.7837528159743861\n",
      "Epoch 65, Training Loss: 0.7827583802373785\n",
      "Epoch 66, Training Loss: 0.7832823052442164\n",
      "Epoch 67, Training Loss: 0.7825683822309164\n",
      "Epoch 68, Training Loss: 0.7838881240751511\n",
      "Epoch 69, Training Loss: 0.7830196960528094\n",
      "Epoch 70, Training Loss: 0.7831104421077815\n",
      "Epoch 71, Training Loss: 0.783378349390245\n",
      "Epoch 72, Training Loss: 0.7831302604280916\n",
      "Epoch 73, Training Loss: 0.7825490290060976\n",
      "Epoch 74, Training Loss: 0.7824563632334085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:42:43,969] Trial 239 finished with value: 0.6340666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7822902792378476\n",
      "Epoch 1, Training Loss: 0.8516711420171401\n",
      "Epoch 2, Training Loss: 0.8239753981197582\n",
      "Epoch 3, Training Loss: 0.8146891697014079\n",
      "Epoch 4, Training Loss: 0.8132367742061615\n",
      "Epoch 5, Training Loss: 0.8126284943608677\n",
      "Epoch 6, Training Loss: 0.8138006610028884\n",
      "Epoch 7, Training Loss: 0.8159758408630595\n",
      "Epoch 8, Training Loss: 0.8086144261500414\n",
      "Epoch 9, Training Loss: 0.8099798659717335\n",
      "Epoch 10, Training Loss: 0.8079947745800018\n",
      "Epoch 11, Training Loss: 0.8107363677024841\n",
      "Epoch 12, Training Loss: 0.810165766617831\n",
      "Epoch 13, Training Loss: 0.810339830272338\n",
      "Epoch 14, Training Loss: 0.8098891815017252\n",
      "Epoch 15, Training Loss: 0.8079929867912742\n",
      "Epoch 16, Training Loss: 0.8103256050278159\n",
      "Epoch 17, Training Loss: 0.8078169007161085\n",
      "Epoch 18, Training Loss: 0.8083132667401257\n",
      "Epoch 19, Training Loss: 0.8104133909590104\n",
      "Epoch 20, Training Loss: 0.8100007674974554\n",
      "Epoch 21, Training Loss: 0.807103540616877\n",
      "Epoch 22, Training Loss: 0.8096978688240051\n",
      "Epoch 23, Training Loss: 0.8108826297170976\n",
      "Epoch 24, Training Loss: 0.8072786989632775\n",
      "Epoch 25, Training Loss: 0.8090984265944537\n",
      "Epoch 26, Training Loss: 0.8098189074852887\n",
      "Epoch 27, Training Loss: 0.8081010887903326\n",
      "Epoch 28, Training Loss: 0.8115435323294471\n",
      "Epoch 29, Training Loss: 0.8086899904643788\n",
      "Epoch 30, Training Loss: 0.8092753591256984\n",
      "Epoch 31, Training Loss: 0.8074991088053759\n",
      "Epoch 32, Training Loss: 0.808148811564726\n",
      "Epoch 33, Training Loss: 0.8120156730623806\n",
      "Epoch 34, Training Loss: 0.8077249035414528\n",
      "Epoch 35, Training Loss: 0.8110409914746004\n",
      "Epoch 36, Training Loss: 0.8083028173446656\n",
      "Epoch 37, Training Loss: 0.8087418385814218\n",
      "Epoch 38, Training Loss: 0.8097180076206432\n",
      "Epoch 39, Training Loss: 0.8097864195879768\n",
      "Epoch 40, Training Loss: 0.8118887067542356\n",
      "Epoch 41, Training Loss: 0.8137234259352965\n",
      "Epoch 42, Training Loss: 0.8082926110660329\n",
      "Epoch 43, Training Loss: 0.8095762263326084\n",
      "Epoch 44, Training Loss: 0.8110373836405137\n",
      "Epoch 45, Training Loss: 0.8112394311147577\n",
      "Epoch 46, Training Loss: 0.8081599180838641\n",
      "Epoch 47, Training Loss: 0.8112635987646439\n",
      "Epoch 48, Training Loss: 0.8100130135872785\n",
      "Epoch 49, Training Loss: 0.8115273772968965\n",
      "Epoch 50, Training Loss: 0.8096297285136055\n",
      "Epoch 51, Training Loss: 0.8094000842290766\n",
      "Epoch 52, Training Loss: 0.8097607792125029\n",
      "Epoch 53, Training Loss: 0.8089672813696019\n",
      "Epoch 54, Training Loss: 0.8092892097024357\n",
      "Epoch 55, Training Loss: 0.808622234989615\n",
      "Epoch 56, Training Loss: 0.8133601684430066\n",
      "Epoch 57, Training Loss: 0.8120875392240636\n",
      "Epoch 58, Training Loss: 0.8103353274569792\n",
      "Epoch 59, Training Loss: 0.8078841314596288\n",
      "Epoch 60, Training Loss: 0.8136996378618128\n",
      "Epoch 61, Training Loss: 0.8111631402548621\n",
      "Epoch 62, Training Loss: 0.810268451606526\n",
      "Epoch 63, Training Loss: 0.8107484559451833\n",
      "Epoch 64, Training Loss: 0.810412520310458\n",
      "Epoch 65, Training Loss: 0.8109978619743796\n",
      "Epoch 66, Training Loss: 0.8123546176096972\n",
      "Epoch 67, Training Loss: 0.8080748687070959\n",
      "Epoch 68, Training Loss: 0.8108785034628475\n",
      "Epoch 69, Training Loss: 0.8100452489011428\n",
      "Epoch 70, Training Loss: 0.8137335010135875\n",
      "Epoch 71, Training Loss: 0.8104703133246478\n",
      "Epoch 72, Training Loss: 0.8095798628470477\n",
      "Epoch 73, Training Loss: 0.811915417138268\n",
      "Epoch 74, Training Loss: 0.8119640822971569\n",
      "Epoch 75, Training Loss: 0.8076447978440453\n",
      "Epoch 76, Training Loss: 0.8086309634236728\n",
      "Epoch 77, Training Loss: 0.8106860388727749\n",
      "Epoch 78, Training Loss: 0.8095557541706982\n",
      "Epoch 79, Training Loss: 0.8110354414406945\n",
      "Epoch 80, Training Loss: 0.8101055122824277\n",
      "Epoch 81, Training Loss: 0.8121729324144475\n",
      "Epoch 82, Training Loss: 0.8098435427862055\n",
      "Epoch 83, Training Loss: 0.8076685317123637\n",
      "Epoch 84, Training Loss: 0.8104308238450219\n",
      "Epoch 85, Training Loss: 0.8119492777656107\n",
      "Epoch 86, Training Loss: 0.8082648608263802\n",
      "Epoch 87, Training Loss: 0.8066012106923496\n",
      "Epoch 88, Training Loss: 0.810425546800389\n",
      "Epoch 89, Training Loss: 0.8145357070249669\n",
      "Epoch 90, Training Loss: 0.8126534850457136\n",
      "Epoch 91, Training Loss: 0.8094428138873156\n",
      "Epoch 92, Training Loss: 0.8100014894850114\n",
      "Epoch 93, Training Loss: 0.8117912011988023\n",
      "Epoch 94, Training Loss: 0.8129203268359689\n",
      "Epoch 95, Training Loss: 0.8147456563220304\n",
      "Epoch 96, Training Loss: 0.8209352343222674\n",
      "Epoch 97, Training Loss: 0.8213618504299837\n",
      "Epoch 98, Training Loss: 0.8225240030008204\n",
      "Epoch 99, Training Loss: 0.8244352447285371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:45:00,434] Trial 240 finished with value: 0.6054666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8266228468277875\n",
      "Epoch 1, Training Loss: 1.0028602505431456\n",
      "Epoch 2, Training Loss: 0.9228282473367804\n",
      "Epoch 3, Training Loss: 0.8980279013689827\n",
      "Epoch 4, Training Loss: 0.8669941405689016\n",
      "Epoch 5, Training Loss: 0.8325493678625893\n",
      "Epoch 6, Training Loss: 0.8144377038759344\n",
      "Epoch 7, Training Loss: 0.8084657749708961\n",
      "Epoch 8, Training Loss: 0.8063304241264567\n",
      "Epoch 9, Training Loss: 0.8046839129223543\n",
      "Epoch 10, Training Loss: 0.8038097181039698\n",
      "Epoch 11, Training Loss: 0.8028525019393248\n",
      "Epoch 12, Training Loss: 0.8022250230873332\n",
      "Epoch 13, Training Loss: 0.8017490471110624\n",
      "Epoch 14, Training Loss: 0.8009318041801453\n",
      "Epoch 15, Training Loss: 0.8008490807869855\n",
      "Epoch 16, Training Loss: 0.8002190508561976\n",
      "Epoch 17, Training Loss: 0.7997012628527249\n",
      "Epoch 18, Training Loss: 0.7994734088813558\n",
      "Epoch 19, Training Loss: 0.7987628241146312\n",
      "Epoch 20, Training Loss: 0.7981113154046676\n",
      "Epoch 21, Training Loss: 0.7974667016898884\n",
      "Epoch 22, Training Loss: 0.7971183070715736\n",
      "Epoch 23, Training Loss: 0.7960817006055046\n",
      "Epoch 24, Training Loss: 0.7961569694911732\n",
      "Epoch 25, Training Loss: 0.7957844125523287\n",
      "Epoch 26, Training Loss: 0.7952560217941509\n",
      "Epoch 27, Training Loss: 0.7950145562256083\n",
      "Epoch 28, Training Loss: 0.7947105356524973\n",
      "Epoch 29, Training Loss: 0.7943480396971984\n",
      "Epoch 30, Training Loss: 0.7937943084800945\n",
      "Epoch 31, Training Loss: 0.7934161339787876\n",
      "Epoch 32, Training Loss: 0.7926756173021653\n",
      "Epoch 33, Training Loss: 0.7924319000805126\n",
      "Epoch 34, Training Loss: 0.7918284864986644\n",
      "Epoch 35, Training Loss: 0.7915760761148789\n",
      "Epoch 36, Training Loss: 0.7910354314130895\n",
      "Epoch 37, Training Loss: 0.7911909607578727\n",
      "Epoch 38, Training Loss: 0.7908466519327725\n",
      "Epoch 39, Training Loss: 0.7901259251201854\n",
      "Epoch 40, Training Loss: 0.7897388402854695\n",
      "Epoch 41, Training Loss: 0.7894109304512248\n",
      "Epoch 42, Training Loss: 0.7896044279547298\n",
      "Epoch 43, Training Loss: 0.788951246808557\n",
      "Epoch 44, Training Loss: 0.7886774975412032\n",
      "Epoch 45, Training Loss: 0.7886206875829136\n",
      "Epoch 46, Training Loss: 0.788438505565419\n",
      "Epoch 47, Training Loss: 0.7881099618883693\n",
      "Epoch 48, Training Loss: 0.7879537573281457\n",
      "Epoch 49, Training Loss: 0.7877950118569766\n",
      "Epoch 50, Training Loss: 0.7872888269845177\n",
      "Epoch 51, Training Loss: 0.7873179362801944\n",
      "Epoch 52, Training Loss: 0.7873033603499917\n",
      "Epoch 53, Training Loss: 0.7869328720429364\n",
      "Epoch 54, Training Loss: 0.78673895036473\n",
      "Epoch 55, Training Loss: 0.7862853776707369\n",
      "Epoch 56, Training Loss: 0.7865075275477241\n",
      "Epoch 57, Training Loss: 0.7861915783321156\n",
      "Epoch 58, Training Loss: 0.7861134568382712\n",
      "Epoch 59, Training Loss: 0.7859873416143305\n",
      "Epoch 60, Training Loss: 0.7857313398753896\n",
      "Epoch 61, Training Loss: 0.7858932532983668\n",
      "Epoch 62, Training Loss: 0.7858282406189863\n",
      "Epoch 63, Training Loss: 0.7855139106161454\n",
      "Epoch 64, Training Loss: 0.7853672390124377\n",
      "Epoch 65, Training Loss: 0.7852315870453329\n",
      "Epoch 66, Training Loss: 0.785425160071429\n",
      "Epoch 67, Training Loss: 0.7851149986771976\n",
      "Epoch 68, Training Loss: 0.7849987581898185\n",
      "Epoch 69, Training Loss: 0.7849232205923866\n",
      "Epoch 70, Training Loss: 0.7847860912715687\n",
      "Epoch 71, Training Loss: 0.7847577118873597\n",
      "Epoch 72, Training Loss: 0.784765063384\n",
      "Epoch 73, Training Loss: 0.7846354296628166\n",
      "Epoch 74, Training Loss: 0.7846037631175097\n",
      "Epoch 75, Training Loss: 0.7843316256999969\n",
      "Epoch 76, Training Loss: 0.7845096215781043\n",
      "Epoch 77, Training Loss: 0.7841417365915635\n",
      "Epoch 78, Training Loss: 0.7844407097031089\n",
      "Epoch 79, Training Loss: 0.7840618408427519\n",
      "Epoch 80, Training Loss: 0.7839466324974509\n",
      "Epoch 81, Training Loss: 0.784047029579387\n",
      "Epoch 82, Training Loss: 0.783697412925608\n",
      "Epoch 83, Training Loss: 0.7838011608404272\n",
      "Epoch 84, Training Loss: 0.783749881141326\n",
      "Epoch 85, Training Loss: 0.7836284714586594\n",
      "Epoch 86, Training Loss: 0.7834204978802625\n",
      "Epoch 87, Training Loss: 0.7834652658771066\n",
      "Epoch 88, Training Loss: 0.7832165170417112\n",
      "Epoch 89, Training Loss: 0.7834149314375485\n",
      "Epoch 90, Training Loss: 0.783309875866946\n",
      "Epoch 91, Training Loss: 0.7831883008339826\n",
      "Epoch 92, Training Loss: 0.7831443905830383\n",
      "Epoch 93, Training Loss: 0.7829883650471182\n",
      "Epoch 94, Training Loss: 0.7829743866359486\n",
      "Epoch 95, Training Loss: 0.7830605321070727\n",
      "Epoch 96, Training Loss: 0.7828076020409079\n",
      "Epoch 97, Training Loss: 0.7827652715234196\n",
      "Epoch 98, Training Loss: 0.7828555565721849\n",
      "Epoch 99, Training Loss: 0.7828593697968651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:46:47,956] Trial 241 finished with value: 0.6393333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7829343377842622\n",
      "Epoch 1, Training Loss: 0.838370785222334\n",
      "Epoch 2, Training Loss: 0.8068288651634665\n",
      "Epoch 3, Training Loss: 0.8026592235004201\n",
      "Epoch 4, Training Loss: 0.7994193638072294\n",
      "Epoch 5, Training Loss: 0.8000874417669633\n",
      "Epoch 6, Training Loss: 0.7964841844755061\n",
      "Epoch 7, Training Loss: 0.796076999762479\n",
      "Epoch 8, Training Loss: 0.7953532491010779\n",
      "Epoch 9, Training Loss: 0.7953445837076972\n",
      "Epoch 10, Training Loss: 0.7942801893458646\n",
      "Epoch 11, Training Loss: 0.794488341107088\n",
      "Epoch 12, Training Loss: 0.7939438202801873\n",
      "Epoch 13, Training Loss: 0.7944022449325113\n",
      "Epoch 14, Training Loss: 0.7937652807375964\n",
      "Epoch 15, Training Loss: 0.7926746831220739\n",
      "Epoch 16, Training Loss: 0.7922839896117939\n",
      "Epoch 17, Training Loss: 0.7931257801897386\n",
      "Epoch 18, Training Loss: 0.7922205163450802\n",
      "Epoch 19, Training Loss: 0.7925791811241824\n",
      "Epoch 20, Training Loss: 0.793013843648574\n",
      "Epoch 21, Training Loss: 0.7919997214569765\n",
      "Epoch 22, Training Loss: 0.7906542341148152\n",
      "Epoch 23, Training Loss: 0.7916687831457924\n",
      "Epoch 24, Training Loss: 0.7915615527770099\n",
      "Epoch 25, Training Loss: 0.7919077908291536\n",
      "Epoch 26, Training Loss: 0.7903569213081809\n",
      "Epoch 27, Training Loss: 0.7909780132770539\n",
      "Epoch 28, Training Loss: 0.7908969058709986\n",
      "Epoch 29, Training Loss: 0.7903413127450382\n",
      "Epoch 30, Training Loss: 0.7920647752986235\n",
      "Epoch 31, Training Loss: 0.79012778450461\n",
      "Epoch 32, Training Loss: 0.789912048367893\n",
      "Epoch 33, Training Loss: 0.7908093451051151\n",
      "Epoch 34, Training Loss: 0.7890451748230878\n",
      "Epoch 35, Training Loss: 0.7906945406689363\n",
      "Epoch 36, Training Loss: 0.7901658295182621\n",
      "Epoch 37, Training Loss: 0.7903637480034548\n",
      "Epoch 38, Training Loss: 0.7897971848880544\n",
      "Epoch 39, Training Loss: 0.7898869916971992\n",
      "Epoch 40, Training Loss: 0.790337942978915\n",
      "Epoch 41, Training Loss: 0.7904144201559179\n",
      "Epoch 42, Training Loss: 0.7894806838736814\n",
      "Epoch 43, Training Loss: 0.7895567040583666\n",
      "Epoch 44, Training Loss: 0.7892650590924656\n",
      "Epoch 45, Training Loss: 0.7894713439660914\n",
      "Epoch 46, Training Loss: 0.7903942960851332\n",
      "Epoch 47, Training Loss: 0.790255559051738\n",
      "Epoch 48, Training Loss: 0.7901240662266227\n",
      "Epoch 49, Training Loss: 0.7894867954534642\n",
      "Epoch 50, Training Loss: 0.7898599206700044\n",
      "Epoch 51, Training Loss: 0.7898487722873688\n",
      "Epoch 52, Training Loss: 0.7893616779411541\n",
      "Epoch 53, Training Loss: 0.7892659202042748\n",
      "Epoch 54, Training Loss: 0.7894120494758382\n",
      "Epoch 55, Training Loss: 0.7897603689923006\n",
      "Epoch 56, Training Loss: 0.789206559307435\n",
      "Epoch 57, Training Loss: 0.789602336883545\n",
      "Epoch 58, Training Loss: 0.789626464212642\n",
      "Epoch 59, Training Loss: 0.7896384834541994\n",
      "Epoch 60, Training Loss: 0.7897520399093628\n",
      "Epoch 61, Training Loss: 0.7892248347226312\n",
      "Epoch 62, Training Loss: 0.7893758287149317\n",
      "Epoch 63, Training Loss: 0.7889952400852652\n",
      "Epoch 64, Training Loss: 0.7892538602211896\n",
      "Epoch 65, Training Loss: 0.7888619909566992\n",
      "Epoch 66, Training Loss: 0.7886264498093549\n",
      "Epoch 67, Training Loss: 0.7893764845763935\n",
      "Epoch 68, Training Loss: 0.7883549522652346\n",
      "Epoch 69, Training Loss: 0.788221729292589\n",
      "Epoch 70, Training Loss: 0.7888798069252687\n",
      "Epoch 71, Training Loss: 0.7879768757259145\n",
      "Epoch 72, Training Loss: 0.7887325017592486\n",
      "Epoch 73, Training Loss: 0.7881852094566121\n",
      "Epoch 74, Training Loss: 0.7882436952871434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:48:29,538] Trial 242 finished with value: 0.6358666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7883389693147996\n",
      "Epoch 1, Training Loss: 1.1080893195376678\n",
      "Epoch 2, Training Loss: 1.0791911609032574\n",
      "Epoch 3, Training Loss: 1.0732705465485068\n",
      "Epoch 4, Training Loss: 1.0677177722313824\n",
      "Epoch 5, Training Loss: 1.0624877583279329\n",
      "Epoch 6, Training Loss: 1.0575440078623155\n",
      "Epoch 7, Training Loss: 1.052835680737215\n",
      "Epoch 8, Training Loss: 1.0483553304391748\n",
      "Epoch 9, Training Loss: 1.044061376768\n",
      "Epoch 10, Training Loss: 1.0399333331865424\n",
      "Epoch 11, Training Loss: 1.036005848085179\n",
      "Epoch 12, Training Loss: 1.0322220040068908\n",
      "Epoch 13, Training Loss: 1.028554176232394\n",
      "Epoch 14, Training Loss: 1.0250977159247678\n",
      "Epoch 15, Training Loss: 1.0217407773522769\n",
      "Epoch 16, Training Loss: 1.0184990152891944\n",
      "Epoch 17, Training Loss: 1.0154244892737445\n",
      "Epoch 18, Training Loss: 1.012450780097176\n",
      "Epoch 19, Training Loss: 1.0096178303045384\n",
      "Epoch 20, Training Loss: 1.0068920707001405\n",
      "Epoch 21, Training Loss: 1.0042804236271803\n",
      "Epoch 22, Training Loss: 1.0017792700318728\n",
      "Epoch 23, Training Loss: 0.9993977393823511\n",
      "Epoch 24, Training Loss: 0.9971342938086566\n",
      "Epoch 25, Training Loss: 0.9949748049062841\n",
      "Epoch 26, Training Loss: 0.9929055348564597\n",
      "Epoch 27, Training Loss: 0.9909513927207274\n",
      "Epoch 28, Training Loss: 0.989087302123799\n",
      "Epoch 29, Training Loss: 0.9873268920533798\n",
      "Epoch 30, Training Loss: 0.9856454075785244\n",
      "Epoch 31, Training Loss: 0.9840589955975028\n",
      "Epoch 32, Training Loss: 0.9825710077846751\n",
      "Epoch 33, Training Loss: 0.9811502169160282\n",
      "Epoch 34, Training Loss: 0.979806025799583\n",
      "Epoch 35, Training Loss: 0.9785564360197853\n",
      "Epoch 36, Training Loss: 0.9773680876984315\n",
      "Epoch 37, Training Loss: 0.9762394534139072\n",
      "Epoch 38, Training Loss: 0.9751888607530033\n",
      "Epoch 39, Training Loss: 0.9742014805709615\n",
      "Epoch 40, Training Loss: 0.9732680459583507\n",
      "Epoch 41, Training Loss: 0.9723935945595013\n",
      "Epoch 42, Training Loss: 0.9715730090702281\n",
      "Epoch 43, Training Loss: 0.9707897109143874\n",
      "Epoch 44, Training Loss: 0.970064162717146\n",
      "Epoch 45, Training Loss: 0.9693837931576897\n",
      "Epoch 46, Training Loss: 0.9687452880073996\n",
      "Epoch 47, Training Loss: 0.9681340399910422\n",
      "Epoch 48, Training Loss: 0.9675665796504301\n",
      "Epoch 49, Training Loss: 0.9670444029219011\n",
      "Epoch 50, Training Loss: 0.9665311818964342\n",
      "Epoch 51, Training Loss: 0.9660606002106386\n",
      "Epoch 52, Training Loss: 0.9656286325174219\n",
      "Epoch 53, Training Loss: 0.9652116200503181\n",
      "Epoch 54, Training Loss: 0.9648140772651224\n",
      "Epoch 55, Training Loss: 0.964452231982175\n",
      "Epoch 56, Training Loss: 0.9640901634973639\n",
      "Epoch 57, Training Loss: 0.9637754619822783\n",
      "Epoch 58, Training Loss: 0.9634614903786604\n",
      "Epoch 59, Training Loss: 0.963168398772969\n",
      "Epoch 60, Training Loss: 0.9628836113565108\n",
      "Epoch 61, Training Loss: 0.962612016621758\n",
      "Epoch 62, Training Loss: 0.9623685925848344\n",
      "Epoch 63, Training Loss: 0.9621270113131579\n",
      "Epoch 64, Training Loss: 0.9618890195734361\n",
      "Epoch 65, Training Loss: 0.9616786769558402\n",
      "Epoch 66, Training Loss: 0.9614702493302962\n",
      "Epoch 67, Training Loss: 0.9612571306789622\n",
      "Epoch 68, Training Loss: 0.9610779350645402\n",
      "Epoch 69, Training Loss: 0.9608922202446881\n",
      "Epoch 70, Training Loss: 0.9607079365674187\n",
      "Epoch 71, Training Loss: 0.9605439282866085\n",
      "Epoch 72, Training Loss: 0.9603741941732519\n",
      "Epoch 73, Training Loss: 0.9602130984558779\n",
      "Epoch 74, Training Loss: 0.9600597972028395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:49:38,785] Trial 243 finished with value: 0.526 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9599108970165253\n",
      "Epoch 1, Training Loss: 0.952113586081598\n",
      "Epoch 2, Training Loss: 0.8885584954928635\n",
      "Epoch 3, Training Loss: 0.841462945310693\n",
      "Epoch 4, Training Loss: 0.8202632922875254\n",
      "Epoch 5, Training Loss: 0.8133764382591822\n",
      "Epoch 6, Training Loss: 0.8110205566972718\n",
      "Epoch 7, Training Loss: 0.8093283320728101\n",
      "Epoch 8, Training Loss: 0.8082509826896782\n",
      "Epoch 9, Training Loss: 0.8076966507990557\n",
      "Epoch 10, Training Loss: 0.8069343572272394\n",
      "Epoch 11, Training Loss: 0.8067945856797067\n",
      "Epoch 12, Training Loss: 0.8056429747800181\n",
      "Epoch 13, Training Loss: 0.8056739771276489\n",
      "Epoch 14, Training Loss: 0.8063822752551029\n",
      "Epoch 15, Training Loss: 0.8046923130078423\n",
      "Epoch 16, Training Loss: 0.8047002122814494\n",
      "Epoch 17, Training Loss: 0.8045800679608395\n",
      "Epoch 18, Training Loss: 0.804687900381877\n",
      "Epoch 19, Training Loss: 0.8037660130880829\n",
      "Epoch 20, Training Loss: 0.8037659874536041\n",
      "Epoch 21, Training Loss: 0.8035235317129837\n",
      "Epoch 22, Training Loss: 0.8035934987821077\n",
      "Epoch 23, Training Loss: 0.8045658321308911\n",
      "Epoch 24, Training Loss: 0.8022987922331444\n",
      "Epoch 25, Training Loss: 0.8026226622717721\n",
      "Epoch 26, Training Loss: 0.8024574654442923\n",
      "Epoch 27, Training Loss: 0.8016320597856564\n",
      "Epoch 28, Training Loss: 0.8024105719157628\n",
      "Epoch 29, Training Loss: 0.8017096120612066\n",
      "Epoch 30, Training Loss: 0.8019267304499347\n",
      "Epoch 31, Training Loss: 0.801771392678856\n",
      "Epoch 32, Training Loss: 0.8016291871106714\n",
      "Epoch 33, Training Loss: 0.8010986418652355\n",
      "Epoch 34, Training Loss: 0.8006679893436288\n",
      "Epoch 35, Training Loss: 0.800904949625632\n",
      "Epoch 36, Training Loss: 0.8005172808367507\n",
      "Epoch 37, Training Loss: 0.8000469799328568\n",
      "Epoch 38, Training Loss: 0.7999779757700468\n",
      "Epoch 39, Training Loss: 0.7999441880032532\n",
      "Epoch 40, Training Loss: 0.7995554941041129\n",
      "Epoch 41, Training Loss: 0.7997591190768364\n",
      "Epoch 42, Training Loss: 0.7992296151200632\n",
      "Epoch 43, Training Loss: 0.7992087052280742\n",
      "Epoch 44, Training Loss: 0.7989315091219164\n",
      "Epoch 45, Training Loss: 0.7992356828280858\n",
      "Epoch 46, Training Loss: 0.7989372603875354\n",
      "Epoch 47, Training Loss: 0.7990826122742847\n",
      "Epoch 48, Training Loss: 0.798940999077675\n",
      "Epoch 49, Training Loss: 0.7986921627718703\n",
      "Epoch 50, Training Loss: 0.7989672512040102\n",
      "Epoch 51, Training Loss: 0.7989379707135652\n",
      "Epoch 52, Training Loss: 0.7993001660009972\n",
      "Epoch 53, Training Loss: 0.7983543618281085\n",
      "Epoch 54, Training Loss: 0.7983169446314188\n",
      "Epoch 55, Training Loss: 0.7984118663278738\n",
      "Epoch 56, Training Loss: 0.797645513141962\n",
      "Epoch 57, Training Loss: 0.7980839456830706\n",
      "Epoch 58, Training Loss: 0.7982879022906597\n",
      "Epoch 59, Training Loss: 0.7981356333072921\n",
      "Epoch 60, Training Loss: 0.7985620867937131\n",
      "Epoch 61, Training Loss: 0.7982495087429993\n",
      "Epoch 62, Training Loss: 0.7980312307078139\n",
      "Epoch 63, Training Loss: 0.7979571656176918\n",
      "Epoch 64, Training Loss: 0.7981990837513055\n",
      "Epoch 65, Training Loss: 0.7975286035609425\n",
      "Epoch 66, Training Loss: 0.7977123737335206\n",
      "Epoch 67, Training Loss: 0.7972186452464054\n",
      "Epoch 68, Training Loss: 0.7978376171642676\n",
      "Epoch 69, Training Loss: 0.7977357072937757\n",
      "Epoch 70, Training Loss: 0.7974555879607237\n",
      "Epoch 71, Training Loss: 0.797905941565234\n",
      "Epoch 72, Training Loss: 0.7971234162947289\n",
      "Epoch 73, Training Loss: 0.7971100511407494\n",
      "Epoch 74, Training Loss: 0.7979082751991158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:50:52,670] Trial 244 finished with value: 0.6348 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7976136476473701\n",
      "Epoch 1, Training Loss: 0.8499299134226407\n",
      "Epoch 2, Training Loss: 0.8182970634628745\n",
      "Epoch 3, Training Loss: 0.8138020693554597\n",
      "Epoch 4, Training Loss: 0.8112358019632452\n",
      "Epoch 5, Training Loss: 0.8077563445708331\n",
      "Epoch 6, Training Loss: 0.806370217589771\n",
      "Epoch 7, Training Loss: 0.8056943562451531\n",
      "Epoch 8, Training Loss: 0.8043107072044822\n",
      "Epoch 9, Training Loss: 0.8041178793065689\n",
      "Epoch 10, Training Loss: 0.8030053156263688\n",
      "Epoch 11, Training Loss: 0.8029697400682113\n",
      "Epoch 12, Training Loss: 0.8014246985491584\n",
      "Epoch 13, Training Loss: 0.8008209790201748\n",
      "Epoch 14, Training Loss: 0.8006614696278291\n",
      "Epoch 15, Training Loss: 0.8002177314197316\n",
      "Epoch 16, Training Loss: 0.7995715672128341\n",
      "Epoch 17, Training Loss: 0.7991597426638883\n",
      "Epoch 18, Training Loss: 0.7988918393499711\n",
      "Epoch 19, Training Loss: 0.7989009405584896\n",
      "Epoch 20, Training Loss: 0.798977455111111\n",
      "Epoch 21, Training Loss: 0.7981893466500675\n",
      "Epoch 22, Training Loss: 0.7976783571523779\n",
      "Epoch 23, Training Loss: 0.7975040338319891\n",
      "Epoch 24, Training Loss: 0.7973843988951514\n",
      "Epoch 25, Training Loss: 0.796463153222028\n",
      "Epoch 26, Training Loss: 0.796847455852172\n",
      "Epoch 27, Training Loss: 0.796811290769016\n",
      "Epoch 28, Training Loss: 0.7959312797995175\n",
      "Epoch 29, Training Loss: 0.7965792429447174\n",
      "Epoch 30, Training Loss: 0.7961779268348919\n",
      "Epoch 31, Training Loss: 0.7956609990316279\n",
      "Epoch 32, Training Loss: 0.7960543980317957\n",
      "Epoch 33, Training Loss: 0.7951192192470327\n",
      "Epoch 34, Training Loss: 0.7957482315512264\n",
      "Epoch 35, Training Loss: 0.7955296646847444\n",
      "Epoch 36, Training Loss: 0.7951349016498117\n",
      "Epoch 37, Training Loss: 0.7951739661833819\n",
      "Epoch 38, Training Loss: 0.7945090755294351\n",
      "Epoch 39, Training Loss: 0.7945483685942257\n",
      "Epoch 40, Training Loss: 0.7947362868926104\n",
      "Epoch 41, Training Loss: 0.7943167838629555\n",
      "Epoch 42, Training Loss: 0.7951999427991755\n",
      "Epoch 43, Training Loss: 0.7947879440644208\n",
      "Epoch 44, Training Loss: 0.7944423981975106\n",
      "Epoch 45, Training Loss: 0.7943912585342632\n",
      "Epoch 46, Training Loss: 0.7950995953644023\n",
      "Epoch 47, Training Loss: 0.794291191591936\n",
      "Epoch 48, Training Loss: 0.7941620917881236\n",
      "Epoch 49, Training Loss: 0.7937983625776628\n",
      "Epoch 50, Training Loss: 0.7943979896517361\n",
      "Epoch 51, Training Loss: 0.7942625549260308\n",
      "Epoch 52, Training Loss: 0.7942787218795103\n",
      "Epoch 53, Training Loss: 0.7940857967909645\n",
      "Epoch 54, Training Loss: 0.7942231653017157\n",
      "Epoch 55, Training Loss: 0.7936497160266427\n",
      "Epoch 56, Training Loss: 0.7940911776178023\n",
      "Epoch 57, Training Loss: 0.793145812960232\n",
      "Epoch 58, Training Loss: 0.79399482867297\n",
      "Epoch 59, Training Loss: 0.7935609389753903\n",
      "Epoch 60, Training Loss: 0.7937118016972261\n",
      "Epoch 61, Training Loss: 0.7937086378125583\n",
      "Epoch 62, Training Loss: 0.7940959285287296\n",
      "Epoch 63, Training Loss: 0.7935220060629004\n",
      "Epoch 64, Training Loss: 0.7935862767696381\n",
      "Epoch 65, Training Loss: 0.7931794496143565\n",
      "Epoch 66, Training Loss: 0.7927946411862092\n",
      "Epoch 67, Training Loss: 0.7931412897390477\n",
      "Epoch 68, Training Loss: 0.7935473621592802\n",
      "Epoch 69, Training Loss: 0.79307828875149\n",
      "Epoch 70, Training Loss: 0.7927232940056744\n",
      "Epoch 71, Training Loss: 0.7932383552719565\n",
      "Epoch 72, Training Loss: 0.7930041578236748\n",
      "Epoch 73, Training Loss: 0.7928049933209139\n",
      "Epoch 74, Training Loss: 0.7932017271658953\n",
      "Epoch 75, Training Loss: 0.7926645001944373\n",
      "Epoch 76, Training Loss: 0.7923952278670142\n",
      "Epoch 77, Training Loss: 0.7926571540271534\n",
      "Epoch 78, Training Loss: 0.7925417048089645\n",
      "Epoch 79, Training Loss: 0.7926057302951812\n",
      "Epoch 80, Training Loss: 0.7922245471617755\n",
      "Epoch 81, Training Loss: 0.7925607859387117\n",
      "Epoch 82, Training Loss: 0.7925225582543541\n",
      "Epoch 83, Training Loss: 0.7925903811875512\n",
      "Epoch 84, Training Loss: 0.7921857363336227\n",
      "Epoch 85, Training Loss: 0.7925407463662765\n",
      "Epoch 86, Training Loss: 0.7924913545215831\n",
      "Epoch 87, Training Loss: 0.7923215086319867\n",
      "Epoch 88, Training Loss: 0.7924784091640921\n",
      "Epoch 89, Training Loss: 0.7923252588159898\n",
      "Epoch 90, Training Loss: 0.7918706882701201\n",
      "Epoch 91, Training Loss: 0.7922115576968474\n",
      "Epoch 92, Training Loss: 0.7919236178959117\n",
      "Epoch 93, Training Loss: 0.7916202522025388\n",
      "Epoch 94, Training Loss: 0.7922802810107961\n",
      "Epoch 95, Training Loss: 0.7923086546449101\n",
      "Epoch 96, Training Loss: 0.7919952180105098\n",
      "Epoch 97, Training Loss: 0.7922047803682439\n",
      "Epoch 98, Training Loss: 0.7917440864619086\n",
      "Epoch 99, Training Loss: 0.7919357308219461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:52:45,202] Trial 245 finished with value: 0.6382 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7919521430660696\n",
      "Epoch 1, Training Loss: 0.9989740658523445\n",
      "Epoch 2, Training Loss: 0.9424609806304588\n",
      "Epoch 3, Training Loss: 0.9304466597119668\n",
      "Epoch 4, Training Loss: 0.9237552195563352\n",
      "Epoch 5, Training Loss: 0.9178147619828245\n",
      "Epoch 6, Training Loss: 0.9130349443371134\n",
      "Epoch 7, Training Loss: 0.9086403151203816\n",
      "Epoch 8, Training Loss: 0.9043475804472328\n",
      "Epoch 9, Training Loss: 0.899632373788303\n",
      "Epoch 10, Training Loss: 0.8953018451095524\n",
      "Epoch 11, Training Loss: 0.8905100034591847\n",
      "Epoch 12, Training Loss: 0.884535939621746\n",
      "Epoch 13, Training Loss: 0.878605163993692\n",
      "Epoch 14, Training Loss: 0.8723478630073088\n",
      "Epoch 15, Training Loss: 0.866251780574483\n",
      "Epoch 16, Training Loss: 0.859182363882997\n",
      "Epoch 17, Training Loss: 0.8535097875093159\n",
      "Epoch 18, Training Loss: 0.847167164132111\n",
      "Epoch 19, Training Loss: 0.8416546848483552\n",
      "Epoch 20, Training Loss: 0.8365420365692081\n",
      "Epoch 21, Training Loss: 0.8323228243598364\n",
      "Epoch 22, Training Loss: 0.8284148368620334\n",
      "Epoch 23, Training Loss: 0.8247658576284136\n",
      "Epoch 24, Training Loss: 0.8221747186847199\n",
      "Epoch 25, Training Loss: 0.818764588214401\n",
      "Epoch 26, Training Loss: 0.8170021258798758\n",
      "Epoch 27, Training Loss: 0.815342716973527\n",
      "Epoch 28, Training Loss: 0.8137935520114755\n",
      "Epoch 29, Training Loss: 0.812204653069489\n",
      "Epoch 30, Training Loss: 0.8113927645790846\n",
      "Epoch 31, Training Loss: 0.80975780782843\n",
      "Epoch 32, Training Loss: 0.8089921668956154\n",
      "Epoch 33, Training Loss: 0.8087478921825725\n",
      "Epoch 34, Training Loss: 0.8074398263952786\n",
      "Epoch 35, Training Loss: 0.806554887348548\n",
      "Epoch 36, Training Loss: 0.8059786079073311\n",
      "Epoch 37, Training Loss: 0.8058861203659746\n",
      "Epoch 38, Training Loss: 0.8052766649346602\n",
      "Epoch 39, Training Loss: 0.8051212188892795\n",
      "Epoch 40, Training Loss: 0.804520126751491\n",
      "Epoch 41, Training Loss: 0.8038847421344958\n",
      "Epoch 42, Training Loss: 0.8043836208214438\n",
      "Epoch 43, Training Loss: 0.8032741283115588\n",
      "Epoch 44, Training Loss: 0.8030905278105485\n",
      "Epoch 45, Training Loss: 0.8027054895135693\n",
      "Epoch 46, Training Loss: 0.8021923456873212\n",
      "Epoch 47, Training Loss: 0.8026496561846339\n",
      "Epoch 48, Training Loss: 0.8023178446561771\n",
      "Epoch 49, Training Loss: 0.802100832659499\n",
      "Epoch 50, Training Loss: 0.8011794105060118\n",
      "Epoch 51, Training Loss: 0.801560183754541\n",
      "Epoch 52, Training Loss: 0.8014771194386303\n",
      "Epoch 53, Training Loss: 0.8011617548483655\n",
      "Epoch 54, Training Loss: 0.8010029096352427\n",
      "Epoch 55, Training Loss: 0.8004540948939503\n",
      "Epoch 56, Training Loss: 0.8012232188891647\n",
      "Epoch 57, Training Loss: 0.8008654843595692\n",
      "Epoch 58, Training Loss: 0.8012226779658095\n",
      "Epoch 59, Training Loss: 0.8003409463660162\n",
      "Epoch 60, Training Loss: 0.800434238301184\n",
      "Epoch 61, Training Loss: 0.800089815326203\n",
      "Epoch 62, Training Loss: 0.7999637801844375\n",
      "Epoch 63, Training Loss: 0.7992686546834787\n",
      "Epoch 64, Training Loss: 0.7996977168814581\n",
      "Epoch 65, Training Loss: 0.7989922428937782\n",
      "Epoch 66, Training Loss: 0.7993814826907968\n",
      "Epoch 67, Training Loss: 0.7989119168959166\n",
      "Epoch 68, Training Loss: 0.7989603560669978\n",
      "Epoch 69, Training Loss: 0.7990720790131648\n",
      "Epoch 70, Training Loss: 0.7992983247104444\n",
      "Epoch 71, Training Loss: 0.7992285741899247\n",
      "Epoch 72, Training Loss: 0.7980993029766513\n",
      "Epoch 73, Training Loss: 0.798294133485708\n",
      "Epoch 74, Training Loss: 0.7988376845094495\n",
      "Epoch 75, Training Loss: 0.7981049442201629\n",
      "Epoch 76, Training Loss: 0.7986214364381661\n",
      "Epoch 77, Training Loss: 0.7983908173733187\n",
      "Epoch 78, Training Loss: 0.7982393655561864\n",
      "Epoch 79, Training Loss: 0.7983657758935053\n",
      "Epoch 80, Training Loss: 0.7988221819239452\n",
      "Epoch 81, Training Loss: 0.7980846583395076\n",
      "Epoch 82, Training Loss: 0.7981951637375624\n",
      "Epoch 83, Training Loss: 0.7979179537385926\n",
      "Epoch 84, Training Loss: 0.7981379667619117\n",
      "Epoch 85, Training Loss: 0.7979433848445577\n",
      "Epoch 86, Training Loss: 0.7979122126012816\n",
      "Epoch 87, Training Loss: 0.7978303502376815\n",
      "Epoch 88, Training Loss: 0.7977885118104462\n",
      "Epoch 89, Training Loss: 0.7974290650590021\n",
      "Epoch 90, Training Loss: 0.7980076788959647\n",
      "Epoch 91, Training Loss: 0.7975740646957454\n",
      "Epoch 92, Training Loss: 0.7977061315586692\n",
      "Epoch 93, Training Loss: 0.797597488872987\n",
      "Epoch 94, Training Loss: 0.7971907746522946\n",
      "Epoch 95, Training Loss: 0.7974429963226605\n",
      "Epoch 96, Training Loss: 0.7970312142730656\n",
      "Epoch 97, Training Loss: 0.7971445844585734\n",
      "Epoch 98, Training Loss: 0.7971865178947162\n",
      "Epoch 99, Training Loss: 0.7965962924455342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:54:07,487] Trial 246 finished with value: 0.6354 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7966887862162483\n",
      "Epoch 1, Training Loss: 0.9939663898675961\n",
      "Epoch 2, Training Loss: 0.9542179909863866\n",
      "Epoch 3, Training Loss: 0.9458659372831646\n",
      "Epoch 4, Training Loss: 0.9393650625881396\n",
      "Epoch 5, Training Loss: 0.9325601267635374\n",
      "Epoch 6, Training Loss: 0.9252893720354353\n",
      "Epoch 7, Training Loss: 0.9185770571679998\n",
      "Epoch 8, Training Loss: 0.9114943066037687\n",
      "Epoch 9, Training Loss: 0.9031936877652218\n",
      "Epoch 10, Training Loss: 0.895751967286705\n",
      "Epoch 11, Training Loss: 0.888135964619486\n",
      "Epoch 12, Training Loss: 0.8803506194200731\n",
      "Epoch 13, Training Loss: 0.8732494514687617\n",
      "Epoch 14, Training Loss: 0.8658264152985766\n",
      "Epoch 15, Training Loss: 0.8590015127246541\n",
      "Epoch 16, Training Loss: 0.8529443804482768\n",
      "Epoch 17, Training Loss: 0.8467408564753999\n",
      "Epoch 18, Training Loss: 0.8415551286890991\n",
      "Epoch 19, Training Loss: 0.8370258063302004\n",
      "Epoch 20, Training Loss: 0.8324145671120263\n",
      "Epoch 21, Training Loss: 0.8290909174689673\n",
      "Epoch 22, Training Loss: 0.82576570403307\n",
      "Epoch 23, Training Loss: 0.8230595158455067\n",
      "Epoch 24, Training Loss: 0.8200459702570636\n",
      "Epoch 25, Training Loss: 0.8181167779112221\n",
      "Epoch 26, Training Loss: 0.8169186115264893\n",
      "Epoch 27, Training Loss: 0.8148295901771775\n",
      "Epoch 28, Training Loss: 0.8134168230501333\n",
      "Epoch 29, Training Loss: 0.812353013242994\n",
      "Epoch 30, Training Loss: 0.8116797883707778\n",
      "Epoch 31, Training Loss: 0.8111594007427531\n",
      "Epoch 32, Training Loss: 0.8099430655178271\n",
      "Epoch 33, Training Loss: 0.8099184435113032\n",
      "Epoch 34, Training Loss: 0.8089872786873266\n",
      "Epoch 35, Training Loss: 0.8086779171362856\n",
      "Epoch 36, Training Loss: 0.8079966668795823\n",
      "Epoch 37, Training Loss: 0.8074996998435573\n",
      "Epoch 38, Training Loss: 0.80783905794746\n",
      "Epoch 39, Training Loss: 0.8069101094303275\n",
      "Epoch 40, Training Loss: 0.8074751155717033\n",
      "Epoch 41, Training Loss: 0.8070985189954141\n",
      "Epoch 42, Training Loss: 0.8068831356844508\n",
      "Epoch 43, Training Loss: 0.8062805439296522\n",
      "Epoch 44, Training Loss: 0.8063676017567627\n",
      "Epoch 45, Training Loss: 0.8063806227275303\n",
      "Epoch 46, Training Loss: 0.805739422669088\n",
      "Epoch 47, Training Loss: 0.8057419295597793\n",
      "Epoch 48, Training Loss: 0.8056421583756468\n",
      "Epoch 49, Training Loss: 0.8056437291597065\n",
      "Epoch 50, Training Loss: 0.805305924989227\n",
      "Epoch 51, Training Loss: 0.8047780128349935\n",
      "Epoch 52, Training Loss: 0.804986770260603\n",
      "Epoch 53, Training Loss: 0.8048549824191216\n",
      "Epoch 54, Training Loss: 0.8049872718359294\n",
      "Epoch 55, Training Loss: 0.8047717511205745\n",
      "Epoch 56, Training Loss: 0.8042049505656823\n",
      "Epoch 57, Training Loss: 0.8041388173748676\n",
      "Epoch 58, Training Loss: 0.8051147859795649\n",
      "Epoch 59, Training Loss: 0.8036469624454814\n",
      "Epoch 60, Training Loss: 0.8050929689765873\n",
      "Epoch 61, Training Loss: 0.8039917806933697\n",
      "Epoch 62, Training Loss: 0.8038505110525547\n",
      "Epoch 63, Training Loss: 0.8032746602718095\n",
      "Epoch 64, Training Loss: 0.8030085996577614\n",
      "Epoch 65, Training Loss: 0.8035063369829852\n",
      "Epoch 66, Training Loss: 0.8031808422024088\n",
      "Epoch 67, Training Loss: 0.802705378819229\n",
      "Epoch 68, Training Loss: 0.8029730630100221\n",
      "Epoch 69, Training Loss: 0.8032108547992276\n",
      "Epoch 70, Training Loss: 0.8030875586925592\n",
      "Epoch 71, Training Loss: 0.8027055077983024\n",
      "Epoch 72, Training Loss: 0.8030718676129678\n",
      "Epoch 73, Training Loss: 0.8024888314698871\n",
      "Epoch 74, Training Loss: 0.8026691044183601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:55:10,121] Trial 247 finished with value: 0.6342666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8026041005787097\n",
      "Epoch 1, Training Loss: 0.9027347417702353\n",
      "Epoch 2, Training Loss: 0.8182842925078887\n",
      "Epoch 3, Training Loss: 0.8055590958523571\n",
      "Epoch 4, Training Loss: 0.8026528705331616\n",
      "Epoch 5, Training Loss: 0.8002302574035817\n",
      "Epoch 6, Training Loss: 0.7986418870158661\n",
      "Epoch 7, Training Loss: 0.7962357020019588\n",
      "Epoch 8, Training Loss: 0.7959258922060629\n",
      "Epoch 9, Training Loss: 0.7943612040433669\n",
      "Epoch 10, Training Loss: 0.7935898498484962\n",
      "Epoch 11, Training Loss: 0.7918703136587502\n",
      "Epoch 12, Training Loss: 0.7915602118449103\n",
      "Epoch 13, Training Loss: 0.7904037313353747\n",
      "Epoch 14, Training Loss: 0.7892386057323083\n",
      "Epoch 15, Training Loss: 0.7880042446287054\n",
      "Epoch 16, Training Loss: 0.7877351058156867\n",
      "Epoch 17, Training Loss: 0.7873454372685654\n",
      "Epoch 18, Training Loss: 0.7881055177602553\n",
      "Epoch 19, Training Loss: 0.7872556816366382\n",
      "Epoch 20, Training Loss: 0.7877338230161739\n",
      "Epoch 21, Training Loss: 0.7862036303470009\n",
      "Epoch 22, Training Loss: 0.7863372881609695\n",
      "Epoch 23, Training Loss: 0.7849188609230787\n",
      "Epoch 24, Training Loss: 0.7847490297224289\n",
      "Epoch 25, Training Loss: 0.7846566310502533\n",
      "Epoch 26, Training Loss: 0.784679064714819\n",
      "Epoch 27, Training Loss: 0.7841699180746438\n",
      "Epoch 28, Training Loss: 0.7843578315318975\n",
      "Epoch 29, Training Loss: 0.7830955223033302\n",
      "Epoch 30, Training Loss: 0.7826112098263619\n",
      "Epoch 31, Training Loss: 0.7832002160244418\n",
      "Epoch 32, Training Loss: 0.783198879177409\n",
      "Epoch 33, Training Loss: 0.7830998284476144\n",
      "Epoch 34, Training Loss: 0.7833324100738182\n",
      "Epoch 35, Training Loss: 0.7824869640787742\n",
      "Epoch 36, Training Loss: 0.7830559910688185\n",
      "Epoch 37, Training Loss: 0.7824385776555628\n",
      "Epoch 38, Training Loss: 0.7815766478391518\n",
      "Epoch 39, Training Loss: 0.7814927005230036\n",
      "Epoch 40, Training Loss: 0.7817751715057775\n",
      "Epoch 41, Training Loss: 0.781366825641546\n",
      "Epoch 42, Training Loss: 0.7808738926299532\n",
      "Epoch 43, Training Loss: 0.7805135683009499\n",
      "Epoch 44, Training Loss: 0.7810183219443586\n",
      "Epoch 45, Training Loss: 0.7803218924909606\n",
      "Epoch 46, Training Loss: 0.7803904362190935\n",
      "Epoch 47, Training Loss: 0.7811835991708855\n",
      "Epoch 48, Training Loss: 0.7799670761689208\n",
      "Epoch 49, Training Loss: 0.7812569511564155\n",
      "Epoch 50, Training Loss: 0.7807558458550532\n",
      "Epoch 51, Training Loss: 0.7791697825704302\n",
      "Epoch 52, Training Loss: 0.7801844081484286\n",
      "Epoch 53, Training Loss: 0.7797342457269367\n",
      "Epoch 54, Training Loss: 0.7797801229290496\n",
      "Epoch 55, Training Loss: 0.7789931040509303\n",
      "Epoch 56, Training Loss: 0.7799623035846797\n",
      "Epoch 57, Training Loss: 0.7786634438019946\n",
      "Epoch 58, Training Loss: 0.7792051434516907\n",
      "Epoch 59, Training Loss: 0.778823438040296\n",
      "Epoch 60, Training Loss: 0.778976907945217\n",
      "Epoch 61, Training Loss: 0.779244988125966\n",
      "Epoch 62, Training Loss: 0.7786077555857207\n",
      "Epoch 63, Training Loss: 0.7779989622141186\n",
      "Epoch 64, Training Loss: 0.7777260593005589\n",
      "Epoch 65, Training Loss: 0.7787589401230777\n",
      "Epoch 66, Training Loss: 0.7785634700517009\n",
      "Epoch 67, Training Loss: 0.7788412205258707\n",
      "Epoch 68, Training Loss: 0.778223823425465\n",
      "Epoch 69, Training Loss: 0.7783126149858747\n",
      "Epoch 70, Training Loss: 0.7782474184394779\n",
      "Epoch 71, Training Loss: 0.7777759850473333\n",
      "Epoch 72, Training Loss: 0.777743903497108\n",
      "Epoch 73, Training Loss: 0.7774197932472803\n",
      "Epoch 74, Training Loss: 0.7778737216963804\n",
      "Epoch 75, Training Loss: 0.7778436356021049\n",
      "Epoch 76, Training Loss: 0.7778472218298375\n",
      "Epoch 77, Training Loss: 0.7776324001470006\n",
      "Epoch 78, Training Loss: 0.7772463884568752\n",
      "Epoch 79, Training Loss: 0.7771684717414971\n",
      "Epoch 80, Training Loss: 0.7772642938714278\n",
      "Epoch 81, Training Loss: 0.7777724782327065\n",
      "Epoch 82, Training Loss: 0.7768692958623843\n",
      "Epoch 83, Training Loss: 0.7765859957476308\n",
      "Epoch 84, Training Loss: 0.7772256780387764\n",
      "Epoch 85, Training Loss: 0.7770378902442473\n",
      "Epoch 86, Training Loss: 0.7763956943848975\n",
      "Epoch 87, Training Loss: 0.7776790592903481\n",
      "Epoch 88, Training Loss: 0.776973719794051\n",
      "Epoch 89, Training Loss: 0.7765682186399188\n",
      "Epoch 90, Training Loss: 0.7766600148122114\n",
      "Epoch 91, Training Loss: 0.7769816655861704\n",
      "Epoch 92, Training Loss: 0.7768083470208305\n",
      "Epoch 93, Training Loss: 0.7763197330603923\n",
      "Epoch 94, Training Loss: 0.7758892963703413\n",
      "Epoch 95, Training Loss: 0.7769904118731505\n",
      "Epoch 96, Training Loss: 0.7764051384495613\n",
      "Epoch 97, Training Loss: 0.7764149673899313\n",
      "Epoch 98, Training Loss: 0.776653290243077\n",
      "Epoch 99, Training Loss: 0.7769006998915421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:57:08,640] Trial 248 finished with value: 0.6423333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7760034396236104\n",
      "Epoch 1, Training Loss: 0.8449697427283552\n",
      "Epoch 2, Training Loss: 0.821686019305896\n",
      "Epoch 3, Training Loss: 0.8123616024963838\n",
      "Epoch 4, Training Loss: 0.8118307620959174\n",
      "Epoch 5, Training Loss: 0.8091682800673005\n",
      "Epoch 6, Training Loss: 0.813200296943349\n",
      "Epoch 7, Training Loss: 0.8109279025766186\n",
      "Epoch 8, Training Loss: 0.8084462786079349\n",
      "Epoch 9, Training Loss: 0.8089510510738631\n",
      "Epoch 10, Training Loss: 0.8105204004094116\n",
      "Epoch 11, Training Loss: 0.8066178668262367\n",
      "Epoch 12, Training Loss: 0.8065620994209347\n",
      "Epoch 13, Training Loss: 0.8135269814864137\n",
      "Epoch 14, Training Loss: 0.8087291460288198\n",
      "Epoch 15, Training Loss: 0.8070305716722531\n",
      "Epoch 16, Training Loss: 0.8080906867980957\n",
      "Epoch 17, Training Loss: 0.81068022152535\n",
      "Epoch 18, Training Loss: 0.8074225728673147\n",
      "Epoch 19, Training Loss: 0.8083478197119289\n",
      "Epoch 20, Training Loss: 0.8062832907626504\n",
      "Epoch 21, Training Loss: 0.8140762158802577\n",
      "Epoch 22, Training Loss: 0.8053397342674714\n",
      "Epoch 23, Training Loss: 0.8101859758671065\n",
      "Epoch 24, Training Loss: 0.8044618633456696\n",
      "Epoch 25, Training Loss: 0.8057801247539377\n",
      "Epoch 26, Training Loss: 0.803701577061101\n",
      "Epoch 27, Training Loss: 0.8074777548474477\n",
      "Epoch 28, Training Loss: 0.8084761769251716\n",
      "Epoch 29, Training Loss: 0.8078955471963811\n",
      "Epoch 30, Training Loss: 0.8150864792049379\n",
      "Epoch 31, Training Loss: 0.8043345335730933\n",
      "Epoch 32, Training Loss: 0.803762844272126\n",
      "Epoch 33, Training Loss: 0.8108074832679634\n",
      "Epoch 34, Training Loss: 0.8081667946693593\n",
      "Epoch 35, Training Loss: 0.8097558195429637\n",
      "Epoch 36, Training Loss: 0.8075378254840249\n",
      "Epoch 37, Training Loss: 0.8064851420266288\n",
      "Epoch 38, Training Loss: 0.8061430646960897\n",
      "Epoch 39, Training Loss: 0.8053473906409472\n",
      "Epoch 40, Training Loss: 0.808985876409631\n",
      "Epoch 41, Training Loss: 0.8081077140076716\n",
      "Epoch 42, Training Loss: 0.8091045086545156\n",
      "Epoch 43, Training Loss: 0.8063024449169187\n",
      "Epoch 44, Training Loss: 0.8066807893882121\n",
      "Epoch 45, Training Loss: 0.8076377916156797\n",
      "Epoch 46, Training Loss: 0.8054173796248615\n",
      "Epoch 47, Training Loss: 0.8086857563570926\n",
      "Epoch 48, Training Loss: 0.8110236789947166\n",
      "Epoch 49, Training Loss: 0.8040569205929462\n",
      "Epoch 50, Training Loss: 0.8075587985210849\n",
      "Epoch 51, Training Loss: 0.8093004028599962\n",
      "Epoch 52, Training Loss: 0.8069136861571692\n",
      "Epoch 53, Training Loss: 0.8081554163667493\n",
      "Epoch 54, Training Loss: 0.8054640264439403\n",
      "Epoch 55, Training Loss: 0.8052158929351577\n",
      "Epoch 56, Training Loss: 0.8091665774359739\n",
      "Epoch 57, Training Loss: 0.8094186669901797\n",
      "Epoch 58, Training Loss: 0.806648379967625\n",
      "Epoch 59, Training Loss: 0.8109810915208401\n",
      "Epoch 60, Training Loss: 0.8080727382261951\n",
      "Epoch 61, Training Loss: 0.8085752346461876\n",
      "Epoch 62, Training Loss: 0.805996633024144\n",
      "Epoch 63, Training Loss: 0.8095247556392412\n",
      "Epoch 64, Training Loss: 0.8129471856400483\n",
      "Epoch 65, Training Loss: 0.8094366482325963\n",
      "Epoch 66, Training Loss: 0.8064804152438515\n",
      "Epoch 67, Training Loss: 0.8072106737839548\n",
      "Epoch 68, Training Loss: 0.8085654311610344\n",
      "Epoch 69, Training Loss: 0.8071546388747997\n",
      "Epoch 70, Training Loss: 0.8058910099187292\n",
      "Epoch 71, Training Loss: 0.8080205509537145\n",
      "Epoch 72, Training Loss: 0.8064817144906611\n",
      "Epoch 73, Training Loss: 0.8078211794222208\n",
      "Epoch 74, Training Loss: 0.8086804228617732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 15:58:37,613] Trial 249 finished with value: 0.6150666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8052280034337725\n",
      "Epoch 1, Training Loss: 0.8745472650778922\n",
      "Epoch 2, Training Loss: 0.8087278567758718\n",
      "Epoch 3, Training Loss: 0.8039477921070013\n",
      "Epoch 4, Training Loss: 0.8020897348124282\n",
      "Epoch 5, Training Loss: 0.8002617248018882\n",
      "Epoch 6, Training Loss: 0.7987975180597233\n",
      "Epoch 7, Training Loss: 0.7965422097005342\n",
      "Epoch 8, Training Loss: 0.7958899354576168\n",
      "Epoch 9, Training Loss: 0.7933153336209462\n",
      "Epoch 10, Training Loss: 0.793508670025302\n",
      "Epoch 11, Training Loss: 0.7914347639657501\n",
      "Epoch 12, Training Loss: 0.7896993825310155\n",
      "Epoch 13, Training Loss: 0.7899558355933741\n",
      "Epoch 14, Training Loss: 0.7894585811105886\n",
      "Epoch 15, Training Loss: 0.7887056965576975\n",
      "Epoch 16, Training Loss: 0.7884001890519508\n",
      "Epoch 17, Training Loss: 0.7878740646785363\n",
      "Epoch 18, Training Loss: 0.7868594576541642\n",
      "Epoch 19, Training Loss: 0.7868851391892684\n",
      "Epoch 20, Training Loss: 0.7856352166125649\n",
      "Epoch 21, Training Loss: 0.7850277066230774\n",
      "Epoch 22, Training Loss: 0.7850746688986183\n",
      "Epoch 23, Training Loss: 0.7851797653320141\n",
      "Epoch 24, Training Loss: 0.7847620448671786\n",
      "Epoch 25, Training Loss: 0.7843628996296933\n",
      "Epoch 26, Training Loss: 0.7843925033296858\n",
      "Epoch 27, Training Loss: 0.7838691677365984\n",
      "Epoch 28, Training Loss: 0.7834405625673165\n",
      "Epoch 29, Training Loss: 0.7841446889970536\n",
      "Epoch 30, Training Loss: 0.7834371789057452\n",
      "Epoch 31, Training Loss: 0.7835099489169013\n",
      "Epoch 32, Training Loss: 0.7824280439014721\n",
      "Epoch 33, Training Loss: 0.7824408463069371\n",
      "Epoch 34, Training Loss: 0.7825168706420669\n",
      "Epoch 35, Training Loss: 0.7841952942367784\n",
      "Epoch 36, Training Loss: 0.7837263310762276\n",
      "Epoch 37, Training Loss: 0.7826122927486449\n",
      "Epoch 38, Training Loss: 0.7823498117296319\n",
      "Epoch 39, Training Loss: 0.7816763427920808\n",
      "Epoch 40, Training Loss: 0.7816948877241379\n",
      "Epoch 41, Training Loss: 0.781410929493438\n",
      "Epoch 42, Training Loss: 0.781156259102929\n",
      "Epoch 43, Training Loss: 0.7814841264172604\n",
      "Epoch 44, Training Loss: 0.7811247076307024\n",
      "Epoch 45, Training Loss: 0.7817871039075063\n",
      "Epoch 46, Training Loss: 0.780617728717345\n",
      "Epoch 47, Training Loss: 0.7812646583506936\n",
      "Epoch 48, Training Loss: 0.7811059485700794\n",
      "Epoch 49, Training Loss: 0.7804019814147088\n",
      "Epoch 50, Training Loss: 0.7806241160944889\n",
      "Epoch 51, Training Loss: 0.7802318044174883\n",
      "Epoch 52, Training Loss: 0.780190926835053\n",
      "Epoch 53, Training Loss: 0.7796217378817106\n",
      "Epoch 54, Training Loss: 0.7795205877239543\n",
      "Epoch 55, Training Loss: 0.7807515272520539\n",
      "Epoch 56, Training Loss: 0.7801273151447898\n",
      "Epoch 57, Training Loss: 0.7789209413349181\n",
      "Epoch 58, Training Loss: 0.7795880385807582\n",
      "Epoch 59, Training Loss: 0.779323409015971\n",
      "Epoch 60, Training Loss: 0.779685474069495\n",
      "Epoch 61, Training Loss: 0.7796217849380092\n",
      "Epoch 62, Training Loss: 0.7790084926705612\n",
      "Epoch 63, Training Loss: 0.7795962681447653\n",
      "Epoch 64, Training Loss: 0.7788609934032411\n",
      "Epoch 65, Training Loss: 0.7786151902119917\n",
      "Epoch 66, Training Loss: 0.7784464508967293\n",
      "Epoch 67, Training Loss: 0.7798043271652738\n",
      "Epoch 68, Training Loss: 0.778964056287493\n",
      "Epoch 69, Training Loss: 0.7782108358870772\n",
      "Epoch 70, Training Loss: 0.7788224330522064\n",
      "Epoch 71, Training Loss: 0.7784297202762804\n",
      "Epoch 72, Training Loss: 0.7787171392512501\n",
      "Epoch 73, Training Loss: 0.7782787676144364\n",
      "Epoch 74, Training Loss: 0.7775382176825875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:00:05,775] Trial 250 finished with value: 0.6402666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7782536771960724\n",
      "Epoch 1, Training Loss: 0.9286171758981575\n",
      "Epoch 2, Training Loss: 0.8678896999000607\n",
      "Epoch 3, Training Loss: 0.8288080603556526\n",
      "Epoch 4, Training Loss: 0.8124441329697917\n",
      "Epoch 5, Training Loss: 0.8058052655449487\n",
      "Epoch 6, Training Loss: 0.8028914195254333\n",
      "Epoch 7, Training Loss: 0.8016456168397028\n",
      "Epoch 8, Training Loss: 0.8015997332737859\n",
      "Epoch 9, Training Loss: 0.8001571343357402\n",
      "Epoch 10, Training Loss: 0.8003028497660071\n",
      "Epoch 11, Training Loss: 0.7987448065800774\n",
      "Epoch 12, Training Loss: 0.7986018310812183\n",
      "Epoch 13, Training Loss: 0.7986277461948251\n",
      "Epoch 14, Training Loss: 0.7989431104265657\n",
      "Epoch 15, Training Loss: 0.7987766818892686\n",
      "Epoch 16, Training Loss: 0.7982738175786528\n",
      "Epoch 17, Training Loss: 0.7976785569262684\n",
      "Epoch 18, Training Loss: 0.7975585518923021\n",
      "Epoch 19, Training Loss: 0.7972784012780154\n",
      "Epoch 20, Training Loss: 0.7976504350067082\n",
      "Epoch 21, Training Loss: 0.7982164517381137\n",
      "Epoch 22, Training Loss: 0.7970942117217789\n",
      "Epoch 23, Training Loss: 0.7972522279373685\n",
      "Epoch 24, Training Loss: 0.7967000230810696\n",
      "Epoch 25, Training Loss: 0.7966208344115351\n",
      "Epoch 26, Training Loss: 0.7964281452329536\n",
      "Epoch 27, Training Loss: 0.7967991060780403\n",
      "Epoch 28, Training Loss: 0.796918147040489\n",
      "Epoch 29, Training Loss: 0.7963008626063067\n",
      "Epoch 30, Training Loss: 0.7961683457059071\n",
      "Epoch 31, Training Loss: 0.7959697982422391\n",
      "Epoch 32, Training Loss: 0.7964144046145274\n",
      "Epoch 33, Training Loss: 0.7961787557243405\n",
      "Epoch 34, Training Loss: 0.7957053367356609\n",
      "Epoch 35, Training Loss: 0.7956524959184174\n",
      "Epoch 36, Training Loss: 0.7959894059295941\n",
      "Epoch 37, Training Loss: 0.7949977273331549\n",
      "Epoch 38, Training Loss: 0.7952296794805311\n",
      "Epoch 39, Training Loss: 0.7949371318171795\n",
      "Epoch 40, Training Loss: 0.794201233691739\n",
      "Epoch 41, Training Loss: 0.7948115991470509\n",
      "Epoch 42, Training Loss: 0.7943254908224694\n",
      "Epoch 43, Training Loss: 0.7942848859873033\n",
      "Epoch 44, Training Loss: 0.793518038262102\n",
      "Epoch 45, Training Loss: 0.7927056359169179\n",
      "Epoch 46, Training Loss: 0.7928392294654273\n",
      "Epoch 47, Training Loss: 0.7924447553498405\n",
      "Epoch 48, Training Loss: 0.7923839388037086\n",
      "Epoch 49, Training Loss: 0.7917670691820016\n",
      "Epoch 50, Training Loss: 0.7918200050977836\n",
      "Epoch 51, Training Loss: 0.7913205251657873\n",
      "Epoch 52, Training Loss: 0.7911258414275664\n",
      "Epoch 53, Training Loss: 0.7915044994282543\n",
      "Epoch 54, Training Loss: 0.790962454967929\n",
      "Epoch 55, Training Loss: 0.7905212795824037\n",
      "Epoch 56, Training Loss: 0.790370007654778\n",
      "Epoch 57, Training Loss: 0.7899376413875953\n",
      "Epoch 58, Training Loss: 0.7917603216673198\n",
      "Epoch 59, Training Loss: 0.7898443404445075\n",
      "Epoch 60, Training Loss: 0.7898234281324803\n",
      "Epoch 61, Training Loss: 0.7904448461711855\n",
      "Epoch 62, Training Loss: 0.7899497009757767\n",
      "Epoch 63, Training Loss: 0.7892682249832871\n",
      "Epoch 64, Training Loss: 0.7904419647123581\n",
      "Epoch 65, Training Loss: 0.7893207963247945\n",
      "Epoch 66, Training Loss: 0.790154080283373\n",
      "Epoch 67, Training Loss: 0.7895722773738374\n",
      "Epoch 68, Training Loss: 0.7893797195943675\n",
      "Epoch 69, Training Loss: 0.7888371849418583\n",
      "Epoch 70, Training Loss: 0.789290948559467\n",
      "Epoch 71, Training Loss: 0.7893702915736607\n",
      "Epoch 72, Training Loss: 0.7892063416932759\n",
      "Epoch 73, Training Loss: 0.789193904579134\n",
      "Epoch 74, Training Loss: 0.7885990118173728\n",
      "Epoch 75, Training Loss: 0.788501965551448\n",
      "Epoch 76, Training Loss: 0.7889135936149081\n",
      "Epoch 77, Training Loss: 0.7890762178521408\n",
      "Epoch 78, Training Loss: 0.7893053510135277\n",
      "Epoch 79, Training Loss: 0.7895755040914493\n",
      "Epoch 80, Training Loss: 0.788467488163396\n",
      "Epoch 81, Training Loss: 0.7881295668451409\n",
      "Epoch 82, Training Loss: 0.7880757235942927\n",
      "Epoch 83, Training Loss: 0.7890058094397523\n",
      "Epoch 84, Training Loss: 0.7887325905319443\n",
      "Epoch 85, Training Loss: 0.7881450282003647\n",
      "Epoch 86, Training Loss: 0.78816210425886\n",
      "Epoch 87, Training Loss: 0.7878586973462786\n",
      "Epoch 88, Training Loss: 0.7880181260575029\n",
      "Epoch 89, Training Loss: 0.7875098160334996\n",
      "Epoch 90, Training Loss: 0.787908693274161\n",
      "Epoch 91, Training Loss: 0.7873134985005945\n",
      "Epoch 92, Training Loss: 0.7876893506910568\n",
      "Epoch 93, Training Loss: 0.7871090145487535\n",
      "Epoch 94, Training Loss: 0.7872631509501234\n",
      "Epoch 95, Training Loss: 0.7876708253881985\n",
      "Epoch 96, Training Loss: 0.7871646811639456\n",
      "Epoch 97, Training Loss: 0.78764149569031\n",
      "Epoch 98, Training Loss: 0.7871958646559177\n",
      "Epoch 99, Training Loss: 0.7881845921502078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:01:43,048] Trial 251 finished with value: 0.6406666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.787291641791064\n",
      "Epoch 1, Training Loss: 1.0789950642866246\n",
      "Epoch 2, Training Loss: 1.0403453705591315\n",
      "Epoch 3, Training Loss: 1.014627946545096\n",
      "Epoch 4, Training Loss: 0.9956820816152236\n",
      "Epoch 5, Training Loss: 0.9818865553070517\n",
      "Epoch 6, Training Loss: 0.9721976463934955\n",
      "Epoch 7, Training Loss: 0.965575698333628\n",
      "Epoch 8, Training Loss: 0.9610700639556436\n",
      "Epoch 9, Training Loss: 0.9579423339226667\n",
      "Epoch 10, Training Loss: 0.9556523539739497\n",
      "Epoch 11, Training Loss: 0.9538774356421302\n",
      "Epoch 12, Training Loss: 0.9523988243411569\n",
      "Epoch 13, Training Loss: 0.951087845844381\n",
      "Epoch 14, Training Loss: 0.9498630119772519\n",
      "Epoch 15, Training Loss: 0.9486886485183941\n",
      "Epoch 16, Training Loss: 0.9475411525894614\n",
      "Epoch 17, Training Loss: 0.9463964929300196\n",
      "Epoch 18, Training Loss: 0.9452507761646719\n",
      "Epoch 19, Training Loss: 0.9440927558085498\n",
      "Epoch 20, Training Loss: 0.9429204874178942\n",
      "Epoch 21, Training Loss: 0.9417375291796292\n",
      "Epoch 22, Training Loss: 0.9405365894822514\n",
      "Epoch 23, Training Loss: 0.9393103289604187\n",
      "Epoch 24, Training Loss: 0.9380605591044706\n",
      "Epoch 25, Training Loss: 0.9367760812535005\n",
      "Epoch 26, Training Loss: 0.9354831030789543\n",
      "Epoch 27, Training Loss: 0.9341501588681165\n",
      "Epoch 28, Training Loss: 0.9327945870511672\n",
      "Epoch 29, Training Loss: 0.9314002096652985\n",
      "Epoch 30, Training Loss: 0.9299731358359842\n",
      "Epoch 31, Training Loss: 0.9285029646228341\n",
      "Epoch 32, Training Loss: 0.9269882144647487\n",
      "Epoch 33, Training Loss: 0.9254633984144996\n",
      "Epoch 34, Training Loss: 0.9238738404302036\n",
      "Epoch 35, Training Loss: 0.9222455003682305\n",
      "Epoch 36, Training Loss: 0.9205775941119475\n",
      "Epoch 37, Training Loss: 0.9188610161052031\n",
      "Epoch 38, Training Loss: 0.917104072079939\n",
      "Epoch 39, Training Loss: 0.9153022859377019\n",
      "Epoch 40, Training Loss: 0.913453477340586\n",
      "Epoch 41, Training Loss: 0.9115541406940011\n",
      "Epoch 42, Training Loss: 0.9096228991536534\n",
      "Epoch 43, Training Loss: 0.907628728501937\n",
      "Epoch 44, Training Loss: 0.9055913851541632\n",
      "Epoch 45, Training Loss: 0.9035220178435831\n",
      "Epoch 46, Training Loss: 0.9013570995190564\n",
      "Epoch 47, Training Loss: 0.8992504229966332\n",
      "Epoch 48, Training Loss: 0.897059502251008\n",
      "Epoch 49, Training Loss: 0.8948461555032169\n",
      "Epoch 50, Training Loss: 0.8925714677221634\n",
      "Epoch 51, Training Loss: 0.8902820956005769\n",
      "Epoch 52, Training Loss: 0.8879936962267931\n",
      "Epoch 53, Training Loss: 0.8856736367590288\n",
      "Epoch 54, Training Loss: 0.8833477134564344\n",
      "Epoch 55, Training Loss: 0.8810090055185206\n",
      "Epoch 56, Training Loss: 0.8786490276981802\n",
      "Epoch 57, Training Loss: 0.8763168014498318\n",
      "Epoch 58, Training Loss: 0.8740076562937569\n",
      "Epoch 59, Training Loss: 0.8716748124711654\n",
      "Epoch 60, Training Loss: 0.8693985983904671\n",
      "Epoch 61, Training Loss: 0.8671256367599263\n",
      "Epoch 62, Training Loss: 0.8648945149954628\n",
      "Epoch 63, Training Loss: 0.8627001433512744\n",
      "Epoch 64, Training Loss: 0.8605163478851319\n",
      "Epoch 65, Training Loss: 0.8584112008178936\n",
      "Epoch 66, Training Loss: 0.8563999704753651\n",
      "Epoch 67, Training Loss: 0.8543722154112423\n",
      "Epoch 68, Training Loss: 0.8524195349917693\n",
      "Epoch 69, Training Loss: 0.8505414544834811\n",
      "Epoch 70, Training Loss: 0.8486983137270984\n",
      "Epoch 71, Training Loss: 0.8469558562250699\n",
      "Epoch 72, Training Loss: 0.845236974884482\n",
      "Epoch 73, Training Loss: 0.843632957163979\n",
      "Epoch 74, Training Loss: 0.842056649572709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:03:04,744] Trial 252 finished with value: 0.6098 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8405444964240579\n",
      "Epoch 1, Training Loss: 0.8731551276235019\n",
      "Epoch 2, Training Loss: 0.8189029687292435\n",
      "Epoch 3, Training Loss: 0.8132266632248374\n",
      "Epoch 4, Training Loss: 0.8118304047865026\n",
      "Epoch 5, Training Loss: 0.8102610344045302\n",
      "Epoch 6, Training Loss: 0.8088019552651574\n",
      "Epoch 7, Training Loss: 0.8089244177060969\n",
      "Epoch 8, Training Loss: 0.8066701086829691\n",
      "Epoch 9, Training Loss: 0.8073958477553199\n",
      "Epoch 10, Training Loss: 0.8069787856410532\n",
      "Epoch 11, Training Loss: 0.8062403413828682\n",
      "Epoch 12, Training Loss: 0.8053402809535756\n",
      "Epoch 13, Training Loss: 0.8047530089406406\n",
      "Epoch 14, Training Loss: 0.8050044837418725\n",
      "Epoch 15, Training Loss: 0.8051146775834701\n",
      "Epoch 16, Training Loss: 0.8049845673757441\n",
      "Epoch 17, Training Loss: 0.80433710287599\n",
      "Epoch 18, Training Loss: 0.8044779756489921\n",
      "Epoch 19, Training Loss: 0.8041351772757137\n",
      "Epoch 20, Training Loss: 0.803920519842821\n",
      "Epoch 21, Training Loss: 0.802372865747003\n",
      "Epoch 22, Training Loss: 0.8030661330503576\n",
      "Epoch 23, Training Loss: 0.801859948424732\n",
      "Epoch 24, Training Loss: 0.8012113671443042\n",
      "Epoch 25, Training Loss: 0.8008467916881337\n",
      "Epoch 26, Training Loss: 0.8016712894159205\n",
      "Epoch 27, Training Loss: 0.8013234860055587\n",
      "Epoch 28, Training Loss: 0.8003907909112818\n",
      "Epoch 29, Training Loss: 0.8006695677252377\n",
      "Epoch 30, Training Loss: 0.7998106768551995\n",
      "Epoch 31, Training Loss: 0.8006899346323574\n",
      "Epoch 32, Training Loss: 0.7996088188536027\n",
      "Epoch 33, Training Loss: 0.7993521632166469\n",
      "Epoch 34, Training Loss: 0.7998287753497852\n",
      "Epoch 35, Training Loss: 0.79894747467602\n",
      "Epoch 36, Training Loss: 0.7991462328854729\n",
      "Epoch 37, Training Loss: 0.7990197909579557\n",
      "Epoch 38, Training Loss: 0.7983497473071604\n",
      "Epoch 39, Training Loss: 0.797610726777245\n",
      "Epoch 40, Training Loss: 0.797474166505477\n",
      "Epoch 41, Training Loss: 0.7966523197819205\n",
      "Epoch 42, Training Loss: 0.7965550086077522\n",
      "Epoch 43, Training Loss: 0.7967580345798941\n",
      "Epoch 44, Training Loss: 0.7961537774871378\n",
      "Epoch 45, Training Loss: 0.7959865619855768\n",
      "Epoch 46, Training Loss: 0.7962064098610597\n",
      "Epoch 47, Training Loss: 0.7952932408276726\n",
      "Epoch 48, Training Loss: 0.7953865150844349\n",
      "Epoch 49, Training Loss: 0.7942809657489552\n",
      "Epoch 50, Training Loss: 0.7949198582593132\n",
      "Epoch 51, Training Loss: 0.7942188417210299\n",
      "Epoch 52, Training Loss: 0.7942668264753678\n",
      "Epoch 53, Training Loss: 0.7942769331791821\n",
      "Epoch 54, Training Loss: 0.7945552346986883\n",
      "Epoch 55, Training Loss: 0.7947351554562064\n",
      "Epoch 56, Training Loss: 0.793676580401028\n",
      "Epoch 57, Training Loss: 0.79423133001608\n",
      "Epoch 58, Training Loss: 0.7938796612795661\n",
      "Epoch 59, Training Loss: 0.7937388845752267\n",
      "Epoch 60, Training Loss: 0.7938158704954035\n",
      "Epoch 61, Training Loss: 0.7936342373315026\n",
      "Epoch 62, Training Loss: 0.7938571700629066\n",
      "Epoch 63, Training Loss: 0.79362657568034\n",
      "Epoch 64, Training Loss: 0.7931894068858203\n",
      "Epoch 65, Training Loss: 0.7934429149066701\n",
      "Epoch 66, Training Loss: 0.7922136799728169\n",
      "Epoch 67, Training Loss: 0.7931173445897944\n",
      "Epoch 68, Training Loss: 0.7926740654777078\n",
      "Epoch 69, Training Loss: 0.7929519711522495\n",
      "Epoch 70, Training Loss: 0.7927278534103842\n",
      "Epoch 71, Training Loss: 0.792052557328168\n",
      "Epoch 72, Training Loss: 0.7921459550717298\n",
      "Epoch 73, Training Loss: 0.7923370541544521\n",
      "Epoch 74, Training Loss: 0.7922186732292176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:04:33,066] Trial 253 finished with value: 0.6361333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7928430097944597\n",
      "Epoch 1, Training Loss: 0.9908755942653207\n",
      "Epoch 2, Training Loss: 0.9398111300608691\n",
      "Epoch 3, Training Loss: 0.9041315432155833\n",
      "Epoch 4, Training Loss: 0.8497252440452576\n",
      "Epoch 5, Training Loss: 0.8205242325979121\n",
      "Epoch 6, Training Loss: 0.8137425521542044\n",
      "Epoch 7, Training Loss: 0.8111921969582053\n",
      "Epoch 8, Training Loss: 0.8099285343114068\n",
      "Epoch 9, Training Loss: 0.80842992922839\n",
      "Epoch 10, Training Loss: 0.8073599576248842\n",
      "Epoch 11, Training Loss: 0.8075214727485881\n",
      "Epoch 12, Training Loss: 0.8061137014978073\n",
      "Epoch 13, Training Loss: 0.8052696050615872\n",
      "Epoch 14, Training Loss: 0.8048452202712788\n",
      "Epoch 15, Training Loss: 0.8042375276369207\n",
      "Epoch 16, Training Loss: 0.803442148250692\n",
      "Epoch 17, Training Loss: 0.8033011521311367\n",
      "Epoch 18, Training Loss: 0.8030521939081304\n",
      "Epoch 19, Training Loss: 0.8029534180725322\n",
      "Epoch 20, Training Loss: 0.8024482591011944\n",
      "Epoch 21, Training Loss: 0.8022887620505165\n",
      "Epoch 22, Training Loss: 0.8019883261007421\n",
      "Epoch 23, Training Loss: 0.802009062206044\n",
      "Epoch 24, Training Loss: 0.8018303791214438\n",
      "Epoch 25, Training Loss: 0.8013376538893756\n",
      "Epoch 26, Training Loss: 0.8016458828309003\n",
      "Epoch 27, Training Loss: 0.8008298639690175\n",
      "Epoch 28, Training Loss: 0.800868058274774\n",
      "Epoch 29, Training Loss: 0.8006021051547106\n",
      "Epoch 30, Training Loss: 0.8006293313643512\n",
      "Epoch 31, Training Loss: 0.800415839026956\n",
      "Epoch 32, Training Loss: 0.8000989028285531\n",
      "Epoch 33, Training Loss: 0.8004011950773351\n",
      "Epoch 34, Training Loss: 0.7994366292392506\n",
      "Epoch 35, Training Loss: 0.7998077593831455\n",
      "Epoch 36, Training Loss: 0.8005720147665809\n",
      "Epoch 37, Training Loss: 0.7997417869287379\n",
      "Epoch 38, Training Loss: 0.7993763302354252\n",
      "Epoch 39, Training Loss: 0.7993960454884698\n",
      "Epoch 40, Training Loss: 0.7992467394997091\n",
      "Epoch 41, Training Loss: 0.7993138389026417\n",
      "Epoch 42, Training Loss: 0.7984276487546809\n",
      "Epoch 43, Training Loss: 0.7987042195656721\n",
      "Epoch 44, Training Loss: 0.7991298435014836\n",
      "Epoch 45, Training Loss: 0.7983875180693234\n",
      "Epoch 46, Training Loss: 0.7981438133295845\n",
      "Epoch 47, Training Loss: 0.7983352575582616\n",
      "Epoch 48, Training Loss: 0.7982868430894964\n",
      "Epoch 49, Training Loss: 0.7979630961137659\n",
      "Epoch 50, Training Loss: 0.7978059327602387\n",
      "Epoch 51, Training Loss: 0.7975351816766403\n",
      "Epoch 52, Training Loss: 0.7979674975311055\n",
      "Epoch 53, Training Loss: 0.7978661141676061\n",
      "Epoch 54, Training Loss: 0.797413976613213\n",
      "Epoch 55, Training Loss: 0.7974892041262458\n",
      "Epoch 56, Training Loss: 0.7978279235783745\n",
      "Epoch 57, Training Loss: 0.7971908322502586\n",
      "Epoch 58, Training Loss: 0.7974193240614499\n",
      "Epoch 59, Training Loss: 0.797197766865001\n",
      "Epoch 60, Training Loss: 0.7972193330175736\n",
      "Epoch 61, Training Loss: 0.7969155474270091\n",
      "Epoch 62, Training Loss: 0.7969341782962575\n",
      "Epoch 63, Training Loss: 0.7965100265951718\n",
      "Epoch 64, Training Loss: 0.796389570236206\n",
      "Epoch 65, Training Loss: 0.7966304541335386\n",
      "Epoch 66, Training Loss: 0.7966675594273736\n",
      "Epoch 67, Training Loss: 0.7964378382177913\n",
      "Epoch 68, Training Loss: 0.7964068579673768\n",
      "Epoch 69, Training Loss: 0.7959810504492592\n",
      "Epoch 70, Training Loss: 0.7961513649014865\n",
      "Epoch 71, Training Loss: 0.7959233230001787\n",
      "Epoch 72, Training Loss: 0.7962779508618747\n",
      "Epoch 73, Training Loss: 0.7958177749549641\n",
      "Epoch 74, Training Loss: 0.7957267500372494\n",
      "Epoch 75, Training Loss: 0.7954437917821547\n",
      "Epoch 76, Training Loss: 0.7954067025465124\n",
      "Epoch 77, Training Loss: 0.7957291479671703\n",
      "Epoch 78, Training Loss: 0.7953026179005118\n",
      "Epoch 79, Training Loss: 0.7952610219927395\n",
      "Epoch 80, Training Loss: 0.7948493990477393\n",
      "Epoch 81, Training Loss: 0.7947494869372423\n",
      "Epoch 82, Training Loss: 0.7945238601460176\n",
      "Epoch 83, Training Loss: 0.7944919695573694\n",
      "Epoch 84, Training Loss: 0.7946419582647436\n",
      "Epoch 85, Training Loss: 0.7944633633248946\n",
      "Epoch 86, Training Loss: 0.7940113082352807\n",
      "Epoch 87, Training Loss: 0.7943164418725407\n",
      "Epoch 88, Training Loss: 0.7939367128119749\n",
      "Epoch 89, Training Loss: 0.7939462279572206\n",
      "Epoch 90, Training Loss: 0.7934287875540116\n",
      "Epoch 91, Training Loss: 0.7935491768051596\n",
      "Epoch 92, Training Loss: 0.7932546361754922\n",
      "Epoch 93, Training Loss: 0.7930843194092021\n",
      "Epoch 94, Training Loss: 0.7930157941229203\n",
      "Epoch 95, Training Loss: 0.7932784615544712\n",
      "Epoch 96, Training Loss: 0.7931044406049392\n",
      "Epoch 97, Training Loss: 0.792829706177992\n",
      "Epoch 98, Training Loss: 0.7925563401334426\n",
      "Epoch 99, Training Loss: 0.7928577007265651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:06:47,131] Trial 254 finished with value: 0.6352666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7922976535208085\n",
      "Epoch 1, Training Loss: 0.8973233862484202\n",
      "Epoch 2, Training Loss: 0.8251348636430853\n",
      "Epoch 3, Training Loss: 0.8195902366497937\n",
      "Epoch 4, Training Loss: 0.8158512119685902\n",
      "Epoch 5, Training Loss: 0.8128541280942805\n",
      "Epoch 6, Training Loss: 0.8125317040611716\n",
      "Epoch 7, Training Loss: 0.8097106101933648\n",
      "Epoch 8, Training Loss: 0.8091727537968579\n",
      "Epoch 9, Training Loss: 0.8081319301268634\n",
      "Epoch 10, Training Loss: 0.8070570874214172\n",
      "Epoch 11, Training Loss: 0.8068476522670073\n",
      "Epoch 12, Training Loss: 0.8061323538948508\n",
      "Epoch 13, Training Loss: 0.8054517567858976\n",
      "Epoch 14, Training Loss: 0.8047060246327344\n",
      "Epoch 15, Training Loss: 0.8046640429076026\n",
      "Epoch 16, Training Loss: 0.8043616667915793\n",
      "Epoch 17, Training Loss: 0.8037653628517599\n",
      "Epoch 18, Training Loss: 0.8039586346990922\n",
      "Epoch 19, Training Loss: 0.8032797923508812\n",
      "Epoch 20, Training Loss: 0.803284434150247\n",
      "Epoch 21, Training Loss: 0.8032794178233427\n",
      "Epoch 22, Training Loss: 0.8030786611753351\n",
      "Epoch 23, Training Loss: 0.8023853177884046\n",
      "Epoch 24, Training Loss: 0.8024012144874124\n",
      "Epoch 25, Training Loss: 0.8021065594168271\n",
      "Epoch 26, Training Loss: 0.8022714357516345\n",
      "Epoch 27, Training Loss: 0.8020088288363288\n",
      "Epoch 28, Training Loss: 0.8015227132685044\n",
      "Epoch 29, Training Loss: 0.8012406483117271\n",
      "Epoch 30, Training Loss: 0.8008859262746924\n",
      "Epoch 31, Training Loss: 0.801122239407371\n",
      "Epoch 32, Training Loss: 0.801285020463607\n",
      "Epoch 33, Training Loss: 0.8005949255298166\n",
      "Epoch 34, Training Loss: 0.8008967443073497\n",
      "Epoch 35, Training Loss: 0.8004670128401588\n",
      "Epoch 36, Training Loss: 0.8002118215841405\n",
      "Epoch 37, Training Loss: 0.7997694595421062\n",
      "Epoch 38, Training Loss: 0.8001593860457925\n",
      "Epoch 39, Training Loss: 0.7999694376833298\n",
      "Epoch 40, Training Loss: 0.7999362838969512\n",
      "Epoch 41, Training Loss: 0.7995941641050227\n",
      "Epoch 42, Training Loss: 0.7991898948304793\n",
      "Epoch 43, Training Loss: 0.7990939417306114\n",
      "Epoch 44, Training Loss: 0.7993369260956259\n",
      "Epoch 45, Training Loss: 0.7982296338502098\n",
      "Epoch 46, Training Loss: 0.7987066330629237\n",
      "Epoch 47, Training Loss: 0.7980698633895201\n",
      "Epoch 48, Training Loss: 0.7979418753876406\n",
      "Epoch 49, Training Loss: 0.7979793005831102\n",
      "Epoch 50, Training Loss: 0.7978532229451573\n",
      "Epoch 51, Training Loss: 0.7971296403688543\n",
      "Epoch 52, Training Loss: 0.7972628312952378\n",
      "Epoch 53, Training Loss: 0.797157277359682\n",
      "Epoch 54, Training Loss: 0.7967025744915008\n",
      "Epoch 55, Training Loss: 0.7967725535701303\n",
      "Epoch 56, Training Loss: 0.7966849707154666\n",
      "Epoch 57, Training Loss: 0.7963039665362415\n",
      "Epoch 58, Training Loss: 0.7963211962755988\n",
      "Epoch 59, Training Loss: 0.7964083947153653\n",
      "Epoch 60, Training Loss: 0.7961669987089494\n",
      "Epoch 61, Training Loss: 0.7961978555426878\n",
      "Epoch 62, Training Loss: 0.7957538514277515\n",
      "Epoch 63, Training Loss: 0.7957529011894675\n",
      "Epoch 64, Training Loss: 0.7955584073066712\n",
      "Epoch 65, Training Loss: 0.7952302166293649\n",
      "Epoch 66, Training Loss: 0.795485673862345\n",
      "Epoch 67, Training Loss: 0.7951400120118085\n",
      "Epoch 68, Training Loss: 0.7946725651095895\n",
      "Epoch 69, Training Loss: 0.7949671896766214\n",
      "Epoch 70, Training Loss: 0.7948242465888753\n",
      "Epoch 71, Training Loss: 0.7948290528970606\n",
      "Epoch 72, Training Loss: 0.7944189577242907\n",
      "Epoch 73, Training Loss: 0.7941598817881416\n",
      "Epoch 74, Training Loss: 0.7943833778185003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:08:10,412] Trial 255 finished with value: 0.632 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7941885992358713\n",
      "Epoch 1, Training Loss: 1.0584023003017202\n",
      "Epoch 2, Training Loss: 1.0240580055292914\n",
      "Epoch 3, Training Loss: 1.0043119333070867\n",
      "Epoch 4, Training Loss: 0.9899529349803925\n",
      "Epoch 5, Training Loss: 0.9792779742268956\n",
      "Epoch 6, Training Loss: 0.9712803192699657\n",
      "Epoch 7, Training Loss: 0.9651422027279349\n",
      "Epoch 8, Training Loss: 0.9603733874769772\n",
      "Epoch 9, Training Loss: 0.9566007661118227\n",
      "Epoch 10, Training Loss: 0.9535301934270298\n",
      "Epoch 11, Training Loss: 0.9509674669013304\n",
      "Epoch 12, Training Loss: 0.9487824163717382\n",
      "Epoch 13, Training Loss: 0.9468778033817515\n",
      "Epoch 14, Training Loss: 0.9451910344292136\n",
      "Epoch 15, Training Loss: 0.943646606838002\n",
      "Epoch 16, Training Loss: 0.9422304522991181\n",
      "Epoch 17, Training Loss: 0.940908898045035\n",
      "Epoch 18, Training Loss: 0.9396552701557384\n",
      "Epoch 19, Training Loss: 0.9384837629514582\n",
      "Epoch 20, Training Loss: 0.9373595813442679\n",
      "Epoch 21, Training Loss: 0.9362614606408511\n",
      "Epoch 22, Training Loss: 0.9352260459871853\n",
      "Epoch 23, Training Loss: 0.9342111973201528\n",
      "Epoch 24, Training Loss: 0.933233370290083\n",
      "Epoch 25, Training Loss: 0.9322416955583236\n",
      "Epoch 26, Training Loss: 0.9313471712084378\n",
      "Epoch 27, Training Loss: 0.930426547387067\n",
      "Epoch 28, Training Loss: 0.929527557737687\n",
      "Epoch 29, Training Loss: 0.9286593104811276\n",
      "Epoch 30, Training Loss: 0.9278008046570946\n",
      "Epoch 31, Training Loss: 0.9269562473717858\n",
      "Epoch 32, Training Loss: 0.9261266813558691\n",
      "Epoch 33, Training Loss: 0.9253102505207061\n",
      "Epoch 34, Training Loss: 0.9244985973834992\n",
      "Epoch 35, Training Loss: 0.9237063244511099\n",
      "Epoch 36, Training Loss: 0.9229183989412645\n",
      "Epoch 37, Training Loss: 0.922137696462519\n",
      "Epoch 38, Training Loss: 0.9213574337959289\n",
      "Epoch 39, Training Loss: 0.9205951436828165\n",
      "Epoch 40, Training Loss: 0.9198225552194259\n",
      "Epoch 41, Training Loss: 0.9190668876732097\n",
      "Epoch 42, Training Loss: 0.9183128290316638\n",
      "Epoch 43, Training Loss: 0.9175650834336\n",
      "Epoch 44, Training Loss: 0.9168111621632296\n",
      "Epoch 45, Training Loss: 0.9160692118897158\n",
      "Epoch 46, Training Loss: 0.9153134383874781\n",
      "Epoch 47, Training Loss: 0.9145759605660158\n",
      "Epoch 48, Training Loss: 0.9138239659982569\n",
      "Epoch 49, Training Loss: 0.913074385138119\n",
      "Epoch 50, Training Loss: 0.9123347960500157\n",
      "Epoch 51, Training Loss: 0.9116018133303698\n",
      "Epoch 52, Training Loss: 0.9108496102866005\n",
      "Epoch 53, Training Loss: 0.9101137651415432\n",
      "Epoch 54, Training Loss: 0.9093643933184007\n",
      "Epoch 55, Training Loss: 0.9086135484190548\n",
      "Epoch 56, Training Loss: 0.9078716093652388\n",
      "Epoch 57, Training Loss: 0.9071131500075845\n",
      "Epoch 58, Training Loss: 0.9063568952504326\n",
      "Epoch 59, Training Loss: 0.9055956479381112\n",
      "Epoch 60, Training Loss: 0.9048375556749456\n",
      "Epoch 61, Training Loss: 0.9040782664102667\n",
      "Epoch 62, Training Loss: 0.9033144572903128\n",
      "Epoch 63, Training Loss: 0.9025405483386096\n",
      "Epoch 64, Training Loss: 0.9017707877299365\n",
      "Epoch 65, Training Loss: 0.9009931109933292\n",
      "Epoch 66, Training Loss: 0.9002151269071242\n",
      "Epoch 67, Training Loss: 0.8994279875474818\n",
      "Epoch 68, Training Loss: 0.898652125526877\n",
      "Epoch 69, Training Loss: 0.8978676645896014\n",
      "Epoch 70, Training Loss: 0.8970709267083337\n",
      "Epoch 71, Training Loss: 0.8962871273124919\n",
      "Epoch 72, Training Loss: 0.8954826310101678\n",
      "Epoch 73, Training Loss: 0.8946823273686801\n",
      "Epoch 74, Training Loss: 0.8938926911354065\n",
      "Epoch 75, Training Loss: 0.8930754169997047\n",
      "Epoch 76, Training Loss: 0.8922829182007733\n",
      "Epoch 77, Training Loss: 0.8914582452353309\n",
      "Epoch 78, Training Loss: 0.8906552257958581\n",
      "Epoch 79, Training Loss: 0.8898479889420902\n",
      "Epoch 80, Training Loss: 0.8890282548876369\n",
      "Epoch 81, Training Loss: 0.8882151971845066\n",
      "Epoch 82, Training Loss: 0.8873930764899535\n",
      "Epoch 83, Training Loss: 0.8865707475998822\n",
      "Epoch 84, Training Loss: 0.8857378393762252\n",
      "Epoch 85, Training Loss: 0.8849016119452083\n",
      "Epoch 86, Training Loss: 0.8840772946441875\n",
      "Epoch 87, Training Loss: 0.8832414106060477\n",
      "Epoch 88, Training Loss: 0.8824146220263313\n",
      "Epoch 89, Training Loss: 0.8815749449589673\n",
      "Epoch 90, Training Loss: 0.8807365503731895\n",
      "Epoch 91, Training Loss: 0.8799069398290971\n",
      "Epoch 92, Training Loss: 0.8790648419015548\n",
      "Epoch 93, Training Loss: 0.8782224927930271\n",
      "Epoch 94, Training Loss: 0.8773700601213119\n",
      "Epoch 95, Training Loss: 0.8765471242455876\n",
      "Epoch 96, Training Loss: 0.875689095398959\n",
      "Epoch 97, Training Loss: 0.874861170614467\n",
      "Epoch 98, Training Loss: 0.8740076733336729\n",
      "Epoch 99, Training Loss: 0.8731669830574709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:09:43,089] Trial 256 finished with value: 0.5922 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8723328662619871\n",
      "Epoch 1, Training Loss: 1.0933770188163308\n",
      "Epoch 2, Training Loss: 1.0914402006654178\n",
      "Epoch 3, Training Loss: 1.0911553309945499\n",
      "Epoch 4, Training Loss: 1.0908787068198709\n",
      "Epoch 5, Training Loss: 1.0905732532108532\n",
      "Epoch 6, Training Loss: 1.0902943169369417\n",
      "Epoch 7, Training Loss: 1.0900114345550538\n",
      "Epoch 8, Training Loss: 1.089725160739001\n",
      "Epoch 9, Training Loss: 1.0894471865541795\n",
      "Epoch 10, Training Loss: 1.0891568467196295\n",
      "Epoch 11, Training Loss: 1.0888698353486903\n",
      "Epoch 12, Training Loss: 1.088570932640749\n",
      "Epoch 13, Training Loss: 1.0882886498114641\n",
      "Epoch 14, Training Loss: 1.0880163387691273\n",
      "Epoch 15, Training Loss: 1.0877162910910214\n",
      "Epoch 16, Training Loss: 1.0874140384617974\n",
      "Epoch 17, Training Loss: 1.0871305235694437\n",
      "Epoch 18, Training Loss: 1.0868390219351824\n",
      "Epoch 19, Training Loss: 1.086546495101031\n",
      "Epoch 20, Training Loss: 1.0862431801066679\n",
      "Epoch 21, Training Loss: 1.0859311042112463\n",
      "Epoch 22, Training Loss: 1.0856315666086533\n",
      "Epoch 23, Training Loss: 1.0853240414226757\n",
      "Epoch 24, Training Loss: 1.0850021337060367\n",
      "Epoch 25, Training Loss: 1.084716533352347\n",
      "Epoch 26, Training Loss: 1.0843718619907603\n",
      "Epoch 27, Training Loss: 1.0840528612978317\n",
      "Epoch 28, Training Loss: 1.0837144481434542\n",
      "Epoch 29, Training Loss: 1.0833947455181796\n",
      "Epoch 30, Training Loss: 1.0830563385346357\n",
      "Epoch 31, Training Loss: 1.0827115858302396\n",
      "Epoch 32, Training Loss: 1.0823681378364562\n",
      "Epoch 33, Training Loss: 1.082014430550968\n",
      "Epoch 34, Training Loss: 1.0816467080396766\n",
      "Epoch 35, Training Loss: 1.0812809759027817\n",
      "Epoch 36, Training Loss: 1.0809131184746237\n",
      "Epoch 37, Training Loss: 1.0805224218088039\n",
      "Epoch 38, Training Loss: 1.0801305968621198\n",
      "Epoch 39, Training Loss: 1.0797346228711746\n",
      "Epoch 40, Training Loss: 1.079342018435983\n",
      "Epoch 41, Training Loss: 1.0789131698888892\n",
      "Epoch 42, Training Loss: 1.078511438930736\n",
      "Epoch 43, Training Loss: 1.0780803772982428\n",
      "Epoch 44, Training Loss: 1.077641741808723\n",
      "Epoch 45, Training Loss: 1.0771929816638721\n",
      "Epoch 46, Training Loss: 1.0767534514034496\n",
      "Epoch 47, Training Loss: 1.076279475408442\n",
      "Epoch 48, Training Loss: 1.0757849408598508\n",
      "Epoch 49, Training Loss: 1.075300271511078\n",
      "Epoch 50, Training Loss: 1.0748235675867865\n",
      "Epoch 51, Training Loss: 1.0742977313434376\n",
      "Epoch 52, Training Loss: 1.0737929787355311\n",
      "Epoch 53, Training Loss: 1.0732495504267074\n",
      "Epoch 54, Training Loss: 1.0727138927403619\n",
      "Epoch 55, Training Loss: 1.0721491767378415\n",
      "Epoch 56, Training Loss: 1.0715851025020375\n",
      "Epoch 57, Training Loss: 1.0710074353218078\n",
      "Epoch 58, Training Loss: 1.0704042255177217\n",
      "Epoch 59, Training Loss: 1.0697913972069235\n",
      "Epoch 60, Training Loss: 1.0691663770114674\n",
      "Epoch 61, Training Loss: 1.068521990215077\n",
      "Epoch 62, Training Loss: 1.0678671120194827\n",
      "Epoch 63, Training Loss: 1.067183188831105\n",
      "Epoch 64, Training Loss: 1.0664828767495997\n",
      "Epoch 65, Training Loss: 1.065786521434784\n",
      "Epoch 66, Training Loss: 1.0650692094073575\n",
      "Epoch 67, Training Loss: 1.0643371608678032\n",
      "Epoch 68, Training Loss: 1.063555311876185\n",
      "Epoch 69, Training Loss: 1.0627933192253112\n",
      "Epoch 70, Training Loss: 1.0620108011189628\n",
      "Epoch 71, Training Loss: 1.0611864664975335\n",
      "Epoch 72, Training Loss: 1.06034283581902\n",
      "Epoch 73, Training Loss: 1.059502902872422\n",
      "Epoch 74, Training Loss: 1.0586428727823145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:11:05,176] Trial 257 finished with value: 0.46013333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 1.0577384373720955\n",
      "Epoch 1, Training Loss: 1.001822979557783\n",
      "Epoch 2, Training Loss: 0.9493597165982526\n",
      "Epoch 3, Training Loss: 0.9392397989007764\n",
      "Epoch 4, Training Loss: 0.9327460701304271\n",
      "Epoch 5, Training Loss: 0.9275155684105436\n",
      "Epoch 6, Training Loss: 0.9233169941077555\n",
      "Epoch 7, Training Loss: 0.919183691910335\n",
      "Epoch 8, Training Loss: 0.9156321648368262\n",
      "Epoch 9, Training Loss: 0.9114518326924259\n",
      "Epoch 10, Training Loss: 0.9074563157289548\n",
      "Epoch 11, Training Loss: 0.9039388102696354\n",
      "Epoch 12, Training Loss: 0.8990535771040092\n",
      "Epoch 13, Training Loss: 0.8950075681048228\n",
      "Epoch 14, Training Loss: 0.8900312511544478\n",
      "Epoch 15, Training Loss: 0.884715189252581\n",
      "Epoch 16, Training Loss: 0.879614906562002\n",
      "Epoch 17, Training Loss: 0.8737619242273775\n",
      "Epoch 18, Training Loss: 0.8680322540433784\n",
      "Epoch 19, Training Loss: 0.8618431026774241\n",
      "Epoch 20, Training Loss: 0.8554614060803464\n",
      "Epoch 21, Training Loss: 0.8494441769176856\n",
      "Epoch 22, Training Loss: 0.8438245326952827\n",
      "Epoch 23, Training Loss: 0.8386208637316425\n",
      "Epoch 24, Training Loss: 0.8336738696671966\n",
      "Epoch 25, Training Loss: 0.8299792372194448\n",
      "Epoch 26, Training Loss: 0.8263035686392534\n",
      "Epoch 27, Training Loss: 0.8227206515190296\n",
      "Epoch 28, Training Loss: 0.8198820101587396\n",
      "Epoch 29, Training Loss: 0.8171892071576943\n",
      "Epoch 30, Training Loss: 0.8159485136655936\n",
      "Epoch 31, Training Loss: 0.8137040753113596\n",
      "Epoch 32, Training Loss: 0.8128358936847601\n",
      "Epoch 33, Training Loss: 0.8110368077019999\n",
      "Epoch 34, Training Loss: 0.8105689627783639\n",
      "Epoch 35, Training Loss: 0.8094512946623609\n",
      "Epoch 36, Training Loss: 0.8078824352501031\n",
      "Epoch 37, Training Loss: 0.8077424354122994\n",
      "Epoch 38, Training Loss: 0.8076421090534756\n",
      "Epoch 39, Training Loss: 0.8063011625655612\n",
      "Epoch 40, Training Loss: 0.8068493532058888\n",
      "Epoch 41, Training Loss: 0.805370661280209\n",
      "Epoch 42, Training Loss: 0.8046874762477731\n",
      "Epoch 43, Training Loss: 0.8054373842432982\n",
      "Epoch 44, Training Loss: 0.8043122776468894\n",
      "Epoch 45, Training Loss: 0.8037696120434238\n",
      "Epoch 46, Training Loss: 0.8039034625641386\n",
      "Epoch 47, Training Loss: 0.8033839647034954\n",
      "Epoch 48, Training Loss: 0.8032040616623441\n",
      "Epoch 49, Training Loss: 0.802897202161918\n",
      "Epoch 50, Training Loss: 0.8028810757443421\n",
      "Epoch 51, Training Loss: 0.8022885032166216\n",
      "Epoch 52, Training Loss: 0.801589679583571\n",
      "Epoch 53, Training Loss: 0.8020325543288898\n",
      "Epoch 54, Training Loss: 0.8017087166470692\n",
      "Epoch 55, Training Loss: 0.8016336147050213\n",
      "Epoch 56, Training Loss: 0.8016372242368254\n",
      "Epoch 57, Training Loss: 0.8019572992970173\n",
      "Epoch 58, Training Loss: 0.8010561919750128\n",
      "Epoch 59, Training Loss: 0.8009054206367722\n",
      "Epoch 60, Training Loss: 0.800932608690477\n",
      "Epoch 61, Training Loss: 0.8007756342565207\n",
      "Epoch 62, Training Loss: 0.8005702105679906\n",
      "Epoch 63, Training Loss: 0.8009814705167498\n",
      "Epoch 64, Training Loss: 0.8012284191031205\n",
      "Epoch 65, Training Loss: 0.8004445231946787\n",
      "Epoch 66, Training Loss: 0.8003006524609444\n",
      "Epoch 67, Training Loss: 0.7997705300051466\n",
      "Epoch 68, Training Loss: 0.800364229284731\n",
      "Epoch 69, Training Loss: 0.7997849312940039\n",
      "Epoch 70, Training Loss: 0.800187865683907\n",
      "Epoch 71, Training Loss: 0.7996027040302305\n",
      "Epoch 72, Training Loss: 0.8002333204549058\n",
      "Epoch 73, Training Loss: 0.7998108181738316\n",
      "Epoch 74, Training Loss: 0.7994748895329641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:12:06,047] Trial 258 finished with value: 0.635 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7992698879170238\n",
      "Epoch 1, Training Loss: 1.0833779146796778\n",
      "Epoch 2, Training Loss: 1.0550394692815337\n",
      "Epoch 3, Training Loss: 1.0343022612700785\n",
      "Epoch 4, Training Loss: 1.0177518792618485\n",
      "Epoch 5, Training Loss: 1.0044946603308944\n",
      "Epoch 6, Training Loss: 0.9931576450068251\n",
      "Epoch 7, Training Loss: 0.9848582185300669\n",
      "Epoch 8, Training Loss: 0.9778152864678462\n",
      "Epoch 9, Training Loss: 0.9729616304089252\n",
      "Epoch 10, Training Loss: 0.9687809442218981\n",
      "Epoch 11, Training Loss: 0.9657562839357476\n",
      "Epoch 12, Training Loss: 0.963331137115794\n",
      "Epoch 13, Training Loss: 0.9622139548896846\n",
      "Epoch 14, Training Loss: 0.9604828063706706\n",
      "Epoch 15, Training Loss: 0.9588239164280712\n",
      "Epoch 16, Training Loss: 0.9575884845920075\n",
      "Epoch 17, Training Loss: 0.9568185241598832\n",
      "Epoch 18, Training Loss: 0.9562886416463924\n",
      "Epoch 19, Training Loss: 0.9549767490616419\n",
      "Epoch 20, Training Loss: 0.9542372020563685\n",
      "Epoch 21, Training Loss: 0.9535686155010883\n",
      "Epoch 22, Training Loss: 0.9524153650255132\n",
      "Epoch 23, Training Loss: 0.9518519960848012\n",
      "Epoch 24, Training Loss: 0.9510105962143804\n",
      "Epoch 25, Training Loss: 0.9503597082052015\n",
      "Epoch 26, Training Loss: 0.9495961774560742\n",
      "Epoch 27, Training Loss: 0.948411256388614\n",
      "Epoch 28, Training Loss: 0.9479483624149982\n",
      "Epoch 29, Training Loss: 0.9468941834636201\n",
      "Epoch 30, Training Loss: 0.9460954209915677\n",
      "Epoch 31, Training Loss: 0.9455434150265571\n",
      "Epoch 32, Training Loss: 0.9444721553558694\n",
      "Epoch 33, Training Loss: 0.9437177036041604\n",
      "Epoch 34, Training Loss: 0.9427569374105984\n",
      "Epoch 35, Training Loss: 0.9417885418225052\n",
      "Epoch 36, Training Loss: 0.940966498493252\n",
      "Epoch 37, Training Loss: 0.9400883925588508\n",
      "Epoch 38, Training Loss: 0.9390762918873837\n",
      "Epoch 39, Training Loss: 0.9381699329928348\n",
      "Epoch 40, Training Loss: 0.9370287986626302\n",
      "Epoch 41, Training Loss: 0.9360106478956409\n",
      "Epoch 42, Training Loss: 0.9351115449926907\n",
      "Epoch 43, Training Loss: 0.9347218649727957\n",
      "Epoch 44, Training Loss: 0.933277246019894\n",
      "Epoch 45, Training Loss: 0.9324725842117367\n",
      "Epoch 46, Training Loss: 0.9312056016204948\n",
      "Epoch 47, Training Loss: 0.9299302880925343\n",
      "Epoch 48, Training Loss: 0.9292963934123964\n",
      "Epoch 49, Training Loss: 0.9278061546777424\n",
      "Epoch 50, Training Loss: 0.9266243790325366\n",
      "Epoch 51, Training Loss: 0.9253953943575235\n",
      "Epoch 52, Training Loss: 0.924486836365291\n",
      "Epoch 53, Training Loss: 0.9232922643647158\n",
      "Epoch 54, Training Loss: 0.9220954978376403\n",
      "Epoch 55, Training Loss: 0.9208628723495885\n",
      "Epoch 56, Training Loss: 0.9189826409619554\n",
      "Epoch 57, Training Loss: 0.9183551895887332\n",
      "Epoch 58, Training Loss: 0.916843451145\n",
      "Epoch 59, Training Loss: 0.9152035820753055\n",
      "Epoch 60, Training Loss: 0.9137579377432515\n",
      "Epoch 61, Training Loss: 0.9125694180789746\n",
      "Epoch 62, Training Loss: 0.9114344700834804\n",
      "Epoch 63, Training Loss: 0.9094352849444053\n",
      "Epoch 64, Training Loss: 0.9082061232480788\n",
      "Epoch 65, Training Loss: 0.9069571126672559\n",
      "Epoch 66, Training Loss: 0.9054824960859198\n",
      "Epoch 67, Training Loss: 0.9039107796841098\n",
      "Epoch 68, Training Loss: 0.9025490082296214\n",
      "Epoch 69, Training Loss: 0.9004366367383111\n",
      "Epoch 70, Training Loss: 0.8989019960389102\n",
      "Epoch 71, Training Loss: 0.8969236233180626\n",
      "Epoch 72, Training Loss: 0.8957305904617883\n",
      "Epoch 73, Training Loss: 0.8941216203503143\n",
      "Epoch 74, Training Loss: 0.8916287305659818\n",
      "Epoch 75, Training Loss: 0.890118650386208\n",
      "Epoch 76, Training Loss: 0.8884331527509187\n",
      "Epoch 77, Training Loss: 0.8869381367712093\n",
      "Epoch 78, Training Loss: 0.8850075043233714\n",
      "Epoch 79, Training Loss: 0.8833303245386683\n",
      "Epoch 80, Training Loss: 0.8811942865077714\n",
      "Epoch 81, Training Loss: 0.8800544631212277\n",
      "Epoch 82, Training Loss: 0.8775287240967715\n",
      "Epoch 83, Training Loss: 0.8758755369293959\n",
      "Epoch 84, Training Loss: 0.8751175656354517\n",
      "Epoch 85, Training Loss: 0.8717797576036669\n",
      "Epoch 86, Training Loss: 0.8707150023682673\n",
      "Epoch 87, Training Loss: 0.8688393265681159\n",
      "Epoch 88, Training Loss: 0.8676220804228818\n",
      "Epoch 89, Training Loss: 0.8649267775671823\n",
      "Epoch 90, Training Loss: 0.8637503549568635\n",
      "Epoch 91, Training Loss: 0.8623268007335806\n",
      "Epoch 92, Training Loss: 0.86068727002108\n",
      "Epoch 93, Training Loss: 0.8584678337986308\n",
      "Epoch 94, Training Loss: 0.8573472470269168\n",
      "Epoch 95, Training Loss: 0.8558221957737342\n",
      "Epoch 96, Training Loss: 0.8536575349649989\n",
      "Epoch 97, Training Loss: 0.8522657901720894\n",
      "Epoch 98, Training Loss: 0.8510475684825639\n",
      "Epoch 99, Training Loss: 0.8491699489435756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:13:37,200] Trial 259 finished with value: 0.6088 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8484146918569292\n",
      "Epoch 1, Training Loss: 0.8775059468764111\n",
      "Epoch 2, Training Loss: 0.8187690498237323\n",
      "Epoch 3, Training Loss: 0.8125466456090598\n",
      "Epoch 4, Training Loss: 0.8108475060391247\n",
      "Epoch 5, Training Loss: 0.810228175059297\n",
      "Epoch 6, Training Loss: 0.8091317977224077\n",
      "Epoch 7, Training Loss: 0.8091808013449934\n",
      "Epoch 8, Training Loss: 0.8084164892820488\n",
      "Epoch 9, Training Loss: 0.8061016386612914\n",
      "Epoch 10, Training Loss: 0.8079224479825873\n",
      "Epoch 11, Training Loss: 0.8055276449461629\n",
      "Epoch 12, Training Loss: 0.8054231229581331\n",
      "Epoch 13, Training Loss: 0.8048503498385723\n",
      "Epoch 14, Training Loss: 0.806097310109246\n",
      "Epoch 15, Training Loss: 0.8043678830440779\n",
      "Epoch 16, Training Loss: 0.8028398392792034\n",
      "Epoch 17, Training Loss: 0.8038196538624011\n",
      "Epoch 18, Training Loss: 0.8041359164661035\n",
      "Epoch 19, Training Loss: 0.8029119431524349\n",
      "Epoch 20, Training Loss: 0.8034290773527962\n",
      "Epoch 21, Training Loss: 0.8039790181289042\n",
      "Epoch 22, Training Loss: 0.8029300200311761\n",
      "Epoch 23, Training Loss: 0.8019641666484059\n",
      "Epoch 24, Training Loss: 0.8026125436438654\n",
      "Epoch 25, Training Loss: 0.8017313094067394\n",
      "Epoch 26, Training Loss: 0.8027280506334806\n",
      "Epoch 27, Training Loss: 0.8008171458889667\n",
      "Epoch 28, Training Loss: 0.8011389895489341\n",
      "Epoch 29, Training Loss: 0.8004214477718324\n",
      "Epoch 30, Training Loss: 0.8001667857170105\n",
      "Epoch 31, Training Loss: 0.7998944453727034\n",
      "Epoch 32, Training Loss: 0.7990052506439668\n",
      "Epoch 33, Training Loss: 0.7990718375471302\n",
      "Epoch 34, Training Loss: 0.7987885167724208\n",
      "Epoch 35, Training Loss: 0.7989284571848417\n",
      "Epoch 36, Training Loss: 0.7981587818690709\n",
      "Epoch 37, Training Loss: 0.7975862125256904\n",
      "Epoch 38, Training Loss: 0.7982628914646637\n",
      "Epoch 39, Training Loss: 0.7986698916083887\n",
      "Epoch 40, Training Loss: 0.7966088104965096\n",
      "Epoch 41, Training Loss: 0.7967284078884842\n",
      "Epoch 42, Training Loss: 0.7963853191612358\n",
      "Epoch 43, Training Loss: 0.797383535446081\n",
      "Epoch 44, Training Loss: 0.7968813927550065\n",
      "Epoch 45, Training Loss: 0.796317364129805\n",
      "Epoch 46, Training Loss: 0.7972763984723199\n",
      "Epoch 47, Training Loss: 0.795474196018133\n",
      "Epoch 48, Training Loss: 0.7967509210557866\n",
      "Epoch 49, Training Loss: 0.7959754113864181\n",
      "Epoch 50, Training Loss: 0.7954889828997447\n",
      "Epoch 51, Training Loss: 0.7954641092092471\n",
      "Epoch 52, Training Loss: 0.7948204625818066\n",
      "Epoch 53, Training Loss: 0.7954662196618274\n",
      "Epoch 54, Training Loss: 0.7947914067067599\n",
      "Epoch 55, Training Loss: 0.7936799944343423\n",
      "Epoch 56, Training Loss: 0.7944457021871008\n",
      "Epoch 57, Training Loss: 0.7937127106619957\n",
      "Epoch 58, Training Loss: 0.7945979678541197\n",
      "Epoch 59, Training Loss: 0.7946396816942028\n",
      "Epoch 60, Training Loss: 0.7940051682909629\n",
      "Epoch 61, Training Loss: 0.7950184335385947\n",
      "Epoch 62, Training Loss: 0.7945456719039974\n",
      "Epoch 63, Training Loss: 0.7925749811911045\n",
      "Epoch 64, Training Loss: 0.793698367229978\n",
      "Epoch 65, Training Loss: 0.7935424021312169\n",
      "Epoch 66, Training Loss: 0.7926712234217421\n",
      "Epoch 67, Training Loss: 0.7924961396177909\n",
      "Epoch 68, Training Loss: 0.7930262645384423\n",
      "Epoch 69, Training Loss: 0.792824737828477\n",
      "Epoch 70, Training Loss: 0.7925450556260303\n",
      "Epoch 71, Training Loss: 0.7930328951742416\n",
      "Epoch 72, Training Loss: 0.7925218917373428\n",
      "Epoch 73, Training Loss: 0.792425529042581\n",
      "Epoch 74, Training Loss: 0.7929755049540584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:14:50,708] Trial 260 finished with value: 0.6318 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7924575171076266\n",
      "Epoch 1, Training Loss: 0.9743918690260719\n",
      "Epoch 2, Training Loss: 0.9222643439909991\n",
      "Epoch 3, Training Loss: 0.9078509708713083\n",
      "Epoch 4, Training Loss: 0.8945511713448693\n",
      "Epoch 5, Training Loss: 0.8800525665283203\n",
      "Epoch 6, Training Loss: 0.8645925112331615\n",
      "Epoch 7, Training Loss: 0.849610593178693\n",
      "Epoch 8, Training Loss: 0.83668720981654\n",
      "Epoch 9, Training Loss: 0.8266067761533401\n",
      "Epoch 10, Training Loss: 0.819148764259675\n",
      "Epoch 11, Training Loss: 0.8142216978353612\n",
      "Epoch 12, Training Loss: 0.810715898696114\n",
      "Epoch 13, Training Loss: 0.8083112904604743\n",
      "Epoch 14, Training Loss: 0.8064350346256705\n",
      "Epoch 15, Training Loss: 0.805053548041512\n",
      "Epoch 16, Training Loss: 0.8038362550735474\n",
      "Epoch 17, Training Loss: 0.8029700007158167\n",
      "Epoch 18, Training Loss: 0.802127361087238\n",
      "Epoch 19, Training Loss: 0.8016275394664091\n",
      "Epoch 20, Training Loss: 0.8012938695094165\n",
      "Epoch 21, Training Loss: 0.8007553937154658\n",
      "Epoch 22, Training Loss: 0.8005154137751636\n",
      "Epoch 23, Training Loss: 0.8001771942306968\n",
      "Epoch 24, Training Loss: 0.8000353257796343\n",
      "Epoch 25, Training Loss: 0.799711970651851\n",
      "Epoch 26, Training Loss: 0.7995160573370317\n",
      "Epoch 27, Training Loss: 0.7993564875686869\n",
      "Epoch 28, Training Loss: 0.7991075574650484\n",
      "Epoch 29, Training Loss: 0.7987788096596213\n",
      "Epoch 30, Training Loss: 0.7985611151246463\n",
      "Epoch 31, Training Loss: 0.7984192229719723\n",
      "Epoch 32, Training Loss: 0.7980359076051151\n",
      "Epoch 33, Training Loss: 0.7980816330629237\n",
      "Epoch 34, Training Loss: 0.7978895256098579\n",
      "Epoch 35, Training Loss: 0.7975870792304768\n",
      "Epoch 36, Training Loss: 0.7974649509962868\n",
      "Epoch 37, Training Loss: 0.7973067789218005\n",
      "Epoch 38, Training Loss: 0.7970834969773012\n",
      "Epoch 39, Training Loss: 0.7967518181660596\n",
      "Epoch 40, Training Loss: 0.7964905548095703\n",
      "Epoch 41, Training Loss: 0.7961532951102537\n",
      "Epoch 42, Training Loss: 0.7958778382750118\n",
      "Epoch 43, Training Loss: 0.7957351665637072\n",
      "Epoch 44, Training Loss: 0.7957067442641539\n",
      "Epoch 45, Training Loss: 0.7953498696579653\n",
      "Epoch 46, Training Loss: 0.795097793340683\n",
      "Epoch 47, Training Loss: 0.7948798594755285\n",
      "Epoch 48, Training Loss: 0.794847450466717\n",
      "Epoch 49, Training Loss: 0.7946405936689938\n",
      "Epoch 50, Training Loss: 0.7945400047302246\n",
      "Epoch 51, Training Loss: 0.7942238878502565\n",
      "Epoch 52, Training Loss: 0.7941532230377197\n",
      "Epoch 53, Training Loss: 0.7939120161533356\n",
      "Epoch 54, Training Loss: 0.7938087684266707\n",
      "Epoch 55, Training Loss: 0.7937165591296028\n",
      "Epoch 56, Training Loss: 0.7936934673084932\n",
      "Epoch 57, Training Loss: 0.793535915332682\n",
      "Epoch 58, Training Loss: 0.7932679663686192\n",
      "Epoch 59, Training Loss: 0.7931230293301975\n",
      "Epoch 60, Training Loss: 0.7931248260245604\n",
      "Epoch 61, Training Loss: 0.7929407252984888\n",
      "Epoch 62, Training Loss: 0.7925711164754979\n",
      "Epoch 63, Training Loss: 0.7926763203564812\n",
      "Epoch 64, Training Loss: 0.7924656224250793\n",
      "Epoch 65, Training Loss: 0.7924837031083949\n",
      "Epoch 66, Training Loss: 0.7924550361493055\n",
      "Epoch 67, Training Loss: 0.792273659215254\n",
      "Epoch 68, Training Loss: 0.7921293384888592\n",
      "Epoch 69, Training Loss: 0.7920937399303212\n",
      "Epoch 70, Training Loss: 0.7921261749548071\n",
      "Epoch 71, Training Loss: 0.7919459978973165\n",
      "Epoch 72, Training Loss: 0.791891770362854\n",
      "Epoch 73, Training Loss: 0.7917854961226968\n",
      "Epoch 74, Training Loss: 0.7915163967889898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:15:59,023] Trial 261 finished with value: 0.6386 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7915848237626693\n",
      "Epoch 1, Training Loss: 0.8833718777404111\n",
      "Epoch 2, Training Loss: 0.8224055974623736\n",
      "Epoch 3, Training Loss: 0.8133017176039079\n",
      "Epoch 4, Training Loss: 0.8077309780261096\n",
      "Epoch 5, Training Loss: 0.8035613227591795\n",
      "Epoch 6, Training Loss: 0.8019311456119314\n",
      "Epoch 7, Training Loss: 0.7994195011082817\n",
      "Epoch 8, Training Loss: 0.7980237081471612\n",
      "Epoch 9, Training Loss: 0.7979256456038532\n",
      "Epoch 10, Training Loss: 0.7960609426217921\n",
      "Epoch 11, Training Loss: 0.7964001281121198\n",
      "Epoch 12, Training Loss: 0.7949706682738136\n",
      "Epoch 13, Training Loss: 0.7942949859534993\n",
      "Epoch 14, Training Loss: 0.7944097900390625\n",
      "Epoch 15, Training Loss: 0.7936016740518458\n",
      "Epoch 16, Training Loss: 0.7928161498378304\n",
      "Epoch 17, Training Loss: 0.7924107180623448\n",
      "Epoch 18, Training Loss: 0.7926345146403593\n",
      "Epoch 19, Training Loss: 0.7921759390830994\n",
      "Epoch 20, Training Loss: 0.7918921678907731\n",
      "Epoch 21, Training Loss: 0.7915018337614396\n",
      "Epoch 22, Training Loss: 0.7909435780609355\n",
      "Epoch 23, Training Loss: 0.7911101841225343\n",
      "Epoch 24, Training Loss: 0.7905377463733448\n",
      "Epoch 25, Training Loss: 0.7907039099581101\n",
      "Epoch 26, Training Loss: 0.7911188642417684\n",
      "Epoch 27, Training Loss: 0.7898316738184761\n",
      "Epoch 28, Training Loss: 0.7900347963501425\n",
      "Epoch 29, Training Loss: 0.7905247539632461\n",
      "Epoch 30, Training Loss: 0.78987667371245\n",
      "Epoch 31, Training Loss: 0.7903203777705922\n",
      "Epoch 32, Training Loss: 0.7899398280592526\n",
      "Epoch 33, Training Loss: 0.789698564936133\n",
      "Epoch 34, Training Loss: 0.7895908792579875\n",
      "Epoch 35, Training Loss: 0.7897617789577035\n",
      "Epoch 36, Training Loss: 0.7903153067476609\n",
      "Epoch 37, Training Loss: 0.7891885518326479\n",
      "Epoch 38, Training Loss: 0.7890283235381631\n",
      "Epoch 39, Training Loss: 0.7900408478344187\n",
      "Epoch 40, Training Loss: 0.7893510683143841\n",
      "Epoch 41, Training Loss: 0.789342846660053\n",
      "Epoch 42, Training Loss: 0.7890934975007001\n",
      "Epoch 43, Training Loss: 0.7887288801109089\n",
      "Epoch 44, Training Loss: 0.7890840985494502\n",
      "Epoch 45, Training Loss: 0.7892503807825201\n",
      "Epoch 46, Training Loss: 0.7887833582653719\n",
      "Epoch 47, Training Loss: 0.7888583254814148\n",
      "Epoch 48, Training Loss: 0.788488134285983\n",
      "Epoch 49, Training Loss: 0.7883068248103646\n",
      "Epoch 50, Training Loss: 0.7885509796703563\n",
      "Epoch 51, Training Loss: 0.7886352656869328\n",
      "Epoch 52, Training Loss: 0.7878684270381927\n",
      "Epoch 53, Training Loss: 0.7879304073838627\n",
      "Epoch 54, Training Loss: 0.7877473827670602\n",
      "Epoch 55, Training Loss: 0.7870959914431852\n",
      "Epoch 56, Training Loss: 0.7876902109033921\n",
      "Epoch 57, Training Loss: 0.7881237921995276\n",
      "Epoch 58, Training Loss: 0.7875249047840343\n",
      "Epoch 59, Training Loss: 0.7881439875855165\n",
      "Epoch 60, Training Loss: 0.787497187502244\n",
      "Epoch 61, Training Loss: 0.7876591587767882\n",
      "Epoch 62, Training Loss: 0.7879381747806773\n",
      "Epoch 63, Training Loss: 0.7878693329586702\n",
      "Epoch 64, Training Loss: 0.7876115970751818\n",
      "Epoch 65, Training Loss: 0.7874304019703584\n",
      "Epoch 66, Training Loss: 0.787634819465525\n",
      "Epoch 67, Training Loss: 0.7871868826361264\n",
      "Epoch 68, Training Loss: 0.7873865825288436\n",
      "Epoch 69, Training Loss: 0.7872781252861023\n",
      "Epoch 70, Training Loss: 0.7876879912965438\n",
      "Epoch 71, Training Loss: 0.7873919625843272\n",
      "Epoch 72, Training Loss: 0.78756339227452\n",
      "Epoch 73, Training Loss: 0.7874302163544823\n",
      "Epoch 74, Training Loss: 0.787176623134052\n",
      "Epoch 75, Training Loss: 0.7870549741913291\n",
      "Epoch 76, Training Loss: 0.7869620620503145\n",
      "Epoch 77, Training Loss: 0.7875108199259814\n",
      "Epoch 78, Training Loss: 0.7874032038099625\n",
      "Epoch 79, Training Loss: 0.7868624604449552\n",
      "Epoch 80, Training Loss: 0.787349889348535\n",
      "Epoch 81, Training Loss: 0.7864292420359219\n",
      "Epoch 82, Training Loss: 0.7867380263524897\n",
      "Epoch 83, Training Loss: 0.7864121324875776\n",
      "Epoch 84, Training Loss: 0.7875048680165234\n",
      "Epoch 85, Training Loss: 0.7864680936757256\n",
      "Epoch 86, Training Loss: 0.7873788393245024\n",
      "Epoch 87, Training Loss: 0.7871128527557149\n",
      "Epoch 88, Training Loss: 0.7865420358321246\n",
      "Epoch 89, Training Loss: 0.7871604446102591\n",
      "Epoch 90, Training Loss: 0.7871651777800391\n",
      "Epoch 91, Training Loss: 0.7865084491056554\n",
      "Epoch 92, Training Loss: 0.7870030533566195\n",
      "Epoch 93, Training Loss: 0.7864206633147072\n",
      "Epoch 94, Training Loss: 0.7868010532154757\n",
      "Epoch 95, Training Loss: 0.7869136529109058\n",
      "Epoch 96, Training Loss: 0.7872841497729807\n",
      "Epoch 97, Training Loss: 0.7871076214313507\n",
      "Epoch 98, Training Loss: 0.7864800591328565\n",
      "Epoch 99, Training Loss: 0.7863652258059558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:18:04,511] Trial 262 finished with value: 0.6392 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7869696827495799\n",
      "Epoch 1, Training Loss: 0.8671103747267472\n",
      "Epoch 2, Training Loss: 0.8147571523386733\n",
      "Epoch 3, Training Loss: 0.8085023527754877\n",
      "Epoch 4, Training Loss: 0.8073664956523063\n",
      "Epoch 5, Training Loss: 0.8018079877796029\n",
      "Epoch 6, Training Loss: 0.7979675778769013\n",
      "Epoch 7, Training Loss: 0.7965023963074935\n",
      "Epoch 8, Training Loss: 0.7946140208190545\n",
      "Epoch 9, Training Loss: 0.7954218534598673\n",
      "Epoch 10, Training Loss: 0.7940542340278626\n",
      "Epoch 11, Training Loss: 0.7941117226629328\n",
      "Epoch 12, Training Loss: 0.7925484440380469\n",
      "Epoch 13, Training Loss: 0.7938698092797645\n",
      "Epoch 14, Training Loss: 0.7930350604810212\n",
      "Epoch 15, Training Loss: 0.7915871619281912\n",
      "Epoch 16, Training Loss: 0.792381413180129\n",
      "Epoch 17, Training Loss: 0.7904875248894656\n",
      "Epoch 18, Training Loss: 0.7924803840486627\n",
      "Epoch 19, Training Loss: 0.791316114422074\n",
      "Epoch 20, Training Loss: 0.7896131521777103\n",
      "Epoch 21, Training Loss: 0.7906480662804797\n",
      "Epoch 22, Training Loss: 0.7905998046236827\n",
      "Epoch 23, Training Loss: 0.7899777609602849\n",
      "Epoch 24, Training Loss: 0.7893899304526193\n",
      "Epoch 25, Training Loss: 0.7899170581559489\n",
      "Epoch 26, Training Loss: 0.7890305153409342\n",
      "Epoch 27, Training Loss: 0.7883688192618521\n",
      "Epoch 28, Training Loss: 0.7896831496317583\n",
      "Epoch 29, Training Loss: 0.7885989769060809\n",
      "Epoch 30, Training Loss: 0.7876042598172238\n",
      "Epoch 31, Training Loss: 0.7875950184979833\n",
      "Epoch 32, Training Loss: 0.7874893041481649\n",
      "Epoch 33, Training Loss: 0.787716361364924\n",
      "Epoch 34, Training Loss: 0.7879134588671807\n",
      "Epoch 35, Training Loss: 0.7884297787695003\n",
      "Epoch 36, Training Loss: 0.7874742128795251\n",
      "Epoch 37, Training Loss: 0.7865848357516124\n",
      "Epoch 38, Training Loss: 0.7869327788066147\n",
      "Epoch 39, Training Loss: 0.7873788647185591\n",
      "Epoch 40, Training Loss: 0.7870474465807578\n",
      "Epoch 41, Training Loss: 0.7860877630405856\n",
      "Epoch 42, Training Loss: 0.7869104243758925\n",
      "Epoch 43, Training Loss: 0.7859246847324801\n",
      "Epoch 44, Training Loss: 0.7851123240657318\n",
      "Epoch 45, Training Loss: 0.786023171503741\n",
      "Epoch 46, Training Loss: 0.7863338235625648\n",
      "Epoch 47, Training Loss: 0.7863486977448141\n",
      "Epoch 48, Training Loss: 0.7865287349636393\n",
      "Epoch 49, Training Loss: 0.7860729000622169\n",
      "Epoch 50, Training Loss: 0.7855593561229849\n",
      "Epoch 51, Training Loss: 0.7855545814772298\n",
      "Epoch 52, Training Loss: 0.786443995712395\n",
      "Epoch 53, Training Loss: 0.7853145897836613\n",
      "Epoch 54, Training Loss: 0.784412796067116\n",
      "Epoch 55, Training Loss: 0.7856077363616542\n",
      "Epoch 56, Training Loss: 0.7852250849393974\n",
      "Epoch 57, Training Loss: 0.7863948615870081\n",
      "Epoch 58, Training Loss: 0.7855568275415807\n",
      "Epoch 59, Training Loss: 0.7854347253204288\n",
      "Epoch 60, Training Loss: 0.7842059018020343\n",
      "Epoch 61, Training Loss: 0.7848442264069292\n",
      "Epoch 62, Training Loss: 0.7849290048269401\n",
      "Epoch 63, Training Loss: 0.7844048285842838\n",
      "Epoch 64, Training Loss: 0.7839055029969466\n",
      "Epoch 65, Training Loss: 0.7841427610332804\n",
      "Epoch 66, Training Loss: 0.7839273422284234\n",
      "Epoch 67, Training Loss: 0.7835666596889496\n",
      "Epoch 68, Training Loss: 0.7829340681097561\n",
      "Epoch 69, Training Loss: 0.7842428936994166\n",
      "Epoch 70, Training Loss: 0.783184748484676\n",
      "Epoch 71, Training Loss: 0.78285947849876\n",
      "Epoch 72, Training Loss: 0.783386534676516\n",
      "Epoch 73, Training Loss: 0.782768670239843\n",
      "Epoch 74, Training Loss: 0.7824562066479733\n",
      "Epoch 75, Training Loss: 0.782875733178361\n",
      "Epoch 76, Training Loss: 0.7840125751674624\n",
      "Epoch 77, Training Loss: 0.7825357163759102\n",
      "Epoch 78, Training Loss: 0.7830121077989277\n",
      "Epoch 79, Training Loss: 0.7822945509638105\n",
      "Epoch 80, Training Loss: 0.7822353810296023\n",
      "Epoch 81, Training Loss: 0.7826592083264114\n",
      "Epoch 82, Training Loss: 0.7824099077317948\n",
      "Epoch 83, Training Loss: 0.7818934843056184\n",
      "Epoch 84, Training Loss: 0.7830189558796417\n",
      "Epoch 85, Training Loss: 0.7824630829624664\n",
      "Epoch 86, Training Loss: 0.7828084987805302\n",
      "Epoch 87, Training Loss: 0.7813963927720723\n",
      "Epoch 88, Training Loss: 0.7815908416769558\n",
      "Epoch 89, Training Loss: 0.7819956346562035\n",
      "Epoch 90, Training Loss: 0.7813833500657763\n",
      "Epoch 91, Training Loss: 0.7822698341276413\n",
      "Epoch 92, Training Loss: 0.7813675824860881\n",
      "Epoch 93, Training Loss: 0.7816488386993121\n",
      "Epoch 94, Training Loss: 0.7808793175489382\n",
      "Epoch 95, Training Loss: 0.7823376086421479\n",
      "Epoch 96, Training Loss: 0.7809708990548786\n",
      "Epoch 97, Training Loss: 0.7823834880850369\n",
      "Epoch 98, Training Loss: 0.7819779223965523\n",
      "Epoch 99, Training Loss: 0.7810507724159642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:20:01,838] Trial 263 finished with value: 0.6383333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7813040810420101\n",
      "Epoch 1, Training Loss: 0.851366126537323\n",
      "Epoch 2, Training Loss: 0.8213164730179578\n",
      "Epoch 3, Training Loss: 0.8182411624076671\n",
      "Epoch 4, Training Loss: 0.8176935229982648\n",
      "Epoch 5, Training Loss: 0.8126819707397231\n",
      "Epoch 6, Training Loss: 0.8109369286020895\n",
      "Epoch 7, Training Loss: 0.8123630791678464\n",
      "Epoch 8, Training Loss: 0.8098140690559731\n",
      "Epoch 9, Training Loss: 0.8101694810659366\n",
      "Epoch 10, Training Loss: 0.81192579511413\n",
      "Epoch 11, Training Loss: 0.8097379606469233\n",
      "Epoch 12, Training Loss: 0.8091830547590901\n",
      "Epoch 13, Training Loss: 0.81044099922467\n",
      "Epoch 14, Training Loss: 0.8100814767349932\n",
      "Epoch 15, Training Loss: 0.8087628637937675\n",
      "Epoch 16, Training Loss: 0.8091556533834988\n",
      "Epoch 17, Training Loss: 0.808678116296467\n",
      "Epoch 18, Training Loss: 0.8097716980410697\n",
      "Epoch 19, Training Loss: 0.808301942420185\n",
      "Epoch 20, Training Loss: 0.8086408412546143\n",
      "Epoch 21, Training Loss: 0.8078176463456979\n",
      "Epoch 22, Training Loss: 0.8073740049412376\n",
      "Epoch 23, Training Loss: 0.8069217207736539\n",
      "Epoch 24, Training Loss: 0.8072473257107842\n",
      "Epoch 25, Training Loss: 0.8074888465996075\n",
      "Epoch 26, Training Loss: 0.8069460452499246\n",
      "Epoch 27, Training Loss: 0.8072840803547909\n",
      "Epoch 28, Training Loss: 0.8054073982668999\n",
      "Epoch 29, Training Loss: 0.8060397826639333\n",
      "Epoch 30, Training Loss: 0.8044559370306201\n",
      "Epoch 31, Training Loss: 0.8044240748075614\n",
      "Epoch 32, Training Loss: 0.80560732111895\n",
      "Epoch 33, Training Loss: 0.8035909352445961\n",
      "Epoch 34, Training Loss: 0.805252518689722\n",
      "Epoch 35, Training Loss: 0.8036357063099854\n",
      "Epoch 36, Training Loss: 0.8028312842648728\n",
      "Epoch 37, Training Loss: 0.8032440241118123\n",
      "Epoch 38, Training Loss: 0.8046080650243544\n",
      "Epoch 39, Training Loss: 0.8037996269706497\n",
      "Epoch 40, Training Loss: 0.8018678151575246\n",
      "Epoch 41, Training Loss: 0.8034320118732022\n",
      "Epoch 42, Training Loss: 0.8009529488427298\n",
      "Epoch 43, Training Loss: 0.802363625953072\n",
      "Epoch 44, Training Loss: 0.8026791792166861\n",
      "Epoch 45, Training Loss: 0.800985378519933\n",
      "Epoch 46, Training Loss: 0.8022092299353808\n",
      "Epoch 47, Training Loss: 0.8007361553665391\n",
      "Epoch 48, Training Loss: 0.8003882089951881\n",
      "Epoch 49, Training Loss: 0.8003378359895004\n",
      "Epoch 50, Training Loss: 0.8015506807126497\n",
      "Epoch 51, Training Loss: 0.8010800937960919\n",
      "Epoch 52, Training Loss: 0.8008298705395003\n",
      "Epoch 53, Training Loss: 0.8000353053996437\n",
      "Epoch 54, Training Loss: 0.8009946382135377\n",
      "Epoch 55, Training Loss: 0.8001565099658823\n",
      "Epoch 56, Training Loss: 0.7990087100437709\n",
      "Epoch 57, Training Loss: 0.7972291805690392\n",
      "Epoch 58, Training Loss: 0.8020419716835022\n",
      "Epoch 59, Training Loss: 0.7999930414938389\n",
      "Epoch 60, Training Loss: 0.8008214461176019\n",
      "Epoch 61, Training Loss: 0.8004607114576756\n",
      "Epoch 62, Training Loss: 0.7979223098073687\n",
      "Epoch 63, Training Loss: 0.7993976869081196\n",
      "Epoch 64, Training Loss: 0.799111507171975\n",
      "Epoch 65, Training Loss: 0.7985227642202736\n",
      "Epoch 66, Training Loss: 0.7993787631056363\n",
      "Epoch 67, Training Loss: 0.7991081375824778\n",
      "Epoch 68, Training Loss: 0.7985329848483093\n",
      "Epoch 69, Training Loss: 0.8000672196087084\n",
      "Epoch 70, Training Loss: 0.7984094634091944\n",
      "Epoch 71, Training Loss: 0.7997607752792817\n",
      "Epoch 72, Training Loss: 0.79809967261508\n",
      "Epoch 73, Training Loss: 0.7988476662707508\n",
      "Epoch 74, Training Loss: 0.7980652605680595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:21:15,998] Trial 264 finished with value: 0.6302666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7977911504587732\n",
      "Epoch 1, Training Loss: 0.8872784036047319\n",
      "Epoch 2, Training Loss: 0.8379197603814742\n",
      "Epoch 3, Training Loss: 0.8303373500178842\n",
      "Epoch 4, Training Loss: 0.8273462456815383\n",
      "Epoch 5, Training Loss: 0.8239612908924328\n",
      "Epoch 6, Training Loss: 0.8229475667897392\n",
      "Epoch 7, Training Loss: 0.8220830953121185\n",
      "Epoch 8, Training Loss: 0.8204871902045081\n",
      "Epoch 9, Training Loss: 0.820616213574129\n",
      "Epoch 10, Training Loss: 0.8204896025096668\n",
      "Epoch 11, Training Loss: 0.8215907018324908\n",
      "Epoch 12, Training Loss: 0.8179927229881286\n",
      "Epoch 13, Training Loss: 0.8196895032069262\n",
      "Epoch 14, Training Loss: 0.8197556769146639\n",
      "Epoch 15, Training Loss: 0.8192389232270858\n",
      "Epoch 16, Training Loss: 0.8172360512789558\n",
      "Epoch 17, Training Loss: 0.8177179241881651\n",
      "Epoch 18, Training Loss: 0.818359860532424\n",
      "Epoch 19, Training Loss: 0.817072912384482\n",
      "Epoch 20, Training Loss: 0.8164318003373987\n",
      "Epoch 21, Training Loss: 0.8168451563750996\n",
      "Epoch 22, Training Loss: 0.8172322587405934\n",
      "Epoch 23, Training Loss: 0.818441009942223\n",
      "Epoch 24, Training Loss: 0.8156174102250268\n",
      "Epoch 25, Training Loss: 0.8171913060721229\n",
      "Epoch 26, Training Loss: 0.8174192643165589\n",
      "Epoch 27, Training Loss: 0.8165294403188369\n",
      "Epoch 28, Training Loss: 0.8166834681174334\n",
      "Epoch 29, Training Loss: 0.8167598059598138\n",
      "Epoch 30, Training Loss: 0.8144727934108061\n",
      "Epoch 31, Training Loss: 0.8165119246174307\n",
      "Epoch 32, Training Loss: 0.8156846016996047\n",
      "Epoch 33, Training Loss: 0.8149029313816744\n",
      "Epoch 34, Training Loss: 0.8146304885078879\n",
      "Epoch 35, Training Loss: 0.8132044722753412\n",
      "Epoch 36, Training Loss: 0.8133835698576535\n",
      "Epoch 37, Training Loss: 0.8124062390888438\n",
      "Epoch 38, Training Loss: 0.8114836607259862\n",
      "Epoch 39, Training Loss: 0.8138317759598003\n",
      "Epoch 40, Training Loss: 0.8127323809090783\n",
      "Epoch 41, Training Loss: 0.8140048078228446\n",
      "Epoch 42, Training Loss: 0.8136098692697638\n",
      "Epoch 43, Training Loss: 0.8101740494896383\n",
      "Epoch 44, Training Loss: 0.8109145737395567\n",
      "Epoch 45, Training Loss: 0.8121535714233623\n",
      "Epoch 46, Training Loss: 0.8120739323952618\n",
      "Epoch 47, Training Loss: 0.8155678689479828\n",
      "Epoch 48, Training Loss: 0.8092487358345705\n",
      "Epoch 49, Training Loss: 0.8114776250194101\n",
      "Epoch 50, Training Loss: 0.8119474002894234\n",
      "Epoch 51, Training Loss: 0.8087026219508228\n",
      "Epoch 52, Training Loss: 0.8126389316250296\n",
      "Epoch 53, Training Loss: 0.8114826736730688\n",
      "Epoch 54, Training Loss: 0.8093659243162941\n",
      "Epoch 55, Training Loss: 0.81054852085955\n",
      "Epoch 56, Training Loss: 0.81142552838606\n",
      "Epoch 57, Training Loss: 0.8110605571550481\n",
      "Epoch 58, Training Loss: 0.809324142371907\n",
      "Epoch 59, Training Loss: 0.8093416370363796\n",
      "Epoch 60, Training Loss: 0.8097395944595337\n",
      "Epoch 61, Training Loss: 0.8094216173536637\n",
      "Epoch 62, Training Loss: 0.8112344558799968\n",
      "Epoch 63, Training Loss: 0.8088637124790865\n",
      "Epoch 64, Training Loss: 0.8070893096923828\n",
      "Epoch 65, Training Loss: 0.8098732157314525\n",
      "Epoch 66, Training Loss: 0.8081356908293331\n",
      "Epoch 67, Training Loss: 0.8076959119123571\n",
      "Epoch 68, Training Loss: 0.8074811934723574\n",
      "Epoch 69, Training Loss: 0.8080937750900493\n",
      "Epoch 70, Training Loss: 0.8072475253834444\n",
      "Epoch 71, Training Loss: 0.8075621317414676\n",
      "Epoch 72, Training Loss: 0.8072166926019332\n",
      "Epoch 73, Training Loss: 0.8094378192284528\n",
      "Epoch 74, Training Loss: 0.807012482180315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:22:40,304] Trial 265 finished with value: 0.6312 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8087890358532176\n",
      "Epoch 1, Training Loss: 0.8551004719913454\n",
      "Epoch 2, Training Loss: 0.8198704489191672\n",
      "Epoch 3, Training Loss: 0.815505570038817\n",
      "Epoch 4, Training Loss: 0.8112225379262652\n",
      "Epoch 5, Training Loss: 0.809127073987086\n",
      "Epoch 6, Training Loss: 0.8069040018813054\n",
      "Epoch 7, Training Loss: 0.8050498297339992\n",
      "Epoch 8, Training Loss: 0.8032401135093288\n",
      "Epoch 9, Training Loss: 0.8033211878367833\n",
      "Epoch 10, Training Loss: 0.8022619197243138\n",
      "Epoch 11, Training Loss: 0.801955494934455\n",
      "Epoch 12, Training Loss: 0.8009912323234673\n",
      "Epoch 13, Training Loss: 0.8016019507458335\n",
      "Epoch 14, Training Loss: 0.7996513840399291\n",
      "Epoch 15, Training Loss: 0.8003097423933503\n",
      "Epoch 16, Training Loss: 0.798748284892032\n",
      "Epoch 17, Training Loss: 0.7996639769776424\n",
      "Epoch 18, Training Loss: 0.7986458180542279\n",
      "Epoch 19, Training Loss: 0.7987341678232178\n",
      "Epoch 20, Training Loss: 0.7985683116697727\n",
      "Epoch 21, Training Loss: 0.7978939932988103\n",
      "Epoch 22, Training Loss: 0.7978117712458274\n",
      "Epoch 23, Training Loss: 0.7980391350903906\n",
      "Epoch 24, Training Loss: 0.7973334264038201\n",
      "Epoch 25, Training Loss: 0.7971337019052721\n",
      "Epoch 26, Training Loss: 0.7972292572932136\n",
      "Epoch 27, Training Loss: 0.7964068834046673\n",
      "Epoch 28, Training Loss: 0.7974753636166565\n",
      "Epoch 29, Training Loss: 0.7965116529536427\n",
      "Epoch 30, Training Loss: 0.7968645345895811\n",
      "Epoch 31, Training Loss: 0.7965341737395839\n",
      "Epoch 32, Training Loss: 0.7963548753494607\n",
      "Epoch 33, Training Loss: 0.7963493711966321\n",
      "Epoch 34, Training Loss: 0.7960705040092755\n",
      "Epoch 35, Training Loss: 0.7955343951856283\n",
      "Epoch 36, Training Loss: 0.7957929075212407\n",
      "Epoch 37, Training Loss: 0.7951665914148316\n",
      "Epoch 38, Training Loss: 0.7957884692607966\n",
      "Epoch 39, Training Loss: 0.7952777822214858\n",
      "Epoch 40, Training Loss: 0.7954735669874607\n",
      "Epoch 41, Training Loss: 0.7952508214721106\n",
      "Epoch 42, Training Loss: 0.7947258907153194\n",
      "Epoch 43, Training Loss: 0.794503827112958\n",
      "Epoch 44, Training Loss: 0.7947148324851703\n",
      "Epoch 45, Training Loss: 0.7944578281918863\n",
      "Epoch 46, Training Loss: 0.7954909262800576\n",
      "Epoch 47, Training Loss: 0.794720917357538\n",
      "Epoch 48, Training Loss: 0.7942805007884377\n",
      "Epoch 49, Training Loss: 0.7945639569060247\n",
      "Epoch 50, Training Loss: 0.7938682344174923\n",
      "Epoch 51, Training Loss: 0.7937983652254692\n",
      "Epoch 52, Training Loss: 0.7944094832678487\n",
      "Epoch 53, Training Loss: 0.7934689117553538\n",
      "Epoch 54, Training Loss: 0.7937537662965014\n",
      "Epoch 55, Training Loss: 0.793728558461469\n",
      "Epoch 56, Training Loss: 0.7940206218482857\n",
      "Epoch 57, Training Loss: 0.794110273687463\n",
      "Epoch 58, Training Loss: 0.793801418641456\n",
      "Epoch 59, Training Loss: 0.7933446968408455\n",
      "Epoch 60, Training Loss: 0.7931351949397782\n",
      "Epoch 61, Training Loss: 0.7936980645459397\n",
      "Epoch 62, Training Loss: 0.7936786765442755\n",
      "Epoch 63, Training Loss: 0.7928939757490516\n",
      "Epoch 64, Training Loss: 0.7927338090158047\n",
      "Epoch 65, Training Loss: 0.793120649076046\n",
      "Epoch 66, Training Loss: 0.7930239465899933\n",
      "Epoch 67, Training Loss: 0.79407936164311\n",
      "Epoch 68, Training Loss: 0.7923659477915083\n",
      "Epoch 69, Training Loss: 0.7928609900008466\n",
      "Epoch 70, Training Loss: 0.7927879518136046\n",
      "Epoch 71, Training Loss: 0.792936326686601\n",
      "Epoch 72, Training Loss: 0.7923922096876274\n",
      "Epoch 73, Training Loss: 0.7927057176604306\n",
      "Epoch 74, Training Loss: 0.7922188295457596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:23:51,649] Trial 266 finished with value: 0.6156 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7919227403805669\n",
      "Epoch 1, Training Loss: 0.8689761242472139\n",
      "Epoch 2, Training Loss: 0.8135682111395929\n",
      "Epoch 3, Training Loss: 0.8064235312598093\n",
      "Epoch 4, Training Loss: 0.8041117908363056\n",
      "Epoch 5, Training Loss: 0.8018115495380602\n",
      "Epoch 6, Training Loss: 0.7979000679532388\n",
      "Epoch 7, Training Loss: 0.7963081628756415\n",
      "Epoch 8, Training Loss: 0.7960034922549599\n",
      "Epoch 9, Training Loss: 0.796699593569103\n",
      "Epoch 10, Training Loss: 0.7955944518397625\n",
      "Epoch 11, Training Loss: 0.7940322648313709\n",
      "Epoch 12, Training Loss: 0.7943136568356277\n",
      "Epoch 13, Training Loss: 0.7925656046186175\n",
      "Epoch 14, Training Loss: 0.7931764554260369\n",
      "Epoch 15, Training Loss: 0.7934394523613435\n",
      "Epoch 16, Training Loss: 0.7913517810348282\n",
      "Epoch 17, Training Loss: 0.7917775964378414\n",
      "Epoch 18, Training Loss: 0.7924841157475808\n",
      "Epoch 19, Training Loss: 0.7913586100241295\n",
      "Epoch 20, Training Loss: 0.7911714533217867\n",
      "Epoch 21, Training Loss: 0.7907366735594613\n",
      "Epoch 22, Training Loss: 0.7899391039869839\n",
      "Epoch 23, Training Loss: 0.7913055666407248\n",
      "Epoch 24, Training Loss: 0.7893675524489324\n",
      "Epoch 25, Training Loss: 0.7902817206275194\n",
      "Epoch 26, Training Loss: 0.7885870514059425\n",
      "Epoch 27, Training Loss: 0.7884501430325042\n",
      "Epoch 28, Training Loss: 0.7890427985585722\n",
      "Epoch 29, Training Loss: 0.7893461832426545\n",
      "Epoch 30, Training Loss: 0.7892301659835013\n",
      "Epoch 31, Training Loss: 0.7881733680129948\n",
      "Epoch 32, Training Loss: 0.7881490793443264\n",
      "Epoch 33, Training Loss: 0.7866721118303169\n",
      "Epoch 34, Training Loss: 0.7876902271930436\n",
      "Epoch 35, Training Loss: 0.7872846788033507\n",
      "Epoch 36, Training Loss: 0.7872687033244542\n",
      "Epoch 37, Training Loss: 0.7872954416992073\n",
      "Epoch 38, Training Loss: 0.7866905574511764\n",
      "Epoch 39, Training Loss: 0.786472110237394\n",
      "Epoch 40, Training Loss: 0.7878045695168632\n",
      "Epoch 41, Training Loss: 0.787090405844208\n",
      "Epoch 42, Training Loss: 0.7859561704155198\n",
      "Epoch 43, Training Loss: 0.7860325314944848\n",
      "Epoch 44, Training Loss: 0.7858713595042551\n",
      "Epoch 45, Training Loss: 0.7861419205378769\n",
      "Epoch 46, Training Loss: 0.786180699767923\n",
      "Epoch 47, Training Loss: 0.7848420336730498\n",
      "Epoch 48, Training Loss: 0.7862872602348041\n",
      "Epoch 49, Training Loss: 0.7852322956673184\n",
      "Epoch 50, Training Loss: 0.7855336676862903\n",
      "Epoch 51, Training Loss: 0.7839715997975572\n",
      "Epoch 52, Training Loss: 0.7848591852905159\n",
      "Epoch 53, Training Loss: 0.7839676593479358\n",
      "Epoch 54, Training Loss: 0.7843012709366648\n",
      "Epoch 55, Training Loss: 0.7849968211991446\n",
      "Epoch 56, Training Loss: 0.7853345054432862\n",
      "Epoch 57, Training Loss: 0.7849648226472669\n",
      "Epoch 58, Training Loss: 0.7843155854626706\n",
      "Epoch 59, Training Loss: 0.7849624122892107\n",
      "Epoch 60, Training Loss: 0.7850167692155766\n",
      "Epoch 61, Training Loss: 0.7841402519914441\n",
      "Epoch 62, Training Loss: 0.7844954994388093\n",
      "Epoch 63, Training Loss: 0.784418327198889\n",
      "Epoch 64, Training Loss: 0.7834643834515622\n",
      "Epoch 65, Training Loss: 0.7844296821974274\n",
      "Epoch 66, Training Loss: 0.7834506250861892\n",
      "Epoch 67, Training Loss: 0.7842215160678204\n",
      "Epoch 68, Training Loss: 0.783884104570948\n",
      "Epoch 69, Training Loss: 0.7838392116969689\n",
      "Epoch 70, Training Loss: 0.7834883282059117\n",
      "Epoch 71, Training Loss: 0.783594058241163\n",
      "Epoch 72, Training Loss: 0.7835700976221185\n",
      "Epoch 73, Training Loss: 0.7837331670567506\n",
      "Epoch 74, Training Loss: 0.7835261353872772\n",
      "Epoch 75, Training Loss: 0.7821194262881028\n",
      "Epoch 76, Training Loss: 0.7822161954148371\n",
      "Epoch 77, Training Loss: 0.782694143191316\n",
      "Epoch 78, Training Loss: 0.7825032209095202\n",
      "Epoch 79, Training Loss: 0.7812604059402207\n",
      "Epoch 80, Training Loss: 0.7832942171204359\n",
      "Epoch 81, Training Loss: 0.782049254367226\n",
      "Epoch 82, Training Loss: 0.7822613924965822\n",
      "Epoch 83, Training Loss: 0.7829247151102339\n",
      "Epoch 84, Training Loss: 0.7821255983266615\n",
      "Epoch 85, Training Loss: 0.7816021323204041\n",
      "Epoch 86, Training Loss: 0.783086695706934\n",
      "Epoch 87, Training Loss: 0.7817232170499357\n",
      "Epoch 88, Training Loss: 0.7818302639444968\n",
      "Epoch 89, Training Loss: 0.7813005165049904\n",
      "Epoch 90, Training Loss: 0.7829462251268832\n",
      "Epoch 91, Training Loss: 0.7815940242960937\n",
      "Epoch 92, Training Loss: 0.7811445189597911\n",
      "Epoch 93, Training Loss: 0.78125325486176\n",
      "Epoch 94, Training Loss: 0.7809272965990511\n",
      "Epoch 95, Training Loss: 0.7822260304501182\n",
      "Epoch 96, Training Loss: 0.7813067169565904\n",
      "Epoch 97, Training Loss: 0.7819916255492017\n",
      "Epoch 98, Training Loss: 0.7825542017033226\n",
      "Epoch 99, Training Loss: 0.780963879509976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:25:48,975] Trial 267 finished with value: 0.6366 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7815322321160395\n",
      "Epoch 1, Training Loss: 1.0701797607249783\n",
      "Epoch 2, Training Loss: 1.0307869185182386\n",
      "Epoch 3, Training Loss: 1.0043757479889948\n",
      "Epoch 4, Training Loss: 0.9862161943787022\n",
      "Epoch 5, Training Loss: 0.9746811802225902\n",
      "Epoch 6, Training Loss: 0.9675918685762506\n",
      "Epoch 7, Training Loss: 0.9631604041372027\n",
      "Epoch 8, Training Loss: 0.9594866020338876\n",
      "Epoch 9, Training Loss: 0.9581907162988993\n",
      "Epoch 10, Training Loss: 0.9563309964380766\n",
      "Epoch 11, Training Loss: 0.9553888632838887\n",
      "Epoch 12, Training Loss: 0.9545204107026408\n",
      "Epoch 13, Training Loss: 0.9535385921485442\n",
      "Epoch 14, Training Loss: 0.9521248215123227\n",
      "Epoch 15, Training Loss: 0.9510571618725483\n",
      "Epoch 16, Training Loss: 0.9506385956491743\n",
      "Epoch 17, Training Loss: 0.9494056295631523\n",
      "Epoch 18, Training Loss: 0.94875094334882\n",
      "Epoch 19, Training Loss: 0.9472906032899269\n",
      "Epoch 20, Training Loss: 0.9464657291433864\n",
      "Epoch 21, Training Loss: 0.9457869080672586\n",
      "Epoch 22, Training Loss: 0.9446683255353369\n",
      "Epoch 23, Training Loss: 0.9436378209214461\n",
      "Epoch 24, Training Loss: 0.942370985862904\n",
      "Epoch 25, Training Loss: 0.9415974083699679\n",
      "Epoch 26, Training Loss: 0.9405519028355305\n",
      "Epoch 27, Training Loss: 0.9395027036953689\n",
      "Epoch 28, Training Loss: 0.9387091534478323\n",
      "Epoch 29, Training Loss: 0.9369486198389441\n",
      "Epoch 30, Training Loss: 0.9358324809181959\n",
      "Epoch 31, Training Loss: 0.9347194003879575\n",
      "Epoch 32, Training Loss: 0.9334746674487465\n",
      "Epoch 33, Training Loss: 0.9318251086356945\n",
      "Epoch 34, Training Loss: 0.9314167389296051\n",
      "Epoch 35, Training Loss: 0.9295333744888019\n",
      "Epoch 36, Training Loss: 0.928136738709041\n",
      "Epoch 37, Training Loss: 0.9274124346281353\n",
      "Epoch 38, Training Loss: 0.9256894258628214\n",
      "Epoch 39, Training Loss: 0.9241430559552701\n",
      "Epoch 40, Training Loss: 0.9225655259046339\n",
      "Epoch 41, Training Loss: 0.9214205808209297\n",
      "Epoch 42, Training Loss: 0.9197135355239524\n",
      "Epoch 43, Training Loss: 0.9187653728893825\n",
      "Epoch 44, Training Loss: 0.9165914065855786\n",
      "Epoch 45, Training Loss: 0.9154776712109272\n",
      "Epoch 46, Training Loss: 0.9138246374022692\n",
      "Epoch 47, Training Loss: 0.912062560705314\n",
      "Epoch 48, Training Loss: 0.910507180995511\n",
      "Epoch 49, Training Loss: 0.9088681520375991\n",
      "Epoch 50, Training Loss: 0.9073890856334141\n",
      "Epoch 51, Training Loss: 0.9055781566110769\n",
      "Epoch 52, Training Loss: 0.9039032847361457\n",
      "Epoch 53, Training Loss: 0.9023173291880385\n",
      "Epoch 54, Training Loss: 0.9004755354465399\n",
      "Epoch 55, Training Loss: 0.8990593246947554\n",
      "Epoch 56, Training Loss: 0.8970502425853472\n",
      "Epoch 57, Training Loss: 0.8949880706636529\n",
      "Epoch 58, Training Loss: 0.893542462542541\n",
      "Epoch 59, Training Loss: 0.8915587276444399\n",
      "Epoch 60, Training Loss: 0.8901103976973914\n",
      "Epoch 61, Training Loss: 0.8876039185022053\n",
      "Epoch 62, Training Loss: 0.8867122905594962\n",
      "Epoch 63, Training Loss: 0.8848924601884712\n",
      "Epoch 64, Training Loss: 0.8827801854090583\n",
      "Epoch 65, Training Loss: 0.8807038999141608\n",
      "Epoch 66, Training Loss: 0.8796227296492211\n",
      "Epoch 67, Training Loss: 0.877341275914271\n",
      "Epoch 68, Training Loss: 0.8757279370064126\n",
      "Epoch 69, Training Loss: 0.8744262601199903\n",
      "Epoch 70, Training Loss: 0.8721896989901263\n",
      "Epoch 71, Training Loss: 0.8710693471413806\n",
      "Epoch 72, Training Loss: 0.8694292629571786\n",
      "Epoch 73, Training Loss: 0.8677104102041489\n",
      "Epoch 74, Training Loss: 0.8659788094068829\n",
      "Epoch 75, Training Loss: 0.8642525065214114\n",
      "Epoch 76, Training Loss: 0.8625889336256156\n",
      "Epoch 77, Training Loss: 0.8613511607162935\n",
      "Epoch 78, Training Loss: 0.8599592688388394\n",
      "Epoch 79, Training Loss: 0.8577029453184372\n",
      "Epoch 80, Training Loss: 0.8566479408651366\n",
      "Epoch 81, Training Loss: 0.8547605287759824\n",
      "Epoch 82, Training Loss: 0.8533557441897859\n",
      "Epoch 83, Training Loss: 0.8521551229003677\n",
      "Epoch 84, Training Loss: 0.851203631727319\n",
      "Epoch 85, Training Loss: 0.8493877421644397\n",
      "Epoch 86, Training Loss: 0.8485515582830386\n",
      "Epoch 87, Training Loss: 0.8463458814567193\n",
      "Epoch 88, Training Loss: 0.8458144464887174\n",
      "Epoch 89, Training Loss: 0.8442827300917833\n",
      "Epoch 90, Training Loss: 0.8431994193478635\n",
      "Epoch 91, Training Loss: 0.8421828525406974\n",
      "Epoch 92, Training Loss: 0.8409566604105153\n",
      "Epoch 93, Training Loss: 0.8390112630406716\n",
      "Epoch 94, Training Loss: 0.8381438708843145\n",
      "Epoch 95, Training Loss: 0.8372042430970902\n",
      "Epoch 96, Training Loss: 0.8360656486418014\n",
      "Epoch 97, Training Loss: 0.8350964345430073\n",
      "Epoch 98, Training Loss: 0.8341859938506794\n",
      "Epoch 99, Training Loss: 0.8331187701763068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:27:11,024] Trial 268 finished with value: 0.6152666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8325996487660515\n",
      "Epoch 1, Training Loss: 0.930630780252299\n",
      "Epoch 2, Training Loss: 0.8813616816262554\n",
      "Epoch 3, Training Loss: 0.844911645677753\n",
      "Epoch 4, Training Loss: 0.8223852315343412\n",
      "Epoch 5, Training Loss: 0.8120746269261927\n",
      "Epoch 6, Training Loss: 0.8074160427975475\n",
      "Epoch 7, Training Loss: 0.8057014505666001\n",
      "Epoch 8, Training Loss: 0.804176856611008\n",
      "Epoch 9, Training Loss: 0.8030874550790715\n",
      "Epoch 10, Training Loss: 0.8024623768670218\n",
      "Epoch 11, Training Loss: 0.8020929577655362\n",
      "Epoch 12, Training Loss: 0.8021126600136435\n",
      "Epoch 13, Training Loss: 0.8016739731444452\n",
      "Epoch 14, Training Loss: 0.8009321654649605\n",
      "Epoch 15, Training Loss: 0.8002482626671181\n",
      "Epoch 16, Training Loss: 0.800050379548754\n",
      "Epoch 17, Training Loss: 0.799989718781378\n",
      "Epoch 18, Training Loss: 0.7995329862250421\n",
      "Epoch 19, Training Loss: 0.799603093297858\n",
      "Epoch 20, Training Loss: 0.7990512183734348\n",
      "Epoch 21, Training Loss: 0.799992156835427\n",
      "Epoch 22, Training Loss: 0.7982598665961645\n",
      "Epoch 23, Training Loss: 0.7991436171352415\n",
      "Epoch 24, Training Loss: 0.7982246279716492\n",
      "Epoch 25, Training Loss: 0.7978208733680553\n",
      "Epoch 26, Training Loss: 0.7975441555331524\n",
      "Epoch 27, Training Loss: 0.7978343735063883\n",
      "Epoch 28, Training Loss: 0.7966640937597231\n",
      "Epoch 29, Training Loss: 0.7976770995254804\n",
      "Epoch 30, Training Loss: 0.796701152880389\n",
      "Epoch 31, Training Loss: 0.7968409865422357\n",
      "Epoch 32, Training Loss: 0.7958159033517193\n",
      "Epoch 33, Training Loss: 0.7961424002970071\n",
      "Epoch 34, Training Loss: 0.7962029584368369\n",
      "Epoch 35, Training Loss: 0.7960266115970182\n",
      "Epoch 36, Training Loss: 0.7947038584185723\n",
      "Epoch 37, Training Loss: 0.7951676159873045\n",
      "Epoch 38, Training Loss: 0.7944454755101885\n",
      "Epoch 39, Training Loss: 0.7939648993033216\n",
      "Epoch 40, Training Loss: 0.7939243883118594\n",
      "Epoch 41, Training Loss: 0.7934863406016415\n",
      "Epoch 42, Training Loss: 0.7926158597594813\n",
      "Epoch 43, Training Loss: 0.7930508901302079\n",
      "Epoch 44, Training Loss: 0.7923271192643876\n",
      "Epoch 45, Training Loss: 0.7921728012257052\n",
      "Epoch 46, Training Loss: 0.7926762004543965\n",
      "Epoch 47, Training Loss: 0.7914562072072711\n",
      "Epoch 48, Training Loss: 0.791281441100558\n",
      "Epoch 49, Training Loss: 0.7917040926173218\n",
      "Epoch 50, Training Loss: 0.7912535027453774\n",
      "Epoch 51, Training Loss: 0.7909474518962373\n",
      "Epoch 52, Training Loss: 0.7911548234466323\n",
      "Epoch 53, Training Loss: 0.791090136183832\n",
      "Epoch 54, Training Loss: 0.7906178165199165\n",
      "Epoch 55, Training Loss: 0.7907221622933123\n",
      "Epoch 56, Training Loss: 0.7910233030641886\n",
      "Epoch 57, Training Loss: 0.7912082098480454\n",
      "Epoch 58, Training Loss: 0.7910556486674718\n",
      "Epoch 59, Training Loss: 0.7901753692698658\n",
      "Epoch 60, Training Loss: 0.7901640859761633\n",
      "Epoch 61, Training Loss: 0.7907039358203573\n",
      "Epoch 62, Training Loss: 0.7904367170835797\n",
      "Epoch 63, Training Loss: 0.7903045327143562\n",
      "Epoch 64, Training Loss: 0.7906606552296115\n",
      "Epoch 65, Training Loss: 0.7903459011163927\n",
      "Epoch 66, Training Loss: 0.7904206695413231\n",
      "Epoch 67, Training Loss: 0.7908536367846611\n",
      "Epoch 68, Training Loss: 0.7897372627616825\n",
      "Epoch 69, Training Loss: 0.7895089452428029\n",
      "Epoch 70, Training Loss: 0.7897608337097598\n",
      "Epoch 71, Training Loss: 0.79025730575834\n",
      "Epoch 72, Training Loss: 0.790111926265229\n",
      "Epoch 73, Training Loss: 0.7904092499188015\n",
      "Epoch 74, Training Loss: 0.7910513206532127\n",
      "Epoch 75, Training Loss: 0.789664604251546\n",
      "Epoch 76, Training Loss: 0.789851947476093\n",
      "Epoch 77, Training Loss: 0.7900713656181679\n",
      "Epoch 78, Training Loss: 0.7898199827151191\n",
      "Epoch 79, Training Loss: 0.7895229835259286\n",
      "Epoch 80, Training Loss: 0.7902065621282821\n",
      "Epoch 81, Training Loss: 0.7900522646151091\n",
      "Epoch 82, Training Loss: 0.7897625654263604\n",
      "Epoch 83, Training Loss: 0.7899831713590407\n",
      "Epoch 84, Training Loss: 0.790246758156253\n",
      "Epoch 85, Training Loss: 0.789944284840634\n",
      "Epoch 86, Training Loss: 0.7904289956379654\n",
      "Epoch 87, Training Loss: 0.7890546894611272\n",
      "Epoch 88, Training Loss: 0.7895607679410088\n",
      "Epoch 89, Training Loss: 0.7898107559161078\n",
      "Epoch 90, Training Loss: 0.7891018316261751\n",
      "Epoch 91, Training Loss: 0.7893529733320824\n",
      "Epoch 92, Training Loss: 0.7900886275714502\n",
      "Epoch 93, Training Loss: 0.7893465103063368\n",
      "Epoch 94, Training Loss: 0.7887978350309501\n",
      "Epoch 95, Training Loss: 0.7899646926643257\n",
      "Epoch 96, Training Loss: 0.7893197909333652\n",
      "Epoch 97, Training Loss: 0.7894334840595274\n",
      "Epoch 98, Training Loss: 0.789356914050597\n",
      "Epoch 99, Training Loss: 0.789582591935208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:28:43,466] Trial 269 finished with value: 0.6387333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7895553049288297\n",
      "Epoch 1, Training Loss: 0.9194305091745714\n",
      "Epoch 2, Training Loss: 0.8511691958062789\n",
      "Epoch 3, Training Loss: 0.8219013150299297\n",
      "Epoch 4, Training Loss: 0.8133483266129213\n",
      "Epoch 5, Training Loss: 0.8107014444295098\n",
      "Epoch 6, Training Loss: 0.8097141092665056\n",
      "Epoch 7, Training Loss: 0.8086084647038404\n",
      "Epoch 8, Training Loss: 0.8079476158057942\n",
      "Epoch 9, Training Loss: 0.8073891520500183\n",
      "Epoch 10, Training Loss: 0.8068405523019678\n",
      "Epoch 11, Training Loss: 0.8066152413452373\n",
      "Epoch 12, Training Loss: 0.8064486838789547\n",
      "Epoch 13, Training Loss: 0.8057414973483367\n",
      "Epoch 14, Training Loss: 0.8054870829862707\n",
      "Epoch 15, Training Loss: 0.8051165689440335\n",
      "Epoch 16, Training Loss: 0.8047065667545095\n",
      "Epoch 17, Training Loss: 0.8043796011980842\n",
      "Epoch 18, Training Loss: 0.8041451153334449\n",
      "Epoch 19, Training Loss: 0.8040847727831673\n",
      "Epoch 20, Training Loss: 0.8037142961866716\n",
      "Epoch 21, Training Loss: 0.803471585301792\n",
      "Epoch 22, Training Loss: 0.8032273714682635\n",
      "Epoch 23, Training Loss: 0.8032141084530774\n",
      "Epoch 24, Training Loss: 0.8026273213414585\n",
      "Epoch 25, Training Loss: 0.802358261346817\n",
      "Epoch 26, Training Loss: 0.8024783818862018\n",
      "Epoch 27, Training Loss: 0.8021367244159474\n",
      "Epoch 28, Training Loss: 0.8019262825040256\n",
      "Epoch 29, Training Loss: 0.80190721876481\n",
      "Epoch 30, Training Loss: 0.8017301216546227\n",
      "Epoch 31, Training Loss: 0.801375537479625\n",
      "Epoch 32, Training Loss: 0.8013381126347711\n",
      "Epoch 33, Training Loss: 0.8010843106578378\n",
      "Epoch 34, Training Loss: 0.8011513191110947\n",
      "Epoch 35, Training Loss: 0.8006591936419992\n",
      "Epoch 36, Training Loss: 0.8006357672635247\n",
      "Epoch 37, Training Loss: 0.8002751828642453\n",
      "Epoch 38, Training Loss: 0.8004433664854835\n",
      "Epoch 39, Training Loss: 0.8002828316828784\n",
      "Epoch 40, Training Loss: 0.8002828347682953\n",
      "Epoch 41, Training Loss: 0.8000640059919918\n",
      "Epoch 42, Training Loss: 0.7998382315214942\n",
      "Epoch 43, Training Loss: 0.7996645334187676\n",
      "Epoch 44, Training Loss: 0.7999126913968254\n",
      "Epoch 45, Training Loss: 0.7996263910041136\n",
      "Epoch 46, Training Loss: 0.7995469786840327\n",
      "Epoch 47, Training Loss: 0.799229814305025\n",
      "Epoch 48, Training Loss: 0.7994305636602289\n",
      "Epoch 49, Training Loss: 0.7993624189320733\n",
      "Epoch 50, Training Loss: 0.7993147078682394\n",
      "Epoch 51, Training Loss: 0.7993714570999145\n",
      "Epoch 52, Training Loss: 0.7991881279384389\n",
      "Epoch 53, Training Loss: 0.7988336624117458\n",
      "Epoch 54, Training Loss: 0.7991141479856828\n",
      "Epoch 55, Training Loss: 0.798937778192408\n",
      "Epoch 56, Training Loss: 0.7987776109050302\n",
      "Epoch 57, Training Loss: 0.7989317147170796\n",
      "Epoch 58, Training Loss: 0.7988836925170001\n",
      "Epoch 59, Training Loss: 0.7987625241980834\n",
      "Epoch 60, Training Loss: 0.7985391085989335\n",
      "Epoch 61, Training Loss: 0.7987266721444971\n",
      "Epoch 62, Training Loss: 0.7985429859862608\n",
      "Epoch 63, Training Loss: 0.7987098022769479\n",
      "Epoch 64, Training Loss: 0.7983914416677812\n",
      "Epoch 65, Training Loss: 0.7984485446004307\n",
      "Epoch 66, Training Loss: 0.7979491983441745\n",
      "Epoch 67, Training Loss: 0.7984121786846834\n",
      "Epoch 68, Training Loss: 0.7981145129484288\n",
      "Epoch 69, Training Loss: 0.7984399061343249\n",
      "Epoch 70, Training Loss: 0.7980278304043938\n",
      "Epoch 71, Training Loss: 0.7981529667096979\n",
      "Epoch 72, Training Loss: 0.7981773398203008\n",
      "Epoch 73, Training Loss: 0.7979555714130402\n",
      "Epoch 74, Training Loss: 0.7979391892517315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:30:06,795] Trial 270 finished with value: 0.6336666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7980575306275312\n",
      "Epoch 1, Training Loss: 0.85930372189073\n",
      "Epoch 2, Training Loss: 0.8279239815824172\n",
      "Epoch 3, Training Loss: 0.8226667803876541\n",
      "Epoch 4, Training Loss: 0.8188548385395723\n",
      "Epoch 5, Training Loss: 0.8152859761434443\n",
      "Epoch 6, Training Loss: 0.813615200659808\n",
      "Epoch 7, Training Loss: 0.8116066905330209\n",
      "Epoch 8, Training Loss: 0.8104696911924025\n",
      "Epoch 9, Training Loss: 0.8105145201262306\n",
      "Epoch 10, Training Loss: 0.8090138156273786\n",
      "Epoch 11, Training Loss: 0.8093849985739764\n",
      "Epoch 12, Training Loss: 0.808923401481965\n",
      "Epoch 13, Training Loss: 0.8086095159194049\n",
      "Epoch 14, Training Loss: 0.8077487695217133\n",
      "Epoch 15, Training Loss: 0.8080297595613143\n",
      "Epoch 16, Training Loss: 0.8072693659277523\n",
      "Epoch 17, Training Loss: 0.8066949260234832\n",
      "Epoch 18, Training Loss: 0.8070857941403109\n",
      "Epoch 19, Training Loss: 0.8064429466163411\n",
      "Epoch 20, Training Loss: 0.8075350069999695\n",
      "Epoch 21, Training Loss: 0.8062177216305452\n",
      "Epoch 22, Training Loss: 0.806171236669316\n",
      "Epoch 23, Training Loss: 0.8059139390552745\n",
      "Epoch 24, Training Loss: 0.8059833119897282\n",
      "Epoch 25, Training Loss: 0.8057735495006337\n",
      "Epoch 26, Training Loss: 0.8053660144525415\n",
      "Epoch 27, Training Loss: 0.8055184852375704\n",
      "Epoch 28, Training Loss: 0.8057006359100342\n",
      "Epoch 29, Training Loss: 0.8050617517443264\n",
      "Epoch 30, Training Loss: 0.8045042953771704\n",
      "Epoch 31, Training Loss: 0.8043152476058287\n",
      "Epoch 32, Training Loss: 0.8049654086197123\n",
      "Epoch 33, Training Loss: 0.8039980597355787\n",
      "Epoch 34, Training Loss: 0.8039025530394386\n",
      "Epoch 35, Training Loss: 0.8033796176489661\n",
      "Epoch 36, Training Loss: 0.8037272963804357\n",
      "Epoch 37, Training Loss: 0.8032750229975757\n",
      "Epoch 38, Training Loss: 0.8026895087606767\n",
      "Epoch 39, Training Loss: 0.8027942416247199\n",
      "Epoch 40, Training Loss: 0.8027444555478938\n",
      "Epoch 41, Training Loss: 0.8026560091972351\n",
      "Epoch 42, Training Loss: 0.8024708501731648\n",
      "Epoch 43, Training Loss: 0.8020690035820007\n",
      "Epoch 44, Training Loss: 0.8017307644731858\n",
      "Epoch 45, Training Loss: 0.8015871676276712\n",
      "Epoch 46, Training Loss: 0.8014943263109993\n",
      "Epoch 47, Training Loss: 0.8015444268198574\n",
      "Epoch 48, Training Loss: 0.8009184918684118\n",
      "Epoch 49, Training Loss: 0.8007864414243138\n",
      "Epoch 50, Training Loss: 0.8007040061670191\n",
      "Epoch 51, Training Loss: 0.8004971659183502\n",
      "Epoch 52, Training Loss: 0.8001403657127829\n",
      "Epoch 53, Training Loss: 0.8002723057129804\n",
      "Epoch 54, Training Loss: 0.8000226597926196\n",
      "Epoch 55, Training Loss: 0.7996619279244367\n",
      "Epoch 56, Training Loss: 0.799955155358595\n",
      "Epoch 57, Training Loss: 0.799424301035264\n",
      "Epoch 58, Training Loss: 0.7990328858880436\n",
      "Epoch 59, Training Loss: 0.7988531376333797\n",
      "Epoch 60, Training Loss: 0.7987297326676985\n",
      "Epoch 61, Training Loss: 0.7991579766133252\n",
      "Epoch 62, Training Loss: 0.7981634629473967\n",
      "Epoch 63, Training Loss: 0.798348908143885\n",
      "Epoch 64, Training Loss: 0.7985434050419752\n",
      "Epoch 65, Training Loss: 0.7983972311019898\n",
      "Epoch 66, Training Loss: 0.7981273037545821\n",
      "Epoch 67, Training Loss: 0.7978507811882917\n",
      "Epoch 68, Training Loss: 0.7976975198353038\n",
      "Epoch 69, Training Loss: 0.798020972293966\n",
      "Epoch 70, Training Loss: 0.7973064440839431\n",
      "Epoch 71, Training Loss: 0.7976308631896972\n",
      "Epoch 72, Training Loss: 0.7973390009823967\n",
      "Epoch 73, Training Loss: 0.7966888684384963\n",
      "Epoch 74, Training Loss: 0.7966311245806077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:31:29,946] Trial 271 finished with value: 0.6335333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7966427947493161\n",
      "Epoch 1, Training Loss: 0.9594125493133769\n",
      "Epoch 2, Training Loss: 0.9226209474311156\n",
      "Epoch 3, Training Loss: 0.9095941021161921\n",
      "Epoch 4, Training Loss: 0.8972308905685649\n",
      "Epoch 5, Training Loss: 0.8833544025000404\n",
      "Epoch 6, Training Loss: 0.8679978425362531\n",
      "Epoch 7, Training Loss: 0.8526341022463406\n",
      "Epoch 8, Training Loss: 0.8391512224253486\n",
      "Epoch 9, Training Loss: 0.8285051068838905\n",
      "Epoch 10, Training Loss: 0.8207191512163947\n",
      "Epoch 11, Training Loss: 0.8150115766244777\n",
      "Epoch 12, Training Loss: 0.8115772170880261\n",
      "Epoch 13, Training Loss: 0.809058780670166\n",
      "Epoch 14, Training Loss: 0.8071887793260463\n",
      "Epoch 15, Training Loss: 0.8056206543305341\n",
      "Epoch 16, Training Loss: 0.8047092133409837\n",
      "Epoch 17, Training Loss: 0.8039833068847656\n",
      "Epoch 18, Training Loss: 0.8032946440051584\n",
      "Epoch 19, Training Loss: 0.802568295843461\n",
      "Epoch 20, Training Loss: 0.8021394224026623\n",
      "Epoch 21, Training Loss: 0.8018894797914169\n",
      "Epoch 22, Training Loss: 0.8016301337410422\n",
      "Epoch 23, Training Loss: 0.8014699022910174\n",
      "Epoch 24, Training Loss: 0.800880712481106\n",
      "Epoch 25, Training Loss: 0.8007910643605625\n",
      "Epoch 26, Training Loss: 0.8006545920932994\n",
      "Epoch 27, Training Loss: 0.8003250098929686\n",
      "Epoch 28, Training Loss: 0.8002228975997252\n",
      "Epoch 29, Training Loss: 0.7997981290957507\n",
      "Epoch 30, Training Loss: 0.7996027536251966\n",
      "Epoch 31, Training Loss: 0.7994206575786367\n",
      "Epoch 32, Training Loss: 0.7993630714977489\n",
      "Epoch 33, Training Loss: 0.7992495684763965\n",
      "Epoch 34, Training Loss: 0.7988174867630005\n",
      "Epoch 35, Training Loss: 0.7989767902037677\n",
      "Epoch 36, Training Loss: 0.7987417746291441\n",
      "Epoch 37, Training Loss: 0.7986226845488829\n",
      "Epoch 38, Training Loss: 0.7984322205711814\n",
      "Epoch 39, Training Loss: 0.798237810275134\n",
      "Epoch 40, Training Loss: 0.7983859182806576\n",
      "Epoch 41, Training Loss: 0.7982664468709161\n",
      "Epoch 42, Training Loss: 0.798268953070921\n",
      "Epoch 43, Training Loss: 0.7979982895710889\n",
      "Epoch 44, Training Loss: 0.7980568923669703\n",
      "Epoch 45, Training Loss: 0.7979514118503122\n",
      "Epoch 46, Training Loss: 0.7979747616543489\n",
      "Epoch 47, Training Loss: 0.7978086704366347\n",
      "Epoch 48, Training Loss: 0.7976976050348843\n",
      "Epoch 49, Training Loss: 0.7975329309351304\n",
      "Epoch 50, Training Loss: 0.797514202594757\n",
      "Epoch 51, Training Loss: 0.7976092337860781\n",
      "Epoch 52, Training Loss: 0.7974901697214912\n",
      "Epoch 53, Training Loss: 0.7973532070833094\n",
      "Epoch 54, Training Loss: 0.7970990097522735\n",
      "Epoch 55, Training Loss: 0.7972313436339883\n",
      "Epoch 56, Training Loss: 0.7970560400626239\n",
      "Epoch 57, Training Loss: 0.7971218280231251\n",
      "Epoch 58, Training Loss: 0.7969247423901278\n",
      "Epoch 59, Training Loss: 0.7971025611372555\n",
      "Epoch 60, Training Loss: 0.7969976120135364\n",
      "Epoch 61, Training Loss: 0.7969014788375182\n",
      "Epoch 62, Training Loss: 0.7967627074437983\n",
      "Epoch 63, Training Loss: 0.7968538490463706\n",
      "Epoch 64, Training Loss: 0.7966641135075513\n",
      "Epoch 65, Training Loss: 0.7966390686876633\n",
      "Epoch 66, Training Loss: 0.7965802410770865\n",
      "Epoch 67, Training Loss: 0.7965819499071907\n",
      "Epoch 68, Training Loss: 0.7964091122150421\n",
      "Epoch 69, Training Loss: 0.796460250195335\n",
      "Epoch 70, Training Loss: 0.7963015516365276\n",
      "Epoch 71, Training Loss: 0.7963827251686769\n",
      "Epoch 72, Training Loss: 0.7961606948515948\n",
      "Epoch 73, Training Loss: 0.7961278331279754\n",
      "Epoch 74, Training Loss: 0.7960600436435026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:32:41,178] Trial 272 finished with value: 0.6364666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7958485671351938\n",
      "Epoch 1, Training Loss: 0.9748259181836072\n",
      "Epoch 2, Training Loss: 0.9432195345093223\n",
      "Epoch 3, Training Loss: 0.9249204829159905\n",
      "Epoch 4, Training Loss: 0.9023036109699922\n",
      "Epoch 5, Training Loss: 0.8783735487741583\n",
      "Epoch 6, Training Loss: 0.8565400850071626\n",
      "Epoch 7, Training Loss: 0.8398887223355911\n",
      "Epoch 8, Training Loss: 0.8287084885905771\n",
      "Epoch 9, Training Loss: 0.8215907029544606\n",
      "Epoch 10, Training Loss: 0.8171452980882982\n",
      "Epoch 11, Training Loss: 0.8142799920194289\n",
      "Epoch 12, Training Loss: 0.8126027134586783\n",
      "Epoch 13, Training Loss: 0.8113003195734585\n",
      "Epoch 14, Training Loss: 0.8107104974634507\n",
      "Epoch 15, Training Loss: 0.8098081486365375\n",
      "Epoch 16, Training Loss: 0.8096111646119286\n",
      "Epoch 17, Training Loss: 0.8092608616632574\n",
      "Epoch 18, Training Loss: 0.8088693971493665\n",
      "Epoch 19, Training Loss: 0.8084267994235543\n",
      "Epoch 20, Training Loss: 0.8083306981535519\n",
      "Epoch 21, Training Loss: 0.8079882451365976\n",
      "Epoch 22, Training Loss: 0.8079215049743652\n",
      "Epoch 23, Training Loss: 0.8077350322639241\n",
      "Epoch 24, Training Loss: 0.8072685314627255\n",
      "Epoch 25, Training Loss: 0.8072971144844504\n",
      "Epoch 26, Training Loss: 0.8071429975593791\n",
      "Epoch 27, Training Loss: 0.8068116788303151\n",
      "Epoch 28, Training Loss: 0.8067160412143258\n",
      "Epoch 29, Training Loss: 0.806491670328028\n",
      "Epoch 30, Training Loss: 0.8061806842860053\n",
      "Epoch 31, Training Loss: 0.806339330182356\n",
      "Epoch 32, Training Loss: 0.8060437058701234\n",
      "Epoch 33, Training Loss: 0.8058112297338598\n",
      "Epoch 34, Training Loss: 0.8057354420072892\n",
      "Epoch 35, Training Loss: 0.8055395943978253\n",
      "Epoch 36, Training Loss: 0.8054074593852548\n",
      "Epoch 37, Training Loss: 0.8052015442707959\n",
      "Epoch 38, Training Loss: 0.805112110306235\n",
      "Epoch 39, Training Loss: 0.8048750827592962\n",
      "Epoch 40, Training Loss: 0.8046703769178951\n",
      "Epoch 41, Training Loss: 0.8045823270433089\n",
      "Epoch 42, Training Loss: 0.804483355143491\n",
      "Epoch 43, Training Loss: 0.8041671248744516\n",
      "Epoch 44, Training Loss: 0.8040917226146249\n",
      "Epoch 45, Training Loss: 0.8039891357281629\n",
      "Epoch 46, Training Loss: 0.8038185402926277\n",
      "Epoch 47, Training Loss: 0.8039021972347709\n",
      "Epoch 48, Training Loss: 0.8037876672604505\n",
      "Epoch 49, Training Loss: 0.8035565123137306\n",
      "Epoch 50, Training Loss: 0.8033943918873282\n",
      "Epoch 51, Training Loss: 0.8033392891463111\n",
      "Epoch 52, Training Loss: 0.8029575277777279\n",
      "Epoch 53, Training Loss: 0.8029711679150077\n",
      "Epoch 54, Training Loss: 0.8029505930227392\n",
      "Epoch 55, Training Loss: 0.8026921500879175\n",
      "Epoch 56, Training Loss: 0.8026002154630774\n",
      "Epoch 57, Training Loss: 0.8023379759227528\n",
      "Epoch 58, Training Loss: 0.8022022463994868\n",
      "Epoch 59, Training Loss: 0.8021688468316022\n",
      "Epoch 60, Training Loss: 0.8022541419197531\n",
      "Epoch 61, Training Loss: 0.8020072901950163\n",
      "Epoch 62, Training Loss: 0.8017104376764859\n",
      "Epoch 63, Training Loss: 0.8018419616362628\n",
      "Epoch 64, Training Loss: 0.801528390155119\n",
      "Epoch 65, Training Loss: 0.8015463225280537\n",
      "Epoch 66, Training Loss: 0.8014373380997601\n",
      "Epoch 67, Training Loss: 0.8011263965157902\n",
      "Epoch 68, Training Loss: 0.8011541706674239\n",
      "Epoch 69, Training Loss: 0.800856191480861\n",
      "Epoch 70, Training Loss: 0.8008377971368678\n",
      "Epoch 71, Training Loss: 0.8009232248278225\n",
      "Epoch 72, Training Loss: 0.8008776894036461\n",
      "Epoch 73, Training Loss: 0.8006156397567076\n",
      "Epoch 74, Training Loss: 0.8005459591220407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:34:06,256] Trial 273 finished with value: 0.6340666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8005167425380033\n",
      "Epoch 1, Training Loss: 1.0975185754603909\n",
      "Epoch 2, Training Loss: 1.0671959670862756\n",
      "Epoch 3, Training Loss: 1.0492135078387153\n",
      "Epoch 4, Training Loss: 1.0351771772355962\n",
      "Epoch 5, Training Loss: 1.0231642239972165\n",
      "Epoch 6, Training Loss: 1.0129126631227652\n",
      "Epoch 7, Training Loss: 1.0038595922011182\n",
      "Epoch 8, Training Loss: 0.995838479260753\n",
      "Epoch 9, Training Loss: 0.9888305716048506\n",
      "Epoch 10, Training Loss: 0.9827071597701624\n",
      "Epoch 11, Training Loss: 0.9769290086918307\n",
      "Epoch 12, Training Loss: 0.9722959711139364\n",
      "Epoch 13, Training Loss: 0.9683209210410154\n",
      "Epoch 14, Training Loss: 0.9643975430861451\n",
      "Epoch 15, Training Loss: 0.9616403741944105\n",
      "Epoch 16, Training Loss: 0.9590385955079157\n",
      "Epoch 17, Training Loss: 0.9567556824899257\n",
      "Epoch 18, Training Loss: 0.9549090240234719\n",
      "Epoch 19, Training Loss: 0.9525688924287495\n",
      "Epoch 20, Training Loss: 0.9511875587298457\n",
      "Epoch 21, Training Loss: 0.9499598315783909\n",
      "Epoch 22, Training Loss: 0.9485449878793014\n",
      "Epoch 23, Training Loss: 0.9473583454476263\n",
      "Epoch 24, Training Loss: 0.9462557851820064\n",
      "Epoch 25, Training Loss: 0.9454082863671439\n",
      "Epoch 26, Training Loss: 0.9442950217347396\n",
      "Epoch 27, Training Loss: 0.943582544739085\n",
      "Epoch 28, Training Loss: 0.9426931275460954\n",
      "Epoch 29, Training Loss: 0.9419481424460734\n",
      "Epoch 30, Training Loss: 0.9410717913978979\n",
      "Epoch 31, Training Loss: 0.9402545797197442\n",
      "Epoch 32, Training Loss: 0.9398080861658082\n",
      "Epoch 33, Training Loss: 0.9392286072100016\n",
      "Epoch 34, Training Loss: 0.9386071507195781\n",
      "Epoch 35, Training Loss: 0.9375926794862388\n",
      "Epoch 36, Training Loss: 0.9368784279751599\n",
      "Epoch 37, Training Loss: 0.9363958872350535\n",
      "Epoch 38, Training Loss: 0.9355010292583839\n",
      "Epoch 39, Training Loss: 0.9351232210496314\n",
      "Epoch 40, Training Loss: 0.934738258311623\n",
      "Epoch 41, Training Loss: 0.9339255025512294\n",
      "Epoch 42, Training Loss: 0.933508555064524\n",
      "Epoch 43, Training Loss: 0.9332742181039394\n",
      "Epoch 44, Training Loss: 0.9324545733910754\n",
      "Epoch 45, Training Loss: 0.9316733238392306\n",
      "Epoch 46, Training Loss: 0.9317854472569057\n",
      "Epoch 47, Training Loss: 0.9306789509335854\n",
      "Epoch 48, Training Loss: 0.9300477312023478\n",
      "Epoch 49, Training Loss: 0.9296143607985704\n",
      "Epoch 50, Training Loss: 0.9292180404627234\n",
      "Epoch 51, Training Loss: 0.9290819209321101\n",
      "Epoch 52, Training Loss: 0.9287407431387363\n",
      "Epoch 53, Training Loss: 0.92868795869942\n",
      "Epoch 54, Training Loss: 0.9277688719276199\n",
      "Epoch 55, Training Loss: 0.927107702699819\n",
      "Epoch 56, Training Loss: 0.9260922832596571\n",
      "Epoch 57, Training Loss: 0.926281928478327\n",
      "Epoch 58, Training Loss: 0.9256548445923884\n",
      "Epoch 59, Training Loss: 0.9258169512999685\n",
      "Epoch 60, Training Loss: 0.9244293871678804\n",
      "Epoch 61, Training Loss: 0.9242743675870106\n",
      "Epoch 62, Training Loss: 0.9237037395176134\n",
      "Epoch 63, Training Loss: 0.9231327756006915\n",
      "Epoch 64, Training Loss: 0.9225584048973886\n",
      "Epoch 65, Training Loss: 0.9227000747408186\n",
      "Epoch 66, Training Loss: 0.9224563709775307\n",
      "Epoch 67, Training Loss: 0.9216190694866324\n",
      "Epoch 68, Training Loss: 0.9215301749401523\n",
      "Epoch 69, Training Loss: 0.9206269292006816\n",
      "Epoch 70, Training Loss: 0.9207058830368787\n",
      "Epoch 71, Training Loss: 0.9199910670294797\n",
      "Epoch 72, Training Loss: 0.9199767496352805\n",
      "Epoch 73, Training Loss: 0.9190968184542835\n",
      "Epoch 74, Training Loss: 0.9188182948227216\n",
      "Epoch 75, Training Loss: 0.9188054517695778\n",
      "Epoch 76, Training Loss: 0.917992445311152\n",
      "Epoch 77, Training Loss: 0.9173883758093181\n",
      "Epoch 78, Training Loss: 0.916717459295029\n",
      "Epoch 79, Training Loss: 0.916281521320343\n",
      "Epoch 80, Training Loss: 0.9160798545170548\n",
      "Epoch 81, Training Loss: 0.9156276799682388\n",
      "Epoch 82, Training Loss: 0.9157224011600466\n",
      "Epoch 83, Training Loss: 0.915289728982108\n",
      "Epoch 84, Training Loss: 0.9150515418303641\n",
      "Epoch 85, Training Loss: 0.9143829142240654\n",
      "Epoch 86, Training Loss: 0.9140142073308615\n",
      "Epoch 87, Training Loss: 0.9132081796352128\n",
      "Epoch 88, Training Loss: 0.9132568624682893\n",
      "Epoch 89, Training Loss: 0.9127314575632712\n",
      "Epoch 90, Training Loss: 0.9123126318580226\n",
      "Epoch 91, Training Loss: 0.9119973374488658\n",
      "Epoch 92, Training Loss: 0.9113369947089288\n",
      "Epoch 93, Training Loss: 0.9110628042902266\n",
      "Epoch 94, Training Loss: 0.9102743250983102\n",
      "Epoch 95, Training Loss: 0.9102381316342748\n",
      "Epoch 96, Training Loss: 0.909645944878571\n",
      "Epoch 97, Training Loss: 0.9094083064480831\n",
      "Epoch 98, Training Loss: 0.9091168692237452\n",
      "Epoch 99, Training Loss: 0.908938599439492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:35:30,017] Trial 274 finished with value: 0.567 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.9081231343118767\n",
      "Epoch 1, Training Loss: 0.8724929748563206\n",
      "Epoch 2, Training Loss: 0.8177605259418488\n",
      "Epoch 3, Training Loss: 0.8127546749395482\n",
      "Epoch 4, Training Loss: 0.8116543028635137\n",
      "Epoch 5, Training Loss: 0.8106895645225749\n",
      "Epoch 6, Training Loss: 0.8098972413820379\n",
      "Epoch 7, Training Loss: 0.8090974464136012\n",
      "Epoch 8, Training Loss: 0.8073714394429151\n",
      "Epoch 9, Training Loss: 0.806490072222317\n",
      "Epoch 10, Training Loss: 0.8066433009680579\n",
      "Epoch 11, Training Loss: 0.8064516852883732\n",
      "Epoch 12, Training Loss: 0.8059477321540608\n",
      "Epoch 13, Training Loss: 0.8057908692780663\n",
      "Epoch 14, Training Loss: 0.8046909301421221\n",
      "Epoch 15, Training Loss: 0.8034373564579907\n",
      "Epoch 16, Training Loss: 0.8040225022680619\n",
      "Epoch 17, Training Loss: 0.8027195791637196\n",
      "Epoch 18, Training Loss: 0.8034942152219661\n",
      "Epoch 19, Training Loss: 0.802774114258149\n",
      "Epoch 20, Training Loss: 0.8030609708673814\n",
      "Epoch 21, Training Loss: 0.8026453500635484\n",
      "Epoch 22, Training Loss: 0.8020886785843793\n",
      "Epoch 23, Training Loss: 0.8007723613346325\n",
      "Epoch 24, Training Loss: 0.8015397101290086\n",
      "Epoch 25, Training Loss: 0.8011974812956417\n",
      "Epoch 26, Training Loss: 0.8002494342186872\n",
      "Epoch 27, Training Loss: 0.8001139363120584\n",
      "Epoch 28, Training Loss: 0.7990662529889275\n",
      "Epoch 29, Training Loss: 0.799818602029015\n",
      "Epoch 30, Training Loss: 0.7999871378085193\n",
      "Epoch 31, Training Loss: 0.7995440489404342\n",
      "Epoch 32, Training Loss: 0.7987871597093694\n",
      "Epoch 33, Training Loss: 0.7984162245077245\n",
      "Epoch 34, Training Loss: 0.7980052811257979\n",
      "Epoch 35, Training Loss: 0.7975715440161087\n",
      "Epoch 36, Training Loss: 0.7969197611949023\n",
      "Epoch 37, Training Loss: 0.7968210925775415\n",
      "Epoch 38, Training Loss: 0.797124652932672\n",
      "Epoch 39, Training Loss: 0.7972033396187951\n",
      "Epoch 40, Training Loss: 0.7967206737574409\n",
      "Epoch 41, Training Loss: 0.7964301379287944\n",
      "Epoch 42, Training Loss: 0.7968218466814827\n",
      "Epoch 43, Training Loss: 0.7955946078019983\n",
      "Epoch 44, Training Loss: 0.7948472231977126\n",
      "Epoch 45, Training Loss: 0.795489130861619\n",
      "Epoch 46, Training Loss: 0.7953667859470143\n",
      "Epoch 47, Training Loss: 0.7952490857068231\n",
      "Epoch 48, Training Loss: 0.795015736397575\n",
      "Epoch 49, Training Loss: 0.7944085355365977\n",
      "Epoch 50, Training Loss: 0.7947804235009586\n",
      "Epoch 51, Training Loss: 0.7942635893821717\n",
      "Epoch 52, Training Loss: 0.7942371430116542\n",
      "Epoch 53, Training Loss: 0.793798774060081\n",
      "Epoch 54, Training Loss: 0.7938096615146188\n",
      "Epoch 55, Training Loss: 0.7936433510920581\n",
      "Epoch 56, Training Loss: 0.7934222097957836\n",
      "Epoch 57, Training Loss: 0.7938556237781749\n",
      "Epoch 58, Training Loss: 0.793226139124702\n",
      "Epoch 59, Training Loss: 0.7930005640843335\n",
      "Epoch 60, Training Loss: 0.7923783741277807\n",
      "Epoch 61, Training Loss: 0.7934156055310193\n",
      "Epoch 62, Training Loss: 0.7933016997926375\n",
      "Epoch 63, Training Loss: 0.7929936688086566\n",
      "Epoch 64, Training Loss: 0.7924996458782869\n",
      "Epoch 65, Training Loss: 0.7928511475815493\n",
      "Epoch 66, Training Loss: 0.7926592597540687\n",
      "Epoch 67, Training Loss: 0.7924792803736294\n",
      "Epoch 68, Training Loss: 0.7926487198997947\n",
      "Epoch 69, Training Loss: 0.7925011399914237\n",
      "Epoch 70, Training Loss: 0.7925418267530554\n",
      "Epoch 71, Training Loss: 0.7920596113625695\n",
      "Epoch 72, Training Loss: 0.7928303987839642\n",
      "Epoch 73, Training Loss: 0.7920930993556976\n",
      "Epoch 74, Training Loss: 0.7920273560636184\n",
      "Epoch 75, Training Loss: 0.7921394180550295\n",
      "Epoch 76, Training Loss: 0.7923478199453915\n",
      "Epoch 77, Training Loss: 0.792261627000921\n",
      "Epoch 78, Training Loss: 0.7919814912711873\n",
      "Epoch 79, Training Loss: 0.7925733261248644\n",
      "Epoch 80, Training Loss: 0.7917748636357924\n",
      "Epoch 81, Training Loss: 0.7910511464231155\n",
      "Epoch 82, Training Loss: 0.7920567776876337\n",
      "Epoch 83, Training Loss: 0.7917324409765356\n",
      "Epoch 84, Training Loss: 0.7920755966971902\n",
      "Epoch 85, Training Loss: 0.7916449957735399\n",
      "Epoch 86, Training Loss: 0.792316172543694\n",
      "Epoch 87, Training Loss: 0.7915533729160533\n",
      "Epoch 88, Training Loss: 0.7910633660064024\n",
      "Epoch 89, Training Loss: 0.7915065359368044\n",
      "Epoch 90, Training Loss: 0.7913163454392377\n",
      "Epoch 91, Training Loss: 0.7919003802187302\n",
      "Epoch 92, Training Loss: 0.7911821817650514\n",
      "Epoch 93, Training Loss: 0.7909281365310444\n",
      "Epoch 94, Training Loss: 0.7903789192788742\n",
      "Epoch 95, Training Loss: 0.7911255219403435\n",
      "Epoch 96, Training Loss: 0.7909838121077594\n",
      "Epoch 97, Training Loss: 0.7905818588593427\n",
      "Epoch 98, Training Loss: 0.7910556367565604\n",
      "Epoch 99, Training Loss: 0.7909742865842931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:37:28,570] Trial 275 finished with value: 0.6386 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7909503999177148\n",
      "Epoch 1, Training Loss: 1.0130431791893522\n",
      "Epoch 2, Training Loss: 0.9427802201500513\n",
      "Epoch 3, Training Loss: 0.926569916700062\n",
      "Epoch 4, Training Loss: 0.9137545434155858\n",
      "Epoch 5, Training Loss: 0.898673517363412\n",
      "Epoch 6, Training Loss: 0.8774606486012165\n",
      "Epoch 7, Training Loss: 0.8492129334829803\n",
      "Epoch 8, Training Loss: 0.8265907733960259\n",
      "Epoch 9, Training Loss: 0.8136862026121383\n",
      "Epoch 10, Training Loss: 0.8090475121835121\n",
      "Epoch 11, Training Loss: 0.806791026879074\n",
      "Epoch 12, Training Loss: 0.8054011271412211\n",
      "Epoch 13, Training Loss: 0.8036366517382457\n",
      "Epoch 14, Training Loss: 0.8022926147719075\n",
      "Epoch 15, Training Loss: 0.8014269466686966\n",
      "Epoch 16, Training Loss: 0.8022855045203876\n",
      "Epoch 17, Training Loss: 0.7999566200084256\n",
      "Epoch 18, Training Loss: 0.7999616931255599\n",
      "Epoch 19, Training Loss: 0.7990241748946053\n",
      "Epoch 20, Training Loss: 0.7985742873715279\n",
      "Epoch 21, Training Loss: 0.7984488992762745\n",
      "Epoch 22, Training Loss: 0.7979628622980046\n",
      "Epoch 23, Training Loss: 0.7971926284015627\n",
      "Epoch 24, Training Loss: 0.7971309084641306\n",
      "Epoch 25, Training Loss: 0.7964557352818941\n",
      "Epoch 26, Training Loss: 0.7961880727818138\n",
      "Epoch 27, Training Loss: 0.795457132597615\n",
      "Epoch 28, Training Loss: 0.7955171934644082\n",
      "Epoch 29, Training Loss: 0.794517179180805\n",
      "Epoch 30, Training Loss: 0.794165605441072\n",
      "Epoch 31, Training Loss: 0.7942103347383944\n",
      "Epoch 32, Training Loss: 0.7938286346600468\n",
      "Epoch 33, Training Loss: 0.7932426907066116\n",
      "Epoch 34, Training Loss: 0.793412350801597\n",
      "Epoch 35, Training Loss: 0.7928025888321095\n",
      "Epoch 36, Training Loss: 0.7926623222523166\n",
      "Epoch 37, Training Loss: 0.7926558561791155\n",
      "Epoch 38, Training Loss: 0.7918275420827077\n",
      "Epoch 39, Training Loss: 0.7910142882425982\n",
      "Epoch 40, Training Loss: 0.7916567045046871\n",
      "Epoch 41, Training Loss: 0.7909590569653906\n",
      "Epoch 42, Training Loss: 0.7903473584275497\n",
      "Epoch 43, Training Loss: 0.7900446799464692\n",
      "Epoch 44, Training Loss: 0.7893692638641013\n",
      "Epoch 45, Training Loss: 0.789389609573479\n",
      "Epoch 46, Training Loss: 0.7892669770950661\n",
      "Epoch 47, Training Loss: 0.7884895188467843\n",
      "Epoch 48, Training Loss: 0.7884935205144094\n",
      "Epoch 49, Training Loss: 0.788513866134156\n",
      "Epoch 50, Training Loss: 0.7881623474278845\n",
      "Epoch 51, Training Loss: 0.7883379529293318\n",
      "Epoch 52, Training Loss: 0.787193916390713\n",
      "Epoch 53, Training Loss: 0.7880338586362681\n",
      "Epoch 54, Training Loss: 0.7873216292015592\n",
      "Epoch 55, Training Loss: 0.787364853952164\n",
      "Epoch 56, Training Loss: 0.7876689333664744\n",
      "Epoch 57, Training Loss: 0.7861518154466959\n",
      "Epoch 58, Training Loss: 0.786506700426116\n",
      "Epoch 59, Training Loss: 0.7864643517293428\n",
      "Epoch 60, Training Loss: 0.786266581456464\n",
      "Epoch 61, Training Loss: 0.7864516454531734\n",
      "Epoch 62, Training Loss: 0.7858714781309429\n",
      "Epoch 63, Training Loss: 0.7859468192982494\n",
      "Epoch 64, Training Loss: 0.7856232313285196\n",
      "Epoch 65, Training Loss: 0.7857065928609748\n",
      "Epoch 66, Training Loss: 0.7851010949091803\n",
      "Epoch 67, Training Loss: 0.7850546646835213\n",
      "Epoch 68, Training Loss: 0.7858169224029197\n",
      "Epoch 69, Training Loss: 0.7849663756843797\n",
      "Epoch 70, Training Loss: 0.785126559268263\n",
      "Epoch 71, Training Loss: 0.7849786298615592\n",
      "Epoch 72, Training Loss: 0.7849028674283423\n",
      "Epoch 73, Training Loss: 0.7845749260787677\n",
      "Epoch 74, Training Loss: 0.7845632706369673\n",
      "Epoch 75, Training Loss: 0.7844378242815347\n",
      "Epoch 76, Training Loss: 0.7843638941757661\n",
      "Epoch 77, Training Loss: 0.7846404561422822\n",
      "Epoch 78, Training Loss: 0.7837250883417918\n",
      "Epoch 79, Training Loss: 0.7839618068888672\n",
      "Epoch 80, Training Loss: 0.7838573104456852\n",
      "Epoch 81, Training Loss: 0.7834663557827024\n",
      "Epoch 82, Training Loss: 0.7833778557024504\n",
      "Epoch 83, Training Loss: 0.783601452891988\n",
      "Epoch 84, Training Loss: 0.7838008696871592\n",
      "Epoch 85, Training Loss: 0.7845636124001409\n",
      "Epoch 86, Training Loss: 0.7835241276518743\n",
      "Epoch 87, Training Loss: 0.7832911382044169\n",
      "Epoch 88, Training Loss: 0.7833213100756021\n",
      "Epoch 89, Training Loss: 0.7834773901709937\n",
      "Epoch 90, Training Loss: 0.7833731511481723\n",
      "Epoch 91, Training Loss: 0.7828294739687354\n",
      "Epoch 92, Training Loss: 0.7830149056319904\n",
      "Epoch 93, Training Loss: 0.7834378816131362\n",
      "Epoch 94, Training Loss: 0.7831355176473919\n",
      "Epoch 95, Training Loss: 0.7830949146944778\n",
      "Epoch 96, Training Loss: 0.7820727822924018\n",
      "Epoch 97, Training Loss: 0.7821462177244344\n",
      "Epoch 98, Training Loss: 0.7823446112019675\n",
      "Epoch 99, Training Loss: 0.78254090063554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:38:58,672] Trial 276 finished with value: 0.6306 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7822419729447903\n",
      "Epoch 1, Training Loss: 0.8908468978745597\n",
      "Epoch 2, Training Loss: 0.8150800650281117\n",
      "Epoch 3, Training Loss: 0.8114652694616102\n",
      "Epoch 4, Training Loss: 0.8104242197553018\n",
      "Epoch 5, Training Loss: 0.8076015267157017\n",
      "Epoch 6, Training Loss: 0.8062362577682151\n",
      "Epoch 7, Training Loss: 0.8046581579330272\n",
      "Epoch 8, Training Loss: 0.8042473219391099\n",
      "Epoch 9, Training Loss: 0.8032424129937824\n",
      "Epoch 10, Training Loss: 0.8023584422312284\n",
      "Epoch 11, Training Loss: 0.8025342238576789\n",
      "Epoch 12, Training Loss: 0.800853168964386\n",
      "Epoch 13, Training Loss: 0.8001686708371442\n",
      "Epoch 14, Training Loss: 0.8000656109555323\n",
      "Epoch 15, Training Loss: 0.7994049369840693\n",
      "Epoch 16, Training Loss: 0.799965494826324\n",
      "Epoch 17, Training Loss: 0.7993721457352315\n",
      "Epoch 18, Training Loss: 0.799081912524718\n",
      "Epoch 19, Training Loss: 0.7982976000111802\n",
      "Epoch 20, Training Loss: 0.7970069041825775\n",
      "Epoch 21, Training Loss: 0.7974204370850011\n",
      "Epoch 22, Training Loss: 0.7967160928518252\n",
      "Epoch 23, Training Loss: 0.7967163517959136\n",
      "Epoch 24, Training Loss: 0.7968695203164466\n",
      "Epoch 25, Training Loss: 0.7943472467418901\n",
      "Epoch 26, Training Loss: 0.7942663450886432\n",
      "Epoch 27, Training Loss: 0.7941896309529929\n",
      "Epoch 28, Training Loss: 0.7932566191020765\n",
      "Epoch 29, Training Loss: 0.7923778272212896\n",
      "Epoch 30, Training Loss: 0.7915017057182198\n",
      "Epoch 31, Training Loss: 0.7904193352935905\n",
      "Epoch 32, Training Loss: 0.7899348982294699\n",
      "Epoch 33, Training Loss: 0.7898759769317799\n",
      "Epoch 34, Training Loss: 0.7894594170097121\n",
      "Epoch 35, Training Loss: 0.7884129288501309\n",
      "Epoch 36, Training Loss: 0.7882153450098253\n",
      "Epoch 37, Training Loss: 0.7872194214871056\n",
      "Epoch 38, Training Loss: 0.7870011381636884\n",
      "Epoch 39, Training Loss: 0.7875787316408372\n",
      "Epoch 40, Training Loss: 0.786597794608066\n",
      "Epoch 41, Training Loss: 0.7860314330660311\n",
      "Epoch 42, Training Loss: 0.7853133736248303\n",
      "Epoch 43, Training Loss: 0.7854846219371135\n",
      "Epoch 44, Training Loss: 0.7855585705965085\n",
      "Epoch 45, Training Loss: 0.7851623701869993\n",
      "Epoch 46, Training Loss: 0.7855191611705866\n",
      "Epoch 47, Training Loss: 0.7843892120777216\n",
      "Epoch 48, Training Loss: 0.7841990436826434\n",
      "Epoch 49, Training Loss: 0.7842949171711627\n",
      "Epoch 50, Training Loss: 0.7849917698623543\n",
      "Epoch 51, Training Loss: 0.7839014546315473\n",
      "Epoch 52, Training Loss: 0.7836855827417589\n",
      "Epoch 53, Training Loss: 0.784027732225289\n",
      "Epoch 54, Training Loss: 0.7830168509841862\n",
      "Epoch 55, Training Loss: 0.7837792845596945\n",
      "Epoch 56, Training Loss: 0.7836356522445392\n",
      "Epoch 57, Training Loss: 0.7829833571175884\n",
      "Epoch 58, Training Loss: 0.7829357541145239\n",
      "Epoch 59, Training Loss: 0.7837349439025821\n",
      "Epoch 60, Training Loss: 0.7832309535571507\n",
      "Epoch 61, Training Loss: 0.7835959493665767\n",
      "Epoch 62, Training Loss: 0.7833838251300325\n",
      "Epoch 63, Training Loss: 0.7834973217849445\n",
      "Epoch 64, Training Loss: 0.7825157972206747\n",
      "Epoch 65, Training Loss: 0.7825393770870409\n",
      "Epoch 66, Training Loss: 0.7826692656466835\n",
      "Epoch 67, Training Loss: 0.782353891465897\n",
      "Epoch 68, Training Loss: 0.7832770071531597\n",
      "Epoch 69, Training Loss: 0.7827775057993437\n",
      "Epoch 70, Training Loss: 0.7826284587831426\n",
      "Epoch 71, Training Loss: 0.7824429195626338\n",
      "Epoch 72, Training Loss: 0.7818880651230202\n",
      "Epoch 73, Training Loss: 0.782804837173089\n",
      "Epoch 74, Training Loss: 0.7825038462653195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:40:28,111] Trial 277 finished with value: 0.6412 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7824829859841139\n",
      "Epoch 1, Training Loss: 0.8652207423869829\n",
      "Epoch 2, Training Loss: 0.8274737744403065\n",
      "Epoch 3, Training Loss: 0.8227883853410419\n",
      "Epoch 4, Training Loss: 0.819830522501379\n",
      "Epoch 5, Training Loss: 0.8177433694215646\n",
      "Epoch 6, Training Loss: 0.8148713805621728\n",
      "Epoch 7, Training Loss: 0.8137866674509263\n",
      "Epoch 8, Training Loss: 0.8118555978724831\n",
      "Epoch 9, Training Loss: 0.8104328531071656\n",
      "Epoch 10, Training Loss: 0.8113656560281166\n",
      "Epoch 11, Training Loss: 0.8095850288419795\n",
      "Epoch 12, Training Loss: 0.8090263411514741\n",
      "Epoch 13, Training Loss: 0.8079892648790116\n",
      "Epoch 14, Training Loss: 0.8082247010747293\n",
      "Epoch 15, Training Loss: 0.8076016349003727\n",
      "Epoch 16, Training Loss: 0.8070811022493176\n",
      "Epoch 17, Training Loss: 0.8070355012004536\n",
      "Epoch 18, Training Loss: 0.8068649344426349\n",
      "Epoch 19, Training Loss: 0.8066110712244995\n",
      "Epoch 20, Training Loss: 0.805581704207829\n",
      "Epoch 21, Training Loss: 0.8058731231474339\n",
      "Epoch 22, Training Loss: 0.8065407154255344\n",
      "Epoch 23, Training Loss: 0.8057635232918244\n",
      "Epoch 24, Training Loss: 0.8064954149991946\n",
      "Epoch 25, Training Loss: 0.8053765970961492\n",
      "Epoch 26, Training Loss: 0.8054903146915866\n",
      "Epoch 27, Training Loss: 0.8054553550885136\n",
      "Epoch 28, Training Loss: 0.8051587483040372\n",
      "Epoch 29, Training Loss: 0.8050345501505343\n",
      "Epoch 30, Training Loss: 0.8051780561755474\n",
      "Epoch 31, Training Loss: 0.8046874357345409\n",
      "Epoch 32, Training Loss: 0.8045402071529761\n",
      "Epoch 33, Training Loss: 0.8040382600368414\n",
      "Epoch 34, Training Loss: 0.8044040449579856\n",
      "Epoch 35, Training Loss: 0.8051476615712159\n",
      "Epoch 36, Training Loss: 0.8041443973555601\n",
      "Epoch 37, Training Loss: 0.8032722029022704\n",
      "Epoch 38, Training Loss: 0.803286904320681\n",
      "Epoch 39, Training Loss: 0.8043944117718174\n",
      "Epoch 40, Training Loss: 0.8038642584829402\n",
      "Epoch 41, Training Loss: 0.8038704286840626\n",
      "Epoch 42, Training Loss: 0.8029726010516174\n",
      "Epoch 43, Training Loss: 0.8025703431968402\n",
      "Epoch 44, Training Loss: 0.8034258776141289\n",
      "Epoch 45, Training Loss: 0.8030081126026641\n",
      "Epoch 46, Training Loss: 0.8025676788244033\n",
      "Epoch 47, Training Loss: 0.8031823990040255\n",
      "Epoch 48, Training Loss: 0.8019132170462071\n",
      "Epoch 49, Training Loss: 0.8024227806500026\n",
      "Epoch 50, Training Loss: 0.8023909290034071\n",
      "Epoch 51, Training Loss: 0.8019184405642344\n",
      "Epoch 52, Training Loss: 0.8020290086143895\n",
      "Epoch 53, Training Loss: 0.8017750990121885\n",
      "Epoch 54, Training Loss: 0.8013346079596899\n",
      "Epoch 55, Training Loss: 0.8018764868714756\n",
      "Epoch 56, Training Loss: 0.8016956640365428\n",
      "Epoch 57, Training Loss: 0.8005664484841483\n",
      "Epoch 58, Training Loss: 0.8014367579517508\n",
      "Epoch 59, Training Loss: 0.8007040345579162\n",
      "Epoch 60, Training Loss: 0.799983565161999\n",
      "Epoch 61, Training Loss: 0.79993626847303\n",
      "Epoch 62, Training Loss: 0.7995589533694705\n",
      "Epoch 63, Training Loss: 0.7997914460368623\n",
      "Epoch 64, Training Loss: 0.7996875270865017\n",
      "Epoch 65, Training Loss: 0.7999001892885768\n",
      "Epoch 66, Training Loss: 0.8001128911075736\n",
      "Epoch 67, Training Loss: 0.7997560499306012\n",
      "Epoch 68, Training Loss: 0.7992687279120424\n",
      "Epoch 69, Training Loss: 0.7994120302953218\n",
      "Epoch 70, Training Loss: 0.7988105678916874\n",
      "Epoch 71, Training Loss: 0.798899925741038\n",
      "Epoch 72, Training Loss: 0.7983087848899956\n",
      "Epoch 73, Training Loss: 0.7988055520487908\n",
      "Epoch 74, Training Loss: 0.7975720295332428\n",
      "Epoch 75, Training Loss: 0.7983172596845411\n",
      "Epoch 76, Training Loss: 0.7980401306224049\n",
      "Epoch 77, Training Loss: 0.7981339986162974\n",
      "Epoch 78, Training Loss: 0.7977746644414457\n",
      "Epoch 79, Training Loss: 0.7981066519156435\n",
      "Epoch 80, Training Loss: 0.7979263297597269\n",
      "Epoch 81, Training Loss: 0.7971250067079874\n",
      "Epoch 82, Training Loss: 0.7972791236146052\n",
      "Epoch 83, Training Loss: 0.7976864203474575\n",
      "Epoch 84, Training Loss: 0.7973171789843337\n",
      "Epoch 85, Training Loss: 0.7979473497634544\n",
      "Epoch 86, Training Loss: 0.7969709377539785\n",
      "Epoch 87, Training Loss: 0.797244227649574\n",
      "Epoch 88, Training Loss: 0.7972887861997562\n",
      "Epoch 89, Training Loss: 0.7971334867907646\n",
      "Epoch 90, Training Loss: 0.7965854793562925\n",
      "Epoch 91, Training Loss: 0.7959105959064082\n",
      "Epoch 92, Training Loss: 0.796591333607982\n",
      "Epoch 93, Training Loss: 0.7962625127985962\n",
      "Epoch 94, Training Loss: 0.7965844148980048\n",
      "Epoch 95, Training Loss: 0.7949766029988913\n",
      "Epoch 96, Training Loss: 0.7962900790056788\n",
      "Epoch 97, Training Loss: 0.7960024501596178\n",
      "Epoch 98, Training Loss: 0.7969312632890572\n",
      "Epoch 99, Training Loss: 0.7955694124214632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:42:01,290] Trial 278 finished with value: 0.5982 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7969136086621679\n",
      "Epoch 1, Training Loss: 0.9738476997964522\n",
      "Epoch 2, Training Loss: 0.9296168263519512\n",
      "Epoch 3, Training Loss: 0.8881821782448712\n",
      "Epoch 4, Training Loss: 0.845011502644595\n",
      "Epoch 5, Training Loss: 0.8232907345715691\n",
      "Epoch 6, Training Loss: 0.8162538419050329\n",
      "Epoch 7, Training Loss: 0.8138897614619311\n",
      "Epoch 8, Training Loss: 0.8129217060173259\n",
      "Epoch 9, Training Loss: 0.8119626348158893\n",
      "Epoch 10, Training Loss: 0.8106008253378026\n",
      "Epoch 11, Training Loss: 0.8093444709216847\n",
      "Epoch 12, Training Loss: 0.8085457283609053\n",
      "Epoch 13, Training Loss: 0.80736783322166\n",
      "Epoch 14, Training Loss: 0.8066127652981702\n",
      "Epoch 15, Training Loss: 0.8063754432341632\n",
      "Epoch 16, Training Loss: 0.8054561597459456\n",
      "Epoch 17, Training Loss: 0.8049638088310466\n",
      "Epoch 18, Training Loss: 0.8042458501984091\n",
      "Epoch 19, Training Loss: 0.8041408691686742\n",
      "Epoch 20, Training Loss: 0.8037994097962099\n",
      "Epoch 21, Training Loss: 0.8030589699745179\n",
      "Epoch 22, Training Loss: 0.8030291403742398\n",
      "Epoch 23, Training Loss: 0.8028710362490485\n",
      "Epoch 24, Training Loss: 0.8024419008984285\n",
      "Epoch 25, Training Loss: 0.8023687136173249\n",
      "Epoch 26, Training Loss: 0.8021204570461722\n",
      "Epoch 27, Training Loss: 0.8015816523748286\n",
      "Epoch 28, Training Loss: 0.801744315554114\n",
      "Epoch 29, Training Loss: 0.8015260150853325\n",
      "Epoch 30, Training Loss: 0.8014399530607111\n",
      "Epoch 31, Training Loss: 0.8007441202331992\n",
      "Epoch 32, Training Loss: 0.8007665012163274\n",
      "Epoch 33, Training Loss: 0.800888153665206\n",
      "Epoch 34, Training Loss: 0.8003372477082645\n",
      "Epoch 35, Training Loss: 0.8002740162961623\n",
      "Epoch 36, Training Loss: 0.8001581919894499\n",
      "Epoch 37, Training Loss: 0.8001279576385723\n",
      "Epoch 38, Training Loss: 0.7998671866865719\n",
      "Epoch 39, Training Loss: 0.7998088657855987\n",
      "Epoch 40, Training Loss: 0.7998117225310382\n",
      "Epoch 41, Training Loss: 0.7994635387027965\n",
      "Epoch 42, Training Loss: 0.7996640753044801\n",
      "Epoch 43, Training Loss: 0.7990686517603257\n",
      "Epoch 44, Training Loss: 0.7991438060648302\n",
      "Epoch 45, Training Loss: 0.7989994619173162\n",
      "Epoch 46, Training Loss: 0.7987281154183781\n",
      "Epoch 47, Training Loss: 0.7984192569115582\n",
      "Epoch 48, Training Loss: 0.7988293749444625\n",
      "Epoch 49, Training Loss: 0.7984720837368685\n",
      "Epoch 50, Training Loss: 0.7984205496311187\n",
      "Epoch 51, Training Loss: 0.7979465122082654\n",
      "Epoch 52, Training Loss: 0.7981286253648646\n",
      "Epoch 53, Training Loss: 0.7983801097028396\n",
      "Epoch 54, Training Loss: 0.7976244138970094\n",
      "Epoch 55, Training Loss: 0.7979444395794588\n",
      "Epoch 56, Training Loss: 0.7977417658357059\n",
      "Epoch 57, Training Loss: 0.7974761951670927\n",
      "Epoch 58, Training Loss: 0.7972261735972236\n",
      "Epoch 59, Training Loss: 0.7973567084705129\n",
      "Epoch 60, Training Loss: 0.7972015272869784\n",
      "Epoch 61, Training Loss: 0.7968679496821235\n",
      "Epoch 62, Training Loss: 0.7970152505005107\n",
      "Epoch 63, Training Loss: 0.796732386490878\n",
      "Epoch 64, Training Loss: 0.7968400700653301\n",
      "Epoch 65, Training Loss: 0.7963988086055307\n",
      "Epoch 66, Training Loss: 0.7964808348347159\n",
      "Epoch 67, Training Loss: 0.7962915977309731\n",
      "Epoch 68, Training Loss: 0.7961267502167646\n",
      "Epoch 69, Training Loss: 0.7959432267441469\n",
      "Epoch 70, Training Loss: 0.7959170910190133\n",
      "Epoch 71, Training Loss: 0.795864591107649\n",
      "Epoch 72, Training Loss: 0.7956022589346942\n",
      "Epoch 73, Training Loss: 0.7954030197508195\n",
      "Epoch 74, Training Loss: 0.7951836781642017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:43:36,812] Trial 279 finished with value: 0.6336 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7951531326069551\n",
      "Epoch 1, Training Loss: 0.8837577357011683\n",
      "Epoch 2, Training Loss: 0.815016965936212\n",
      "Epoch 3, Training Loss: 0.8119045103297514\n",
      "Epoch 4, Training Loss: 0.8082458180539748\n",
      "Epoch 5, Training Loss: 0.8051489303392523\n",
      "Epoch 6, Training Loss: 0.8034125340686125\n",
      "Epoch 7, Training Loss: 0.7985189892263973\n",
      "Epoch 8, Training Loss: 0.7970592420241412\n",
      "Epoch 9, Training Loss: 0.7943843819113339\n",
      "Epoch 10, Training Loss: 0.794824622518876\n",
      "Epoch 11, Training Loss: 0.7925804774200215\n",
      "Epoch 12, Training Loss: 0.7916286284082076\n",
      "Epoch 13, Training Loss: 0.7905518890829647\n",
      "Epoch 14, Training Loss: 0.7912163659404305\n",
      "Epoch 15, Training Loss: 0.789361194933162\n",
      "Epoch 16, Training Loss: 0.789452263397329\n",
      "Epoch 17, Training Loss: 0.7897373557090759\n",
      "Epoch 18, Training Loss: 0.7889294657987707\n",
      "Epoch 19, Training Loss: 0.7881623646091013\n",
      "Epoch 20, Training Loss: 0.7880905669576982\n",
      "Epoch 21, Training Loss: 0.787753536490833\n",
      "Epoch 22, Training Loss: 0.7880440389408785\n",
      "Epoch 23, Training Loss: 0.7876516250301809\n",
      "Epoch 24, Training Loss: 0.7876232621249031\n",
      "Epoch 25, Training Loss: 0.7875465874812182\n",
      "Epoch 26, Training Loss: 0.7866458654403686\n",
      "Epoch 27, Training Loss: 0.7868483973951901\n",
      "Epoch 28, Training Loss: 0.7873610400452333\n",
      "Epoch 29, Training Loss: 0.7861162841320037\n",
      "Epoch 30, Training Loss: 0.7861169897107517\n",
      "Epoch 31, Training Loss: 0.7855176265800701\n",
      "Epoch 32, Training Loss: 0.7859715934360728\n",
      "Epoch 33, Training Loss: 0.7858527818848106\n",
      "Epoch 34, Training Loss: 0.7864439611575182\n",
      "Epoch 35, Training Loss: 0.7849911478687736\n",
      "Epoch 36, Training Loss: 0.7849768327965456\n",
      "Epoch 37, Training Loss: 0.785070519026588\n",
      "Epoch 38, Training Loss: 0.7847272945852841\n",
      "Epoch 39, Training Loss: 0.7848705682333778\n",
      "Epoch 40, Training Loss: 0.7847328802417306\n",
      "Epoch 41, Training Loss: 0.7851524220494663\n",
      "Epoch 42, Training Loss: 0.7842137815671809\n",
      "Epoch 43, Training Loss: 0.7841337638742784\n",
      "Epoch 44, Training Loss: 0.7848189962611479\n",
      "Epoch 45, Training Loss: 0.7850047075047213\n",
      "Epoch 46, Training Loss: 0.7842058707686032\n",
      "Epoch 47, Training Loss: 0.7834359796608196\n",
      "Epoch 48, Training Loss: 0.7844760213879978\n",
      "Epoch 49, Training Loss: 0.7843027546826531\n",
      "Epoch 50, Training Loss: 0.7839143523047952\n",
      "Epoch 51, Training Loss: 0.7837273740768432\n",
      "Epoch 52, Training Loss: 0.7839899870928596\n",
      "Epoch 53, Training Loss: 0.783347294961705\n",
      "Epoch 54, Training Loss: 0.7834460432389203\n",
      "Epoch 55, Training Loss: 0.7827790900538949\n",
      "Epoch 56, Training Loss: 0.7828351757105659\n",
      "Epoch 57, Training Loss: 0.7837966648270102\n",
      "Epoch 58, Training Loss: 0.7822971386068007\n",
      "Epoch 59, Training Loss: 0.7825992641729467\n",
      "Epoch 60, Training Loss: 0.7833766334197101\n",
      "Epoch 61, Training Loss: 0.7830146236980663\n",
      "Epoch 62, Training Loss: 0.7823204424100764\n",
      "Epoch 63, Training Loss: 0.7830270026711856\n",
      "Epoch 64, Training Loss: 0.78293673248852\n",
      "Epoch 65, Training Loss: 0.7824943673610687\n",
      "Epoch 66, Training Loss: 0.7821216799932368\n",
      "Epoch 67, Training Loss: 0.7822196745872497\n",
      "Epoch 68, Training Loss: 0.7820237921265994\n",
      "Epoch 69, Training Loss: 0.7817942704873927\n",
      "Epoch 70, Training Loss: 0.782260396270191\n",
      "Epoch 71, Training Loss: 0.7820236187822679\n",
      "Epoch 72, Training Loss: 0.7814972780732548\n",
      "Epoch 73, Training Loss: 0.7810269036012537\n",
      "Epoch 74, Training Loss: 0.7811676124965443\n",
      "Epoch 75, Training Loss: 0.7811114398872151\n",
      "Epoch 76, Training Loss: 0.7816464130317463\n",
      "Epoch 77, Training Loss: 0.781432662080316\n",
      "Epoch 78, Training Loss: 0.7813508414520937\n",
      "Epoch 79, Training Loss: 0.7809459243802463\n",
      "Epoch 80, Training Loss: 0.7811168420314789\n",
      "Epoch 81, Training Loss: 0.7804499107248642\n",
      "Epoch 82, Training Loss: 0.7805198439429788\n",
      "Epoch 83, Training Loss: 0.7814224356062272\n",
      "Epoch 84, Training Loss: 0.7811367065766278\n",
      "Epoch 85, Training Loss: 0.7808700510333566\n",
      "Epoch 86, Training Loss: 0.7803096885540907\n",
      "Epoch 87, Training Loss: 0.7806460417719449\n",
      "Epoch 88, Training Loss: 0.7804135600258323\n",
      "Epoch 89, Training Loss: 0.7800938072625329\n",
      "Epoch 90, Training Loss: 0.7804777598381043\n",
      "Epoch 91, Training Loss: 0.7802522763785193\n",
      "Epoch 92, Training Loss: 0.78035407241653\n",
      "Epoch 93, Training Loss: 0.779787512176177\n",
      "Epoch 94, Training Loss: 0.7796439064951504\n",
      "Epoch 95, Training Loss: 0.780415881802054\n",
      "Epoch 96, Training Loss: 0.7799227133919211\n",
      "Epoch 97, Training Loss: 0.77988449419246\n",
      "Epoch 98, Training Loss: 0.7797986651869381\n",
      "Epoch 99, Training Loss: 0.7795475701023551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:45:50,710] Trial 280 finished with value: 0.6404 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7795636364992927\n",
      "Epoch 1, Training Loss: 0.9573622177418013\n",
      "Epoch 2, Training Loss: 0.8681374953205424\n",
      "Epoch 3, Training Loss: 0.8573018421804098\n",
      "Epoch 4, Training Loss: 0.8540479695886598\n",
      "Epoch 5, Training Loss: 0.8496900645413793\n",
      "Epoch 6, Training Loss: 0.847517999132773\n",
      "Epoch 7, Training Loss: 0.847569633785047\n",
      "Epoch 8, Training Loss: 0.8448999767016647\n",
      "Epoch 9, Training Loss: 0.8437402106765518\n",
      "Epoch 10, Training Loss: 0.8434143740431707\n",
      "Epoch 11, Training Loss: 0.8424371109869248\n",
      "Epoch 12, Training Loss: 0.8422904426890209\n",
      "Epoch 13, Training Loss: 0.8434107561756794\n",
      "Epoch 14, Training Loss: 0.8421078024950243\n",
      "Epoch 15, Training Loss: 0.8421136918820833\n",
      "Epoch 16, Training Loss: 0.8426519990863657\n",
      "Epoch 17, Training Loss: 0.8406185400217099\n",
      "Epoch 18, Training Loss: 0.8420244813861704\n",
      "Epoch 19, Training Loss: 0.8404261874077016\n",
      "Epoch 20, Training Loss: 0.8397603962654457\n",
      "Epoch 21, Training Loss: 0.8419912429680502\n",
      "Epoch 22, Training Loss: 0.8391555513654436\n",
      "Epoch 23, Training Loss: 0.839594346688206\n",
      "Epoch 24, Training Loss: 0.8404216498360598\n",
      "Epoch 25, Training Loss: 0.8406199196227511\n",
      "Epoch 26, Training Loss: 0.8392789373720498\n",
      "Epoch 27, Training Loss: 0.8400910096957271\n",
      "Epoch 28, Training Loss: 0.8377324082797631\n",
      "Epoch 29, Training Loss: 0.8405019894578403\n",
      "Epoch 30, Training Loss: 0.8412112769327665\n",
      "Epoch 31, Training Loss: 0.8398650163098386\n",
      "Epoch 32, Training Loss: 0.8377438051359994\n",
      "Epoch 33, Training Loss: 0.8378600419912123\n",
      "Epoch 34, Training Loss: 0.8382343383659994\n",
      "Epoch 35, Training Loss: 0.8383978301421144\n",
      "Epoch 36, Training Loss: 0.8399222141817997\n",
      "Epoch 37, Training Loss: 0.8378376668109033\n",
      "Epoch 38, Training Loss: 0.8385032242402098\n",
      "Epoch 39, Training Loss: 0.8384608065275322\n",
      "Epoch 40, Training Loss: 0.8381334198148627\n",
      "Epoch 41, Training Loss: 0.8386962007759209\n",
      "Epoch 42, Training Loss: 0.8390429398170988\n",
      "Epoch 43, Training Loss: 0.8378364237627589\n",
      "Epoch 44, Training Loss: 0.8367572058412366\n",
      "Epoch 45, Training Loss: 0.836064498765128\n",
      "Epoch 46, Training Loss: 0.8375127214238159\n",
      "Epoch 47, Training Loss: 0.8387232888013797\n",
      "Epoch 48, Training Loss: 0.8361459135112906\n",
      "Epoch 49, Training Loss: 0.8363919000876577\n",
      "Epoch 50, Training Loss: 0.8373822167851871\n",
      "Epoch 51, Training Loss: 0.8362231996722688\n",
      "Epoch 52, Training Loss: 0.8346310666629246\n",
      "Epoch 53, Training Loss: 0.8408657081145093\n",
      "Epoch 54, Training Loss: 0.8364428599974266\n",
      "Epoch 55, Training Loss: 0.8356648771386398\n",
      "Epoch 56, Training Loss: 0.8339398359893856\n",
      "Epoch 57, Training Loss: 0.8388266832308662\n",
      "Epoch 58, Training Loss: 0.8372100892819856\n",
      "Epoch 59, Training Loss: 0.8369779853892506\n",
      "Epoch 60, Training Loss: 0.8342647240574198\n",
      "Epoch 61, Training Loss: 0.8384914494994887\n",
      "Epoch 62, Training Loss: 0.8372129999605337\n",
      "Epoch 63, Training Loss: 0.8341536356990499\n",
      "Epoch 64, Training Loss: 0.835159322821108\n",
      "Epoch 65, Training Loss: 0.8366322312139927\n",
      "Epoch 66, Training Loss: 0.8356469555456836\n",
      "Epoch 67, Training Loss: 0.8347244688442775\n",
      "Epoch 68, Training Loss: 0.8377784692255178\n",
      "Epoch 69, Training Loss: 0.8373968329644741\n",
      "Epoch 70, Training Loss: 0.8379850772986734\n",
      "Epoch 71, Training Loss: 0.8375072520478327\n",
      "Epoch 72, Training Loss: 0.8372986348948084\n",
      "Epoch 73, Training Loss: 0.8352717694483305\n",
      "Epoch 74, Training Loss: 0.8334546188214668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:47:14,067] Trial 281 finished with value: 0.5562666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8339994010172392\n",
      "Epoch 1, Training Loss: 0.9089645846445757\n",
      "Epoch 2, Training Loss: 0.8279435631027795\n",
      "Epoch 3, Training Loss: 0.8213648102337257\n",
      "Epoch 4, Training Loss: 0.8163967096715942\n",
      "Epoch 5, Training Loss: 0.8149745494799506\n",
      "Epoch 6, Training Loss: 0.8126292145341859\n",
      "Epoch 7, Training Loss: 0.8111678054458217\n",
      "Epoch 8, Training Loss: 0.8109039186535025\n",
      "Epoch 9, Training Loss: 0.8087119995203234\n",
      "Epoch 10, Training Loss: 0.8091183409655005\n",
      "Epoch 11, Training Loss: 0.8085035022936369\n",
      "Epoch 12, Training Loss: 0.8073617080996808\n",
      "Epoch 13, Training Loss: 0.8059744444108548\n",
      "Epoch 14, Training Loss: 0.8066350631247786\n",
      "Epoch 15, Training Loss: 0.8062375325009339\n",
      "Epoch 16, Training Loss: 0.8057700780997599\n",
      "Epoch 17, Training Loss: 0.8049382579954047\n",
      "Epoch 18, Training Loss: 0.8046572079335836\n",
      "Epoch 19, Training Loss: 0.8045104666760093\n",
      "Epoch 20, Training Loss: 0.8047621028763907\n",
      "Epoch 21, Training Loss: 0.8042189482459449\n",
      "Epoch 22, Training Loss: 0.803423563878339\n",
      "Epoch 23, Training Loss: 0.8035142947856645\n",
      "Epoch 24, Training Loss: 0.8027550908855926\n",
      "Epoch 25, Training Loss: 0.8027349867318806\n",
      "Epoch 26, Training Loss: 0.8021021934380209\n",
      "Epoch 27, Training Loss: 0.8023102219839742\n",
      "Epoch 28, Training Loss: 0.8022902751327458\n",
      "Epoch 29, Training Loss: 0.802301720031222\n",
      "Epoch 30, Training Loss: 0.8026046528852075\n",
      "Epoch 31, Training Loss: 0.8015395918286833\n",
      "Epoch 32, Training Loss: 0.8016264675255108\n",
      "Epoch 33, Training Loss: 0.8013176841843397\n",
      "Epoch 34, Training Loss: 0.8014746273370613\n",
      "Epoch 35, Training Loss: 0.8014153523552686\n",
      "Epoch 36, Training Loss: 0.8016441516410139\n",
      "Epoch 37, Training Loss: 0.8002217593049644\n",
      "Epoch 38, Training Loss: 0.8006685757099238\n",
      "Epoch 39, Training Loss: 0.8001468973948543\n",
      "Epoch 40, Training Loss: 0.8001458608118215\n",
      "Epoch 41, Training Loss: 0.8007381017046763\n",
      "Epoch 42, Training Loss: 0.8002348227608472\n",
      "Epoch 43, Training Loss: 0.8003621236722273\n",
      "Epoch 44, Training Loss: 0.7999234605552559\n",
      "Epoch 45, Training Loss: 0.7993826566782213\n",
      "Epoch 46, Training Loss: 0.7991817247598691\n",
      "Epoch 47, Training Loss: 0.7994150261233623\n",
      "Epoch 48, Training Loss: 0.7994275496418315\n",
      "Epoch 49, Training Loss: 0.7986286210834532\n",
      "Epoch 50, Training Loss: 0.7983992903752435\n",
      "Epoch 51, Training Loss: 0.7992069451432479\n",
      "Epoch 52, Training Loss: 0.7985675958762491\n",
      "Epoch 53, Training Loss: 0.7979869410507661\n",
      "Epoch 54, Training Loss: 0.7981646968906088\n",
      "Epoch 55, Training Loss: 0.798377033373467\n",
      "Epoch 56, Training Loss: 0.7986057424903812\n",
      "Epoch 57, Training Loss: 0.797556782216954\n",
      "Epoch 58, Training Loss: 0.7973610944317696\n",
      "Epoch 59, Training Loss: 0.7971985449468283\n",
      "Epoch 60, Training Loss: 0.7966862536910782\n",
      "Epoch 61, Training Loss: 0.7974391472967047\n",
      "Epoch 62, Training Loss: 0.7976697109695664\n",
      "Epoch 63, Training Loss: 0.7968217790574956\n",
      "Epoch 64, Training Loss: 0.7966983480561048\n",
      "Epoch 65, Training Loss: 0.7961400799733356\n",
      "Epoch 66, Training Loss: 0.7961476803722238\n",
      "Epoch 67, Training Loss: 0.7965321764013821\n",
      "Epoch 68, Training Loss: 0.7959070160872954\n",
      "Epoch 69, Training Loss: 0.7967328503615874\n",
      "Epoch 70, Training Loss: 0.7960043355934602\n",
      "Epoch 71, Training Loss: 0.7955180454523043\n",
      "Epoch 72, Training Loss: 0.7959778990064349\n",
      "Epoch 73, Training Loss: 0.7953509274281954\n",
      "Epoch 74, Training Loss: 0.7954255202659091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:48:24,642] Trial 282 finished with value: 0.6048 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7950722604765927\n",
      "Epoch 1, Training Loss: 1.0894307418430553\n",
      "Epoch 2, Training Loss: 1.0727358484268188\n",
      "Epoch 3, Training Loss: 1.0602474049960866\n",
      "Epoch 4, Training Loss: 1.0485307039934046\n",
      "Epoch 5, Training Loss: 1.0370481570327983\n",
      "Epoch 6, Training Loss: 1.025642697460511\n",
      "Epoch 7, Training Loss: 1.0144158740604625\n",
      "Epoch 8, Training Loss: 1.003567413302029\n",
      "Epoch 9, Training Loss: 0.9933516689609079\n",
      "Epoch 10, Training Loss: 0.9840211664227878\n",
      "Epoch 11, Training Loss: 0.9757620710485122\n",
      "Epoch 12, Training Loss: 0.9686784151021172\n",
      "Epoch 13, Training Loss: 0.9627466900909648\n",
      "Epoch 14, Training Loss: 0.95785073427593\n",
      "Epoch 15, Training Loss: 0.9538683757361244\n",
      "Epoch 16, Training Loss: 0.9506296610832214\n",
      "Epoch 17, Training Loss: 0.947962799913743\n",
      "Epoch 18, Training Loss: 0.9457198038521935\n",
      "Epoch 19, Training Loss: 0.9438255754639121\n",
      "Epoch 20, Training Loss: 0.942169030974893\n",
      "Epoch 21, Training Loss: 0.9406940781368929\n",
      "Epoch 22, Training Loss: 0.939338939260034\n",
      "Epoch 23, Training Loss: 0.9381085670695586\n",
      "Epoch 24, Training Loss: 0.9369475002849803\n",
      "Epoch 25, Training Loss: 0.935836272239685\n",
      "Epoch 26, Training Loss: 0.9347774374485016\n",
      "Epoch 27, Training Loss: 0.9337575786955217\n",
      "Epoch 28, Training Loss: 0.932768496835933\n",
      "Epoch 29, Training Loss: 0.9318120991482454\n",
      "Epoch 30, Training Loss: 0.930893799276913\n",
      "Epoch 31, Training Loss: 0.9300446529248182\n",
      "Epoch 32, Training Loss: 0.9292063365263098\n",
      "Epoch 33, Training Loss: 0.9283840957809897\n",
      "Epoch 34, Training Loss: 0.927598925338072\n",
      "Epoch 35, Training Loss: 0.9268284787149991\n",
      "Epoch 36, Training Loss: 0.9260856798115898\n",
      "Epoch 37, Training Loss: 0.9253602586774266\n",
      "Epoch 38, Training Loss: 0.9246129920202143\n",
      "Epoch 39, Training Loss: 0.9239281867532169\n",
      "Epoch 40, Training Loss: 0.9232293462052065\n",
      "Epoch 41, Training Loss: 0.9225345437666949\n",
      "Epoch 42, Training Loss: 0.9218618428005891\n",
      "Epoch 43, Training Loss: 0.9211920037690331\n",
      "Epoch 44, Training Loss: 0.9205316305861754\n",
      "Epoch 45, Training Loss: 0.9198664966751547\n",
      "Epoch 46, Training Loss: 0.9191977550702937\n",
      "Epoch 47, Training Loss: 0.9185640910793753\n",
      "Epoch 48, Training Loss: 0.9179186630249023\n",
      "Epoch 49, Training Loss: 0.917272167486303\n",
      "Epoch 50, Training Loss: 0.9166242469759548\n",
      "Epoch 51, Training Loss: 0.9159808438665726\n",
      "Epoch 52, Training Loss: 0.9153309308080112\n",
      "Epoch 53, Training Loss: 0.9146834755645079\n",
      "Epoch 54, Training Loss: 0.914036453962326\n",
      "Epoch 55, Training Loss: 0.913389747283038\n",
      "Epoch 56, Training Loss: 0.912727292916354\n",
      "Epoch 57, Training Loss: 0.9120690471985761\n",
      "Epoch 58, Training Loss: 0.9114064800739289\n",
      "Epoch 59, Training Loss: 0.9107092616838567\n",
      "Epoch 60, Training Loss: 0.9100416795646443\n",
      "Epoch 61, Training Loss: 0.909347175359726\n",
      "Epoch 62, Training Loss: 0.9086376436317668\n",
      "Epoch 63, Training Loss: 0.9079166277015911\n",
      "Epoch 64, Training Loss: 0.907181744926116\n",
      "Epoch 65, Training Loss: 0.9064396187137155\n",
      "Epoch 66, Training Loss: 0.9056684045230641\n",
      "Epoch 67, Training Loss: 0.9048874282135683\n",
      "Epoch 68, Training Loss: 0.9040881047529333\n",
      "Epoch 69, Training Loss: 0.9032832952807931\n",
      "Epoch 70, Training Loss: 0.9024252233785741\n",
      "Epoch 71, Training Loss: 0.9015834461240207\n",
      "Epoch 72, Training Loss: 0.9007051845157847\n",
      "Epoch 73, Training Loss: 0.8998039761711569\n",
      "Epoch 74, Training Loss: 0.8988774102575638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:49:45,143] Trial 283 finished with value: 0.5727333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8979331629416522\n",
      "Epoch 1, Training Loss: 0.8944665749270216\n",
      "Epoch 2, Training Loss: 0.8371926409857614\n",
      "Epoch 3, Training Loss: 0.8319806094456436\n",
      "Epoch 4, Training Loss: 0.8245213341892214\n",
      "Epoch 5, Training Loss: 0.8256030072843222\n",
      "Epoch 6, Training Loss: 0.8220925200254398\n",
      "Epoch 7, Training Loss: 0.823359928095251\n",
      "Epoch 8, Training Loss: 0.8211566561146786\n",
      "Epoch 9, Training Loss: 0.821092366634455\n",
      "Epoch 10, Training Loss: 0.8207990174903009\n",
      "Epoch 11, Training Loss: 0.8198180113519941\n",
      "Epoch 12, Training Loss: 0.8178406403477031\n",
      "Epoch 13, Training Loss: 0.8181159467625438\n",
      "Epoch 14, Training Loss: 0.8194750421925595\n",
      "Epoch 15, Training Loss: 0.8183473703556491\n",
      "Epoch 16, Training Loss: 0.8187742927020654\n",
      "Epoch 17, Training Loss: 0.8155319244341743\n",
      "Epoch 18, Training Loss: 0.8195714145674742\n",
      "Epoch 19, Training Loss: 0.816379249454441\n",
      "Epoch 20, Training Loss: 0.8139076888113094\n",
      "Epoch 21, Training Loss: 0.8154744228922335\n",
      "Epoch 22, Training Loss: 0.8149428218827212\n",
      "Epoch 23, Training Loss: 0.8156082523496527\n",
      "Epoch 24, Training Loss: 0.8158674080569045\n",
      "Epoch 25, Training Loss: 0.8138081553287075\n",
      "Epoch 26, Training Loss: 0.8131040971978266\n",
      "Epoch 27, Training Loss: 0.8137049363071757\n",
      "Epoch 28, Training Loss: 0.817061015717069\n",
      "Epoch 29, Training Loss: 0.8153091220927418\n",
      "Epoch 30, Training Loss: 0.8123771195124863\n",
      "Epoch 31, Training Loss: 0.8173143902219328\n",
      "Epoch 32, Training Loss: 0.812259448739819\n",
      "Epoch 33, Training Loss: 0.8148897012373558\n",
      "Epoch 34, Training Loss: 0.8137593663724741\n",
      "Epoch 35, Training Loss: 0.810188300986039\n",
      "Epoch 36, Training Loss: 0.8137378878163216\n",
      "Epoch 37, Training Loss: 0.8125087424328453\n",
      "Epoch 38, Training Loss: 0.8135097112870754\n",
      "Epoch 39, Training Loss: 0.8146536291093754\n",
      "Epoch 40, Training Loss: 0.8107743680028987\n",
      "Epoch 41, Training Loss: 0.811024052308018\n",
      "Epoch 42, Training Loss: 0.8105337665493327\n",
      "Epoch 43, Training Loss: 0.8109532045242481\n",
      "Epoch 44, Training Loss: 0.8143548580936919\n",
      "Epoch 45, Training Loss: 0.8095930532405251\n",
      "Epoch 46, Training Loss: 0.8120911738029997\n",
      "Epoch 47, Training Loss: 0.8112600147275997\n",
      "Epoch 48, Training Loss: 0.809209945909959\n",
      "Epoch 49, Training Loss: 0.810620377386423\n",
      "Epoch 50, Training Loss: 0.8084937243533313\n",
      "Epoch 51, Training Loss: 0.8105726257302708\n",
      "Epoch 52, Training Loss: 0.811542967118715\n",
      "Epoch 53, Training Loss: 0.8053488742140003\n",
      "Epoch 54, Training Loss: 0.8107083801040076\n",
      "Epoch 55, Training Loss: 0.8071856485273605\n",
      "Epoch 56, Training Loss: 0.8117225566304715\n",
      "Epoch 57, Training Loss: 0.809051377611949\n",
      "Epoch 58, Training Loss: 0.8092307154397319\n",
      "Epoch 59, Training Loss: 0.8074775087205988\n",
      "Epoch 60, Training Loss: 0.8076772356391849\n",
      "Epoch 61, Training Loss: 0.8089281831468854\n",
      "Epoch 62, Training Loss: 0.8105972575065785\n",
      "Epoch 63, Training Loss: 0.8069183472403907\n",
      "Epoch 64, Training Loss: 0.8067222211593972\n",
      "Epoch 65, Training Loss: 0.8081133529655915\n",
      "Epoch 66, Training Loss: 0.8089926968839832\n",
      "Epoch 67, Training Loss: 0.8126666189136361\n",
      "Epoch 68, Training Loss: 0.8064779484182372\n",
      "Epoch 69, Training Loss: 0.8081916192420443\n",
      "Epoch 70, Training Loss: 0.8052239410859302\n",
      "Epoch 71, Training Loss: 0.8088779367002329\n",
      "Epoch 72, Training Loss: 0.8065196264955334\n",
      "Epoch 73, Training Loss: 0.8103254164968218\n",
      "Epoch 74, Training Loss: 0.8067584363141455\n",
      "Epoch 75, Training Loss: 0.8076647181259958\n",
      "Epoch 76, Training Loss: 0.8062571797155796\n",
      "Epoch 77, Training Loss: 0.8072405828569168\n",
      "Epoch 78, Training Loss: 0.8044317534991673\n",
      "Epoch 79, Training Loss: 0.8082333701893799\n",
      "Epoch 80, Training Loss: 0.8076513806680091\n",
      "Epoch 81, Training Loss: 0.8065970311487528\n",
      "Epoch 82, Training Loss: 0.8061100348494107\n",
      "Epoch 83, Training Loss: 0.8138243985355349\n",
      "Epoch 84, Training Loss: 0.8080863561845364\n",
      "Epoch 85, Training Loss: 0.8028471684993658\n",
      "Epoch 86, Training Loss: 0.8063134179975754\n",
      "Epoch 87, Training Loss: 0.8046289893917571\n",
      "Epoch 88, Training Loss: 0.8065220648184754\n",
      "Epoch 89, Training Loss: 0.805584744851392\n",
      "Epoch 90, Training Loss: 0.8032782944073354\n",
      "Epoch 91, Training Loss: 0.8088588918062081\n",
      "Epoch 92, Training Loss: 0.8019555937974973\n",
      "Epoch 93, Training Loss: 0.8067715157243542\n",
      "Epoch 94, Training Loss: 0.8069964822073629\n",
      "Epoch 95, Training Loss: 0.8080668395623228\n",
      "Epoch 96, Training Loss: 0.8022048352356244\n",
      "Epoch 97, Training Loss: 0.8065421307893624\n",
      "Epoch 98, Training Loss: 0.8067476467978686\n",
      "Epoch 99, Training Loss: 0.8039683422647921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:51:19,987] Trial 284 finished with value: 0.5822666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8083926943908061\n",
      "Epoch 1, Training Loss: 0.9979668299058326\n",
      "Epoch 2, Training Loss: 0.9534402077359364\n",
      "Epoch 3, Training Loss: 0.9454752968666249\n",
      "Epoch 4, Training Loss: 0.9385256058291385\n",
      "Epoch 5, Training Loss: 0.9315289739379309\n",
      "Epoch 6, Training Loss: 0.9239555930732785\n",
      "Epoch 7, Training Loss: 0.9165842303656098\n",
      "Epoch 8, Training Loss: 0.9097325319634344\n",
      "Epoch 9, Training Loss: 0.9017811974188439\n",
      "Epoch 10, Training Loss: 0.8943666998605083\n",
      "Epoch 11, Training Loss: 0.8864958309589472\n",
      "Epoch 12, Training Loss: 0.8788815422165662\n",
      "Epoch 13, Training Loss: 0.8722419471669017\n",
      "Epoch 14, Training Loss: 0.8650052884467563\n",
      "Epoch 15, Training Loss: 0.8591831396396895\n",
      "Epoch 16, Training Loss: 0.853099765096392\n",
      "Epoch 17, Training Loss: 0.8470539269590737\n",
      "Epoch 18, Training Loss: 0.8431839752914314\n",
      "Epoch 19, Training Loss: 0.8380989735287832\n",
      "Epoch 20, Training Loss: 0.8340718844779452\n",
      "Epoch 21, Training Loss: 0.83070308570575\n",
      "Epoch 22, Training Loss: 0.8278005356179144\n",
      "Epoch 23, Training Loss: 0.8254984936319796\n",
      "Epoch 24, Training Loss: 0.8232438371593791\n",
      "Epoch 25, Training Loss: 0.821164952364183\n",
      "Epoch 26, Training Loss: 0.8194940116172447\n",
      "Epoch 27, Training Loss: 0.8184011212865213\n",
      "Epoch 28, Training Loss: 0.8169832393639069\n",
      "Epoch 29, Training Loss: 0.8153687831154444\n",
      "Epoch 30, Training Loss: 0.8155195791918532\n",
      "Epoch 31, Training Loss: 0.8145043267343277\n",
      "Epoch 32, Training Loss: 0.813157632117881\n",
      "Epoch 33, Training Loss: 0.8128063875929754\n",
      "Epoch 34, Training Loss: 0.8120142083418996\n",
      "Epoch 35, Training Loss: 0.8119879620415824\n",
      "Epoch 36, Training Loss: 0.8115977104445149\n",
      "Epoch 37, Training Loss: 0.8110434022164883\n",
      "Epoch 38, Training Loss: 0.8107182804803202\n",
      "Epoch 39, Training Loss: 0.8106929262777917\n",
      "Epoch 40, Training Loss: 0.8104279016193591\n",
      "Epoch 41, Training Loss: 0.8092762190596502\n",
      "Epoch 42, Training Loss: 0.8095370826864601\n",
      "Epoch 43, Training Loss: 0.8091461283820016\n",
      "Epoch 44, Training Loss: 0.8089763034555248\n",
      "Epoch 45, Training Loss: 0.8087842791600335\n",
      "Epoch 46, Training Loss: 0.8083777323701328\n",
      "Epoch 47, Training Loss: 0.808636122359369\n",
      "Epoch 48, Training Loss: 0.8089018534896966\n",
      "Epoch 49, Training Loss: 0.8080553316532221\n",
      "Epoch 50, Training Loss: 0.8084154281401097\n",
      "Epoch 51, Training Loss: 0.8078866016595884\n",
      "Epoch 52, Training Loss: 0.8084985306388454\n",
      "Epoch 53, Training Loss: 0.8073498591444546\n",
      "Epoch 54, Training Loss: 0.807625881352819\n",
      "Epoch 55, Training Loss: 0.8072287118524537\n",
      "Epoch 56, Training Loss: 0.806364889790241\n",
      "Epoch 57, Training Loss: 0.8064209180666988\n",
      "Epoch 58, Training Loss: 0.8067100587644075\n",
      "Epoch 59, Training Loss: 0.8060090200793474\n",
      "Epoch 60, Training Loss: 0.8062805739560521\n",
      "Epoch 61, Training Loss: 0.8057979303195064\n",
      "Epoch 62, Training Loss: 0.8062259172138415\n",
      "Epoch 63, Training Loss: 0.8060122414639121\n",
      "Epoch 64, Training Loss: 0.8066200499247788\n",
      "Epoch 65, Training Loss: 0.8053684905955666\n",
      "Epoch 66, Training Loss: 0.805668424484425\n",
      "Epoch 67, Training Loss: 0.8049352516805319\n",
      "Epoch 68, Training Loss: 0.8051545650439155\n",
      "Epoch 69, Training Loss: 0.8047056082496069\n",
      "Epoch 70, Training Loss: 0.8051517781458403\n",
      "Epoch 71, Training Loss: 0.8045499853621748\n",
      "Epoch 72, Training Loss: 0.8043513807139002\n",
      "Epoch 73, Training Loss: 0.8042805108808934\n",
      "Epoch 74, Training Loss: 0.8043083083360715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:52:22,191] Trial 285 finished with value: 0.6333333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8044020878641229\n",
      "Epoch 1, Training Loss: 0.9057317143992374\n",
      "Epoch 2, Training Loss: 0.8204159901554423\n",
      "Epoch 3, Training Loss: 0.8171202217725883\n",
      "Epoch 4, Training Loss: 0.8117710858359373\n",
      "Epoch 5, Training Loss: 0.8094307726487181\n",
      "Epoch 6, Training Loss: 0.8086637615261222\n",
      "Epoch 7, Training Loss: 0.8076506259746121\n",
      "Epoch 8, Training Loss: 0.8056124691676376\n",
      "Epoch 9, Training Loss: 0.8059845494148427\n",
      "Epoch 10, Training Loss: 0.8070032299909377\n",
      "Epoch 11, Training Loss: 0.804759677101795\n",
      "Epoch 12, Training Loss: 0.8048927901382733\n",
      "Epoch 13, Training Loss: 0.8059219373796219\n",
      "Epoch 14, Training Loss: 0.8052998558022922\n",
      "Epoch 15, Training Loss: 0.804040581570532\n",
      "Epoch 16, Training Loss: 0.8030597654500402\n",
      "Epoch 17, Training Loss: 0.8035986306075763\n",
      "Epoch 18, Training Loss: 0.802886268160397\n",
      "Epoch 19, Training Loss: 0.803760139924243\n",
      "Epoch 20, Training Loss: 0.802314305395112\n",
      "Epoch 21, Training Loss: 0.8028291950548502\n",
      "Epoch 22, Training Loss: 0.8024175364272039\n",
      "Epoch 23, Training Loss: 0.801713172206305\n",
      "Epoch 24, Training Loss: 0.8020076114432256\n",
      "Epoch 25, Training Loss: 0.8022617825888153\n",
      "Epoch 26, Training Loss: 0.8015151085710167\n",
      "Epoch 27, Training Loss: 0.8012410407675836\n",
      "Epoch 28, Training Loss: 0.8013551571315393\n",
      "Epoch 29, Training Loss: 0.8010643086935344\n",
      "Epoch 30, Training Loss: 0.80031905228034\n",
      "Epoch 31, Training Loss: 0.8013968959786838\n",
      "Epoch 32, Training Loss: 0.8014398724512947\n",
      "Epoch 33, Training Loss: 0.7998484665289858\n",
      "Epoch 34, Training Loss: 0.8005842007192454\n",
      "Epoch 35, Training Loss: 0.8010322010606752\n",
      "Epoch 36, Training Loss: 0.799913868240844\n",
      "Epoch 37, Training Loss: 0.8004744050198032\n",
      "Epoch 38, Training Loss: 0.8007949060067199\n",
      "Epoch 39, Training Loss: 0.8007731797103596\n",
      "Epoch 40, Training Loss: 0.7997304388454982\n",
      "Epoch 41, Training Loss: 0.7992883846275789\n",
      "Epoch 42, Training Loss: 0.7995288363973001\n",
      "Epoch 43, Training Loss: 0.7995609606119026\n",
      "Epoch 44, Training Loss: 0.798866221510378\n",
      "Epoch 45, Training Loss: 0.7995940421756945\n",
      "Epoch 46, Training Loss: 0.7988053395335836\n",
      "Epoch 47, Training Loss: 0.7991554315825155\n",
      "Epoch 48, Training Loss: 0.7996956683639297\n",
      "Epoch 49, Training Loss: 0.7988252052687165\n",
      "Epoch 50, Training Loss: 0.7978700276604273\n",
      "Epoch 51, Training Loss: 0.7991351208292452\n",
      "Epoch 52, Training Loss: 0.7989828794522393\n",
      "Epoch 53, Training Loss: 0.7975684463977813\n",
      "Epoch 54, Training Loss: 0.7979487619005647\n",
      "Epoch 55, Training Loss: 0.7977990241875326\n",
      "Epoch 56, Training Loss: 0.7981253172221937\n",
      "Epoch 57, Training Loss: 0.7981198678339334\n",
      "Epoch 58, Training Loss: 0.797038165519112\n",
      "Epoch 59, Training Loss: 0.7972132868336556\n",
      "Epoch 60, Training Loss: 0.7967445858439108\n",
      "Epoch 61, Training Loss: 0.7972270779143599\n",
      "Epoch 62, Training Loss: 0.7972482218778223\n",
      "Epoch 63, Training Loss: 0.7966836787704238\n",
      "Epoch 64, Training Loss: 0.7964338347427827\n",
      "Epoch 65, Training Loss: 0.7966401313480578\n",
      "Epoch 66, Training Loss: 0.7966638840230784\n",
      "Epoch 67, Training Loss: 0.7959181534616571\n",
      "Epoch 68, Training Loss: 0.7967298854562573\n",
      "Epoch 69, Training Loss: 0.7961509138121641\n",
      "Epoch 70, Training Loss: 0.7960670149415956\n",
      "Epoch 71, Training Loss: 0.7966363145892782\n",
      "Epoch 72, Training Loss: 0.7955160975456238\n",
      "Epoch 73, Training Loss: 0.7958429082891995\n",
      "Epoch 74, Training Loss: 0.7959049775188131\n",
      "Epoch 75, Training Loss: 0.7965220120616425\n",
      "Epoch 76, Training Loss: 0.7958023213802423\n",
      "Epoch 77, Training Loss: 0.7951553296325798\n",
      "Epoch 78, Training Loss: 0.7955665423457784\n",
      "Epoch 79, Training Loss: 0.7954024828466257\n",
      "Epoch 80, Training Loss: 0.7944623646879555\n",
      "Epoch 81, Training Loss: 0.7946306833647248\n",
      "Epoch 82, Training Loss: 0.7942327741393469\n",
      "Epoch 83, Training Loss: 0.7946874577299993\n",
      "Epoch 84, Training Loss: 0.794876077479886\n",
      "Epoch 85, Training Loss: 0.7946094517421005\n",
      "Epoch 86, Training Loss: 0.794694157292072\n",
      "Epoch 87, Training Loss: 0.795275240285056\n",
      "Epoch 88, Training Loss: 0.7943806714581367\n",
      "Epoch 89, Training Loss: 0.7944128020365435\n",
      "Epoch 90, Training Loss: 0.7945402727091223\n",
      "Epoch 91, Training Loss: 0.7936313911488182\n",
      "Epoch 92, Training Loss: 0.7937180065570917\n",
      "Epoch 93, Training Loss: 0.7931001100325047\n",
      "Epoch 94, Training Loss: 0.7936178668997341\n",
      "Epoch 95, Training Loss: 0.7939893217015087\n",
      "Epoch 96, Training Loss: 0.7935300898731203\n",
      "Epoch 97, Training Loss: 0.7938919456381547\n",
      "Epoch 98, Training Loss: 0.7934651727963211\n",
      "Epoch 99, Training Loss: 0.7933494298977959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:54:01,136] Trial 286 finished with value: 0.6296 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7924760852541243\n",
      "Epoch 1, Training Loss: 0.9331642175421996\n",
      "Epoch 2, Training Loss: 0.8728752107479993\n",
      "Epoch 3, Training Loss: 0.8301986796014449\n",
      "Epoch 4, Training Loss: 0.8118953281991622\n",
      "Epoch 5, Training Loss: 0.8060646686834447\n",
      "Epoch 6, Training Loss: 0.8037234858905568\n",
      "Epoch 7, Training Loss: 0.8024103140129762\n",
      "Epoch 8, Training Loss: 0.8012908458008485\n",
      "Epoch 9, Training Loss: 0.8004526948227602\n",
      "Epoch 10, Training Loss: 0.8000374477751114\n",
      "Epoch 11, Training Loss: 0.7996161793260014\n",
      "Epoch 12, Training Loss: 0.7993580806956572\n",
      "Epoch 13, Training Loss: 0.7986886638052323\n",
      "Epoch 14, Training Loss: 0.7984873956091264\n",
      "Epoch 15, Training Loss: 0.798377595578923\n",
      "Epoch 16, Training Loss: 0.798068021325504\n",
      "Epoch 17, Training Loss: 0.7979666550019208\n",
      "Epoch 18, Training Loss: 0.797794727157144\n",
      "Epoch 19, Training Loss: 0.7974508465738858\n",
      "Epoch 20, Training Loss: 0.7971474246417775\n",
      "Epoch 21, Training Loss: 0.7968774468758527\n",
      "Epoch 22, Training Loss: 0.7967818006347207\n",
      "Epoch 23, Training Loss: 0.7967090336715474\n",
      "Epoch 24, Training Loss: 0.7961494763458477\n",
      "Epoch 25, Training Loss: 0.7956528773027308\n",
      "Epoch 26, Training Loss: 0.795854652559056\n",
      "Epoch 27, Training Loss: 0.7953795972992392\n",
      "Epoch 28, Training Loss: 0.7948473041197833\n",
      "Epoch 29, Training Loss: 0.7944425264526817\n",
      "Epoch 30, Training Loss: 0.7937709133064046\n",
      "Epoch 31, Training Loss: 0.7934656665605657\n",
      "Epoch 32, Training Loss: 0.7931070351600646\n",
      "Epoch 33, Training Loss: 0.7926539931577795\n",
      "Epoch 34, Training Loss: 0.792565284055822\n",
      "Epoch 35, Training Loss: 0.7922348569421207\n",
      "Epoch 36, Training Loss: 0.7921956039176268\n",
      "Epoch 37, Training Loss: 0.7916908633708953\n",
      "Epoch 38, Training Loss: 0.7917061132543227\n",
      "Epoch 39, Training Loss: 0.7912078214392942\n",
      "Epoch 40, Training Loss: 0.7910212306415334\n",
      "Epoch 41, Training Loss: 0.7907577389128068\n",
      "Epoch 42, Training Loss: 0.7905969294379739\n",
      "Epoch 43, Training Loss: 0.7908850892852335\n",
      "Epoch 44, Training Loss: 0.790312956010594\n",
      "Epoch 45, Training Loss: 0.7907364131422604\n",
      "Epoch 46, Training Loss: 0.7907337723760044\n",
      "Epoch 47, Training Loss: 0.7903943899098564\n",
      "Epoch 48, Training Loss: 0.789751740623923\n",
      "Epoch 49, Training Loss: 0.7897280836105347\n",
      "Epoch 50, Training Loss: 0.7898689948110019\n",
      "Epoch 51, Training Loss: 0.7898888667190777\n",
      "Epoch 52, Training Loss: 0.7895694111375248\n",
      "Epoch 53, Training Loss: 0.7895171979595633\n",
      "Epoch 54, Training Loss: 0.7899712045052473\n",
      "Epoch 55, Training Loss: 0.7895393532865188\n",
      "Epoch 56, Training Loss: 0.7895429524253397\n",
      "Epoch 57, Training Loss: 0.7894066985915689\n",
      "Epoch 58, Training Loss: 0.7893839138395646\n",
      "Epoch 59, Training Loss: 0.7895986352948582\n",
      "Epoch 60, Training Loss: 0.7894821240621455\n",
      "Epoch 61, Training Loss: 0.7892794999655556\n",
      "Epoch 62, Training Loss: 0.7892396959837745\n",
      "Epoch 63, Training Loss: 0.7891009584595176\n",
      "Epoch 64, Training Loss: 0.7890469403126661\n",
      "Epoch 65, Training Loss: 0.7889937120325425\n",
      "Epoch 66, Training Loss: 0.788784712973763\n",
      "Epoch 67, Training Loss: 0.7888033302391276\n",
      "Epoch 68, Training Loss: 0.7891778443140142\n",
      "Epoch 69, Training Loss: 0.7890465924319099\n",
      "Epoch 70, Training Loss: 0.7887664058629205\n",
      "Epoch 71, Training Loss: 0.7891030071763432\n",
      "Epoch 72, Training Loss: 0.78870948973824\n",
      "Epoch 73, Training Loss: 0.788466215764775\n",
      "Epoch 74, Training Loss: 0.7889167375424329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:55:30,498] Trial 287 finished with value: 0.6385333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7885639526563533\n",
      "Epoch 1, Training Loss: 0.8974952265094308\n",
      "Epoch 2, Training Loss: 0.8281091847840477\n",
      "Epoch 3, Training Loss: 0.8212853003249448\n",
      "Epoch 4, Training Loss: 0.8173143501141492\n",
      "Epoch 5, Training Loss: 0.8145463364264545\n",
      "Epoch 6, Training Loss: 0.8123299558723674\n",
      "Epoch 7, Training Loss: 0.8107177061894361\n",
      "Epoch 8, Training Loss: 0.8104596472487731\n",
      "Epoch 9, Training Loss: 0.8092139940402087\n",
      "Epoch 10, Training Loss: 0.8088746091197518\n",
      "Epoch 11, Training Loss: 0.8068002197321723\n",
      "Epoch 12, Training Loss: 0.8060847683513865\n",
      "Epoch 13, Training Loss: 0.8069360897120308\n",
      "Epoch 14, Training Loss: 0.8055725310830509\n",
      "Epoch 15, Training Loss: 0.8050466316587784\n",
      "Epoch 16, Training Loss: 0.805334331778919\n",
      "Epoch 17, Training Loss: 0.8048925323346082\n",
      "Epoch 18, Training Loss: 0.8044156842371997\n",
      "Epoch 19, Training Loss: 0.8034979869337643\n",
      "Epoch 20, Training Loss: 0.8033585306476144\n",
      "Epoch 21, Training Loss: 0.8026784060983096\n",
      "Epoch 22, Training Loss: 0.8026781700639164\n",
      "Epoch 23, Training Loss: 0.8019830717759974\n",
      "Epoch 24, Training Loss: 0.8018134173224954\n",
      "Epoch 25, Training Loss: 0.8011073344595292\n",
      "Epoch 26, Training Loss: 0.8006726842768052\n",
      "Epoch 27, Training Loss: 0.8000797621642842\n",
      "Epoch 28, Training Loss: 0.7994379774261924\n",
      "Epoch 29, Training Loss: 0.7995851983743555\n",
      "Epoch 30, Training Loss: 0.7990577835195205\n",
      "Epoch 31, Training Loss: 0.7990025964204003\n",
      "Epoch 32, Training Loss: 0.7985331897875841\n",
      "Epoch 33, Training Loss: 0.7988290059566497\n",
      "Epoch 34, Training Loss: 0.7982756346113542\n",
      "Epoch 35, Training Loss: 0.7980063965741326\n",
      "Epoch 36, Training Loss: 0.797958508309196\n",
      "Epoch 37, Training Loss: 0.7975427124780767\n",
      "Epoch 38, Training Loss: 0.7971497659122243\n",
      "Epoch 39, Training Loss: 0.7968329219257131\n",
      "Epoch 40, Training Loss: 0.7970421843668993\n",
      "Epoch 41, Training Loss: 0.7970996975898743\n",
      "Epoch 42, Training Loss: 0.796754404516781\n",
      "Epoch 43, Training Loss: 0.7968267125943128\n",
      "Epoch 44, Training Loss: 0.796443237136392\n",
      "Epoch 45, Training Loss: 0.79602146877962\n",
      "Epoch 46, Training Loss: 0.7959522906471701\n",
      "Epoch 47, Training Loss: 0.7960368342259351\n",
      "Epoch 48, Training Loss: 0.7960201801272\n",
      "Epoch 49, Training Loss: 0.7953994734848246\n",
      "Epoch 50, Training Loss: 0.7959379259979024\n",
      "Epoch 51, Training Loss: 0.7957130265937132\n",
      "Epoch 52, Training Loss: 0.7953110739764045\n",
      "Epoch 53, Training Loss: 0.7953190152785358\n",
      "Epoch 54, Training Loss: 0.7951980993326973\n",
      "Epoch 55, Training Loss: 0.7950963891954983\n",
      "Epoch 56, Training Loss: 0.7950625738676856\n",
      "Epoch 57, Training Loss: 0.794642520722221\n",
      "Epoch 58, Training Loss: 0.794712824681226\n",
      "Epoch 59, Training Loss: 0.7947638990598567\n",
      "Epoch 60, Training Loss: 0.7941353398210862\n",
      "Epoch 61, Training Loss: 0.7945477644836202\n",
      "Epoch 62, Training Loss: 0.7941974920385024\n",
      "Epoch 63, Training Loss: 0.7943989572805517\n",
      "Epoch 64, Training Loss: 0.7942474202548756\n",
      "Epoch 65, Training Loss: 0.7936113999871647\n",
      "Epoch 66, Training Loss: 0.7939888733274797\n",
      "Epoch 67, Training Loss: 0.7939513198768391\n",
      "Epoch 68, Training Loss: 0.7939415564957787\n",
      "Epoch 69, Training Loss: 0.7936976240662967\n",
      "Epoch 70, Training Loss: 0.7937616747968337\n",
      "Epoch 71, Training Loss: 0.793220267716576\n",
      "Epoch 72, Training Loss: 0.7931827303241281\n",
      "Epoch 73, Training Loss: 0.7930703435925877\n",
      "Epoch 74, Training Loss: 0.7930145105193643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:56:54,697] Trial 288 finished with value: 0.6374666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7934562516212463\n",
      "Epoch 1, Training Loss: 0.8707235417867961\n",
      "Epoch 2, Training Loss: 0.8276846521779111\n",
      "Epoch 3, Training Loss: 0.8236836590264973\n",
      "Epoch 4, Training Loss: 0.8181550204305721\n",
      "Epoch 5, Training Loss: 0.8164126628323605\n",
      "Epoch 6, Training Loss: 0.8129450969229963\n",
      "Epoch 7, Training Loss: 0.8122819257858104\n",
      "Epoch 8, Training Loss: 0.8125987904412406\n",
      "Epoch 9, Training Loss: 0.8108878698564114\n",
      "Epoch 10, Training Loss: 0.8107011486713152\n",
      "Epoch 11, Training Loss: 0.8091715396795057\n",
      "Epoch 12, Training Loss: 0.8085169889873132\n",
      "Epoch 13, Training Loss: 0.8079152572423892\n",
      "Epoch 14, Training Loss: 0.8073950345354869\n",
      "Epoch 15, Training Loss: 0.8071073180302641\n",
      "Epoch 16, Training Loss: 0.806023379196798\n",
      "Epoch 17, Training Loss: 0.8076028924239309\n",
      "Epoch 18, Training Loss: 0.80575016028899\n",
      "Epoch 19, Training Loss: 0.805973255903201\n",
      "Epoch 20, Training Loss: 0.8054905443263233\n",
      "Epoch 21, Training Loss: 0.8052946064705239\n",
      "Epoch 22, Training Loss: 0.8038377337437823\n",
      "Epoch 23, Training Loss: 0.8052894911371675\n",
      "Epoch 24, Training Loss: 0.8046249647785847\n",
      "Epoch 25, Training Loss: 0.8037436253146122\n",
      "Epoch 26, Training Loss: 0.8035666261400495\n",
      "Epoch 27, Training Loss: 0.8043227993456045\n",
      "Epoch 28, Training Loss: 0.8027989010165508\n",
      "Epoch 29, Training Loss: 0.8030141272042927\n",
      "Epoch 30, Training Loss: 0.8034327171798935\n",
      "Epoch 31, Training Loss: 0.8029442172301443\n",
      "Epoch 32, Training Loss: 0.8026938495779397\n",
      "Epoch 33, Training Loss: 0.8034353613853454\n",
      "Epoch 34, Training Loss: 0.8036822658732421\n",
      "Epoch 35, Training Loss: 0.802436555776381\n",
      "Epoch 36, Training Loss: 0.8034357573753013\n",
      "Epoch 37, Training Loss: 0.8024405243701505\n",
      "Epoch 38, Training Loss: 0.8029715509342968\n",
      "Epoch 39, Training Loss: 0.8026867593141427\n",
      "Epoch 40, Training Loss: 0.8023762326939662\n",
      "Epoch 41, Training Loss: 0.8016675744737898\n",
      "Epoch 42, Training Loss: 0.802246864487354\n",
      "Epoch 43, Training Loss: 0.8033768265767205\n",
      "Epoch 44, Training Loss: 0.8018326474311657\n",
      "Epoch 45, Training Loss: 0.8027371750738388\n",
      "Epoch 46, Training Loss: 0.8025895854584256\n",
      "Epoch 47, Training Loss: 0.8013401552250511\n",
      "Epoch 48, Training Loss: 0.8020548215486053\n",
      "Epoch 49, Training Loss: 0.8017678721506792\n",
      "Epoch 50, Training Loss: 0.8018815755844116\n",
      "Epoch 51, Training Loss: 0.8011304362375934\n",
      "Epoch 52, Training Loss: 0.801417761099966\n",
      "Epoch 53, Training Loss: 0.8010585193347214\n",
      "Epoch 54, Training Loss: 0.8014509389274999\n",
      "Epoch 55, Training Loss: 0.8018548651745445\n",
      "Epoch 56, Training Loss: 0.8004763048394282\n",
      "Epoch 57, Training Loss: 0.8009472646211323\n",
      "Epoch 58, Training Loss: 0.800700440980438\n",
      "Epoch 59, Training Loss: 0.801251440926602\n",
      "Epoch 60, Training Loss: 0.8007442455542715\n",
      "Epoch 61, Training Loss: 0.8010458685401687\n",
      "Epoch 62, Training Loss: 0.8008743104181791\n",
      "Epoch 63, Training Loss: 0.8002173789461753\n",
      "Epoch 64, Training Loss: 0.8004533426205914\n",
      "Epoch 65, Training Loss: 0.8011363923101497\n",
      "Epoch 66, Training Loss: 0.8001022126441611\n",
      "Epoch 67, Training Loss: 0.8001162921575675\n",
      "Epoch 68, Training Loss: 0.8005857380709254\n",
      "Epoch 69, Training Loss: 0.8005041396707521\n",
      "Epoch 70, Training Loss: 0.7997560616722681\n",
      "Epoch 71, Training Loss: 0.7999616681184983\n",
      "Epoch 72, Training Loss: 0.8006344883065475\n",
      "Epoch 73, Training Loss: 0.8003702471130772\n",
      "Epoch 74, Training Loss: 0.8002408744697284\n",
      "Epoch 75, Training Loss: 0.8005631626100469\n",
      "Epoch 76, Training Loss: 0.8014317580631801\n",
      "Epoch 77, Training Loss: 0.7994589785884197\n",
      "Epoch 78, Training Loss: 0.7996584916473332\n",
      "Epoch 79, Training Loss: 0.7992277821203819\n",
      "Epoch 80, Training Loss: 0.8001522373436089\n",
      "Epoch 81, Training Loss: 0.8001987428593457\n",
      "Epoch 82, Training Loss: 0.8000333409560354\n",
      "Epoch 83, Training Loss: 0.7999312293260618\n",
      "Epoch 84, Training Loss: 0.7999873396149255\n",
      "Epoch 85, Training Loss: 0.8005928467090865\n",
      "Epoch 86, Training Loss: 0.7997026123498615\n",
      "Epoch 87, Training Loss: 0.8004167174934445\n",
      "Epoch 88, Training Loss: 0.8000080617746912\n",
      "Epoch 89, Training Loss: 0.8002218218674337\n",
      "Epoch 90, Training Loss: 0.7999959881144358\n",
      "Epoch 91, Training Loss: 0.8004281023391208\n",
      "Epoch 92, Training Loss: 0.7995229313248082\n",
      "Epoch 93, Training Loss: 0.7992942296024552\n",
      "Epoch 94, Training Loss: 0.7994567408597559\n",
      "Epoch 95, Training Loss: 0.7993642943246024\n",
      "Epoch 96, Training Loss: 0.799509613047865\n",
      "Epoch 97, Training Loss: 0.8000717822770427\n",
      "Epoch 98, Training Loss: 0.7991723039096459\n",
      "Epoch 99, Training Loss: 0.8001177250890803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 16:58:29,589] Trial 289 finished with value: 0.6069333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7994719562673928\n",
      "Epoch 1, Training Loss: 0.9992816703660148\n",
      "Epoch 2, Training Loss: 0.9503357478550503\n",
      "Epoch 3, Training Loss: 0.9394340931921077\n",
      "Epoch 4, Training Loss: 0.9282921696067753\n",
      "Epoch 5, Training Loss: 0.9147015862895134\n",
      "Epoch 6, Training Loss: 0.899060921292556\n",
      "Epoch 7, Training Loss: 0.8808839425108487\n",
      "Epoch 8, Training Loss: 0.86246412956625\n",
      "Epoch 9, Training Loss: 0.8473161718002835\n",
      "Epoch 10, Training Loss: 0.8355945623010621\n",
      "Epoch 11, Training Loss: 0.8267672667826028\n",
      "Epoch 12, Training Loss: 0.8218483603986583\n",
      "Epoch 13, Training Loss: 0.8182602480838174\n",
      "Epoch 14, Training Loss: 0.8162507767964127\n",
      "Epoch 15, Training Loss: 0.8142056767205547\n",
      "Epoch 16, Training Loss: 0.8135839884442494\n",
      "Epoch 17, Training Loss: 0.8126139042072726\n",
      "Epoch 18, Training Loss: 0.811416987279304\n",
      "Epoch 19, Training Loss: 0.8112114034200969\n",
      "Epoch 20, Training Loss: 0.8104156991592923\n",
      "Epoch 21, Training Loss: 0.8097037880940545\n",
      "Epoch 22, Training Loss: 0.8094613486662843\n",
      "Epoch 23, Training Loss: 0.8087258712689679\n",
      "Epoch 24, Training Loss: 0.8079061840709887\n",
      "Epoch 25, Training Loss: 0.8075962677037806\n",
      "Epoch 26, Training Loss: 0.8066762188323459\n",
      "Epoch 27, Training Loss: 0.8065234533826211\n",
      "Epoch 28, Training Loss: 0.8060625956470805\n",
      "Epoch 29, Training Loss: 0.8051655006587953\n",
      "Epoch 30, Training Loss: 0.805062270702276\n",
      "Epoch 31, Training Loss: 0.8047871987622484\n",
      "Epoch 32, Training Loss: 0.8048948150828369\n",
      "Epoch 33, Training Loss: 0.8042573577479313\n",
      "Epoch 34, Training Loss: 0.8041074539485731\n",
      "Epoch 35, Training Loss: 0.8038426722799029\n",
      "Epoch 36, Training Loss: 0.8033828398338834\n",
      "Epoch 37, Training Loss: 0.8036884358054713\n",
      "Epoch 38, Training Loss: 0.8033780443937258\n",
      "Epoch 39, Training Loss: 0.8022262142116862\n",
      "Epoch 40, Training Loss: 0.8018390943233232\n",
      "Epoch 41, Training Loss: 0.8016822014536177\n",
      "Epoch 42, Training Loss: 0.801657365945945\n",
      "Epoch 43, Training Loss: 0.8010144015900175\n",
      "Epoch 44, Training Loss: 0.8008742083284192\n",
      "Epoch 45, Training Loss: 0.8013172408691922\n",
      "Epoch 46, Training Loss: 0.8005568554526881\n",
      "Epoch 47, Training Loss: 0.8012694286224538\n",
      "Epoch 48, Training Loss: 0.8011220641602251\n",
      "Epoch 49, Training Loss: 0.8000492230393833\n",
      "Epoch 50, Training Loss: 0.7998516531815206\n",
      "Epoch 51, Training Loss: 0.7997703871332613\n",
      "Epoch 52, Training Loss: 0.7999117147653623\n",
      "Epoch 53, Training Loss: 0.7990539966669298\n",
      "Epoch 54, Training Loss: 0.7998626423061342\n",
      "Epoch 55, Training Loss: 0.7988301927433874\n",
      "Epoch 56, Training Loss: 0.7988029015691657\n",
      "Epoch 57, Training Loss: 0.7988797829563457\n",
      "Epoch 58, Training Loss: 0.7989085541631943\n",
      "Epoch 59, Training Loss: 0.7984708006220652\n",
      "Epoch 60, Training Loss: 0.7988488666993335\n",
      "Epoch 61, Training Loss: 0.7988052845897531\n",
      "Epoch 62, Training Loss: 0.7988771637579553\n",
      "Epoch 63, Training Loss: 0.7980515499760333\n",
      "Epoch 64, Training Loss: 0.7979590299434232\n",
      "Epoch 65, Training Loss: 0.7986427145793026\n",
      "Epoch 66, Training Loss: 0.7980430930180658\n",
      "Epoch 67, Training Loss: 0.7975238086585712\n",
      "Epoch 68, Training Loss: 0.7981256287797053\n",
      "Epoch 69, Training Loss: 0.7983609571492761\n",
      "Epoch 70, Training Loss: 0.798146330772486\n",
      "Epoch 71, Training Loss: 0.7984697306962838\n",
      "Epoch 72, Training Loss: 0.7970215541079528\n",
      "Epoch 73, Training Loss: 0.7973327165259454\n",
      "Epoch 74, Training Loss: 0.7980992679309128\n",
      "Epoch 75, Training Loss: 0.7975412456612838\n",
      "Epoch 76, Training Loss: 0.7970364125151383\n",
      "Epoch 77, Training Loss: 0.7970970704143209\n",
      "Epoch 78, Training Loss: 0.7973409903676887\n",
      "Epoch 79, Training Loss: 0.7969071669237954\n",
      "Epoch 80, Training Loss: 0.7969156998440735\n",
      "Epoch 81, Training Loss: 0.7970211327524114\n",
      "Epoch 82, Training Loss: 0.7967027771741824\n",
      "Epoch 83, Training Loss: 0.7971898629253072\n",
      "Epoch 84, Training Loss: 0.797320268476816\n",
      "Epoch 85, Training Loss: 0.7969432026820076\n",
      "Epoch 86, Training Loss: 0.796447619818207\n",
      "Epoch 87, Training Loss: 0.7969597569981912\n",
      "Epoch 88, Training Loss: 0.7967040036853991\n",
      "Epoch 89, Training Loss: 0.7970399457709234\n",
      "Epoch 90, Training Loss: 0.7964889023985181\n",
      "Epoch 91, Training Loss: 0.7970865524801096\n",
      "Epoch 92, Training Loss: 0.7964220171584222\n",
      "Epoch 93, Training Loss: 0.7964658206566833\n",
      "Epoch 94, Training Loss: 0.7959914855939105\n",
      "Epoch 95, Training Loss: 0.7964110396858445\n",
      "Epoch 96, Training Loss: 0.7962132278241609\n",
      "Epoch 97, Training Loss: 0.7970541652880216\n",
      "Epoch 98, Training Loss: 0.7960510291551289\n",
      "Epoch 99, Training Loss: 0.7961602153634666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:00:01,900] Trial 290 finished with value: 0.6335333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7963596153976326\n",
      "Epoch 1, Training Loss: 0.9792042048353898\n",
      "Epoch 2, Training Loss: 0.9479751028512654\n",
      "Epoch 3, Training Loss: 0.9291362833259698\n",
      "Epoch 4, Training Loss: 0.9041658418519156\n",
      "Epoch 5, Training Loss: 0.8717190092667602\n",
      "Epoch 6, Training Loss: 0.8420439540891719\n",
      "Epoch 7, Training Loss: 0.8236402703407115\n",
      "Epoch 8, Training Loss: 0.8162149422150805\n",
      "Epoch 9, Training Loss: 0.8133612255404766\n",
      "Epoch 10, Training Loss: 0.8111054140822331\n",
      "Epoch 11, Training Loss: 0.8098857531870218\n",
      "Epoch 12, Training Loss: 0.8089295779852043\n",
      "Epoch 13, Training Loss: 0.8079815838569985\n",
      "Epoch 14, Training Loss: 0.8080551306107887\n",
      "Epoch 15, Training Loss: 0.8068295947591165\n",
      "Epoch 16, Training Loss: 0.8054798157591568\n",
      "Epoch 17, Training Loss: 0.8040820452503692\n",
      "Epoch 18, Training Loss: 0.804208094822733\n",
      "Epoch 19, Training Loss: 0.8033085212671667\n",
      "Epoch 20, Training Loss: 0.8030547843839889\n",
      "Epoch 21, Training Loss: 0.803083286249548\n",
      "Epoch 22, Training Loss: 0.8026258812811141\n",
      "Epoch 23, Training Loss: 0.8013885921105406\n",
      "Epoch 24, Training Loss: 0.8011798310996895\n",
      "Epoch 25, Training Loss: 0.8009202324358144\n",
      "Epoch 26, Training Loss: 0.8010162041599589\n",
      "Epoch 27, Training Loss: 0.8014011077414778\n",
      "Epoch 28, Training Loss: 0.800859895057248\n",
      "Epoch 29, Training Loss: 0.8002214516912188\n",
      "Epoch 30, Training Loss: 0.8002253423059793\n",
      "Epoch 31, Training Loss: 0.7998356713388199\n",
      "Epoch 32, Training Loss: 0.7997873574271238\n",
      "Epoch 33, Training Loss: 0.7996573606828101\n",
      "Epoch 34, Training Loss: 0.798935087641379\n",
      "Epoch 35, Training Loss: 0.7990618196645177\n",
      "Epoch 36, Training Loss: 0.8001085209667235\n",
      "Epoch 37, Training Loss: 0.7991963856202319\n",
      "Epoch 38, Training Loss: 0.7984233484680491\n",
      "Epoch 39, Training Loss: 0.7989695561559577\n",
      "Epoch 40, Training Loss: 0.7987890273108518\n",
      "Epoch 41, Training Loss: 0.7991978087819609\n",
      "Epoch 42, Training Loss: 0.7983340240062627\n",
      "Epoch 43, Training Loss: 0.7982364012782736\n",
      "Epoch 44, Training Loss: 0.7983760478801297\n",
      "Epoch 45, Training Loss: 0.7983070730266715\n",
      "Epoch 46, Training Loss: 0.7978469793957875\n",
      "Epoch 47, Training Loss: 0.7978747144677585\n",
      "Epoch 48, Training Loss: 0.7985798403732759\n",
      "Epoch 49, Training Loss: 0.7980220941672648\n",
      "Epoch 50, Training Loss: 0.7979451689505039\n",
      "Epoch 51, Training Loss: 0.7980056073432579\n",
      "Epoch 52, Training Loss: 0.7979900049984007\n",
      "Epoch 53, Training Loss: 0.797647025710658\n",
      "Epoch 54, Training Loss: 0.7975730283816058\n",
      "Epoch 55, Training Loss: 0.7978467631160765\n",
      "Epoch 56, Training Loss: 0.7974673884255545\n",
      "Epoch 57, Training Loss: 0.7976679639708727\n",
      "Epoch 58, Training Loss: 0.797335696489291\n",
      "Epoch 59, Training Loss: 0.7971534237825781\n",
      "Epoch 60, Training Loss: 0.7970959857890481\n",
      "Epoch 61, Training Loss: 0.7969022978517346\n",
      "Epoch 62, Training Loss: 0.7968351933292876\n",
      "Epoch 63, Training Loss: 0.796926334388274\n",
      "Epoch 64, Training Loss: 0.7975193913717915\n",
      "Epoch 65, Training Loss: 0.7965506536620004\n",
      "Epoch 66, Training Loss: 0.7965888650793779\n",
      "Epoch 67, Training Loss: 0.796396906035287\n",
      "Epoch 68, Training Loss: 0.7969936729373789\n",
      "Epoch 69, Training Loss: 0.7961648385327561\n",
      "Epoch 70, Training Loss: 0.7962829264483058\n",
      "Epoch 71, Training Loss: 0.7957892832899452\n",
      "Epoch 72, Training Loss: 0.7970611706712192\n",
      "Epoch 73, Training Loss: 0.7966846732268656\n",
      "Epoch 74, Training Loss: 0.7966833576223904\n",
      "Epoch 75, Training Loss: 0.7960579691076637\n",
      "Epoch 76, Training Loss: 0.7957905973706927\n",
      "Epoch 77, Training Loss: 0.7956123923896847\n",
      "Epoch 78, Training Loss: 0.7953319020737383\n",
      "Epoch 79, Training Loss: 0.7956650048270262\n",
      "Epoch 80, Training Loss: 0.7951057043290676\n",
      "Epoch 81, Training Loss: 0.7959883178983416\n",
      "Epoch 82, Training Loss: 0.795410600819982\n",
      "Epoch 83, Training Loss: 0.7947142244281625\n",
      "Epoch 84, Training Loss: 0.7954921804872671\n",
      "Epoch 85, Training Loss: 0.7955601432269677\n",
      "Epoch 86, Training Loss: 0.7942130281961054\n",
      "Epoch 87, Training Loss: 0.7945034957469854\n",
      "Epoch 88, Training Loss: 0.7953463129531172\n",
      "Epoch 89, Training Loss: 0.7947380524828918\n",
      "Epoch 90, Training Loss: 0.7950445901182361\n",
      "Epoch 91, Training Loss: 0.7939686906068845\n",
      "Epoch 92, Training Loss: 0.7939185877491657\n",
      "Epoch 93, Training Loss: 0.7943323911580824\n",
      "Epoch 94, Training Loss: 0.7940458325515116\n",
      "Epoch 95, Training Loss: 0.7947779590025881\n",
      "Epoch 96, Training Loss: 0.7944205795015608\n",
      "Epoch 97, Training Loss: 0.7929737793323689\n",
      "Epoch 98, Training Loss: 0.7934469525079082\n",
      "Epoch 99, Training Loss: 0.7931035886133524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:01:35,301] Trial 291 finished with value: 0.6341333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.793044560565088\n",
      "Epoch 1, Training Loss: 0.9198353824758888\n",
      "Epoch 2, Training Loss: 0.8652488235244178\n",
      "Epoch 3, Training Loss: 0.8297968754194733\n",
      "Epoch 4, Training Loss: 0.8146902754790801\n",
      "Epoch 5, Training Loss: 0.8080394968950659\n",
      "Epoch 6, Training Loss: 0.8051882697227306\n",
      "Epoch 7, Training Loss: 0.803243815181847\n",
      "Epoch 8, Training Loss: 0.8020208503966941\n",
      "Epoch 9, Training Loss: 0.8016374713496158\n",
      "Epoch 10, Training Loss: 0.8019920220052389\n",
      "Epoch 11, Training Loss: 0.8001116316569479\n",
      "Epoch 12, Training Loss: 0.7999570501478095\n",
      "Epoch 13, Training Loss: 0.8010112595737429\n",
      "Epoch 14, Training Loss: 0.7997633493036256\n",
      "Epoch 15, Training Loss: 0.7993899073815883\n",
      "Epoch 16, Training Loss: 0.7993002902296252\n",
      "Epoch 17, Training Loss: 0.7989411452659091\n",
      "Epoch 18, Training Loss: 0.7989409268350529\n",
      "Epoch 19, Training Loss: 0.7997481699276687\n",
      "Epoch 20, Training Loss: 0.798632800579071\n",
      "Epoch 21, Training Loss: 0.7986416611456333\n",
      "Epoch 22, Training Loss: 0.7980845880687685\n",
      "Epoch 23, Training Loss: 0.7977539306296442\n",
      "Epoch 24, Training Loss: 0.7984903320334011\n",
      "Epoch 25, Training Loss: 0.7974945549678085\n",
      "Epoch 26, Training Loss: 0.7974846600589895\n",
      "Epoch 27, Training Loss: 0.7970899424158541\n",
      "Epoch 28, Training Loss: 0.7971749537869504\n",
      "Epoch 29, Training Loss: 0.797683850983928\n",
      "Epoch 30, Training Loss: 0.7972730223397563\n",
      "Epoch 31, Training Loss: 0.7967668515398987\n",
      "Epoch 32, Training Loss: 0.7966795319005062\n",
      "Epoch 33, Training Loss: 0.7958135983101408\n",
      "Epoch 34, Training Loss: 0.7961092890653395\n",
      "Epoch 35, Training Loss: 0.7964307859427947\n",
      "Epoch 36, Training Loss: 0.7953412567314349\n",
      "Epoch 37, Training Loss: 0.7957261390255806\n",
      "Epoch 38, Training Loss: 0.7952050670645291\n",
      "Epoch 39, Training Loss: 0.7953005943083226\n",
      "Epoch 40, Training Loss: 0.7947084209972755\n",
      "Epoch 41, Training Loss: 0.7947331697420966\n",
      "Epoch 42, Training Loss: 0.7938468258183702\n",
      "Epoch 43, Training Loss: 0.7937587130338626\n",
      "Epoch 44, Training Loss: 0.7938931890896388\n",
      "Epoch 45, Training Loss: 0.7938759436284689\n",
      "Epoch 46, Training Loss: 0.7932629775283928\n",
      "Epoch 47, Training Loss: 0.7931434713808218\n",
      "Epoch 48, Training Loss: 0.7926771754609014\n",
      "Epoch 49, Training Loss: 0.792436134725585\n",
      "Epoch 50, Training Loss: 0.7919998065869611\n",
      "Epoch 51, Training Loss: 0.7931088282649679\n",
      "Epoch 52, Training Loss: 0.7924346332263229\n",
      "Epoch 53, Training Loss: 0.7916151898247855\n",
      "Epoch 54, Training Loss: 0.7909021562203429\n",
      "Epoch 55, Training Loss: 0.791260173894409\n",
      "Epoch 56, Training Loss: 0.7913584423244447\n",
      "Epoch 57, Training Loss: 0.7912080365912358\n",
      "Epoch 58, Training Loss: 0.7905229593578138\n",
      "Epoch 59, Training Loss: 0.7905895481432291\n",
      "Epoch 60, Training Loss: 0.7909651748219827\n",
      "Epoch 61, Training Loss: 0.7904906331148362\n",
      "Epoch 62, Training Loss: 0.7903846030844781\n",
      "Epoch 63, Training Loss: 0.7901289724765863\n",
      "Epoch 64, Training Loss: 0.7906146918024336\n",
      "Epoch 65, Training Loss: 0.7897179132117365\n",
      "Epoch 66, Training Loss: 0.7899758626643877\n",
      "Epoch 67, Training Loss: 0.7908483119835531\n",
      "Epoch 68, Training Loss: 0.79093123271053\n",
      "Epoch 69, Training Loss: 0.7899672779821811\n",
      "Epoch 70, Training Loss: 0.7896900656528043\n",
      "Epoch 71, Training Loss: 0.7903170831221387\n",
      "Epoch 72, Training Loss: 0.7894890401596414\n",
      "Epoch 73, Training Loss: 0.7894541154230448\n",
      "Epoch 74, Training Loss: 0.7895998518269761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:02:44,533] Trial 292 finished with value: 0.6354 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7890887666465645\n",
      "Epoch 1, Training Loss: 1.0445629873696496\n",
      "Epoch 2, Training Loss: 1.0042910300984103\n",
      "Epoch 3, Training Loss: 0.9837165096928092\n",
      "Epoch 4, Training Loss: 0.9722868770711562\n",
      "Epoch 5, Training Loss: 0.9655674117452958\n",
      "Epoch 6, Training Loss: 0.9613591347021215\n",
      "Epoch 7, Training Loss: 0.9585248669456033\n",
      "Epoch 8, Training Loss: 0.9564529887367698\n",
      "Epoch 9, Training Loss: 0.954849615798277\n",
      "Epoch 10, Training Loss: 0.9534705575774698\n",
      "Epoch 11, Training Loss: 0.9522738817860098\n",
      "Epoch 12, Training Loss: 0.9511436141238493\n",
      "Epoch 13, Training Loss: 0.9500962133267347\n",
      "Epoch 14, Training Loss: 0.9490640261593987\n",
      "Epoch 15, Training Loss: 0.9480847794168136\n",
      "Epoch 16, Training Loss: 0.9470964276089388\n",
      "Epoch 17, Training Loss: 0.9461199867024142\n",
      "Epoch 18, Training Loss: 0.9451630263468799\n",
      "Epoch 19, Training Loss: 0.944197658440646\n",
      "Epoch 20, Training Loss: 0.9432422272598042\n",
      "Epoch 21, Training Loss: 0.9423020686121548\n",
      "Epoch 22, Training Loss: 0.9413522606737473\n",
      "Epoch 23, Training Loss: 0.9404019237967098\n",
      "Epoch 24, Training Loss: 0.939466299870435\n",
      "Epoch 25, Training Loss: 0.9385095568965464\n",
      "Epoch 26, Training Loss: 0.9375759260093465\n",
      "Epoch 27, Training Loss: 0.936627402305603\n",
      "Epoch 28, Training Loss: 0.9356668531193453\n",
      "Epoch 29, Training Loss: 0.9347474384307861\n",
      "Epoch 30, Training Loss: 0.9337922908979304\n",
      "Epoch 31, Training Loss: 0.932844937619041\n",
      "Epoch 32, Training Loss: 0.9318803534087012\n",
      "Epoch 33, Training Loss: 0.9309452498660368\n",
      "Epoch 34, Training Loss: 0.9299941680010627\n",
      "Epoch 35, Training Loss: 0.9290415939863991\n",
      "Epoch 36, Training Loss: 0.9280859912143035\n",
      "Epoch 37, Training Loss: 0.9271161145322463\n",
      "Epoch 38, Training Loss: 0.9261650609268862\n",
      "Epoch 39, Training Loss: 0.9252084376531489\n",
      "Epoch 40, Training Loss: 0.9242463792071623\n",
      "Epoch 41, Training Loss: 0.9232802365106695\n",
      "Epoch 42, Training Loss: 0.9223140810517704\n",
      "Epoch 43, Training Loss: 0.9213510305741254\n",
      "Epoch 44, Training Loss: 0.9203816748366637\n",
      "Epoch 45, Training Loss: 0.9194045096986434\n",
      "Epoch 46, Training Loss: 0.9184208778774037\n",
      "Epoch 47, Training Loss: 0.9174472782191109\n",
      "Epoch 48, Training Loss: 0.9164729024382199\n",
      "Epoch 49, Training Loss: 0.9154799134591046\n",
      "Epoch 50, Training Loss: 0.9145179378986359\n",
      "Epoch 51, Training Loss: 0.9135215492108288\n",
      "Epoch 52, Training Loss: 0.9125456946036395\n",
      "Epoch 53, Training Loss: 0.9115452635288238\n",
      "Epoch 54, Training Loss: 0.9105688464641571\n",
      "Epoch 55, Training Loss: 0.9095739631091847\n",
      "Epoch 56, Training Loss: 0.908580054956324\n",
      "Epoch 57, Training Loss: 0.9075922324376948\n",
      "Epoch 58, Training Loss: 0.9066011074711294\n",
      "Epoch 59, Training Loss: 0.9056040652359233\n",
      "Epoch 60, Training Loss: 0.9046036862625795\n",
      "Epoch 61, Training Loss: 0.9036168570378248\n",
      "Epoch 62, Training Loss: 0.9026152549771702\n",
      "Epoch 63, Training Loss: 0.9016249329202315\n",
      "Epoch 64, Training Loss: 0.9006261918124031\n",
      "Epoch 65, Training Loss: 0.8996316177704755\n",
      "Epoch 66, Training Loss: 0.8986342233770034\n",
      "Epoch 67, Training Loss: 0.8976202957770404\n",
      "Epoch 68, Training Loss: 0.8966480506167692\n",
      "Epoch 69, Training Loss: 0.8956535894730512\n",
      "Epoch 70, Training Loss: 0.8946572216819314\n",
      "Epoch 71, Training Loss: 0.8936702466712279\n",
      "Epoch 72, Training Loss: 0.8926781657162834\n",
      "Epoch 73, Training Loss: 0.8916950398332932\n",
      "Epoch 74, Training Loss: 0.8906923163638395\n",
      "Epoch 75, Training Loss: 0.889729397577398\n",
      "Epoch 76, Training Loss: 0.888725533765905\n",
      "Epoch 77, Training Loss: 0.8877623776828542\n",
      "Epoch 78, Training Loss: 0.8867855201749241\n",
      "Epoch 79, Training Loss: 0.8858123890091392\n",
      "Epoch 80, Training Loss: 0.884839582934099\n",
      "Epoch 81, Training Loss: 0.8838632776456721\n",
      "Epoch 82, Training Loss: 0.8829007350697237\n",
      "Epoch 83, Training Loss: 0.8819475693562452\n",
      "Epoch 84, Training Loss: 0.8809836398853975\n",
      "Epoch 85, Training Loss: 0.880038213449366\n",
      "Epoch 86, Training Loss: 0.8790825010748471\n",
      "Epoch 87, Training Loss: 0.8781532442569733\n",
      "Epoch 88, Training Loss: 0.8771888185248655\n",
      "Epoch 89, Training Loss: 0.876265535214368\n",
      "Epoch 90, Training Loss: 0.8753464766810922\n",
      "Epoch 91, Training Loss: 0.8744121660204495\n",
      "Epoch 92, Training Loss: 0.8734982292792376\n",
      "Epoch 93, Training Loss: 0.8725811158909517\n",
      "Epoch 94, Training Loss: 0.8716772514932296\n",
      "Epoch 95, Training Loss: 0.8707577837915982\n",
      "Epoch 96, Training Loss: 0.8698768639564514\n",
      "Epoch 97, Training Loss: 0.8689769856368794\n",
      "Epoch 98, Training Loss: 0.8681004114711985\n",
      "Epoch 99, Training Loss: 0.8672242949289434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:04:16,316] Trial 293 finished with value: 0.5986666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8663528751625734\n",
      "Epoch 1, Training Loss: 0.9910545710255118\n",
      "Epoch 2, Training Loss: 0.9486518849344815\n",
      "Epoch 3, Training Loss: 0.9305904114246368\n",
      "Epoch 4, Training Loss: 0.9073710466833675\n",
      "Epoch 5, Training Loss: 0.8811145712347591\n",
      "Epoch 6, Training Loss: 0.8572268163456637\n",
      "Epoch 7, Training Loss: 0.839421178523232\n",
      "Epoch 8, Training Loss: 0.8275932329542497\n",
      "Epoch 9, Training Loss: 0.8205123552154092\n",
      "Epoch 10, Training Loss: 0.8160282770325156\n",
      "Epoch 11, Training Loss: 0.8132231534228606\n",
      "Epoch 12, Training Loss: 0.811666648458032\n",
      "Epoch 13, Training Loss: 0.8104763143904069\n",
      "Epoch 14, Training Loss: 0.8095608513495501\n",
      "Epoch 15, Training Loss: 0.80894371193998\n",
      "Epoch 16, Training Loss: 0.8086421707097222\n",
      "Epoch 17, Training Loss: 0.8083688738766839\n",
      "Epoch 18, Training Loss: 0.8079250137244954\n",
      "Epoch 19, Training Loss: 0.8078928331767812\n",
      "Epoch 20, Training Loss: 0.8071624147190767\n",
      "Epoch 21, Training Loss: 0.8068412127915551\n",
      "Epoch 22, Training Loss: 0.8068068730831146\n",
      "Epoch 23, Training Loss: 0.8063860419918509\n",
      "Epoch 24, Training Loss: 0.8064096823159386\n",
      "Epoch 25, Training Loss: 0.8061123175480787\n",
      "Epoch 26, Training Loss: 0.8059275978453019\n",
      "Epoch 27, Training Loss: 0.8056853180773118\n",
      "Epoch 28, Training Loss: 0.805571869541617\n",
      "Epoch 29, Training Loss: 0.8055148400278652\n",
      "Epoch 30, Training Loss: 0.8053144730539883\n",
      "Epoch 31, Training Loss: 0.8053518659227035\n",
      "Epoch 32, Training Loss: 0.8050588375680587\n",
      "Epoch 33, Training Loss: 0.8045569802733029\n",
      "Epoch 34, Training Loss: 0.8045435193706961\n",
      "Epoch 35, Training Loss: 0.8044947847899269\n",
      "Epoch 36, Training Loss: 0.8041900517660029\n",
      "Epoch 37, Training Loss: 0.8041302255321952\n",
      "Epoch 38, Training Loss: 0.8038463539936963\n",
      "Epoch 39, Training Loss: 0.8037642894772923\n",
      "Epoch 40, Training Loss: 0.8037707999874564\n",
      "Epoch 41, Training Loss: 0.8033747107842389\n",
      "Epoch 42, Training Loss: 0.803468867189744\n",
      "Epoch 43, Training Loss: 0.803114882146611\n",
      "Epoch 44, Training Loss: 0.802944067436106\n",
      "Epoch 45, Training Loss: 0.8028852728535147\n",
      "Epoch 46, Training Loss: 0.8028906106948852\n",
      "Epoch 47, Training Loss: 0.802561616266475\n",
      "Epoch 48, Training Loss: 0.8024865759120268\n",
      "Epoch 49, Training Loss: 0.802432764488108\n",
      "Epoch 50, Training Loss: 0.8021822445532855\n",
      "Epoch 51, Training Loss: 0.8021573502175948\n",
      "Epoch 52, Training Loss: 0.8020524132952971\n",
      "Epoch 53, Training Loss: 0.8018579204643473\n",
      "Epoch 54, Training Loss: 0.8019154305317823\n",
      "Epoch 55, Training Loss: 0.8015947282314301\n",
      "Epoch 56, Training Loss: 0.8016875830818625\n",
      "Epoch 57, Training Loss: 0.8015281445839826\n",
      "Epoch 58, Training Loss: 0.8011348593936247\n",
      "Epoch 59, Training Loss: 0.8011760042695438\n",
      "Epoch 60, Training Loss: 0.8009777313821456\n",
      "Epoch 61, Training Loss: 0.8008807859701269\n",
      "Epoch 62, Training Loss: 0.801048869525685\n",
      "Epoch 63, Training Loss: 0.8007552615334006\n",
      "Epoch 64, Training Loss: 0.8009614487956552\n",
      "Epoch 65, Training Loss: 0.8005177761526668\n",
      "Epoch 66, Training Loss: 0.8007742325698628\n",
      "Epoch 67, Training Loss: 0.8002601099715513\n",
      "Epoch 68, Training Loss: 0.8001811041551478\n",
      "Epoch 69, Training Loss: 0.8002170658111573\n",
      "Epoch 70, Training Loss: 0.8001374854059781\n",
      "Epoch 71, Training Loss: 0.7999862959104426\n",
      "Epoch 72, Training Loss: 0.7999688353959252\n",
      "Epoch 73, Training Loss: 0.7998522778819589\n",
      "Epoch 74, Training Loss: 0.8000740186607136\n",
      "Epoch 75, Training Loss: 0.7995362170303569\n",
      "Epoch 76, Training Loss: 0.7993594436785754\n",
      "Epoch 77, Training Loss: 0.7993953484647415\n",
      "Epoch 78, Training Loss: 0.7997049570083619\n",
      "Epoch 79, Training Loss: 0.799484247600331\n",
      "Epoch 80, Training Loss: 0.7993989409418667\n",
      "Epoch 81, Training Loss: 0.7991683540624731\n",
      "Epoch 82, Training Loss: 0.7990834189162535\n",
      "Epoch 83, Training Loss: 0.799185948862749\n",
      "Epoch 84, Training Loss: 0.7993705897471484\n",
      "Epoch 85, Training Loss: 0.7988152776044958\n",
      "Epoch 86, Training Loss: 0.7991122439328362\n",
      "Epoch 87, Training Loss: 0.7987910776979783\n",
      "Epoch 88, Training Loss: 0.7988564606975107\n",
      "Epoch 89, Training Loss: 0.7987866222157198\n",
      "Epoch 90, Training Loss: 0.7986276934427373\n",
      "Epoch 91, Training Loss: 0.7987897185017081\n",
      "Epoch 92, Training Loss: 0.7987671988851884\n",
      "Epoch 93, Training Loss: 0.7986752404184903\n",
      "Epoch 94, Training Loss: 0.7985668162037345\n",
      "Epoch 95, Training Loss: 0.7984002284442677\n",
      "Epoch 96, Training Loss: 0.7984795125091777\n",
      "Epoch 97, Training Loss: 0.7984679515922771\n",
      "Epoch 98, Training Loss: 0.7985415014799904\n",
      "Epoch 99, Training Loss: 0.7983604516001309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:06:13,900] Trial 294 finished with value: 0.6351333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7982006528798271\n",
      "Epoch 1, Training Loss: 0.8960142899276619\n",
      "Epoch 2, Training Loss: 0.8279754141219576\n",
      "Epoch 3, Training Loss: 0.821161116962146\n",
      "Epoch 4, Training Loss: 0.8169609725027156\n",
      "Epoch 5, Training Loss: 0.8138856784741682\n",
      "Epoch 6, Training Loss: 0.8130088141089992\n",
      "Epoch 7, Training Loss: 0.811778562051013\n",
      "Epoch 8, Training Loss: 0.8097230049004233\n",
      "Epoch 9, Training Loss: 0.808942789332311\n",
      "Epoch 10, Training Loss: 0.8080681025533748\n",
      "Epoch 11, Training Loss: 0.8090401526680566\n",
      "Epoch 12, Training Loss: 0.8072916902993855\n",
      "Epoch 13, Training Loss: 0.8065892428383792\n",
      "Epoch 14, Training Loss: 0.8075671355527146\n",
      "Epoch 15, Training Loss: 0.8058682706123008\n",
      "Epoch 16, Training Loss: 0.80512731944708\n",
      "Epoch 17, Training Loss: 0.8055584102644956\n",
      "Epoch 18, Training Loss: 0.804719050486285\n",
      "Epoch 19, Training Loss: 0.8042689448012446\n",
      "Epoch 20, Training Loss: 0.8045148262403962\n",
      "Epoch 21, Training Loss: 0.8040494371177559\n",
      "Epoch 22, Training Loss: 0.8039804461307095\n",
      "Epoch 23, Training Loss: 0.8028464406056511\n",
      "Epoch 24, Training Loss: 0.8025480900491987\n",
      "Epoch 25, Training Loss: 0.801667472832185\n",
      "Epoch 26, Training Loss: 0.8016291597732028\n",
      "Epoch 27, Training Loss: 0.8013355295460923\n",
      "Epoch 28, Training Loss: 0.8008676179369589\n",
      "Epoch 29, Training Loss: 0.801113448196784\n",
      "Epoch 30, Training Loss: 0.8008381836396411\n",
      "Epoch 31, Training Loss: 0.7999537776287337\n",
      "Epoch 32, Training Loss: 0.8005934639980918\n",
      "Epoch 33, Training Loss: 0.7993620359807982\n",
      "Epoch 34, Training Loss: 0.7997383727166886\n",
      "Epoch 35, Training Loss: 0.8006805384965767\n",
      "Epoch 36, Training Loss: 0.7988897784311969\n",
      "Epoch 37, Training Loss: 0.7990719430428699\n",
      "Epoch 38, Training Loss: 0.7987784318457869\n",
      "Epoch 39, Training Loss: 0.799499253402079\n",
      "Epoch 40, Training Loss: 0.7982177461896623\n",
      "Epoch 41, Training Loss: 0.7986154052547942\n",
      "Epoch 42, Training Loss: 0.7976060077660065\n",
      "Epoch 43, Training Loss: 0.7983137164797102\n",
      "Epoch 44, Training Loss: 0.7979716884462457\n",
      "Epoch 45, Training Loss: 0.7972983055544975\n",
      "Epoch 46, Training Loss: 0.7970196173603373\n",
      "Epoch 47, Training Loss: 0.7975115906923337\n",
      "Epoch 48, Training Loss: 0.7973418306587334\n",
      "Epoch 49, Training Loss: 0.7968869918271115\n",
      "Epoch 50, Training Loss: 0.7965676869664874\n",
      "Epoch 51, Training Loss: 0.7964596331567693\n",
      "Epoch 52, Training Loss: 0.7961943233819833\n",
      "Epoch 53, Training Loss: 0.7953974633288563\n",
      "Epoch 54, Training Loss: 0.7957043229189135\n",
      "Epoch 55, Training Loss: 0.7961914529477744\n",
      "Epoch 56, Training Loss: 0.7956443241664342\n",
      "Epoch 57, Training Loss: 0.7956177205071413\n",
      "Epoch 58, Training Loss: 0.7951295009233001\n",
      "Epoch 59, Training Loss: 0.7954246884898135\n",
      "Epoch 60, Training Loss: 0.7951323541483485\n",
      "Epoch 61, Training Loss: 0.7952124406520585\n",
      "Epoch 62, Training Loss: 0.7945345764769647\n",
      "Epoch 63, Training Loss: 0.7948018857410976\n",
      "Epoch 64, Training Loss: 0.7945145719033435\n",
      "Epoch 65, Training Loss: 0.7946158764057589\n",
      "Epoch 66, Training Loss: 0.7947594267085083\n",
      "Epoch 67, Training Loss: 0.7942791570398144\n",
      "Epoch 68, Training Loss: 0.7944362777516358\n",
      "Epoch 69, Training Loss: 0.7942134577528874\n",
      "Epoch 70, Training Loss: 0.7941056872669019\n",
      "Epoch 71, Training Loss: 0.794173160531467\n",
      "Epoch 72, Training Loss: 0.7937256591660636\n",
      "Epoch 73, Training Loss: 0.794661539658568\n",
      "Epoch 74, Training Loss: 0.7944331967741027\n",
      "Epoch 75, Training Loss: 0.793445092574098\n",
      "Epoch 76, Training Loss: 0.7931665816701444\n",
      "Epoch 77, Training Loss: 0.7926655022721542\n",
      "Epoch 78, Training Loss: 0.7931785371070518\n",
      "Epoch 79, Training Loss: 0.7930132165887303\n",
      "Epoch 80, Training Loss: 0.7930337792052362\n",
      "Epoch 81, Training Loss: 0.7929108734417679\n",
      "Epoch 82, Training Loss: 0.794118014403752\n",
      "Epoch 83, Training Loss: 0.7925422122603969\n",
      "Epoch 84, Training Loss: 0.7927183133318908\n",
      "Epoch 85, Training Loss: 0.7933500176981876\n",
      "Epoch 86, Training Loss: 0.7925643357119165\n",
      "Epoch 87, Training Loss: 0.7935279528001198\n",
      "Epoch 88, Training Loss: 0.792461225323211\n",
      "Epoch 89, Training Loss: 0.7920370736516508\n",
      "Epoch 90, Training Loss: 0.7922029270265335\n",
      "Epoch 91, Training Loss: 0.7926299227807755\n",
      "Epoch 92, Training Loss: 0.7923916255621085\n",
      "Epoch 93, Training Loss: 0.7914840838066617\n",
      "Epoch 94, Training Loss: 0.7923691118570199\n",
      "Epoch 95, Training Loss: 0.7924864777945038\n",
      "Epoch 96, Training Loss: 0.7916830893745996\n",
      "Epoch 97, Training Loss: 0.7910318099466481\n",
      "Epoch 98, Training Loss: 0.7914218924995652\n",
      "Epoch 99, Training Loss: 0.7916673810858476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:07:48,111] Trial 295 finished with value: 0.6269333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7915257213707257\n",
      "Epoch 1, Training Loss: 0.8648249953312981\n",
      "Epoch 2, Training Loss: 0.8174132482449811\n",
      "Epoch 3, Training Loss: 0.8138086236509166\n",
      "Epoch 4, Training Loss: 0.8102570807127129\n",
      "Epoch 5, Training Loss: 0.8089587484087263\n",
      "Epoch 6, Training Loss: 0.8067430527586686\n",
      "Epoch 7, Training Loss: 0.8056697062979964\n",
      "Epoch 8, Training Loss: 0.8042274119262408\n",
      "Epoch 9, Training Loss: 0.8036723633457844\n",
      "Epoch 10, Training Loss: 0.8023236412751047\n",
      "Epoch 11, Training Loss: 0.8011316328120411\n",
      "Epoch 12, Training Loss: 0.8013987282165012\n",
      "Epoch 13, Training Loss: 0.8018824330846169\n",
      "Epoch 14, Training Loss: 0.7999146166600679\n",
      "Epoch 15, Training Loss: 0.7996254067671926\n",
      "Epoch 16, Training Loss: 0.7989556602965621\n",
      "Epoch 17, Training Loss: 0.798641816745127\n",
      "Epoch 18, Training Loss: 0.7980681368282863\n",
      "Epoch 19, Training Loss: 0.7970804591824238\n",
      "Epoch 20, Training Loss: 0.797949186751717\n",
      "Epoch 21, Training Loss: 0.7958857945033482\n",
      "Epoch 22, Training Loss: 0.7955669122531002\n",
      "Epoch 23, Training Loss: 0.7948832329950835\n",
      "Epoch 24, Training Loss: 0.7946193367018736\n",
      "Epoch 25, Training Loss: 0.7933973197650193\n",
      "Epoch 26, Training Loss: 0.7921338666650586\n",
      "Epoch 27, Training Loss: 0.7917284658080653\n",
      "Epoch 28, Training Loss: 0.7906571434852773\n",
      "Epoch 29, Training Loss: 0.7898910953586263\n",
      "Epoch 30, Training Loss: 0.7893594738236047\n",
      "Epoch 31, Training Loss: 0.7889056806277511\n",
      "Epoch 32, Training Loss: 0.7885812466305898\n",
      "Epoch 33, Training Loss: 0.788428755362231\n",
      "Epoch 34, Training Loss: 0.7871201191629682\n",
      "Epoch 35, Training Loss: 0.7869360957826886\n",
      "Epoch 36, Training Loss: 0.7866472977444642\n",
      "Epoch 37, Training Loss: 0.7859792696802239\n",
      "Epoch 38, Training Loss: 0.7858361088243643\n",
      "Epoch 39, Training Loss: 0.7860199228265232\n",
      "Epoch 40, Training Loss: 0.7856735879317263\n",
      "Epoch 41, Training Loss: 0.785660447662038\n",
      "Epoch 42, Training Loss: 0.7858376074554329\n",
      "Epoch 43, Training Loss: 0.7855886272021703\n",
      "Epoch 44, Training Loss: 0.7860074808723048\n",
      "Epoch 45, Training Loss: 0.7852938506836281\n",
      "Epoch 46, Training Loss: 0.7847617800074412\n",
      "Epoch 47, Training Loss: 0.7843255225877116\n",
      "Epoch 48, Training Loss: 0.7844883730537013\n",
      "Epoch 49, Training Loss: 0.7844933715081752\n",
      "Epoch 50, Training Loss: 0.7841932048474936\n",
      "Epoch 51, Training Loss: 0.7845817974635533\n",
      "Epoch 52, Training Loss: 0.7836948684283666\n",
      "Epoch 53, Training Loss: 0.7840414268629892\n",
      "Epoch 54, Training Loss: 0.7837293413348664\n",
      "Epoch 55, Training Loss: 0.783587514278584\n",
      "Epoch 56, Training Loss: 0.7838066386997251\n",
      "Epoch 57, Training Loss: 0.783508224684493\n",
      "Epoch 58, Training Loss: 0.7835933480047642\n",
      "Epoch 59, Training Loss: 0.7840268505247016\n",
      "Epoch 60, Training Loss: 0.7838255080961644\n",
      "Epoch 61, Training Loss: 0.782916876337582\n",
      "Epoch 62, Training Loss: 0.7833334898590145\n",
      "Epoch 63, Training Loss: 0.7827919188298678\n",
      "Epoch 64, Training Loss: 0.7829107735390054\n",
      "Epoch 65, Training Loss: 0.7827750948138703\n",
      "Epoch 66, Training Loss: 0.7824192529334162\n",
      "Epoch 67, Training Loss: 0.7831222877466589\n",
      "Epoch 68, Training Loss: 0.7830796658544612\n",
      "Epoch 69, Training Loss: 0.7825455354568653\n",
      "Epoch 70, Training Loss: 0.7828812332081615\n",
      "Epoch 71, Training Loss: 0.7827104326477624\n",
      "Epoch 72, Training Loss: 0.7822220007279762\n",
      "Epoch 73, Training Loss: 0.7823034662053101\n",
      "Epoch 74, Training Loss: 0.7820807633543373\n",
      "Epoch 75, Training Loss: 0.7823341124039843\n",
      "Epoch 76, Training Loss: 0.7821713443089249\n",
      "Epoch 77, Training Loss: 0.7822107090089554\n",
      "Epoch 78, Training Loss: 0.7826186702663737\n",
      "Epoch 79, Training Loss: 0.781603062959542\n",
      "Epoch 80, Training Loss: 0.7819583447355973\n",
      "Epoch 81, Training Loss: 0.7814392841848216\n",
      "Epoch 82, Training Loss: 0.7819208059095799\n",
      "Epoch 83, Training Loss: 0.782116464923199\n",
      "Epoch 84, Training Loss: 0.7824724071904232\n",
      "Epoch 85, Training Loss: 0.7819975605584625\n",
      "Epoch 86, Training Loss: 0.7811487413886794\n",
      "Epoch 87, Training Loss: 0.7818022324626607\n",
      "Epoch 88, Training Loss: 0.7814921952728042\n",
      "Epoch 89, Training Loss: 0.7814338526331392\n",
      "Epoch 90, Training Loss: 0.7814439035896071\n",
      "Epoch 91, Training Loss: 0.7812685370445251\n",
      "Epoch 92, Training Loss: 0.7811861809034993\n",
      "Epoch 93, Training Loss: 0.7810865228337452\n",
      "Epoch 94, Training Loss: 0.7812849872094348\n",
      "Epoch 95, Training Loss: 0.7813192204425209\n",
      "Epoch 96, Training Loss: 0.7804748103134614\n",
      "Epoch 97, Training Loss: 0.7807762068017085\n",
      "Epoch 98, Training Loss: 0.7805269900121187\n",
      "Epoch 99, Training Loss: 0.7806210305457725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:09:40,260] Trial 296 finished with value: 0.6332 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7805822652085384\n",
      "Epoch 1, Training Loss: 0.8929316393768086\n",
      "Epoch 2, Training Loss: 0.8189126537126653\n",
      "Epoch 3, Training Loss: 0.8134799436260672\n",
      "Epoch 4, Training Loss: 0.8110273346480201\n",
      "Epoch 5, Training Loss: 0.8107177876023686\n",
      "Epoch 6, Training Loss: 0.8085818056499257\n",
      "Epoch 7, Training Loss: 0.8065908004255856\n",
      "Epoch 8, Training Loss: 0.8063383510533502\n",
      "Epoch 9, Training Loss: 0.8062145248581382\n",
      "Epoch 10, Training Loss: 0.805939438343048\n",
      "Epoch 11, Training Loss: 0.8052935802936554\n",
      "Epoch 12, Training Loss: 0.8043418302255518\n",
      "Epoch 13, Training Loss: 0.8038208467820112\n",
      "Epoch 14, Training Loss: 0.8037351666478549\n",
      "Epoch 15, Training Loss: 0.8043330421167262\n",
      "Epoch 16, Training Loss: 0.8035686380021713\n",
      "Epoch 17, Training Loss: 0.8023611089061288\n",
      "Epoch 18, Training Loss: 0.80345535271308\n",
      "Epoch 19, Training Loss: 0.8034246656473946\n",
      "Epoch 20, Training Loss: 0.8024306347790886\n",
      "Epoch 21, Training Loss: 0.8020221203214982\n",
      "Epoch 22, Training Loss: 0.8021443329839145\n",
      "Epoch 23, Training Loss: 0.8020625763780931\n",
      "Epoch 24, Training Loss: 0.8024497676596922\n",
      "Epoch 25, Training Loss: 0.8016294137870564\n",
      "Epoch 26, Training Loss: 0.8012785747471978\n",
      "Epoch 27, Training Loss: 0.8013214025076698\n",
      "Epoch 28, Training Loss: 0.8008690647517933\n",
      "Epoch 29, Training Loss: 0.801296210639617\n",
      "Epoch 30, Training Loss: 0.799835502540364\n",
      "Epoch 31, Training Loss: 0.8005978127788095\n",
      "Epoch 32, Training Loss: 0.8012620771632475\n",
      "Epoch 33, Training Loss: 0.8000333623325123\n",
      "Epoch 34, Training Loss: 0.8005524215978734\n",
      "Epoch 35, Training Loss: 0.799927681684494\n",
      "Epoch 36, Training Loss: 0.8000942843100604\n",
      "Epoch 37, Training Loss: 0.7994000200664296\n",
      "Epoch 38, Training Loss: 0.7997868762998019\n",
      "Epoch 39, Training Loss: 0.7987747519857743\n",
      "Epoch 40, Training Loss: 0.7993755032034481\n",
      "Epoch 41, Training Loss: 0.7989674453174367\n",
      "Epoch 42, Training Loss: 0.7996884625799515\n",
      "Epoch 43, Training Loss: 0.7997960280670839\n",
      "Epoch 44, Training Loss: 0.7995043840127832\n",
      "Epoch 45, Training Loss: 0.7984627006334417\n",
      "Epoch 46, Training Loss: 0.7986063775595497\n",
      "Epoch 47, Training Loss: 0.7987017864339492\n",
      "Epoch 48, Training Loss: 0.79821821177707\n",
      "Epoch 49, Training Loss: 0.7983249980561874\n",
      "Epoch 50, Training Loss: 0.7981077464889078\n",
      "Epoch 51, Training Loss: 0.7969541523035835\n",
      "Epoch 52, Training Loss: 0.7971459409068612\n",
      "Epoch 53, Training Loss: 0.7964954090118408\n",
      "Epoch 54, Training Loss: 0.797751741549548\n",
      "Epoch 55, Training Loss: 0.797009598086862\n",
      "Epoch 56, Training Loss: 0.7966113679549274\n",
      "Epoch 57, Training Loss: 0.795962105218102\n",
      "Epoch 58, Training Loss: 0.7962682699455934\n",
      "Epoch 59, Training Loss: 0.7963923645019532\n",
      "Epoch 60, Training Loss: 0.7962458311810213\n",
      "Epoch 61, Training Loss: 0.7965540638390709\n",
      "Epoch 62, Training Loss: 0.7958451197427862\n",
      "Epoch 63, Training Loss: 0.7956261151678422\n",
      "Epoch 64, Training Loss: 0.7958199639180127\n",
      "Epoch 65, Training Loss: 0.7949860180125518\n",
      "Epoch 66, Training Loss: 0.7957257769388311\n",
      "Epoch 67, Training Loss: 0.7953126657710355\n",
      "Epoch 68, Training Loss: 0.7952042545991785\n",
      "Epoch 69, Training Loss: 0.7946860307104447\n",
      "Epoch 70, Training Loss: 0.7950188120673685\n",
      "Epoch 71, Training Loss: 0.7948465655130499\n",
      "Epoch 72, Training Loss: 0.7950717717058519\n",
      "Epoch 73, Training Loss: 0.7950138873913709\n",
      "Epoch 74, Training Loss: 0.7944488325539757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:11:08,766] Trial 297 finished with value: 0.6362666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7941957481468425\n",
      "Epoch 1, Training Loss: 1.0452114253115834\n",
      "Epoch 2, Training Loss: 0.9908941196319753\n",
      "Epoch 3, Training Loss: 0.9691299571130508\n",
      "Epoch 4, Training Loss: 0.9608605482524499\n",
      "Epoch 5, Training Loss: 0.9571134429228934\n",
      "Epoch 6, Training Loss: 0.9550476032092159\n",
      "Epoch 7, Training Loss: 0.9521600935692177\n",
      "Epoch 8, Training Loss: 0.9507574654163274\n",
      "Epoch 9, Training Loss: 0.9482831105253751\n",
      "Epoch 10, Training Loss: 0.9462934416039546\n",
      "Epoch 11, Training Loss: 0.9442157931793901\n",
      "Epoch 12, Training Loss: 0.9418133457800499\n",
      "Epoch 13, Training Loss: 0.9393512190732741\n",
      "Epoch 14, Training Loss: 0.9373228226389204\n",
      "Epoch 15, Training Loss: 0.9350516926973386\n",
      "Epoch 16, Training Loss: 0.9325455563408988\n",
      "Epoch 17, Training Loss: 0.9290981225501326\n",
      "Epoch 18, Training Loss: 0.9266524242279225\n",
      "Epoch 19, Training Loss: 0.9234680661581512\n",
      "Epoch 20, Training Loss: 0.9202762247028208\n",
      "Epoch 21, Training Loss: 0.9180768360768942\n",
      "Epoch 22, Training Loss: 0.9141827273189573\n",
      "Epoch 23, Training Loss: 0.9113054694089674\n",
      "Epoch 24, Training Loss: 0.9073075893230008\n",
      "Epoch 25, Training Loss: 0.9040054741658663\n",
      "Epoch 26, Training Loss: 0.900329577653928\n",
      "Epoch 27, Training Loss: 0.8964086862435018\n",
      "Epoch 28, Training Loss: 0.8925538680607215\n",
      "Epoch 29, Training Loss: 0.8892515287363439\n",
      "Epoch 30, Training Loss: 0.8853349009850868\n",
      "Epoch 31, Training Loss: 0.8813518456050328\n",
      "Epoch 32, Training Loss: 0.8782022555071608\n",
      "Epoch 33, Training Loss: 0.8745058270325338\n",
      "Epoch 34, Training Loss: 0.8712212040908355\n",
      "Epoch 35, Training Loss: 0.8670754317054175\n",
      "Epoch 36, Training Loss: 0.8637293502800447\n",
      "Epoch 37, Training Loss: 0.8603194791571538\n",
      "Epoch 38, Training Loss: 0.8572381659557945\n",
      "Epoch 39, Training Loss: 0.8540671101190094\n",
      "Epoch 40, Training Loss: 0.8513287432211682\n",
      "Epoch 41, Training Loss: 0.8481318168174056\n",
      "Epoch 42, Training Loss: 0.8456718896564684\n",
      "Epoch 43, Training Loss: 0.8430363604896947\n",
      "Epoch 44, Training Loss: 0.8403583237103054\n",
      "Epoch 45, Training Loss: 0.8382479612988637\n",
      "Epoch 46, Training Loss: 0.8362417097378494\n",
      "Epoch 47, Training Loss: 0.8343213560885953\n",
      "Epoch 48, Training Loss: 0.8324048961911883\n",
      "Epoch 49, Training Loss: 0.8304641538992861\n",
      "Epoch 50, Training Loss: 0.8284049999445005\n",
      "Epoch 51, Training Loss: 0.8271955305472353\n",
      "Epoch 52, Training Loss: 0.8262954068363161\n",
      "Epoch 53, Training Loss: 0.8249135335585228\n",
      "Epoch 54, Training Loss: 0.8234062872434917\n",
      "Epoch 55, Training Loss: 0.8221353495031372\n",
      "Epoch 56, Training Loss: 0.8209005103971725\n",
      "Epoch 57, Training Loss: 0.8201291944747581\n",
      "Epoch 58, Training Loss: 0.819419873208928\n",
      "Epoch 59, Training Loss: 0.8189697492391543\n",
      "Epoch 60, Training Loss: 0.8173952267582255\n",
      "Epoch 61, Training Loss: 0.816875589969463\n",
      "Epoch 62, Training Loss: 0.8166352644898838\n",
      "Epoch 63, Training Loss: 0.8153385927802638\n",
      "Epoch 64, Training Loss: 0.8149678871147614\n",
      "Epoch 65, Training Loss: 0.8144866696874001\n",
      "Epoch 66, Training Loss: 0.8146089255361628\n",
      "Epoch 67, Training Loss: 0.8135786997644525\n",
      "Epoch 68, Training Loss: 0.8131796218398818\n",
      "Epoch 69, Training Loss: 0.812968955810805\n",
      "Epoch 70, Training Loss: 0.81286653522262\n",
      "Epoch 71, Training Loss: 0.8131706883136491\n",
      "Epoch 72, Training Loss: 0.8122902709738653\n",
      "Epoch 73, Training Loss: 0.8123064647939868\n",
      "Epoch 74, Training Loss: 0.8113953764277293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:12:11,218] Trial 298 finished with value: 0.6284666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8114377234215127\n",
      "Epoch 1, Training Loss: 0.9374729666990392\n",
      "Epoch 2, Training Loss: 0.870651369375341\n",
      "Epoch 3, Training Loss: 0.8321171040394727\n",
      "Epoch 4, Training Loss: 0.8185644482865053\n",
      "Epoch 5, Training Loss: 0.8141237632667317\n",
      "Epoch 6, Training Loss: 0.8123680515850291\n",
      "Epoch 7, Training Loss: 0.8111767795506646\n",
      "Epoch 8, Training Loss: 0.8102660867045908\n",
      "Epoch 9, Training Loss: 0.8093291383631089\n",
      "Epoch 10, Training Loss: 0.8090238665131961\n",
      "Epoch 11, Training Loss: 0.808119774425731\n",
      "Epoch 12, Training Loss: 0.8076329574865453\n",
      "Epoch 13, Training Loss: 0.8072334879286149\n",
      "Epoch 14, Training Loss: 0.8067980995598961\n",
      "Epoch 15, Training Loss: 0.8062975129660438\n",
      "Epoch 16, Training Loss: 0.8058641268225277\n",
      "Epoch 17, Training Loss: 0.8055193432639627\n",
      "Epoch 18, Training Loss: 0.8053316120540395\n",
      "Epoch 19, Training Loss: 0.8045185382927166\n",
      "Epoch 20, Training Loss: 0.8043209401298972\n",
      "Epoch 21, Training Loss: 0.8039226654697867\n",
      "Epoch 22, Training Loss: 0.8037393207409803\n",
      "Epoch 23, Training Loss: 0.8035450085471658\n",
      "Epoch 24, Training Loss: 0.8031056898481705\n",
      "Epoch 25, Training Loss: 0.8030850171341616\n",
      "Epoch 26, Training Loss: 0.8026742214315078\n",
      "Epoch 27, Training Loss: 0.8025234761658837\n",
      "Epoch 28, Training Loss: 0.8023692572116852\n",
      "Epoch 29, Training Loss: 0.8020010920833139\n",
      "Epoch 30, Training Loss: 0.8017332376452053\n",
      "Epoch 31, Training Loss: 0.8017316757230197\n",
      "Epoch 32, Training Loss: 0.8015429519905763\n",
      "Epoch 33, Training Loss: 0.8010151314735413\n",
      "Epoch 34, Training Loss: 0.8009321993238786\n",
      "Epoch 35, Training Loss: 0.800926940090516\n",
      "Epoch 36, Training Loss: 0.8009019532624413\n",
      "Epoch 37, Training Loss: 0.8005106702972861\n",
      "Epoch 38, Training Loss: 0.8001654806557824\n",
      "Epoch 39, Training Loss: 0.8004091396051295\n",
      "Epoch 40, Training Loss: 0.8002253805889803\n",
      "Epoch 41, Training Loss: 0.8002349750434651\n",
      "Epoch 42, Training Loss: 0.7999792785504285\n",
      "Epoch 43, Training Loss: 0.7997486698627472\n",
      "Epoch 44, Training Loss: 0.7997217395726373\n",
      "Epoch 45, Training Loss: 0.7998307092750774\n",
      "Epoch 46, Training Loss: 0.7995106409577762\n",
      "Epoch 47, Training Loss: 0.7994033251089209\n",
      "Epoch 48, Training Loss: 0.7992152142524719\n",
      "Epoch 49, Training Loss: 0.7990121630360099\n",
      "Epoch 50, Training Loss: 0.799375361624886\n",
      "Epoch 51, Training Loss: 0.7990936852202696\n",
      "Epoch 52, Training Loss: 0.7988890556728139\n",
      "Epoch 53, Training Loss: 0.7989338153250077\n",
      "Epoch 54, Training Loss: 0.7989428893958821\n",
      "Epoch 55, Training Loss: 0.79893885549377\n",
      "Epoch 56, Training Loss: 0.7985041370812584\n",
      "Epoch 57, Training Loss: 0.7986782010863809\n",
      "Epoch 58, Training Loss: 0.7985405769768883\n",
      "Epoch 59, Training Loss: 0.7984237919835483\n",
      "Epoch 60, Training Loss: 0.7986988384583417\n",
      "Epoch 61, Training Loss: 0.7983415783152861\n",
      "Epoch 62, Training Loss: 0.7986978314904606\n",
      "Epoch 63, Training Loss: 0.7984814354952644\n",
      "Epoch 64, Training Loss: 0.798643950434292\n",
      "Epoch 65, Training Loss: 0.798261981010437\n",
      "Epoch 66, Training Loss: 0.7982140117533066\n",
      "Epoch 67, Training Loss: 0.7980560046785018\n",
      "Epoch 68, Training Loss: 0.7983954362308278\n",
      "Epoch 69, Training Loss: 0.798130040659624\n",
      "Epoch 70, Training Loss: 0.7979314912066741\n",
      "Epoch 71, Training Loss: 0.7981772879993214\n",
      "Epoch 72, Training Loss: 0.7979991831498987\n",
      "Epoch 73, Training Loss: 0.7978505468368531\n",
      "Epoch 74, Training Loss: 0.7978541503233069\n",
      "Epoch 75, Training Loss: 0.7979935285624336\n",
      "Epoch 76, Training Loss: 0.7977188899937798\n",
      "Epoch 77, Training Loss: 0.7977110278606415\n",
      "Epoch 78, Training Loss: 0.7979893263648538\n",
      "Epoch 79, Training Loss: 0.7976608697105857\n",
      "Epoch 80, Training Loss: 0.7977295307552114\n",
      "Epoch 81, Training Loss: 0.7977923044737647\n",
      "Epoch 82, Training Loss: 0.7976151755978079\n",
      "Epoch 83, Training Loss: 0.7975173406039967\n",
      "Epoch 84, Training Loss: 0.7975937914848328\n",
      "Epoch 85, Training Loss: 0.7975923278051265\n",
      "Epoch 86, Training Loss: 0.7978064206768485\n",
      "Epoch 87, Training Loss: 0.7975604525734397\n",
      "Epoch 88, Training Loss: 0.7975055522778455\n",
      "Epoch 89, Training Loss: 0.7974023265698377\n",
      "Epoch 90, Training Loss: 0.7976683623650495\n",
      "Epoch 91, Training Loss: 0.7973070279990926\n",
      "Epoch 92, Training Loss: 0.7973975535000072\n",
      "Epoch 93, Training Loss: 0.7975201973494361\n",
      "Epoch 94, Training Loss: 0.7973853415601394\n",
      "Epoch 95, Training Loss: 0.7975254858942593\n",
      "Epoch 96, Training Loss: 0.7972785799643572\n",
      "Epoch 97, Training Loss: 0.7972181236042696\n",
      "Epoch 98, Training Loss: 0.7973353074578677\n",
      "Epoch 99, Training Loss: 0.7971231900243199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:14:02,799] Trial 299 finished with value: 0.6354666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7971288444014156\n",
      "Epoch 1, Training Loss: 0.8907820316185628\n",
      "Epoch 2, Training Loss: 0.8172304945780818\n",
      "Epoch 3, Training Loss: 0.8121825547146618\n",
      "Epoch 4, Training Loss: 0.8092237004660126\n",
      "Epoch 5, Training Loss: 0.8065035301043575\n",
      "Epoch 6, Training Loss: 0.801925015628786\n",
      "Epoch 7, Training Loss: 0.8002963334097898\n",
      "Epoch 8, Training Loss: 0.7982810881801118\n",
      "Epoch 9, Training Loss: 0.7951157939165159\n",
      "Epoch 10, Training Loss: 0.7949130320011225\n",
      "Epoch 11, Training Loss: 0.7934083722587815\n",
      "Epoch 12, Training Loss: 0.7919690090910833\n",
      "Epoch 13, Training Loss: 0.7914227171948082\n",
      "Epoch 14, Training Loss: 0.7909397780447078\n",
      "Epoch 15, Training Loss: 0.7919396489186394\n",
      "Epoch 16, Training Loss: 0.7905507845986158\n",
      "Epoch 17, Training Loss: 0.7901777472711147\n",
      "Epoch 18, Training Loss: 0.7909564329269237\n",
      "Epoch 19, Training Loss: 0.788914984688723\n",
      "Epoch 20, Training Loss: 0.7901372794818161\n",
      "Epoch 21, Training Loss: 0.7877043508945551\n",
      "Epoch 22, Training Loss: 0.7890589805473959\n",
      "Epoch 23, Training Loss: 0.7889063614651672\n",
      "Epoch 24, Training Loss: 0.7888934160533704\n",
      "Epoch 25, Training Loss: 0.788053804411924\n",
      "Epoch 26, Training Loss: 0.7870880020740337\n",
      "Epoch 27, Training Loss: 0.7865902562786762\n",
      "Epoch 28, Training Loss: 0.7866683608607242\n",
      "Epoch 29, Training Loss: 0.7864302989235498\n",
      "Epoch 30, Training Loss: 0.786702012836485\n",
      "Epoch 31, Training Loss: 0.7864639768026825\n",
      "Epoch 32, Training Loss: 0.7871374808756032\n",
      "Epoch 33, Training Loss: 0.7872186954756428\n",
      "Epoch 34, Training Loss: 0.7858669404696701\n",
      "Epoch 35, Training Loss: 0.7851679669286972\n",
      "Epoch 36, Training Loss: 0.7854341784814247\n",
      "Epoch 37, Training Loss: 0.7852533023160203\n",
      "Epoch 38, Training Loss: 0.7858009112508674\n",
      "Epoch 39, Training Loss: 0.7857009092667946\n",
      "Epoch 40, Training Loss: 0.7845398400959216\n",
      "Epoch 41, Training Loss: 0.7853031640662287\n",
      "Epoch 42, Training Loss: 0.7846897317054576\n",
      "Epoch 43, Training Loss: 0.7853639990763557\n",
      "Epoch 44, Training Loss: 0.7846543979824038\n",
      "Epoch 45, Training Loss: 0.7843622818925327\n",
      "Epoch 46, Training Loss: 0.7847002482055722\n",
      "Epoch 47, Training Loss: 0.784034881511129\n",
      "Epoch 48, Training Loss: 0.7845927449993622\n",
      "Epoch 49, Training Loss: 0.7826713038566417\n",
      "Epoch 50, Training Loss: 0.7843541347890868\n",
      "Epoch 51, Training Loss: 0.7846042062106885\n",
      "Epoch 52, Training Loss: 0.784169396242701\n",
      "Epoch 53, Training Loss: 0.7837339938135075\n",
      "Epoch 54, Training Loss: 0.783072857390669\n",
      "Epoch 55, Training Loss: 0.7824670845404603\n",
      "Epoch 56, Training Loss: 0.7841525560931155\n",
      "Epoch 57, Training Loss: 0.7833229303359985\n",
      "Epoch 58, Training Loss: 0.7840109040862635\n",
      "Epoch 59, Training Loss: 0.783103461014597\n",
      "Epoch 60, Training Loss: 0.7831175633839198\n",
      "Epoch 61, Training Loss: 0.7835819959640503\n",
      "Epoch 62, Training Loss: 0.7819848465740232\n",
      "Epoch 63, Training Loss: 0.7827838643152911\n",
      "Epoch 64, Training Loss: 0.7826008514354104\n",
      "Epoch 65, Training Loss: 0.7817653759081561\n",
      "Epoch 66, Training Loss: 0.7830054707993243\n",
      "Epoch 67, Training Loss: 0.7829247340223843\n",
      "Epoch 68, Training Loss: 0.7822443291656953\n",
      "Epoch 69, Training Loss: 0.7828396098058027\n",
      "Epoch 70, Training Loss: 0.7826768587406416\n",
      "Epoch 71, Training Loss: 0.7822489618358756\n",
      "Epoch 72, Training Loss: 0.7811810603715423\n",
      "Epoch 73, Training Loss: 0.7817872014260829\n",
      "Epoch 74, Training Loss: 0.7814480596018913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:15:31,698] Trial 300 finished with value: 0.6398666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7814731334385119\n",
      "Epoch 1, Training Loss: 0.8544665785660421\n",
      "Epoch 2, Training Loss: 0.8176116290845369\n",
      "Epoch 3, Training Loss: 0.8133181768252438\n",
      "Epoch 4, Training Loss: 0.8088188917117012\n",
      "Epoch 5, Training Loss: 0.8077182344924239\n",
      "Epoch 6, Training Loss: 0.8065338508527081\n",
      "Epoch 7, Training Loss: 0.8052439685154679\n",
      "Epoch 8, Training Loss: 0.8047646809341316\n",
      "Epoch 9, Training Loss: 0.8041921246320681\n",
      "Epoch 10, Training Loss: 0.8026898022881128\n",
      "Epoch 11, Training Loss: 0.8027331234817218\n",
      "Epoch 12, Training Loss: 0.8015015917164939\n",
      "Epoch 13, Training Loss: 0.8011260488875827\n",
      "Epoch 14, Training Loss: 0.8011169106440437\n",
      "Epoch 15, Training Loss: 0.799992853627169\n",
      "Epoch 16, Training Loss: 0.8006640545407632\n",
      "Epoch 17, Training Loss: 0.8000410946688258\n",
      "Epoch 18, Training Loss: 0.7996821949356481\n",
      "Epoch 19, Training Loss: 0.7985683252936915\n",
      "Epoch 20, Training Loss: 0.797931056363242\n",
      "Epoch 21, Training Loss: 0.7978985333801212\n",
      "Epoch 22, Training Loss: 0.7983329564108884\n",
      "Epoch 23, Training Loss: 0.7965883033616202\n",
      "Epoch 24, Training Loss: 0.797995742819363\n",
      "Epoch 25, Training Loss: 0.7970499064689293\n",
      "Epoch 26, Training Loss: 0.7973519091319321\n",
      "Epoch 27, Training Loss: 0.7973135545737762\n",
      "Epoch 28, Training Loss: 0.7972576252499918\n",
      "Epoch 29, Training Loss: 0.7965301597925057\n",
      "Epoch 30, Training Loss: 0.795728069499023\n",
      "Epoch 31, Training Loss: 0.7958416290749285\n",
      "Epoch 32, Training Loss: 0.7957468752574204\n",
      "Epoch 33, Training Loss: 0.7956181659734338\n",
      "Epoch 34, Training Loss: 0.7957582319589486\n",
      "Epoch 35, Training Loss: 0.7948995654744313\n",
      "Epoch 36, Training Loss: 0.7952642634398955\n",
      "Epoch 37, Training Loss: 0.795305493810123\n",
      "Epoch 38, Training Loss: 0.7947395806922052\n",
      "Epoch 39, Training Loss: 0.7949315984446303\n",
      "Epoch 40, Training Loss: 0.795360082880895\n",
      "Epoch 41, Training Loss: 0.7952773873071025\n",
      "Epoch 42, Training Loss: 0.7956218681837383\n",
      "Epoch 43, Training Loss: 0.7954410476792128\n",
      "Epoch 44, Training Loss: 0.7941832587234956\n",
      "Epoch 45, Training Loss: 0.7942575582884308\n",
      "Epoch 46, Training Loss: 0.7944496230971544\n",
      "Epoch 47, Training Loss: 0.7941301320728503\n",
      "Epoch 48, Training Loss: 0.794498046000201\n",
      "Epoch 49, Training Loss: 0.7942548300090589\n",
      "Epoch 50, Training Loss: 0.794299943016884\n",
      "Epoch 51, Training Loss: 0.7941035701816244\n",
      "Epoch 52, Training Loss: 0.7935966439713212\n",
      "Epoch 53, Training Loss: 0.7940109386479944\n",
      "Epoch 54, Training Loss: 0.7939666385041144\n",
      "Epoch 55, Training Loss: 0.7931746536627748\n",
      "Epoch 56, Training Loss: 0.7939857095704043\n",
      "Epoch 57, Training Loss: 0.7940118815665854\n",
      "Epoch 58, Training Loss: 0.7937495146478926\n",
      "Epoch 59, Training Loss: 0.7937685175049574\n",
      "Epoch 60, Training Loss: 0.7941236914548658\n",
      "Epoch 61, Training Loss: 0.7937210030125496\n",
      "Epoch 62, Training Loss: 0.7934970105501046\n",
      "Epoch 63, Training Loss: 0.792714382114267\n",
      "Epoch 64, Training Loss: 0.7930489357252767\n",
      "Epoch 65, Training Loss: 0.7933900912901513\n",
      "Epoch 66, Training Loss: 0.7928890745442613\n",
      "Epoch 67, Training Loss: 0.7940196975729519\n",
      "Epoch 68, Training Loss: 0.7929874252555962\n",
      "Epoch 69, Training Loss: 0.7927655889575643\n",
      "Epoch 70, Training Loss: 0.7923631792677972\n",
      "Epoch 71, Training Loss: 0.7934324152487561\n",
      "Epoch 72, Training Loss: 0.7926455579754105\n",
      "Epoch 73, Training Loss: 0.7928495987913662\n",
      "Epoch 74, Training Loss: 0.7924096401472737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:16:41,526] Trial 301 finished with value: 0.6252 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7926996596773764\n",
      "Epoch 1, Training Loss: 0.8540145513706637\n",
      "Epoch 2, Training Loss: 0.8199233799948729\n",
      "Epoch 3, Training Loss: 0.8168133348450625\n",
      "Epoch 4, Training Loss: 0.813969441822597\n",
      "Epoch 5, Training Loss: 0.8108026716045867\n",
      "Epoch 6, Training Loss: 0.8104187375620792\n",
      "Epoch 7, Training Loss: 0.808672721403882\n",
      "Epoch 8, Training Loss: 0.8091722790459941\n",
      "Epoch 9, Training Loss: 0.8077939470907799\n",
      "Epoch 10, Training Loss: 0.8061470556976204\n",
      "Epoch 11, Training Loss: 0.8083070072016322\n",
      "Epoch 12, Training Loss: 0.805591254574912\n",
      "Epoch 13, Training Loss: 0.8061176246270201\n",
      "Epoch 14, Training Loss: 0.8059475207687321\n",
      "Epoch 15, Training Loss: 0.8048752182408383\n",
      "Epoch 16, Training Loss: 0.8049672407315189\n",
      "Epoch 17, Training Loss: 0.8051327951868674\n",
      "Epoch 18, Training Loss: 0.8053465369948767\n",
      "Epoch 19, Training Loss: 0.8049950011690756\n",
      "Epoch 20, Training Loss: 0.8032736892987015\n",
      "Epoch 21, Training Loss: 0.8037752306550965\n",
      "Epoch 22, Training Loss: 0.8041019639574496\n",
      "Epoch 23, Training Loss: 0.8050832531505958\n",
      "Epoch 24, Training Loss: 0.8043239591713238\n",
      "Epoch 25, Training Loss: 0.8032051662753399\n",
      "Epoch 26, Training Loss: 0.8049729817791988\n",
      "Epoch 27, Training Loss: 0.8033454676319782\n",
      "Epoch 28, Training Loss: 0.802477370885978\n",
      "Epoch 29, Training Loss: 0.8030596723233847\n",
      "Epoch 30, Training Loss: 0.8038180926688632\n",
      "Epoch 31, Training Loss: 0.8034075686806127\n",
      "Epoch 32, Training Loss: 0.8023943080937952\n",
      "Epoch 33, Training Loss: 0.8032104440201494\n",
      "Epoch 34, Training Loss: 0.8014378032289949\n",
      "Epoch 35, Training Loss: 0.8016374768171095\n",
      "Epoch 36, Training Loss: 0.8023497066999736\n",
      "Epoch 37, Training Loss: 0.8019582687463975\n",
      "Epoch 38, Training Loss: 0.8023956025453438\n",
      "Epoch 39, Training Loss: 0.8017227703467348\n",
      "Epoch 40, Training Loss: 0.8006134252799185\n",
      "Epoch 41, Training Loss: 0.8011635423602914\n",
      "Epoch 42, Training Loss: 0.8015713194259128\n",
      "Epoch 43, Training Loss: 0.8007600804916898\n",
      "Epoch 44, Training Loss: 0.801536638485758\n",
      "Epoch 45, Training Loss: 0.8002930659100526\n",
      "Epoch 46, Training Loss: 0.8015133450802108\n",
      "Epoch 47, Training Loss: 0.8003511930766859\n",
      "Epoch 48, Training Loss: 0.8011397307080433\n",
      "Epoch 49, Training Loss: 0.8002014667467964\n",
      "Epoch 50, Training Loss: 0.8001268941656987\n",
      "Epoch 51, Training Loss: 0.7993519325901691\n",
      "Epoch 52, Training Loss: 0.7995427532303602\n",
      "Epoch 53, Training Loss: 0.8001577391660303\n",
      "Epoch 54, Training Loss: 0.7996041892166424\n",
      "Epoch 55, Training Loss: 0.7989224515463177\n",
      "Epoch 56, Training Loss: 0.7991600879152915\n",
      "Epoch 57, Training Loss: 0.7990639055133762\n",
      "Epoch 58, Training Loss: 0.7997418644732999\n",
      "Epoch 59, Training Loss: 0.7987352467121038\n",
      "Epoch 60, Training Loss: 0.7983403167330233\n",
      "Epoch 61, Training Loss: 0.7988792863107266\n",
      "Epoch 62, Training Loss: 0.799527164688684\n",
      "Epoch 63, Training Loss: 0.7989955495174667\n",
      "Epoch 64, Training Loss: 0.798586292284772\n",
      "Epoch 65, Training Loss: 0.797731668339636\n",
      "Epoch 66, Training Loss: 0.7977997715311839\n",
      "Epoch 67, Training Loss: 0.7984549482065932\n",
      "Epoch 68, Training Loss: 0.7978775887561024\n",
      "Epoch 69, Training Loss: 0.7983282150182509\n",
      "Epoch 70, Training Loss: 0.797669073603207\n",
      "Epoch 71, Training Loss: 0.7970917016940009\n",
      "Epoch 72, Training Loss: 0.7974589982427153\n",
      "Epoch 73, Training Loss: 0.7968215678867541\n",
      "Epoch 74, Training Loss: 0.7985619279675018\n",
      "Epoch 75, Training Loss: 0.7965643880062534\n",
      "Epoch 76, Training Loss: 0.7971478269512492\n",
      "Epoch 77, Training Loss: 0.7968788353124059\n",
      "Epoch 78, Training Loss: 0.7977372312904301\n",
      "Epoch 79, Training Loss: 0.7960704037121364\n",
      "Epoch 80, Training Loss: 0.7960470524049343\n",
      "Epoch 81, Training Loss: 0.7965276816733797\n",
      "Epoch 82, Training Loss: 0.7962643269309424\n",
      "Epoch 83, Training Loss: 0.7951297840229551\n",
      "Epoch 84, Training Loss: 0.7955206371787795\n",
      "Epoch 85, Training Loss: 0.7949634283108818\n",
      "Epoch 86, Training Loss: 0.7961568519585115\n",
      "Epoch 87, Training Loss: 0.7967928171157836\n",
      "Epoch 88, Training Loss: 0.7961775632729208\n",
      "Epoch 89, Training Loss: 0.7968769147880095\n",
      "Epoch 90, Training Loss: 0.7959534562620005\n",
      "Epoch 91, Training Loss: 0.7954346996500976\n",
      "Epoch 92, Training Loss: 0.7972740992567593\n",
      "Epoch 93, Training Loss: 0.795032072694678\n",
      "Epoch 94, Training Loss: 0.7951748344234955\n",
      "Epoch 95, Training Loss: 0.7954519061217631\n",
      "Epoch 96, Training Loss: 0.7951205613917874\n",
      "Epoch 97, Training Loss: 0.7950868251628446\n",
      "Epoch 98, Training Loss: 0.7949523230244343\n",
      "Epoch 99, Training Loss: 0.7947594055555817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:18:20,848] Trial 302 finished with value: 0.6292 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7946203890599702\n",
      "Epoch 1, Training Loss: 0.8536344899850733\n",
      "Epoch 2, Training Loss: 0.8180179169598748\n",
      "Epoch 3, Training Loss: 0.8157913692558513\n",
      "Epoch 4, Training Loss: 0.8117606654587914\n",
      "Epoch 5, Training Loss: 0.8084353972182554\n",
      "Epoch 6, Training Loss: 0.8066018575079301\n",
      "Epoch 7, Training Loss: 0.8054918639800128\n",
      "Epoch 8, Training Loss: 0.8035045592223897\n",
      "Epoch 9, Training Loss: 0.8032659649848938\n",
      "Epoch 10, Training Loss: 0.8024984640934888\n",
      "Epoch 11, Training Loss: 0.801427334617166\n",
      "Epoch 12, Training Loss: 0.8014396456409904\n",
      "Epoch 13, Training Loss: 0.801011444680831\n",
      "Epoch 14, Training Loss: 0.8000919899519752\n",
      "Epoch 15, Training Loss: 0.8000418059966143\n",
      "Epoch 16, Training Loss: 0.7998348250809838\n",
      "Epoch 17, Training Loss: 0.7985403268477496\n",
      "Epoch 18, Training Loss: 0.7995841017190148\n",
      "Epoch 19, Training Loss: 0.7983234068926643\n",
      "Epoch 20, Training Loss: 0.7984456204666811\n",
      "Epoch 21, Training Loss: 0.7978680663950303\n",
      "Epoch 22, Training Loss: 0.7980221472066992\n",
      "Epoch 23, Training Loss: 0.7975645115796257\n",
      "Epoch 24, Training Loss: 0.7968682449705461\n",
      "Epoch 25, Training Loss: 0.7971677169379066\n",
      "Epoch 26, Training Loss: 0.7973999965892119\n",
      "Epoch 27, Training Loss: 0.7965719666901757\n",
      "Epoch 28, Training Loss: 0.7963740383176242\n",
      "Epoch 29, Training Loss: 0.7962331316050362\n",
      "Epoch 30, Training Loss: 0.7959621925213758\n",
      "Epoch 31, Training Loss: 0.7964440780527452\n",
      "Epoch 32, Training Loss: 0.7961272438834696\n",
      "Epoch 33, Training Loss: 0.7960704700385823\n",
      "Epoch 34, Training Loss: 0.7957405539119945\n",
      "Epoch 35, Training Loss: 0.7954139661087709\n",
      "Epoch 36, Training Loss: 0.7950755568111644\n",
      "Epoch 37, Training Loss: 0.7956468969232896\n",
      "Epoch 38, Training Loss: 0.7951333577492657\n",
      "Epoch 39, Training Loss: 0.7953680823830998\n",
      "Epoch 40, Training Loss: 0.7949940785940955\n",
      "Epoch 41, Training Loss: 0.7956822607096504\n",
      "Epoch 42, Training Loss: 0.7945285783795749\n",
      "Epoch 43, Training Loss: 0.7947803676829619\n",
      "Epoch 44, Training Loss: 0.7950161957039552\n",
      "Epoch 45, Training Loss: 0.7950377136819503\n",
      "Epoch 46, Training Loss: 0.7945325614424312\n",
      "Epoch 47, Training Loss: 0.7946407127380372\n",
      "Epoch 48, Training Loss: 0.7945968953300925\n",
      "Epoch 49, Training Loss: 0.7944342568341424\n",
      "Epoch 50, Training Loss: 0.7939125752449036\n",
      "Epoch 51, Training Loss: 0.7941942869915681\n",
      "Epoch 52, Training Loss: 0.7938253791191998\n",
      "Epoch 53, Training Loss: 0.7939229804628035\n",
      "Epoch 54, Training Loss: 0.7943998839574702\n",
      "Epoch 55, Training Loss: 0.7937389999277451\n",
      "Epoch 56, Training Loss: 0.7938345445604885\n",
      "Epoch 57, Training Loss: 0.7937895848470575\n",
      "Epoch 58, Training Loss: 0.7936099501918344\n",
      "Epoch 59, Training Loss: 0.7942335439429563\n",
      "Epoch 60, Training Loss: 0.7936652443689458\n",
      "Epoch 61, Training Loss: 0.7934931119750528\n",
      "Epoch 62, Training Loss: 0.7930697783301859\n",
      "Epoch 63, Training Loss: 0.792933017506319\n",
      "Epoch 64, Training Loss: 0.7938292387653799\n",
      "Epoch 65, Training Loss: 0.7930286196400137\n",
      "Epoch 66, Training Loss: 0.7926671157864963\n",
      "Epoch 67, Training Loss: 0.7926018508742837\n",
      "Epoch 68, Training Loss: 0.7929713986901676\n",
      "Epoch 69, Training Loss: 0.7930369950743282\n",
      "Epoch 70, Training Loss: 0.7926474588057574\n",
      "Epoch 71, Training Loss: 0.7930984274780049\n",
      "Epoch 72, Training Loss: 0.7929937223125907\n",
      "Epoch 73, Training Loss: 0.7926943724996903\n",
      "Epoch 74, Training Loss: 0.7929590666995329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:19:44,877] Trial 303 finished with value: 0.6348666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7928812902113971\n",
      "Epoch 1, Training Loss: 0.8902181174474604\n",
      "Epoch 2, Training Loss: 0.822564764513689\n",
      "Epoch 3, Training Loss: 0.8153580608087427\n",
      "Epoch 4, Training Loss: 0.8091585464337293\n",
      "Epoch 5, Training Loss: 0.8050689245672787\n",
      "Epoch 6, Training Loss: 0.8024351835952086\n",
      "Epoch 7, Training Loss: 0.8000286619803485\n",
      "Epoch 8, Training Loss: 0.7984179816526525\n",
      "Epoch 9, Training Loss: 0.7966074668659884\n",
      "Epoch 10, Training Loss: 0.7950770038015702\n",
      "Epoch 11, Training Loss: 0.7943576084165012\n",
      "Epoch 12, Training Loss: 0.793920638070387\n",
      "Epoch 13, Training Loss: 0.7935468862337225\n",
      "Epoch 14, Training Loss: 0.7925260834132923\n",
      "Epoch 15, Training Loss: 0.7924827152841232\n",
      "Epoch 16, Training Loss: 0.7922198263336631\n",
      "Epoch 17, Training Loss: 0.7913724677001729\n",
      "Epoch 18, Training Loss: 0.7910699587008533\n",
      "Epoch 19, Training Loss: 0.7903102173524744\n",
      "Epoch 20, Training Loss: 0.7903337752819062\n",
      "Epoch 21, Training Loss: 0.790162312002743\n",
      "Epoch 22, Training Loss: 0.7898187625408173\n",
      "Epoch 23, Training Loss: 0.789792488042046\n",
      "Epoch 24, Training Loss: 0.7894310921079972\n",
      "Epoch 25, Training Loss: 0.7894026953332565\n",
      "Epoch 26, Training Loss: 0.7890672361149508\n",
      "Epoch 27, Training Loss: 0.788687303907731\n",
      "Epoch 28, Training Loss: 0.7883620999841129\n",
      "Epoch 29, Training Loss: 0.7880881937812356\n",
      "Epoch 30, Training Loss: 0.7883384343455819\n",
      "Epoch 31, Training Loss: 0.7876273144693936\n",
      "Epoch 32, Training Loss: 0.7880213359524222\n",
      "Epoch 33, Training Loss: 0.7873144120328567\n",
      "Epoch 34, Training Loss: 0.787030213580412\n",
      "Epoch 35, Training Loss: 0.787075922278797\n",
      "Epoch 36, Training Loss: 0.7868040567285874\n",
      "Epoch 37, Training Loss: 0.786604195061852\n",
      "Epoch 38, Training Loss: 0.7867342188077815\n",
      "Epoch 39, Training Loss: 0.786299635031644\n",
      "Epoch 40, Training Loss: 0.7860097136217005\n",
      "Epoch 41, Training Loss: 0.7863025699643528\n",
      "Epoch 42, Training Loss: 0.7859154193541583\n",
      "Epoch 43, Training Loss: 0.78591802351615\n",
      "Epoch 44, Training Loss: 0.7863147402510924\n",
      "Epoch 45, Training Loss: 0.7859686916014728\n",
      "Epoch 46, Training Loss: 0.7855962060479557\n",
      "Epoch 47, Training Loss: 0.785803689956665\n",
      "Epoch 48, Training Loss: 0.7860916237971362\n",
      "Epoch 49, Training Loss: 0.785885887145996\n",
      "Epoch 50, Training Loss: 0.7855238016212688\n",
      "Epoch 51, Training Loss: 0.7847943111026988\n",
      "Epoch 52, Training Loss: 0.7853445020843954\n",
      "Epoch 53, Training Loss: 0.7850082609933965\n",
      "Epoch 54, Training Loss: 0.7852966577866498\n",
      "Epoch 55, Training Loss: 0.7847277474403381\n",
      "Epoch 56, Training Loss: 0.784796387237661\n",
      "Epoch 57, Training Loss: 0.7845899336478289\n",
      "Epoch 58, Training Loss: 0.7844560647010803\n",
      "Epoch 59, Training Loss: 0.7845529932134292\n",
      "Epoch 60, Training Loss: 0.7851574969992918\n",
      "Epoch 61, Training Loss: 0.7848805212273318\n",
      "Epoch 62, Training Loss: 0.7844235629193923\n",
      "Epoch 63, Training Loss: 0.7845995027177474\n",
      "Epoch 64, Training Loss: 0.7845279991626739\n",
      "Epoch 65, Training Loss: 0.7843071215994217\n",
      "Epoch 66, Training Loss: 0.7845052998206195\n",
      "Epoch 67, Training Loss: 0.7844518440611222\n",
      "Epoch 68, Training Loss: 0.7844399318975561\n",
      "Epoch 69, Training Loss: 0.7840486330845776\n",
      "Epoch 70, Training Loss: 0.784322750007405\n",
      "Epoch 71, Training Loss: 0.7840392767681795\n",
      "Epoch 72, Training Loss: 0.7843220834872302\n",
      "Epoch 73, Training Loss: 0.7838452072704539\n",
      "Epoch 74, Training Loss: 0.7845073990260854\n",
      "Epoch 75, Training Loss: 0.78428139833843\n",
      "Epoch 76, Training Loss: 0.784021961478626\n",
      "Epoch 77, Training Loss: 0.7840846082042245\n",
      "Epoch 78, Training Loss: 0.7836516533879673\n",
      "Epoch 79, Training Loss: 0.783958150919746\n",
      "Epoch 80, Training Loss: 0.783785294083988\n",
      "Epoch 81, Training Loss: 0.7840798785406\n",
      "Epoch 82, Training Loss: 0.7837399197325987\n",
      "Epoch 83, Training Loss: 0.7839493367952459\n",
      "Epoch 84, Training Loss: 0.7834727776751799\n",
      "Epoch 85, Training Loss: 0.7834365975155549\n",
      "Epoch 86, Training Loss: 0.7841559002679938\n",
      "Epoch 87, Training Loss: 0.7838589920015896\n",
      "Epoch 88, Training Loss: 0.783633205610163\n",
      "Epoch 89, Training Loss: 0.7839691248360802\n",
      "Epoch 90, Training Loss: 0.7838429812824025\n",
      "Epoch 91, Training Loss: 0.7836419885298785\n",
      "Epoch 92, Training Loss: 0.7836376043628244\n",
      "Epoch 93, Training Loss: 0.783740832455018\n",
      "Epoch 94, Training Loss: 0.7837158891032724\n",
      "Epoch 95, Training Loss: 0.7844782679922441\n",
      "Epoch 96, Training Loss: 0.7838641999048345\n",
      "Epoch 97, Training Loss: 0.7842034738905289\n",
      "Epoch 98, Training Loss: 0.7834336709274965\n",
      "Epoch 99, Training Loss: 0.783325603499132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:21:53,737] Trial 304 finished with value: 0.635 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7839962831665488\n",
      "Epoch 1, Training Loss: 0.8720085183480628\n",
      "Epoch 2, Training Loss: 0.827793434239868\n",
      "Epoch 3, Training Loss: 0.8227943376490944\n",
      "Epoch 4, Training Loss: 0.8187068228882954\n",
      "Epoch 5, Training Loss: 0.8152262961057792\n",
      "Epoch 6, Training Loss: 0.8141731672717216\n",
      "Epoch 7, Training Loss: 0.812945372538459\n",
      "Epoch 8, Training Loss: 0.8112142731372575\n",
      "Epoch 9, Training Loss: 0.8107910033455469\n",
      "Epoch 10, Training Loss: 0.810090504642716\n",
      "Epoch 11, Training Loss: 0.8102674437644787\n",
      "Epoch 12, Training Loss: 0.8082310852251555\n",
      "Epoch 13, Training Loss: 0.8081397091535697\n",
      "Epoch 14, Training Loss: 0.80817573231862\n",
      "Epoch 15, Training Loss: 0.8086516605283981\n",
      "Epoch 16, Training Loss: 0.8069642761596163\n",
      "Epoch 17, Training Loss: 0.8066765961790443\n",
      "Epoch 18, Training Loss: 0.806633568616738\n",
      "Epoch 19, Training Loss: 0.8064509825598924\n",
      "Epoch 20, Training Loss: 0.8061623094673444\n",
      "Epoch 21, Training Loss: 0.8056574109801673\n",
      "Epoch 22, Training Loss: 0.8055721566193086\n",
      "Epoch 23, Training Loss: 0.805648332341273\n",
      "Epoch 24, Training Loss: 0.8057821712995831\n",
      "Epoch 25, Training Loss: 0.8046262082300688\n",
      "Epoch 26, Training Loss: 0.8059158754528017\n",
      "Epoch 27, Training Loss: 0.8039469716244174\n",
      "Epoch 28, Training Loss: 0.8051013327182683\n",
      "Epoch 29, Training Loss: 0.8052971464350708\n",
      "Epoch 30, Training Loss: 0.8039619997928017\n",
      "Epoch 31, Training Loss: 0.803091359676275\n",
      "Epoch 32, Training Loss: 0.8039372721112761\n",
      "Epoch 33, Training Loss: 0.8040610427246955\n",
      "Epoch 34, Training Loss: 0.8044506815143098\n",
      "Epoch 35, Training Loss: 0.8041398936644533\n",
      "Epoch 36, Training Loss: 0.8026518075986016\n",
      "Epoch 37, Training Loss: 0.8028152606541052\n",
      "Epoch 38, Training Loss: 0.8029586973046898\n",
      "Epoch 39, Training Loss: 0.8024486800781766\n",
      "Epoch 40, Training Loss: 0.8029579759540414\n",
      "Epoch 41, Training Loss: 0.8030229784492263\n",
      "Epoch 42, Training Loss: 0.8012723907492214\n",
      "Epoch 43, Training Loss: 0.8021971433682549\n",
      "Epoch 44, Training Loss: 0.8016351153527884\n",
      "Epoch 45, Training Loss: 0.8006889051960823\n",
      "Epoch 46, Training Loss: 0.8030275536659068\n",
      "Epoch 47, Training Loss: 0.8010966277660284\n",
      "Epoch 48, Training Loss: 0.8015405883466391\n",
      "Epoch 49, Training Loss: 0.8016932339596569\n",
      "Epoch 50, Training Loss: 0.8027616159360211\n",
      "Epoch 51, Training Loss: 0.8019887087040378\n",
      "Epoch 52, Training Loss: 0.8010414496400302\n",
      "Epoch 53, Training Loss: 0.8008712067639917\n",
      "Epoch 54, Training Loss: 0.8021968963450955\n",
      "Epoch 55, Training Loss: 0.8007139700695984\n",
      "Epoch 56, Training Loss: 0.801756835790505\n",
      "Epoch 57, Training Loss: 0.8013087610553081\n",
      "Epoch 58, Training Loss: 0.801205367432501\n",
      "Epoch 59, Training Loss: 0.8012101589288927\n",
      "Epoch 60, Training Loss: 0.8010144918484795\n",
      "Epoch 61, Training Loss: 0.8014025721334873\n",
      "Epoch 62, Training Loss: 0.8019392810369793\n",
      "Epoch 63, Training Loss: 0.8005362960628997\n",
      "Epoch 64, Training Loss: 0.8017007222749237\n",
      "Epoch 65, Training Loss: 0.8008966120114004\n",
      "Epoch 66, Training Loss: 0.8007185257467112\n",
      "Epoch 67, Training Loss: 0.8009262978582454\n",
      "Epoch 68, Training Loss: 0.800791050885853\n",
      "Epoch 69, Training Loss: 0.8002401575110012\n",
      "Epoch 70, Training Loss: 0.7999250356864213\n",
      "Epoch 71, Training Loss: 0.8003500310101903\n",
      "Epoch 72, Training Loss: 0.8000669693588314\n",
      "Epoch 73, Training Loss: 0.8011300545886046\n",
      "Epoch 74, Training Loss: 0.801167496881987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:23:03,748] Trial 305 finished with value: 0.5922666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8011811216074721\n",
      "Epoch 1, Training Loss: 0.8856456438401588\n",
      "Epoch 2, Training Loss: 0.8378338833500568\n",
      "Epoch 3, Training Loss: 0.8300808897592071\n",
      "Epoch 4, Training Loss: 0.8296392827105702\n",
      "Epoch 5, Training Loss: 0.8242429344277633\n",
      "Epoch 6, Training Loss: 0.8230363052590449\n",
      "Epoch 7, Training Loss: 0.8208719139708612\n",
      "Epoch 8, Training Loss: 0.8179788041831856\n",
      "Epoch 9, Training Loss: 0.8195837832034979\n",
      "Epoch 10, Training Loss: 0.8174831429818519\n",
      "Epoch 11, Training Loss: 0.8166496382620102\n",
      "Epoch 12, Training Loss: 0.8178422021686582\n",
      "Epoch 13, Training Loss: 0.8161970395790903\n",
      "Epoch 14, Training Loss: 0.8164822359730427\n",
      "Epoch 15, Training Loss: 0.8163237044685765\n",
      "Epoch 16, Training Loss: 0.8153432145154565\n",
      "Epoch 17, Training Loss: 0.8180195852329857\n",
      "Epoch 18, Training Loss: 0.8177560967610294\n",
      "Epoch 19, Training Loss: 0.8164656501067312\n",
      "Epoch 20, Training Loss: 0.8163307953597908\n",
      "Epoch 21, Training Loss: 0.8147701666767436\n",
      "Epoch 22, Training Loss: 0.814784112281369\n",
      "Epoch 23, Training Loss: 0.8168905009004407\n",
      "Epoch 24, Training Loss: 0.8165152535402685\n",
      "Epoch 25, Training Loss: 0.8136256181207815\n",
      "Epoch 26, Training Loss: 0.8138452845408504\n",
      "Epoch 27, Training Loss: 0.8151533959503461\n",
      "Epoch 28, Training Loss: 0.813424104945104\n",
      "Epoch 29, Training Loss: 0.8136712685563511\n",
      "Epoch 30, Training Loss: 0.8135536336361018\n",
      "Epoch 31, Training Loss: 0.8116924805748732\n",
      "Epoch 32, Training Loss: 0.8132721340745912\n",
      "Epoch 33, Training Loss: 0.8135138172852365\n",
      "Epoch 34, Training Loss: 0.8133684498923165\n",
      "Epoch 35, Training Loss: 0.8134695609709374\n",
      "Epoch 36, Training Loss: 0.8133983950865896\n",
      "Epoch 37, Training Loss: 0.8120063670595786\n",
      "Epoch 38, Training Loss: 0.8125339910500031\n",
      "Epoch 39, Training Loss: 0.8148565367648476\n",
      "Epoch 40, Training Loss: 0.811190061237579\n",
      "Epoch 41, Training Loss: 0.8124729771363107\n",
      "Epoch 42, Training Loss: 0.8113754931249116\n",
      "Epoch 43, Training Loss: 0.8132952924061538\n",
      "Epoch 44, Training Loss: 0.8115238381507701\n",
      "Epoch 45, Training Loss: 0.814511675942213\n",
      "Epoch 46, Training Loss: 0.8126597961985079\n",
      "Epoch 47, Training Loss: 0.8102120221109319\n",
      "Epoch 48, Training Loss: 0.8108510175145658\n",
      "Epoch 49, Training Loss: 0.8109091265757281\n",
      "Epoch 50, Training Loss: 0.8121755594597724\n",
      "Epoch 51, Training Loss: 0.8134833235489695\n",
      "Epoch 52, Training Loss: 0.809190979756807\n",
      "Epoch 53, Training Loss: 0.8118324551367222\n",
      "Epoch 54, Training Loss: 0.8113287841467033\n",
      "Epoch 55, Training Loss: 0.811687489649407\n",
      "Epoch 56, Training Loss: 0.8112352054818233\n",
      "Epoch 57, Training Loss: 0.8097212973393892\n",
      "Epoch 58, Training Loss: 0.8091308708029582\n",
      "Epoch 59, Training Loss: 0.8091759893230925\n",
      "Epoch 60, Training Loss: 0.8107359708699965\n",
      "Epoch 61, Training Loss: 0.8107567119419127\n",
      "Epoch 62, Training Loss: 0.80920593917818\n",
      "Epoch 63, Training Loss: 0.8082410621463805\n",
      "Epoch 64, Training Loss: 0.8109444661247999\n",
      "Epoch 65, Training Loss: 0.8088341397450383\n",
      "Epoch 66, Training Loss: 0.8083903758149398\n",
      "Epoch 67, Training Loss: 0.8095863665853228\n",
      "Epoch 68, Training Loss: 0.8095094511383458\n",
      "Epoch 69, Training Loss: 0.8073475996354469\n",
      "Epoch 70, Training Loss: 0.809527938168748\n",
      "Epoch 71, Training Loss: 0.8068932908818238\n",
      "Epoch 72, Training Loss: 0.807158154742162\n",
      "Epoch 73, Training Loss: 0.8093370442103622\n",
      "Epoch 74, Training Loss: 0.8061037965286943\n",
      "Epoch 75, Training Loss: 0.8059344255834594\n",
      "Epoch 76, Training Loss: 0.8104256505356695\n",
      "Epoch 77, Training Loss: 0.8054281995708781\n",
      "Epoch 78, Training Loss: 0.808099361200978\n",
      "Epoch 79, Training Loss: 0.8065413698217923\n",
      "Epoch 80, Training Loss: 0.8077920082816504\n",
      "Epoch 81, Training Loss: 0.8058219794043922\n",
      "Epoch 82, Training Loss: 0.8062340182469303\n",
      "Epoch 83, Training Loss: 0.8071001236600087\n",
      "Epoch 84, Training Loss: 0.8061648765004668\n",
      "Epoch 85, Training Loss: 0.8063733323624259\n",
      "Epoch 86, Training Loss: 0.8048281855152962\n",
      "Epoch 87, Training Loss: 0.8051166901911112\n",
      "Epoch 88, Training Loss: 0.8054425287963752\n",
      "Epoch 89, Training Loss: 0.8057552999123595\n",
      "Epoch 90, Training Loss: 0.8080856180728826\n",
      "Epoch 91, Training Loss: 0.8051553803278988\n",
      "Epoch 92, Training Loss: 0.8080347281649597\n",
      "Epoch 93, Training Loss: 0.8033603261287947\n",
      "Epoch 94, Training Loss: 0.8049903459118721\n",
      "Epoch 95, Training Loss: 0.8037165308357181\n",
      "Epoch 96, Training Loss: 0.8039561398047254\n",
      "Epoch 97, Training Loss: 0.8054333162487002\n",
      "Epoch 98, Training Loss: 0.8074720646205701\n",
      "Epoch 99, Training Loss: 0.8029875635204459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:24:37,889] Trial 306 finished with value: 0.5658666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8050579875931704\n",
      "Epoch 1, Training Loss: 0.9371683148075552\n",
      "Epoch 2, Training Loss: 0.8769785797595978\n",
      "Epoch 3, Training Loss: 0.8356167671960942\n",
      "Epoch 4, Training Loss: 0.8151877537194421\n",
      "Epoch 5, Training Loss: 0.8075038492679596\n",
      "Epoch 6, Training Loss: 0.8042184804467594\n",
      "Epoch 7, Training Loss: 0.8029333969424752\n",
      "Epoch 8, Training Loss: 0.8016229183533612\n",
      "Epoch 9, Training Loss: 0.8008882654414458\n",
      "Epoch 10, Training Loss: 0.8005138804632075\n",
      "Epoch 11, Training Loss: 0.8004191680515513\n",
      "Epoch 12, Training Loss: 0.7997475577101988\n",
      "Epoch 13, Training Loss: 0.7993269792023827\n",
      "Epoch 14, Training Loss: 0.7992916055286632\n",
      "Epoch 15, Training Loss: 0.7989586442358354\n",
      "Epoch 16, Training Loss: 0.798769625565585\n",
      "Epoch 17, Training Loss: 0.798567018438788\n",
      "Epoch 18, Training Loss: 0.7983281371172737\n",
      "Epoch 19, Training Loss: 0.7983179795040803\n",
      "Epoch 20, Training Loss: 0.798146233698901\n",
      "Epoch 21, Training Loss: 0.7978881204128265\n",
      "Epoch 22, Training Loss: 0.797651911202599\n",
      "Epoch 23, Training Loss: 0.7980463410826291\n",
      "Epoch 24, Training Loss: 0.797582995821448\n",
      "Epoch 25, Training Loss: 0.7973211115248063\n",
      "Epoch 26, Training Loss: 0.7973029540566837\n",
      "Epoch 27, Training Loss: 0.796935860970441\n",
      "Epoch 28, Training Loss: 0.796610975265503\n",
      "Epoch 29, Training Loss: 0.7965121534291436\n",
      "Epoch 30, Training Loss: 0.7965917674232932\n",
      "Epoch 31, Training Loss: 0.7962518120513243\n",
      "Epoch 32, Training Loss: 0.7961644445447361\n",
      "Epoch 33, Training Loss: 0.7959054702169754\n",
      "Epoch 34, Training Loss: 0.7955758955198176\n",
      "Epoch 35, Training Loss: 0.7952343834147734\n",
      "Epoch 36, Training Loss: 0.7951645140788134\n",
      "Epoch 37, Training Loss: 0.7946336331788232\n",
      "Epoch 38, Training Loss: 0.7945394876424005\n",
      "Epoch 39, Training Loss: 0.7943213532952701\n",
      "Epoch 40, Training Loss: 0.7937742148427402\n",
      "Epoch 41, Training Loss: 0.7938650624892291\n",
      "Epoch 42, Training Loss: 0.7935406927501454\n",
      "Epoch 43, Training Loss: 0.7931684346760021\n",
      "Epoch 44, Training Loss: 0.7931842727520887\n",
      "Epoch 45, Training Loss: 0.792703491379233\n",
      "Epoch 46, Training Loss: 0.7927889890530531\n",
      "Epoch 47, Training Loss: 0.792544803759631\n",
      "Epoch 48, Training Loss: 0.7919726414540235\n",
      "Epoch 49, Training Loss: 0.7921928614728591\n",
      "Epoch 50, Training Loss: 0.7918332367083606\n",
      "Epoch 51, Training Loss: 0.7917979820335612\n",
      "Epoch 52, Training Loss: 0.7915448195092818\n",
      "Epoch 53, Training Loss: 0.7913397253962124\n",
      "Epoch 54, Training Loss: 0.7913886169125052\n",
      "Epoch 55, Training Loss: 0.7912322136935066\n",
      "Epoch 56, Training Loss: 0.790907083679648\n",
      "Epoch 57, Training Loss: 0.7908732834984274\n",
      "Epoch 58, Training Loss: 0.7909732374022989\n",
      "Epoch 59, Training Loss: 0.7910401361830094\n",
      "Epoch 60, Training Loss: 0.7907637990222258\n",
      "Epoch 61, Training Loss: 0.7907343504709355\n",
      "Epoch 62, Training Loss: 0.7904243574422949\n",
      "Epoch 63, Training Loss: 0.7907621480436886\n",
      "Epoch 64, Training Loss: 0.7902761091905481\n",
      "Epoch 65, Training Loss: 0.790249899555655\n",
      "Epoch 66, Training Loss: 0.7903563191610224\n",
      "Epoch 67, Training Loss: 0.7901871716976165\n",
      "Epoch 68, Training Loss: 0.7899865177799673\n",
      "Epoch 69, Training Loss: 0.7899680496664608\n",
      "Epoch 70, Training Loss: 0.7900268385690802\n",
      "Epoch 71, Training Loss: 0.7898124902388629\n",
      "Epoch 72, Training Loss: 0.7897931556140675\n",
      "Epoch 73, Training Loss: 0.7895570260636947\n",
      "Epoch 74, Training Loss: 0.7894631997977986\n",
      "Epoch 75, Training Loss: 0.7898114939998178\n",
      "Epoch 76, Training Loss: 0.7895692690681009\n",
      "Epoch 77, Training Loss: 0.7893705692711999\n",
      "Epoch 78, Training Loss: 0.7894900537939633\n",
      "Epoch 79, Training Loss: 0.7897109513423022\n",
      "Epoch 80, Training Loss: 0.7895138246171615\n",
      "Epoch 81, Training Loss: 0.7892822968258577\n",
      "Epoch 82, Training Loss: 0.7892056571034824\n",
      "Epoch 83, Training Loss: 0.7890590525374693\n",
      "Epoch 84, Training Loss: 0.7893205238791073\n",
      "Epoch 85, Training Loss: 0.7890492094264311\n",
      "Epoch 86, Training Loss: 0.7891352278344771\n",
      "Epoch 87, Training Loss: 0.7888863445029539\n",
      "Epoch 88, Training Loss: 0.7891019798026365\n",
      "Epoch 89, Training Loss: 0.7890017768214731\n",
      "Epoch 90, Training Loss: 0.7890872001647949\n",
      "Epoch 91, Training Loss: 0.7887778225365807\n",
      "Epoch 92, Training Loss: 0.7884340047135072\n",
      "Epoch 93, Training Loss: 0.7885849710071788\n",
      "Epoch 94, Training Loss: 0.7887024070234859\n",
      "Epoch 95, Training Loss: 0.7883544734646293\n",
      "Epoch 96, Training Loss: 0.7882546388401704\n",
      "Epoch 97, Training Loss: 0.7884852374301238\n",
      "Epoch 98, Training Loss: 0.7882240198640262\n",
      "Epoch 99, Training Loss: 0.7887201414388769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:26:35,148] Trial 307 finished with value: 0.6374666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7883918244698468\n",
      "Epoch 1, Training Loss: 0.9041906931346521\n",
      "Epoch 2, Training Loss: 0.8326418421322241\n",
      "Epoch 3, Training Loss: 0.82365131306469\n",
      "Epoch 4, Training Loss: 0.8184117046513952\n",
      "Epoch 5, Training Loss: 0.8156986891775203\n",
      "Epoch 6, Training Loss: 0.8115700502592819\n",
      "Epoch 7, Training Loss: 0.8106554469667879\n",
      "Epoch 8, Training Loss: 0.8099280015866559\n",
      "Epoch 9, Training Loss: 0.8090030161061681\n",
      "Epoch 10, Training Loss: 0.8078084832743595\n",
      "Epoch 11, Training Loss: 0.807403534337094\n",
      "Epoch 12, Training Loss: 0.8072835774349987\n",
      "Epoch 13, Training Loss: 0.8067730192851303\n",
      "Epoch 14, Training Loss: 0.8057638656824155\n",
      "Epoch 15, Training Loss: 0.8056984939073262\n",
      "Epoch 16, Training Loss: 0.8050680386392693\n",
      "Epoch 17, Training Loss: 0.804479519944442\n",
      "Epoch 18, Training Loss: 0.8038688200757019\n",
      "Epoch 19, Training Loss: 0.8043704159277723\n",
      "Epoch 20, Training Loss: 0.8038228313725694\n",
      "Epoch 21, Training Loss: 0.8028944680565282\n",
      "Epoch 22, Training Loss: 0.8035245603188537\n",
      "Epoch 23, Training Loss: 0.8020668936403175\n",
      "Epoch 24, Training Loss: 0.8031469061858671\n",
      "Epoch 25, Training Loss: 0.8021974761683242\n",
      "Epoch 26, Training Loss: 0.8026733657471219\n",
      "Epoch 27, Training Loss: 0.8014110009473069\n",
      "Epoch 28, Training Loss: 0.8012416575188027\n",
      "Epoch 29, Training Loss: 0.8008930359567915\n",
      "Epoch 30, Training Loss: 0.8009536040456672\n",
      "Epoch 31, Training Loss: 0.799948951982914\n",
      "Epoch 32, Training Loss: 0.8003183043092713\n",
      "Epoch 33, Training Loss: 0.7995555597140377\n",
      "Epoch 34, Training Loss: 0.7984104330826522\n",
      "Epoch 35, Training Loss: 0.7997148446570662\n",
      "Epoch 36, Training Loss: 0.7983846232407075\n",
      "Epoch 37, Training Loss: 0.7983313401838891\n",
      "Epoch 38, Training Loss: 0.7982879853786382\n",
      "Epoch 39, Training Loss: 0.7975119573281224\n",
      "Epoch 40, Training Loss: 0.7975041848376281\n",
      "Epoch 41, Training Loss: 0.7973841208264344\n",
      "Epoch 42, Training Loss: 0.7965229587447374\n",
      "Epoch 43, Training Loss: 0.7975027780783804\n",
      "Epoch 44, Training Loss: 0.7970615089387822\n",
      "Epoch 45, Training Loss: 0.795752753709492\n",
      "Epoch 46, Training Loss: 0.7960045641526243\n",
      "Epoch 47, Training Loss: 0.7957293637713095\n",
      "Epoch 48, Training Loss: 0.7953986035253768\n",
      "Epoch 49, Training Loss: 0.7959112587727999\n",
      "Epoch 50, Training Loss: 0.7957035959215093\n",
      "Epoch 51, Training Loss: 0.7958094625544727\n",
      "Epoch 52, Training Loss: 0.7960323556921536\n",
      "Epoch 53, Training Loss: 0.794686722396908\n",
      "Epoch 54, Training Loss: 0.795010905337513\n",
      "Epoch 55, Training Loss: 0.7946224735195475\n",
      "Epoch 56, Training Loss: 0.7955400047445655\n",
      "Epoch 57, Training Loss: 0.7943469017968142\n",
      "Epoch 58, Training Loss: 0.7946564638525023\n",
      "Epoch 59, Training Loss: 0.7939569402009921\n",
      "Epoch 60, Training Loss: 0.7943698926079542\n",
      "Epoch 61, Training Loss: 0.7942474777537181\n",
      "Epoch 62, Training Loss: 0.7937081889998644\n",
      "Epoch 63, Training Loss: 0.7943604666487615\n",
      "Epoch 64, Training Loss: 0.7942529281279198\n",
      "Epoch 65, Training Loss: 0.7939738014586886\n",
      "Epoch 66, Training Loss: 0.7936042801778119\n",
      "Epoch 67, Training Loss: 0.7940355873645697\n",
      "Epoch 68, Training Loss: 0.7940255808650999\n",
      "Epoch 69, Training Loss: 0.7932608831197696\n",
      "Epoch 70, Training Loss: 0.7931048659453714\n",
      "Epoch 71, Training Loss: 0.7932617822984107\n",
      "Epoch 72, Training Loss: 0.7937367605983763\n",
      "Epoch 73, Training Loss: 0.7929312126528948\n",
      "Epoch 74, Training Loss: 0.792767616411797\n",
      "Epoch 75, Training Loss: 0.7931367741491562\n",
      "Epoch 76, Training Loss: 0.7930004550998372\n",
      "Epoch 77, Training Loss: 0.7938870370836186\n",
      "Epoch 78, Training Loss: 0.7925121819166313\n",
      "Epoch 79, Training Loss: 0.792702289183337\n",
      "Epoch 80, Training Loss: 0.7929401795666917\n",
      "Epoch 81, Training Loss: 0.7926504532197365\n",
      "Epoch 82, Training Loss: 0.7927304778780256\n",
      "Epoch 83, Training Loss: 0.7920460485874262\n",
      "Epoch 84, Training Loss: 0.792298252241952\n",
      "Epoch 85, Training Loss: 0.7922339071008496\n",
      "Epoch 86, Training Loss: 0.7936433630778377\n",
      "Epoch 87, Training Loss: 0.7917617829222429\n",
      "Epoch 88, Training Loss: 0.7921072645294935\n",
      "Epoch 89, Training Loss: 0.7919605807254189\n",
      "Epoch 90, Training Loss: 0.7920783818216253\n",
      "Epoch 91, Training Loss: 0.792203724832463\n",
      "Epoch 92, Training Loss: 0.7917619201473723\n",
      "Epoch 93, Training Loss: 0.7927223745145295\n",
      "Epoch 94, Training Loss: 0.7918028988336262\n",
      "Epoch 95, Training Loss: 0.7916231690492845\n",
      "Epoch 96, Training Loss: 0.7918083260830183\n",
      "Epoch 97, Training Loss: 0.7916071183699415\n",
      "Epoch 98, Training Loss: 0.7915341309138707\n",
      "Epoch 99, Training Loss: 0.7914902696932169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:28:09,487] Trial 308 finished with value: 0.6294 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7905462546456129\n",
      "Epoch 1, Training Loss: 1.0862183108049281\n",
      "Epoch 2, Training Loss: 1.0731469196431778\n",
      "Epoch 3, Training Loss: 1.0609923633407143\n",
      "Epoch 4, Training Loss: 1.04881279482561\n",
      "Epoch 5, Training Loss: 1.036496398378821\n",
      "Epoch 6, Training Loss: 1.024039319964016\n",
      "Epoch 7, Training Loss: 1.0117279224535998\n",
      "Epoch 8, Training Loss: 1.0000422136222615\n",
      "Epoch 9, Training Loss: 0.9894610864975873\n",
      "Epoch 10, Training Loss: 0.980270660204046\n",
      "Epoch 11, Training Loss: 0.9725437613094554\n",
      "Epoch 12, Training Loss: 0.966208953927545\n",
      "Epoch 13, Training Loss: 0.9610714561798993\n",
      "Epoch 14, Training Loss: 0.9568886442044202\n",
      "Epoch 15, Training Loss: 0.9534313491512747\n",
      "Epoch 16, Training Loss: 0.9505282938480377\n",
      "Epoch 17, Training Loss: 0.9480430928398581\n",
      "Epoch 18, Training Loss: 0.9458632713205675\n",
      "Epoch 19, Training Loss: 0.9438919075797586\n",
      "Epoch 20, Training Loss: 0.9421069340846118\n",
      "Epoch 21, Training Loss: 0.9404313776773565\n",
      "Epoch 22, Training Loss: 0.938857455183478\n",
      "Epoch 23, Training Loss: 0.9373603174265693\n",
      "Epoch 24, Training Loss: 0.9359095557998208\n",
      "Epoch 25, Training Loss: 0.9345092765022727\n",
      "Epoch 26, Training Loss: 0.9331580450955559\n",
      "Epoch 27, Training Loss: 0.9318171870007235\n",
      "Epoch 28, Training Loss: 0.9305022478103637\n",
      "Epoch 29, Training Loss: 0.9292251643713783\n",
      "Epoch 30, Training Loss: 0.9279601197382983\n",
      "Epoch 31, Training Loss: 0.9267175412178039\n",
      "Epoch 32, Training Loss: 0.9254870393696953\n",
      "Epoch 33, Training Loss: 0.9242683845407823\n",
      "Epoch 34, Training Loss: 0.923076314645655\n",
      "Epoch 35, Training Loss: 0.9218869521337397\n",
      "Epoch 36, Training Loss: 0.9207126567644232\n",
      "Epoch 37, Training Loss: 0.9195383402179269\n",
      "Epoch 38, Training Loss: 0.9183867070955388\n",
      "Epoch 39, Training Loss: 0.9172357668596155\n",
      "Epoch 40, Training Loss: 0.9160893691287321\n",
      "Epoch 41, Training Loss: 0.9149345970153808\n",
      "Epoch 42, Training Loss: 0.9137965151141672\n",
      "Epoch 43, Training Loss: 0.9126429583044613\n",
      "Epoch 44, Training Loss: 0.9114942655843847\n",
      "Epoch 45, Training Loss: 0.9103292459600112\n",
      "Epoch 46, Training Loss: 0.9091792299466974\n",
      "Epoch 47, Training Loss: 0.9080163516717799\n",
      "Epoch 48, Training Loss: 0.9068412223282982\n",
      "Epoch 49, Training Loss: 0.9056562191598556\n",
      "Epoch 50, Training Loss: 0.9044608425392824\n",
      "Epoch 51, Training Loss: 0.9032496981059803\n",
      "Epoch 52, Training Loss: 0.9020261235798106\n",
      "Epoch 53, Training Loss: 0.9007762238558601\n",
      "Epoch 54, Training Loss: 0.8995060431256013\n",
      "Epoch 55, Training Loss: 0.8982299473005183\n",
      "Epoch 56, Training Loss: 0.8969231958950267\n",
      "Epoch 57, Training Loss: 0.8955926790658165\n",
      "Epoch 58, Training Loss: 0.8942383522145888\n",
      "Epoch 59, Training Loss: 0.8928728762794943\n",
      "Epoch 60, Training Loss: 0.8914831755441778\n",
      "Epoch 61, Training Loss: 0.8900647455804488\n",
      "Epoch 62, Training Loss: 0.8886290928896736\n",
      "Epoch 63, Training Loss: 0.8871622194262112\n",
      "Epoch 64, Training Loss: 0.8856412066431607\n",
      "Epoch 65, Training Loss: 0.8841453827829922\n",
      "Epoch 66, Training Loss: 0.882588434008991\n",
      "Epoch 67, Training Loss: 0.8809895840111901\n",
      "Epoch 68, Training Loss: 0.8793679698775796\n",
      "Epoch 69, Training Loss: 0.8776948539649739\n",
      "Epoch 70, Training Loss: 0.8760007037134732\n",
      "Epoch 71, Training Loss: 0.8742576536010294\n",
      "Epoch 72, Training Loss: 0.8724948978424072\n",
      "Epoch 73, Training Loss: 0.8706936038241667\n",
      "Epoch 74, Training Loss: 0.868886185393614\n",
      "Epoch 75, Training Loss: 0.8670102428688723\n",
      "Epoch 76, Training Loss: 0.8651287599170909\n",
      "Epoch 77, Training Loss: 0.8632155244490679\n",
      "Epoch 78, Training Loss: 0.8613037152851329\n",
      "Epoch 79, Training Loss: 0.8593477907601524\n",
      "Epoch 80, Training Loss: 0.85741550859283\n",
      "Epoch 81, Training Loss: 0.8554306173324585\n",
      "Epoch 82, Training Loss: 0.8534792009521933\n",
      "Epoch 83, Training Loss: 0.8515249308417825\n",
      "Epoch 84, Training Loss: 0.8495513441983391\n",
      "Epoch 85, Training Loss: 0.847637014389038\n",
      "Epoch 86, Training Loss: 0.8457348010820501\n",
      "Epoch 87, Training Loss: 0.8438482986478244\n",
      "Epoch 88, Training Loss: 0.8419549993907705\n",
      "Epoch 89, Training Loss: 0.8401560458716224\n",
      "Epoch 90, Training Loss: 0.8383367722875932\n",
      "Epoch 91, Training Loss: 0.8365714601208182\n",
      "Epoch 92, Training Loss: 0.8349068216716542\n",
      "Epoch 93, Training Loss: 0.8332114571683547\n",
      "Epoch 94, Training Loss: 0.8315921620761647\n",
      "Epoch 95, Training Loss: 0.8300185714749729\n",
      "Epoch 96, Training Loss: 0.8284867191314698\n",
      "Epoch 97, Training Loss: 0.8270349834947025\n",
      "Epoch 98, Training Loss: 0.8256228832637562\n",
      "Epoch 99, Training Loss: 0.8242763869902667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:29:59,497] Trial 309 finished with value: 0.6208666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8229737624000101\n",
      "Epoch 1, Training Loss: 0.9147748048165265\n",
      "Epoch 2, Training Loss: 0.8711674360667958\n",
      "Epoch 3, Training Loss: 0.8342247782034032\n",
      "Epoch 4, Training Loss: 0.8143744059169994\n",
      "Epoch 5, Training Loss: 0.8067635578968946\n",
      "Epoch 6, Training Loss: 0.8039601856820724\n",
      "Epoch 7, Training Loss: 0.8026625403235941\n",
      "Epoch 8, Training Loss: 0.8019867404769448\n",
      "Epoch 9, Training Loss: 0.8011038833505967\n",
      "Epoch 10, Training Loss: 0.8009338149603675\n",
      "Epoch 11, Training Loss: 0.8003537533563726\n",
      "Epoch 12, Training Loss: 0.8000776229886448\n",
      "Epoch 13, Training Loss: 0.7997403349595912\n",
      "Epoch 14, Training Loss: 0.7996006173246047\n",
      "Epoch 15, Training Loss: 0.7989498139128965\n",
      "Epoch 16, Training Loss: 0.7990595400333405\n",
      "Epoch 17, Training Loss: 0.7987558608195361\n",
      "Epoch 18, Training Loss: 0.7984253596558291\n",
      "Epoch 19, Training Loss: 0.7983679087021771\n",
      "Epoch 20, Training Loss: 0.798316010797725\n",
      "Epoch 21, Training Loss: 0.7978283851286945\n",
      "Epoch 22, Training Loss: 0.7978349065079409\n",
      "Epoch 23, Training Loss: 0.7980163925535538\n",
      "Epoch 24, Training Loss: 0.7975218159310958\n",
      "Epoch 25, Training Loss: 0.7975333143683041\n",
      "Epoch 26, Training Loss: 0.7973955777112175\n",
      "Epoch 27, Training Loss: 0.7972583141046412\n",
      "Epoch 28, Training Loss: 0.7971632951848647\n",
      "Epoch 29, Training Loss: 0.7970873255589429\n",
      "Epoch 30, Training Loss: 0.7968889262395746\n",
      "Epoch 31, Training Loss: 0.7967248544272254\n",
      "Epoch 32, Training Loss: 0.7967240084620083\n",
      "Epoch 33, Training Loss: 0.7965007529539221\n",
      "Epoch 34, Training Loss: 0.7965902385992162\n",
      "Epoch 35, Training Loss: 0.7964311156553381\n",
      "Epoch 36, Training Loss: 0.7964494499038247\n",
      "Epoch 37, Training Loss: 0.796566472824882\n",
      "Epoch 38, Training Loss: 0.7961482092913459\n",
      "Epoch 39, Training Loss: 0.7959915677238913\n",
      "Epoch 40, Training Loss: 0.7958649445982541\n",
      "Epoch 41, Training Loss: 0.7958124013508068\n",
      "Epoch 42, Training Loss: 0.7955827174467199\n",
      "Epoch 43, Training Loss: 0.7952805754717659\n",
      "Epoch 44, Training Loss: 0.7947188798820272\n",
      "Epoch 45, Training Loss: 0.7946200913541457\n",
      "Epoch 46, Training Loss: 0.7939263575918534\n",
      "Epoch 47, Training Loss: 0.7935959736038657\n",
      "Epoch 48, Training Loss: 0.7931608415351195\n",
      "Epoch 49, Training Loss: 0.7928099046033972\n",
      "Epoch 50, Training Loss: 0.7926897637984331\n",
      "Epoch 51, Training Loss: 0.7921828179499683\n",
      "Epoch 52, Training Loss: 0.7917725268532249\n",
      "Epoch 53, Training Loss: 0.7918522765355952\n",
      "Epoch 54, Training Loss: 0.7914968578955707\n",
      "Epoch 55, Training Loss: 0.7912394555877237\n",
      "Epoch 56, Training Loss: 0.7911667220732745\n",
      "Epoch 57, Training Loss: 0.7910973166718203\n",
      "Epoch 58, Training Loss: 0.79079546009793\n",
      "Epoch 59, Training Loss: 0.7905535766657661\n",
      "Epoch 60, Training Loss: 0.7903230933582082\n",
      "Epoch 61, Training Loss: 0.7904151531528024\n",
      "Epoch 62, Training Loss: 0.7899353629701278\n",
      "Epoch 63, Training Loss: 0.7898089940407697\n",
      "Epoch 64, Training Loss: 0.7901217440296622\n",
      "Epoch 65, Training Loss: 0.7897967054563411\n",
      "Epoch 66, Training Loss: 0.789928781004513\n",
      "Epoch 67, Training Loss: 0.78963430853451\n",
      "Epoch 68, Training Loss: 0.7895788352629718\n",
      "Epoch 69, Training Loss: 0.7894061429360334\n",
      "Epoch 70, Training Loss: 0.7892830091364244\n",
      "Epoch 71, Training Loss: 0.7888144160719479\n",
      "Epoch 72, Training Loss: 0.7890956847106709\n",
      "Epoch 73, Training Loss: 0.7891454064144807\n",
      "Epoch 74, Training Loss: 0.7890619334753822\n",
      "Epoch 75, Training Loss: 0.7889181471572203\n",
      "Epoch 76, Training Loss: 0.7887100710588343\n",
      "Epoch 77, Training Loss: 0.7887384367690367\n",
      "Epoch 78, Training Loss: 0.7885090291500092\n",
      "Epoch 79, Training Loss: 0.7883924142753377\n",
      "Epoch 80, Training Loss: 0.788542247169158\n",
      "Epoch 81, Training Loss: 0.7886761764217826\n",
      "Epoch 82, Training Loss: 0.7884328072211322\n",
      "Epoch 83, Training Loss: 0.7884170505579781\n",
      "Epoch 84, Training Loss: 0.7882802328642677\n",
      "Epoch 85, Training Loss: 0.7881277078039506\n",
      "Epoch 86, Training Loss: 0.7879477420975181\n",
      "Epoch 87, Training Loss: 0.7879402159242069\n",
      "Epoch 88, Training Loss: 0.7878732457581689\n",
      "Epoch 89, Training Loss: 0.7876686207687154\n",
      "Epoch 90, Training Loss: 0.7877100173164816\n",
      "Epoch 91, Training Loss: 0.787730234931497\n",
      "Epoch 92, Training Loss: 0.7877232240929323\n",
      "Epoch 93, Training Loss: 0.7874635423632229\n",
      "Epoch 94, Training Loss: 0.7873051599895253\n",
      "Epoch 95, Training Loss: 0.7876236308322233\n",
      "Epoch 96, Training Loss: 0.7874932313666624\n",
      "Epoch 97, Training Loss: 0.787419341732474\n",
      "Epoch 98, Training Loss: 0.7872014235047733\n",
      "Epoch 99, Training Loss: 0.7872783753451179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:31:52,413] Trial 310 finished with value: 0.6398666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7871401630429661\n",
      "Epoch 1, Training Loss: 0.9059369985322306\n",
      "Epoch 2, Training Loss: 0.8183030986248102\n",
      "Epoch 3, Training Loss: 0.81219756262643\n",
      "Epoch 4, Training Loss: 0.8090300008318478\n",
      "Epoch 5, Training Loss: 0.8071625921959268\n",
      "Epoch 6, Training Loss: 0.8053918558852117\n",
      "Epoch 7, Training Loss: 0.803034536193188\n",
      "Epoch 8, Training Loss: 0.801638160343457\n",
      "Epoch 9, Training Loss: 0.801033594016742\n",
      "Epoch 10, Training Loss: 0.8008813140087558\n",
      "Epoch 11, Training Loss: 0.8000476267104758\n",
      "Epoch 12, Training Loss: 0.7998127064310518\n",
      "Epoch 13, Training Loss: 0.8001268465716139\n",
      "Epoch 14, Training Loss: 0.7997752995419323\n",
      "Epoch 15, Training Loss: 0.7994792576123001\n",
      "Epoch 16, Training Loss: 0.7987215492062103\n",
      "Epoch 17, Training Loss: 0.7983866167247744\n",
      "Epoch 18, Training Loss: 0.798134061895815\n",
      "Epoch 19, Training Loss: 0.7971254840829318\n",
      "Epoch 20, Training Loss: 0.7968640127576383\n",
      "Epoch 21, Training Loss: 0.7972890289206254\n",
      "Epoch 22, Training Loss: 0.7972956089148844\n",
      "Epoch 23, Training Loss: 0.7968332312160865\n",
      "Epoch 24, Training Loss: 0.7963939355728321\n",
      "Epoch 25, Training Loss: 0.7958083226268453\n",
      "Epoch 26, Training Loss: 0.7951518277476605\n",
      "Epoch 27, Training Loss: 0.7947762656032591\n",
      "Epoch 28, Training Loss: 0.7952233341403474\n",
      "Epoch 29, Training Loss: 0.794169647084143\n",
      "Epoch 30, Training Loss: 0.7941603730495711\n",
      "Epoch 31, Training Loss: 0.7937464732872812\n",
      "Epoch 32, Training Loss: 0.7924988347784917\n",
      "Epoch 33, Training Loss: 0.7922239827034169\n",
      "Epoch 34, Training Loss: 0.7911802572415287\n",
      "Epoch 35, Training Loss: 0.791846265739068\n",
      "Epoch 36, Training Loss: 0.7899244365835548\n",
      "Epoch 37, Training Loss: 0.7905302035181145\n",
      "Epoch 38, Training Loss: 0.7892265930211634\n",
      "Epoch 39, Training Loss: 0.7893030503638705\n",
      "Epoch 40, Training Loss: 0.7880840273728048\n",
      "Epoch 41, Training Loss: 0.7878509033891491\n",
      "Epoch 42, Training Loss: 0.78758370168227\n",
      "Epoch 43, Training Loss: 0.78744076042247\n",
      "Epoch 44, Training Loss: 0.7874076522382578\n",
      "Epoch 45, Training Loss: 0.7868410443004809\n",
      "Epoch 46, Training Loss: 0.7865939938036123\n",
      "Epoch 47, Training Loss: 0.7858940084177749\n",
      "Epoch 48, Training Loss: 0.7854292917968635\n",
      "Epoch 49, Training Loss: 0.7860352758178137\n",
      "Epoch 50, Training Loss: 0.7855488896369934\n",
      "Epoch 51, Training Loss: 0.7852338686921543\n",
      "Epoch 52, Training Loss: 0.784621144416637\n",
      "Epoch 53, Training Loss: 0.7845037616285166\n",
      "Epoch 54, Training Loss: 0.7846636134879034\n",
      "Epoch 55, Training Loss: 0.7839350126291577\n",
      "Epoch 56, Training Loss: 0.7844535560536205\n",
      "Epoch 57, Training Loss: 0.7843645160359548\n",
      "Epoch 58, Training Loss: 0.7842048853859865\n",
      "Epoch 59, Training Loss: 0.7834499788463564\n",
      "Epoch 60, Training Loss: 0.784206241682956\n",
      "Epoch 61, Training Loss: 0.7839928458507796\n",
      "Epoch 62, Training Loss: 0.7839016309358123\n",
      "Epoch 63, Training Loss: 0.7836233473361883\n",
      "Epoch 64, Training Loss: 0.7837444999164208\n",
      "Epoch 65, Training Loss: 0.7832842860903059\n",
      "Epoch 66, Training Loss: 0.782769779334391\n",
      "Epoch 67, Training Loss: 0.7833637856899347\n",
      "Epoch 68, Training Loss: 0.7837717130668181\n",
      "Epoch 69, Training Loss: 0.7834207471151997\n",
      "Epoch 70, Training Loss: 0.7828632202363552\n",
      "Epoch 71, Training Loss: 0.7832390395322241\n",
      "Epoch 72, Training Loss: 0.7831295509087411\n",
      "Epoch 73, Training Loss: 0.7825826168956613\n",
      "Epoch 74, Training Loss: 0.782663713541246\n",
      "Epoch 75, Training Loss: 0.7831368051973501\n",
      "Epoch 76, Training Loss: 0.7826225931483104\n",
      "Epoch 77, Training Loss: 0.7825792141426775\n",
      "Epoch 78, Training Loss: 0.782509916707089\n",
      "Epoch 79, Training Loss: 0.7821121575240803\n",
      "Epoch 80, Training Loss: 0.7824407007461204\n",
      "Epoch 81, Training Loss: 0.7824300311561814\n",
      "Epoch 82, Training Loss: 0.7825605248150073\n",
      "Epoch 83, Training Loss: 0.7823561871858468\n",
      "Epoch 84, Training Loss: 0.7827068626432491\n",
      "Epoch 85, Training Loss: 0.7813534441747163\n",
      "Epoch 86, Training Loss: 0.7822535335569454\n",
      "Epoch 87, Training Loss: 0.7821510451180594\n",
      "Epoch 88, Training Loss: 0.7822227328343498\n",
      "Epoch 89, Training Loss: 0.7827451798252594\n",
      "Epoch 90, Training Loss: 0.7814849953006084\n",
      "Epoch 91, Training Loss: 0.7824911759311991\n",
      "Epoch 92, Training Loss: 0.781646950710985\n",
      "Epoch 93, Training Loss: 0.780905705219821\n",
      "Epoch 94, Training Loss: 0.782101893693881\n",
      "Epoch 95, Training Loss: 0.7818933256586692\n",
      "Epoch 96, Training Loss: 0.7816832039589272\n",
      "Epoch 97, Training Loss: 0.7820368235272572\n",
      "Epoch 98, Training Loss: 0.7826541577963004\n",
      "Epoch 99, Training Loss: 0.7819705426244807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:33:50,089] Trial 311 finished with value: 0.6413333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7815460830702817\n",
      "Epoch 1, Training Loss: 0.8615621285578784\n",
      "Epoch 2, Training Loss: 0.8255320882797241\n",
      "Epoch 3, Training Loss: 0.8214033225003411\n",
      "Epoch 4, Training Loss: 0.8180768174984876\n",
      "Epoch 5, Training Loss: 0.8157969832420349\n",
      "Epoch 6, Training Loss: 0.8128798313701854\n",
      "Epoch 7, Training Loss: 0.8124744970658246\n",
      "Epoch 8, Training Loss: 0.8109095116923837\n",
      "Epoch 9, Training Loss: 0.8103423781254713\n",
      "Epoch 10, Training Loss: 0.8091464942343095\n",
      "Epoch 11, Training Loss: 0.8101664782972897\n",
      "Epoch 12, Training Loss: 0.8080873976034276\n",
      "Epoch 13, Training Loss: 0.8076381945610046\n",
      "Epoch 14, Training Loss: 0.8080168106976677\n",
      "Epoch 15, Training Loss: 0.8075130531367134\n",
      "Epoch 16, Training Loss: 0.8071971311288721\n",
      "Epoch 17, Training Loss: 0.8066621746035183\n",
      "Epoch 18, Training Loss: 0.80652051245465\n",
      "Epoch 19, Training Loss: 0.8057315086617189\n",
      "Epoch 20, Training Loss: 0.8061005278895883\n",
      "Epoch 21, Training Loss: 0.8060981024012847\n",
      "Epoch 22, Training Loss: 0.8056558066957137\n",
      "Epoch 23, Training Loss: 0.8051533313358531\n",
      "Epoch 24, Training Loss: 0.8048102777144488\n",
      "Epoch 25, Training Loss: 0.8046017128579757\n",
      "Epoch 26, Training Loss: 0.8038983025971581\n",
      "Epoch 27, Training Loss: 0.8036660096925847\n",
      "Epoch 28, Training Loss: 0.8040601127989152\n",
      "Epoch 29, Training Loss: 0.8033448212287005\n",
      "Epoch 30, Training Loss: 0.8038989649800693\n",
      "Epoch 31, Training Loss: 0.8035269859959098\n",
      "Epoch 32, Training Loss: 0.803637858839596\n",
      "Epoch 33, Training Loss: 0.8034667903535506\n",
      "Epoch 34, Training Loss: 0.802943790660185\n",
      "Epoch 35, Training Loss: 0.8024162442543927\n",
      "Epoch 36, Training Loss: 0.8026326452283298\n",
      "Epoch 37, Training Loss: 0.8029920888648313\n",
      "Epoch 38, Training Loss: 0.8028673815026003\n",
      "Epoch 39, Training Loss: 0.8026450912391438\n",
      "Epoch 40, Training Loss: 0.8030114723654355\n",
      "Epoch 41, Training Loss: 0.8024910864409278\n",
      "Epoch 42, Training Loss: 0.8030230932375964\n",
      "Epoch 43, Training Loss: 0.8025141452340518\n",
      "Epoch 44, Training Loss: 0.8019419459034415\n",
      "Epoch 45, Training Loss: 0.802108261865728\n",
      "Epoch 46, Training Loss: 0.8021572354260613\n",
      "Epoch 47, Training Loss: 0.8021857934138354\n",
      "Epoch 48, Training Loss: 0.8026377473157995\n",
      "Epoch 49, Training Loss: 0.8025082214439616\n",
      "Epoch 50, Training Loss: 0.8023246217475218\n",
      "Epoch 51, Training Loss: 0.8023252993471482\n",
      "Epoch 52, Training Loss: 0.80191260842716\n",
      "Epoch 53, Training Loss: 0.8020990847138798\n",
      "Epoch 54, Training Loss: 0.8017001867294311\n",
      "Epoch 55, Training Loss: 0.8017358981861787\n",
      "Epoch 56, Training Loss: 0.8015768276242649\n",
      "Epoch 57, Training Loss: 0.8022288868707769\n",
      "Epoch 58, Training Loss: 0.8021071232066435\n",
      "Epoch 59, Training Loss: 0.80119812670876\n",
      "Epoch 60, Training Loss: 0.8018739565681009\n",
      "Epoch 61, Training Loss: 0.801854363189024\n",
      "Epoch 62, Training Loss: 0.801198714130065\n",
      "Epoch 63, Training Loss: 0.8014580618633943\n",
      "Epoch 64, Training Loss: 0.8012838565602022\n",
      "Epoch 65, Training Loss: 0.801369752813788\n",
      "Epoch 66, Training Loss: 0.8015649158113143\n",
      "Epoch 67, Training Loss: 0.8015295174542595\n",
      "Epoch 68, Training Loss: 0.8010220580942491\n",
      "Epoch 69, Training Loss: 0.8011745835051817\n",
      "Epoch 70, Training Loss: 0.8019728704761057\n",
      "Epoch 71, Training Loss: 0.8014580934889176\n",
      "Epoch 72, Training Loss: 0.8015992848312153\n",
      "Epoch 73, Training Loss: 0.8015707400967093\n",
      "Epoch 74, Training Loss: 0.8015444402133717\n",
      "Epoch 75, Training Loss: 0.8017178797721862\n",
      "Epoch 76, Training Loss: 0.80188677184722\n",
      "Epoch 77, Training Loss: 0.8013152935925651\n",
      "Epoch 78, Training Loss: 0.8008712344309863\n",
      "Epoch 79, Training Loss: 0.8016054288078757\n",
      "Epoch 80, Training Loss: 0.8008265071756699\n",
      "Epoch 81, Training Loss: 0.8014215829091914\n",
      "Epoch 82, Training Loss: 0.8010236569713144\n",
      "Epoch 83, Training Loss: 0.8014543844671811\n",
      "Epoch 84, Training Loss: 0.8011540138721466\n",
      "Epoch 85, Training Loss: 0.801171084432041\n",
      "Epoch 86, Training Loss: 0.8009110519465278\n",
      "Epoch 87, Training Loss: 0.8009165885869195\n",
      "Epoch 88, Training Loss: 0.8014489160565769\n",
      "Epoch 89, Training Loss: 0.8008321507538066\n",
      "Epoch 90, Training Loss: 0.8018920747672811\n",
      "Epoch 91, Training Loss: 0.8009111609178431\n",
      "Epoch 92, Training Loss: 0.8010607166851268\n",
      "Epoch 93, Training Loss: 0.8014299741913291\n",
      "Epoch 94, Training Loss: 0.8013217512299032\n",
      "Epoch 95, Training Loss: 0.8008623670830446\n",
      "Epoch 96, Training Loss: 0.8008425789720872\n",
      "Epoch 97, Training Loss: 0.8009595187972574\n",
      "Epoch 98, Training Loss: 0.8010177631237928\n",
      "Epoch 99, Training Loss: 0.8011313632656546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:35:42,079] Trial 312 finished with value: 0.6314666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8008240206802593\n",
      "Epoch 1, Training Loss: 0.8755199815946467\n",
      "Epoch 2, Training Loss: 0.8275614673249861\n",
      "Epoch 3, Training Loss: 0.8194434526387383\n",
      "Epoch 4, Training Loss: 0.8155335403891171\n",
      "Epoch 5, Training Loss: 0.8129343990017386\n",
      "Epoch 6, Training Loss: 0.8103656706389258\n",
      "Epoch 7, Training Loss: 0.8099977217001073\n",
      "Epoch 8, Training Loss: 0.8093812835917753\n",
      "Epoch 9, Training Loss: 0.8072851873846615\n",
      "Epoch 10, Training Loss: 0.8062395445739522\n",
      "Epoch 11, Training Loss: 0.8050606563512017\n",
      "Epoch 12, Training Loss: 0.8046981944056119\n",
      "Epoch 13, Training Loss: 0.8048950931605171\n",
      "Epoch 14, Training Loss: 0.8049844809139476\n",
      "Epoch 15, Training Loss: 0.8034364539735458\n",
      "Epoch 16, Training Loss: 0.8036221213901744\n",
      "Epoch 17, Training Loss: 0.8039435545135947\n",
      "Epoch 18, Training Loss: 0.8024856195730321\n",
      "Epoch 19, Training Loss: 0.8021940719380098\n",
      "Epoch 20, Training Loss: 0.80187514648718\n",
      "Epoch 21, Training Loss: 0.8015867561452529\n",
      "Epoch 22, Training Loss: 0.8010227925637189\n",
      "Epoch 23, Training Loss: 0.8009122033680186\n",
      "Epoch 24, Training Loss: 0.8008650784632739\n",
      "Epoch 25, Training Loss: 0.8009027417267071\n",
      "Epoch 26, Training Loss: 0.8013331450434292\n",
      "Epoch 27, Training Loss: 0.799860630386016\n",
      "Epoch 28, Training Loss: 0.8000017437514136\n",
      "Epoch 29, Training Loss: 0.8004231464862823\n",
      "Epoch 30, Training Loss: 0.7999862331502577\n",
      "Epoch 31, Training Loss: 0.7989878568228553\n",
      "Epoch 32, Training Loss: 0.7999146914482117\n",
      "Epoch 33, Training Loss: 0.800427975584479\n",
      "Epoch 34, Training Loss: 0.799169763396768\n",
      "Epoch 35, Training Loss: 0.7987641192183775\n",
      "Epoch 36, Training Loss: 0.7990384603247923\n",
      "Epoch 37, Training Loss: 0.7987425133060007\n",
      "Epoch 38, Training Loss: 0.7986036980853362\n",
      "Epoch 39, Training Loss: 0.7987452578544617\n",
      "Epoch 40, Training Loss: 0.7984783803715425\n",
      "Epoch 41, Training Loss: 0.7984959696320927\n",
      "Epoch 42, Training Loss: 0.7985067242734573\n",
      "Epoch 43, Training Loss: 0.7989046572236453\n",
      "Epoch 44, Training Loss: 0.7987613903774935\n",
      "Epoch 45, Training Loss: 0.7982206583724303\n",
      "Epoch 46, Training Loss: 0.7969785143347348\n",
      "Epoch 47, Training Loss: 0.797911479473114\n",
      "Epoch 48, Training Loss: 0.7988610578284544\n",
      "Epoch 49, Training Loss: 0.7975620544657988\n",
      "Epoch 50, Training Loss: 0.7980482079702265\n",
      "Epoch 51, Training Loss: 0.7984462313091054\n",
      "Epoch 52, Training Loss: 0.7981774585387286\n",
      "Epoch 53, Training Loss: 0.7983498175003949\n",
      "Epoch 54, Training Loss: 0.7977510097447563\n",
      "Epoch 55, Training Loss: 0.7975424586324131\n",
      "Epoch 56, Training Loss: 0.7978689994531519\n",
      "Epoch 57, Training Loss: 0.7978703527590808\n",
      "Epoch 58, Training Loss: 0.797401511669159\n",
      "Epoch 59, Training Loss: 0.7975272532070384\n",
      "Epoch 60, Training Loss: 0.7978780843229855\n",
      "Epoch 61, Training Loss: 0.7966672709408928\n",
      "Epoch 62, Training Loss: 0.7965648190414204\n",
      "Epoch 63, Training Loss: 0.797759495342479\n",
      "Epoch 64, Training Loss: 0.7970647494933184\n",
      "Epoch 65, Training Loss: 0.7965438736887539\n",
      "Epoch 66, Training Loss: 0.7969332768636591\n",
      "Epoch 67, Training Loss: 0.7975056306754842\n",
      "Epoch 68, Training Loss: 0.7970343788932351\n",
      "Epoch 69, Training Loss: 0.7971008664720198\n",
      "Epoch 70, Training Loss: 0.7966544837811413\n",
      "Epoch 71, Training Loss: 0.7967197000279146\n",
      "Epoch 72, Training Loss: 0.7967390183140249\n",
      "Epoch 73, Training Loss: 0.7968317825653973\n",
      "Epoch 74, Training Loss: 0.7967310233677135\n",
      "Epoch 75, Training Loss: 0.7963657122499802\n",
      "Epoch 76, Training Loss: 0.796655685340657\n",
      "Epoch 77, Training Loss: 0.7966179755154777\n",
      "Epoch 78, Training Loss: 0.7963853372545804\n",
      "Epoch 79, Training Loss: 0.7965519732587478\n",
      "Epoch 80, Training Loss: 0.7966477633925045\n",
      "Epoch 81, Training Loss: 0.7967858664428487\n",
      "Epoch 82, Training Loss: 0.7969180620417875\n",
      "Epoch 83, Training Loss: 0.7961246207882376\n",
      "Epoch 84, Training Loss: 0.7971207513528712\n",
      "Epoch 85, Training Loss: 0.7959911236342262\n",
      "Epoch 86, Training Loss: 0.796560581852408\n",
      "Epoch 87, Training Loss: 0.7962446507986854\n",
      "Epoch 88, Training Loss: 0.7964314260202295\n",
      "Epoch 89, Training Loss: 0.7961747105682597\n",
      "Epoch 90, Training Loss: 0.7957937141726998\n",
      "Epoch 91, Training Loss: 0.797200166758369\n",
      "Epoch 92, Training Loss: 0.7963521386595334\n",
      "Epoch 93, Training Loss: 0.7963206569587483\n",
      "Epoch 94, Training Loss: 0.7958616281256956\n",
      "Epoch 95, Training Loss: 0.7967226450583514\n",
      "Epoch 96, Training Loss: 0.796444770448348\n",
      "Epoch 97, Training Loss: 0.7963103529986213\n",
      "Epoch 98, Training Loss: 0.7966573633867151\n",
      "Epoch 99, Training Loss: 0.7959530806541443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:37:52,093] Trial 313 finished with value: 0.6312666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7962509148261127\n",
      "Epoch 1, Training Loss: 0.9872471073795768\n",
      "Epoch 2, Training Loss: 0.9338681595465717\n",
      "Epoch 3, Training Loss: 0.8923813475580776\n",
      "Epoch 4, Training Loss: 0.8412196096251993\n",
      "Epoch 5, Training Loss: 0.8172244576145621\n",
      "Epoch 6, Training Loss: 0.8127477654990027\n",
      "Epoch 7, Training Loss: 0.8112162759023555\n",
      "Epoch 8, Training Loss: 0.8106173635230345\n",
      "Epoch 9, Training Loss: 0.8095894555484547\n",
      "Epoch 10, Training Loss: 0.8079384699288537\n",
      "Epoch 11, Training Loss: 0.8076229920106776\n",
      "Epoch 12, Training Loss: 0.8070694466198192\n",
      "Epoch 13, Training Loss: 0.8058989831980536\n",
      "Epoch 14, Training Loss: 0.8051200110772077\n",
      "Epoch 15, Training Loss: 0.8052669049711788\n",
      "Epoch 16, Training Loss: 0.8037152804346646\n",
      "Epoch 17, Training Loss: 0.803495380597956\n",
      "Epoch 18, Training Loss: 0.8026306934216443\n",
      "Epoch 19, Training Loss: 0.8025756685874041\n",
      "Epoch 20, Training Loss: 0.8024226610800799\n",
      "Epoch 21, Training Loss: 0.8022717003962573\n",
      "Epoch 22, Training Loss: 0.8019459189386928\n",
      "Epoch 23, Training Loss: 0.8016763240449569\n",
      "Epoch 24, Training Loss: 0.8015662497632644\n",
      "Epoch 25, Training Loss: 0.8011478039096384\n",
      "Epoch 26, Training Loss: 0.8005091680498684\n",
      "Epoch 27, Training Loss: 0.8006741848412682\n",
      "Epoch 28, Training Loss: 0.8001227119389702\n",
      "Epoch 29, Training Loss: 0.8002368973984438\n",
      "Epoch 30, Training Loss: 0.8000138600433574\n",
      "Epoch 31, Training Loss: 0.7999130403294282\n",
      "Epoch 32, Training Loss: 0.7994923655425801\n",
      "Epoch 33, Training Loss: 0.7999335230098051\n",
      "Epoch 34, Training Loss: 0.7995658745485194\n",
      "Epoch 35, Training Loss: 0.7990394227644977\n",
      "Epoch 36, Training Loss: 0.7988182004760294\n",
      "Epoch 37, Training Loss: 0.7991562715698691\n",
      "Epoch 38, Training Loss: 0.7989758562340455\n",
      "Epoch 39, Training Loss: 0.7986408120043138\n",
      "Epoch 40, Training Loss: 0.7988365923657137\n",
      "Epoch 41, Training Loss: 0.7988819921016693\n",
      "Epoch 42, Training Loss: 0.7981814717545229\n",
      "Epoch 43, Training Loss: 0.7988798184955821\n",
      "Epoch 44, Training Loss: 0.7984326166265151\n",
      "Epoch 45, Training Loss: 0.7977978437087115\n",
      "Epoch 46, Training Loss: 0.7978588210133946\n",
      "Epoch 47, Training Loss: 0.7980240496467141\n",
      "Epoch 48, Training Loss: 0.7979501532105838\n",
      "Epoch 49, Training Loss: 0.7973118049256942\n",
      "Epoch 50, Training Loss: 0.7976695317380569\n",
      "Epoch 51, Training Loss: 0.7979503554456374\n",
      "Epoch 52, Training Loss: 0.7974135122579686\n",
      "Epoch 53, Training Loss: 0.7968522231719073\n",
      "Epoch 54, Training Loss: 0.7973166376001695\n",
      "Epoch 55, Training Loss: 0.7972540641532225\n",
      "Epoch 56, Training Loss: 0.7968776341045604\n",
      "Epoch 57, Training Loss: 0.7970286456276389\n",
      "Epoch 58, Training Loss: 0.7963835162274978\n",
      "Epoch 59, Training Loss: 0.7965894953643574\n",
      "Epoch 60, Training Loss: 0.7964636723434224\n",
      "Epoch 61, Training Loss: 0.7966498920496772\n",
      "Epoch 62, Training Loss: 0.7960611365823185\n",
      "Epoch 63, Training Loss: 0.7960689577635597\n",
      "Epoch 64, Training Loss: 0.7959966397285462\n",
      "Epoch 65, Training Loss: 0.7962337759663077\n",
      "Epoch 66, Training Loss: 0.7959380866499508\n",
      "Epoch 67, Training Loss: 0.7960061992617214\n",
      "Epoch 68, Training Loss: 0.7956106472716612\n",
      "Epoch 69, Training Loss: 0.7956284900272593\n",
      "Epoch 70, Training Loss: 0.7953199622911565\n",
      "Epoch 71, Training Loss: 0.7948488960546606\n",
      "Epoch 72, Training Loss: 0.7952829304162193\n",
      "Epoch 73, Training Loss: 0.7949612331390381\n",
      "Epoch 74, Training Loss: 0.7944945486152873\n",
      "Epoch 75, Training Loss: 0.7946363596355214\n",
      "Epoch 76, Training Loss: 0.7944657510168412\n",
      "Epoch 77, Training Loss: 0.7944074517839095\n",
      "Epoch 78, Training Loss: 0.794362252740299\n",
      "Epoch 79, Training Loss: 0.7941029452576357\n",
      "Epoch 80, Training Loss: 0.7939160978794098\n",
      "Epoch 81, Training Loss: 0.7941190858448253\n",
      "Epoch 82, Training Loss: 0.7936837695626652\n",
      "Epoch 83, Training Loss: 0.7936791707487667\n",
      "Epoch 84, Training Loss: 0.7936778085372027\n",
      "Epoch 85, Training Loss: 0.7928144476694219\n",
      "Epoch 86, Training Loss: 0.7926826991053189\n",
      "Epoch 87, Training Loss: 0.7927926644156961\n",
      "Epoch 88, Training Loss: 0.7922114848389346\n",
      "Epoch 89, Training Loss: 0.7922433086002574\n",
      "Epoch 90, Training Loss: 0.7923227937081281\n",
      "Epoch 91, Training Loss: 0.7920694740379558\n",
      "Epoch 92, Training Loss: 0.7913733102994807\n",
      "Epoch 93, Training Loss: 0.7917229817895328\n",
      "Epoch 94, Training Loss: 0.791415289850796\n",
      "Epoch 95, Training Loss: 0.7909983420372009\n",
      "Epoch 96, Training Loss: 0.7909816002144533\n",
      "Epoch 97, Training Loss: 0.7907861429803512\n",
      "Epoch 98, Training Loss: 0.7907362175689024\n",
      "Epoch 99, Training Loss: 0.7905596439277425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:40:09,255] Trial 314 finished with value: 0.6364 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7898908575843362\n",
      "Epoch 1, Training Loss: 0.9839952842628255\n",
      "Epoch 2, Training Loss: 0.9376780392141904\n",
      "Epoch 3, Training Loss: 0.9045732432954452\n",
      "Epoch 4, Training Loss: 0.8653575180558597\n",
      "Epoch 5, Training Loss: 0.831252133285298\n",
      "Epoch 6, Training Loss: 0.816673351876876\n",
      "Epoch 7, Training Loss: 0.8125356879655052\n",
      "Epoch 8, Training Loss: 0.8110482054598191\n",
      "Epoch 9, Training Loss: 0.8099288354901707\n",
      "Epoch 10, Training Loss: 0.809475034755819\n",
      "Epoch 11, Training Loss: 0.8086948122697718\n",
      "Epoch 12, Training Loss: 0.808059128592996\n",
      "Epoch 13, Training Loss: 0.8068281780972201\n",
      "Epoch 14, Training Loss: 0.8062225554269903\n",
      "Epoch 15, Training Loss: 0.8058647434150471\n",
      "Epoch 16, Training Loss: 0.8051986436282887\n",
      "Epoch 17, Training Loss: 0.8047215722588932\n",
      "Epoch 18, Training Loss: 0.8043348836898804\n",
      "Epoch 19, Training Loss: 0.803973235873615\n",
      "Epoch 20, Training Loss: 0.8034651715615216\n",
      "Epoch 21, Training Loss: 0.8035333074541653\n",
      "Epoch 22, Training Loss: 0.803325438078712\n",
      "Epoch 23, Training Loss: 0.8027430244053111\n",
      "Epoch 24, Training Loss: 0.8026598854625926\n",
      "Epoch 25, Training Loss: 0.8024713845813976\n",
      "Epoch 26, Training Loss: 0.8021264299224404\n",
      "Epoch 27, Training Loss: 0.8020326247636009\n",
      "Epoch 28, Training Loss: 0.8018322855584762\n",
      "Epoch 29, Training Loss: 0.8013266209995046\n",
      "Epoch 30, Training Loss: 0.8014886231282178\n",
      "Epoch 31, Training Loss: 0.8011452102661133\n",
      "Epoch 32, Training Loss: 0.801350424009211\n",
      "Epoch 33, Training Loss: 0.8015823259774376\n",
      "Epoch 34, Training Loss: 0.8010040692020866\n",
      "Epoch 35, Training Loss: 0.8011133043205036\n",
      "Epoch 36, Training Loss: 0.800799687609953\n",
      "Epoch 37, Training Loss: 0.8007542192935944\n",
      "Epoch 38, Training Loss: 0.8003574794881484\n",
      "Epoch 39, Training Loss: 0.8005137263326084\n",
      "Epoch 40, Training Loss: 0.8002115126918344\n",
      "Epoch 41, Training Loss: 0.8000160466222203\n",
      "Epoch 42, Training Loss: 0.8001670981855954\n",
      "Epoch 43, Training Loss: 0.7998157573447507\n",
      "Epoch 44, Training Loss: 0.7999867342500125\n",
      "Epoch 45, Training Loss: 0.7996371484504027\n",
      "Epoch 46, Training Loss: 0.79946349754053\n",
      "Epoch 47, Training Loss: 0.7990903411893283\n",
      "Epoch 48, Training Loss: 0.7992595875263214\n",
      "Epoch 49, Training Loss: 0.7990121448040008\n",
      "Epoch 50, Training Loss: 0.7990850529249977\n",
      "Epoch 51, Training Loss: 0.7989064318993512\n",
      "Epoch 52, Training Loss: 0.7988081545689527\n",
      "Epoch 53, Training Loss: 0.7984806616867289\n",
      "Epoch 54, Training Loss: 0.7985197553213905\n",
      "Epoch 55, Training Loss: 0.7985602453876944\n",
      "Epoch 56, Training Loss: 0.7981631220088286\n",
      "Epoch 57, Training Loss: 0.7985331031154184\n",
      "Epoch 58, Training Loss: 0.7977825315559611\n",
      "Epoch 59, Training Loss: 0.7980196214423461\n",
      "Epoch 60, Training Loss: 0.7979278502043555\n",
      "Epoch 61, Training Loss: 0.7979655541391933\n",
      "Epoch 62, Training Loss: 0.7978541818787069\n",
      "Epoch 63, Training Loss: 0.7974326295712415\n",
      "Epoch 64, Training Loss: 0.7975125579272999\n",
      "Epoch 65, Training Loss: 0.7971348308815676\n",
      "Epoch 66, Training Loss: 0.7973632810396306\n",
      "Epoch 67, Training Loss: 0.7969444342921762\n",
      "Epoch 68, Training Loss: 0.7970610427155215\n",
      "Epoch 69, Training Loss: 0.7969474903976216\n",
      "Epoch 70, Training Loss: 0.7967553694107953\n",
      "Epoch 71, Training Loss: 0.7970579014806186\n",
      "Epoch 72, Training Loss: 0.7965547618445228\n",
      "Epoch 73, Training Loss: 0.7966126187408672\n",
      "Epoch 74, Training Loss: 0.7964510383325465\n",
      "Epoch 75, Training Loss: 0.7963352037878597\n",
      "Epoch 76, Training Loss: 0.7962262250395382\n",
      "Epoch 77, Training Loss: 0.79612196578699\n",
      "Epoch 78, Training Loss: 0.7956933245939367\n",
      "Epoch 79, Training Loss: 0.7959292763120988\n",
      "Epoch 80, Training Loss: 0.7954762612370884\n",
      "Epoch 81, Training Loss: 0.7957634622910443\n",
      "Epoch 82, Training Loss: 0.7955788604652181\n",
      "Epoch 83, Training Loss: 0.7954239461001228\n",
      "Epoch 84, Training Loss: 0.7952257166890537\n",
      "Epoch 85, Training Loss: 0.7953196705088896\n",
      "Epoch 86, Training Loss: 0.7950591965983895\n",
      "Epoch 87, Training Loss: 0.7946415378766901\n",
      "Epoch 88, Training Loss: 0.7949047973576714\n",
      "Epoch 89, Training Loss: 0.7947472939070533\n",
      "Epoch 90, Training Loss: 0.7944370681398055\n",
      "Epoch 91, Training Loss: 0.7946266765454236\n",
      "Epoch 92, Training Loss: 0.7941154515743256\n",
      "Epoch 93, Training Loss: 0.7941462664744433\n",
      "Epoch 94, Training Loss: 0.7941463067251093\n",
      "Epoch 95, Training Loss: 0.7937199818386751\n",
      "Epoch 96, Training Loss: 0.7935748417237226\n",
      "Epoch 97, Training Loss: 0.7934937489733976\n",
      "Epoch 98, Training Loss: 0.7932429271585801\n",
      "Epoch 99, Training Loss: 0.7930824718054603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:42:15,525] Trial 315 finished with value: 0.6335333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7930368361052345\n",
      "Epoch 1, Training Loss: 0.9930422695955836\n",
      "Epoch 2, Training Loss: 0.943517984393844\n",
      "Epoch 3, Training Loss: 0.9269497272663547\n",
      "Epoch 4, Training Loss: 0.9056845323483746\n",
      "Epoch 5, Training Loss: 0.882293827282755\n",
      "Epoch 6, Training Loss: 0.8613948101387885\n",
      "Epoch 7, Training Loss: 0.8447686202544019\n",
      "Epoch 8, Training Loss: 0.8324199402242675\n",
      "Epoch 9, Training Loss: 0.824756493962797\n",
      "Epoch 10, Training Loss: 0.820262240646477\n",
      "Epoch 11, Training Loss: 0.8166430455401428\n",
      "Epoch 12, Training Loss: 0.8143641553427043\n",
      "Epoch 13, Training Loss: 0.8129811052092932\n",
      "Epoch 14, Training Loss: 0.8120817690863645\n",
      "Epoch 15, Training Loss: 0.8110533093151293\n",
      "Epoch 16, Training Loss: 0.8105375006682891\n",
      "Epoch 17, Training Loss: 0.8101540408636394\n",
      "Epoch 18, Training Loss: 0.8096910544804165\n",
      "Epoch 19, Training Loss: 0.809194592873853\n",
      "Epoch 20, Training Loss: 0.809881595919903\n",
      "Epoch 21, Training Loss: 0.8084649490234547\n",
      "Epoch 22, Training Loss: 0.808861003513623\n",
      "Epoch 23, Training Loss: 0.807960499856705\n",
      "Epoch 24, Training Loss: 0.8073575574204438\n",
      "Epoch 25, Training Loss: 0.8076493323297429\n",
      "Epoch 26, Training Loss: 0.8070085649203537\n",
      "Epoch 27, Training Loss: 0.8065740286855769\n",
      "Epoch 28, Training Loss: 0.8066612877343831\n",
      "Epoch 29, Training Loss: 0.8063997177253092\n",
      "Epoch 30, Training Loss: 0.8074038880211967\n",
      "Epoch 31, Training Loss: 0.8057982960141691\n",
      "Epoch 32, Training Loss: 0.8058819078861322\n",
      "Epoch 33, Training Loss: 0.8057489935616802\n",
      "Epoch 34, Training Loss: 0.8053725346586758\n",
      "Epoch 35, Training Loss: 0.8059680384800847\n",
      "Epoch 36, Training Loss: 0.8055495836680993\n",
      "Epoch 37, Training Loss: 0.8051148763276581\n",
      "Epoch 38, Training Loss: 0.8054190675118812\n",
      "Epoch 39, Training Loss: 0.8051455809657735\n",
      "Epoch 40, Training Loss: 0.805039261964927\n",
      "Epoch 41, Training Loss: 0.8053787355136154\n",
      "Epoch 42, Training Loss: 0.8044596204184051\n",
      "Epoch 43, Training Loss: 0.8041288880477274\n",
      "Epoch 44, Training Loss: 0.8046682008227012\n",
      "Epoch 45, Training Loss: 0.8047229386810073\n",
      "Epoch 46, Training Loss: 0.8033498079705059\n",
      "Epoch 47, Training Loss: 0.8042814087150688\n",
      "Epoch 48, Training Loss: 0.8045364306385355\n",
      "Epoch 49, Training Loss: 0.8040866617869614\n",
      "Epoch 50, Training Loss: 0.8033155190317254\n",
      "Epoch 51, Training Loss: 0.803686605449906\n",
      "Epoch 52, Training Loss: 0.8034275186689277\n",
      "Epoch 53, Training Loss: 0.8035466558054873\n",
      "Epoch 54, Training Loss: 0.8032314758551748\n",
      "Epoch 55, Training Loss: 0.8027047936181376\n",
      "Epoch 56, Training Loss: 0.8028328098748859\n",
      "Epoch 57, Training Loss: 0.8023427158370053\n",
      "Epoch 58, Training Loss: 0.8025827594269487\n",
      "Epoch 59, Training Loss: 0.8026404774278626\n",
      "Epoch 60, Training Loss: 0.8026745797996234\n",
      "Epoch 61, Training Loss: 0.8017981401959756\n",
      "Epoch 62, Training Loss: 0.8021863665795864\n",
      "Epoch 63, Training Loss: 0.8014218396710274\n",
      "Epoch 64, Training Loss: 0.8022363057710175\n",
      "Epoch 65, Training Loss: 0.8020419689945708\n",
      "Epoch 66, Training Loss: 0.8022364424583607\n",
      "Epoch 67, Training Loss: 0.8012199186292807\n",
      "Epoch 68, Training Loss: 0.8015673272591785\n",
      "Epoch 69, Training Loss: 0.8014632691118054\n",
      "Epoch 70, Training Loss: 0.801126023521997\n",
      "Epoch 71, Training Loss: 0.8018609406356525\n",
      "Epoch 72, Training Loss: 0.8010520872316862\n",
      "Epoch 73, Training Loss: 0.8009569771307752\n",
      "Epoch 74, Training Loss: 0.8008951979472225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:43:28,368] Trial 316 finished with value: 0.6346666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8015082384410658\n",
      "Epoch 1, Training Loss: 0.8468393731834297\n",
      "Epoch 2, Training Loss: 0.8161378728715997\n",
      "Epoch 3, Training Loss: 0.8126853064486855\n",
      "Epoch 4, Training Loss: 0.8130002088116524\n",
      "Epoch 5, Training Loss: 0.8126833602001793\n",
      "Epoch 6, Training Loss: 0.8096759693963187\n",
      "Epoch 7, Training Loss: 0.8069775476491541\n",
      "Epoch 8, Training Loss: 0.8054334035493378\n",
      "Epoch 9, Training Loss: 0.8060598515030136\n",
      "Epoch 10, Training Loss: 0.8052582872541327\n",
      "Epoch 11, Training Loss: 0.8044084228071054\n",
      "Epoch 12, Training Loss: 0.8031242909288048\n",
      "Epoch 13, Training Loss: 0.8033031716382593\n",
      "Epoch 14, Training Loss: 0.80140431303727\n",
      "Epoch 15, Training Loss: 0.8020563767368633\n",
      "Epoch 16, Training Loss: 0.8012759260665205\n",
      "Epoch 17, Training Loss: 0.8031223799949302\n",
      "Epoch 18, Training Loss: 0.7999225504416272\n",
      "Epoch 19, Training Loss: 0.8010809264684978\n",
      "Epoch 20, Training Loss: 0.8017105099850131\n",
      "Epoch 21, Training Loss: 0.800681023310898\n",
      "Epoch 22, Training Loss: 0.7995689308732972\n",
      "Epoch 23, Training Loss: 0.8022568129059068\n",
      "Epoch 24, Training Loss: 0.8017547471182687\n",
      "Epoch 25, Training Loss: 0.8003206124879364\n",
      "Epoch 26, Training Loss: 0.7990023191710164\n",
      "Epoch 27, Training Loss: 0.7994837606759896\n",
      "Epoch 28, Training Loss: 0.7990665410694323\n",
      "Epoch 29, Training Loss: 0.800530961760901\n",
      "Epoch 30, Training Loss: 0.7989164524508598\n",
      "Epoch 31, Training Loss: 0.7995551683848962\n",
      "Epoch 32, Training Loss: 0.7995121672637481\n",
      "Epoch 33, Training Loss: 0.7985322905662364\n",
      "Epoch 34, Training Loss: 0.7984735050595793\n",
      "Epoch 35, Training Loss: 0.7982940251665904\n",
      "Epoch 36, Training Loss: 0.7992965209752994\n",
      "Epoch 37, Training Loss: 0.7994803966436171\n",
      "Epoch 38, Training Loss: 0.7990592463572223\n",
      "Epoch 39, Training Loss: 0.7985382520166555\n",
      "Epoch 40, Training Loss: 0.7984632896301441\n",
      "Epoch 41, Training Loss: 0.7975484072713923\n",
      "Epoch 42, Training Loss: 0.7980884499567792\n",
      "Epoch 43, Training Loss: 0.7987075592342175\n",
      "Epoch 44, Training Loss: 0.797937321931796\n",
      "Epoch 45, Training Loss: 0.7973010628743279\n",
      "Epoch 46, Training Loss: 0.7976582139954531\n",
      "Epoch 47, Training Loss: 0.7983829325303099\n",
      "Epoch 48, Training Loss: 0.7983874604217989\n",
      "Epoch 49, Training Loss: 0.7972501053845972\n",
      "Epoch 50, Training Loss: 0.7966772186128717\n",
      "Epoch 51, Training Loss: 0.7976607344204322\n",
      "Epoch 52, Training Loss: 0.7971360895866738\n",
      "Epoch 53, Training Loss: 0.7979685450855054\n",
      "Epoch 54, Training Loss: 0.7982088448409748\n",
      "Epoch 55, Training Loss: 0.7974072299505535\n",
      "Epoch 56, Training Loss: 0.7983386852687463\n",
      "Epoch 57, Training Loss: 0.7971373596585782\n",
      "Epoch 58, Training Loss: 0.7976831679057358\n",
      "Epoch 59, Training Loss: 0.7977728276324452\n",
      "Epoch 60, Training Loss: 0.7967979816565837\n",
      "Epoch 61, Training Loss: 0.7968468898221066\n",
      "Epoch 62, Training Loss: 0.7959873483593303\n",
      "Epoch 63, Training Loss: 0.7957405745534969\n",
      "Epoch 64, Training Loss: 0.7967623002547071\n",
      "Epoch 65, Training Loss: 0.7981876008492663\n",
      "Epoch 66, Training Loss: 0.7968451557302834\n",
      "Epoch 67, Training Loss: 0.7961028718410578\n",
      "Epoch 68, Training Loss: 0.7955723272230392\n",
      "Epoch 69, Training Loss: 0.7964704915096885\n",
      "Epoch 70, Training Loss: 0.7962449709275612\n",
      "Epoch 71, Training Loss: 0.7961088479013372\n",
      "Epoch 72, Training Loss: 0.7977346579831346\n",
      "Epoch 73, Training Loss: 0.7961112246477514\n",
      "Epoch 74, Training Loss: 0.7961243814095519\n",
      "Epoch 75, Training Loss: 0.7959855636259667\n",
      "Epoch 76, Training Loss: 0.7967383927868721\n",
      "Epoch 77, Training Loss: 0.7958906278574377\n",
      "Epoch 78, Training Loss: 0.7958585661156733\n",
      "Epoch 79, Training Loss: 0.7953445237382014\n",
      "Epoch 80, Training Loss: 0.79645880792374\n",
      "Epoch 81, Training Loss: 0.7954556783339135\n",
      "Epoch 82, Training Loss: 0.7960259764714348\n",
      "Epoch 83, Training Loss: 0.7950442994447579\n",
      "Epoch 84, Training Loss: 0.7956543238539445\n",
      "Epoch 85, Training Loss: 0.7949088895231261\n",
      "Epoch 86, Training Loss: 0.7957328028248665\n",
      "Epoch 87, Training Loss: 0.7971262524002477\n",
      "Epoch 88, Training Loss: 0.7960014660555618\n",
      "Epoch 89, Training Loss: 0.7971568322719488\n",
      "Epoch 90, Training Loss: 0.7954294801654672\n",
      "Epoch 91, Training Loss: 0.7945041447653807\n",
      "Epoch 92, Training Loss: 0.7949942741627084\n",
      "Epoch 93, Training Loss: 0.7961087631999998\n",
      "Epoch 94, Training Loss: 0.7955515319243409\n",
      "Epoch 95, Training Loss: 0.7948254763631892\n",
      "Epoch 96, Training Loss: 0.7948060351206844\n",
      "Epoch 97, Training Loss: 0.7955170590178411\n",
      "Epoch 98, Training Loss: 0.7951385211227532\n",
      "Epoch 99, Training Loss: 0.7949125479038497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:45:06,529] Trial 317 finished with value: 0.6357333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.795765813759395\n",
      "Epoch 1, Training Loss: 0.8536329997213263\n",
      "Epoch 2, Training Loss: 0.8171032368688655\n",
      "Epoch 3, Training Loss: 0.8156129064416526\n",
      "Epoch 4, Training Loss: 0.8140005940781501\n",
      "Epoch 5, Training Loss: 0.8107448439849051\n",
      "Epoch 6, Training Loss: 0.8080693229248649\n",
      "Epoch 7, Training Loss: 0.8075172334237206\n",
      "Epoch 8, Training Loss: 0.8080024503227463\n",
      "Epoch 9, Training Loss: 0.8068766391366944\n",
      "Epoch 10, Training Loss: 0.8057100248516054\n",
      "Epoch 11, Training Loss: 0.8059296011028433\n",
      "Epoch 12, Training Loss: 0.8059300554426093\n",
      "Epoch 13, Training Loss: 0.8052970196967735\n",
      "Epoch 14, Training Loss: 0.8055674325254627\n",
      "Epoch 15, Training Loss: 0.8066630717506982\n",
      "Epoch 16, Training Loss: 0.804471627034639\n",
      "Epoch 17, Training Loss: 0.8053913784206361\n",
      "Epoch 18, Training Loss: 0.8048822040844681\n",
      "Epoch 19, Training Loss: 0.8039316773414612\n",
      "Epoch 20, Training Loss: 0.8038803033362654\n",
      "Epoch 21, Training Loss: 0.8051926811834923\n",
      "Epoch 22, Training Loss: 0.8029791524535731\n",
      "Epoch 23, Training Loss: 0.8045934919127844\n",
      "Epoch 24, Training Loss: 0.8039688172196984\n",
      "Epoch 25, Training Loss: 0.8045165195500941\n",
      "Epoch 26, Training Loss: 0.8043537475112685\n",
      "Epoch 27, Training Loss: 0.8040921663879452\n",
      "Epoch 28, Training Loss: 0.8033359263176308\n",
      "Epoch 29, Training Loss: 0.8034722836394059\n",
      "Epoch 30, Training Loss: 0.804837461611382\n",
      "Epoch 31, Training Loss: 0.8022088703356292\n",
      "Epoch 32, Training Loss: 0.8029910572489402\n",
      "Epoch 33, Training Loss: 0.8026956240037331\n",
      "Epoch 34, Training Loss: 0.8032657586542288\n",
      "Epoch 35, Training Loss: 0.8032194031808609\n",
      "Epoch 36, Training Loss: 0.80194116418523\n",
      "Epoch 37, Training Loss: 0.8023864998853296\n",
      "Epoch 38, Training Loss: 0.8025499834153885\n",
      "Epoch 39, Training Loss: 0.8029516804487186\n",
      "Epoch 40, Training Loss: 0.8024582646843186\n",
      "Epoch 41, Training Loss: 0.8017435675276849\n",
      "Epoch 42, Training Loss: 0.8016717130080202\n",
      "Epoch 43, Training Loss: 0.8007898909705026\n",
      "Epoch 44, Training Loss: 0.8016573611058687\n",
      "Epoch 45, Training Loss: 0.8009708677467547\n",
      "Epoch 46, Training Loss: 0.8014858654567174\n",
      "Epoch 47, Training Loss: 0.8004564792590034\n",
      "Epoch 48, Training Loss: 0.8001739008982379\n",
      "Epoch 49, Training Loss: 0.8006558128765651\n",
      "Epoch 50, Training Loss: 0.8001821508981232\n",
      "Epoch 51, Training Loss: 0.8004724149417161\n",
      "Epoch 52, Training Loss: 0.7999604631187325\n",
      "Epoch 53, Training Loss: 0.7993258404552488\n",
      "Epoch 54, Training Loss: 0.799241160450125\n",
      "Epoch 55, Training Loss: 0.7998998289717767\n",
      "Epoch 56, Training Loss: 0.798478064680458\n",
      "Epoch 57, Training Loss: 0.800357597214835\n",
      "Epoch 58, Training Loss: 0.7985445513761134\n",
      "Epoch 59, Training Loss: 0.7988274281634424\n",
      "Epoch 60, Training Loss: 0.7990541937655973\n",
      "Epoch 61, Training Loss: 0.7988056582615788\n",
      "Epoch 62, Training Loss: 0.7987113726318331\n",
      "Epoch 63, Training Loss: 0.7989500600592534\n",
      "Epoch 64, Training Loss: 0.7989917398395395\n",
      "Epoch 65, Training Loss: 0.7984131771819036\n",
      "Epoch 66, Training Loss: 0.7974667593948823\n",
      "Epoch 67, Training Loss: 0.7984592570398087\n",
      "Epoch 68, Training Loss: 0.7977772462636905\n",
      "Epoch 69, Training Loss: 0.7973292886762691\n",
      "Epoch 70, Training Loss: 0.7977123710445891\n",
      "Epoch 71, Training Loss: 0.7981708247858779\n",
      "Epoch 72, Training Loss: 0.7980467852793242\n",
      "Epoch 73, Training Loss: 0.7975294382052314\n",
      "Epoch 74, Training Loss: 0.7964548028501353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:46:20,206] Trial 318 finished with value: 0.6302666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7965014972184834\n",
      "Epoch 1, Training Loss: 0.8516831582434037\n",
      "Epoch 2, Training Loss: 0.8133364920756396\n",
      "Epoch 3, Training Loss: 0.8084231105271508\n",
      "Epoch 4, Training Loss: 0.8053240072727204\n",
      "Epoch 5, Training Loss: 0.8024534496139077\n",
      "Epoch 6, Training Loss: 0.8011468507963069\n",
      "Epoch 7, Training Loss: 0.7994324743747712\n",
      "Epoch 8, Training Loss: 0.7988391719144934\n",
      "Epoch 9, Training Loss: 0.7985612413462471\n",
      "Epoch 10, Training Loss: 0.7976910893356098\n",
      "Epoch 11, Training Loss: 0.7968516289486605\n",
      "Epoch 12, Training Loss: 0.795545576319975\n",
      "Epoch 13, Training Loss: 0.7959868945093715\n",
      "Epoch 14, Training Loss: 0.7960142411905177\n",
      "Epoch 15, Training Loss: 0.795799350458033\n",
      "Epoch 16, Training Loss: 0.7951165216109332\n",
      "Epoch 17, Training Loss: 0.7946177881605485\n",
      "Epoch 18, Training Loss: 0.7942245066867155\n",
      "Epoch 19, Training Loss: 0.7933701215070836\n",
      "Epoch 20, Training Loss: 0.7943152854723089\n",
      "Epoch 21, Training Loss: 0.7938354689233443\n",
      "Epoch 22, Training Loss: 0.7931402496029349\n",
      "Epoch 23, Training Loss: 0.793727524210425\n",
      "Epoch 24, Training Loss: 0.7936159377939561\n",
      "Epoch 25, Training Loss: 0.7932322472684523\n",
      "Epoch 26, Training Loss: 0.7929393376322353\n",
      "Epoch 27, Training Loss: 0.7929148574436412\n",
      "Epoch 28, Training Loss: 0.793017251701916\n",
      "Epoch 29, Training Loss: 0.7921438274664038\n",
      "Epoch 30, Training Loss: 0.7922086320203894\n",
      "Epoch 31, Training Loss: 0.7930633332448848\n",
      "Epoch 32, Training Loss: 0.7925540259305168\n",
      "Epoch 33, Training Loss: 0.7926893282637877\n",
      "Epoch 34, Training Loss: 0.7924954839313731\n",
      "Epoch 35, Training Loss: 0.791918441548067\n",
      "Epoch 36, Training Loss: 0.7917467106791104\n",
      "Epoch 37, Training Loss: 0.7921905415899614\n",
      "Epoch 38, Training Loss: 0.7916601595457863\n",
      "Epoch 39, Training Loss: 0.7913149458520553\n",
      "Epoch 40, Training Loss: 0.7917472767829895\n",
      "Epoch 41, Training Loss: 0.7918151734155767\n",
      "Epoch 42, Training Loss: 0.7913302127052756\n",
      "Epoch 43, Training Loss: 0.7914044194361742\n",
      "Epoch 44, Training Loss: 0.7916240374480977\n",
      "Epoch 45, Training Loss: 0.791197720415452\n",
      "Epoch 46, Training Loss: 0.7915179209148182\n",
      "Epoch 47, Training Loss: 0.791719855631099\n",
      "Epoch 48, Training Loss: 0.7909574359304765\n",
      "Epoch 49, Training Loss: 0.7915420603050906\n",
      "Epoch 50, Training Loss: 0.7910978858611163\n",
      "Epoch 51, Training Loss: 0.7915390862436855\n",
      "Epoch 52, Training Loss: 0.7915308371010948\n",
      "Epoch 53, Training Loss: 0.7914345467090607\n",
      "Epoch 54, Training Loss: 0.7910982229429133\n",
      "Epoch 55, Training Loss: 0.7916412465011372\n",
      "Epoch 56, Training Loss: 0.7912553398048177\n",
      "Epoch 57, Training Loss: 0.7909613175953136\n",
      "Epoch 58, Training Loss: 0.7910842908129972\n",
      "Epoch 59, Training Loss: 0.7917869790161357\n",
      "Epoch 60, Training Loss: 0.7908658452594981\n",
      "Epoch 61, Training Loss: 0.7901754506896523\n",
      "Epoch 62, Training Loss: 0.7905913937091827\n",
      "Epoch 63, Training Loss: 0.7902865573939155\n",
      "Epoch 64, Training Loss: 0.7906895849985235\n",
      "Epoch 65, Training Loss: 0.7907373900974498\n",
      "Epoch 66, Training Loss: 0.7902805512792924\n",
      "Epoch 67, Training Loss: 0.790232327264898\n",
      "Epoch 68, Training Loss: 0.7905012302538927\n",
      "Epoch 69, Training Loss: 0.7901816610728993\n",
      "Epoch 70, Training Loss: 0.7903822329465081\n",
      "Epoch 71, Training Loss: 0.7900768164326163\n",
      "Epoch 72, Training Loss: 0.7903452970701106\n",
      "Epoch 73, Training Loss: 0.7901859183872447\n",
      "Epoch 74, Training Loss: 0.7902611926022698\n",
      "Epoch 75, Training Loss: 0.7900541355329401\n",
      "Epoch 76, Training Loss: 0.7912511887971093\n",
      "Epoch 77, Training Loss: 0.7902894190479728\n",
      "Epoch 78, Training Loss: 0.7903427179420696\n",
      "Epoch 79, Training Loss: 0.7900770361984477\n",
      "Epoch 80, Training Loss: 0.7896449276279001\n",
      "Epoch 81, Training Loss: 0.7905398340786205\n",
      "Epoch 82, Training Loss: 0.7899394316533033\n",
      "Epoch 83, Training Loss: 0.7902573053977069\n",
      "Epoch 84, Training Loss: 0.7896413646726047\n",
      "Epoch 85, Training Loss: 0.7902701773363001\n",
      "Epoch 86, Training Loss: 0.7895151714717641\n",
      "Epoch 87, Training Loss: 0.7897030681021073\n",
      "Epoch 88, Training Loss: 0.7896705829395967\n",
      "Epoch 89, Training Loss: 0.7895400569018196\n",
      "Epoch 90, Training Loss: 0.7898710304148057\n",
      "Epoch 91, Training Loss: 0.7894756815012763\n",
      "Epoch 92, Training Loss: 0.7900015605197234\n",
      "Epoch 93, Training Loss: 0.7897229497572955\n",
      "Epoch 94, Training Loss: 0.7899223893530228\n",
      "Epoch 95, Training Loss: 0.7893914511624505\n",
      "Epoch 96, Training Loss: 0.7901652612405665\n",
      "Epoch 97, Training Loss: 0.7894720090136809\n",
      "Epoch 98, Training Loss: 0.7895861277159523\n",
      "Epoch 99, Training Loss: 0.7900703851615681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:48:27,706] Trial 319 finished with value: 0.6386 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7896574230053846\n",
      "Epoch 1, Training Loss: 1.0903451333326453\n",
      "Epoch 2, Training Loss: 1.0794773976943073\n",
      "Epoch 3, Training Loss: 1.0716032960835624\n",
      "Epoch 4, Training Loss: 1.06438350018333\n",
      "Epoch 5, Training Loss: 1.0576972373794107\n",
      "Epoch 6, Training Loss: 1.0514180395182442\n",
      "Epoch 7, Training Loss: 1.0455482658217936\n",
      "Epoch 8, Training Loss: 1.0400228157464195\n",
      "Epoch 9, Training Loss: 1.034820467373904\n",
      "Epoch 10, Training Loss: 1.029921227833804\n",
      "Epoch 11, Training Loss: 1.0252645758320305\n",
      "Epoch 12, Training Loss: 1.0209048902988433\n",
      "Epoch 13, Training Loss: 1.0167541564212126\n",
      "Epoch 14, Training Loss: 1.0128370857940001\n",
      "Epoch 15, Training Loss: 1.009138316266677\n",
      "Epoch 16, Training Loss: 1.0056305214236765\n",
      "Epoch 17, Training Loss: 1.002328715534771\n",
      "Epoch 18, Training Loss: 0.9992089539415696\n",
      "Epoch 19, Training Loss: 0.9962840007333195\n",
      "Epoch 20, Training Loss: 0.9935137105689329\n",
      "Epoch 21, Training Loss: 0.9908887243270874\n",
      "Epoch 22, Training Loss: 0.9884516976160161\n",
      "Epoch 23, Training Loss: 0.9861637764117297\n",
      "Epoch 24, Training Loss: 0.9840031556522145\n",
      "Epoch 25, Training Loss: 0.981983367695528\n",
      "Epoch 26, Training Loss: 0.9800752008662504\n",
      "Epoch 27, Training Loss: 0.9782753535579233\n",
      "Epoch 28, Training Loss: 0.9766277619670419\n",
      "Epoch 29, Training Loss: 0.975072408914566\n",
      "Epoch 30, Training Loss: 0.9736095754539266\n",
      "Epoch 31, Training Loss: 0.9722539291662329\n",
      "Epoch 32, Training Loss: 0.9709766558338614\n",
      "Epoch 33, Training Loss: 0.9697851763052099\n",
      "Epoch 34, Training Loss: 0.9686696021697101\n",
      "Epoch 35, Training Loss: 0.9676134791093715\n",
      "Epoch 36, Training Loss: 0.9666713805058423\n",
      "Epoch 37, Training Loss: 0.9657626084720388\n",
      "Epoch 38, Training Loss: 0.9648989746149849\n",
      "Epoch 39, Training Loss: 0.9641197046812843\n",
      "Epoch 40, Training Loss: 0.963382511138916\n",
      "Epoch 41, Training Loss: 0.9627077502362869\n",
      "Epoch 42, Training Loss: 0.9620606893651625\n",
      "Epoch 43, Training Loss: 0.9614566612944884\n",
      "Epoch 44, Training Loss: 0.9608866636192097\n",
      "Epoch 45, Training Loss: 0.9603607610393973\n",
      "Epoch 46, Training Loss: 0.959872534345178\n",
      "Epoch 47, Training Loss: 0.9593739961175357\n",
      "Epoch 48, Training Loss: 0.958965025509105\n",
      "Epoch 49, Training Loss: 0.9585574816255008\n",
      "Epoch 50, Training Loss: 0.9581764680497786\n",
      "Epoch 51, Training Loss: 0.9578081415681278\n",
      "Epoch 52, Training Loss: 0.9574689588126014\n",
      "Epoch 53, Training Loss: 0.9571389628859127\n",
      "Epoch 54, Training Loss: 0.956826115285649\n",
      "Epoch 55, Training Loss: 0.9565407045448527\n",
      "Epoch 56, Training Loss: 0.9562637186050416\n",
      "Epoch 57, Training Loss: 0.9559953278653762\n",
      "Epoch 58, Training Loss: 0.9557340374413659\n",
      "Epoch 59, Training Loss: 0.9554866492047029\n",
      "Epoch 60, Training Loss: 0.955286421916064\n",
      "Epoch 61, Training Loss: 0.9550600114289453\n",
      "Epoch 62, Training Loss: 0.9548276844445397\n",
      "Epoch 63, Training Loss: 0.9546422949258019\n",
      "Epoch 64, Training Loss: 0.9544425350077012\n",
      "Epoch 65, Training Loss: 0.9542501877335942\n",
      "Epoch 66, Training Loss: 0.9540703192177941\n",
      "Epoch 67, Training Loss: 0.9538941560072057\n",
      "Epoch 68, Training Loss: 0.9537233327416813\n",
      "Epoch 69, Training Loss: 0.9535617532449611\n",
      "Epoch 70, Training Loss: 0.9533833772995892\n",
      "Epoch 71, Training Loss: 0.9532372200489044\n",
      "Epoch 72, Training Loss: 0.9530764877796173\n",
      "Epoch 73, Training Loss: 0.952901524515713\n",
      "Epoch 74, Training Loss: 0.9527681961480309\n",
      "Epoch 75, Training Loss: 0.9526270391660578\n",
      "Epoch 76, Training Loss: 0.9524759534527274\n",
      "Epoch 77, Training Loss: 0.952323925705517\n",
      "Epoch 78, Training Loss: 0.9521901479188134\n",
      "Epoch 79, Training Loss: 0.9520519926267512\n",
      "Epoch 80, Training Loss: 0.9519087237470291\n",
      "Epoch 81, Training Loss: 0.9517602231222041\n",
      "Epoch 82, Training Loss: 0.9516475590537576\n",
      "Epoch 83, Training Loss: 0.9515032287906198\n",
      "Epoch 84, Training Loss: 0.9513684763627894\n",
      "Epoch 85, Training Loss: 0.9512363717135262\n",
      "Epoch 86, Training Loss: 0.9511154592738432\n",
      "Epoch 87, Training Loss: 0.9509795464487637\n",
      "Epoch 88, Training Loss: 0.950856749941321\n",
      "Epoch 89, Training Loss: 0.9507239538781783\n",
      "Epoch 90, Training Loss: 0.9506046039216659\n",
      "Epoch 91, Training Loss: 0.9504481637477875\n",
      "Epoch 92, Training Loss: 0.9503289506715886\n",
      "Epoch 93, Training Loss: 0.950212735077914\n",
      "Epoch 94, Training Loss: 0.9500853722235736\n",
      "Epoch 95, Training Loss: 0.9499577205321368\n",
      "Epoch 96, Training Loss: 0.9498325030943927\n",
      "Epoch 97, Training Loss: 0.9497031237097348\n",
      "Epoch 98, Training Loss: 0.9495896861833685\n",
      "Epoch 99, Training Loss: 0.949453094776939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:49:58,831] Trial 320 finished with value: 0.5351333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.9493324932631324\n",
      "Epoch 1, Training Loss: 1.104280219579998\n",
      "Epoch 2, Training Loss: 1.086724199029736\n",
      "Epoch 3, Training Loss: 1.0761253014543002\n",
      "Epoch 4, Training Loss: 1.0674606694314712\n",
      "Epoch 5, Training Loss: 1.05899006029717\n",
      "Epoch 6, Training Loss: 1.0503141112793657\n",
      "Epoch 7, Training Loss: 1.0413084180731522\n",
      "Epoch 8, Training Loss: 1.0320192313732062\n",
      "Epoch 9, Training Loss: 1.02220174334103\n",
      "Epoch 10, Training Loss: 1.0123768174558654\n",
      "Epoch 11, Training Loss: 1.0026042459602642\n",
      "Epoch 12, Training Loss: 0.9928369825047658\n",
      "Epoch 13, Training Loss: 0.983722768690353\n",
      "Epoch 14, Training Loss: 0.9753368971043064\n",
      "Epoch 15, Training Loss: 0.9675951452183544\n",
      "Epoch 16, Training Loss: 0.9603112893893306\n",
      "Epoch 17, Training Loss: 0.9542213630855532\n",
      "Epoch 18, Training Loss: 0.9490274501026125\n",
      "Epoch 19, Training Loss: 0.944734717759871\n",
      "Epoch 20, Training Loss: 0.9412521677805965\n",
      "Epoch 21, Training Loss: 0.9374486052900328\n",
      "Epoch 22, Training Loss: 0.9349313702798427\n",
      "Epoch 23, Training Loss: 0.9322625987511829\n",
      "Epoch 24, Training Loss: 0.9306302067928744\n",
      "Epoch 25, Training Loss: 0.9285760524577664\n",
      "Epoch 26, Training Loss: 0.9276564634832224\n",
      "Epoch 27, Training Loss: 0.9258345179091719\n",
      "Epoch 28, Training Loss: 0.9247316767398576\n",
      "Epoch 29, Training Loss: 0.9234476597685563\n",
      "Epoch 30, Training Loss: 0.9223357401396098\n",
      "Epoch 31, Training Loss: 0.9217969522440344\n",
      "Epoch 32, Training Loss: 0.9206914362154509\n",
      "Epoch 33, Training Loss: 0.9191180228290702\n",
      "Epoch 34, Training Loss: 0.9184674891314112\n",
      "Epoch 35, Training Loss: 0.9172843863193254\n",
      "Epoch 36, Training Loss: 0.9167244951527818\n",
      "Epoch 37, Training Loss: 0.9157064302523333\n",
      "Epoch 38, Training Loss: 0.9152444477368118\n",
      "Epoch 39, Training Loss: 0.914177882223201\n",
      "Epoch 40, Training Loss: 0.9129280221193357\n",
      "Epoch 41, Training Loss: 0.9127160676439902\n",
      "Epoch 42, Training Loss: 0.91167817142673\n",
      "Epoch 43, Training Loss: 0.9109497504126757\n",
      "Epoch 44, Training Loss: 0.910242089651581\n",
      "Epoch 45, Training Loss: 0.9090136141705334\n",
      "Epoch 46, Training Loss: 0.9082980803080968\n",
      "Epoch 47, Training Loss: 0.9077832348364636\n",
      "Epoch 48, Training Loss: 0.9064057800106536\n",
      "Epoch 49, Training Loss: 0.9054742303109706\n",
      "Epoch 50, Training Loss: 0.9051128331879924\n",
      "Epoch 51, Training Loss: 0.9045033592030518\n",
      "Epoch 52, Training Loss: 0.9039575864497881\n",
      "Epoch 53, Training Loss: 0.9024029860819193\n",
      "Epoch 54, Training Loss: 0.9020262752260481\n",
      "Epoch 55, Training Loss: 0.9009780106687905\n",
      "Epoch 56, Training Loss: 0.9003132694645932\n",
      "Epoch 57, Training Loss: 0.8996545293277367\n",
      "Epoch 58, Training Loss: 0.8986060018826248\n",
      "Epoch 59, Training Loss: 0.8976031564232102\n",
      "Epoch 60, Training Loss: 0.8966484439104123\n",
      "Epoch 61, Training Loss: 0.895962901760761\n",
      "Epoch 62, Training Loss: 0.8948945820779729\n",
      "Epoch 63, Training Loss: 0.8943815770005821\n",
      "Epoch 64, Training Loss: 0.893283348244832\n",
      "Epoch 65, Training Loss: 0.8925071083513417\n",
      "Epoch 66, Training Loss: 0.8920344953250168\n",
      "Epoch 67, Training Loss: 0.8905457178452858\n",
      "Epoch 68, Training Loss: 0.8895401479606342\n",
      "Epoch 69, Training Loss: 0.8884113208691876\n",
      "Epoch 70, Training Loss: 0.8876430390472699\n",
      "Epoch 71, Training Loss: 0.8866202501426066\n",
      "Epoch 72, Training Loss: 0.8858079844847657\n",
      "Epoch 73, Training Loss: 0.8843045259776868\n",
      "Epoch 74, Training Loss: 0.883022718053115\n",
      "Epoch 75, Training Loss: 0.8821762985752938\n",
      "Epoch 76, Training Loss: 0.8806673588609337\n",
      "Epoch 77, Training Loss: 0.8794651018945794\n",
      "Epoch 78, Training Loss: 0.8792609599299896\n",
      "Epoch 79, Training Loss: 0.8780638317416485\n",
      "Epoch 80, Training Loss: 0.8768742090777347\n",
      "Epoch 81, Training Loss: 0.8749785090747633\n",
      "Epoch 82, Training Loss: 0.8737136643632014\n",
      "Epoch 83, Training Loss: 0.8725778945406577\n",
      "Epoch 84, Training Loss: 0.8717318054428674\n",
      "Epoch 85, Training Loss: 0.8699603943000163\n",
      "Epoch 86, Training Loss: 0.8688643585470386\n",
      "Epoch 87, Training Loss: 0.8671824296614281\n",
      "Epoch 88, Training Loss: 0.8660390356429537\n",
      "Epoch 89, Training Loss: 0.8651764250339422\n",
      "Epoch 90, Training Loss: 0.8634116886253643\n",
      "Epoch 91, Training Loss: 0.86217625759598\n",
      "Epoch 92, Training Loss: 0.8606486202182626\n",
      "Epoch 93, Training Loss: 0.8593290996730776\n",
      "Epoch 94, Training Loss: 0.8576843307430583\n",
      "Epoch 95, Training Loss: 0.8564228009460564\n",
      "Epoch 96, Training Loss: 0.8550051680184845\n",
      "Epoch 97, Training Loss: 0.8538221196124428\n",
      "Epoch 98, Training Loss: 0.8524470558740143\n",
      "Epoch 99, Training Loss: 0.8509840336957373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:51:30,823] Trial 321 finished with value: 0.6071333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8499814624176886\n",
      "Epoch 1, Training Loss: 0.984243730727364\n",
      "Epoch 2, Training Loss: 0.9502953055325677\n",
      "Epoch 3, Training Loss: 0.9429680880378274\n",
      "Epoch 4, Training Loss: 0.9359580287512611\n",
      "Epoch 5, Training Loss: 0.9286727860394646\n",
      "Epoch 6, Training Loss: 0.9210674559368807\n",
      "Epoch 7, Training Loss: 0.913127903026693\n",
      "Epoch 8, Training Loss: 0.9048155384905198\n",
      "Epoch 9, Training Loss: 0.8960841689390294\n",
      "Epoch 10, Training Loss: 0.8871807596262764\n",
      "Epoch 11, Training Loss: 0.8783968062260572\n",
      "Epoch 12, Training Loss: 0.8697294905606439\n",
      "Epoch 13, Training Loss: 0.8614331223684198\n",
      "Epoch 14, Training Loss: 0.853792476163191\n",
      "Epoch 15, Training Loss: 0.8468661087400773\n",
      "Epoch 16, Training Loss: 0.8407694947719574\n",
      "Epoch 17, Training Loss: 0.8354168185065775\n",
      "Epoch 18, Training Loss: 0.8309973689387826\n",
      "Epoch 19, Training Loss: 0.8271498307059792\n",
      "Epoch 20, Training Loss: 0.8240666864900028\n",
      "Epoch 21, Training Loss: 0.8213619845053729\n",
      "Epoch 22, Training Loss: 0.8192779862179476\n",
      "Epoch 23, Training Loss: 0.8173602649744819\n",
      "Epoch 24, Training Loss: 0.8159780688846813\n",
      "Epoch 25, Training Loss: 0.8147442282648647\n",
      "Epoch 26, Training Loss: 0.8135421909304226\n",
      "Epoch 27, Training Loss: 0.8126833743908826\n",
      "Epoch 28, Training Loss: 0.8119456557666554\n",
      "Epoch 29, Training Loss: 0.8112422240481657\n",
      "Epoch 30, Training Loss: 0.8106128759243909\n",
      "Epoch 31, Training Loss: 0.8101911001345691\n",
      "Epoch 32, Training Loss: 0.809637801436817\n",
      "Epoch 33, Training Loss: 0.8092098984297584\n",
      "Epoch 34, Training Loss: 0.8088195507666643\n",
      "Epoch 35, Training Loss: 0.8085179467762218\n",
      "Epoch 36, Training Loss: 0.8080093718977536\n",
      "Epoch 37, Training Loss: 0.8078165086577921\n",
      "Epoch 38, Training Loss: 0.8073586407829734\n",
      "Epoch 39, Training Loss: 0.8071862062285928\n",
      "Epoch 40, Training Loss: 0.8068242542182698\n",
      "Epoch 41, Training Loss: 0.8065682302503024\n",
      "Epoch 42, Training Loss: 0.8062935878950007\n",
      "Epoch 43, Training Loss: 0.8061695326777065\n",
      "Epoch 44, Training Loss: 0.8058486457432018\n",
      "Epoch 45, Training Loss: 0.8056207494174733\n",
      "Epoch 46, Training Loss: 0.8053538059487062\n",
      "Epoch 47, Training Loss: 0.8052032852172851\n",
      "Epoch 48, Training Loss: 0.804964205307119\n",
      "Epoch 49, Training Loss: 0.8047814630760866\n",
      "Epoch 50, Training Loss: 0.8047060474227457\n",
      "Epoch 51, Training Loss: 0.8044362108146443\n",
      "Epoch 52, Training Loss: 0.8041922553146587\n",
      "Epoch 53, Training Loss: 0.8040686443272759\n",
      "Epoch 54, Training Loss: 0.803848068784265\n",
      "Epoch 55, Training Loss: 0.8037237129491919\n",
      "Epoch 56, Training Loss: 0.8034569990634918\n",
      "Epoch 57, Training Loss: 0.8033266528213725\n",
      "Epoch 58, Training Loss: 0.8032171154022216\n",
      "Epoch 59, Training Loss: 0.8030283012109645\n",
      "Epoch 60, Training Loss: 0.8029154936706319\n",
      "Epoch 61, Training Loss: 0.8026044171697954\n",
      "Epoch 62, Training Loss: 0.802587285602794\n",
      "Epoch 63, Training Loss: 0.8024222407621496\n",
      "Epoch 64, Training Loss: 0.8023579487379859\n",
      "Epoch 65, Training Loss: 0.8021765841456021\n",
      "Epoch 66, Training Loss: 0.8019770303193261\n",
      "Epoch 67, Training Loss: 0.8019524018203511\n",
      "Epoch 68, Training Loss: 0.801825621548821\n",
      "Epoch 69, Training Loss: 0.8016904615654665\n",
      "Epoch 70, Training Loss: 0.8016213360253502\n",
      "Epoch 71, Training Loss: 0.8014180138531853\n",
      "Epoch 72, Training Loss: 0.8014348608606002\n",
      "Epoch 73, Training Loss: 0.8012270729682025\n",
      "Epoch 74, Training Loss: 0.8012097999628852\n",
      "Epoch 75, Training Loss: 0.8010281280209036\n",
      "Epoch 76, Training Loss: 0.8008847885272082\n",
      "Epoch 77, Training Loss: 0.8008634008379544\n",
      "Epoch 78, Training Loss: 0.8009333524984472\n",
      "Epoch 79, Training Loss: 0.8006536225010367\n",
      "Epoch 80, Training Loss: 0.8005094221059014\n",
      "Epoch 81, Training Loss: 0.8006371187462527\n",
      "Epoch 82, Training Loss: 0.8002961064086241\n",
      "Epoch 83, Training Loss: 0.8002628109735601\n",
      "Epoch 84, Training Loss: 0.8002818042390487\n",
      "Epoch 85, Training Loss: 0.8001332246556001\n",
      "Epoch 86, Training Loss: 0.8001224984842188\n",
      "Epoch 87, Training Loss: 0.8000104431545033\n",
      "Epoch 88, Training Loss: 0.7998215652914609\n",
      "Epoch 89, Training Loss: 0.7999362203654121\n",
      "Epoch 90, Training Loss: 0.7998120395576253\n",
      "Epoch 91, Training Loss: 0.7998212765244876\n",
      "Epoch 92, Training Loss: 0.7996507261781132\n",
      "Epoch 93, Training Loss: 0.7996637971260968\n",
      "Epoch 94, Training Loss: 0.7996445009287666\n",
      "Epoch 95, Training Loss: 0.7993944261354559\n",
      "Epoch 96, Training Loss: 0.7993116244147805\n",
      "Epoch 97, Training Loss: 0.7994232710670023\n",
      "Epoch 98, Training Loss: 0.7992843324296615\n",
      "Epoch 99, Training Loss: 0.7992619147020228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:53:03,984] Trial 322 finished with value: 0.6338 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7991291140808778\n",
      "Epoch 1, Training Loss: 0.8868118558210485\n",
      "Epoch 2, Training Loss: 0.8156062028688543\n",
      "Epoch 3, Training Loss: 0.8061470363420599\n",
      "Epoch 4, Training Loss: 0.8029272343130672\n",
      "Epoch 5, Training Loss: 0.800673135028166\n",
      "Epoch 6, Training Loss: 0.7984691386363085\n",
      "Epoch 7, Training Loss: 0.795767929764355\n",
      "Epoch 8, Training Loss: 0.7936075914607329\n",
      "Epoch 9, Training Loss: 0.7926231348514556\n",
      "Epoch 10, Training Loss: 0.7912878615715925\n",
      "Epoch 11, Training Loss: 0.7908887717303108\n",
      "Epoch 12, Training Loss: 0.7901978011692271\n",
      "Epoch 13, Training Loss: 0.7894112557523391\n",
      "Epoch 14, Training Loss: 0.7885119135239546\n",
      "Epoch 15, Training Loss: 0.7884188789479872\n",
      "Epoch 16, Training Loss: 0.7882431455219493\n",
      "Epoch 17, Training Loss: 0.7871762522529153\n",
      "Epoch 18, Training Loss: 0.787023884899476\n",
      "Epoch 19, Training Loss: 0.7864005707291996\n",
      "Epoch 20, Training Loss: 0.786201127907809\n",
      "Epoch 21, Training Loss: 0.785817337737364\n",
      "Epoch 22, Training Loss: 0.7854259685207816\n",
      "Epoch 23, Training Loss: 0.7853934591657975\n",
      "Epoch 24, Training Loss: 0.7851619333379409\n",
      "Epoch 25, Training Loss: 0.784491640960469\n",
      "Epoch 26, Training Loss: 0.7841971645635717\n",
      "Epoch 27, Training Loss: 0.7838993154553806\n",
      "Epoch 28, Training Loss: 0.7834237690532909\n",
      "Epoch 29, Training Loss: 0.7837565373673159\n",
      "Epoch 30, Training Loss: 0.7831653508719276\n",
      "Epoch 31, Training Loss: 0.7830002906041987\n",
      "Epoch 32, Training Loss: 0.782988659844679\n",
      "Epoch 33, Training Loss: 0.7829995893730837\n",
      "Epoch 34, Training Loss: 0.7824594132339253\n",
      "Epoch 35, Training Loss: 0.7821892935388228\n",
      "Epoch 36, Training Loss: 0.7819037652716917\n",
      "Epoch 37, Training Loss: 0.7819546192533829\n",
      "Epoch 38, Training Loss: 0.7816713086997762\n",
      "Epoch 39, Training Loss: 0.7814021663806018\n",
      "Epoch 40, Training Loss: 0.7810726100556991\n",
      "Epoch 41, Training Loss: 0.7814225690505084\n",
      "Epoch 42, Training Loss: 0.7808483650403865\n",
      "Epoch 43, Training Loss: 0.7809356764484854\n",
      "Epoch 44, Training Loss: 0.7804885113239288\n",
      "Epoch 45, Training Loss: 0.780760642079746\n",
      "Epoch 46, Training Loss: 0.7806984902830685\n",
      "Epoch 47, Training Loss: 0.7798944043411928\n",
      "Epoch 48, Training Loss: 0.7804513433400322\n",
      "Epoch 49, Training Loss: 0.7801251570617451\n",
      "Epoch 50, Training Loss: 0.7799997705571792\n",
      "Epoch 51, Training Loss: 0.7797845274560592\n",
      "Epoch 52, Training Loss: 0.7798025928525364\n",
      "Epoch 53, Training Loss: 0.779591470746433\n",
      "Epoch 54, Training Loss: 0.7798289010104011\n",
      "Epoch 55, Training Loss: 0.7794748264200547\n",
      "Epoch 56, Training Loss: 0.7792230155187495\n",
      "Epoch 57, Training Loss: 0.7791837431402767\n",
      "Epoch 58, Training Loss: 0.7788809563833125\n",
      "Epoch 59, Training Loss: 0.7791427854930654\n",
      "Epoch 60, Training Loss: 0.7785636017603033\n",
      "Epoch 61, Training Loss: 0.7788272547020632\n",
      "Epoch 62, Training Loss: 0.7789351917715633\n",
      "Epoch 63, Training Loss: 0.7785414197865654\n",
      "Epoch 64, Training Loss: 0.7787647997631746\n",
      "Epoch 65, Training Loss: 0.7784043713176951\n",
      "Epoch 66, Training Loss: 0.7781825794893152\n",
      "Epoch 67, Training Loss: 0.7783592375587015\n",
      "Epoch 68, Training Loss: 0.7783924034763785\n",
      "Epoch 69, Training Loss: 0.7783644127845765\n",
      "Epoch 70, Training Loss: 0.7780514129470376\n",
      "Epoch 71, Training Loss: 0.7780790242728065\n",
      "Epoch 72, Training Loss: 0.7780112001475166\n",
      "Epoch 73, Training Loss: 0.7774030050109414\n",
      "Epoch 74, Training Loss: 0.7776254428134245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:54:40,127] Trial 323 finished with value: 0.6384 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7775275703738718\n",
      "Epoch 1, Training Loss: 1.0070085638410904\n",
      "Epoch 2, Training Loss: 0.9492395131026997\n",
      "Epoch 3, Training Loss: 0.9349857957924114\n",
      "Epoch 4, Training Loss: 0.9257508869030896\n",
      "Epoch 5, Training Loss: 0.917786747637917\n",
      "Epoch 6, Training Loss: 0.9102215850353241\n",
      "Epoch 7, Training Loss: 0.9027377394367667\n",
      "Epoch 8, Training Loss: 0.8947117981489967\n",
      "Epoch 9, Training Loss: 0.8861477077708525\n",
      "Epoch 10, Training Loss: 0.8775031595370348\n",
      "Epoch 11, Training Loss: 0.8688614874727586\n",
      "Epoch 12, Training Loss: 0.8605810260071474\n",
      "Epoch 13, Training Loss: 0.8525440827537986\n",
      "Epoch 14, Training Loss: 0.8451627469062806\n",
      "Epoch 15, Training Loss: 0.8385790269515093\n",
      "Epoch 16, Training Loss: 0.8328843224048614\n",
      "Epoch 17, Training Loss: 0.8279976284503937\n",
      "Epoch 18, Training Loss: 0.8237788299953236\n",
      "Epoch 19, Training Loss: 0.8203154359845554\n",
      "Epoch 20, Training Loss: 0.8174454006728004\n",
      "Epoch 21, Training Loss: 0.815054936689489\n",
      "Epoch 22, Training Loss: 0.8131071960926056\n",
      "Epoch 23, Training Loss: 0.8114476351878223\n",
      "Epoch 24, Training Loss: 0.8101274751915651\n",
      "Epoch 25, Training Loss: 0.8089446856695063\n",
      "Epoch 26, Training Loss: 0.8079912959126865\n",
      "Epoch 27, Training Loss: 0.8071353963543387\n",
      "Epoch 28, Training Loss: 0.8063525816272287\n",
      "Epoch 29, Training Loss: 0.8057271789101993\n",
      "Epoch 30, Training Loss: 0.8051679074764252\n",
      "Epoch 31, Training Loss: 0.8047485625743866\n",
      "Epoch 32, Training Loss: 0.8042216925761279\n",
      "Epoch 33, Training Loss: 0.8038311191166149\n",
      "Epoch 34, Training Loss: 0.8036298152979683\n",
      "Epoch 35, Training Loss: 0.8032490353724536\n",
      "Epoch 36, Training Loss: 0.8029529590466443\n",
      "Epoch 37, Training Loss: 0.8026955372445723\n",
      "Epoch 38, Training Loss: 0.8026110237486223\n",
      "Epoch 39, Training Loss: 0.8023480206377366\n",
      "Epoch 40, Training Loss: 0.8021757847421309\n",
      "Epoch 41, Training Loss: 0.8020616339935976\n",
      "Epoch 42, Training Loss: 0.8018997462356792\n",
      "Epoch 43, Training Loss: 0.8016943873377407\n",
      "Epoch 44, Training Loss: 0.8017340532471152\n",
      "Epoch 45, Training Loss: 0.8013625412127551\n",
      "Epoch 46, Training Loss: 0.8013755956116845\n",
      "Epoch 47, Training Loss: 0.8013237524733824\n",
      "Epoch 48, Training Loss: 0.8011564958796782\n",
      "Epoch 49, Training Loss: 0.8010262551728417\n",
      "Epoch 50, Training Loss: 0.8008796100756701\n",
      "Epoch 51, Training Loss: 0.8008138589999255\n",
      "Epoch 52, Training Loss: 0.80057276634609\n",
      "Epoch 53, Training Loss: 0.800456139830982\n",
      "Epoch 54, Training Loss: 0.8004801891831791\n",
      "Epoch 55, Training Loss: 0.8003172556792989\n",
      "Epoch 56, Training Loss: 0.8001440220019397\n",
      "Epoch 57, Training Loss: 0.800030768478618\n",
      "Epoch 58, Training Loss: 0.7999791864787831\n",
      "Epoch 59, Training Loss: 0.7998667806036333\n",
      "Epoch 60, Training Loss: 0.7997735495427075\n",
      "Epoch 61, Training Loss: 0.7997232169964734\n",
      "Epoch 62, Training Loss: 0.7995827223974116\n",
      "Epoch 63, Training Loss: 0.7994974380380967\n",
      "Epoch 64, Training Loss: 0.7994043073934667\n",
      "Epoch 65, Training Loss: 0.7991776187980876\n",
      "Epoch 66, Training Loss: 0.7992129410014434\n",
      "Epoch 67, Training Loss: 0.799207249248729\n",
      "Epoch 68, Training Loss: 0.7991106389550602\n",
      "Epoch 69, Training Loss: 0.7989960040064419\n",
      "Epoch 70, Training Loss: 0.7989314498620875\n",
      "Epoch 71, Training Loss: 0.7988758760340073\n",
      "Epoch 72, Training Loss: 0.7988387416390812\n",
      "Epoch 73, Training Loss: 0.7987085056304931\n",
      "Epoch 74, Training Loss: 0.7987340829652898\n",
      "Epoch 75, Training Loss: 0.7986119852346533\n",
      "Epoch 76, Training Loss: 0.7985647460993599\n",
      "Epoch 77, Training Loss: 0.7985349737195407\n",
      "Epoch 78, Training Loss: 0.7984265443156747\n",
      "Epoch 79, Training Loss: 0.7983921423379112\n",
      "Epoch 80, Training Loss: 0.7984300383399514\n",
      "Epoch 81, Training Loss: 0.798279911700417\n",
      "Epoch 82, Training Loss: 0.7983386967462652\n",
      "Epoch 83, Training Loss: 0.7982257077974432\n",
      "Epoch 84, Training Loss: 0.7981920373439789\n",
      "Epoch 85, Training Loss: 0.7981445745860829\n",
      "Epoch 86, Training Loss: 0.7979335308776182\n",
      "Epoch 87, Training Loss: 0.7979504363677081\n",
      "Epoch 88, Training Loss: 0.7980581461682039\n",
      "Epoch 89, Training Loss: 0.7978337718458737\n",
      "Epoch 90, Training Loss: 0.7978917711622575\n",
      "Epoch 91, Training Loss: 0.7978885595938738\n",
      "Epoch 92, Training Loss: 0.79783676441978\n",
      "Epoch 93, Training Loss: 0.797860538188149\n",
      "Epoch 94, Training Loss: 0.7976504191931556\n",
      "Epoch 95, Training Loss: 0.7976320858562694\n",
      "Epoch 96, Training Loss: 0.7975616424223956\n",
      "Epoch 97, Training Loss: 0.7975236035094542\n",
      "Epoch 98, Training Loss: 0.7975033378601074\n",
      "Epoch 99, Training Loss: 0.7974939464120304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:56:13,895] Trial 324 finished with value: 0.6369333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7973021226069507\n",
      "Epoch 1, Training Loss: 0.9336172216078814\n",
      "Epoch 2, Training Loss: 0.8791560705269085\n",
      "Epoch 3, Training Loss: 0.8380510046201594\n",
      "Epoch 4, Training Loss: 0.8158173840186175\n",
      "Epoch 5, Training Loss: 0.8077135046089396\n",
      "Epoch 6, Training Loss: 0.8047323543183944\n",
      "Epoch 7, Training Loss: 0.8028715663797715\n",
      "Epoch 8, Training Loss: 0.8019500856539782\n",
      "Epoch 9, Training Loss: 0.8007672511128818\n",
      "Epoch 10, Training Loss: 0.8007568765387816\n",
      "Epoch 11, Training Loss: 0.7999446125591503\n",
      "Epoch 12, Training Loss: 0.799465143329957\n",
      "Epoch 13, Training Loss: 0.7990150324036094\n",
      "Epoch 14, Training Loss: 0.7988718147838817\n",
      "Epoch 15, Training Loss: 0.7985989966813256\n",
      "Epoch 16, Training Loss: 0.7983609102052801\n",
      "Epoch 17, Training Loss: 0.7984192465333377\n",
      "Epoch 18, Training Loss: 0.7981012944614186\n",
      "Epoch 19, Training Loss: 0.7979864111367394\n",
      "Epoch 20, Training Loss: 0.7978076257425196\n",
      "Epoch 21, Training Loss: 0.7975542789347032\n",
      "Epoch 22, Training Loss: 0.797656170129776\n",
      "Epoch 23, Training Loss: 0.7971938227204716\n",
      "Epoch 24, Training Loss: 0.7979304776472204\n",
      "Epoch 25, Training Loss: 0.797441999912262\n",
      "Epoch 26, Training Loss: 0.7973456867302166\n",
      "Epoch 27, Training Loss: 0.7972259975180906\n",
      "Epoch 28, Training Loss: 0.796644654975218\n",
      "Epoch 29, Training Loss: 0.796583743165521\n",
      "Epoch 30, Training Loss: 0.7968125149782966\n",
      "Epoch 31, Training Loss: 0.7961237662680009\n",
      "Epoch 32, Training Loss: 0.7964333389085881\n",
      "Epoch 33, Training Loss: 0.7967756344290341\n",
      "Epoch 34, Training Loss: 0.7958970230467179\n",
      "Epoch 35, Training Loss: 0.7963243182967691\n",
      "Epoch 36, Training Loss: 0.7961136943452498\n",
      "Epoch 37, Training Loss: 0.7959614748113295\n",
      "Epoch 38, Training Loss: 0.7955995974821203\n",
      "Epoch 39, Training Loss: 0.7957126996797674\n",
      "Epoch 40, Training Loss: 0.7955722843899447\n",
      "Epoch 41, Training Loss: 0.7952510311323053\n",
      "Epoch 42, Training Loss: 0.7952727311498978\n",
      "Epoch 43, Training Loss: 0.7950564521200517\n",
      "Epoch 44, Training Loss: 0.7951932165201973\n",
      "Epoch 45, Training Loss: 0.7944528578309452\n",
      "Epoch 46, Training Loss: 0.7943482850579654\n",
      "Epoch 47, Training Loss: 0.7942570970338934\n",
      "Epoch 48, Training Loss: 0.7934098820826586\n",
      "Epoch 49, Training Loss: 0.7936715266985052\n",
      "Epoch 50, Training Loss: 0.7929478686697343\n",
      "Epoch 51, Training Loss: 0.792764892578125\n",
      "Epoch 52, Training Loss: 0.7924844084066504\n",
      "Epoch 53, Training Loss: 0.7923001060766333\n",
      "Epoch 54, Training Loss: 0.7917308205015519\n",
      "Epoch 55, Training Loss: 0.7914710088337169\n",
      "Epoch 56, Training Loss: 0.7912554308947395\n",
      "Epoch 57, Training Loss: 0.7911130356788635\n",
      "Epoch 58, Training Loss: 0.7908037859552047\n",
      "Epoch 59, Training Loss: 0.7906480599150938\n",
      "Epoch 60, Training Loss: 0.7905127333893496\n",
      "Epoch 61, Training Loss: 0.7905213303425733\n",
      "Epoch 62, Training Loss: 0.7904684743460487\n",
      "Epoch 63, Training Loss: 0.7901960036333869\n",
      "Epoch 64, Training Loss: 0.7903066979436313\n",
      "Epoch 65, Training Loss: 0.7901389124814202\n",
      "Epoch 66, Training Loss: 0.7901190854521358\n",
      "Epoch 67, Training Loss: 0.7899864404341754\n",
      "Epoch 68, Training Loss: 0.7902225033675923\n",
      "Epoch 69, Training Loss: 0.7899522012822768\n",
      "Epoch 70, Training Loss: 0.7901618762577282\n",
      "Epoch 71, Training Loss: 0.7898033565633438\n",
      "Epoch 72, Training Loss: 0.7897942027624916\n",
      "Epoch 73, Training Loss: 0.7896300401407129\n",
      "Epoch 74, Training Loss: 0.7897458539289587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:57:42,308] Trial 325 finished with value: 0.6356666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7897574419835035\n",
      "Epoch 1, Training Loss: 0.8710116900415982\n",
      "Epoch 2, Training Loss: 0.814525764058618\n",
      "Epoch 3, Training Loss: 0.8104754480193643\n",
      "Epoch 4, Training Loss: 0.807515340833103\n",
      "Epoch 5, Training Loss: 0.8050751535331502\n",
      "Epoch 6, Training Loss: 0.8042788641593036\n",
      "Epoch 7, Training Loss: 0.8032740865034216\n",
      "Epoch 8, Training Loss: 0.802254182100296\n",
      "Epoch 9, Training Loss: 0.8011006425408757\n",
      "Epoch 10, Training Loss: 0.8018803769700668\n",
      "Epoch 11, Training Loss: 0.8004745878892786\n",
      "Epoch 12, Training Loss: 0.8006743288741393\n",
      "Epoch 13, Training Loss: 0.7996904983941246\n",
      "Epoch 14, Training Loss: 0.8000772549825557\n",
      "Epoch 15, Training Loss: 0.7983200978531557\n",
      "Epoch 16, Training Loss: 0.7981278451050029\n",
      "Epoch 17, Training Loss: 0.797550209199681\n",
      "Epoch 18, Training Loss: 0.7966498034841875\n",
      "Epoch 19, Training Loss: 0.7956226532599505\n",
      "Epoch 20, Training Loss: 0.7955883969278896\n",
      "Epoch 21, Training Loss: 0.7945784582110013\n",
      "Epoch 22, Training Loss: 0.7938397573022281\n",
      "Epoch 23, Training Loss: 0.7926078656140496\n",
      "Epoch 24, Training Loss: 0.7915406081255745\n",
      "Epoch 25, Training Loss: 0.7912633131532109\n",
      "Epoch 26, Training Loss: 0.7902182344128104\n",
      "Epoch 27, Training Loss: 0.7889404656606562\n",
      "Epoch 28, Training Loss: 0.7886991233685438\n",
      "Epoch 29, Training Loss: 0.7878695471146527\n",
      "Epoch 30, Training Loss: 0.7877291342791389\n",
      "Epoch 31, Training Loss: 0.7868250315329608\n",
      "Epoch 32, Training Loss: 0.7865884966710035\n",
      "Epoch 33, Training Loss: 0.7864289409272811\n",
      "Epoch 34, Training Loss: 0.7860350437725292\n",
      "Epoch 35, Training Loss: 0.7859982581699596\n",
      "Epoch 36, Training Loss: 0.7855740653066074\n",
      "Epoch 37, Training Loss: 0.7851475052973803\n",
      "Epoch 38, Training Loss: 0.7848764690230875\n",
      "Epoch 39, Training Loss: 0.7845489952844732\n",
      "Epoch 40, Training Loss: 0.7844932041448706\n",
      "Epoch 41, Training Loss: 0.7843947900744046\n",
      "Epoch 42, Training Loss: 0.78440024544211\n",
      "Epoch 43, Training Loss: 0.7846960477267995\n",
      "Epoch 44, Training Loss: 0.7841268232289482\n",
      "Epoch 45, Training Loss: 0.7845541166557986\n",
      "Epoch 46, Training Loss: 0.7840443792763878\n",
      "Epoch 47, Training Loss: 0.7837579145151026\n",
      "Epoch 48, Training Loss: 0.7837659282544079\n",
      "Epoch 49, Training Loss: 0.7836648377951454\n",
      "Epoch 50, Training Loss: 0.7834832641657661\n",
      "Epoch 51, Training Loss: 0.7834909776379081\n",
      "Epoch 52, Training Loss: 0.7836134717744939\n",
      "Epoch 53, Training Loss: 0.7827834367752076\n",
      "Epoch 54, Training Loss: 0.7833952020196354\n",
      "Epoch 55, Training Loss: 0.7830238748297972\n",
      "Epoch 56, Training Loss: 0.7830962228073793\n",
      "Epoch 57, Training Loss: 0.7828958542907939\n",
      "Epoch 58, Training Loss: 0.7828423206245197\n",
      "Epoch 59, Training Loss: 0.7831358815641964\n",
      "Epoch 60, Training Loss: 0.7830172703546636\n",
      "Epoch 61, Training Loss: 0.7830442419472863\n",
      "Epoch 62, Training Loss: 0.7824909384811626\n",
      "Epoch 63, Training Loss: 0.7828284609317779\n",
      "Epoch 64, Training Loss: 0.7822025379012613\n",
      "Epoch 65, Training Loss: 0.7824440116742079\n",
      "Epoch 66, Training Loss: 0.7824289112932542\n",
      "Epoch 67, Training Loss: 0.7825517599021687\n",
      "Epoch 68, Training Loss: 0.7825215111059302\n",
      "Epoch 69, Training Loss: 0.782078966813929\n",
      "Epoch 70, Training Loss: 0.7824210535077488\n",
      "Epoch 71, Training Loss: 0.7828272512379815\n",
      "Epoch 72, Training Loss: 0.7819777105135076\n",
      "Epoch 73, Training Loss: 0.7821451661867254\n",
      "Epoch 74, Training Loss: 0.7819504956638111\n",
      "Epoch 75, Training Loss: 0.782112252852496\n",
      "Epoch 76, Training Loss: 0.7819110574441798\n",
      "Epoch 77, Training Loss: 0.7819612245699938\n",
      "Epoch 78, Training Loss: 0.7819900528122397\n",
      "Epoch 79, Training Loss: 0.782090334892273\n",
      "Epoch 80, Training Loss: 0.7817702392269583\n",
      "Epoch 81, Training Loss: 0.7817670390886419\n",
      "Epoch 82, Training Loss: 0.7819824971170987\n",
      "Epoch 83, Training Loss: 0.7811703944206237\n",
      "Epoch 84, Training Loss: 0.781595374696395\n",
      "Epoch 85, Training Loss: 0.7815514250362621\n",
      "Epoch 86, Training Loss: 0.7816986443715938\n",
      "Epoch 87, Training Loss: 0.7818374315430137\n",
      "Epoch 88, Training Loss: 0.7811701068457435\n",
      "Epoch 89, Training Loss: 0.7812733937011046\n",
      "Epoch 90, Training Loss: 0.7812448840281543\n",
      "Epoch 91, Training Loss: 0.7815758763341343\n",
      "Epoch 92, Training Loss: 0.7814525841264164\n",
      "Epoch 93, Training Loss: 0.7811442590461057\n",
      "Epoch 94, Training Loss: 0.7811976476977853\n",
      "Epoch 95, Training Loss: 0.7811651578370262\n",
      "Epoch 96, Training Loss: 0.7813450503349304\n",
      "Epoch 97, Training Loss: 0.7810186265496647\n",
      "Epoch 98, Training Loss: 0.7813703937390272\n",
      "Epoch 99, Training Loss: 0.7811872219338136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 17:59:59,615] Trial 326 finished with value: 0.6397333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7810805962366216\n",
      "Epoch 1, Training Loss: 0.9046675254527787\n",
      "Epoch 2, Training Loss: 0.8223672818420524\n",
      "Epoch 3, Training Loss: 0.8155636052439984\n",
      "Epoch 4, Training Loss: 0.8128196076342934\n",
      "Epoch 5, Training Loss: 0.8099301696719979\n",
      "Epoch 6, Training Loss: 0.8113565118689287\n",
      "Epoch 7, Training Loss: 0.8078729282644458\n",
      "Epoch 8, Training Loss: 0.8067308440244287\n",
      "Epoch 9, Training Loss: 0.806576342959153\n",
      "Epoch 10, Training Loss: 0.8073866455178512\n",
      "Epoch 11, Training Loss: 0.8039210787393096\n",
      "Epoch 12, Training Loss: 0.8054461662034343\n",
      "Epoch 13, Training Loss: 0.8033474327030038\n",
      "Epoch 14, Training Loss: 0.804657108532755\n",
      "Epoch 15, Training Loss: 0.8029271882279475\n",
      "Epoch 16, Training Loss: 0.8028735235221404\n",
      "Epoch 17, Training Loss: 0.802810531809814\n",
      "Epoch 18, Training Loss: 0.8027280256264192\n",
      "Epoch 19, Training Loss: 0.8027993946147144\n",
      "Epoch 20, Training Loss: 0.8028776677031266\n",
      "Epoch 21, Training Loss: 0.8026028453855586\n",
      "Epoch 22, Training Loss: 0.8028649792635352\n",
      "Epoch 23, Training Loss: 0.8023793594281476\n",
      "Epoch 24, Training Loss: 0.802784185839775\n",
      "Epoch 25, Training Loss: 0.8016518059081601\n",
      "Epoch 26, Training Loss: 0.8021136538426679\n",
      "Epoch 27, Training Loss: 0.8020808742458659\n",
      "Epoch 28, Training Loss: 0.8020179133666189\n",
      "Epoch 29, Training Loss: 0.801433155142275\n",
      "Epoch 30, Training Loss: 0.802556510258438\n",
      "Epoch 31, Training Loss: 0.7997091716393492\n",
      "Epoch 32, Training Loss: 0.8004396594556651\n",
      "Epoch 33, Training Loss: 0.8000378784380461\n",
      "Epoch 34, Training Loss: 0.8004946180752346\n",
      "Epoch 35, Training Loss: 0.800723307652581\n",
      "Epoch 36, Training Loss: 0.799610027514006\n",
      "Epoch 37, Training Loss: 0.8000529332268507\n",
      "Epoch 38, Training Loss: 0.8003800881536384\n",
      "Epoch 39, Training Loss: 0.7992842218481508\n",
      "Epoch 40, Training Loss: 0.7990831895878441\n",
      "Epoch 41, Training Loss: 0.7997398540489655\n",
      "Epoch 42, Training Loss: 0.7985511133545323\n",
      "Epoch 43, Training Loss: 0.8000901971544538\n",
      "Epoch 44, Training Loss: 0.799626628527964\n",
      "Epoch 45, Training Loss: 0.8005651434561364\n",
      "Epoch 46, Training Loss: 0.7985345637887941\n",
      "Epoch 47, Training Loss: 0.7992728653707002\n",
      "Epoch 48, Training Loss: 0.7987058739913138\n",
      "Epoch 49, Training Loss: 0.7985172517317578\n",
      "Epoch 50, Training Loss: 0.7985397902646459\n",
      "Epoch 51, Training Loss: 0.798348811873816\n",
      "Epoch 52, Training Loss: 0.7980084666632172\n",
      "Epoch 53, Training Loss: 0.7972899085596988\n",
      "Epoch 54, Training Loss: 0.7977849041160784\n",
      "Epoch 55, Training Loss: 0.7968836084344333\n",
      "Epoch 56, Training Loss: 0.7983419993766269\n",
      "Epoch 57, Training Loss: 0.7989850141948327\n",
      "Epoch 58, Training Loss: 0.7967359589454823\n",
      "Epoch 59, Training Loss: 0.797696450419892\n",
      "Epoch 60, Training Loss: 0.7960051562553062\n",
      "Epoch 61, Training Loss: 0.7964251585472796\n",
      "Epoch 62, Training Loss: 0.7962648921443107\n",
      "Epoch 63, Training Loss: 0.79617844255347\n",
      "Epoch 64, Training Loss: 0.7976003386920556\n",
      "Epoch 65, Training Loss: 0.7969418411864374\n",
      "Epoch 66, Training Loss: 0.7967101576633023\n",
      "Epoch 67, Training Loss: 0.7966011964288869\n",
      "Epoch 68, Training Loss: 0.7959352459226335\n",
      "Epoch 69, Training Loss: 0.7956211006731019\n",
      "Epoch 70, Training Loss: 0.7959815197421196\n",
      "Epoch 71, Training Loss: 0.7952517700374575\n",
      "Epoch 72, Training Loss: 0.7956519959564495\n",
      "Epoch 73, Training Loss: 0.7948667508318908\n",
      "Epoch 74, Training Loss: 0.7947155671908444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:01:14,303] Trial 327 finished with value: 0.6342666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7956765324549567\n",
      "Epoch 1, Training Loss: 0.8605840503721309\n",
      "Epoch 2, Training Loss: 0.8163981851778532\n",
      "Epoch 3, Training Loss: 0.8098882393729417\n",
      "Epoch 4, Training Loss: 0.8064602462868942\n",
      "Epoch 5, Training Loss: 0.8043604969082022\n",
      "Epoch 6, Training Loss: 0.8025655006107532\n",
      "Epoch 7, Training Loss: 0.7999081869322554\n",
      "Epoch 8, Training Loss: 0.8007161728421548\n",
      "Epoch 9, Training Loss: 0.7991172630087774\n",
      "Epoch 10, Training Loss: 0.7975517878855082\n",
      "Epoch 11, Training Loss: 0.7977694169919294\n",
      "Epoch 12, Training Loss: 0.7961025511859952\n",
      "Epoch 13, Training Loss: 0.7961330673748389\n",
      "Epoch 14, Training Loss: 0.7950062266866067\n",
      "Epoch 15, Training Loss: 0.7954750806765449\n",
      "Epoch 16, Training Loss: 0.7952369261505012\n",
      "Epoch 17, Training Loss: 0.7940352288403906\n",
      "Epoch 18, Training Loss: 0.7944986728797282\n",
      "Epoch 19, Training Loss: 0.7949603144387554\n",
      "Epoch 20, Training Loss: 0.7936832337451161\n",
      "Epoch 21, Training Loss: 0.7938840714612402\n",
      "Epoch 22, Training Loss: 0.7936133561277748\n",
      "Epoch 23, Training Loss: 0.793720799639709\n",
      "Epoch 24, Training Loss: 0.7923104448426038\n",
      "Epoch 25, Training Loss: 0.7931119366696007\n",
      "Epoch 26, Training Loss: 0.7927003798628212\n",
      "Epoch 27, Training Loss: 0.792710148391867\n",
      "Epoch 28, Training Loss: 0.7930882769419735\n",
      "Epoch 29, Training Loss: 0.7922759280168921\n",
      "Epoch 30, Training Loss: 0.7914974027110222\n",
      "Epoch 31, Training Loss: 0.7927015599451567\n",
      "Epoch 32, Training Loss: 0.7909915013420851\n",
      "Epoch 33, Training Loss: 0.7919396926585893\n",
      "Epoch 34, Training Loss: 0.7911696303159671\n",
      "Epoch 35, Training Loss: 0.7916807131659716\n",
      "Epoch 36, Training Loss: 0.7911242836400082\n",
      "Epoch 37, Training Loss: 0.790872081329948\n",
      "Epoch 38, Training Loss: 0.7918408756865595\n",
      "Epoch 39, Training Loss: 0.7917008619559439\n",
      "Epoch 40, Training Loss: 0.791128547030284\n",
      "Epoch 41, Training Loss: 0.7912847316354737\n",
      "Epoch 42, Training Loss: 0.7904230198465791\n",
      "Epoch 43, Training Loss: 0.7904766432324747\n",
      "Epoch 44, Training Loss: 0.7910401289624379\n",
      "Epoch 45, Training Loss: 0.7903501925611854\n",
      "Epoch 46, Training Loss: 0.7904375282445348\n",
      "Epoch 47, Training Loss: 0.7899092940459574\n",
      "Epoch 48, Training Loss: 0.79029444330617\n",
      "Epoch 49, Training Loss: 0.7906895211764744\n",
      "Epoch 50, Training Loss: 0.7899573720487437\n",
      "Epoch 51, Training Loss: 0.7899053439161832\n",
      "Epoch 52, Training Loss: 0.7901868329908615\n",
      "Epoch 53, Training Loss: 0.7903999048964422\n",
      "Epoch 54, Training Loss: 0.791301165039378\n",
      "Epoch 55, Training Loss: 0.7902683491097358\n",
      "Epoch 56, Training Loss: 0.7900128382489198\n",
      "Epoch 57, Training Loss: 0.7896573652002148\n",
      "Epoch 58, Training Loss: 0.7906362083621491\n",
      "Epoch 59, Training Loss: 0.790169578476956\n",
      "Epoch 60, Training Loss: 0.7895855134591124\n",
      "Epoch 61, Training Loss: 0.7888295697090321\n",
      "Epoch 62, Training Loss: 0.789654266834259\n",
      "Epoch 63, Training Loss: 0.7896418674547869\n",
      "Epoch 64, Training Loss: 0.7897924963692974\n",
      "Epoch 65, Training Loss: 0.7889013388999423\n",
      "Epoch 66, Training Loss: 0.7893402763775417\n",
      "Epoch 67, Training Loss: 0.789981248414606\n",
      "Epoch 68, Training Loss: 0.78968358129487\n",
      "Epoch 69, Training Loss: 0.7899256367432443\n",
      "Epoch 70, Training Loss: 0.7894341452677447\n",
      "Epoch 71, Training Loss: 0.7885751580833492\n",
      "Epoch 72, Training Loss: 0.7891649158377396\n",
      "Epoch 73, Training Loss: 0.7890337508423884\n",
      "Epoch 74, Training Loss: 0.788335100600594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:02:38,842] Trial 328 finished with value: 0.6093333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7894074690073056\n",
      "Epoch 1, Training Loss: 1.0393672079899732\n",
      "Epoch 2, Training Loss: 0.9789590354526744\n",
      "Epoch 3, Training Loss: 0.9629489542456234\n",
      "Epoch 4, Training Loss: 0.9581379760714138\n",
      "Epoch 5, Training Loss: 0.95542249286876\n",
      "Epoch 6, Training Loss: 0.9529041999929091\n",
      "Epoch 7, Training Loss: 0.95056633142864\n",
      "Epoch 8, Training Loss: 0.9479562999921687\n",
      "Epoch 9, Training Loss: 0.9452970672354979\n",
      "Epoch 10, Training Loss: 0.9425936032042784\n",
      "Epoch 11, Training Loss: 0.9397348192860099\n",
      "Epoch 12, Training Loss: 0.9366823202020982\n",
      "Epoch 13, Training Loss: 0.9333161155616536\n",
      "Epoch 14, Training Loss: 0.9298813941899468\n",
      "Epoch 15, Training Loss: 0.9262226776515736\n",
      "Epoch 16, Training Loss: 0.9225251552637885\n",
      "Epoch 17, Training Loss: 0.9186435109727523\n",
      "Epoch 18, Training Loss: 0.914480066650054\n",
      "Epoch 19, Training Loss: 0.9100280697205487\n",
      "Epoch 20, Training Loss: 0.9054968137600843\n",
      "Epoch 21, Training Loss: 0.9007800429708818\n",
      "Epoch 22, Training Loss: 0.8960062277317047\n",
      "Epoch 23, Training Loss: 0.8912450562505161\n",
      "Epoch 24, Training Loss: 0.8862911248908324\n",
      "Epoch 25, Training Loss: 0.8813736669456258\n",
      "Epoch 26, Training Loss: 0.8765293223016403\n",
      "Epoch 27, Training Loss: 0.8719675880319931\n",
      "Epoch 28, Training Loss: 0.86733290644253\n",
      "Epoch 29, Training Loss: 0.8627196323871612\n",
      "Epoch 30, Training Loss: 0.8585619768675636\n",
      "Epoch 31, Training Loss: 0.8544743202714359\n",
      "Epoch 32, Training Loss: 0.8507470628794502\n",
      "Epoch 33, Training Loss: 0.8471835250714246\n",
      "Epoch 34, Training Loss: 0.8438668582719915\n",
      "Epoch 35, Training Loss: 0.8406690232192768\n",
      "Epoch 36, Training Loss: 0.8380013936407426\n",
      "Epoch 37, Training Loss: 0.8354438715822556\n",
      "Epoch 38, Training Loss: 0.8330027707184062\n",
      "Epoch 39, Training Loss: 0.8308173077246722\n",
      "Epoch 40, Training Loss: 0.8289988751972422\n",
      "Epoch 41, Training Loss: 0.8271380594197442\n",
      "Epoch 42, Training Loss: 0.8256278594802408\n",
      "Epoch 43, Training Loss: 0.8241317413834964\n",
      "Epoch 44, Training Loss: 0.8228967715712154\n",
      "Epoch 45, Training Loss: 0.8219102911388173\n",
      "Epoch 46, Training Loss: 0.8209415470852571\n",
      "Epoch 47, Training Loss: 0.8198118524691638\n",
      "Epoch 48, Training Loss: 0.819163484433118\n",
      "Epoch 49, Training Loss: 0.8184197701426114\n",
      "Epoch 50, Training Loss: 0.8176581450069652\n",
      "Epoch 51, Training Loss: 0.8171090490677777\n",
      "Epoch 52, Training Loss: 0.8166700905912063\n",
      "Epoch 53, Training Loss: 0.8160569082288182\n",
      "Epoch 54, Training Loss: 0.8156861128526576\n",
      "Epoch 55, Training Loss: 0.8153633903755861\n",
      "Epoch 56, Training Loss: 0.8149081932797152\n",
      "Epoch 57, Training Loss: 0.814585518766852\n",
      "Epoch 58, Training Loss: 0.8142875759040609\n",
      "Epoch 59, Training Loss: 0.8139298618541044\n",
      "Epoch 60, Training Loss: 0.813707609316882\n",
      "Epoch 61, Training Loss: 0.8134660936103147\n",
      "Epoch 62, Training Loss: 0.8132616255563848\n",
      "Epoch 63, Training Loss: 0.8130207302991082\n",
      "Epoch 64, Training Loss: 0.8128840124607086\n",
      "Epoch 65, Training Loss: 0.812727882791968\n",
      "Epoch 66, Training Loss: 0.8124755205827601\n",
      "Epoch 67, Training Loss: 0.8123501183706171\n",
      "Epoch 68, Training Loss: 0.8119657756300533\n",
      "Epoch 69, Training Loss: 0.811968838186825\n",
      "Epoch 70, Training Loss: 0.8118187507461099\n",
      "Epoch 71, Training Loss: 0.8116580911944894\n",
      "Epoch 72, Training Loss: 0.8113742850107305\n",
      "Epoch 73, Training Loss: 0.8114939219811383\n",
      "Epoch 74, Training Loss: 0.8112707403828116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:03:48,948] Trial 329 finished with value: 0.6263333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8110969425650204\n",
      "Epoch 1, Training Loss: 0.9800206277603494\n",
      "Epoch 2, Training Loss: 0.9434089793298478\n",
      "Epoch 3, Training Loss: 0.9142598393268155\n",
      "Epoch 4, Training Loss: 0.8728619485869443\n",
      "Epoch 5, Training Loss: 0.8380838278540992\n",
      "Epoch 6, Training Loss: 0.8212394223177344\n",
      "Epoch 7, Training Loss: 0.8151596323888105\n",
      "Epoch 8, Training Loss: 0.8135041979918802\n",
      "Epoch 9, Training Loss: 0.8119356763990302\n",
      "Epoch 10, Training Loss: 0.8104186487377139\n",
      "Epoch 11, Training Loss: 0.809380134604031\n",
      "Epoch 12, Training Loss: 0.8084400962169905\n",
      "Epoch 13, Training Loss: 0.808085606869002\n",
      "Epoch 14, Training Loss: 0.8075556140196951\n",
      "Epoch 15, Training Loss: 0.8061152137311778\n",
      "Epoch 16, Training Loss: 0.8052457934931705\n",
      "Epoch 17, Training Loss: 0.8053100135093345\n",
      "Epoch 18, Training Loss: 0.8043852182259237\n",
      "Epoch 19, Training Loss: 0.80385326021596\n",
      "Epoch 20, Training Loss: 0.8036506725433178\n",
      "Epoch 21, Training Loss: 0.8028698162476819\n",
      "Epoch 22, Training Loss: 0.8030380368232727\n",
      "Epoch 23, Training Loss: 0.8028124526031035\n",
      "Epoch 24, Training Loss: 0.8023021709650082\n",
      "Epoch 25, Training Loss: 0.8023136461587776\n",
      "Epoch 26, Training Loss: 0.8020322226940241\n",
      "Epoch 27, Training Loss: 0.8024143310417806\n",
      "Epoch 28, Training Loss: 0.8017633451555009\n",
      "Epoch 29, Training Loss: 0.8021964025676699\n",
      "Epoch 30, Training Loss: 0.8016913254458206\n",
      "Epoch 31, Training Loss: 0.8012378652292983\n",
      "Epoch 32, Training Loss: 0.8008371148790632\n",
      "Epoch 33, Training Loss: 0.8005887977162698\n",
      "Epoch 34, Training Loss: 0.8019072024445785\n",
      "Epoch 35, Training Loss: 0.8010307260025713\n",
      "Epoch 36, Training Loss: 0.8010404025701652\n",
      "Epoch 37, Training Loss: 0.8007994182127759\n",
      "Epoch 38, Training Loss: 0.8016125928190417\n",
      "Epoch 39, Training Loss: 0.8011909469626004\n",
      "Epoch 40, Training Loss: 0.8001032384714686\n",
      "Epoch 41, Training Loss: 0.8002205215898671\n",
      "Epoch 42, Training Loss: 0.8001240476629787\n",
      "Epoch 43, Training Loss: 0.8000892464379619\n",
      "Epoch 44, Training Loss: 0.799669261086256\n",
      "Epoch 45, Training Loss: 0.7992612613771195\n",
      "Epoch 46, Training Loss: 0.8001428786076997\n",
      "Epoch 47, Training Loss: 0.7990508209493824\n",
      "Epoch 48, Training Loss: 0.7989054732752922\n",
      "Epoch 49, Training Loss: 0.7984108011973532\n",
      "Epoch 50, Training Loss: 0.7987553787410707\n",
      "Epoch 51, Training Loss: 0.7991677015347588\n",
      "Epoch 52, Training Loss: 0.7990727525008352\n",
      "Epoch 53, Training Loss: 0.7985437573346876\n",
      "Epoch 54, Training Loss: 0.7985161602945257\n",
      "Epoch 55, Training Loss: 0.7990279210241218\n",
      "Epoch 56, Training Loss: 0.7985822077084305\n",
      "Epoch 57, Training Loss: 0.7981450057567511\n",
      "Epoch 58, Training Loss: 0.7984222075096646\n",
      "Epoch 59, Training Loss: 0.7986325531973875\n",
      "Epoch 60, Training Loss: 0.7978195336528291\n",
      "Epoch 61, Training Loss: 0.797829922966491\n",
      "Epoch 62, Training Loss: 0.7982048459519121\n",
      "Epoch 63, Training Loss: 0.7975265917025114\n",
      "Epoch 64, Training Loss: 0.7978638720691652\n",
      "Epoch 65, Training Loss: 0.7979447362118197\n",
      "Epoch 66, Training Loss: 0.7980541186225145\n",
      "Epoch 67, Training Loss: 0.797577546861835\n",
      "Epoch 68, Training Loss: 0.7975124714966108\n",
      "Epoch 69, Training Loss: 0.7970670213376669\n",
      "Epoch 70, Training Loss: 0.7969347133224172\n",
      "Epoch 71, Training Loss: 0.7983343739258616\n",
      "Epoch 72, Training Loss: 0.7968359908663241\n",
      "Epoch 73, Training Loss: 0.7967545683670761\n",
      "Epoch 74, Training Loss: 0.7965656147415476\n",
      "Epoch 75, Training Loss: 0.796919972824871\n",
      "Epoch 76, Training Loss: 0.7971883302344416\n",
      "Epoch 77, Training Loss: 0.7967523714653532\n",
      "Epoch 78, Training Loss: 0.7965799839873063\n",
      "Epoch 79, Training Loss: 0.7970916370700176\n",
      "Epoch 80, Training Loss: 0.7964115540784105\n",
      "Epoch 81, Training Loss: 0.7966478566478069\n",
      "Epoch 82, Training Loss: 0.7963066189808953\n",
      "Epoch 83, Training Loss: 0.7971649232663607\n",
      "Epoch 84, Training Loss: 0.7958433066991936\n",
      "Epoch 85, Training Loss: 0.796325127164224\n",
      "Epoch 86, Training Loss: 0.7958997362538388\n",
      "Epoch 87, Training Loss: 0.7963011229844917\n",
      "Epoch 88, Training Loss: 0.79562199958285\n",
      "Epoch 89, Training Loss: 0.7962539292815932\n",
      "Epoch 90, Training Loss: 0.795464147123179\n",
      "Epoch 91, Training Loss: 0.7960168781137108\n",
      "Epoch 92, Training Loss: 0.7964109539985657\n",
      "Epoch 93, Training Loss: 0.7952284869394805\n",
      "Epoch 94, Training Loss: 0.7951184399145886\n",
      "Epoch 95, Training Loss: 0.7946584343910217\n",
      "Epoch 96, Training Loss: 0.7950004376863179\n",
      "Epoch 97, Training Loss: 0.7950741213067134\n",
      "Epoch 98, Training Loss: 0.794580882742889\n",
      "Epoch 99, Training Loss: 0.7943263767357159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:05:41,230] Trial 330 finished with value: 0.6258 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.795027550449945\n",
      "Epoch 1, Training Loss: 0.9672853671803194\n",
      "Epoch 2, Training Loss: 0.9253531399193932\n",
      "Epoch 3, Training Loss: 0.8882156920433044\n",
      "Epoch 4, Training Loss: 0.8486309915430406\n",
      "Epoch 5, Training Loss: 0.8255923527829787\n",
      "Epoch 6, Training Loss: 0.8171373633777395\n",
      "Epoch 7, Training Loss: 0.8138469806138207\n",
      "Epoch 8, Training Loss: 0.8115797932007733\n",
      "Epoch 9, Training Loss: 0.8100621125277351\n",
      "Epoch 10, Training Loss: 0.8088577801339767\n",
      "Epoch 11, Training Loss: 0.8074320109451518\n",
      "Epoch 12, Training Loss: 0.8065057413017048\n",
      "Epoch 13, Training Loss: 0.8054070754612194\n",
      "Epoch 14, Training Loss: 0.8049912787184996\n",
      "Epoch 15, Training Loss: 0.8043492679736194\n",
      "Epoch 16, Training Loss: 0.8035500835671144\n",
      "Epoch 17, Training Loss: 0.8033792682255015\n",
      "Epoch 18, Training Loss: 0.8024426906249102\n",
      "Epoch 19, Training Loss: 0.8022235875971178\n",
      "Epoch 20, Training Loss: 0.8016563427448272\n",
      "Epoch 21, Training Loss: 0.8010206520557404\n",
      "Epoch 22, Training Loss: 0.8011698868695427\n",
      "Epoch 23, Training Loss: 0.8009592696498422\n",
      "Epoch 24, Training Loss: 0.8007568695264704\n",
      "Epoch 25, Training Loss: 0.8002599265996148\n",
      "Epoch 26, Training Loss: 0.8002177221634809\n",
      "Epoch 27, Training Loss: 0.7996766159113715\n",
      "Epoch 28, Training Loss: 0.7996954995043137\n",
      "Epoch 29, Training Loss: 0.7994418298496919\n",
      "Epoch 30, Training Loss: 0.7993918662912706\n",
      "Epoch 31, Training Loss: 0.799134617132299\n",
      "Epoch 32, Training Loss: 0.7988499054488014\n",
      "Epoch 33, Training Loss: 0.7987847190744737\n",
      "Epoch 34, Training Loss: 0.7987337451822617\n",
      "Epoch 35, Training Loss: 0.7987988024599412\n",
      "Epoch 36, Training Loss: 0.7984661958498114\n",
      "Epoch 37, Training Loss: 0.7980377195863163\n",
      "Epoch 38, Training Loss: 0.7982218323034399\n",
      "Epoch 39, Training Loss: 0.7979072939648347\n",
      "Epoch 40, Training Loss: 0.7978889907808865\n",
      "Epoch 41, Training Loss: 0.7976710436624639\n",
      "Epoch 42, Training Loss: 0.7975825027157278\n",
      "Epoch 43, Training Loss: 0.7975165485634523\n",
      "Epoch 44, Training Loss: 0.7968488598571104\n",
      "Epoch 45, Training Loss: 0.7968714223188512\n",
      "Epoch 46, Training Loss: 0.7967080242493574\n",
      "Epoch 47, Training Loss: 0.7964250988118788\n",
      "Epoch 48, Training Loss: 0.7966212985796087\n",
      "Epoch 49, Training Loss: 0.7962643351975609\n",
      "Epoch 50, Training Loss: 0.7958227918428533\n",
      "Epoch 51, Training Loss: 0.7958157378084519\n",
      "Epoch 52, Training Loss: 0.7957279025105869\n",
      "Epoch 53, Training Loss: 0.7953577467273263\n",
      "Epoch 54, Training Loss: 0.7953707096857183\n",
      "Epoch 55, Training Loss: 0.7949652261593763\n",
      "Epoch 56, Training Loss: 0.7948544296797584\n",
      "Epoch 57, Training Loss: 0.7948842392248265\n",
      "Epoch 58, Training Loss: 0.7945537293658537\n",
      "Epoch 59, Training Loss: 0.7945124837931464\n",
      "Epoch 60, Training Loss: 0.7940416181788725\n",
      "Epoch 61, Training Loss: 0.793579754408668\n",
      "Epoch 62, Training Loss: 0.7936813393059898\n",
      "Epoch 63, Training Loss: 0.7931652099945966\n",
      "Epoch 64, Training Loss: 0.7932202750795028\n",
      "Epoch 65, Training Loss: 0.7931915699734408\n",
      "Epoch 66, Training Loss: 0.7927328321513007\n",
      "Epoch 67, Training Loss: 0.7924525145222159\n",
      "Epoch 68, Training Loss: 0.7923106662666096\n",
      "Epoch 69, Training Loss: 0.7918230485916138\n",
      "Epoch 70, Training Loss: 0.7917245099825018\n",
      "Epoch 71, Training Loss: 0.7915439928279203\n",
      "Epoch 72, Training Loss: 0.7912229781291064\n",
      "Epoch 73, Training Loss: 0.7906976896173814\n",
      "Epoch 74, Training Loss: 0.7908096430582159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:07:04,263] Trial 331 finished with value: 0.633 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7904849750855389\n",
      "Epoch 1, Training Loss: 1.081154976034523\n",
      "Epoch 2, Training Loss: 1.0305805465332547\n",
      "Epoch 3, Training Loss: 1.0038894083266867\n",
      "Epoch 4, Training Loss: 0.9882021027400081\n",
      "Epoch 5, Training Loss: 0.9779750428701702\n",
      "Epoch 6, Training Loss: 0.9711169540433955\n",
      "Epoch 7, Training Loss: 0.9666616232771622\n",
      "Epoch 8, Training Loss: 0.9630833806848167\n",
      "Epoch 9, Training Loss: 0.9603384996715345\n",
      "Epoch 10, Training Loss: 0.9587647482864838\n",
      "Epoch 11, Training Loss: 0.9569965430668422\n",
      "Epoch 12, Training Loss: 0.9559357265780742\n",
      "Epoch 13, Training Loss: 0.9544734710141232\n",
      "Epoch 14, Training Loss: 0.9530006005351704\n",
      "Epoch 15, Training Loss: 0.9520093570078226\n",
      "Epoch 16, Training Loss: 0.9511437439380732\n",
      "Epoch 17, Training Loss: 0.9508258912796365\n",
      "Epoch 18, Training Loss: 0.9494389582397347\n",
      "Epoch 19, Training Loss: 0.9487769244308758\n",
      "Epoch 20, Training Loss: 0.9475709879308715\n",
      "Epoch 21, Training Loss: 0.9466862812974399\n",
      "Epoch 22, Training Loss: 0.9457245953997275\n",
      "Epoch 23, Training Loss: 0.9454594445407839\n",
      "Epoch 24, Training Loss: 0.94417456773887\n",
      "Epoch 25, Training Loss: 0.9433742627165371\n",
      "Epoch 26, Training Loss: 0.9432545816987977\n",
      "Epoch 27, Training Loss: 0.9416124515963676\n",
      "Epoch 28, Training Loss: 0.9413964981423285\n",
      "Epoch 29, Training Loss: 0.9404762577293511\n",
      "Epoch 30, Training Loss: 0.939590012758298\n",
      "Epoch 31, Training Loss: 0.9385938573600654\n",
      "Epoch 32, Training Loss: 0.9379080602997227\n",
      "Epoch 33, Training Loss: 0.9372399079172234\n",
      "Epoch 34, Training Loss: 0.9363328455982352\n",
      "Epoch 35, Training Loss: 0.9356036836043337\n",
      "Epoch 36, Training Loss: 0.9352178699988172\n",
      "Epoch 37, Training Loss: 0.9340894777075689\n",
      "Epoch 38, Training Loss: 0.9330287841925944\n",
      "Epoch 39, Training Loss: 0.9324494568028845\n",
      "Epoch 40, Training Loss: 0.9317324710071535\n",
      "Epoch 41, Training Loss: 0.9306728438327186\n",
      "Epoch 42, Training Loss: 0.9303567040235476\n",
      "Epoch 43, Training Loss: 0.929648949149856\n",
      "Epoch 44, Training Loss: 0.9287267782634362\n",
      "Epoch 45, Training Loss: 0.9279568527874194\n",
      "Epoch 46, Training Loss: 0.9271756532496975\n",
      "Epoch 47, Training Loss: 0.9262330693409855\n",
      "Epoch 48, Training Loss: 0.9255273961483088\n",
      "Epoch 49, Training Loss: 0.9246767848954165\n",
      "Epoch 50, Training Loss: 0.9237829945141212\n",
      "Epoch 51, Training Loss: 0.9232753349426097\n",
      "Epoch 52, Training Loss: 0.9223440852380337\n",
      "Epoch 53, Training Loss: 0.9216595067117447\n",
      "Epoch 54, Training Loss: 0.9209152836548654\n",
      "Epoch 55, Training Loss: 0.9198514411323949\n",
      "Epoch 56, Training Loss: 0.9190355049936395\n",
      "Epoch 57, Training Loss: 0.9186797998005286\n",
      "Epoch 58, Training Loss: 0.918158943850295\n",
      "Epoch 59, Training Loss: 0.9169801029047572\n",
      "Epoch 60, Training Loss: 0.916584729162374\n",
      "Epoch 61, Training Loss: 0.9157446545765813\n",
      "Epoch 62, Training Loss: 0.9148166881468063\n",
      "Epoch 63, Training Loss: 0.9139724830039462\n",
      "Epoch 64, Training Loss: 0.9133484988284291\n",
      "Epoch 65, Training Loss: 0.912630920302599\n",
      "Epoch 66, Training Loss: 0.9114594500763972\n",
      "Epoch 67, Training Loss: 0.9107781956070348\n",
      "Epoch 68, Training Loss: 0.909915356438859\n",
      "Epoch 69, Training Loss: 0.9093605368657219\n",
      "Epoch 70, Training Loss: 0.9087901361006543\n",
      "Epoch 71, Training Loss: 0.9078848806538976\n",
      "Epoch 72, Training Loss: 0.906694385611025\n",
      "Epoch 73, Training Loss: 0.9061020273911325\n",
      "Epoch 74, Training Loss: 0.9053353338313282\n",
      "Epoch 75, Training Loss: 0.9044874821390424\n",
      "Epoch 76, Training Loss: 0.9042999497033599\n",
      "Epoch 77, Training Loss: 0.903373060100957\n",
      "Epoch 78, Training Loss: 0.9025930595577212\n",
      "Epoch 79, Training Loss: 0.9013887942285466\n",
      "Epoch 80, Training Loss: 0.900905482302931\n",
      "Epoch 81, Training Loss: 0.8999925998816812\n",
      "Epoch 82, Training Loss: 0.8988004628876994\n",
      "Epoch 83, Training Loss: 0.8982630261801239\n",
      "Epoch 84, Training Loss: 0.8975209759590321\n",
      "Epoch 85, Training Loss: 0.8970294658402751\n",
      "Epoch 86, Training Loss: 0.8957664144666572\n",
      "Epoch 87, Training Loss: 0.8950611915803494\n",
      "Epoch 88, Training Loss: 0.8941153873178296\n",
      "Epoch 89, Training Loss: 0.8935646469431712\n",
      "Epoch 90, Training Loss: 0.8927482818302356\n",
      "Epoch 91, Training Loss: 0.8920840546600801\n",
      "Epoch 92, Training Loss: 0.8910426297582181\n",
      "Epoch 93, Training Loss: 0.890848037533294\n",
      "Epoch 94, Training Loss: 0.8905471286379305\n",
      "Epoch 95, Training Loss: 0.8887983875167101\n",
      "Epoch 96, Training Loss: 0.8884243101105654\n",
      "Epoch 97, Training Loss: 0.8876623543581568\n",
      "Epoch 98, Training Loss: 0.8866225676429003\n",
      "Epoch 99, Training Loss: 0.8857192593409603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:08:27,255] Trial 332 finished with value: 0.5851333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8849776855088715\n",
      "Epoch 1, Training Loss: 1.0948850208655336\n",
      "Epoch 2, Training Loss: 1.0834274729391686\n",
      "Epoch 3, Training Loss: 1.0740373030641026\n",
      "Epoch 4, Training Loss: 1.0650949060468746\n",
      "Epoch 5, Training Loss: 1.0560052832266442\n",
      "Epoch 6, Training Loss: 1.04686204661104\n",
      "Epoch 7, Training Loss: 1.0372569753711385\n",
      "Epoch 8, Training Loss: 1.027368788432358\n",
      "Epoch 9, Training Loss: 1.0171999408786458\n",
      "Epoch 10, Training Loss: 1.0075119037377207\n",
      "Epoch 11, Training Loss: 0.9980056585225844\n",
      "Epoch 12, Training Loss: 0.9892197719193939\n",
      "Epoch 13, Training Loss: 0.9812182544765616\n",
      "Epoch 14, Training Loss: 0.9737631894592056\n",
      "Epoch 15, Training Loss: 0.9673553854899299\n",
      "Epoch 16, Training Loss: 0.9620118654760202\n",
      "Epoch 17, Training Loss: 0.9573519049730516\n",
      "Epoch 18, Training Loss: 0.9535213910547414\n",
      "Epoch 19, Training Loss: 0.9501867793556443\n",
      "Epoch 20, Training Loss: 0.9477276534962474\n",
      "Epoch 21, Training Loss: 0.9453677335179838\n",
      "Epoch 22, Training Loss: 0.943064690711803\n",
      "Epoch 23, Training Loss: 0.9416471155962549\n",
      "Epoch 24, Training Loss: 0.9395409937191727\n",
      "Epoch 25, Training Loss: 0.9387052200790634\n",
      "Epoch 26, Training Loss: 0.9369215671281169\n",
      "Epoch 27, Training Loss: 0.9362046036505162\n",
      "Epoch 28, Training Loss: 0.9355589307340464\n",
      "Epoch 29, Training Loss: 0.9342005614051245\n",
      "Epoch 30, Training Loss: 0.9333039589394304\n",
      "Epoch 31, Training Loss: 0.9322675152828819\n",
      "Epoch 32, Training Loss: 0.9316034767860757\n",
      "Epoch 33, Training Loss: 0.9303297010579503\n",
      "Epoch 34, Training Loss: 0.9297821511003308\n",
      "Epoch 35, Training Loss: 0.9291274434641787\n",
      "Epoch 36, Training Loss: 0.928139839405404\n",
      "Epoch 37, Training Loss: 0.9275146798979967\n",
      "Epoch 38, Training Loss: 0.9266338642378499\n",
      "Epoch 39, Training Loss: 0.9261214202507995\n",
      "Epoch 40, Training Loss: 0.9252261277428246\n",
      "Epoch 41, Training Loss: 0.9244634440967014\n",
      "Epoch 42, Training Loss: 0.9241106875857016\n",
      "Epoch 43, Training Loss: 0.9237101587137782\n",
      "Epoch 44, Training Loss: 0.9230882447465022\n",
      "Epoch 45, Training Loss: 0.9219578404175608\n",
      "Epoch 46, Training Loss: 0.9217779976981026\n",
      "Epoch 47, Training Loss: 0.9208316806563758\n",
      "Epoch 48, Training Loss: 0.9201708813358966\n",
      "Epoch 49, Training Loss: 0.9199621651405678\n",
      "Epoch 50, Training Loss: 0.9190156864044362\n",
      "Epoch 51, Training Loss: 0.9190885532171207\n",
      "Epoch 52, Training Loss: 0.917836860785807\n",
      "Epoch 53, Training Loss: 0.9182520714917577\n",
      "Epoch 54, Training Loss: 0.9170886291597122\n",
      "Epoch 55, Training Loss: 0.9161841268826248\n",
      "Epoch 56, Training Loss: 0.9160654932036436\n",
      "Epoch 57, Training Loss: 0.9156018595946462\n",
      "Epoch 58, Training Loss: 0.9146460972334209\n",
      "Epoch 59, Training Loss: 0.9149757906906587\n",
      "Epoch 60, Training Loss: 0.9143567995917529\n",
      "Epoch 61, Training Loss: 0.9133935300031103\n",
      "Epoch 62, Training Loss: 0.9124320565309739\n",
      "Epoch 63, Training Loss: 0.9127252464007615\n",
      "Epoch 64, Training Loss: 0.9115954001147047\n",
      "Epoch 65, Training Loss: 0.9116156958995905\n",
      "Epoch 66, Training Loss: 0.9109482652262637\n",
      "Epoch 67, Training Loss: 0.910018325389776\n",
      "Epoch 68, Training Loss: 0.9098934795623436\n",
      "Epoch 69, Training Loss: 0.9088597068212982\n",
      "Epoch 70, Training Loss: 0.908910944497675\n",
      "Epoch 71, Training Loss: 0.9077822797280505\n",
      "Epoch 72, Training Loss: 0.907176641324409\n",
      "Epoch 73, Training Loss: 0.9074201965690556\n",
      "Epoch 74, Training Loss: 0.9065707914811328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:09:36,853] Trial 333 finished with value: 0.5684 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9056513203714127\n",
      "Epoch 1, Training Loss: 1.1007114837044163\n",
      "Epoch 2, Training Loss: 1.0866316784593395\n",
      "Epoch 3, Training Loss: 1.0807237662767109\n",
      "Epoch 4, Training Loss: 1.0752285276140485\n",
      "Epoch 5, Training Loss: 1.0699904377299143\n",
      "Epoch 6, Training Loss: 1.0652274205272358\n",
      "Epoch 7, Training Loss: 1.0603646054303735\n",
      "Epoch 8, Training Loss: 1.0559768302100045\n",
      "Epoch 9, Training Loss: 1.0517742162360284\n",
      "Epoch 10, Training Loss: 1.0477989953263362\n",
      "Epoch 11, Training Loss: 1.0440614798015222\n",
      "Epoch 12, Training Loss: 1.0401628494262696\n",
      "Epoch 13, Training Loss: 1.0367201266432167\n",
      "Epoch 14, Training Loss: 1.0332998533894244\n",
      "Epoch 15, Training Loss: 1.0299778553776275\n",
      "Epoch 16, Training Loss: 1.0269344734966306\n",
      "Epoch 17, Training Loss: 1.0238266688540465\n",
      "Epoch 18, Training Loss: 1.0207099648346578\n",
      "Epoch 19, Training Loss: 1.018072637938019\n",
      "Epoch 20, Training Loss: 1.0154700009446396\n",
      "Epoch 21, Training Loss: 1.0128492999793892\n",
      "Epoch 22, Training Loss: 1.010270921628278\n",
      "Epoch 23, Training Loss: 1.0077386000102624\n",
      "Epoch 24, Training Loss: 1.0053951530528247\n",
      "Epoch 25, Training Loss: 1.003310689979926\n",
      "Epoch 26, Training Loss: 1.0010191135836723\n",
      "Epoch 27, Training Loss: 0.9992575614972222\n",
      "Epoch 28, Training Loss: 0.9973115649438442\n",
      "Epoch 29, Training Loss: 0.9953907334714904\n",
      "Epoch 30, Training Loss: 0.993441409096682\n",
      "Epoch 31, Training Loss: 0.9917384485553081\n",
      "Epoch 32, Training Loss: 0.9901881701067874\n",
      "Epoch 33, Training Loss: 0.9887058760886802\n",
      "Epoch 34, Training Loss: 0.9871143364368524\n",
      "Epoch 35, Training Loss: 0.9857784215668987\n",
      "Epoch 36, Training Loss: 0.9844964142132523\n",
      "Epoch 37, Training Loss: 0.9830976482620812\n",
      "Epoch 38, Training Loss: 0.9816300180621613\n",
      "Epoch 39, Training Loss: 0.9804221848796185\n",
      "Epoch 40, Training Loss: 0.9793765795858282\n",
      "Epoch 41, Training Loss: 0.9786246002168584\n",
      "Epoch 42, Training Loss: 0.9776701296182503\n",
      "Epoch 43, Training Loss: 0.9764692615745659\n",
      "Epoch 44, Training Loss: 0.9754119385453991\n",
      "Epoch 45, Training Loss: 0.9744302718262924\n",
      "Epoch 46, Training Loss: 0.973659732825774\n",
      "Epoch 47, Training Loss: 0.9727277352397603\n",
      "Epoch 48, Training Loss: 0.9718617432099536\n",
      "Epoch 49, Training Loss: 0.9713967402178542\n",
      "Epoch 50, Training Loss: 0.9707840784151751\n",
      "Epoch 51, Training Loss: 0.9701852743786977\n",
      "Epoch 52, Training Loss: 0.9691197586238832\n",
      "Epoch 53, Training Loss: 0.968989796566784\n",
      "Epoch 54, Training Loss: 0.9679401696176457\n",
      "Epoch 55, Training Loss: 0.9676318551364698\n",
      "Epoch 56, Training Loss: 0.9670355724212819\n",
      "Epoch 57, Training Loss: 0.9665600453104292\n",
      "Epoch 58, Training Loss: 0.9658420577981418\n",
      "Epoch 59, Training Loss: 0.9655596168417679\n",
      "Epoch 60, Training Loss: 0.9649538261549814\n",
      "Epoch 61, Training Loss: 0.9648450240156704\n",
      "Epoch 62, Training Loss: 0.9642325439847501\n",
      "Epoch 63, Training Loss: 0.9643985429204496\n",
      "Epoch 64, Training Loss: 0.9637376961851478\n",
      "Epoch 65, Training Loss: 0.9636109743799482\n",
      "Epoch 66, Training Loss: 0.9628824302128383\n",
      "Epoch 67, Training Loss: 0.9631794787887343\n",
      "Epoch 68, Training Loss: 0.9625706120541221\n",
      "Epoch 69, Training Loss: 0.9620996468945553\n",
      "Epoch 70, Training Loss: 0.9619208813609933\n",
      "Epoch 71, Training Loss: 0.9616374472030124\n",
      "Epoch 72, Training Loss: 0.9612270286208705\n",
      "Epoch 73, Training Loss: 0.961256588491282\n",
      "Epoch 74, Training Loss: 0.9609053173459562\n",
      "Epoch 75, Training Loss: 0.9607793007578168\n",
      "Epoch 76, Training Loss: 0.9602026136297929\n",
      "Epoch 77, Training Loss: 0.9605152260091968\n",
      "Epoch 78, Training Loss: 0.9601607894538937\n",
      "Epoch 79, Training Loss: 0.9598094917777785\n",
      "Epoch 80, Training Loss: 0.960156912731945\n",
      "Epoch 81, Training Loss: 0.9595344407217843\n",
      "Epoch 82, Training Loss: 0.959531179048065\n",
      "Epoch 83, Training Loss: 0.9591870895902017\n",
      "Epoch 84, Training Loss: 0.9590163533848928\n",
      "Epoch 85, Training Loss: 0.9588137103202647\n",
      "Epoch 86, Training Loss: 0.9591149689559649\n",
      "Epoch 87, Training Loss: 0.9589879309324394\n",
      "Epoch 88, Training Loss: 0.9587041684559413\n",
      "Epoch 89, Training Loss: 0.9583858355543667\n",
      "Epoch 90, Training Loss: 0.9583845714877423\n",
      "Epoch 91, Training Loss: 0.958316325155416\n",
      "Epoch 92, Training Loss: 0.958201229303403\n",
      "Epoch 93, Training Loss: 0.9583570510821235\n",
      "Epoch 94, Training Loss: 0.9578239367420512\n",
      "Epoch 95, Training Loss: 0.9573702573776245\n",
      "Epoch 96, Training Loss: 0.9576905206630104\n",
      "Epoch 97, Training Loss: 0.9574106643970748\n",
      "Epoch 98, Training Loss: 0.9572995659103967\n",
      "Epoch 99, Training Loss: 0.9570511936245109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:10:58,300] Trial 334 finished with value: 0.5288 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.9571053601745376\n",
      "Epoch 1, Training Loss: 0.8873776071950009\n",
      "Epoch 2, Training Loss: 0.819421130284331\n",
      "Epoch 3, Training Loss: 0.8070907332843408\n",
      "Epoch 4, Training Loss: 0.8048501824077807\n",
      "Epoch 5, Training Loss: 0.8018958256656962\n",
      "Epoch 6, Training Loss: 0.8002659024152541\n",
      "Epoch 7, Training Loss: 0.7991432402366982\n",
      "Epoch 8, Training Loss: 0.797204089523258\n",
      "Epoch 9, Training Loss: 0.7956253138699926\n",
      "Epoch 10, Training Loss: 0.7940637768659377\n",
      "Epoch 11, Training Loss: 0.7931249398037903\n",
      "Epoch 12, Training Loss: 0.7922876171599653\n",
      "Epoch 13, Training Loss: 0.7906582651281715\n",
      "Epoch 14, Training Loss: 0.7896353177558211\n",
      "Epoch 15, Training Loss: 0.7899554145963569\n",
      "Epoch 16, Training Loss: 0.7891502236065112\n",
      "Epoch 17, Training Loss: 0.7879677800307596\n",
      "Epoch 18, Training Loss: 0.7883780789554568\n",
      "Epoch 19, Training Loss: 0.7870490283894359\n",
      "Epoch 20, Training Loss: 0.7869233175327903\n",
      "Epoch 21, Training Loss: 0.7863747268691099\n",
      "Epoch 22, Training Loss: 0.7863935867646583\n",
      "Epoch 23, Training Loss: 0.7861774674931863\n",
      "Epoch 24, Training Loss: 0.7855400510300371\n",
      "Epoch 25, Training Loss: 0.7858686009744056\n",
      "Epoch 26, Training Loss: 0.7846003552128498\n",
      "Epoch 27, Training Loss: 0.7848617902375702\n",
      "Epoch 28, Training Loss: 0.7840920158794948\n",
      "Epoch 29, Training Loss: 0.7843328449959145\n",
      "Epoch 30, Training Loss: 0.7839289165977249\n",
      "Epoch 31, Training Loss: 0.7832696505058977\n",
      "Epoch 32, Training Loss: 0.7834560489296016\n",
      "Epoch 33, Training Loss: 0.7834056561154531\n",
      "Epoch 34, Training Loss: 0.7832465596665117\n",
      "Epoch 35, Training Loss: 0.7826668652376734\n",
      "Epoch 36, Training Loss: 0.78254895371602\n",
      "Epoch 37, Training Loss: 0.7826098045012109\n",
      "Epoch 38, Training Loss: 0.7835160054658589\n",
      "Epoch 39, Training Loss: 0.782189526862668\n",
      "Epoch 40, Training Loss: 0.7817652834089179\n",
      "Epoch 41, Training Loss: 0.7815747770151698\n",
      "Epoch 42, Training Loss: 0.7818676327404223\n",
      "Epoch 43, Training Loss: 0.7819768557871195\n",
      "Epoch 44, Training Loss: 0.7812354438286975\n",
      "Epoch 45, Training Loss: 0.7816585159839544\n",
      "Epoch 46, Training Loss: 0.7813425959501051\n",
      "Epoch 47, Training Loss: 0.7813090326194476\n",
      "Epoch 48, Training Loss: 0.7816187600444134\n",
      "Epoch 49, Training Loss: 0.7810443395062496\n",
      "Epoch 50, Training Loss: 0.7819389844299259\n",
      "Epoch 51, Training Loss: 0.7809628346808871\n",
      "Epoch 52, Training Loss: 0.7802561589649746\n",
      "Epoch 53, Training Loss: 0.780722861540945\n",
      "Epoch 54, Training Loss: 0.7809589896883283\n",
      "Epoch 55, Training Loss: 0.779815585720808\n",
      "Epoch 56, Training Loss: 0.7799990760652642\n",
      "Epoch 57, Training Loss: 0.7801642633918533\n",
      "Epoch 58, Training Loss: 0.7800966988828846\n",
      "Epoch 59, Training Loss: 0.7796821444554436\n",
      "Epoch 60, Training Loss: 0.779661190330534\n",
      "Epoch 61, Training Loss: 0.7791872502717757\n",
      "Epoch 62, Training Loss: 0.7802636808022521\n",
      "Epoch 63, Training Loss: 0.7795286987957202\n",
      "Epoch 64, Training Loss: 0.7794473076225223\n",
      "Epoch 65, Training Loss: 0.7796508964739348\n",
      "Epoch 66, Training Loss: 0.7795299678816832\n",
      "Epoch 67, Training Loss: 0.7792493529785844\n",
      "Epoch 68, Training Loss: 0.779642615372077\n",
      "Epoch 69, Training Loss: 0.7792271205357143\n",
      "Epoch 70, Training Loss: 0.7791018177692155\n",
      "Epoch 71, Training Loss: 0.7788222565686792\n",
      "Epoch 72, Training Loss: 0.7795256732998038\n",
      "Epoch 73, Training Loss: 0.77889286593387\n",
      "Epoch 74, Training Loss: 0.7788506209402156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:12:22,394] Trial 335 finished with value: 0.6332 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7789040751026985\n",
      "Epoch 1, Training Loss: 1.1218822452358734\n",
      "Epoch 2, Training Loss: 1.0921764149701685\n",
      "Epoch 3, Training Loss: 1.091803853493884\n",
      "Epoch 4, Training Loss: 1.0914429659233953\n",
      "Epoch 5, Training Loss: 1.091252146448408\n",
      "Epoch 6, Training Loss: 1.090898254939488\n",
      "Epoch 7, Training Loss: 1.0906204756041218\n",
      "Epoch 8, Training Loss: 1.0903153806700743\n",
      "Epoch 9, Training Loss: 1.089979172469978\n",
      "Epoch 10, Training Loss: 1.089820322775303\n",
      "Epoch 11, Training Loss: 1.0895029241877392\n",
      "Epoch 12, Training Loss: 1.0893731889868141\n",
      "Epoch 13, Training Loss: 1.0889140855100818\n",
      "Epoch 14, Training Loss: 1.0887490853331143\n",
      "Epoch 15, Training Loss: 1.0885316683833761\n",
      "Epoch 16, Training Loss: 1.0882017608872034\n",
      "Epoch 17, Training Loss: 1.087991508326136\n",
      "Epoch 18, Training Loss: 1.0877584509383467\n",
      "Epoch 19, Training Loss: 1.0874806282215548\n",
      "Epoch 20, Training Loss: 1.087065125228767\n",
      "Epoch 21, Training Loss: 1.0868485777001633\n",
      "Epoch 22, Training Loss: 1.0865048464079547\n",
      "Epoch 23, Training Loss: 1.0863317607937002\n",
      "Epoch 24, Training Loss: 1.0859889078857308\n",
      "Epoch 25, Training Loss: 1.0857429025764753\n",
      "Epoch 26, Training Loss: 1.0854659930207675\n",
      "Epoch 27, Training Loss: 1.0852322243210069\n",
      "Epoch 28, Training Loss: 1.0847591523837326\n",
      "Epoch 29, Training Loss: 1.0845303601788399\n",
      "Epoch 30, Training Loss: 1.08427275141379\n",
      "Epoch 31, Training Loss: 1.0840506474774583\n",
      "Epoch 32, Training Loss: 1.083639461115787\n",
      "Epoch 33, Training Loss: 1.0832417995409858\n",
      "Epoch 34, Training Loss: 1.0829574800075445\n",
      "Epoch 35, Training Loss: 1.0826890850425663\n",
      "Epoch 36, Training Loss: 1.0825056142376779\n",
      "Epoch 37, Training Loss: 1.0820273673624023\n",
      "Epoch 38, Training Loss: 1.0816370591185147\n",
      "Epoch 39, Training Loss: 1.0814127870072099\n",
      "Epoch 40, Training Loss: 1.080993896498716\n",
      "Epoch 41, Training Loss: 1.0806624676051892\n",
      "Epoch 42, Training Loss: 1.0802981765646684\n",
      "Epoch 43, Training Loss: 1.0799175825334133\n",
      "Epoch 44, Training Loss: 1.079629771870778\n",
      "Epoch 45, Training Loss: 1.0792160091543557\n",
      "Epoch 46, Training Loss: 1.078837812215762\n",
      "Epoch 47, Training Loss: 1.0784429650557668\n",
      "Epoch 48, Training Loss: 1.0780238269863271\n",
      "Epoch 49, Training Loss: 1.0776850558761368\n",
      "Epoch 50, Training Loss: 1.077295188079203\n",
      "Epoch 51, Training Loss: 1.0769039754580734\n",
      "Epoch 52, Training Loss: 1.0764413390840804\n",
      "Epoch 53, Training Loss: 1.0760768947744728\n",
      "Epoch 54, Training Loss: 1.075577435816141\n",
      "Epoch 55, Training Loss: 1.0751163161786876\n",
      "Epoch 56, Training Loss: 1.0746426385148127\n",
      "Epoch 57, Training Loss: 1.0742552520637225\n",
      "Epoch 58, Training Loss: 1.0739196655445529\n",
      "Epoch 59, Training Loss: 1.0732525352248572\n",
      "Epoch 60, Training Loss: 1.072825665222971\n",
      "Epoch 61, Training Loss: 1.0723537558003475\n",
      "Epoch 62, Training Loss: 1.0717741702732286\n",
      "Epoch 63, Training Loss: 1.0713585986230607\n",
      "Epoch 64, Training Loss: 1.070719065522789\n",
      "Epoch 65, Training Loss: 1.070323103890383\n",
      "Epoch 66, Training Loss: 1.0696881901948971\n",
      "Epoch 67, Training Loss: 1.0691127067221735\n",
      "Epoch 68, Training Loss: 1.0685925582297762\n",
      "Epoch 69, Training Loss: 1.0680207006913378\n",
      "Epoch 70, Training Loss: 1.0675019508017634\n",
      "Epoch 71, Training Loss: 1.0667742435197185\n",
      "Epoch 72, Training Loss: 1.066069863792649\n",
      "Epoch 73, Training Loss: 1.0655846204972805\n",
      "Epoch 74, Training Loss: 1.0648679783469752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:13:32,132] Trial 336 finished with value: 0.45353333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 1.064204780320476\n",
      "Epoch 1, Training Loss: 1.0046749271364772\n",
      "Epoch 2, Training Loss: 0.957507346868515\n",
      "Epoch 3, Training Loss: 0.9472635936035829\n",
      "Epoch 4, Training Loss: 0.9353254094544579\n",
      "Epoch 5, Training Loss: 0.919678656984778\n",
      "Epoch 6, Training Loss: 0.8993630848912632\n",
      "Epoch 7, Training Loss: 0.8761916636719423\n",
      "Epoch 8, Training Loss: 0.8546297504621394\n",
      "Epoch 9, Training Loss: 0.8386089255529291\n",
      "Epoch 10, Training Loss: 0.8282613668021034\n",
      "Epoch 11, Training Loss: 0.8216572706839618\n",
      "Epoch 12, Training Loss: 0.817374333002988\n",
      "Epoch 13, Training Loss: 0.8144613462335923\n",
      "Epoch 14, Training Loss: 0.8123779773010927\n",
      "Epoch 15, Training Loss: 0.811150559537551\n",
      "Epoch 16, Training Loss: 0.809998234650668\n",
      "Epoch 17, Training Loss: 0.8089344100391164\n",
      "Epoch 18, Training Loss: 0.8081297311362098\n",
      "Epoch 19, Training Loss: 0.8074742651686949\n",
      "Epoch 20, Training Loss: 0.806817792373545\n",
      "Epoch 21, Training Loss: 0.806000530088649\n",
      "Epoch 22, Training Loss: 0.8054757391705233\n",
      "Epoch 23, Training Loss: 0.8049445648754344\n",
      "Epoch 24, Training Loss: 0.8044479048252106\n",
      "Epoch 25, Training Loss: 0.8035789908381069\n",
      "Epoch 26, Training Loss: 0.8030649546314689\n",
      "Epoch 27, Training Loss: 0.8027160556877361\n",
      "Epoch 28, Training Loss: 0.8024462389244753\n",
      "Epoch 29, Training Loss: 0.8019564612472758\n",
      "Epoch 30, Training Loss: 0.8013817410609302\n",
      "Epoch 31, Training Loss: 0.8012014497027677\n",
      "Epoch 32, Training Loss: 0.801047538098167\n",
      "Epoch 33, Training Loss: 0.8007866684128256\n",
      "Epoch 34, Training Loss: 0.8002389304778155\n",
      "Epoch 35, Training Loss: 0.8002193878678715\n",
      "Epoch 36, Training Loss: 0.7999819161611444\n",
      "Epoch 37, Training Loss: 0.7996291997152216\n",
      "Epoch 38, Training Loss: 0.799595990040723\n",
      "Epoch 39, Training Loss: 0.7993596633742838\n",
      "Epoch 40, Training Loss: 0.7992837094559389\n",
      "Epoch 41, Training Loss: 0.7990005245629479\n",
      "Epoch 42, Training Loss: 0.798718639051213\n",
      "Epoch 43, Training Loss: 0.7987006237927605\n",
      "Epoch 44, Training Loss: 0.7989239170972039\n",
      "Epoch 45, Training Loss: 0.7986859653977787\n",
      "Epoch 46, Training Loss: 0.7985310720696169\n",
      "Epoch 47, Training Loss: 0.7983896106130937\n",
      "Epoch 48, Training Loss: 0.7983524896116818\n",
      "Epoch 49, Training Loss: 0.7981231436308692\n",
      "Epoch 50, Training Loss: 0.7980270837335025\n",
      "Epoch 51, Training Loss: 0.7981755581322838\n",
      "Epoch 52, Training Loss: 0.7979317652477937\n",
      "Epoch 53, Training Loss: 0.7977643493343802\n",
      "Epoch 54, Training Loss: 0.7977747612139758\n",
      "Epoch 55, Training Loss: 0.797694322642158\n",
      "Epoch 56, Training Loss: 0.7977079901274513\n",
      "Epoch 57, Training Loss: 0.7975021520081689\n",
      "Epoch 58, Training Loss: 0.7976045533488778\n",
      "Epoch 59, Training Loss: 0.7975523758635802\n",
      "Epoch 60, Training Loss: 0.7973147602642283\n",
      "Epoch 61, Training Loss: 0.7972504438372219\n",
      "Epoch 62, Training Loss: 0.7972665291673997\n",
      "Epoch 63, Training Loss: 0.7970609069571776\n",
      "Epoch 64, Training Loss: 0.7969655614740708\n",
      "Epoch 65, Training Loss: 0.7970012147286359\n",
      "Epoch 66, Training Loss: 0.7969174653642318\n",
      "Epoch 67, Training Loss: 0.7967027955195483\n",
      "Epoch 68, Training Loss: 0.7966770765360663\n",
      "Epoch 69, Training Loss: 0.7967617810473723\n",
      "Epoch 70, Training Loss: 0.7965378006766825\n",
      "Epoch 71, Training Loss: 0.7965799233492683\n",
      "Epoch 72, Training Loss: 0.7965792333378511\n",
      "Epoch 73, Training Loss: 0.7964598082794863\n",
      "Epoch 74, Training Loss: 0.7963774660755606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:14:52,525] Trial 337 finished with value: 0.6343333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7963453379098107\n",
      "Epoch 1, Training Loss: 0.8432655181604273\n",
      "Epoch 2, Training Loss: 0.8138048406208263\n",
      "Epoch 3, Training Loss: 0.809252583139083\n",
      "Epoch 4, Training Loss: 0.8059317668045268\n",
      "Epoch 5, Training Loss: 0.8053427448693444\n",
      "Epoch 6, Training Loss: 0.8061298534449409\n",
      "Epoch 7, Training Loss: 0.8048996578945833\n",
      "Epoch 8, Training Loss: 0.8024909517344306\n",
      "Epoch 9, Training Loss: 0.8027388226985931\n",
      "Epoch 10, Training Loss: 0.8023344106533948\n",
      "Epoch 11, Training Loss: 0.8018918943405151\n",
      "Epoch 12, Training Loss: 0.8005830739526187\n",
      "Epoch 13, Training Loss: 0.8009341678198646\n",
      "Epoch 14, Training Loss: 0.8008775232118719\n",
      "Epoch 15, Training Loss: 0.8011419307484346\n",
      "Epoch 16, Training Loss: 0.7992327329691719\n",
      "Epoch 17, Training Loss: 0.799595266159843\n",
      "Epoch 18, Training Loss: 0.7993661328624276\n",
      "Epoch 19, Training Loss: 0.7990542631990769\n",
      "Epoch 20, Training Loss: 0.8000650096640868\n",
      "Epoch 21, Training Loss: 0.8000954679180594\n",
      "Epoch 22, Training Loss: 0.7982615788543925\n",
      "Epoch 23, Training Loss: 0.7987825203643125\n",
      "Epoch 24, Training Loss: 0.799108987976523\n",
      "Epoch 25, Training Loss: 0.7992237497778499\n",
      "Epoch 26, Training Loss: 0.7992435677612529\n",
      "Epoch 27, Training Loss: 0.7992445715735941\n",
      "Epoch 28, Training Loss: 0.798451803782407\n",
      "Epoch 29, Training Loss: 0.797328271374983\n",
      "Epoch 30, Training Loss: 0.7991121990540448\n",
      "Epoch 31, Training Loss: 0.7987641663411085\n",
      "Epoch 32, Training Loss: 0.7990745335466721\n",
      "Epoch 33, Training Loss: 0.8003457474708557\n",
      "Epoch 34, Training Loss: 0.798679797368891\n",
      "Epoch 35, Training Loss: 0.7986039335587446\n",
      "Epoch 36, Training Loss: 0.7972998672373155\n",
      "Epoch 37, Training Loss: 0.7978231057699989\n",
      "Epoch 38, Training Loss: 0.7986920252267052\n",
      "Epoch 39, Training Loss: 0.7983592980749467\n",
      "Epoch 40, Training Loss: 0.7982402330987594\n",
      "Epoch 41, Training Loss: 0.7987171003397773\n",
      "Epoch 42, Training Loss: 0.7985874382888569\n",
      "Epoch 43, Training Loss: 0.7977138723345364\n",
      "Epoch 44, Training Loss: 0.7994842608535991\n",
      "Epoch 45, Training Loss: 0.7981519308510948\n",
      "Epoch 46, Training Loss: 0.7978330732794369\n",
      "Epoch 47, Training Loss: 0.7980195095959832\n",
      "Epoch 48, Training Loss: 0.7995770303642049\n",
      "Epoch 49, Training Loss: 0.7984692062349881\n",
      "Epoch 50, Training Loss: 0.796715395380469\n",
      "Epoch 51, Training Loss: 0.7984047522264368\n",
      "Epoch 52, Training Loss: 0.7970027777727913\n",
      "Epoch 53, Training Loss: 0.7980098351310281\n",
      "Epoch 54, Training Loss: 0.7976283022235422\n",
      "Epoch 55, Training Loss: 0.7983481955528259\n",
      "Epoch 56, Training Loss: 0.7971810551250682\n",
      "Epoch 57, Training Loss: 0.7974157974299263\n",
      "Epoch 58, Training Loss: 0.7971963696620044\n",
      "Epoch 59, Training Loss: 0.7962735579294317\n",
      "Epoch 60, Training Loss: 0.7980888000656576\n",
      "Epoch 61, Training Loss: 0.7980387039745556\n",
      "Epoch 62, Training Loss: 0.7966752346122966\n",
      "Epoch 63, Training Loss: 0.7971031852329479\n",
      "Epoch 64, Training Loss: 0.7978737047139336\n",
      "Epoch 65, Training Loss: 0.7977738576776842\n",
      "Epoch 66, Training Loss: 0.7968009865985197\n",
      "Epoch 67, Training Loss: 0.7969769496076248\n",
      "Epoch 68, Training Loss: 0.7964737827637617\n",
      "Epoch 69, Training Loss: 0.7983681630387026\n",
      "Epoch 70, Training Loss: 0.7968733422896441\n",
      "Epoch 71, Training Loss: 0.7969772764514474\n",
      "Epoch 72, Training Loss: 0.7976556655238657\n",
      "Epoch 73, Training Loss: 0.7970099833432366\n",
      "Epoch 74, Training Loss: 0.7962136564535253\n",
      "Epoch 75, Training Loss: 0.7970408647901872\n",
      "Epoch 76, Training Loss: 0.79674547903678\n",
      "Epoch 77, Training Loss: 0.7962095208027784\n",
      "Epoch 78, Training Loss: 0.7970258224010468\n",
      "Epoch 79, Training Loss: 0.797595355089973\n",
      "Epoch 80, Training Loss: 0.7965705546210794\n",
      "Epoch 81, Training Loss: 0.7964681618353899\n",
      "Epoch 82, Training Loss: 0.7967803408819086\n",
      "Epoch 83, Training Loss: 0.7971265421895419\n",
      "Epoch 84, Training Loss: 0.7973008639672223\n",
      "Epoch 85, Training Loss: 0.7956144739599789\n",
      "Epoch 86, Training Loss: 0.7973928766390856\n",
      "Epoch 87, Training Loss: 0.7963204256927265\n",
      "Epoch 88, Training Loss: 0.7967237041978275\n",
      "Epoch 89, Training Loss: 0.7961223858244278\n",
      "Epoch 90, Training Loss: 0.796882237476461\n",
      "Epoch 91, Training Loss: 0.7973334201644449\n",
      "Epoch 92, Training Loss: 0.7977630578069126\n",
      "Epoch 93, Training Loss: 0.7955615300290725\n",
      "Epoch 94, Training Loss: 0.796492742580526\n",
      "Epoch 95, Training Loss: 0.7970846914543825\n",
      "Epoch 96, Training Loss: 0.7971228806411519\n",
      "Epoch 97, Training Loss: 0.7960149995719685\n",
      "Epoch 98, Training Loss: 0.7970615952155169\n",
      "Epoch 99, Training Loss: 0.7978073533142315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:17:10,163] Trial 338 finished with value: 0.6395333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7966047461593853\n",
      "Epoch 1, Training Loss: 0.8892704110987046\n",
      "Epoch 2, Training Loss: 0.8273268384793225\n",
      "Epoch 3, Training Loss: 0.8199562122541315\n",
      "Epoch 4, Training Loss: 0.8154850980814765\n",
      "Epoch 5, Training Loss: 0.8133910017855027\n",
      "Epoch 6, Training Loss: 0.8121543291035821\n",
      "Epoch 7, Training Loss: 0.8102640370761647\n",
      "Epoch 8, Training Loss: 0.8092805599465089\n",
      "Epoch 9, Training Loss: 0.8088833735269658\n",
      "Epoch 10, Training Loss: 0.8077570446098552\n",
      "Epoch 11, Training Loss: 0.8075764027763815\n",
      "Epoch 12, Training Loss: 0.806340316954781\n",
      "Epoch 13, Training Loss: 0.806428268586888\n",
      "Epoch 14, Training Loss: 0.8054531175248764\n",
      "Epoch 15, Training Loss: 0.8052735892464132\n",
      "Epoch 16, Training Loss: 0.8046039426326752\n",
      "Epoch 17, Training Loss: 0.8039456311394186\n",
      "Epoch 18, Training Loss: 0.8035256422968472\n",
      "Epoch 19, Training Loss: 0.8035158606837778\n",
      "Epoch 20, Training Loss: 0.8027554650166455\n",
      "Epoch 21, Training Loss: 0.8026111371376935\n",
      "Epoch 22, Training Loss: 0.8024825848551358\n",
      "Epoch 23, Training Loss: 0.8021522249193752\n",
      "Epoch 24, Training Loss: 0.8016779896792243\n",
      "Epoch 25, Training Loss: 0.8014473134629867\n",
      "Epoch 26, Training Loss: 0.8011297039424672\n",
      "Epoch 27, Training Loss: 0.8004065664375529\n",
      "Epoch 28, Training Loss: 0.8001689802197849\n",
      "Epoch 29, Training Loss: 0.8001275039420408\n",
      "Epoch 30, Training Loss: 0.7995061870883493\n",
      "Epoch 31, Training Loss: 0.7989185690879822\n",
      "Epoch 32, Training Loss: 0.7988944481400883\n",
      "Epoch 33, Training Loss: 0.7985981477008146\n",
      "Epoch 34, Training Loss: 0.7980776180940515\n",
      "Epoch 35, Training Loss: 0.7982141475817737\n",
      "Epoch 36, Training Loss: 0.7980475650815403\n",
      "Epoch 37, Training Loss: 0.7980626627276926\n",
      "Epoch 38, Training Loss: 0.7973134791851044\n",
      "Epoch 39, Training Loss: 0.7970116531848908\n",
      "Epoch 40, Training Loss: 0.7965988176710466\n",
      "Epoch 41, Training Loss: 0.796454330332139\n",
      "Epoch 42, Training Loss: 0.7964829118111554\n",
      "Epoch 43, Training Loss: 0.7960386295879588\n",
      "Epoch 44, Training Loss: 0.7957607267183416\n",
      "Epoch 45, Training Loss: 0.7959343835886787\n",
      "Epoch 46, Training Loss: 0.7959252666725832\n",
      "Epoch 47, Training Loss: 0.7955618994376239\n",
      "Epoch 48, Training Loss: 0.7957481516810024\n",
      "Epoch 49, Training Loss: 0.7961810968202703\n",
      "Epoch 50, Training Loss: 0.7954546369524563\n",
      "Epoch 51, Training Loss: 0.7953955824936138\n",
      "Epoch 52, Training Loss: 0.7948519904473249\n",
      "Epoch 53, Training Loss: 0.7948262318442849\n",
      "Epoch 54, Training Loss: 0.7947066262890311\n",
      "Epoch 55, Training Loss: 0.7942659609458026\n",
      "Epoch 56, Training Loss: 0.7946416330337525\n",
      "Epoch 57, Training Loss: 0.7941489521194907\n",
      "Epoch 58, Training Loss: 0.7942105221748352\n",
      "Epoch 59, Training Loss: 0.7945186852707582\n",
      "Epoch 60, Training Loss: 0.7938956745933083\n",
      "Epoch 61, Training Loss: 0.7941154394430273\n",
      "Epoch 62, Training Loss: 0.7936314400504617\n",
      "Epoch 63, Training Loss: 0.7937029445171356\n",
      "Epoch 64, Training Loss: 0.7936327963716844\n",
      "Epoch 65, Training Loss: 0.7942716491222381\n",
      "Epoch 66, Training Loss: 0.7934526498177472\n",
      "Epoch 67, Training Loss: 0.794077099351322\n",
      "Epoch 68, Training Loss: 0.7934261453852934\n",
      "Epoch 69, Training Loss: 0.7936116041856653\n",
      "Epoch 70, Training Loss: 0.7935531109922073\n",
      "Epoch 71, Training Loss: 0.7934777820110321\n",
      "Epoch 72, Training Loss: 0.7934548652172089\n",
      "Epoch 73, Training Loss: 0.7929061836354873\n",
      "Epoch 74, Training Loss: 0.7930474937663359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:18:34,637] Trial 339 finished with value: 0.6374 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7931962867344127\n",
      "Epoch 1, Training Loss: 0.8455305004119873\n",
      "Epoch 2, Training Loss: 0.815960534250035\n",
      "Epoch 3, Training Loss: 0.8129485197628246\n",
      "Epoch 4, Training Loss: 0.8127204242173364\n",
      "Epoch 5, Training Loss: 0.8133487545742708\n",
      "Epoch 6, Training Loss: 0.8094500119545881\n",
      "Epoch 7, Training Loss: 0.8085648948304793\n",
      "Epoch 8, Training Loss: 0.8060444037353292\n",
      "Epoch 9, Training Loss: 0.8066500702325036\n",
      "Epoch 10, Training Loss: 0.8046199358210844\n",
      "Epoch 11, Training Loss: 0.8043589362677406\n",
      "Epoch 12, Training Loss: 0.8039451875406153\n",
      "Epoch 13, Training Loss: 0.8037100526865791\n",
      "Epoch 14, Training Loss: 0.8032545935406404\n",
      "Epoch 15, Training Loss: 0.8018862445915447\n",
      "Epoch 16, Training Loss: 0.8029202482980841\n",
      "Epoch 17, Training Loss: 0.8015281799260308\n",
      "Epoch 18, Training Loss: 0.802010257173987\n",
      "Epoch 19, Training Loss: 0.8011505611503825\n",
      "Epoch 20, Training Loss: 0.801757684875937\n",
      "Epoch 21, Training Loss: 0.8014632096711327\n",
      "Epoch 22, Training Loss: 0.8006611421528984\n",
      "Epoch 23, Training Loss: 0.8007914328575134\n",
      "Epoch 24, Training Loss: 0.8007565433137557\n",
      "Epoch 25, Training Loss: 0.8000872802734375\n",
      "Epoch 26, Training Loss: 0.7998980197485756\n",
      "Epoch 27, Training Loss: 0.799820194384631\n",
      "Epoch 28, Training Loss: 0.7993411709280575\n",
      "Epoch 29, Training Loss: 0.7995401108966155\n",
      "Epoch 30, Training Loss: 0.8008766563499675\n",
      "Epoch 31, Training Loss: 0.8007005162800059\n",
      "Epoch 32, Training Loss: 0.7997080816942103\n",
      "Epoch 33, Training Loss: 0.7995579942534952\n",
      "Epoch 34, Training Loss: 0.8009146049443413\n",
      "Epoch 35, Training Loss: 0.7992023827749141\n",
      "Epoch 36, Training Loss: 0.7991276524347417\n",
      "Epoch 37, Training Loss: 0.7993200383466833\n",
      "Epoch 38, Training Loss: 0.7987135502871345\n",
      "Epoch 39, Training Loss: 0.7984587939346538\n",
      "Epoch 40, Training Loss: 0.7990564233415267\n",
      "Epoch 41, Training Loss: 0.797780825250289\n",
      "Epoch 42, Training Loss: 0.8006532289701349\n",
      "Epoch 43, Training Loss: 0.7986992462943582\n",
      "Epoch 44, Training Loss: 0.797806369486977\n",
      "Epoch 45, Training Loss: 0.7974486602755154\n",
      "Epoch 46, Training Loss: 0.7993537541698007\n",
      "Epoch 47, Training Loss: 0.7982333437134238\n",
      "Epoch 48, Training Loss: 0.7977428502896253\n",
      "Epoch 49, Training Loss: 0.7992364477410036\n",
      "Epoch 50, Training Loss: 0.7983611005895278\n",
      "Epoch 51, Training Loss: 0.797833143192179\n",
      "Epoch 52, Training Loss: 0.7978318674424115\n",
      "Epoch 53, Training Loss: 0.7983621093104868\n",
      "Epoch 54, Training Loss: 0.7991869719589457\n",
      "Epoch 55, Training Loss: 0.7981117111795089\n",
      "Epoch 56, Training Loss: 0.7978970219107235\n",
      "Epoch 57, Training Loss: 0.7984309300254373\n",
      "Epoch 58, Training Loss: 0.7987176029822406\n",
      "Epoch 59, Training Loss: 0.7983613372550291\n",
      "Epoch 60, Training Loss: 0.7988686548261081\n",
      "Epoch 61, Training Loss: 0.79778535267886\n",
      "Epoch 62, Training Loss: 0.798453278611688\n",
      "Epoch 63, Training Loss: 0.7971706802704756\n",
      "Epoch 64, Training Loss: 0.7977102806287654\n",
      "Epoch 65, Training Loss: 0.797048793259789\n",
      "Epoch 66, Training Loss: 0.797497768682592\n",
      "Epoch 67, Training Loss: 0.7988046342485091\n",
      "Epoch 68, Training Loss: 0.7971322403935825\n",
      "Epoch 69, Training Loss: 0.7983776596714468\n",
      "Epoch 70, Training Loss: 0.7978457272052765\n",
      "Epoch 71, Training Loss: 0.7981723340118633\n",
      "Epoch 72, Training Loss: 0.7969643054288976\n",
      "Epoch 73, Training Loss: 0.797576762858559\n",
      "Epoch 74, Training Loss: 0.7976656293167788\n",
      "Epoch 75, Training Loss: 0.797342003583908\n",
      "Epoch 76, Training Loss: 0.7979547928361331\n",
      "Epoch 77, Training Loss: 0.7978180263322943\n",
      "Epoch 78, Training Loss: 0.7971560405983644\n",
      "Epoch 79, Training Loss: 0.7968964805322535\n",
      "Epoch 80, Training Loss: 0.7974275119164411\n",
      "Epoch 81, Training Loss: 0.7978747522830963\n",
      "Epoch 82, Training Loss: 0.7981333497692557\n",
      "Epoch 83, Training Loss: 0.7972389949770535\n",
      "Epoch 84, Training Loss: 0.7975266181721407\n",
      "Epoch 85, Training Loss: 0.7974702971823076\n",
      "Epoch 86, Training Loss: 0.797552812870811\n",
      "Epoch 87, Training Loss: 0.796520552144331\n",
      "Epoch 88, Training Loss: 0.7977040180038003\n",
      "Epoch 89, Training Loss: 0.7976618008052602\n",
      "Epoch 90, Training Loss: 0.7974510255280663\n",
      "Epoch 91, Training Loss: 0.7977558573554544\n",
      "Epoch 92, Training Loss: 0.7978452291909386\n",
      "Epoch 93, Training Loss: 0.7963910579681397\n",
      "Epoch 94, Training Loss: 0.7973865992181441\n",
      "Epoch 95, Training Loss: 0.7974395442710204\n",
      "Epoch 96, Training Loss: 0.7972442253898172\n",
      "Epoch 97, Training Loss: 0.7973210681887234\n",
      "Epoch 98, Training Loss: 0.7966681365405812\n",
      "Epoch 99, Training Loss: 0.7972477237617268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:20:32,838] Trial 340 finished with value: 0.6345333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7972498865688549\n",
      "Epoch 1, Training Loss: 0.8910806491094477\n",
      "Epoch 2, Training Loss: 0.8297848111741684\n",
      "Epoch 3, Training Loss: 0.8209933588084053\n",
      "Epoch 4, Training Loss: 0.8170710691984961\n",
      "Epoch 5, Training Loss: 0.814230352079167\n",
      "Epoch 6, Training Loss: 0.8125813834106221\n",
      "Epoch 7, Training Loss: 0.8105066920028013\n",
      "Epoch 8, Training Loss: 0.8091288367439718\n",
      "Epoch 9, Training Loss: 0.8081508022196152\n",
      "Epoch 10, Training Loss: 0.8081302480837879\n",
      "Epoch 11, Training Loss: 0.807276328942355\n",
      "Epoch 12, Training Loss: 0.806744205530952\n",
      "Epoch 13, Training Loss: 0.8067962537793553\n",
      "Epoch 14, Training Loss: 0.80539940013605\n",
      "Epoch 15, Training Loss: 0.8052970980195439\n",
      "Epoch 16, Training Loss: 0.8049798937404857\n",
      "Epoch 17, Training Loss: 0.8040177155943478\n",
      "Epoch 18, Training Loss: 0.8041871460045085\n",
      "Epoch 19, Training Loss: 0.8031109116357915\n",
      "Epoch 20, Training Loss: 0.8025524400262272\n",
      "Epoch 21, Training Loss: 0.8021073698997497\n",
      "Epoch 22, Training Loss: 0.8014283351337208\n",
      "Epoch 23, Training Loss: 0.8019275737510008\n",
      "Epoch 24, Training Loss: 0.8012467890627244\n",
      "Epoch 25, Training Loss: 0.8009140590359183\n",
      "Epoch 26, Training Loss: 0.8010353943880867\n",
      "Epoch 27, Training Loss: 0.8002495285342721\n",
      "Epoch 28, Training Loss: 0.8002469684095944\n",
      "Epoch 29, Training Loss: 0.7999456588660969\n",
      "Epoch 30, Training Loss: 0.7995660024530747\n",
      "Epoch 31, Training Loss: 0.7995197196567759\n",
      "Epoch 32, Training Loss: 0.7991680310754214\n",
      "Epoch 33, Training Loss: 0.7987677019483903\n",
      "Epoch 34, Training Loss: 0.7987874848001143\n",
      "Epoch 35, Training Loss: 0.7976913832215702\n",
      "Epoch 36, Training Loss: 0.7982197801505818\n",
      "Epoch 37, Training Loss: 0.7976003641941968\n",
      "Epoch 38, Training Loss: 0.7971707513753106\n",
      "Epoch 39, Training Loss: 0.7967463359412025\n",
      "Epoch 40, Training Loss: 0.7969682446648093\n",
      "Epoch 41, Training Loss: 0.7967735315771663\n",
      "Epoch 42, Training Loss: 0.7963721846832948\n",
      "Epoch 43, Training Loss: 0.7962786410135382\n",
      "Epoch 44, Training Loss: 0.7965651749863344\n",
      "Epoch 45, Training Loss: 0.7961597304484423\n",
      "Epoch 46, Training Loss: 0.7958438055655536\n",
      "Epoch 47, Training Loss: 0.7961440200665418\n",
      "Epoch 48, Training Loss: 0.7960218686216017\n",
      "Epoch 49, Training Loss: 0.7957737998401417\n",
      "Epoch 50, Training Loss: 0.7956814047168282\n",
      "Epoch 51, Training Loss: 0.7953048301444334\n",
      "Epoch 52, Training Loss: 0.7953387547941769\n",
      "Epoch 53, Training Loss: 0.7949296617507935\n",
      "Epoch 54, Training Loss: 0.7952482991358814\n",
      "Epoch 55, Training Loss: 0.7954483318328858\n",
      "Epoch 56, Training Loss: 0.7944052567201502\n",
      "Epoch 57, Training Loss: 0.7947632549790775\n",
      "Epoch 58, Training Loss: 0.7943126299100763\n",
      "Epoch 59, Training Loss: 0.7945288987720713\n",
      "Epoch 60, Training Loss: 0.7945630419955534\n",
      "Epoch 61, Training Loss: 0.7946192074523253\n",
      "Epoch 62, Training Loss: 0.7937519406571107\n",
      "Epoch 63, Training Loss: 0.7940396069779115\n",
      "Epoch 64, Training Loss: 0.7941114191447988\n",
      "Epoch 65, Training Loss: 0.7936027434292962\n",
      "Epoch 66, Training Loss: 0.79426818504053\n",
      "Epoch 67, Training Loss: 0.7937336877514334\n",
      "Epoch 68, Training Loss: 0.7937644341412713\n",
      "Epoch 69, Training Loss: 0.7941116229926839\n",
      "Epoch 70, Training Loss: 0.793530830004636\n",
      "Epoch 71, Training Loss: 0.7939766821440528\n",
      "Epoch 72, Training Loss: 0.7929037334638483\n",
      "Epoch 73, Training Loss: 0.793547571476768\n",
      "Epoch 74, Training Loss: 0.7934311731422649\n",
      "Epoch 75, Training Loss: 0.7935113098340876\n",
      "Epoch 76, Training Loss: 0.7930719899429994\n",
      "Epoch 77, Training Loss: 0.7931071990377763\n",
      "Epoch 78, Training Loss: 0.7928331546923694\n",
      "Epoch 79, Training Loss: 0.7933021515257218\n",
      "Epoch 80, Training Loss: 0.7928188211777631\n",
      "Epoch 81, Training Loss: 0.7926880845602821\n",
      "Epoch 82, Training Loss: 0.7928857643464032\n",
      "Epoch 83, Training Loss: 0.7929037430707147\n",
      "Epoch 84, Training Loss: 0.7921899512234856\n",
      "Epoch 85, Training Loss: 0.793019437789917\n",
      "Epoch 86, Training Loss: 0.7929719999958487\n",
      "Epoch 87, Training Loss: 0.7926481458720039\n",
      "Epoch 88, Training Loss: 0.792544353499132\n",
      "Epoch 89, Training Loss: 0.7925022635039162\n",
      "Epoch 90, Training Loss: 0.7924172118130852\n",
      "Epoch 91, Training Loss: 0.7925094788915971\n",
      "Epoch 92, Training Loss: 0.7924859574963065\n",
      "Epoch 93, Training Loss: 0.792103451350156\n",
      "Epoch 94, Training Loss: 0.7922598568130942\n",
      "Epoch 95, Training Loss: 0.7918235846828012\n",
      "Epoch 96, Training Loss: 0.7926251522232505\n",
      "Epoch 97, Training Loss: 0.7924357504003188\n",
      "Epoch 98, Training Loss: 0.7923272585167604\n",
      "Epoch 99, Training Loss: 0.7918758338339189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:22:24,558] Trial 341 finished with value: 0.6345333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7914221629675697\n",
      "Epoch 1, Training Loss: 0.8469736933708191\n",
      "Epoch 2, Training Loss: 0.8126755098034354\n",
      "Epoch 3, Training Loss: 0.8071326006861294\n",
      "Epoch 4, Training Loss: 0.8044018241237192\n",
      "Epoch 5, Training Loss: 0.8014716797015247\n",
      "Epoch 6, Training Loss: 0.8012630847622366\n",
      "Epoch 7, Training Loss: 0.8012634599208832\n",
      "Epoch 8, Training Loss: 0.8005785282920389\n",
      "Epoch 9, Training Loss: 0.7977497116958394\n",
      "Epoch 10, Training Loss: 0.7992417071847354\n",
      "Epoch 11, Training Loss: 0.798695687195834\n",
      "Epoch 12, Training Loss: 0.7995759255044601\n",
      "Epoch 13, Training Loss: 0.7988307760042302\n",
      "Epoch 14, Training Loss: 0.7985957864452811\n",
      "Epoch 15, Training Loss: 0.7972945883694817\n",
      "Epoch 16, Training Loss: 0.7964198625087738\n",
      "Epoch 17, Training Loss: 0.7963690804733949\n",
      "Epoch 18, Training Loss: 0.797701990183662\n",
      "Epoch 19, Training Loss: 0.7969005085440243\n",
      "Epoch 20, Training Loss: 0.796500442869523\n",
      "Epoch 21, Training Loss: 0.7955283668461968\n",
      "Epoch 22, Training Loss: 0.794921305810704\n",
      "Epoch 23, Training Loss: 0.7972566721719854\n",
      "Epoch 24, Training Loss: 0.7953186170493856\n",
      "Epoch 25, Training Loss: 0.7949369656338411\n",
      "Epoch 26, Training Loss: 0.7956559830553391\n",
      "Epoch 27, Training Loss: 0.7955561734648312\n",
      "Epoch 28, Training Loss: 0.7942321693897247\n",
      "Epoch 29, Training Loss: 0.7950844862180597\n",
      "Epoch 30, Training Loss: 0.7945716643333435\n",
      "Epoch 31, Training Loss: 0.7946120703220367\n",
      "Epoch 32, Training Loss: 0.796012276060441\n",
      "Epoch 33, Training Loss: 0.7936818523266737\n",
      "Epoch 34, Training Loss: 0.793063886235742\n",
      "Epoch 35, Training Loss: 0.7952662444815917\n",
      "Epoch 36, Training Loss: 0.7943316193187938\n",
      "Epoch 37, Training Loss: 0.7949101025216719\n",
      "Epoch 38, Training Loss: 0.7948612598110648\n",
      "Epoch 39, Training Loss: 0.7951926391264972\n",
      "Epoch 40, Training Loss: 0.7938932796786813\n",
      "Epoch 41, Training Loss: 0.794444480713676\n",
      "Epoch 42, Training Loss: 0.7938093545156367\n",
      "Epoch 43, Training Loss: 0.7932565752898946\n",
      "Epoch 44, Training Loss: 0.7942633835007162\n",
      "Epoch 45, Training Loss: 0.7945487388442545\n",
      "Epoch 46, Training Loss: 0.7938680676852956\n",
      "Epoch 47, Training Loss: 0.793303358484717\n",
      "Epoch 48, Training Loss: 0.794971162852119\n",
      "Epoch 49, Training Loss: 0.7938385261507596\n",
      "Epoch 50, Training Loss: 0.7951341431281146\n",
      "Epoch 51, Training Loss: 0.7934382229692796\n",
      "Epoch 52, Training Loss: 0.793792130947113\n",
      "Epoch 53, Training Loss: 0.7947350565124961\n",
      "Epoch 54, Training Loss: 0.7933301675319672\n",
      "Epoch 55, Training Loss: 0.7935415517582612\n",
      "Epoch 56, Training Loss: 0.7939542943589828\n",
      "Epoch 57, Training Loss: 0.7929755454904893\n",
      "Epoch 58, Training Loss: 0.7935785156839034\n",
      "Epoch 59, Training Loss: 0.7943412425938775\n",
      "Epoch 60, Training Loss: 0.7934341005016776\n",
      "Epoch 61, Training Loss: 0.7934391909487107\n",
      "Epoch 62, Training Loss: 0.7927913910501143\n",
      "Epoch 63, Training Loss: 0.794827834928737\n",
      "Epoch 64, Training Loss: 0.7921552472254809\n",
      "Epoch 65, Training Loss: 0.7934948177197401\n",
      "Epoch 66, Training Loss: 0.794229598606334\n",
      "Epoch 67, Training Loss: 0.7926596229917863\n",
      "Epoch 68, Training Loss: 0.7932962890933541\n",
      "Epoch 69, Training Loss: 0.7928105368333704\n",
      "Epoch 70, Training Loss: 0.7932960428209865\n",
      "Epoch 71, Training Loss: 0.793354619811563\n",
      "Epoch 72, Training Loss: 0.7922982551771052\n",
      "Epoch 73, Training Loss: 0.793056279631222\n",
      "Epoch 74, Training Loss: 0.7935795772075653\n",
      "Epoch 75, Training Loss: 0.7940129150362576\n",
      "Epoch 76, Training Loss: 0.7937782578608569\n",
      "Epoch 77, Training Loss: 0.792799847055884\n",
      "Epoch 78, Training Loss: 0.7930626421816209\n",
      "Epoch 79, Training Loss: 0.7920544440606061\n",
      "Epoch 80, Training Loss: 0.7927754756983588\n",
      "Epoch 81, Training Loss: 0.7930855176729315\n",
      "Epoch 82, Training Loss: 0.7925668996221878\n",
      "Epoch 83, Training Loss: 0.7927411949634552\n",
      "Epoch 84, Training Loss: 0.7926466876619003\n",
      "Epoch 85, Training Loss: 0.7925309305331286\n",
      "Epoch 86, Training Loss: 0.792628182032529\n",
      "Epoch 87, Training Loss: 0.7935423064933104\n",
      "Epoch 88, Training Loss: 0.7917656653768876\n",
      "Epoch 89, Training Loss: 0.7937156205317554\n",
      "Epoch 90, Training Loss: 0.7942576382440679\n",
      "Epoch 91, Training Loss: 0.7928532655098859\n",
      "Epoch 92, Training Loss: 0.79263223528862\n",
      "Epoch 93, Training Loss: 0.7927068600233863\n",
      "Epoch 94, Training Loss: 0.7925297817061929\n",
      "Epoch 95, Training Loss: 0.7918000690376057\n",
      "Epoch 96, Training Loss: 0.7920302984293769\n",
      "Epoch 97, Training Loss: 0.793378556546043\n",
      "Epoch 98, Training Loss: 0.7925252091884613\n",
      "Epoch 99, Training Loss: 0.7926293164842269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:24:42,792] Trial 342 finished with value: 0.6322 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7924069801498862\n",
      "Epoch 1, Training Loss: 0.8479480504989624\n",
      "Epoch 2, Training Loss: 0.8161016397906425\n",
      "Epoch 3, Training Loss: 0.8046110327082469\n",
      "Epoch 4, Training Loss: 0.8033715534030943\n",
      "Epoch 5, Training Loss: 0.8030319279297851\n",
      "Epoch 6, Training Loss: 0.7994171057428633\n",
      "Epoch 7, Training Loss: 0.8008478884858297\n",
      "Epoch 8, Training Loss: 0.7986439180553407\n",
      "Epoch 9, Training Loss: 0.7987416942316786\n",
      "Epoch 10, Training Loss: 0.7985479351273157\n",
      "Epoch 11, Training Loss: 0.7976416787706819\n",
      "Epoch 12, Training Loss: 0.7979316486451858\n",
      "Epoch 13, Training Loss: 0.7975432909520945\n",
      "Epoch 14, Training Loss: 0.7954833108679693\n",
      "Epoch 15, Training Loss: 0.7965997596432391\n",
      "Epoch 16, Training Loss: 0.7962654285861137\n",
      "Epoch 17, Training Loss: 0.7943375534581062\n",
      "Epoch 18, Training Loss: 0.7939520697844656\n",
      "Epoch 19, Training Loss: 0.7961483885471086\n",
      "Epoch 20, Training Loss: 0.7951096187856861\n",
      "Epoch 21, Training Loss: 0.7941734500397417\n",
      "Epoch 22, Training Loss: 0.794936844639312\n",
      "Epoch 23, Training Loss: 0.7948855703934691\n",
      "Epoch 24, Training Loss: 0.7956289529800415\n",
      "Epoch 25, Training Loss: 0.7948885899737366\n",
      "Epoch 26, Training Loss: 0.795244338548273\n",
      "Epoch 27, Training Loss: 0.794874011842828\n",
      "Epoch 28, Training Loss: 0.7939052857850727\n",
      "Epoch 29, Training Loss: 0.7953071703588156\n",
      "Epoch 30, Training Loss: 0.7945104606169507\n",
      "Epoch 31, Training Loss: 0.7947067253571704\n",
      "Epoch 32, Training Loss: 0.7950371027889108\n",
      "Epoch 33, Training Loss: 0.7938971534707493\n",
      "Epoch 34, Training Loss: 0.7945273350952263\n",
      "Epoch 35, Training Loss: 0.7947834092871587\n",
      "Epoch 36, Training Loss: 0.7942331060431057\n",
      "Epoch 37, Training Loss: 0.7927642734427202\n",
      "Epoch 38, Training Loss: 0.7937782445348295\n",
      "Epoch 39, Training Loss: 0.7933856663847328\n",
      "Epoch 40, Training Loss: 0.793296023687922\n",
      "Epoch 41, Training Loss: 0.7927854959229778\n",
      "Epoch 42, Training Loss: 0.7933778522606183\n",
      "Epoch 43, Training Loss: 0.7922252324290742\n",
      "Epoch 44, Training Loss: 0.792388869855637\n",
      "Epoch 45, Training Loss: 0.7949605434460747\n",
      "Epoch 46, Training Loss: 0.7935093925411539\n",
      "Epoch 47, Training Loss: 0.7936676069309837\n",
      "Epoch 48, Training Loss: 0.7919072259637646\n",
      "Epoch 49, Training Loss: 0.7927179112918394\n",
      "Epoch 50, Training Loss: 0.792621518167338\n",
      "Epoch 51, Training Loss: 0.7927119432535387\n",
      "Epoch 52, Training Loss: 0.792055240340699\n",
      "Epoch 53, Training Loss: 0.7927143357750168\n",
      "Epoch 54, Training Loss: 0.7920492887496948\n",
      "Epoch 55, Training Loss: 0.7922376944606465\n",
      "Epoch 56, Training Loss: 0.7932068481481165\n",
      "Epoch 57, Training Loss: 0.792444128022158\n",
      "Epoch 58, Training Loss: 0.7928366177064136\n",
      "Epoch 59, Training Loss: 0.7919347140125762\n",
      "Epoch 60, Training Loss: 0.7931894619662062\n",
      "Epoch 61, Training Loss: 0.791312563329711\n",
      "Epoch 62, Training Loss: 0.7934674320364357\n",
      "Epoch 63, Training Loss: 0.7924526539063992\n",
      "Epoch 64, Training Loss: 0.7934640570690757\n",
      "Epoch 65, Training Loss: 0.7920090139360356\n",
      "Epoch 66, Training Loss: 0.7932560395477409\n",
      "Epoch 67, Training Loss: 0.7922178558837202\n",
      "Epoch 68, Training Loss: 0.7919650426484588\n",
      "Epoch 69, Training Loss: 0.7912251740469969\n",
      "Epoch 70, Training Loss: 0.7922088167721167\n",
      "Epoch 71, Training Loss: 0.7922686948812098\n",
      "Epoch 72, Training Loss: 0.7919672380712696\n",
      "Epoch 73, Training Loss: 0.7918656735491932\n",
      "Epoch 74, Training Loss: 0.7928813222655676\n",
      "Epoch 75, Training Loss: 0.7936514408068549\n",
      "Epoch 76, Training Loss: 0.7914056744790615\n",
      "Epoch 77, Training Loss: 0.7925953873118063\n",
      "Epoch 78, Training Loss: 0.7912184571861325\n",
      "Epoch 79, Training Loss: 0.7926824098242853\n",
      "Epoch 80, Training Loss: 0.7914828960160564\n",
      "Epoch 81, Training Loss: 0.7924317743545188\n",
      "Epoch 82, Training Loss: 0.7929684572650078\n",
      "Epoch 83, Training Loss: 0.7921824053714149\n",
      "Epoch 84, Training Loss: 0.7934667107754184\n",
      "Epoch 85, Training Loss: 0.7922128393237752\n",
      "Epoch 86, Training Loss: 0.7915803274713961\n",
      "Epoch 87, Training Loss: 0.7927223296093762\n",
      "Epoch 88, Training Loss: 0.7920111162321908\n",
      "Epoch 89, Training Loss: 0.7912086131877469\n",
      "Epoch 90, Training Loss: 0.7912813300476935\n",
      "Epoch 91, Training Loss: 0.7919539434569223\n",
      "Epoch 92, Training Loss: 0.7926274452890668\n",
      "Epoch 93, Training Loss: 0.7919054362110626\n",
      "Epoch 94, Training Loss: 0.7922718887042283\n",
      "Epoch 95, Training Loss: 0.7928146504818049\n",
      "Epoch 96, Training Loss: 0.7922024207903926\n",
      "Epoch 97, Training Loss: 0.7925807580015714\n",
      "Epoch 98, Training Loss: 0.7932597894417612\n",
      "Epoch 99, Training Loss: 0.7909717701431503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:26:42,616] Trial 343 finished with value: 0.6376 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7919992487233384\n",
      "Epoch 1, Training Loss: 0.8664885231426784\n",
      "Epoch 2, Training Loss: 0.8279640009528713\n",
      "Epoch 3, Training Loss: 0.8225494597191201\n",
      "Epoch 4, Training Loss: 0.8198463331487842\n",
      "Epoch 5, Training Loss: 0.8160086625500729\n",
      "Epoch 6, Training Loss: 0.8145857211342431\n",
      "Epoch 7, Training Loss: 0.8135534486376254\n",
      "Epoch 8, Training Loss: 0.8119402189003794\n",
      "Epoch 9, Training Loss: 0.8110576417213096\n",
      "Epoch 10, Training Loss: 0.8098352494992708\n",
      "Epoch 11, Training Loss: 0.8099117505819278\n",
      "Epoch 12, Training Loss: 0.8099601897978245\n",
      "Epoch 13, Training Loss: 0.8096519780338258\n",
      "Epoch 14, Training Loss: 0.8079110301526865\n",
      "Epoch 15, Training Loss: 0.8076717729855301\n",
      "Epoch 16, Training Loss: 0.807494028259937\n",
      "Epoch 17, Training Loss: 0.8078157772695211\n",
      "Epoch 18, Training Loss: 0.8072092406731799\n",
      "Epoch 19, Training Loss: 0.8071882368926715\n",
      "Epoch 20, Training Loss: 0.8079719610680315\n",
      "Epoch 21, Training Loss: 0.80650465116465\n",
      "Epoch 22, Training Loss: 0.8063279371512564\n",
      "Epoch 23, Training Loss: 0.8061421933927034\n",
      "Epoch 24, Training Loss: 0.8065160557739717\n",
      "Epoch 25, Training Loss: 0.8057236408828793\n",
      "Epoch 26, Training Loss: 0.8061664994497945\n",
      "Epoch 27, Training Loss: 0.8048714150163464\n",
      "Epoch 28, Training Loss: 0.8053453401515358\n",
      "Epoch 29, Training Loss: 0.8054682131996729\n",
      "Epoch 30, Training Loss: 0.805204827534525\n",
      "Epoch 31, Training Loss: 0.8053802384469743\n",
      "Epoch 32, Training Loss: 0.8056350489308063\n",
      "Epoch 33, Training Loss: 0.805582895135521\n",
      "Epoch 34, Training Loss: 0.8039134161812919\n",
      "Epoch 35, Training Loss: 0.8048053912650374\n",
      "Epoch 36, Training Loss: 0.8040575917502095\n",
      "Epoch 37, Training Loss: 0.8041311695163411\n",
      "Epoch 38, Training Loss: 0.804375183851199\n",
      "Epoch 39, Training Loss: 0.8037485211415398\n",
      "Epoch 40, Training Loss: 0.8034003824219668\n",
      "Epoch 41, Training Loss: 0.8035844654965222\n",
      "Epoch 42, Training Loss: 0.8029887704920948\n",
      "Epoch 43, Training Loss: 0.8024677811708666\n",
      "Epoch 44, Training Loss: 0.8028850292800961\n",
      "Epoch 45, Training Loss: 0.8028722712868138\n",
      "Epoch 46, Training Loss: 0.8031491463345692\n",
      "Epoch 47, Training Loss: 0.8027286146816455\n",
      "Epoch 48, Training Loss: 0.802393897493979\n",
      "Epoch 49, Training Loss: 0.8024793269042682\n",
      "Epoch 50, Training Loss: 0.8019201588809939\n",
      "Epoch 51, Training Loss: 0.801388584940057\n",
      "Epoch 52, Training Loss: 0.8012089158359327\n",
      "Epoch 53, Training Loss: 0.8006552035647228\n",
      "Epoch 54, Training Loss: 0.801063298910184\n",
      "Epoch 55, Training Loss: 0.8005765605689887\n",
      "Epoch 56, Training Loss: 0.8013250539177342\n",
      "Epoch 57, Training Loss: 0.7998890921137387\n",
      "Epoch 58, Training Loss: 0.800052713630791\n",
      "Epoch 59, Training Loss: 0.8004573498453412\n",
      "Epoch 60, Training Loss: 0.8002912107266877\n",
      "Epoch 61, Training Loss: 0.8005426255383886\n",
      "Epoch 62, Training Loss: 0.8008447686532386\n",
      "Epoch 63, Training Loss: 0.8001532575241606\n",
      "Epoch 64, Training Loss: 0.8001526414003587\n",
      "Epoch 65, Training Loss: 0.7995246034815795\n",
      "Epoch 66, Training Loss: 0.8005256128490419\n",
      "Epoch 67, Training Loss: 0.7996577230611241\n",
      "Epoch 68, Training Loss: 0.7994686001225522\n",
      "Epoch 69, Training Loss: 0.7996393096178098\n",
      "Epoch 70, Training Loss: 0.7984616868478015\n",
      "Epoch 71, Training Loss: 0.7989408970775461\n",
      "Epoch 72, Training Loss: 0.7984934722570548\n",
      "Epoch 73, Training Loss: 0.7988443347744476\n",
      "Epoch 74, Training Loss: 0.7991298325079724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:27:53,846] Trial 344 finished with value: 0.5939333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7985302007287964\n",
      "Epoch 1, Training Loss: 1.0906680989265443\n",
      "Epoch 2, Training Loss: 1.0251207783642937\n",
      "Epoch 3, Training Loss: 0.9949158397141625\n",
      "Epoch 4, Training Loss: 0.9796809924350065\n",
      "Epoch 5, Training Loss: 0.9713558624772465\n",
      "Epoch 6, Training Loss: 0.966435095282162\n",
      "Epoch 7, Training Loss: 0.9632423537618974\n",
      "Epoch 8, Training Loss: 0.9609894008496228\n",
      "Epoch 9, Training Loss: 0.9592132519273197\n",
      "Epoch 10, Training Loss: 0.9577303562444799\n",
      "Epoch 11, Training Loss: 0.9564082788719851\n",
      "Epoch 12, Training Loss: 0.9551748934914084\n",
      "Epoch 13, Training Loss: 0.9540094145606546\n",
      "Epoch 14, Training Loss: 0.9528798962340636\n",
      "Epoch 15, Training Loss: 0.9517735534555771\n",
      "Epoch 16, Training Loss: 0.9506956384462468\n",
      "Epoch 17, Training Loss: 0.9496213225757375\n",
      "Epoch 18, Training Loss: 0.9485708350995008\n",
      "Epoch 19, Training Loss: 0.9475166003143086\n",
      "Epoch 20, Training Loss: 0.9464740482498618\n",
      "Epoch 21, Training Loss: 0.9454303637672873\n",
      "Epoch 22, Training Loss: 0.9443993346831377\n",
      "Epoch 23, Training Loss: 0.9433776394058676\n",
      "Epoch 24, Training Loss: 0.9423519206047059\n",
      "Epoch 25, Training Loss: 0.9413311415560105\n",
      "Epoch 26, Training Loss: 0.9403199238636915\n",
      "Epoch 27, Training Loss: 0.9393020114477943\n",
      "Epoch 28, Training Loss: 0.938292840017992\n",
      "Epoch 29, Training Loss: 0.9372821290352765\n",
      "Epoch 30, Training Loss: 0.9362723869435927\n",
      "Epoch 31, Training Loss: 0.9352663559072157\n",
      "Epoch 32, Training Loss: 0.9342582455803367\n",
      "Epoch 33, Training Loss: 0.9332551218481625\n",
      "Epoch 34, Training Loss: 0.932248490487828\n",
      "Epoch 35, Training Loss: 0.9312409299261429\n",
      "Epoch 36, Training Loss: 0.9302357853160185\n",
      "Epoch 37, Training Loss: 0.9292329922844382\n",
      "Epoch 38, Training Loss: 0.9282261502742767\n",
      "Epoch 39, Training Loss: 0.9272203366896685\n",
      "Epoch 40, Training Loss: 0.9262145501725814\n",
      "Epoch 41, Training Loss: 0.9252028076087727\n",
      "Epoch 42, Training Loss: 0.9242030504170586\n",
      "Epoch 43, Training Loss: 0.9231895812819986\n",
      "Epoch 44, Training Loss: 0.9221769739599789\n",
      "Epoch 45, Training Loss: 0.9211699583249934\n",
      "Epoch 46, Training Loss: 0.9201559255403631\n",
      "Epoch 47, Training Loss: 0.9191537897025838\n",
      "Epoch 48, Training Loss: 0.9181391782620374\n",
      "Epoch 49, Training Loss: 0.9171246166089002\n",
      "Epoch 50, Training Loss: 0.9161071309622596\n",
      "Epoch 51, Training Loss: 0.9150920074827531\n",
      "Epoch 52, Training Loss: 0.9140808719747207\n",
      "Epoch 53, Training Loss: 0.9130632847196916\n",
      "Epoch 54, Training Loss: 0.9120434620099909\n",
      "Epoch 55, Training Loss: 0.9110239519792445\n",
      "Epoch 56, Training Loss: 0.9100072561292087\n",
      "Epoch 57, Training Loss: 0.9089763557209688\n",
      "Epoch 58, Training Loss: 0.9079473864330965\n",
      "Epoch 59, Training Loss: 0.9069419362264521\n",
      "Epoch 60, Training Loss: 0.9059205468963174\n",
      "Epoch 61, Training Loss: 0.904898201157065\n",
      "Epoch 62, Training Loss: 0.9038818580964032\n",
      "Epoch 63, Training Loss: 0.9028561437129974\n",
      "Epoch 64, Training Loss: 0.9018294208190021\n",
      "Epoch 65, Training Loss: 0.9008206946008346\n",
      "Epoch 66, Training Loss: 0.8997832909752341\n",
      "Epoch 67, Training Loss: 0.8987840949787813\n",
      "Epoch 68, Training Loss: 0.8977610813870149\n",
      "Epoch 69, Training Loss: 0.8967466032505036\n",
      "Epoch 70, Training Loss: 0.8957304397751303\n",
      "Epoch 71, Training Loss: 0.894717468584285\n",
      "Epoch 72, Training Loss: 0.8936941284993115\n",
      "Epoch 73, Training Loss: 0.8927018992339864\n",
      "Epoch 74, Training Loss: 0.8916957653270048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:29:03,778] Trial 345 finished with value: 0.5820666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8906822597279268\n",
      "Epoch 1, Training Loss: 0.903945773526242\n",
      "Epoch 2, Training Loss: 0.829313878457349\n",
      "Epoch 3, Training Loss: 0.8187680192011639\n",
      "Epoch 4, Training Loss: 0.8141975711162825\n",
      "Epoch 5, Training Loss: 0.8119083672537839\n",
      "Epoch 6, Training Loss: 0.8089065628392356\n",
      "Epoch 7, Training Loss: 0.8085390031785893\n",
      "Epoch 8, Training Loss: 0.8063043418683504\n",
      "Epoch 9, Training Loss: 0.8054565577578724\n",
      "Epoch 10, Training Loss: 0.8052782142072692\n",
      "Epoch 11, Training Loss: 0.8037418661260963\n",
      "Epoch 12, Training Loss: 0.8037291451504356\n",
      "Epoch 13, Training Loss: 0.8027138066471071\n",
      "Epoch 14, Training Loss: 0.8032964292325472\n",
      "Epoch 15, Training Loss: 0.8019720816074457\n",
      "Epoch 16, Training Loss: 0.803051615568032\n",
      "Epoch 17, Training Loss: 0.8027455081617025\n",
      "Epoch 18, Training Loss: 0.8020648391623246\n",
      "Epoch 19, Training Loss: 0.8016966828726289\n",
      "Epoch 20, Training Loss: 0.8031433142217478\n",
      "Epoch 21, Training Loss: 0.8011501360656623\n",
      "Epoch 22, Training Loss: 0.8024481730353563\n",
      "Epoch 23, Training Loss: 0.8017127854483468\n",
      "Epoch 24, Training Loss: 0.8012368573281998\n",
      "Epoch 25, Training Loss: 0.8007812257996179\n",
      "Epoch 26, Training Loss: 0.8000354186932843\n",
      "Epoch 27, Training Loss: 0.7997488737106323\n",
      "Epoch 28, Training Loss: 0.8005906233213897\n",
      "Epoch 29, Training Loss: 0.8012575872858664\n",
      "Epoch 30, Training Loss: 0.8002742992307906\n",
      "Epoch 31, Training Loss: 0.8000821450599154\n",
      "Epoch 32, Training Loss: 0.8003599400807144\n",
      "Epoch 33, Training Loss: 0.79893682325693\n",
      "Epoch 34, Training Loss: 0.8005478719123325\n",
      "Epoch 35, Training Loss: 0.7992724186495731\n",
      "Epoch 36, Training Loss: 0.7985185527263727\n",
      "Epoch 37, Training Loss: 0.7984803292088043\n",
      "Epoch 38, Training Loss: 0.8007199573337583\n",
      "Epoch 39, Training Loss: 0.7985321476943511\n",
      "Epoch 40, Training Loss: 0.7983727308144247\n",
      "Epoch 41, Training Loss: 0.7986519848493705\n",
      "Epoch 42, Training Loss: 0.7988995078811072\n",
      "Epoch 43, Training Loss: 0.7992110893242341\n",
      "Epoch 44, Training Loss: 0.7995411909612498\n",
      "Epoch 45, Training Loss: 0.798714346007297\n",
      "Epoch 46, Training Loss: 0.7989917872543622\n",
      "Epoch 47, Training Loss: 0.7983787921138276\n",
      "Epoch 48, Training Loss: 0.7995311193000105\n",
      "Epoch 49, Training Loss: 0.7986952144400518\n",
      "Epoch 50, Training Loss: 0.7983538798819807\n",
      "Epoch 51, Training Loss: 0.7982816044549297\n",
      "Epoch 52, Training Loss: 0.7977872697930587\n",
      "Epoch 53, Training Loss: 0.7983684892044928\n",
      "Epoch 54, Training Loss: 0.7980681782378289\n",
      "Epoch 55, Training Loss: 0.7989160388932193\n",
      "Epoch 56, Training Loss: 0.7992281454846375\n",
      "Epoch 57, Training Loss: 0.7982435094682794\n",
      "Epoch 58, Training Loss: 0.7978996534992878\n",
      "Epoch 59, Training Loss: 0.7987054623159251\n",
      "Epoch 60, Training Loss: 0.7987870872468876\n",
      "Epoch 61, Training Loss: 0.7984099291321031\n",
      "Epoch 62, Training Loss: 0.7974279384864004\n",
      "Epoch 63, Training Loss: 0.7978860381850623\n",
      "Epoch 64, Training Loss: 0.7975781940876093\n",
      "Epoch 65, Training Loss: 0.7982290182794843\n",
      "Epoch 66, Training Loss: 0.7972291803897772\n",
      "Epoch 67, Training Loss: 0.7988891619488709\n",
      "Epoch 68, Training Loss: 0.7970494989165686\n",
      "Epoch 69, Training Loss: 0.7975771731003782\n",
      "Epoch 70, Training Loss: 0.7983419209494627\n",
      "Epoch 71, Training Loss: 0.7983450656546686\n",
      "Epoch 72, Training Loss: 0.7974023011394014\n",
      "Epoch 73, Training Loss: 0.7968521036599812\n",
      "Epoch 74, Training Loss: 0.7976909684955625\n",
      "Epoch 75, Training Loss: 0.7972243699812351\n",
      "Epoch 76, Training Loss: 0.7982123872391264\n",
      "Epoch 77, Training Loss: 0.7980475630975308\n",
      "Epoch 78, Training Loss: 0.7984225614626604\n",
      "Epoch 79, Training Loss: 0.7973336280736708\n",
      "Epoch 80, Training Loss: 0.798395680305653\n",
      "Epoch 81, Training Loss: 0.7972194887641677\n",
      "Epoch 82, Training Loss: 0.7980748391689214\n",
      "Epoch 83, Training Loss: 0.7972384463575549\n",
      "Epoch 84, Training Loss: 0.7974354415907896\n",
      "Epoch 85, Training Loss: 0.7971790199889276\n",
      "Epoch 86, Training Loss: 0.7971875594970875\n",
      "Epoch 87, Training Loss: 0.797489569330574\n",
      "Epoch 88, Training Loss: 0.7967568742601495\n",
      "Epoch 89, Training Loss: 0.7965307961729237\n",
      "Epoch 90, Training Loss: 0.797167422179889\n",
      "Epoch 91, Training Loss: 0.7973461051632588\n",
      "Epoch 92, Training Loss: 0.7976699198099008\n",
      "Epoch 93, Training Loss: 0.7974240113021736\n",
      "Epoch 94, Training Loss: 0.796390690211963\n",
      "Epoch 95, Training Loss: 0.7969499573671728\n",
      "Epoch 96, Training Loss: 0.7964850779762842\n",
      "Epoch 97, Training Loss: 0.7975214356766608\n",
      "Epoch 98, Training Loss: 0.796745204656644\n",
      "Epoch 99, Training Loss: 0.7976821369694588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:31:00,106] Trial 346 finished with value: 0.6178666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7964128861301824\n",
      "Epoch 1, Training Loss: 1.0498440878531512\n",
      "Epoch 2, Training Loss: 1.0121302223205566\n",
      "Epoch 3, Training Loss: 0.987692771098193\n",
      "Epoch 4, Training Loss: 0.9730922753670637\n",
      "Epoch 5, Training Loss: 0.9649048093487235\n",
      "Epoch 6, Training Loss: 0.9604601728214938\n",
      "Epoch 7, Training Loss: 0.9578235044198877\n",
      "Epoch 8, Training Loss: 0.9559766402665306\n",
      "Epoch 9, Training Loss: 0.9545429334219764\n",
      "Epoch 10, Training Loss: 0.953225519026027\n",
      "Epoch 11, Training Loss: 0.9519161666140837\n",
      "Epoch 12, Training Loss: 0.9506244693082921\n",
      "Epoch 13, Training Loss: 0.9492808903666103\n",
      "Epoch 14, Training Loss: 0.9480421749984517\n",
      "Epoch 15, Training Loss: 0.9467092897611505\n",
      "Epoch 16, Training Loss: 0.945268056603039\n",
      "Epoch 17, Training Loss: 0.9439217850741218\n",
      "Epoch 18, Training Loss: 0.9424452960491181\n",
      "Epoch 19, Training Loss: 0.9409790618980632\n",
      "Epoch 20, Training Loss: 0.9395586417001837\n",
      "Epoch 21, Training Loss: 0.9380595194592195\n",
      "Epoch 22, Training Loss: 0.9364498296905966\n",
      "Epoch 23, Training Loss: 0.9348433609569774\n",
      "Epoch 24, Training Loss: 0.9330751112629385\n",
      "Epoch 25, Training Loss: 0.9314718526251176\n",
      "Epoch 26, Training Loss: 0.9297139162877027\n",
      "Epoch 27, Training Loss: 0.927890913065742\n",
      "Epoch 28, Training Loss: 0.926024480006274\n",
      "Epoch 29, Training Loss: 0.924156307332656\n",
      "Epoch 30, Training Loss: 0.9221206381741692\n",
      "Epoch 31, Training Loss: 0.9202989975845113\n",
      "Epoch 32, Training Loss: 0.9182452048273647\n",
      "Epoch 33, Training Loss: 0.9161721876088311\n",
      "Epoch 34, Training Loss: 0.9140429371244767\n",
      "Epoch 35, Training Loss: 0.9119655008877025\n",
      "Epoch 36, Training Loss: 0.9097475875125212\n",
      "Epoch 37, Training Loss: 0.9074720951388864\n",
      "Epoch 38, Training Loss: 0.9052658168708577\n",
      "Epoch 39, Training Loss: 0.9029446376071256\n",
      "Epoch 40, Training Loss: 0.9006149374036228\n",
      "Epoch 41, Training Loss: 0.8983422305303461\n",
      "Epoch 42, Training Loss: 0.895958698426976\n",
      "Epoch 43, Training Loss: 0.8935574429876664\n",
      "Epoch 44, Training Loss: 0.8912051681911244\n",
      "Epoch 45, Training Loss: 0.8887421193543602\n",
      "Epoch 46, Training Loss: 0.8863619338063633\n",
      "Epoch 47, Training Loss: 0.8839909185381497\n",
      "Epoch 48, Training Loss: 0.8815725263427285\n",
      "Epoch 49, Training Loss: 0.8792579644567826\n",
      "Epoch 50, Training Loss: 0.8768627921272727\n",
      "Epoch 51, Training Loss: 0.8745512706391951\n",
      "Epoch 52, Training Loss: 0.8722141517611111\n",
      "Epoch 53, Training Loss: 0.8698738736264846\n",
      "Epoch 54, Training Loss: 0.8676386350743911\n",
      "Epoch 55, Training Loss: 0.8653660218855914\n",
      "Epoch 56, Training Loss: 0.8632410555727341\n",
      "Epoch 57, Training Loss: 0.8610473781473497\n",
      "Epoch 58, Training Loss: 0.8589292316577014\n",
      "Epoch 59, Training Loss: 0.8568482478927163\n",
      "Epoch 60, Training Loss: 0.8548939393548405\n",
      "Epoch 61, Training Loss: 0.8528512535375707\n",
      "Epoch 62, Training Loss: 0.8509685873985291\n",
      "Epoch 63, Training Loss: 0.8491360602659338\n",
      "Epoch 64, Training Loss: 0.8473332151945899\n",
      "Epoch 65, Training Loss: 0.8455099044827854\n",
      "Epoch 66, Training Loss: 0.8438181440970477\n",
      "Epoch 67, Training Loss: 0.8421656717973597\n",
      "Epoch 68, Training Loss: 0.8406601474565618\n",
      "Epoch 69, Training Loss: 0.839118099493139\n",
      "Epoch 70, Training Loss: 0.8377042043910307\n",
      "Epoch 71, Training Loss: 0.8362773736084209\n",
      "Epoch 72, Training Loss: 0.8349299179105197\n",
      "Epoch 73, Training Loss: 0.8336011478480171\n",
      "Epoch 74, Training Loss: 0.8323922890074112\n",
      "Epoch 75, Training Loss: 0.8311482558530919\n",
      "Epoch 76, Training Loss: 0.830067072054919\n",
      "Epoch 77, Training Loss: 0.8289588306931889\n",
      "Epoch 78, Training Loss: 0.8278118376170888\n",
      "Epoch 79, Training Loss: 0.8269210084045635\n",
      "Epoch 80, Training Loss: 0.8260269831208622\n",
      "Epoch 81, Training Loss: 0.8252024487186881\n",
      "Epoch 82, Training Loss: 0.8242240989208222\n",
      "Epoch 83, Training Loss: 0.8235039144403794\n",
      "Epoch 84, Training Loss: 0.8226967933598687\n",
      "Epoch 85, Training Loss: 0.822030128030216\n",
      "Epoch 86, Training Loss: 0.8213195812702179\n",
      "Epoch 87, Training Loss: 0.8206216059011572\n",
      "Epoch 88, Training Loss: 0.8200138797479517\n",
      "Epoch 89, Training Loss: 0.8194519175501431\n",
      "Epoch 90, Training Loss: 0.8189317084059996\n",
      "Epoch 91, Training Loss: 0.8183894020669601\n",
      "Epoch 92, Training Loss: 0.8179215038523955\n",
      "Epoch 93, Training Loss: 0.8174431223729077\n",
      "Epoch 94, Training Loss: 0.8169867762397317\n",
      "Epoch 95, Training Loss: 0.8166393395732431\n",
      "Epoch 96, Training Loss: 0.8161874236780055\n",
      "Epoch 97, Training Loss: 0.8158219387250788\n",
      "Epoch 98, Training Loss: 0.8154299853829776\n",
      "Epoch 99, Training Loss: 0.815106094865238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:32:35,330] Trial 347 finished with value: 0.6262666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8147857265612658\n",
      "Epoch 1, Training Loss: 0.9452840983867645\n",
      "Epoch 2, Training Loss: 0.867322888584698\n",
      "Epoch 3, Training Loss: 0.8263940229836633\n",
      "Epoch 4, Training Loss: 0.814543946280199\n",
      "Epoch 5, Training Loss: 0.8114157085558947\n",
      "Epoch 6, Training Loss: 0.81001135987394\n",
      "Epoch 7, Training Loss: 0.8087743085272172\n",
      "Epoch 8, Training Loss: 0.8084147841790144\n",
      "Epoch 9, Training Loss: 0.8079295611381531\n",
      "Epoch 10, Training Loss: 0.807158258452135\n",
      "Epoch 11, Training Loss: 0.8067480841103722\n",
      "Epoch 12, Training Loss: 0.806311093007817\n",
      "Epoch 13, Training Loss: 0.8059325541468227\n",
      "Epoch 14, Training Loss: 0.8051946968892041\n",
      "Epoch 15, Training Loss: 0.8053885522309472\n",
      "Epoch 16, Training Loss: 0.8048060723613291\n",
      "Epoch 17, Training Loss: 0.8046877885566038\n",
      "Epoch 18, Training Loss: 0.804314090644612\n",
      "Epoch 19, Training Loss: 0.8041321340027977\n",
      "Epoch 20, Training Loss: 0.8037310354849871\n",
      "Epoch 21, Training Loss: 0.8036320205997018\n",
      "Epoch 22, Training Loss: 0.803047861141317\n",
      "Epoch 23, Training Loss: 0.8033335443805246\n",
      "Epoch 24, Training Loss: 0.8029139375686646\n",
      "Epoch 25, Training Loss: 0.8024221123667324\n",
      "Epoch 26, Training Loss: 0.802247259757098\n",
      "Epoch 27, Training Loss: 0.8020328805025886\n",
      "Epoch 28, Training Loss: 0.8017330123396481\n",
      "Epoch 29, Training Loss: 0.8015917781521292\n",
      "Epoch 30, Training Loss: 0.8015283786549288\n",
      "Epoch 31, Training Loss: 0.8014283366764293\n",
      "Epoch 32, Training Loss: 0.8009564607984879\n",
      "Epoch 33, Training Loss: 0.800771322180243\n",
      "Epoch 34, Training Loss: 0.8010021612924688\n",
      "Epoch 35, Training Loss: 0.8006202320491567\n",
      "Epoch 36, Training Loss: 0.8004446514213787\n",
      "Epoch 37, Training Loss: 0.800367690114414\n",
      "Epoch 38, Training Loss: 0.8001040193613838\n",
      "Epoch 39, Training Loss: 0.8000667685620925\n",
      "Epoch 40, Training Loss: 0.8001189357392928\n",
      "Epoch 41, Training Loss: 0.7998725917760063\n",
      "Epoch 42, Training Loss: 0.799747891846825\n",
      "Epoch 43, Training Loss: 0.8000503367536208\n",
      "Epoch 44, Training Loss: 0.7995796965851504\n",
      "Epoch 45, Training Loss: 0.7994338482968948\n",
      "Epoch 46, Training Loss: 0.7992088697237127\n",
      "Epoch 47, Training Loss: 0.7991248557848089\n",
      "Epoch 48, Training Loss: 0.7991324027846841\n",
      "Epoch 49, Training Loss: 0.7991541440346662\n",
      "Epoch 50, Training Loss: 0.7989653743014616\n",
      "Epoch 51, Training Loss: 0.7987948419066037\n",
      "Epoch 52, Training Loss: 0.7985312669417437\n",
      "Epoch 53, Training Loss: 0.7986731332891127\n",
      "Epoch 54, Training Loss: 0.798453904390335\n",
      "Epoch 55, Training Loss: 0.798660227060318\n",
      "Epoch 56, Training Loss: 0.7983705075348124\n",
      "Epoch 57, Training Loss: 0.7983333298038033\n",
      "Epoch 58, Training Loss: 0.7985502186943503\n",
      "Epoch 59, Training Loss: 0.7982645100705764\n",
      "Epoch 60, Training Loss: 0.7983940846078537\n",
      "Epoch 61, Training Loss: 0.798077539135428\n",
      "Epoch 62, Training Loss: 0.7984229907568763\n",
      "Epoch 63, Training Loss: 0.7977854917329901\n",
      "Epoch 64, Training Loss: 0.7979697093542885\n",
      "Epoch 65, Training Loss: 0.7977055920572842\n",
      "Epoch 66, Training Loss: 0.7979742268253776\n",
      "Epoch 67, Training Loss: 0.7979684683855842\n",
      "Epoch 68, Training Loss: 0.7974258516115301\n",
      "Epoch 69, Training Loss: 0.7980332751835094\n",
      "Epoch 70, Training Loss: 0.7981392279092003\n",
      "Epoch 71, Training Loss: 0.797860136943705\n",
      "Epoch 72, Training Loss: 0.7974191339576946\n",
      "Epoch 73, Training Loss: 0.7976010774163639\n",
      "Epoch 74, Training Loss: 0.7977543273392845\n",
      "Epoch 75, Training Loss: 0.7975100212938645\n",
      "Epoch 76, Training Loss: 0.7978588066381567\n",
      "Epoch 77, Training Loss: 0.7977741103312549\n",
      "Epoch 78, Training Loss: 0.7975584823944989\n",
      "Epoch 79, Training Loss: 0.7978422586356892\n",
      "Epoch 80, Training Loss: 0.7972139223884134\n",
      "Epoch 81, Training Loss: 0.7977734516648686\n",
      "Epoch 82, Training Loss: 0.7972972298369688\n",
      "Epoch 83, Training Loss: 0.7976820752901189\n",
      "Epoch 84, Training Loss: 0.7971240204923293\n",
      "Epoch 85, Training Loss: 0.7971798029366661\n",
      "Epoch 86, Training Loss: 0.797285385201959\n",
      "Epoch 87, Training Loss: 0.7973599824484657\n",
      "Epoch 88, Training Loss: 0.7972543061480802\n",
      "Epoch 89, Training Loss: 0.7971189347435447\n",
      "Epoch 90, Training Loss: 0.7973222832819995\n",
      "Epoch 91, Training Loss: 0.7969238043532652\n",
      "Epoch 92, Training Loss: 0.7972730802087222\n",
      "Epoch 93, Training Loss: 0.7969194393999436\n",
      "Epoch 94, Training Loss: 0.7972935662550085\n",
      "Epoch 95, Training Loss: 0.7973223690425648\n",
      "Epoch 96, Training Loss: 0.7968686164126677\n",
      "Epoch 97, Training Loss: 0.7968920122875887\n",
      "Epoch 98, Training Loss: 0.7968325283948113\n",
      "Epoch 99, Training Loss: 0.7967908470770892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:34:33,989] Trial 348 finished with value: 0.6344 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7967211470183204\n",
      "Epoch 1, Training Loss: 0.8836836366725147\n",
      "Epoch 2, Training Loss: 0.8161464305748617\n",
      "Epoch 3, Training Loss: 0.8065136406654702\n",
      "Epoch 4, Training Loss: 0.8032221949190126\n",
      "Epoch 5, Training Loss: 0.8009280199395087\n",
      "Epoch 6, Training Loss: 0.7992926061601567\n",
      "Epoch 7, Training Loss: 0.7970636958466437\n",
      "Epoch 8, Training Loss: 0.7953641329492841\n",
      "Epoch 9, Training Loss: 0.7943716775205799\n",
      "Epoch 10, Training Loss: 0.793452603297126\n",
      "Epoch 11, Training Loss: 0.7921890637928382\n",
      "Epoch 12, Training Loss: 0.7910149069657003\n",
      "Epoch 13, Training Loss: 0.79051310693411\n",
      "Epoch 14, Training Loss: 0.7900821324577905\n",
      "Epoch 15, Training Loss: 0.7890873767379532\n",
      "Epoch 16, Training Loss: 0.7882777718673075\n",
      "Epoch 17, Training Loss: 0.7880887263699582\n",
      "Epoch 18, Training Loss: 0.7877483743474\n",
      "Epoch 19, Training Loss: 0.7870191865397576\n",
      "Epoch 20, Training Loss: 0.7866144260069481\n",
      "Epoch 21, Training Loss: 0.7860974630915133\n",
      "Epoch 22, Training Loss: 0.7862483626917789\n",
      "Epoch 23, Training Loss: 0.7855910491226311\n",
      "Epoch 24, Training Loss: 0.7853047640700089\n",
      "Epoch 25, Training Loss: 0.7847904859628893\n",
      "Epoch 26, Training Loss: 0.7850280587834523\n",
      "Epoch 27, Training Loss: 0.7836517938993928\n",
      "Epoch 28, Training Loss: 0.7833750312489675\n",
      "Epoch 29, Training Loss: 0.7832140605252488\n",
      "Epoch 30, Training Loss: 0.7837245840775339\n",
      "Epoch 31, Training Loss: 0.7826919244644337\n",
      "Epoch 32, Training Loss: 0.7828791330631514\n",
      "Epoch 33, Training Loss: 0.7827014467769996\n",
      "Epoch 34, Training Loss: 0.7819295724531762\n",
      "Epoch 35, Training Loss: 0.7816786563486084\n",
      "Epoch 36, Training Loss: 0.7815821477345057\n",
      "Epoch 37, Training Loss: 0.7815352124379094\n",
      "Epoch 38, Training Loss: 0.7816450866541468\n",
      "Epoch 39, Training Loss: 0.7811421032238723\n",
      "Epoch 40, Training Loss: 0.7808165433711576\n",
      "Epoch 41, Training Loss: 0.781238127292547\n",
      "Epoch 42, Training Loss: 0.7809958401479219\n",
      "Epoch 43, Training Loss: 0.7800522278126021\n",
      "Epoch 44, Training Loss: 0.7802180691769248\n",
      "Epoch 45, Training Loss: 0.7804654301557326\n",
      "Epoch 46, Training Loss: 0.780967386593496\n",
      "Epoch 47, Training Loss: 0.7802031093970277\n",
      "Epoch 48, Training Loss: 0.7798802647375522\n",
      "Epoch 49, Training Loss: 0.7793013571796561\n",
      "Epoch 50, Training Loss: 0.7797822410002687\n",
      "Epoch 51, Training Loss: 0.7798384878868447\n",
      "Epoch 52, Training Loss: 0.77976919591875\n",
      "Epoch 53, Training Loss: 0.7790927618069756\n",
      "Epoch 54, Training Loss: 0.778938124950667\n",
      "Epoch 55, Training Loss: 0.7788146572901791\n",
      "Epoch 56, Training Loss: 0.7797572940812075\n",
      "Epoch 57, Training Loss: 0.7787833817919394\n",
      "Epoch 58, Training Loss: 0.7790398519738276\n",
      "Epoch 59, Training Loss: 0.7788138731977994\n",
      "Epoch 60, Training Loss: 0.7785820148941269\n",
      "Epoch 61, Training Loss: 0.7783888746921281\n",
      "Epoch 62, Training Loss: 0.7786629312020495\n",
      "Epoch 63, Training Loss: 0.7784231544437265\n",
      "Epoch 64, Training Loss: 0.7779296158847953\n",
      "Epoch 65, Training Loss: 0.7781330630295259\n",
      "Epoch 66, Training Loss: 0.7785553998516914\n",
      "Epoch 67, Training Loss: 0.7775640188751364\n",
      "Epoch 68, Training Loss: 0.7781279816663355\n",
      "Epoch 69, Training Loss: 0.7784683238294788\n",
      "Epoch 70, Training Loss: 0.7784825656647073\n",
      "Epoch 71, Training Loss: 0.7781399664125944\n",
      "Epoch 72, Training Loss: 0.7778619367377202\n",
      "Epoch 73, Training Loss: 0.7775144706991383\n",
      "Epoch 74, Training Loss: 0.7784284372975055\n",
      "Epoch 75, Training Loss: 0.777044681767772\n",
      "Epoch 76, Training Loss: 0.7768722030453216\n",
      "Epoch 77, Training Loss: 0.7768768059579949\n",
      "Epoch 78, Training Loss: 0.7770621658267831\n",
      "Epoch 79, Training Loss: 0.7779499307610935\n",
      "Epoch 80, Training Loss: 0.7772328785487583\n",
      "Epoch 81, Training Loss: 0.7774154637092935\n",
      "Epoch 82, Training Loss: 0.7769179227656888\n",
      "Epoch 83, Training Loss: 0.777026879697814\n",
      "Epoch 84, Training Loss: 0.7771213118295024\n",
      "Epoch 85, Training Loss: 0.7765269894797103\n",
      "Epoch 86, Training Loss: 0.7768386262700073\n",
      "Epoch 87, Training Loss: 0.7768792619382529\n",
      "Epoch 88, Training Loss: 0.7763794861789933\n",
      "Epoch 89, Training Loss: 0.7765866168459555\n",
      "Epoch 90, Training Loss: 0.7768113601476626\n",
      "Epoch 91, Training Loss: 0.7763712021641265\n",
      "Epoch 92, Training Loss: 0.776291443082623\n",
      "Epoch 93, Training Loss: 0.7761787140279784\n",
      "Epoch 94, Training Loss: 0.7762823302943007\n",
      "Epoch 95, Training Loss: 0.7761457575891251\n",
      "Epoch 96, Training Loss: 0.7763716990786388\n",
      "Epoch 97, Training Loss: 0.775820529550538\n",
      "Epoch 98, Training Loss: 0.7757689606874509\n",
      "Epoch 99, Training Loss: 0.7765119972085595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:36:26,994] Trial 349 finished with value: 0.6363333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7766934497015817\n",
      "Epoch 1, Training Loss: 1.090585618807857\n",
      "Epoch 2, Training Loss: 1.0876121223421025\n",
      "Epoch 3, Training Loss: 1.085069889771311\n",
      "Epoch 4, Training Loss: 1.0823177319720276\n",
      "Epoch 5, Training Loss: 1.078817917888326\n",
      "Epoch 6, Training Loss: 1.0750087013818268\n",
      "Epoch 7, Training Loss: 1.0700757815425557\n",
      "Epoch 8, Training Loss: 1.064131048209685\n",
      "Epoch 9, Training Loss: 1.0569457559657276\n",
      "Epoch 10, Training Loss: 1.0484181402321149\n",
      "Epoch 11, Training Loss: 1.0386198448059254\n",
      "Epoch 12, Training Loss: 1.027566957832279\n",
      "Epoch 13, Training Loss: 1.0163771142636924\n",
      "Epoch 14, Training Loss: 1.0054338451614953\n",
      "Epoch 15, Training Loss: 0.9955023799623762\n",
      "Epoch 16, Training Loss: 0.986636307365016\n",
      "Epoch 17, Training Loss: 0.9796535158515873\n",
      "Epoch 18, Training Loss: 0.9739947972441079\n",
      "Epoch 19, Training Loss: 0.9691805805478777\n",
      "Epoch 20, Training Loss: 0.9656861914727921\n",
      "Epoch 21, Training Loss: 0.962460498254102\n",
      "Epoch 22, Training Loss: 0.9599441468267512\n",
      "Epoch 23, Training Loss: 0.9581841219636731\n",
      "Epoch 24, Training Loss: 0.9568962561456781\n",
      "Epoch 25, Training Loss: 0.9555922611315447\n",
      "Epoch 26, Training Loss: 0.954627135373596\n",
      "Epoch 27, Training Loss: 0.9532505864487555\n",
      "Epoch 28, Training Loss: 0.952578986677012\n",
      "Epoch 29, Training Loss: 0.9519715586102995\n",
      "Epoch 30, Training Loss: 0.9510950240873752\n",
      "Epoch 31, Training Loss: 0.9506388605985426\n",
      "Epoch 32, Training Loss: 0.9500529946241164\n",
      "Epoch 33, Training Loss: 0.9492522096275386\n",
      "Epoch 34, Training Loss: 0.9485871578517713\n",
      "Epoch 35, Training Loss: 0.9478562837256524\n",
      "Epoch 36, Training Loss: 0.9470453863753412\n",
      "Epoch 37, Training Loss: 0.946205050604684\n",
      "Epoch 38, Training Loss: 0.9458178600870577\n",
      "Epoch 39, Training Loss: 0.9448216374655415\n",
      "Epoch 40, Training Loss: 0.9439580857305598\n",
      "Epoch 41, Training Loss: 0.9428195926479828\n",
      "Epoch 42, Training Loss: 0.9422061924647568\n",
      "Epoch 43, Training Loss: 0.9408419586662063\n",
      "Epoch 44, Training Loss: 0.9405338358162041\n",
      "Epoch 45, Training Loss: 0.939734170938793\n",
      "Epoch 46, Training Loss: 0.9381824350894842\n",
      "Epoch 47, Training Loss: 0.9380779038694568\n",
      "Epoch 48, Training Loss: 0.9366035424677053\n",
      "Epoch 49, Training Loss: 0.9358622515111937\n",
      "Epoch 50, Training Loss: 0.9340433075015706\n",
      "Epoch 51, Training Loss: 0.9329796443308206\n",
      "Epoch 52, Training Loss: 0.9318167016918498\n",
      "Epoch 53, Training Loss: 0.9306454246205494\n",
      "Epoch 54, Training Loss: 0.9295689532631322\n",
      "Epoch 55, Training Loss: 0.9277432973223522\n",
      "Epoch 56, Training Loss: 0.9264604601644932\n",
      "Epoch 57, Training Loss: 0.9251812731413017\n",
      "Epoch 58, Training Loss: 0.9233903758507922\n",
      "Epoch 59, Training Loss: 0.9220537796056361\n",
      "Epoch 60, Training Loss: 0.9201356471929335\n",
      "Epoch 61, Training Loss: 0.9182913845643065\n",
      "Epoch 62, Training Loss: 0.9167172784195807\n",
      "Epoch 63, Training Loss: 0.9150012767404542\n",
      "Epoch 64, Training Loss: 0.9130636461695334\n",
      "Epoch 65, Training Loss: 0.9110684161795709\n",
      "Epoch 66, Training Loss: 0.9086060312457551\n",
      "Epoch 67, Training Loss: 0.9069647161584151\n",
      "Epoch 68, Training Loss: 0.9045205283882026\n",
      "Epoch 69, Training Loss: 0.9021211023617508\n",
      "Epoch 70, Training Loss: 0.8993501297513345\n",
      "Epoch 71, Training Loss: 0.897167862626843\n",
      "Epoch 72, Training Loss: 0.8946938203689747\n",
      "Epoch 73, Training Loss: 0.8924603878107286\n",
      "Epoch 74, Training Loss: 0.8898844284222538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:37:37,170] Trial 350 finished with value: 0.5799333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8877813859093459\n",
      "Epoch 1, Training Loss: 1.0854415422095391\n",
      "Epoch 2, Training Loss: 1.0773994816873307\n",
      "Epoch 3, Training Loss: 1.0664024768915392\n",
      "Epoch 4, Training Loss: 1.0502838595469195\n",
      "Epoch 5, Training Loss: 1.0288261345454626\n",
      "Epoch 6, Training Loss: 1.0066730395295567\n",
      "Epoch 7, Training Loss: 0.9897150114066618\n",
      "Epoch 8, Training Loss: 0.9786442976248891\n",
      "Epoch 9, Training Loss: 0.9710957312942448\n",
      "Epoch 10, Training Loss: 0.9655197494908383\n",
      "Epoch 11, Training Loss: 0.9620977077269016\n",
      "Epoch 12, Training Loss: 0.9594152861968019\n",
      "Epoch 13, Training Loss: 0.9575273231456154\n",
      "Epoch 14, Training Loss: 0.956074953706641\n",
      "Epoch 15, Training Loss: 0.9547887486622746\n",
      "Epoch 16, Training Loss: 0.9537360431556415\n",
      "Epoch 17, Training Loss: 0.952550963172339\n",
      "Epoch 18, Training Loss: 0.951416638112606\n",
      "Epoch 19, Training Loss: 0.9500061827494686\n",
      "Epoch 20, Training Loss: 0.948031102804313\n",
      "Epoch 21, Training Loss: 0.9468112906118981\n",
      "Epoch 22, Training Loss: 0.9456700333079001\n",
      "Epoch 23, Training Loss: 0.9441079059937842\n",
      "Epoch 24, Training Loss: 0.9427359572926858\n",
      "Epoch 25, Training Loss: 0.9407801484703121\n",
      "Epoch 26, Training Loss: 0.9386892655738315\n",
      "Epoch 27, Training Loss: 0.9363641852723028\n",
      "Epoch 28, Training Loss: 0.9343117990888151\n",
      "Epoch 29, Training Loss: 0.931898279925038\n",
      "Epoch 30, Training Loss: 0.9293996876343749\n",
      "Epoch 31, Training Loss: 0.927312469661684\n",
      "Epoch 32, Training Loss: 0.9242123524049171\n",
      "Epoch 33, Training Loss: 0.9206117587878292\n",
      "Epoch 34, Training Loss: 0.917168852978183\n",
      "Epoch 35, Training Loss: 0.9136659527183475\n",
      "Epoch 36, Training Loss: 0.9095761805548703\n",
      "Epoch 37, Training Loss: 0.9047763412160085\n",
      "Epoch 38, Training Loss: 0.9001679341595872\n",
      "Epoch 39, Training Loss: 0.8951953619046319\n",
      "Epoch 40, Training Loss: 0.8903587839657203\n",
      "Epoch 41, Training Loss: 0.8846412290307812\n",
      "Epoch 42, Training Loss: 0.8796602724190045\n",
      "Epoch 43, Training Loss: 0.8744539279686777\n",
      "Epoch 44, Training Loss: 0.8690803623737249\n",
      "Epoch 45, Training Loss: 0.8639343172984015\n",
      "Epoch 46, Training Loss: 0.8594525534407537\n",
      "Epoch 47, Training Loss: 0.854596651944899\n",
      "Epoch 48, Training Loss: 0.8498930862971714\n",
      "Epoch 49, Training Loss: 0.8461612980168565\n",
      "Epoch 50, Training Loss: 0.8424795365871344\n",
      "Epoch 51, Training Loss: 0.8393758045999627\n",
      "Epoch 52, Training Loss: 0.8359786531082669\n",
      "Epoch 53, Training Loss: 0.8338576092755884\n",
      "Epoch 54, Training Loss: 0.8310290808964492\n",
      "Epoch 55, Training Loss: 0.8291426515220699\n",
      "Epoch 56, Training Loss: 0.8278049217130905\n",
      "Epoch 57, Training Loss: 0.8263611207331033\n",
      "Epoch 58, Training Loss: 0.8246365565106385\n",
      "Epoch 59, Training Loss: 0.8229032115828722\n",
      "Epoch 60, Training Loss: 0.8222666683949922\n",
      "Epoch 61, Training Loss: 0.8214994253968834\n",
      "Epoch 62, Training Loss: 0.8204533761605284\n",
      "Epoch 63, Training Loss: 0.8201206908190161\n",
      "Epoch 64, Training Loss: 0.8185789013267459\n",
      "Epoch 65, Training Loss: 0.8176450773289329\n",
      "Epoch 66, Training Loss: 0.8178142458872688\n",
      "Epoch 67, Training Loss: 0.8161335723740714\n",
      "Epoch 68, Training Loss: 0.8168779629513734\n",
      "Epoch 69, Training Loss: 0.8149627020036367\n",
      "Epoch 70, Training Loss: 0.8152423344160381\n",
      "Epoch 71, Training Loss: 0.8147697063316977\n",
      "Epoch 72, Training Loss: 0.8136185300081296\n",
      "Epoch 73, Training Loss: 0.8136383593530583\n",
      "Epoch 74, Training Loss: 0.8128863604445207\n",
      "Epoch 75, Training Loss: 0.8129789506582389\n",
      "Epoch 76, Training Loss: 0.812332852980248\n",
      "Epoch 77, Training Loss: 0.8116322601648202\n",
      "Epoch 78, Training Loss: 0.8117534026167447\n",
      "Epoch 79, Training Loss: 0.8115935199242785\n",
      "Epoch 80, Training Loss: 0.8115299071584429\n",
      "Epoch 81, Training Loss: 0.811147665977478\n",
      "Epoch 82, Training Loss: 0.8109662470064665\n",
      "Epoch 83, Training Loss: 0.810620775975679\n",
      "Epoch 84, Training Loss: 0.8101458464350019\n",
      "Epoch 85, Training Loss: 0.8098943057813143\n",
      "Epoch 86, Training Loss: 0.8100143273073928\n",
      "Epoch 87, Training Loss: 0.8094132871556102\n",
      "Epoch 88, Training Loss: 0.8090824180079582\n",
      "Epoch 89, Training Loss: 0.8090385512301796\n",
      "Epoch 90, Training Loss: 0.8094307681671659\n",
      "Epoch 91, Training Loss: 0.8084058345708632\n",
      "Epoch 92, Training Loss: 0.8092467108167204\n",
      "Epoch 93, Training Loss: 0.8088056151131938\n",
      "Epoch 94, Training Loss: 0.8082713340458118\n",
      "Epoch 95, Training Loss: 0.8088196946266002\n",
      "Epoch 96, Training Loss: 0.8078894713767489\n",
      "Epoch 97, Training Loss: 0.808064307485308\n",
      "Epoch 98, Training Loss: 0.8079108795725314\n",
      "Epoch 99, Training Loss: 0.807414976725901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:39:10,987] Trial 351 finished with value: 0.6123333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8074944689757841\n",
      "Epoch 1, Training Loss: 0.9865382069931891\n",
      "Epoch 2, Training Loss: 0.9554150809022717\n",
      "Epoch 3, Training Loss: 0.9491898186224744\n",
      "Epoch 4, Training Loss: 0.9423586750389041\n",
      "Epoch 5, Training Loss: 0.9354877846581595\n",
      "Epoch 6, Training Loss: 0.9290643079836566\n",
      "Epoch 7, Training Loss: 0.9213229079891865\n",
      "Epoch 8, Training Loss: 0.9146121271570823\n",
      "Epoch 9, Training Loss: 0.9071389678725623\n",
      "Epoch 10, Training Loss: 0.899900690236486\n",
      "Epoch 11, Training Loss: 0.8920907132607654\n",
      "Epoch 12, Training Loss: 0.8849673827787987\n",
      "Epoch 13, Training Loss: 0.8775433516143856\n",
      "Epoch 14, Training Loss: 0.8705800487582845\n",
      "Epoch 15, Training Loss: 0.8640275068749163\n",
      "Epoch 16, Training Loss: 0.8573568716085047\n",
      "Epoch 17, Training Loss: 0.8515360255886738\n",
      "Epoch 18, Training Loss: 0.8457280753250409\n",
      "Epoch 19, Training Loss: 0.841159933491757\n",
      "Epoch 20, Training Loss: 0.8367395925342589\n",
      "Epoch 21, Training Loss: 0.8323119703092073\n",
      "Epoch 22, Training Loss: 0.829451462559234\n",
      "Epoch 23, Training Loss: 0.8265373863671955\n",
      "Epoch 24, Training Loss: 0.8234350203571463\n",
      "Epoch 25, Training Loss: 0.8211863115317839\n",
      "Epoch 26, Training Loss: 0.8197933046441329\n",
      "Epoch 27, Training Loss: 0.8179940155574253\n",
      "Epoch 28, Training Loss: 0.8166919132820646\n",
      "Epoch 29, Training Loss: 0.8152889218545498\n",
      "Epoch 30, Training Loss: 0.8142757301043747\n",
      "Epoch 31, Training Loss: 0.8137416229212194\n",
      "Epoch 32, Training Loss: 0.8124906212763678\n",
      "Epoch 33, Training Loss: 0.811685898250207\n",
      "Epoch 34, Training Loss: 0.8118357324958744\n",
      "Epoch 35, Training Loss: 0.8111370935475916\n",
      "Epoch 36, Training Loss: 0.810287634770673\n",
      "Epoch 37, Training Loss: 0.8097851428770482\n",
      "Epoch 38, Training Loss: 0.8094449923450785\n",
      "Epoch 39, Training Loss: 0.8088931861676668\n",
      "Epoch 40, Training Loss: 0.8092390962113115\n",
      "Epoch 41, Training Loss: 0.8090081279439137\n",
      "Epoch 42, Training Loss: 0.8087129004019543\n",
      "Epoch 43, Training Loss: 0.8082388776585572\n",
      "Epoch 44, Training Loss: 0.8075028210654295\n",
      "Epoch 45, Training Loss: 0.8081259362679675\n",
      "Epoch 46, Training Loss: 0.807529017710148\n",
      "Epoch 47, Training Loss: 0.807446881434075\n",
      "Epoch 48, Training Loss: 0.807807537577206\n",
      "Epoch 49, Training Loss: 0.8071293534192824\n",
      "Epoch 50, Training Loss: 0.8073725732645594\n",
      "Epoch 51, Training Loss: 0.8068622857108152\n",
      "Epoch 52, Training Loss: 0.8067642948681251\n",
      "Epoch 53, Training Loss: 0.806208409626681\n",
      "Epoch 54, Training Loss: 0.8071534342335579\n",
      "Epoch 55, Training Loss: 0.8065816064526264\n",
      "Epoch 56, Training Loss: 0.8061083876100698\n",
      "Epoch 57, Training Loss: 0.8058655771994053\n",
      "Epoch 58, Training Loss: 0.8066400583525349\n",
      "Epoch 59, Training Loss: 0.8062656479670589\n",
      "Epoch 60, Training Loss: 0.8056413428227704\n",
      "Epoch 61, Training Loss: 0.805522806931259\n",
      "Epoch 62, Training Loss: 0.804855647839998\n",
      "Epoch 63, Training Loss: 0.8053457902786427\n",
      "Epoch 64, Training Loss: 0.8054860309550637\n",
      "Epoch 65, Training Loss: 0.8046928684514268\n",
      "Epoch 66, Training Loss: 0.8048928778870661\n",
      "Epoch 67, Training Loss: 0.8046867279181803\n",
      "Epoch 68, Training Loss: 0.8051535399336563\n",
      "Epoch 69, Training Loss: 0.8042716167026893\n",
      "Epoch 70, Training Loss: 0.8041453595448257\n",
      "Epoch 71, Training Loss: 0.8046008742841563\n",
      "Epoch 72, Training Loss: 0.8035041478791631\n",
      "Epoch 73, Training Loss: 0.8041721170109913\n",
      "Epoch 74, Training Loss: 0.8037687749790966\n",
      "Epoch 75, Training Loss: 0.8039506556396198\n",
      "Epoch 76, Training Loss: 0.8040697860538512\n",
      "Epoch 77, Training Loss: 0.8034172326998603\n",
      "Epoch 78, Training Loss: 0.803258644817467\n",
      "Epoch 79, Training Loss: 0.8031570393340032\n",
      "Epoch 80, Training Loss: 0.8031906844081735\n",
      "Epoch 81, Training Loss: 0.803155145340396\n",
      "Epoch 82, Training Loss: 0.803699487044399\n",
      "Epoch 83, Training Loss: 0.8027479681753574\n",
      "Epoch 84, Training Loss: 0.8030901435622595\n",
      "Epoch 85, Training Loss: 0.8025117805129603\n",
      "Epoch 86, Training Loss: 0.802614266442177\n",
      "Epoch 87, Training Loss: 0.8031806952971264\n",
      "Epoch 88, Training Loss: 0.8020709124722876\n",
      "Epoch 89, Training Loss: 0.8023244613095334\n",
      "Epoch 90, Training Loss: 0.801996850519252\n",
      "Epoch 91, Training Loss: 0.8019272469936457\n",
      "Epoch 92, Training Loss: 0.8018530476362186\n",
      "Epoch 93, Training Loss: 0.8016073060214968\n",
      "Epoch 94, Training Loss: 0.8017460912690126\n",
      "Epoch 95, Training Loss: 0.8014431705152182\n",
      "Epoch 96, Training Loss: 0.8016117933997534\n",
      "Epoch 97, Training Loss: 0.8015821471250146\n",
      "Epoch 98, Training Loss: 0.8024327807856682\n",
      "Epoch 99, Training Loss: 0.8017561206243988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:40:33,493] Trial 352 finished with value: 0.6336666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8023427998212943\n",
      "Epoch 1, Training Loss: 0.9813629410547369\n",
      "Epoch 2, Training Loss: 0.9405106235952938\n",
      "Epoch 3, Training Loss: 0.9198393100149491\n",
      "Epoch 4, Training Loss: 0.8947638802668627\n",
      "Epoch 5, Training Loss: 0.8693049051481134\n",
      "Epoch 6, Training Loss: 0.8487518121915705\n",
      "Epoch 7, Training Loss: 0.8342518521056456\n",
      "Epoch 8, Training Loss: 0.825008410986732\n",
      "Epoch 9, Training Loss: 0.8194421315894408\n",
      "Epoch 10, Training Loss: 0.815766690198113\n",
      "Epoch 11, Training Loss: 0.813425403062035\n",
      "Epoch 12, Training Loss: 0.8119760572910308\n",
      "Epoch 13, Training Loss: 0.8108181041829726\n",
      "Epoch 14, Training Loss: 0.8102495849132538\n",
      "Epoch 15, Training Loss: 0.8096351600394529\n",
      "Epoch 16, Training Loss: 0.8090977144942564\n",
      "Epoch 17, Training Loss: 0.8085707790711347\n",
      "Epoch 18, Training Loss: 0.8079704551135792\n",
      "Epoch 19, Training Loss: 0.8076873976342819\n",
      "Epoch 20, Training Loss: 0.8074354474684772\n",
      "Epoch 21, Training Loss: 0.807438442847308\n",
      "Epoch 22, Training Loss: 0.8069854875873117\n",
      "Epoch 23, Training Loss: 0.8067927948867574\n",
      "Epoch 24, Training Loss: 0.806536255864536\n",
      "Epoch 25, Training Loss: 0.8064009862086352\n",
      "Epoch 26, Training Loss: 0.8061978039320777\n",
      "Epoch 27, Training Loss: 0.8063194500698763\n",
      "Epoch 28, Training Loss: 0.8060251237364376\n",
      "Epoch 29, Training Loss: 0.8059747413326712\n",
      "Epoch 30, Training Loss: 0.8057095551490784\n",
      "Epoch 31, Training Loss: 0.8054698390119216\n",
      "Epoch 32, Training Loss: 0.8055797560775981\n",
      "Epoch 33, Training Loss: 0.8049766357506023\n",
      "Epoch 34, Training Loss: 0.8050329518318177\n",
      "Epoch 35, Training Loss: 0.8048309419435613\n",
      "Epoch 36, Training Loss: 0.8046262426937327\n",
      "Epoch 37, Training Loss: 0.8046015691757202\n",
      "Epoch 38, Training Loss: 0.8043691450006821\n",
      "Epoch 39, Training Loss: 0.8041768130134134\n",
      "Epoch 40, Training Loss: 0.8041273623354295\n",
      "Epoch 41, Training Loss: 0.8039828225444345\n",
      "Epoch 42, Training Loss: 0.8042146407856661\n",
      "Epoch 43, Training Loss: 0.8037523367825676\n",
      "Epoch 44, Training Loss: 0.8035963330549353\n",
      "Epoch 45, Training Loss: 0.8034497250528897\n",
      "Epoch 46, Training Loss: 0.8033314226655399\n",
      "Epoch 47, Training Loss: 0.8034383806060342\n",
      "Epoch 48, Training Loss: 0.803017173795139\n",
      "Epoch 49, Training Loss: 0.8030242064419915\n",
      "Epoch 50, Training Loss: 0.8027749577690574\n",
      "Epoch 51, Training Loss: 0.8028297190806445\n",
      "Epoch 52, Training Loss: 0.8024699270024019\n",
      "Epoch 53, Training Loss: 0.8025087892307955\n",
      "Epoch 54, Training Loss: 0.8023936782864963\n",
      "Epoch 55, Training Loss: 0.802346381299636\n",
      "Epoch 56, Training Loss: 0.8021109996122472\n",
      "Epoch 57, Training Loss: 0.8023090415141162\n",
      "Epoch 58, Training Loss: 0.8018217032797197\n",
      "Epoch 59, Training Loss: 0.8018775066207436\n",
      "Epoch 60, Training Loss: 0.8016839411679436\n",
      "Epoch 61, Training Loss: 0.80156659042134\n",
      "Epoch 62, Training Loss: 0.8013163567991818\n",
      "Epoch 63, Training Loss: 0.8014975070252138\n",
      "Epoch 64, Training Loss: 0.8012199043526369\n",
      "Epoch 65, Training Loss: 0.801132321357727\n",
      "Epoch 66, Training Loss: 0.8011602888387792\n",
      "Epoch 67, Training Loss: 0.8011376762390137\n",
      "Epoch 68, Training Loss: 0.8011993296006147\n",
      "Epoch 69, Training Loss: 0.800798319087309\n",
      "Epoch 70, Training Loss: 0.8008989967318142\n",
      "Epoch 71, Training Loss: 0.8009779266750111\n",
      "Epoch 72, Training Loss: 0.8006145185582778\n",
      "Epoch 73, Training Loss: 0.8003847749092999\n",
      "Epoch 74, Training Loss: 0.8003642446854535\n",
      "Epoch 75, Training Loss: 0.800487068120171\n",
      "Epoch 76, Training Loss: 0.8003327661402085\n",
      "Epoch 77, Training Loss: 0.8004914715710808\n",
      "Epoch 78, Training Loss: 0.8004421620509203\n",
      "Epoch 79, Training Loss: 0.800301555815865\n",
      "Epoch 80, Training Loss: 0.8000427406675675\n",
      "Epoch 81, Training Loss: 0.799832629105624\n",
      "Epoch 82, Training Loss: 0.8000818594764261\n",
      "Epoch 83, Training Loss: 0.7999527260135202\n",
      "Epoch 84, Training Loss: 0.7999077060643365\n",
      "Epoch 85, Training Loss: 0.79986324541709\n",
      "Epoch 86, Training Loss: 0.799493554059197\n",
      "Epoch 87, Training Loss: 0.7996360961829915\n",
      "Epoch 88, Training Loss: 0.7996628588087419\n",
      "Epoch 89, Training Loss: 0.7998206441542681\n",
      "Epoch 90, Training Loss: 0.7992396426200866\n",
      "Epoch 91, Training Loss: 0.7993781497899224\n",
      "Epoch 92, Training Loss: 0.7992644542806289\n",
      "Epoch 93, Training Loss: 0.799037928721484\n",
      "Epoch 94, Training Loss: 0.7991461821163401\n",
      "Epoch 95, Training Loss: 0.7991348025378059\n",
      "Epoch 96, Training Loss: 0.7990878138121437\n",
      "Epoch 97, Training Loss: 0.7988406058620005\n",
      "Epoch 98, Training Loss: 0.7989910288418041\n",
      "Epoch 99, Training Loss: 0.7990047362972709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:42:31,735] Trial 353 finished with value: 0.6342 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7988924684945274\n",
      "Epoch 1, Training Loss: 0.8603731801229365\n",
      "Epoch 2, Training Loss: 0.8166290475340451\n",
      "Epoch 3, Training Loss: 0.8129611429046182\n",
      "Epoch 4, Training Loss: 0.8101864700457629\n",
      "Epoch 5, Training Loss: 0.8074929432307972\n",
      "Epoch 6, Training Loss: 0.8055448867292965\n",
      "Epoch 7, Training Loss: 0.8042485851400039\n",
      "Epoch 8, Training Loss: 0.8031718899221981\n",
      "Epoch 9, Training Loss: 0.8023412963222055\n",
      "Epoch 10, Training Loss: 0.8021420285281013\n",
      "Epoch 11, Training Loss: 0.8015483413724338\n",
      "Epoch 12, Training Loss: 0.8007370806441587\n",
      "Epoch 13, Training Loss: 0.8005195897466996\n",
      "Epoch 14, Training Loss: 0.7999722200281479\n",
      "Epoch 15, Training Loss: 0.7992161529204425\n",
      "Epoch 16, Training Loss: 0.7988027794220868\n",
      "Epoch 17, Training Loss: 0.7979119636030758\n",
      "Epoch 18, Training Loss: 0.7971033536686617\n",
      "Epoch 19, Training Loss: 0.7963531117579516\n",
      "Epoch 20, Training Loss: 0.7958722820702722\n",
      "Epoch 21, Training Loss: 0.795198009224499\n",
      "Epoch 22, Training Loss: 0.794356982567731\n",
      "Epoch 23, Training Loss: 0.7934683758371016\n",
      "Epoch 24, Training Loss: 0.7926966176313512\n",
      "Epoch 25, Training Loss: 0.7914649872218862\n",
      "Epoch 26, Training Loss: 0.7909692204699796\n",
      "Epoch 27, Training Loss: 0.790482562009026\n",
      "Epoch 28, Training Loss: 0.7897244578950545\n",
      "Epoch 29, Training Loss: 0.7893014426091138\n",
      "Epoch 30, Training Loss: 0.7882838056367987\n",
      "Epoch 31, Training Loss: 0.7877766878464643\n",
      "Epoch 32, Training Loss: 0.7875419062726637\n",
      "Epoch 33, Training Loss: 0.7875276989796582\n",
      "Epoch 34, Training Loss: 0.7870215587054982\n",
      "Epoch 35, Training Loss: 0.7866949271454531\n",
      "Epoch 36, Training Loss: 0.7862536423346576\n",
      "Epoch 37, Training Loss: 0.7859857982747696\n",
      "Epoch 38, Training Loss: 0.7858325268941767\n",
      "Epoch 39, Training Loss: 0.7855309037601247\n",
      "Epoch 40, Training Loss: 0.785242279697867\n",
      "Epoch 41, Training Loss: 0.7854795286935918\n",
      "Epoch 42, Training Loss: 0.7850948321118074\n",
      "Epoch 43, Training Loss: 0.7849572812108433\n",
      "Epoch 44, Training Loss: 0.7852359548737021\n",
      "Epoch 45, Training Loss: 0.7843096344611225\n",
      "Epoch 46, Training Loss: 0.7848024663504433\n",
      "Epoch 47, Training Loss: 0.7844752572564517\n",
      "Epoch 48, Training Loss: 0.7844510734081268\n",
      "Epoch 49, Training Loss: 0.7844624996185303\n",
      "Epoch 50, Training Loss: 0.7843100202083587\n",
      "Epoch 51, Training Loss: 0.7838721132979674\n",
      "Epoch 52, Training Loss: 0.7841639376387877\n",
      "Epoch 53, Training Loss: 0.7838488935021793\n",
      "Epoch 54, Training Loss: 0.7836783252042883\n",
      "Epoch 55, Training Loss: 0.7839894995969885\n",
      "Epoch 56, Training Loss: 0.7837668854348799\n",
      "Epoch 57, Training Loss: 0.7835174629267524\n",
      "Epoch 58, Training Loss: 0.7836651148515589\n",
      "Epoch 59, Training Loss: 0.7832578154872446\n",
      "Epoch 60, Training Loss: 0.7833177258687861\n",
      "Epoch 61, Training Loss: 0.7835098550600164\n",
      "Epoch 62, Training Loss: 0.7832199207474204\n",
      "Epoch 63, Training Loss: 0.7828848523953381\n",
      "Epoch 64, Training Loss: 0.7830173085016363\n",
      "Epoch 65, Training Loss: 0.7831910421567805\n",
      "Epoch 66, Training Loss: 0.7827398451636819\n",
      "Epoch 67, Training Loss: 0.7827524053349214\n",
      "Epoch 68, Training Loss: 0.7826759504570681\n",
      "Epoch 69, Training Loss: 0.7826475622373469\n",
      "Epoch 70, Training Loss: 0.7826766770026263\n",
      "Epoch 71, Training Loss: 0.7826459343293134\n",
      "Epoch 72, Training Loss: 0.7826241764601539\n",
      "Epoch 73, Training Loss: 0.7823191508826087\n",
      "Epoch 74, Training Loss: 0.7826230452341192\n",
      "Epoch 75, Training Loss: 0.7826632123834947\n",
      "Epoch 76, Training Loss: 0.7825144343516406\n",
      "Epoch 77, Training Loss: 0.7824192181755515\n",
      "Epoch 78, Training Loss: 0.7823574756173527\n",
      "Epoch 79, Training Loss: 0.7826273767387166\n",
      "Epoch 80, Training Loss: 0.7820147960326251\n",
      "Epoch 81, Training Loss: 0.7822281528220457\n",
      "Epoch 82, Training Loss: 0.7822345988890704\n",
      "Epoch 83, Training Loss: 0.7819508931917303\n",
      "Epoch 84, Training Loss: 0.7821865936587838\n",
      "Epoch 85, Training Loss: 0.782162024974823\n",
      "Epoch 86, Training Loss: 0.7820488421355977\n",
      "Epoch 87, Training Loss: 0.7818069469928741\n",
      "Epoch 88, Training Loss: 0.7817920510909137\n",
      "Epoch 89, Training Loss: 0.7819351353364832\n",
      "Epoch 90, Training Loss: 0.7813786030516905\n",
      "Epoch 91, Training Loss: 0.781498244860593\n",
      "Epoch 92, Training Loss: 0.7818113004460054\n",
      "Epoch 93, Training Loss: 0.7816768714259652\n",
      "Epoch 94, Training Loss: 0.781620722728617\n",
      "Epoch 95, Training Loss: 0.7812591805878808\n",
      "Epoch 96, Training Loss: 0.781285615458208\n",
      "Epoch 97, Training Loss: 0.7813970398201662\n",
      "Epoch 98, Training Loss: 0.7812469307815327\n",
      "Epoch 99, Training Loss: 0.7814469927899977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:44:41,176] Trial 354 finished with value: 0.6404666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7813969896120183\n",
      "Epoch 1, Training Loss: 0.8392142834519981\n",
      "Epoch 2, Training Loss: 0.8183120611915015\n",
      "Epoch 3, Training Loss: 0.8127135387936929\n",
      "Epoch 4, Training Loss: 0.8131259588370646\n",
      "Epoch 5, Training Loss: 0.8106154188177639\n",
      "Epoch 6, Training Loss: 0.808500359649945\n",
      "Epoch 7, Training Loss: 0.8070957734172506\n",
      "Epoch 8, Training Loss: 0.8070591240000904\n",
      "Epoch 9, Training Loss: 0.8089141674507829\n",
      "Epoch 10, Training Loss: 0.8072982630335299\n",
      "Epoch 11, Training Loss: 0.8054478178346963\n",
      "Epoch 12, Training Loss: 0.803754653070206\n",
      "Epoch 13, Training Loss: 0.8049665543369781\n",
      "Epoch 14, Training Loss: 0.8046665929313889\n",
      "Epoch 15, Training Loss: 0.8042696006315991\n",
      "Epoch 16, Training Loss: 0.8047418668754118\n",
      "Epoch 17, Training Loss: 0.8036910542868134\n",
      "Epoch 18, Training Loss: 0.8041704970194881\n",
      "Epoch 19, Training Loss: 0.802299157748545\n",
      "Epoch 20, Training Loss: 0.8036194712595832\n",
      "Epoch 21, Training Loss: 0.8023073453652231\n",
      "Epoch 22, Training Loss: 0.8033064242592431\n",
      "Epoch 23, Training Loss: 0.8040509671196902\n",
      "Epoch 24, Training Loss: 0.8025430587001313\n",
      "Epoch 25, Training Loss: 0.8024863524544508\n",
      "Epoch 26, Training Loss: 0.8043921440167534\n",
      "Epoch 27, Training Loss: 0.8024342848842305\n",
      "Epoch 28, Training Loss: 0.8017999727026861\n",
      "Epoch 29, Training Loss: 0.8022022666787743\n",
      "Epoch 30, Training Loss: 0.8026207535786736\n",
      "Epoch 31, Training Loss: 0.8024505060418208\n",
      "Epoch 32, Training Loss: 0.8011785666297253\n",
      "Epoch 33, Training Loss: 0.8004824828384514\n",
      "Epoch 34, Training Loss: 0.8016574787914305\n",
      "Epoch 35, Training Loss: 0.8006533380737878\n",
      "Epoch 36, Training Loss: 0.8016745111099759\n",
      "Epoch 37, Training Loss: 0.8015209310933163\n",
      "Epoch 38, Training Loss: 0.8014385633002546\n",
      "Epoch 39, Training Loss: 0.7995645677236686\n",
      "Epoch 40, Training Loss: 0.8009897567275771\n",
      "Epoch 41, Training Loss: 0.8008311531597511\n",
      "Epoch 42, Training Loss: 0.8001769168036325\n",
      "Epoch 43, Training Loss: 0.7997351703787209\n",
      "Epoch 44, Training Loss: 0.7997900505711262\n",
      "Epoch 45, Training Loss: 0.7989975469452995\n",
      "Epoch 46, Training Loss: 0.8008142164775304\n",
      "Epoch 47, Training Loss: 0.8005775712486497\n",
      "Epoch 48, Training Loss: 0.8014881712153442\n",
      "Epoch 49, Training Loss: 0.8008021190650481\n",
      "Epoch 50, Training Loss: 0.8000204611541634\n",
      "Epoch 51, Training Loss: 0.799921551055478\n",
      "Epoch 52, Training Loss: 0.7994172570400668\n",
      "Epoch 53, Training Loss: 0.7992532428045919\n",
      "Epoch 54, Training Loss: 0.8010333549707456\n",
      "Epoch 55, Training Loss: 0.7992479618330647\n",
      "Epoch 56, Training Loss: 0.7991649098414227\n",
      "Epoch 57, Training Loss: 0.7994133692038686\n",
      "Epoch 58, Training Loss: 0.8008418844158488\n",
      "Epoch 59, Training Loss: 0.7986099389262665\n",
      "Epoch 60, Training Loss: 0.7986651664389703\n",
      "Epoch 61, Training Loss: 0.7999747518309973\n",
      "Epoch 62, Training Loss: 0.7993397638313753\n",
      "Epoch 63, Training Loss: 0.7992737688516316\n",
      "Epoch 64, Training Loss: 0.7997572834330394\n",
      "Epoch 65, Training Loss: 0.7993198491576919\n",
      "Epoch 66, Training Loss: 0.7988656187416019\n",
      "Epoch 67, Training Loss: 0.799065201174944\n",
      "Epoch 68, Training Loss: 0.8001937643925946\n",
      "Epoch 69, Training Loss: 0.7989306087780715\n",
      "Epoch 70, Training Loss: 0.7985660067178253\n",
      "Epoch 71, Training Loss: 0.7978222085120983\n",
      "Epoch 72, Training Loss: 0.7980782505264856\n",
      "Epoch 73, Training Loss: 0.7983072850937234\n",
      "Epoch 74, Training Loss: 0.7983713735315137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:45:54,602] Trial 355 finished with value: 0.6346666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7966849721463999\n",
      "Epoch 1, Training Loss: 0.9923328603120675\n",
      "Epoch 2, Training Loss: 0.9383719446963834\n",
      "Epoch 3, Training Loss: 0.9245579649631243\n",
      "Epoch 4, Training Loss: 0.9148261393819537\n",
      "Epoch 5, Training Loss: 0.9059128184963886\n",
      "Epoch 6, Training Loss: 0.8955846195830438\n",
      "Epoch 7, Training Loss: 0.8839560846637066\n",
      "Epoch 8, Training Loss: 0.8718409462978965\n",
      "Epoch 9, Training Loss: 0.8592447174222846\n",
      "Epoch 10, Training Loss: 0.8476326415413304\n",
      "Epoch 11, Training Loss: 0.8382569541608481\n",
      "Epoch 12, Training Loss: 0.8293868570399464\n",
      "Epoch 13, Training Loss: 0.8231712054489251\n",
      "Epoch 14, Training Loss: 0.8180844912851664\n",
      "Epoch 15, Training Loss: 0.8147132199509699\n",
      "Epoch 16, Training Loss: 0.81235044092164\n",
      "Epoch 17, Training Loss: 0.8099465391689673\n",
      "Epoch 18, Training Loss: 0.8082751142351251\n",
      "Epoch 19, Training Loss: 0.8070949307061676\n",
      "Epoch 20, Training Loss: 0.8063718111891496\n",
      "Epoch 21, Training Loss: 0.8054878048430708\n",
      "Epoch 22, Training Loss: 0.8043075338342136\n",
      "Epoch 23, Training Loss: 0.8041268584423495\n",
      "Epoch 24, Training Loss: 0.8033916422298977\n",
      "Epoch 25, Training Loss: 0.8028557189425132\n",
      "Epoch 26, Training Loss: 0.8016279140809425\n",
      "Epoch 27, Training Loss: 0.8015608167289792\n",
      "Epoch 28, Training Loss: 0.8007178635525524\n",
      "Epoch 29, Training Loss: 0.8001121683228285\n",
      "Epoch 30, Training Loss: 0.8001135370785133\n",
      "Epoch 31, Training Loss: 0.7991972342469639\n",
      "Epoch 32, Training Loss: 0.7998389788140031\n",
      "Epoch 33, Training Loss: 0.7990597717744067\n",
      "Epoch 34, Training Loss: 0.799176938103554\n",
      "Epoch 35, Training Loss: 0.7980629345528165\n",
      "Epoch 36, Training Loss: 0.7980335465947488\n",
      "Epoch 37, Training Loss: 0.798046354602154\n",
      "Epoch 38, Training Loss: 0.7978568282342495\n",
      "Epoch 39, Training Loss: 0.797264812315317\n",
      "Epoch 40, Training Loss: 0.7972789457866124\n",
      "Epoch 41, Training Loss: 0.7968824062132298\n",
      "Epoch 42, Training Loss: 0.7968131513523876\n",
      "Epoch 43, Training Loss: 0.7962866784934711\n",
      "Epoch 44, Training Loss: 0.7964071488021908\n",
      "Epoch 45, Training Loss: 0.7962129681630242\n",
      "Epoch 46, Training Loss: 0.7962908351331726\n",
      "Epoch 47, Training Loss: 0.7960499746458871\n",
      "Epoch 48, Training Loss: 0.7952289458056142\n",
      "Epoch 49, Training Loss: 0.796317824474851\n",
      "Epoch 50, Training Loss: 0.795274457626773\n",
      "Epoch 51, Training Loss: 0.7956048875823056\n",
      "Epoch 52, Training Loss: 0.7951362742517227\n",
      "Epoch 53, Training Loss: 0.7950254721749098\n",
      "Epoch 54, Training Loss: 0.7944373342327605\n",
      "Epoch 55, Training Loss: 0.7944358942203952\n",
      "Epoch 56, Training Loss: 0.7942795050771613\n",
      "Epoch 57, Training Loss: 0.7943089725379657\n",
      "Epoch 58, Training Loss: 0.7947753037725176\n",
      "Epoch 59, Training Loss: 0.7939039869416029\n",
      "Epoch 60, Training Loss: 0.7935646147656261\n",
      "Epoch 61, Training Loss: 0.793770106125595\n",
      "Epoch 62, Training Loss: 0.7937789690225644\n",
      "Epoch 63, Training Loss: 0.7933353810382069\n",
      "Epoch 64, Training Loss: 0.7939592461836965\n",
      "Epoch 65, Training Loss: 0.7932160200032973\n",
      "Epoch 66, Training Loss: 0.7934291518720469\n",
      "Epoch 67, Training Loss: 0.7927607041104395\n",
      "Epoch 68, Training Loss: 0.7932260388718512\n",
      "Epoch 69, Training Loss: 0.7932636001056298\n",
      "Epoch 70, Training Loss: 0.7936226747986069\n",
      "Epoch 71, Training Loss: 0.7928768422370567\n",
      "Epoch 72, Training Loss: 0.7926146555663948\n",
      "Epoch 73, Training Loss: 0.7933647816342518\n",
      "Epoch 74, Training Loss: 0.7928594154522831\n",
      "Epoch 75, Training Loss: 0.7924210071563721\n",
      "Epoch 76, Training Loss: 0.7926209959768711\n",
      "Epoch 77, Training Loss: 0.792878230353047\n",
      "Epoch 78, Training Loss: 0.7920987610530136\n",
      "Epoch 79, Training Loss: 0.7922448495276888\n",
      "Epoch 80, Training Loss: 0.7920975977316835\n",
      "Epoch 81, Training Loss: 0.7925815633365086\n",
      "Epoch 82, Training Loss: 0.7915734643774821\n",
      "Epoch 83, Training Loss: 0.791860928840207\n",
      "Epoch 84, Training Loss: 0.7922068184479735\n",
      "Epoch 85, Training Loss: 0.791886065687452\n",
      "Epoch 86, Training Loss: 0.79135818454556\n",
      "Epoch 87, Training Loss: 0.7915950293827774\n",
      "Epoch 88, Training Loss: 0.7916688663618905\n",
      "Epoch 89, Training Loss: 0.7921397249501451\n",
      "Epoch 90, Training Loss: 0.7922661236354283\n",
      "Epoch 91, Training Loss: 0.7917334649795876\n",
      "Epoch 92, Training Loss: 0.791695253114055\n",
      "Epoch 93, Training Loss: 0.7911906703074175\n",
      "Epoch 94, Training Loss: 0.7913252341119866\n",
      "Epoch 95, Training Loss: 0.7912159553147796\n",
      "Epoch 96, Training Loss: 0.7917311425495864\n",
      "Epoch 97, Training Loss: 0.7912561938278657\n",
      "Epoch 98, Training Loss: 0.7914161994941252\n",
      "Epoch 99, Training Loss: 0.7914159635852154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:47:17,916] Trial 356 finished with value: 0.6339333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7906518636789537\n",
      "Epoch 1, Training Loss: 0.8814896559356746\n",
      "Epoch 2, Training Loss: 0.8181090699102646\n",
      "Epoch 3, Training Loss: 0.8147306384000563\n",
      "Epoch 4, Training Loss: 0.8112755108596688\n",
      "Epoch 5, Training Loss: 0.8101476712334424\n",
      "Epoch 6, Training Loss: 0.8083981155452872\n",
      "Epoch 7, Training Loss: 0.8091586516315775\n",
      "Epoch 8, Training Loss: 0.8087034785657897\n",
      "Epoch 9, Training Loss: 0.8078478712784617\n",
      "Epoch 10, Training Loss: 0.8080636304572113\n",
      "Epoch 11, Training Loss: 0.806607194681813\n",
      "Epoch 12, Training Loss: 0.8068137016511501\n",
      "Epoch 13, Training Loss: 0.8060516528617171\n",
      "Epoch 14, Training Loss: 0.8056056148127506\n",
      "Epoch 15, Training Loss: 0.8040577840984315\n",
      "Epoch 16, Training Loss: 0.8040127530133814\n",
      "Epoch 17, Training Loss: 0.8032149650996789\n",
      "Epoch 18, Training Loss: 0.8018746315984797\n",
      "Epoch 19, Training Loss: 0.8031373990209479\n",
      "Epoch 20, Training Loss: 0.8035193732806615\n",
      "Epoch 21, Training Loss: 0.8021438199774663\n",
      "Epoch 22, Training Loss: 0.802137039120036\n",
      "Epoch 23, Training Loss: 0.8017086423429332\n",
      "Epoch 24, Training Loss: 0.8013196980146537\n",
      "Epoch 25, Training Loss: 0.8006704273976778\n",
      "Epoch 26, Training Loss: 0.8013546788602843\n",
      "Epoch 27, Training Loss: 0.801114945483387\n",
      "Epoch 28, Training Loss: 0.8005925443835724\n",
      "Epoch 29, Training Loss: 0.799568604168139\n",
      "Epoch 30, Training Loss: 0.7999691394934977\n",
      "Epoch 31, Training Loss: 0.7997854287463023\n",
      "Epoch 32, Training Loss: 0.7997364850868857\n",
      "Epoch 33, Training Loss: 0.798134578081002\n",
      "Epoch 34, Training Loss: 0.7990266608116322\n",
      "Epoch 35, Training Loss: 0.798163122073152\n",
      "Epoch 36, Training Loss: 0.797565928348025\n",
      "Epoch 37, Training Loss: 0.7982668845277083\n",
      "Epoch 38, Training Loss: 0.797519184144816\n",
      "Epoch 39, Training Loss: 0.7966728446178867\n",
      "Epoch 40, Training Loss: 0.7968262641949762\n",
      "Epoch 41, Training Loss: 0.7972891439172558\n",
      "Epoch 42, Training Loss: 0.7958547685379372\n",
      "Epoch 43, Training Loss: 0.7969295430900459\n",
      "Epoch 44, Training Loss: 0.7953090872083391\n",
      "Epoch 45, Training Loss: 0.7965061902999878\n",
      "Epoch 46, Training Loss: 0.7955950965558676\n",
      "Epoch 47, Training Loss: 0.7954299646212643\n",
      "Epoch 48, Training Loss: 0.7964861289002841\n",
      "Epoch 49, Training Loss: 0.7950917848070761\n",
      "Epoch 50, Training Loss: 0.7956738013970225\n",
      "Epoch 51, Training Loss: 0.7948874893493222\n",
      "Epoch 52, Training Loss: 0.794925694268449\n",
      "Epoch 53, Training Loss: 0.7945272002005039\n",
      "Epoch 54, Training Loss: 0.7955632428477581\n",
      "Epoch 55, Training Loss: 0.7952916889262379\n",
      "Epoch 56, Training Loss: 0.794081351541935\n",
      "Epoch 57, Training Loss: 0.7941958184529068\n",
      "Epoch 58, Training Loss: 0.7937016183272341\n",
      "Epoch 59, Training Loss: 0.793902259482477\n",
      "Epoch 60, Training Loss: 0.794905396153156\n",
      "Epoch 61, Training Loss: 0.7941274236019392\n",
      "Epoch 62, Training Loss: 0.7947351670802985\n",
      "Epoch 63, Training Loss: 0.7937487291214161\n",
      "Epoch 64, Training Loss: 0.793577049728623\n",
      "Epoch 65, Training Loss: 0.7934022823670753\n",
      "Epoch 66, Training Loss: 0.793138641432712\n",
      "Epoch 67, Training Loss: 0.7933675842177599\n",
      "Epoch 68, Training Loss: 0.7937526554989636\n",
      "Epoch 69, Training Loss: 0.7938103034980315\n",
      "Epoch 70, Training Loss: 0.7935928882512832\n",
      "Epoch 71, Training Loss: 0.7922511625110655\n",
      "Epoch 72, Training Loss: 0.7934534282612621\n",
      "Epoch 73, Training Loss: 0.7919855937921911\n",
      "Epoch 74, Training Loss: 0.7925279830631456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:48:32,176] Trial 357 finished with value: 0.6348666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7930841417240917\n",
      "Epoch 1, Training Loss: 1.1022907007905773\n",
      "Epoch 2, Training Loss: 1.0879281178452915\n",
      "Epoch 3, Training Loss: 1.0819795698151553\n",
      "Epoch 4, Training Loss: 1.0765657951957301\n",
      "Epoch 5, Training Loss: 1.071483728401643\n",
      "Epoch 6, Training Loss: 1.0665027526984538\n",
      "Epoch 7, Training Loss: 1.0619571321888974\n",
      "Epoch 8, Training Loss: 1.0573856319699968\n",
      "Epoch 9, Training Loss: 1.053316667205409\n",
      "Epoch 10, Training Loss: 1.0492701806520162\n",
      "Epoch 11, Training Loss: 1.045377804999961\n",
      "Epoch 12, Training Loss: 1.0416746711372433\n",
      "Epoch 13, Training Loss: 1.0379566664086248\n",
      "Epoch 14, Training Loss: 1.0343091975477405\n",
      "Epoch 15, Training Loss: 1.0310468020295738\n",
      "Epoch 16, Training Loss: 1.0278828768801869\n",
      "Epoch 17, Training Loss: 1.0247165991847675\n",
      "Epoch 18, Training Loss: 1.0216364749392173\n",
      "Epoch 19, Training Loss: 1.01873794480374\n",
      "Epoch 20, Training Loss: 1.015861633576845\n",
      "Epoch 21, Training Loss: 1.013354332016823\n",
      "Epoch 22, Training Loss: 1.01074778271797\n",
      "Epoch 23, Training Loss: 1.0082259151272308\n",
      "Epoch 24, Training Loss: 1.005685541952463\n",
      "Epoch 25, Training Loss: 1.003346554318765\n",
      "Epoch 26, Training Loss: 1.0013280915138416\n",
      "Epoch 27, Training Loss: 0.9992416778901466\n",
      "Epoch 28, Training Loss: 0.9970493647388946\n",
      "Epoch 29, Training Loss: 0.9953054479190282\n",
      "Epoch 30, Training Loss: 0.9933206833394846\n",
      "Epoch 31, Training Loss: 0.9914216789087855\n",
      "Epoch 32, Training Loss: 0.9900151328036659\n",
      "Epoch 33, Training Loss: 0.9882753175900395\n",
      "Epoch 34, Training Loss: 0.9865729759510299\n",
      "Epoch 35, Training Loss: 0.9853872720460246\n",
      "Epoch 36, Training Loss: 0.9838873139897684\n",
      "Epoch 37, Training Loss: 0.9826604133261774\n",
      "Epoch 38, Training Loss: 0.98116841818157\n",
      "Epoch 39, Training Loss: 0.9798899059008835\n",
      "Epoch 40, Training Loss: 0.9787511174840139\n",
      "Epoch 41, Training Loss: 0.9777239690149637\n",
      "Epoch 42, Training Loss: 0.9768611152369276\n",
      "Epoch 43, Training Loss: 0.9755024858883449\n",
      "Epoch 44, Training Loss: 0.9742889233101579\n",
      "Epoch 45, Training Loss: 0.9737011406654702\n",
      "Epoch 46, Training Loss: 0.9729219299510009\n",
      "Epoch 47, Training Loss: 0.9720014327450802\n",
      "Epoch 48, Training Loss: 0.971051363747819\n",
      "Epoch 49, Training Loss: 0.9702852367458487\n",
      "Epoch 50, Training Loss: 0.9698427044359365\n",
      "Epoch 51, Training Loss: 0.9689545936154244\n",
      "Epoch 52, Training Loss: 0.9683299308432672\n",
      "Epoch 53, Training Loss: 0.9679334185177222\n",
      "Epoch 54, Training Loss: 0.9671306560810348\n",
      "Epoch 55, Training Loss: 0.9666717830457185\n",
      "Epoch 56, Training Loss: 0.9662370839513335\n",
      "Epoch 57, Training Loss: 0.9657100661356647\n",
      "Epoch 58, Training Loss: 0.964845817877834\n",
      "Epoch 59, Training Loss: 0.9644902749169142\n",
      "Epoch 60, Training Loss: 0.9645097920769139\n",
      "Epoch 61, Training Loss: 0.9642617902361361\n",
      "Epoch 62, Training Loss: 0.9633877865353921\n",
      "Epoch 63, Training Loss: 0.963089849088425\n",
      "Epoch 64, Training Loss: 0.9629616377945233\n",
      "Epoch 65, Training Loss: 0.9623445872973678\n",
      "Epoch 66, Training Loss: 0.9624327397884282\n",
      "Epoch 67, Training Loss: 0.9617131935922723\n",
      "Epoch 68, Training Loss: 0.9616203315275952\n",
      "Epoch 69, Training Loss: 0.9608962900656507\n",
      "Epoch 70, Training Loss: 0.9612223661035524\n",
      "Epoch 71, Training Loss: 0.9607234687733471\n",
      "Epoch 72, Training Loss: 0.9602881273828952\n",
      "Epoch 73, Training Loss: 0.9603112356107038\n",
      "Epoch 74, Training Loss: 0.9599495743450366\n",
      "Epoch 75, Training Loss: 0.9597764661437587\n",
      "Epoch 76, Training Loss: 0.9596707527798818\n",
      "Epoch 77, Training Loss: 0.9595868877002172\n",
      "Epoch 78, Training Loss: 0.959011567624888\n",
      "Epoch 79, Training Loss: 0.9585863517639333\n",
      "Epoch 80, Training Loss: 0.9584195716040474\n",
      "Epoch 81, Training Loss: 0.9581905021703333\n",
      "Epoch 82, Training Loss: 0.958213558501767\n",
      "Epoch 83, Training Loss: 0.9583501386463193\n",
      "Epoch 84, Training Loss: 0.9580644807421175\n",
      "Epoch 85, Training Loss: 0.9580241248123628\n",
      "Epoch 86, Training Loss: 0.9575761966239241\n",
      "Epoch 87, Training Loss: 0.95741430234192\n",
      "Epoch 88, Training Loss: 0.9572350975266076\n",
      "Epoch 89, Training Loss: 0.9573141350782007\n",
      "Epoch 90, Training Loss: 0.9568127958398116\n",
      "Epoch 91, Training Loss: 0.9569006650967705\n",
      "Epoch 92, Training Loss: 0.9565335563250951\n",
      "Epoch 93, Training Loss: 0.9568447750313838\n",
      "Epoch 94, Training Loss: 0.956810822612361\n",
      "Epoch 95, Training Loss: 0.956031916912337\n",
      "Epoch 96, Training Loss: 0.9566663037565418\n",
      "Epoch 97, Training Loss: 0.9561700071607317\n",
      "Epoch 98, Training Loss: 0.9558613272537863\n",
      "Epoch 99, Training Loss: 0.9562173904332899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:49:54,921] Trial 358 finished with value: 0.5305333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.9559357736343728\n",
      "Epoch 1, Training Loss: 1.086565251278698\n",
      "Epoch 2, Training Loss: 1.0705478383186169\n",
      "Epoch 3, Training Loss: 1.05966348450883\n",
      "Epoch 4, Training Loss: 1.0503595843350977\n",
      "Epoch 5, Training Loss: 1.0417760715448767\n",
      "Epoch 6, Training Loss: 1.0331732711397617\n",
      "Epoch 7, Training Loss: 1.0242425542128712\n",
      "Epoch 8, Training Loss: 1.0153567004024535\n",
      "Epoch 9, Training Loss: 1.0064753371970099\n",
      "Epoch 10, Training Loss: 0.9978243043548183\n",
      "Epoch 11, Training Loss: 0.9893809850054576\n",
      "Epoch 12, Training Loss: 0.9816108571855645\n",
      "Epoch 13, Training Loss: 0.9743436115128653\n",
      "Epoch 14, Training Loss: 0.9679580036858867\n",
      "Epoch 15, Training Loss: 0.9623315281437752\n",
      "Epoch 16, Training Loss: 0.9575412841667806\n",
      "Epoch 17, Training Loss: 0.9531684343079875\n",
      "Epoch 18, Training Loss: 0.9494562796183995\n",
      "Epoch 19, Training Loss: 0.9467944001792965\n",
      "Epoch 20, Training Loss: 0.9439535360587271\n",
      "Epoch 21, Training Loss: 0.941909727028438\n",
      "Epoch 22, Training Loss: 0.9398872749249738\n",
      "Epoch 23, Training Loss: 0.9379435878050955\n",
      "Epoch 24, Training Loss: 0.9366349937324238\n",
      "Epoch 25, Training Loss: 0.9355636959685418\n",
      "Epoch 26, Training Loss: 0.9342331824446083\n",
      "Epoch 27, Training Loss: 0.9331558325236902\n",
      "Epoch 28, Training Loss: 0.9311732504600869\n",
      "Epoch 29, Training Loss: 0.9309428990335393\n",
      "Epoch 30, Training Loss: 0.9296827956249839\n",
      "Epoch 31, Training Loss: 0.9283766728594787\n",
      "Epoch 32, Training Loss: 0.9276282682454675\n",
      "Epoch 33, Training Loss: 0.9260666247597314\n",
      "Epoch 34, Training Loss: 0.9256836287957385\n",
      "Epoch 35, Training Loss: 0.9245383762775508\n",
      "Epoch 36, Training Loss: 0.9237184741443261\n",
      "Epoch 37, Training Loss: 0.9228307905053734\n",
      "Epoch 38, Training Loss: 0.9218132991539805\n",
      "Epoch 39, Training Loss: 0.9210973650889289\n",
      "Epoch 40, Training Loss: 0.9199433472819795\n",
      "Epoch 41, Training Loss: 0.9194333641152633\n",
      "Epoch 42, Training Loss: 0.9186211854891669\n",
      "Epoch 43, Training Loss: 0.9177854141794649\n",
      "Epoch 44, Training Loss: 0.9166814688453101\n",
      "Epoch 45, Training Loss: 0.9164146513867198\n",
      "Epoch 46, Training Loss: 0.9151073309711943\n",
      "Epoch 47, Training Loss: 0.9141560708669791\n",
      "Epoch 48, Training Loss: 0.9135771681491593\n",
      "Epoch 49, Training Loss: 0.9129398901659743\n",
      "Epoch 50, Training Loss: 0.9120400770266254\n",
      "Epoch 51, Training Loss: 0.9109927445426023\n",
      "Epoch 52, Training Loss: 0.9104492249345421\n",
      "Epoch 53, Training Loss: 0.9092835102762494\n",
      "Epoch 54, Training Loss: 0.9090874798315808\n",
      "Epoch 55, Training Loss: 0.9076609282565297\n",
      "Epoch 56, Training Loss: 0.9067349452721445\n",
      "Epoch 57, Training Loss: 0.9057394219520397\n",
      "Epoch 58, Training Loss: 0.9049107439535901\n",
      "Epoch 59, Training Loss: 0.9043193388702278\n",
      "Epoch 60, Training Loss: 0.9035373393754313\n",
      "Epoch 61, Training Loss: 0.9026540894257394\n",
      "Epoch 62, Training Loss: 0.9011298158115014\n",
      "Epoch 63, Training Loss: 0.9007334040519887\n",
      "Epoch 64, Training Loss: 0.8995965992597709\n",
      "Epoch 65, Training Loss: 0.8986510291135401\n",
      "Epoch 66, Training Loss: 0.8974822564232618\n",
      "Epoch 67, Training Loss: 0.8967168659195864\n",
      "Epoch 68, Training Loss: 0.8955333787695806\n",
      "Epoch 69, Training Loss: 0.8946007701687346\n",
      "Epoch 70, Training Loss: 0.8937413077605398\n",
      "Epoch 71, Training Loss: 0.8927150712873703\n",
      "Epoch 72, Training Loss: 0.8914199145216691\n",
      "Epoch 73, Training Loss: 0.890176981104944\n",
      "Epoch 74, Training Loss: 0.8894979422253774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:51:04,062] Trial 359 finished with value: 0.5815333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8887526811513685\n",
      "Epoch 1, Training Loss: 1.0076248980106268\n",
      "Epoch 2, Training Loss: 0.9471008028302874\n",
      "Epoch 3, Training Loss: 0.9345264286923229\n",
      "Epoch 4, Training Loss: 0.926170159551434\n",
      "Epoch 5, Training Loss: 0.9192024651326631\n",
      "Epoch 6, Training Loss: 0.913594012690666\n",
      "Epoch 7, Training Loss: 0.9084149287159281\n",
      "Epoch 8, Training Loss: 0.9035322850808165\n",
      "Epoch 9, Training Loss: 0.897772915470869\n",
      "Epoch 10, Training Loss: 0.8923016597453813\n",
      "Epoch 11, Training Loss: 0.8861116096489412\n",
      "Epoch 12, Training Loss: 0.8809048155196627\n",
      "Epoch 13, Training Loss: 0.8740418844653252\n",
      "Epoch 14, Training Loss: 0.8674575826279203\n",
      "Epoch 15, Training Loss: 0.8614748259236041\n",
      "Epoch 16, Training Loss: 0.8548704887691297\n",
      "Epoch 17, Training Loss: 0.849191289109395\n",
      "Epoch 18, Training Loss: 0.8437753266857979\n",
      "Epoch 19, Training Loss: 0.8384374576403683\n",
      "Epoch 20, Training Loss: 0.8334052855807139\n",
      "Epoch 21, Training Loss: 0.829589929706172\n",
      "Epoch 22, Training Loss: 0.8252777457237244\n",
      "Epoch 23, Training Loss: 0.8227752078744702\n",
      "Epoch 24, Training Loss: 0.8191816565237547\n",
      "Epoch 25, Training Loss: 0.8170191830262206\n",
      "Epoch 26, Training Loss: 0.815356122909632\n",
      "Epoch 27, Training Loss: 0.8135657306900598\n",
      "Epoch 28, Training Loss: 0.8115385778864523\n",
      "Epoch 29, Training Loss: 0.8102176947701246\n",
      "Epoch 30, Training Loss: 0.809045550159942\n",
      "Epoch 31, Training Loss: 0.80859913073088\n",
      "Epoch 32, Training Loss: 0.8064087950197377\n",
      "Epoch 33, Training Loss: 0.805646484507654\n",
      "Epoch 34, Training Loss: 0.8055024271620843\n",
      "Epoch 35, Training Loss: 0.8048368697775934\n",
      "Epoch 36, Training Loss: 0.8040513599725594\n",
      "Epoch 37, Training Loss: 0.8035824738947073\n",
      "Epoch 38, Training Loss: 0.8032977869636134\n",
      "Epoch 39, Training Loss: 0.802944845005982\n",
      "Epoch 40, Training Loss: 0.8024573427393921\n",
      "Epoch 41, Training Loss: 0.8018708131367103\n",
      "Epoch 42, Training Loss: 0.8016017145680305\n",
      "Epoch 43, Training Loss: 0.8016488108419835\n",
      "Epoch 44, Training Loss: 0.8012461054593997\n",
      "Epoch 45, Training Loss: 0.8014068077381392\n",
      "Epoch 46, Training Loss: 0.8012584696138711\n",
      "Epoch 47, Training Loss: 0.8003372705968699\n",
      "Epoch 48, Training Loss: 0.8007031615515401\n",
      "Epoch 49, Training Loss: 0.8010066420512092\n",
      "Epoch 50, Training Loss: 0.8004095969343544\n",
      "Epoch 51, Training Loss: 0.8005964955889193\n",
      "Epoch 52, Training Loss: 0.8002136215231472\n",
      "Epoch 53, Training Loss: 0.8004083155689383\n",
      "Epoch 54, Training Loss: 0.7995628895616173\n",
      "Epoch 55, Training Loss: 0.7998510251367899\n",
      "Epoch 56, Training Loss: 0.8000216818393622\n",
      "Epoch 57, Training Loss: 0.7999341161627519\n",
      "Epoch 58, Training Loss: 0.8004239318066073\n",
      "Epoch 59, Training Loss: 0.7996535863195147\n",
      "Epoch 60, Training Loss: 0.8002907917911845\n",
      "Epoch 61, Training Loss: 0.7990972489342654\n",
      "Epoch 62, Training Loss: 0.7995456769054098\n",
      "Epoch 63, Training Loss: 0.799475295920121\n",
      "Epoch 64, Training Loss: 0.7985114412200182\n",
      "Epoch 65, Training Loss: 0.79885185446058\n",
      "Epoch 66, Training Loss: 0.7990307213668536\n",
      "Epoch 67, Training Loss: 0.7989658842409464\n",
      "Epoch 68, Training Loss: 0.7988117398175978\n",
      "Epoch 69, Training Loss: 0.7989348630259808\n",
      "Epoch 70, Training Loss: 0.7987108816777854\n",
      "Epoch 71, Training Loss: 0.7991978324445567\n",
      "Epoch 72, Training Loss: 0.7983559779654769\n",
      "Epoch 73, Training Loss: 0.7984176647394223\n",
      "Epoch 74, Training Loss: 0.7983715176582337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:52:07,167] Trial 360 finished with value: 0.6317333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7985194291387285\n",
      "Epoch 1, Training Loss: 0.8461539781183228\n",
      "Epoch 2, Training Loss: 0.825695007994659\n",
      "Epoch 3, Training Loss: 0.8184625087824083\n",
      "Epoch 4, Training Loss: 0.8150035152757974\n",
      "Epoch 5, Training Loss: 0.8154975183924338\n",
      "Epoch 6, Training Loss: 0.8129532804166464\n",
      "Epoch 7, Training Loss: 0.8124508318148161\n",
      "Epoch 8, Training Loss: 0.8120288968086242\n",
      "Epoch 9, Training Loss: 0.8115243990618484\n",
      "Epoch 10, Training Loss: 0.8108876190687481\n",
      "Epoch 11, Training Loss: 0.8111331918185815\n",
      "Epoch 12, Training Loss: 0.8108282195894342\n",
      "Epoch 13, Training Loss: 0.8103374187211345\n",
      "Epoch 14, Training Loss: 0.8088173026429083\n",
      "Epoch 15, Training Loss: 0.8134063207117238\n",
      "Epoch 16, Training Loss: 0.8088388900111493\n",
      "Epoch 17, Training Loss: 0.8096759655421838\n",
      "Epoch 18, Training Loss: 0.8081565912504841\n",
      "Epoch 19, Training Loss: 0.80952533375948\n",
      "Epoch 20, Training Loss: 0.8087770853723798\n",
      "Epoch 21, Training Loss: 0.8063641139439174\n",
      "Epoch 22, Training Loss: 0.8066477403604895\n",
      "Epoch 23, Training Loss: 0.8052788919075987\n",
      "Epoch 24, Training Loss: 0.8072414012779867\n",
      "Epoch 25, Training Loss: 0.8065473215920584\n",
      "Epoch 26, Training Loss: 0.8049392966399516\n",
      "Epoch 27, Training Loss: 0.8048374015585821\n",
      "Epoch 28, Training Loss: 0.8055829174536512\n",
      "Epoch 29, Training Loss: 0.8048004378501634\n",
      "Epoch 30, Training Loss: 0.805037674509493\n",
      "Epoch 31, Training Loss: 0.8053207288111063\n",
      "Epoch 32, Training Loss: 0.8034844025633389\n",
      "Epoch 33, Training Loss: 0.8033033125382617\n",
      "Epoch 34, Training Loss: 0.8050376638433987\n",
      "Epoch 35, Training Loss: 0.8041787851125675\n",
      "Epoch 36, Training Loss: 0.8027483139719281\n",
      "Epoch 37, Training Loss: 0.8034515385789083\n",
      "Epoch 38, Training Loss: 0.8023218716893877\n",
      "Epoch 39, Training Loss: 0.8037655747026429\n",
      "Epoch 40, Training Loss: 0.802011695929936\n",
      "Epoch 41, Training Loss: 0.801780393069848\n",
      "Epoch 42, Training Loss: 0.8013999333058981\n",
      "Epoch 43, Training Loss: 0.8017632027317707\n",
      "Epoch 44, Training Loss: 0.8000564881733485\n",
      "Epoch 45, Training Loss: 0.8041862958355954\n",
      "Epoch 46, Training Loss: 0.8005868071900275\n",
      "Epoch 47, Training Loss: 0.8008550073867454\n",
      "Epoch 48, Training Loss: 0.8008958653399819\n",
      "Epoch 49, Training Loss: 0.8002675829077126\n",
      "Epoch 50, Training Loss: 0.8006598235072946\n",
      "Epoch 51, Training Loss: 0.7997163540438602\n",
      "Epoch 52, Training Loss: 0.8001539144300877\n",
      "Epoch 53, Training Loss: 0.7999297859973478\n",
      "Epoch 54, Training Loss: 0.8018809879632821\n",
      "Epoch 55, Training Loss: 0.8013453972967047\n",
      "Epoch 56, Training Loss: 0.8008117302916103\n",
      "Epoch 57, Training Loss: 0.8000065347305814\n",
      "Epoch 58, Training Loss: 0.8009396786976578\n",
      "Epoch 59, Training Loss: 0.7981436736601636\n",
      "Epoch 60, Training Loss: 0.8003644758597352\n",
      "Epoch 61, Training Loss: 0.8002236254233167\n",
      "Epoch 62, Training Loss: 0.7993691549265295\n",
      "Epoch 63, Training Loss: 0.799843610587873\n",
      "Epoch 64, Training Loss: 0.7989384175243234\n",
      "Epoch 65, Training Loss: 0.7995855472141639\n",
      "Epoch 66, Training Loss: 0.8003990632250793\n",
      "Epoch 67, Training Loss: 0.7991104351846795\n",
      "Epoch 68, Training Loss: 0.800142226452218\n",
      "Epoch 69, Training Loss: 0.797175335435939\n",
      "Epoch 70, Training Loss: 0.8005373029780567\n",
      "Epoch 71, Training Loss: 0.7987653466991912\n",
      "Epoch 72, Training Loss: 0.7986198731831142\n",
      "Epoch 73, Training Loss: 0.7996586532521068\n",
      "Epoch 74, Training Loss: 0.7990817247476792\n",
      "Epoch 75, Training Loss: 0.7977761755312296\n",
      "Epoch 76, Training Loss: 0.7996018052997446\n",
      "Epoch 77, Training Loss: 0.7994433826969979\n",
      "Epoch 78, Training Loss: 0.7988284067103737\n",
      "Epoch 79, Training Loss: 0.7989518470333931\n",
      "Epoch 80, Training Loss: 0.7992130354382938\n",
      "Epoch 81, Training Loss: 0.7987743156296866\n",
      "Epoch 82, Training Loss: 0.7995335949094672\n",
      "Epoch 83, Training Loss: 0.7984604470711901\n",
      "Epoch 84, Training Loss: 0.7993388238706087\n",
      "Epoch 85, Training Loss: 0.7969636514670867\n",
      "Epoch 86, Training Loss: 0.7972391052353651\n",
      "Epoch 87, Training Loss: 0.7996651064184376\n",
      "Epoch 88, Training Loss: 0.7989973683106272\n",
      "Epoch 89, Training Loss: 0.7996714140239515\n",
      "Epoch 90, Training Loss: 0.7967699032977111\n",
      "Epoch 91, Training Loss: 0.7984621304318421\n",
      "Epoch 92, Training Loss: 0.7965185096389369\n",
      "Epoch 93, Training Loss: 0.7990050679758975\n",
      "Epoch 94, Training Loss: 0.7987918528399073\n",
      "Epoch 95, Training Loss: 0.7982496986711832\n",
      "Epoch 96, Training Loss: 0.7967915160315377\n",
      "Epoch 97, Training Loss: 0.7986398391257551\n",
      "Epoch 98, Training Loss: 0.7977862788322276\n",
      "Epoch 99, Training Loss: 0.7982566733109323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:53:45,811] Trial 361 finished with value: 0.6311333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7980945657966728\n",
      "Epoch 1, Training Loss: 0.8570391565785372\n",
      "Epoch 2, Training Loss: 0.817134209772698\n",
      "Epoch 3, Training Loss: 0.8132583210342809\n",
      "Epoch 4, Training Loss: 0.8110596166517502\n",
      "Epoch 5, Training Loss: 0.8075802309172494\n",
      "Epoch 6, Training Loss: 0.8061162658203813\n",
      "Epoch 7, Training Loss: 0.8047160940959042\n",
      "Epoch 8, Training Loss: 0.8027910273774226\n",
      "Epoch 9, Training Loss: 0.802740136393927\n",
      "Epoch 10, Training Loss: 0.8023156268256051\n",
      "Epoch 11, Training Loss: 0.8010468176433019\n",
      "Epoch 12, Training Loss: 0.8007846797319284\n",
      "Epoch 13, Training Loss: 0.8001906651303284\n",
      "Epoch 14, Training Loss: 0.799815863505342\n",
      "Epoch 15, Training Loss: 0.7999193902302505\n",
      "Epoch 16, Training Loss: 0.7994177207014614\n",
      "Epoch 17, Training Loss: 0.7984636586411555\n",
      "Epoch 18, Training Loss: 0.7985982911031049\n",
      "Epoch 19, Training Loss: 0.79892002989475\n",
      "Epoch 20, Training Loss: 0.7984007453559933\n",
      "Epoch 21, Training Loss: 0.7978703981055353\n",
      "Epoch 22, Training Loss: 0.7973224201596769\n",
      "Epoch 23, Training Loss: 0.7975766442772141\n",
      "Epoch 24, Training Loss: 0.797267973154111\n",
      "Epoch 25, Training Loss: 0.7971036459270276\n",
      "Epoch 26, Training Loss: 0.7967354596109318\n",
      "Epoch 27, Training Loss: 0.7966540468366523\n",
      "Epoch 28, Training Loss: 0.7965065900544475\n",
      "Epoch 29, Training Loss: 0.7966922102117897\n",
      "Epoch 30, Training Loss: 0.7964179448615339\n",
      "Epoch 31, Training Loss: 0.7966805969862113\n",
      "Epoch 32, Training Loss: 0.7962299527978538\n",
      "Epoch 33, Training Loss: 0.7962634515941591\n",
      "Epoch 34, Training Loss: 0.795845890134797\n",
      "Epoch 35, Training Loss: 0.7955146572643653\n",
      "Epoch 36, Training Loss: 0.7950360696118577\n",
      "Epoch 37, Training Loss: 0.7959060635781826\n",
      "Epoch 38, Training Loss: 0.7946956336946416\n",
      "Epoch 39, Training Loss: 0.7949924213545663\n",
      "Epoch 40, Training Loss: 0.7947348694155987\n",
      "Epoch 41, Training Loss: 0.7957380922217118\n",
      "Epoch 42, Training Loss: 0.794523415081483\n",
      "Epoch 43, Training Loss: 0.7943171433040074\n",
      "Epoch 44, Training Loss: 0.7950409693825514\n",
      "Epoch 45, Training Loss: 0.7945792894614371\n",
      "Epoch 46, Training Loss: 0.7950453743898779\n",
      "Epoch 47, Training Loss: 0.7941330973367046\n",
      "Epoch 48, Training Loss: 0.793728684841242\n",
      "Epoch 49, Training Loss: 0.793872200456777\n",
      "Epoch 50, Training Loss: 0.7936994032752245\n",
      "Epoch 51, Training Loss: 0.7936679260175031\n",
      "Epoch 52, Training Loss: 0.7942431004423844\n",
      "Epoch 53, Training Loss: 0.7934778775487628\n",
      "Epoch 54, Training Loss: 0.7936923191959696\n",
      "Epoch 55, Training Loss: 0.7938646612310768\n",
      "Epoch 56, Training Loss: 0.7946126852716718\n",
      "Epoch 57, Training Loss: 0.7937150397695096\n",
      "Epoch 58, Training Loss: 0.7945112885389113\n",
      "Epoch 59, Training Loss: 0.793751917745834\n",
      "Epoch 60, Training Loss: 0.7938443346131117\n",
      "Epoch 61, Training Loss: 0.7931536134472467\n",
      "Epoch 62, Training Loss: 0.7930855468699807\n",
      "Epoch 63, Training Loss: 0.793525755405426\n",
      "Epoch 64, Training Loss: 0.7928175197508102\n",
      "Epoch 65, Training Loss: 0.7934325781979955\n",
      "Epoch 66, Training Loss: 0.7928204505963433\n",
      "Epoch 67, Training Loss: 0.7925545820616242\n",
      "Epoch 68, Training Loss: 0.7925750760207498\n",
      "Epoch 69, Training Loss: 0.792471712782867\n",
      "Epoch 70, Training Loss: 0.7922989430284142\n",
      "Epoch 71, Training Loss: 0.793040917063118\n",
      "Epoch 72, Training Loss: 0.7927462461299466\n",
      "Epoch 73, Training Loss: 0.7927294815393319\n",
      "Epoch 74, Training Loss: 0.7929813869017407\n",
      "Epoch 75, Training Loss: 0.7929701442109015\n",
      "Epoch 76, Training Loss: 0.7924188893540461\n",
      "Epoch 77, Training Loss: 0.7927489410665699\n",
      "Epoch 78, Training Loss: 0.7926433021860911\n",
      "Epoch 79, Training Loss: 0.7923160874753966\n",
      "Epoch 80, Training Loss: 0.7927097714036927\n",
      "Epoch 81, Training Loss: 0.792353574046515\n",
      "Epoch 82, Training Loss: 0.7927744133131844\n",
      "Epoch 83, Training Loss: 0.7918038682830065\n",
      "Epoch 84, Training Loss: 0.7933705644499986\n",
      "Epoch 85, Training Loss: 0.7922972793865921\n",
      "Epoch 86, Training Loss: 0.7918055874960763\n",
      "Epoch 87, Training Loss: 0.7916766425720732\n",
      "Epoch 88, Training Loss: 0.7922349227102179\n",
      "Epoch 89, Training Loss: 0.7922758672470437\n",
      "Epoch 90, Training Loss: 0.7925435887243515\n",
      "Epoch 91, Training Loss: 0.7929204308000722\n",
      "Epoch 92, Training Loss: 0.7913656885462595\n",
      "Epoch 93, Training Loss: 0.7913725416911276\n",
      "Epoch 94, Training Loss: 0.7911824903990093\n",
      "Epoch 95, Training Loss: 0.7919580965113819\n",
      "Epoch 96, Training Loss: 0.791398491088609\n",
      "Epoch 97, Training Loss: 0.7918881111575249\n",
      "Epoch 98, Training Loss: 0.7912271407313813\n",
      "Epoch 99, Training Loss: 0.7910712153839886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:55:22,146] Trial 362 finished with value: 0.619 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7912799904220983\n",
      "Epoch 1, Training Loss: 1.089010744936326\n",
      "Epoch 2, Training Loss: 1.0819200132874882\n",
      "Epoch 3, Training Loss: 1.071479856266695\n",
      "Epoch 4, Training Loss: 1.055040638587054\n",
      "Epoch 5, Training Loss: 1.0301239724720226\n",
      "Epoch 6, Training Loss: 1.0012473413523506\n",
      "Epoch 7, Training Loss: 0.9789302358907812\n",
      "Epoch 8, Training Loss: 0.9671280087442959\n",
      "Epoch 9, Training Loss: 0.9614506643659928\n",
      "Epoch 10, Training Loss: 0.9583538436188417\n",
      "Epoch 11, Training Loss: 0.9563346230983734\n",
      "Epoch 12, Training Loss: 0.9546442479245802\n",
      "Epoch 13, Training Loss: 0.9526517511115354\n",
      "Epoch 14, Training Loss: 0.9509003799803116\n",
      "Epoch 15, Training Loss: 0.9489690561154309\n",
      "Epoch 16, Training Loss: 0.9467944869574378\n",
      "Epoch 17, Training Loss: 0.9446372042683994\n",
      "Epoch 18, Training Loss: 0.9423515859070947\n",
      "Epoch 19, Training Loss: 0.9396351725914899\n",
      "Epoch 20, Training Loss: 0.937104081546559\n",
      "Epoch 21, Training Loss: 0.9338711027538075\n",
      "Epoch 22, Training Loss: 0.9306430505303775\n",
      "Epoch 23, Training Loss: 0.9268858476246105\n",
      "Epoch 24, Training Loss: 0.9228659732902751\n",
      "Epoch 25, Training Loss: 0.9183377164952895\n",
      "Epoch 26, Training Loss: 0.9134623915307662\n",
      "Epoch 27, Training Loss: 0.9081954979896545\n",
      "Epoch 28, Training Loss: 0.9022154154497034\n",
      "Epoch 29, Training Loss: 0.8959320878982544\n",
      "Epoch 30, Training Loss: 0.8893316812375013\n",
      "Epoch 31, Training Loss: 0.8822986362962162\n",
      "Epoch 32, Training Loss: 0.8751521277427673\n",
      "Epoch 33, Training Loss: 0.8682221035396351\n",
      "Epoch 34, Training Loss: 0.8615254961041843\n",
      "Epoch 35, Training Loss: 0.8551321218294256\n",
      "Epoch 36, Training Loss: 0.8491312599182129\n",
      "Epoch 37, Training Loss: 0.8438950660649468\n",
      "Epoch 38, Training Loss: 0.839169682264328\n",
      "Epoch 39, Training Loss: 0.835153452648836\n",
      "Epoch 40, Training Loss: 0.831530431509018\n",
      "Epoch 41, Training Loss: 0.828532790927326\n",
      "Epoch 42, Training Loss: 0.8260997061869677\n",
      "Epoch 43, Training Loss: 0.8237567697553073\n",
      "Epoch 44, Training Loss: 0.8221766290243934\n",
      "Epoch 45, Training Loss: 0.8203334612004897\n",
      "Epoch 46, Training Loss: 0.8191593405078439\n",
      "Epoch 47, Training Loss: 0.8180449574133929\n",
      "Epoch 48, Training Loss: 0.817152410885867\n",
      "Epoch 49, Training Loss: 0.8163189887299257\n",
      "Epoch 50, Training Loss: 0.8152921267818002\n",
      "Epoch 51, Training Loss: 0.8147537561023936\n",
      "Epoch 52, Training Loss: 0.8142149674191195\n",
      "Epoch 53, Training Loss: 0.8136922202390783\n",
      "Epoch 54, Training Loss: 0.8133648366086623\n",
      "Epoch 55, Training Loss: 0.8127413347889395\n",
      "Epoch 56, Training Loss: 0.8120139547656564\n",
      "Epoch 57, Training Loss: 0.8119924314583049\n",
      "Epoch 58, Training Loss: 0.8114183531087987\n",
      "Epoch 59, Training Loss: 0.8111638104214388\n",
      "Epoch 60, Training Loss: 0.8106098714996787\n",
      "Epoch 61, Training Loss: 0.8104337392835056\n",
      "Epoch 62, Training Loss: 0.8100357174172121\n",
      "Epoch 63, Training Loss: 0.809827672032749\n",
      "Epoch 64, Training Loss: 0.8096386377951678\n",
      "Epoch 65, Training Loss: 0.8093615355912377\n",
      "Epoch 66, Training Loss: 0.809057871243533\n",
      "Epoch 67, Training Loss: 0.808778042162166\n",
      "Epoch 68, Training Loss: 0.8086972969419816\n",
      "Epoch 69, Training Loss: 0.8083904933929443\n",
      "Epoch 70, Training Loss: 0.808353848878075\n",
      "Epoch 71, Training Loss: 0.8080640463268056\n",
      "Epoch 72, Training Loss: 0.8078408234259662\n",
      "Epoch 73, Training Loss: 0.8080945749843822\n",
      "Epoch 74, Training Loss: 0.807613167552387\n",
      "Epoch 75, Training Loss: 0.8074529058793012\n",
      "Epoch 76, Training Loss: 0.8071912373514737\n",
      "Epoch 77, Training Loss: 0.8070176700283499\n",
      "Epoch 78, Training Loss: 0.8070628799410428\n",
      "Epoch 79, Training Loss: 0.8068429979156045\n",
      "Epoch 80, Training Loss: 0.8065749955177307\n",
      "Epoch 81, Training Loss: 0.8063764464855194\n",
      "Epoch 82, Training Loss: 0.806169353092418\n",
      "Epoch 83, Training Loss: 0.8061987024896285\n",
      "Epoch 84, Training Loss: 0.8060681652321535\n",
      "Epoch 85, Training Loss: 0.8058595673476948\n",
      "Epoch 86, Training Loss: 0.805804802459829\n",
      "Epoch 87, Training Loss: 0.8056320438665502\n",
      "Epoch 88, Training Loss: 0.8053929303674137\n",
      "Epoch 89, Training Loss: 0.805415590019787\n",
      "Epoch 90, Training Loss: 0.8053063542001387\n",
      "Epoch 91, Training Loss: 0.8051147526853225\n",
      "Epoch 92, Training Loss: 0.80509201372371\n",
      "Epoch 93, Training Loss: 0.8049797179418452\n",
      "Epoch 94, Training Loss: 0.8048949808232925\n",
      "Epoch 95, Training Loss: 0.8047287497099708\n",
      "Epoch 96, Training Loss: 0.8047671102776247\n",
      "Epoch 97, Training Loss: 0.8043374635191525\n",
      "Epoch 98, Training Loss: 0.8044206066692576\n",
      "Epoch 99, Training Loss: 0.8042016599458807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:57:11,375] Trial 363 finished with value: 0.6292666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8042304786513833\n",
      "Epoch 1, Training Loss: 0.9878966790788314\n",
      "Epoch 2, Training Loss: 0.9436544850994559\n",
      "Epoch 3, Training Loss: 0.9224642582500682\n",
      "Epoch 4, Training Loss: 0.8954880844845491\n",
      "Epoch 5, Training Loss: 0.8683721808124991\n",
      "Epoch 6, Training Loss: 0.8467664710914388\n",
      "Epoch 7, Training Loss: 0.8321218918351566\n",
      "Epoch 8, Training Loss: 0.823389483830508\n",
      "Epoch 9, Training Loss: 0.8179754517358893\n",
      "Epoch 10, Training Loss: 0.814846371272031\n",
      "Epoch 11, Training Loss: 0.8127857562373666\n",
      "Epoch 12, Training Loss: 0.8117061307149774\n",
      "Epoch 13, Training Loss: 0.8107710540995878\n",
      "Epoch 14, Training Loss: 0.809963634224499\n",
      "Epoch 15, Training Loss: 0.8092292057766634\n",
      "Epoch 16, Training Loss: 0.8088572627656601\n",
      "Epoch 17, Training Loss: 0.8085090307628408\n",
      "Epoch 18, Training Loss: 0.8082079472962548\n",
      "Epoch 19, Training Loss: 0.8077733430441688\n",
      "Epoch 20, Training Loss: 0.8075284281197717\n",
      "Epoch 21, Training Loss: 0.8071291475436266\n",
      "Epoch 22, Training Loss: 0.8069783090843874\n",
      "Epoch 23, Training Loss: 0.8064993374487933\n",
      "Epoch 24, Training Loss: 0.806603901386261\n",
      "Epoch 25, Training Loss: 0.8061492976020365\n",
      "Epoch 26, Training Loss: 0.805754428470836\n",
      "Epoch 27, Training Loss: 0.8057456787193523\n",
      "Epoch 28, Training Loss: 0.8055793514672448\n",
      "Epoch 29, Training Loss: 0.8055924705898061\n",
      "Epoch 30, Training Loss: 0.8049547221380121\n",
      "Epoch 31, Training Loss: 0.8050878879603217\n",
      "Epoch 32, Training Loss: 0.804768946872038\n",
      "Epoch 33, Training Loss: 0.8043977731115678\n",
      "Epoch 34, Training Loss: 0.8046877620500676\n",
      "Epoch 35, Training Loss: 0.804683912782108\n",
      "Epoch 36, Training Loss: 0.8041274555290446\n",
      "Epoch 37, Training Loss: 0.8040311167520635\n",
      "Epoch 38, Training Loss: 0.8038787625817692\n",
      "Epoch 39, Training Loss: 0.8041950997184305\n",
      "Epoch 40, Training Loss: 0.8038015017789953\n",
      "Epoch 41, Training Loss: 0.8037386526079738\n",
      "Epoch 42, Training Loss: 0.8034841567628523\n",
      "Epoch 43, Training Loss: 0.8031366525678074\n",
      "Epoch 44, Training Loss: 0.8032770134420956\n",
      "Epoch 45, Training Loss: 0.8030743022526011\n",
      "Epoch 46, Training Loss: 0.8028207213738385\n",
      "Epoch 47, Training Loss: 0.8028205983077779\n",
      "Epoch 48, Training Loss: 0.8027971199680777\n",
      "Epoch 49, Training Loss: 0.8025172518281376\n",
      "Epoch 50, Training Loss: 0.8023433595545152\n",
      "Epoch 51, Training Loss: 0.8022733975158018\n",
      "Epoch 52, Training Loss: 0.8022482415507821\n",
      "Epoch 53, Training Loss: 0.8019272338642793\n",
      "Epoch 54, Training Loss: 0.8019959457481609\n",
      "Epoch 55, Training Loss: 0.8016803386632134\n",
      "Epoch 56, Training Loss: 0.8017979072122012\n",
      "Epoch 57, Training Loss: 0.8016387386883006\n",
      "Epoch 58, Training Loss: 0.8016570713940788\n",
      "Epoch 59, Training Loss: 0.8014919102191925\n",
      "Epoch 60, Training Loss: 0.8012782564583947\n",
      "Epoch 61, Training Loss: 0.8012314680043389\n",
      "Epoch 62, Training Loss: 0.800924176678938\n",
      "Epoch 63, Training Loss: 0.8010187982110416\n",
      "Epoch 64, Training Loss: 0.8009910459378187\n",
      "Epoch 65, Training Loss: 0.8007319576600018\n",
      "Epoch 66, Training Loss: 0.8009103974875282\n",
      "Epoch 67, Training Loss: 0.8010173245738534\n",
      "Epoch 68, Training Loss: 0.8003285775465124\n",
      "Epoch 69, Training Loss: 0.8006701058499953\n",
      "Epoch 70, Training Loss: 0.8003777366525987\n",
      "Epoch 71, Training Loss: 0.8002459143891054\n",
      "Epoch 72, Training Loss: 0.7999660754905028\n",
      "Epoch 73, Training Loss: 0.8003209850367378\n",
      "Epoch 74, Training Loss: 0.8003209255022161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 18:58:40,888] Trial 364 finished with value: 0.6356666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.800022426072289\n",
      "Epoch 1, Training Loss: 0.8881869560830733\n",
      "Epoch 2, Training Loss: 0.8124892886245952\n",
      "Epoch 3, Training Loss: 0.805140680214938\n",
      "Epoch 4, Training Loss: 0.8032075736803167\n",
      "Epoch 5, Training Loss: 0.8011489607306088\n",
      "Epoch 6, Training Loss: 0.7985833006746629\n",
      "Epoch 7, Training Loss: 0.7966389480057885\n",
      "Epoch 8, Training Loss: 0.7960065263159135\n",
      "Epoch 9, Training Loss: 0.7936076406871572\n",
      "Epoch 10, Training Loss: 0.7924637037165024\n",
      "Epoch 11, Training Loss: 0.7912015281705296\n",
      "Epoch 12, Training Loss: 0.7900903386228225\n",
      "Epoch 13, Training Loss: 0.7895939502996557\n",
      "Epoch 14, Training Loss: 0.7893076792184044\n",
      "Epoch 15, Training Loss: 0.7886314460109262\n",
      "Epoch 16, Training Loss: 0.7883331380170935\n",
      "Epoch 17, Training Loss: 0.7874124672833611\n",
      "Epoch 18, Training Loss: 0.7868566461170421\n",
      "Epoch 19, Training Loss: 0.7863783201750587\n",
      "Epoch 20, Training Loss: 0.7863974956905141\n",
      "Epoch 21, Training Loss: 0.7859792017235475\n",
      "Epoch 22, Training Loss: 0.7850814033957089\n",
      "Epoch 23, Training Loss: 0.7853062452989467\n",
      "Epoch 24, Training Loss: 0.7848506368609036\n",
      "Epoch 25, Training Loss: 0.7847063923583312\n",
      "Epoch 26, Training Loss: 0.783988544029348\n",
      "Epoch 27, Training Loss: 0.7840509947608499\n",
      "Epoch 28, Training Loss: 0.7842603340569665\n",
      "Epoch 29, Training Loss: 0.7832975910691654\n",
      "Epoch 30, Training Loss: 0.7835652267231661\n",
      "Epoch 31, Training Loss: 0.7830971588807948\n",
      "Epoch 32, Training Loss: 0.7831791223497951\n",
      "Epoch 33, Training Loss: 0.7828429276101729\n",
      "Epoch 34, Training Loss: 0.782713662666433\n",
      "Epoch 35, Training Loss: 0.7826022448259241\n",
      "Epoch 36, Training Loss: 0.7818186867237091\n",
      "Epoch 37, Training Loss: 0.7820126783146578\n",
      "Epoch 38, Training Loss: 0.781935566593619\n",
      "Epoch 39, Training Loss: 0.7818667761718525\n",
      "Epoch 40, Training Loss: 0.7812246265832116\n",
      "Epoch 41, Training Loss: 0.7817863790427937\n",
      "Epoch 42, Training Loss: 0.7814716039685642\n",
      "Epoch 43, Training Loss: 0.781258260712904\n",
      "Epoch 44, Training Loss: 0.7807390883389641\n",
      "Epoch 45, Training Loss: 0.7811494607785169\n",
      "Epoch 46, Training Loss: 0.7811215423836427\n",
      "Epoch 47, Training Loss: 0.7810750090374666\n",
      "Epoch 48, Training Loss: 0.7806937156004065\n",
      "Epoch 49, Training Loss: 0.780149713824777\n",
      "Epoch 50, Training Loss: 0.7804831809857312\n",
      "Epoch 51, Training Loss: 0.7805801813041463\n",
      "Epoch 52, Training Loss: 0.7800214742912965\n",
      "Epoch 53, Training Loss: 0.7804419035070083\n",
      "Epoch 54, Training Loss: 0.7803694392653072\n",
      "Epoch 55, Training Loss: 0.7803589292834787\n",
      "Epoch 56, Training Loss: 0.7798685089279623\n",
      "Epoch 57, Training Loss: 0.779790187513127\n",
      "Epoch 58, Training Loss: 0.7798262056883644\n",
      "Epoch 59, Training Loss: 0.7798362912851221\n",
      "Epoch 60, Training Loss: 0.7788746189369875\n",
      "Epoch 61, Training Loss: 0.7792621283671435\n",
      "Epoch 62, Training Loss: 0.7791599246333627\n",
      "Epoch 63, Training Loss: 0.7793502785177792\n",
      "Epoch 64, Training Loss: 0.7795331241804011\n",
      "Epoch 65, Training Loss: 0.7790818611313315\n",
      "Epoch 66, Training Loss: 0.7794131166794721\n",
      "Epoch 67, Training Loss: 0.7782684319860795\n",
      "Epoch 68, Training Loss: 0.7788233551558327\n",
      "Epoch 69, Training Loss: 0.7786573980836308\n",
      "Epoch 70, Training Loss: 0.7786514134968028\n",
      "Epoch 71, Training Loss: 0.7784231685890871\n",
      "Epoch 72, Training Loss: 0.778431632939507\n",
      "Epoch 73, Training Loss: 0.7779195589177749\n",
      "Epoch 74, Training Loss: 0.7781033967523013\n",
      "Epoch 75, Training Loss: 0.7782102495081284\n",
      "Epoch 76, Training Loss: 0.7779413308816797\n",
      "Epoch 77, Training Loss: 0.7776398743601406\n",
      "Epoch 78, Training Loss: 0.7778822931822609\n",
      "Epoch 79, Training Loss: 0.7775604014536913\n",
      "Epoch 80, Training Loss: 0.7777392803220188\n",
      "Epoch 81, Training Loss: 0.7775550048491534\n",
      "Epoch 82, Training Loss: 0.7775847507224364\n",
      "Epoch 83, Training Loss: 0.7776050401435178\n",
      "Epoch 84, Training Loss: 0.7773264491558075\n",
      "Epoch 85, Training Loss: 0.7772517063337214\n",
      "Epoch 86, Training Loss: 0.7772653001196245\n",
      "Epoch 87, Training Loss: 0.7768817782402039\n",
      "Epoch 88, Training Loss: 0.7767763765419231\n",
      "Epoch 89, Training Loss: 0.7767108348537893\n",
      "Epoch 90, Training Loss: 0.7769066544841318\n",
      "Epoch 91, Training Loss: 0.7767504481007071\n",
      "Epoch 92, Training Loss: 0.7768498913680806\n",
      "Epoch 93, Training Loss: 0.7766078397806953\n",
      "Epoch 94, Training Loss: 0.7765737129660214\n",
      "Epoch 95, Training Loss: 0.7762838108399335\n",
      "Epoch 96, Training Loss: 0.7763621911581825\n",
      "Epoch 97, Training Loss: 0.7762924831755021\n",
      "Epoch 98, Training Loss: 0.7764003387619467\n",
      "Epoch 99, Training Loss: 0.7765143186204574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:00:58,742] Trial 365 finished with value: 0.641 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7761117633651284\n",
      "Epoch 1, Training Loss: 0.8668010981643901\n",
      "Epoch 2, Training Loss: 0.8136092545705683\n",
      "Epoch 3, Training Loss: 0.8053691456598394\n",
      "Epoch 4, Training Loss: 0.8048806907850153\n",
      "Epoch 5, Training Loss: 0.8021462044295142\n",
      "Epoch 6, Training Loss: 0.8003036499023437\n",
      "Epoch 7, Training Loss: 0.7977676260471344\n",
      "Epoch 8, Training Loss: 0.7969084513888639\n",
      "Epoch 9, Training Loss: 0.7973349903611576\n",
      "Epoch 10, Training Loss: 0.7951960978087257\n",
      "Epoch 11, Training Loss: 0.7948795464459587\n",
      "Epoch 12, Training Loss: 0.7936727235597723\n",
      "Epoch 13, Training Loss: 0.7931876928666058\n",
      "Epoch 14, Training Loss: 0.7937801420688629\n",
      "Epoch 15, Training Loss: 0.7929967352923225\n",
      "Epoch 16, Training Loss: 0.792132786021513\n",
      "Epoch 17, Training Loss: 0.7921479506352368\n",
      "Epoch 18, Training Loss: 0.7920270032742445\n",
      "Epoch 19, Training Loss: 0.7908809510399314\n",
      "Epoch 20, Training Loss: 0.7912586915493012\n",
      "Epoch 21, Training Loss: 0.7908026140577653\n",
      "Epoch 22, Training Loss: 0.7899208563215593\n",
      "Epoch 23, Training Loss: 0.7895933290088878\n",
      "Epoch 24, Training Loss: 0.7891882653797374\n",
      "Epoch 25, Training Loss: 0.7892760781680837\n",
      "Epoch 26, Training Loss: 0.7888661370557897\n",
      "Epoch 27, Training Loss: 0.7894561617514666\n",
      "Epoch 28, Training Loss: 0.7893035028261297\n",
      "Epoch 29, Training Loss: 0.7882699732219471\n",
      "Epoch 30, Training Loss: 0.7878347504138946\n",
      "Epoch 31, Training Loss: 0.7879789236713858\n",
      "Epoch 32, Training Loss: 0.7887014527180616\n",
      "Epoch 33, Training Loss: 0.7876149842318366\n",
      "Epoch 34, Training Loss: 0.789061677455902\n",
      "Epoch 35, Training Loss: 0.7871844401079066\n",
      "Epoch 36, Training Loss: 0.7870740710987765\n",
      "Epoch 37, Training Loss: 0.7869071378427394\n",
      "Epoch 38, Training Loss: 0.786963803838281\n",
      "Epoch 39, Training Loss: 0.7863240899759181\n",
      "Epoch 40, Training Loss: 0.7863188603345086\n",
      "Epoch 41, Training Loss: 0.7862279298025019\n",
      "Epoch 42, Training Loss: 0.786678249835968\n",
      "Epoch 43, Training Loss: 0.7865962762692396\n",
      "Epoch 44, Training Loss: 0.7856731381135829\n",
      "Epoch 45, Training Loss: 0.7862846319815692\n",
      "Epoch 46, Training Loss: 0.7862948423273424\n",
      "Epoch 47, Training Loss: 0.7862778719733743\n",
      "Epoch 48, Training Loss: 0.7860333808730631\n",
      "Epoch 49, Training Loss: 0.7858036583311417\n",
      "Epoch 50, Training Loss: 0.7854857444763184\n",
      "Epoch 51, Training Loss: 0.7853394231375526\n",
      "Epoch 52, Training Loss: 0.7857992581058951\n",
      "Epoch 53, Training Loss: 0.7851707323158489\n",
      "Epoch 54, Training Loss: 0.7853422052018783\n",
      "Epoch 55, Training Loss: 0.7845074901160072\n",
      "Epoch 56, Training Loss: 0.7853055159484639\n",
      "Epoch 57, Training Loss: 0.7856395482315737\n",
      "Epoch 58, Training Loss: 0.7855472149568445\n",
      "Epoch 59, Training Loss: 0.7843516964070937\n",
      "Epoch 60, Training Loss: 0.7846485049584333\n",
      "Epoch 61, Training Loss: 0.7845278321294223\n",
      "Epoch 62, Training Loss: 0.7843993520035463\n",
      "Epoch 63, Training Loss: 0.7841959387414595\n",
      "Epoch 64, Training Loss: 0.7840827134777518\n",
      "Epoch 65, Training Loss: 0.7841093438513139\n",
      "Epoch 66, Training Loss: 0.7842744397415834\n",
      "Epoch 67, Training Loss: 0.784015795693678\n",
      "Epoch 68, Training Loss: 0.7844033800854402\n",
      "Epoch 69, Training Loss: 0.7835979835426106\n",
      "Epoch 70, Training Loss: 0.7838952065215391\n",
      "Epoch 71, Training Loss: 0.7842409318334916\n",
      "Epoch 72, Training Loss: 0.783483539609348\n",
      "Epoch 73, Training Loss: 0.783463987532784\n",
      "Epoch 74, Training Loss: 0.7825781204419977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:02:42,484] Trial 366 finished with value: 0.6360666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7836859487084782\n",
      "Epoch 1, Training Loss: 0.9336027723505981\n",
      "Epoch 2, Training Loss: 0.8687480325985671\n",
      "Epoch 3, Training Loss: 0.8311742177583221\n",
      "Epoch 4, Training Loss: 0.816513665278155\n",
      "Epoch 5, Training Loss: 0.8120356886906731\n",
      "Epoch 6, Training Loss: 0.8104423718344896\n",
      "Epoch 7, Training Loss: 0.8089339921348974\n",
      "Epoch 8, Training Loss: 0.8080756284240493\n",
      "Epoch 9, Training Loss: 0.807682990938201\n",
      "Epoch 10, Training Loss: 0.8073692977876592\n",
      "Epoch 11, Training Loss: 0.8073915172340278\n",
      "Epoch 12, Training Loss: 0.8070490866675413\n",
      "Epoch 13, Training Loss: 0.8064886942841953\n",
      "Epoch 14, Training Loss: 0.8062108602738918\n",
      "Epoch 15, Training Loss: 0.8053114101850897\n",
      "Epoch 16, Training Loss: 0.8056591306413923\n",
      "Epoch 17, Training Loss: 0.8050093030122886\n",
      "Epoch 18, Training Loss: 0.8045930361389217\n",
      "Epoch 19, Training Loss: 0.8047457162598918\n",
      "Epoch 20, Training Loss: 0.8043624352691765\n",
      "Epoch 21, Training Loss: 0.8046382497127791\n",
      "Epoch 22, Training Loss: 0.8045000464396369\n",
      "Epoch 23, Training Loss: 0.8040869335482891\n",
      "Epoch 24, Training Loss: 0.8036976410034008\n",
      "Epoch 25, Training Loss: 0.8047617304593997\n",
      "Epoch 26, Training Loss: 0.803451100686439\n",
      "Epoch 27, Training Loss: 0.8022500399360083\n",
      "Epoch 28, Training Loss: 0.8022741588434779\n",
      "Epoch 29, Training Loss: 0.8030548092117883\n",
      "Epoch 30, Training Loss: 0.8025198164739107\n",
      "Epoch 31, Training Loss: 0.8021745777667914\n",
      "Epoch 32, Training Loss: 0.8021520959703545\n",
      "Epoch 33, Training Loss: 0.8020253997996337\n",
      "Epoch 34, Training Loss: 0.8011843561229849\n",
      "Epoch 35, Training Loss: 0.8012171523911612\n",
      "Epoch 36, Training Loss: 0.8008551982112397\n",
      "Epoch 37, Training Loss: 0.8012145966515505\n",
      "Epoch 38, Training Loss: 0.8006869975785563\n",
      "Epoch 39, Training Loss: 0.8010040483080355\n",
      "Epoch 40, Training Loss: 0.8003572696133664\n",
      "Epoch 41, Training Loss: 0.8010711518445409\n",
      "Epoch 42, Training Loss: 0.8010318155575515\n",
      "Epoch 43, Training Loss: 0.8015398365214355\n",
      "Epoch 44, Training Loss: 0.7999761594865555\n",
      "Epoch 45, Training Loss: 0.7996433227582085\n",
      "Epoch 46, Training Loss: 0.7998023203441075\n",
      "Epoch 47, Training Loss: 0.7994944323274426\n",
      "Epoch 48, Training Loss: 0.8000159455421275\n",
      "Epoch 49, Training Loss: 0.7994683484385784\n",
      "Epoch 50, Training Loss: 0.7992099121100921\n",
      "Epoch 51, Training Loss: 0.7986582162684964\n",
      "Epoch 52, Training Loss: 0.799239116234887\n",
      "Epoch 53, Training Loss: 0.798773031216815\n",
      "Epoch 54, Training Loss: 0.7989088366802474\n",
      "Epoch 55, Training Loss: 0.7989793514846859\n",
      "Epoch 56, Training Loss: 0.7989275687619259\n",
      "Epoch 57, Training Loss: 0.7988207665601171\n",
      "Epoch 58, Training Loss: 0.7986950012077962\n",
      "Epoch 59, Training Loss: 0.7985248018028145\n",
      "Epoch 60, Training Loss: 0.7982496333301515\n",
      "Epoch 61, Training Loss: 0.7992782864355503\n",
      "Epoch 62, Training Loss: 0.7981757691032008\n",
      "Epoch 63, Training Loss: 0.7983265893799918\n",
      "Epoch 64, Training Loss: 0.7988212574693493\n",
      "Epoch 65, Training Loss: 0.7987688141657894\n",
      "Epoch 66, Training Loss: 0.7980927672601283\n",
      "Epoch 67, Training Loss: 0.7978878822541775\n",
      "Epoch 68, Training Loss: 0.7979358181917577\n",
      "Epoch 69, Training Loss: 0.798395047510477\n",
      "Epoch 70, Training Loss: 0.797739409145556\n",
      "Epoch 71, Training Loss: 0.7976451705272932\n",
      "Epoch 72, Training Loss: 0.7976754203774875\n",
      "Epoch 73, Training Loss: 0.7979288461513089\n",
      "Epoch 74, Training Loss: 0.7975502214933696\n",
      "Epoch 75, Training Loss: 0.7976054213997117\n",
      "Epoch 76, Training Loss: 0.7979767491046648\n",
      "Epoch 77, Training Loss: 0.7977178163098213\n",
      "Epoch 78, Training Loss: 0.7976112991347348\n",
      "Epoch 79, Training Loss: 0.7974876592930098\n",
      "Epoch 80, Training Loss: 0.7975484808584801\n",
      "Epoch 81, Training Loss: 0.7979176951530285\n",
      "Epoch 82, Training Loss: 0.7970976134888211\n",
      "Epoch 83, Training Loss: 0.7980538777838972\n",
      "Epoch 84, Training Loss: 0.7982120028115753\n",
      "Epoch 85, Training Loss: 0.7974830169426768\n",
      "Epoch 86, Training Loss: 0.7974598367411391\n",
      "Epoch 87, Training Loss: 0.7967492388603382\n",
      "Epoch 88, Training Loss: 0.7974975424601619\n",
      "Epoch 89, Training Loss: 0.7969629343290975\n",
      "Epoch 90, Training Loss: 0.7964902358843868\n",
      "Epoch 91, Training Loss: 0.7972316452435084\n",
      "Epoch 92, Training Loss: 0.7970001677821453\n",
      "Epoch 93, Training Loss: 0.7974019237927028\n",
      "Epoch 94, Training Loss: 0.7971827045419162\n",
      "Epoch 95, Training Loss: 0.7972428962700349\n",
      "Epoch 96, Training Loss: 0.7965376347079313\n",
      "Epoch 97, Training Loss: 0.7971295583517032\n",
      "Epoch 98, Training Loss: 0.7971407952165245\n",
      "Epoch 99, Training Loss: 0.7967220144164293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:04:15,543] Trial 367 finished with value: 0.6189333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.797023710272366\n",
      "Epoch 1, Training Loss: 0.8990827693975061\n",
      "Epoch 2, Training Loss: 0.8215654738863608\n",
      "Epoch 3, Training Loss: 0.8145586019171808\n",
      "Epoch 4, Training Loss: 0.8116725942245999\n",
      "Epoch 5, Training Loss: 0.8067179964897327\n",
      "Epoch 6, Training Loss: 0.8030605019483351\n",
      "Epoch 7, Training Loss: 0.8008381749454297\n",
      "Epoch 8, Training Loss: 0.7993340808646123\n",
      "Epoch 9, Training Loss: 0.7973400007513233\n",
      "Epoch 10, Training Loss: 0.7958474035549881\n",
      "Epoch 11, Training Loss: 0.7945376371082506\n",
      "Epoch 12, Training Loss: 0.7946152857371739\n",
      "Epoch 13, Training Loss: 0.792495965733564\n",
      "Epoch 14, Training Loss: 0.7931626215016931\n",
      "Epoch 15, Training Loss: 0.7909252171677754\n",
      "Epoch 16, Training Loss: 0.7917223691940307\n",
      "Epoch 17, Training Loss: 0.7910433561282051\n",
      "Epoch 18, Training Loss: 0.7915772362759239\n",
      "Epoch 19, Training Loss: 0.7902914684517939\n",
      "Epoch 20, Training Loss: 0.7906300072383163\n",
      "Epoch 21, Training Loss: 0.789588889591676\n",
      "Epoch 22, Training Loss: 0.7897450793954662\n",
      "Epoch 23, Training Loss: 0.7896812719510014\n",
      "Epoch 24, Training Loss: 0.788892444452845\n",
      "Epoch 25, Training Loss: 0.787842820685609\n",
      "Epoch 26, Training Loss: 0.7886708734627057\n",
      "Epoch 27, Training Loss: 0.788453898125125\n",
      "Epoch 28, Training Loss: 0.7877715304381865\n",
      "Epoch 29, Training Loss: 0.7876341030113679\n",
      "Epoch 30, Training Loss: 0.788216391541904\n",
      "Epoch 31, Training Loss: 0.7872686488287789\n",
      "Epoch 32, Training Loss: 0.7872303854253956\n",
      "Epoch 33, Training Loss: 0.7873899264443189\n",
      "Epoch 34, Training Loss: 0.7873388211529954\n",
      "Epoch 35, Training Loss: 0.7869704628349247\n",
      "Epoch 36, Training Loss: 0.7861545877349108\n",
      "Epoch 37, Training Loss: 0.78633747351797\n",
      "Epoch 38, Training Loss: 0.7855801575165943\n",
      "Epoch 39, Training Loss: 0.786172072062815\n",
      "Epoch 40, Training Loss: 0.785603462036391\n",
      "Epoch 41, Training Loss: 0.7856644578446123\n",
      "Epoch 42, Training Loss: 0.7857012989825772\n",
      "Epoch 43, Training Loss: 0.7849470409683715\n",
      "Epoch 44, Training Loss: 0.7856862283290778\n",
      "Epoch 45, Training Loss: 0.7859160330958832\n",
      "Epoch 46, Training Loss: 0.7851007431073296\n",
      "Epoch 47, Training Loss: 0.7862302820485337\n",
      "Epoch 48, Training Loss: 0.7850981810039147\n",
      "Epoch 49, Training Loss: 0.7851493780774281\n",
      "Epoch 50, Training Loss: 0.78472480791852\n",
      "Epoch 51, Training Loss: 0.785282700223134\n",
      "Epoch 52, Training Loss: 0.7847452344750999\n",
      "Epoch 53, Training Loss: 0.7849761695790112\n",
      "Epoch 54, Training Loss: 0.7840391370586883\n",
      "Epoch 55, Training Loss: 0.7847869478670279\n",
      "Epoch 56, Training Loss: 0.7849482953996587\n",
      "Epoch 57, Training Loss: 0.7845090763909476\n",
      "Epoch 58, Training Loss: 0.7842382295687396\n",
      "Epoch 59, Training Loss: 0.7845233471770036\n",
      "Epoch 60, Training Loss: 0.7831088239985301\n",
      "Epoch 61, Training Loss: 0.7840562501348051\n",
      "Epoch 62, Training Loss: 0.7835873459514818\n",
      "Epoch 63, Training Loss: 0.7841047585458684\n",
      "Epoch 64, Training Loss: 0.7837037462040894\n",
      "Epoch 65, Training Loss: 0.7835771967593889\n",
      "Epoch 66, Training Loss: 0.783064934095942\n",
      "Epoch 67, Training Loss: 0.784006340880143\n",
      "Epoch 68, Training Loss: 0.7829083462406818\n",
      "Epoch 69, Training Loss: 0.782806674699138\n",
      "Epoch 70, Training Loss: 0.7832307609400355\n",
      "Epoch 71, Training Loss: 0.782558582689529\n",
      "Epoch 72, Training Loss: 0.7836703059368564\n",
      "Epoch 73, Training Loss: 0.7829354780060904\n",
      "Epoch 74, Training Loss: 0.7830700598264996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:05:39,277] Trial 368 finished with value: 0.6332666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.782394262184774\n",
      "Epoch 1, Training Loss: 0.8464135764626896\n",
      "Epoch 2, Training Loss: 0.810321124160991\n",
      "Epoch 3, Training Loss: 0.808338426772286\n",
      "Epoch 4, Training Loss: 0.8066800773143769\n",
      "Epoch 5, Training Loss: 0.8045639304553761\n",
      "Epoch 6, Training Loss: 0.8032819731095258\n",
      "Epoch 7, Training Loss: 0.801542916578405\n",
      "Epoch 8, Training Loss: 0.8015569836953107\n",
      "Epoch 9, Training Loss: 0.8005684487959918\n",
      "Epoch 10, Training Loss: 0.8005772785579457\n",
      "Epoch 11, Training Loss: 0.8000127963458791\n",
      "Epoch 12, Training Loss: 0.7984091248933006\n",
      "Epoch 13, Training Loss: 0.7985188130771412\n",
      "Epoch 14, Training Loss: 0.7989065499165479\n",
      "Epoch 15, Training Loss: 0.7994111174695632\n",
      "Epoch 16, Training Loss: 0.7972756573733162\n",
      "Epoch 17, Training Loss: 0.7977796036355636\n",
      "Epoch 18, Training Loss: 0.7971867237371557\n",
      "Epoch 19, Training Loss: 0.7980145448796889\n",
      "Epoch 20, Training Loss: 0.7974825125582078\n",
      "Epoch 21, Training Loss: 0.7970034835619085\n",
      "Epoch 22, Training Loss: 0.797087020593531\n",
      "Epoch 23, Training Loss: 0.7965888838908252\n",
      "Epoch 24, Training Loss: 0.7970816525290995\n",
      "Epoch 25, Training Loss: 0.7971502755669987\n",
      "Epoch 26, Training Loss: 0.7966846755672904\n",
      "Epoch 27, Training Loss: 0.7967314669665169\n",
      "Epoch 28, Training Loss: 0.7957466914373286\n",
      "Epoch 29, Training Loss: 0.7977007218669443\n",
      "Epoch 30, Training Loss: 0.7966392214859234\n",
      "Epoch 31, Training Loss: 0.7964873801960665\n",
      "Epoch 32, Training Loss: 0.7959345359662\n",
      "Epoch 33, Training Loss: 0.7963405973069808\n",
      "Epoch 34, Training Loss: 0.7960689832182491\n",
      "Epoch 35, Training Loss: 0.7959443765528061\n",
      "Epoch 36, Training Loss: 0.7958681502061732\n",
      "Epoch 37, Training Loss: 0.7950212426746592\n",
      "Epoch 38, Training Loss: 0.7962076265671674\n",
      "Epoch 39, Training Loss: 0.7949077763978173\n",
      "Epoch 40, Training Loss: 0.7954945823024301\n",
      "Epoch 41, Training Loss: 0.7955709931429694\n",
      "Epoch 42, Training Loss: 0.7949261193415698\n",
      "Epoch 43, Training Loss: 0.7949902849337634\n",
      "Epoch 44, Training Loss: 0.7945236771948198\n",
      "Epoch 45, Training Loss: 0.7948190539724687\n",
      "Epoch 46, Training Loss: 0.795768268809599\n",
      "Epoch 47, Training Loss: 0.7952534054307376\n",
      "Epoch 48, Training Loss: 0.7945637634922477\n",
      "Epoch 49, Training Loss: 0.7946986415105708\n",
      "Epoch 50, Training Loss: 0.794866901846493\n",
      "Epoch 51, Training Loss: 0.7948428234633278\n",
      "Epoch 52, Training Loss: 0.7940198925663443\n",
      "Epoch 53, Training Loss: 0.7952043724060058\n",
      "Epoch 54, Training Loss: 0.7938349393536063\n",
      "Epoch 55, Training Loss: 0.7942716461069443\n",
      "Epoch 56, Training Loss: 0.7938179075717926\n",
      "Epoch 57, Training Loss: 0.7943451286063474\n",
      "Epoch 58, Training Loss: 0.7943468080548679\n",
      "Epoch 59, Training Loss: 0.7940135378697339\n",
      "Epoch 60, Training Loss: 0.7941302464288824\n",
      "Epoch 61, Training Loss: 0.7942666942933027\n",
      "Epoch 62, Training Loss: 0.7942718813699835\n",
      "Epoch 63, Training Loss: 0.7939454101113712\n",
      "Epoch 64, Training Loss: 0.7945559940618627\n",
      "Epoch 65, Training Loss: 0.7941299530337839\n",
      "Epoch 66, Training Loss: 0.7938415769268484\n",
      "Epoch 67, Training Loss: 0.7937210067580728\n",
      "Epoch 68, Training Loss: 0.7936902469045976\n",
      "Epoch 69, Training Loss: 0.7932944000468535\n",
      "Epoch 70, Training Loss: 0.7933883805134717\n",
      "Epoch 71, Training Loss: 0.7940788581090815\n",
      "Epoch 72, Training Loss: 0.7937977792936213\n",
      "Epoch 73, Training Loss: 0.7932959195445566\n",
      "Epoch 74, Training Loss: 0.7942671752677244\n",
      "Epoch 75, Training Loss: 0.7933294803254745\n",
      "Epoch 76, Training Loss: 0.79369742176112\n",
      "Epoch 77, Training Loss: 0.7938497344886556\n",
      "Epoch 78, Training Loss: 0.7943274408228257\n",
      "Epoch 79, Training Loss: 0.7936199257654303\n",
      "Epoch 80, Training Loss: 0.7934000514535343\n",
      "Epoch 81, Training Loss: 0.7932321614377639\n",
      "Epoch 82, Training Loss: 0.7933642445592319\n",
      "Epoch 83, Training Loss: 0.7933938849673552\n",
      "Epoch 84, Training Loss: 0.7930978725237005\n",
      "Epoch 85, Training Loss: 0.7938568228833816\n",
      "Epoch 86, Training Loss: 0.7938456677689272\n",
      "Epoch 87, Training Loss: 0.7933647638909958\n",
      "Epoch 88, Training Loss: 0.792812227010727\n",
      "Epoch 89, Training Loss: 0.7929402403270497\n",
      "Epoch 90, Training Loss: 0.7932205131474663\n",
      "Epoch 91, Training Loss: 0.7932744355762706\n",
      "Epoch 92, Training Loss: 0.7934774577617645\n",
      "Epoch 93, Training Loss: 0.7926960565763361\n",
      "Epoch 94, Training Loss: 0.7933678866133971\n",
      "Epoch 95, Training Loss: 0.7930317427831538\n",
      "Epoch 96, Training Loss: 0.7930757749080658\n",
      "Epoch 97, Training Loss: 0.7929985037971945\n",
      "Epoch 98, Training Loss: 0.7932582814553205\n",
      "Epoch 99, Training Loss: 0.7930423144733204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:07:38,128] Trial 369 finished with value: 0.6348 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7927497487909654\n",
      "Epoch 1, Training Loss: 0.8438779078928151\n",
      "Epoch 2, Training Loss: 0.814176076695435\n",
      "Epoch 3, Training Loss: 0.809569651829569\n",
      "Epoch 4, Training Loss: 0.8075612527087219\n",
      "Epoch 5, Training Loss: 0.8036517042862742\n",
      "Epoch 6, Training Loss: 0.8047050816672189\n",
      "Epoch 7, Training Loss: 0.8023013812258728\n",
      "Epoch 8, Training Loss: 0.8019734901593144\n",
      "Epoch 9, Training Loss: 0.8009763310726424\n",
      "Epoch 10, Training Loss: 0.8012276071354859\n",
      "Epoch 11, Training Loss: 0.8007925106170483\n",
      "Epoch 12, Training Loss: 0.8007763351712908\n",
      "Epoch 13, Training Loss: 0.8000851522710987\n",
      "Epoch 14, Training Loss: 0.8003648283786343\n",
      "Epoch 15, Training Loss: 0.7985889077186584\n",
      "Epoch 16, Training Loss: 0.7999923609253159\n",
      "Epoch 17, Training Loss: 0.7985280867806055\n",
      "Epoch 18, Training Loss: 0.7982212171518713\n",
      "Epoch 19, Training Loss: 0.7991195686777731\n",
      "Epoch 20, Training Loss: 0.7986418242741348\n",
      "Epoch 21, Training Loss: 0.7987227084941434\n",
      "Epoch 22, Training Loss: 0.7980308464595249\n",
      "Epoch 23, Training Loss: 0.7977752263384654\n",
      "Epoch 24, Training Loss: 0.7972666070873576\n",
      "Epoch 25, Training Loss: 0.7972647009039284\n",
      "Epoch 26, Training Loss: 0.7971361426482523\n",
      "Epoch 27, Training Loss: 0.7972775474526829\n",
      "Epoch 28, Training Loss: 0.798070614499257\n",
      "Epoch 29, Training Loss: 0.7975368558912349\n",
      "Epoch 30, Training Loss: 0.796080454041187\n",
      "Epoch 31, Training Loss: 0.7974908888788151\n",
      "Epoch 32, Training Loss: 0.7975947668677882\n",
      "Epoch 33, Training Loss: 0.7959196898273956\n",
      "Epoch 34, Training Loss: 0.7968269458390717\n",
      "Epoch 35, Training Loss: 0.797912137131942\n",
      "Epoch 36, Training Loss: 0.7981428344446914\n",
      "Epoch 37, Training Loss: 0.7947989605423204\n",
      "Epoch 38, Training Loss: 0.7978528090885707\n",
      "Epoch 39, Training Loss: 0.7963426832865952\n",
      "Epoch 40, Training Loss: 0.7960007808262244\n",
      "Epoch 41, Training Loss: 0.7965957147734506\n",
      "Epoch 42, Training Loss: 0.7962041291975437\n",
      "Epoch 43, Training Loss: 0.7962841147766974\n",
      "Epoch 44, Training Loss: 0.7971510111837459\n",
      "Epoch 45, Training Loss: 0.7963720923975894\n",
      "Epoch 46, Training Loss: 0.7962664778967549\n",
      "Epoch 47, Training Loss: 0.7959191899550588\n",
      "Epoch 48, Training Loss: 0.7966378191359957\n",
      "Epoch 49, Training Loss: 0.7960548405360458\n",
      "Epoch 50, Training Loss: 0.7965223020180724\n",
      "Epoch 51, Training Loss: 0.7956712189473604\n",
      "Epoch 52, Training Loss: 0.795548098876064\n",
      "Epoch 53, Training Loss: 0.795323984425767\n",
      "Epoch 54, Training Loss: 0.7971955941135722\n",
      "Epoch 55, Training Loss: 0.7963148347417215\n",
      "Epoch 56, Training Loss: 0.7949976800079632\n",
      "Epoch 57, Training Loss: 0.7950297634404405\n",
      "Epoch 58, Training Loss: 0.7959483669216472\n",
      "Epoch 59, Training Loss: 0.7957970935599248\n",
      "Epoch 60, Training Loss: 0.7953646502996746\n",
      "Epoch 61, Training Loss: 0.7945797904093462\n",
      "Epoch 62, Training Loss: 0.7957177627355533\n",
      "Epoch 63, Training Loss: 0.7958536999566215\n",
      "Epoch 64, Training Loss: 0.7957285457983949\n",
      "Epoch 65, Training Loss: 0.7956902075530892\n",
      "Epoch 66, Training Loss: 0.7973190365877366\n",
      "Epoch 67, Training Loss: 0.7965135175482672\n",
      "Epoch 68, Training Loss: 0.794928046635219\n",
      "Epoch 69, Training Loss: 0.7955245108532726\n",
      "Epoch 70, Training Loss: 0.7949594012776712\n",
      "Epoch 71, Training Loss: 0.794863278077061\n",
      "Epoch 72, Training Loss: 0.795911419302001\n",
      "Epoch 73, Training Loss: 0.7953187428022686\n",
      "Epoch 74, Training Loss: 0.7952010740014843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:09:07,817] Trial 370 finished with value: 0.6386 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7946822960573928\n",
      "Epoch 1, Training Loss: 0.9951195873712239\n",
      "Epoch 2, Training Loss: 0.9480854422526253\n",
      "Epoch 3, Training Loss: 0.9303628084354831\n",
      "Epoch 4, Training Loss: 0.9086987285685718\n",
      "Epoch 5, Training Loss: 0.8847457607885949\n",
      "Epoch 6, Training Loss: 0.8623152866399377\n",
      "Epoch 7, Training Loss: 0.8446691920882777\n",
      "Epoch 8, Training Loss: 0.8328300673262518\n",
      "Epoch 9, Training Loss: 0.8241197448027762\n",
      "Epoch 10, Training Loss: 0.8191719504227316\n",
      "Epoch 11, Training Loss: 0.8156003460848242\n",
      "Epoch 12, Training Loss: 0.813802687028297\n",
      "Epoch 13, Training Loss: 0.8125224512322504\n",
      "Epoch 14, Training Loss: 0.8115287385488811\n",
      "Epoch 15, Training Loss: 0.8104665423694409\n",
      "Epoch 16, Training Loss: 0.809905175069221\n",
      "Epoch 17, Training Loss: 0.8092455164830488\n",
      "Epoch 18, Training Loss: 0.8093892269564751\n",
      "Epoch 19, Training Loss: 0.8082486376278383\n",
      "Epoch 20, Training Loss: 0.808533055172827\n",
      "Epoch 21, Training Loss: 0.8077021703236085\n",
      "Epoch 22, Training Loss: 0.8083065877283426\n",
      "Epoch 23, Training Loss: 0.8072685276655326\n",
      "Epoch 24, Training Loss: 0.8076509994671757\n",
      "Epoch 25, Training Loss: 0.8073572993278504\n",
      "Epoch 26, Training Loss: 0.8070312206010173\n",
      "Epoch 27, Training Loss: 0.806736890086554\n",
      "Epoch 28, Training Loss: 0.8065127997470082\n",
      "Epoch 29, Training Loss: 0.8061816850999244\n",
      "Epoch 30, Training Loss: 0.806242104939052\n",
      "Epoch 31, Training Loss: 0.8060306715786009\n",
      "Epoch 32, Training Loss: 0.8055919119289943\n",
      "Epoch 33, Training Loss: 0.8060454333635201\n",
      "Epoch 34, Training Loss: 0.8051561196047561\n",
      "Epoch 35, Training Loss: 0.8053769134041062\n",
      "Epoch 36, Training Loss: 0.8058947516563243\n",
      "Epoch 37, Training Loss: 0.8047469933678333\n",
      "Epoch 38, Training Loss: 0.8053082064578407\n",
      "Epoch 39, Training Loss: 0.8046773537657315\n",
      "Epoch 40, Training Loss: 0.8048421352429498\n",
      "Epoch 41, Training Loss: 0.8046406971780877\n",
      "Epoch 42, Training Loss: 0.8047448463009712\n",
      "Epoch 43, Training Loss: 0.8039854466466976\n",
      "Epoch 44, Training Loss: 0.8040545909028304\n",
      "Epoch 45, Training Loss: 0.804296048153612\n",
      "Epoch 46, Training Loss: 0.8040250059357263\n",
      "Epoch 47, Training Loss: 0.8032146678831344\n",
      "Epoch 48, Training Loss: 0.8027940771633522\n",
      "Epoch 49, Training Loss: 0.8025741182323686\n",
      "Epoch 50, Training Loss: 0.8028672735493882\n",
      "Epoch 51, Training Loss: 0.80314049191941\n",
      "Epoch 52, Training Loss: 0.8026486120725933\n",
      "Epoch 53, Training Loss: 0.8034830919782022\n",
      "Epoch 54, Training Loss: 0.802705847141438\n",
      "Epoch 55, Training Loss: 0.8024299630545135\n",
      "Epoch 56, Training Loss: 0.8020632595944225\n",
      "Epoch 57, Training Loss: 0.8016100703325487\n",
      "Epoch 58, Training Loss: 0.8016712655698447\n",
      "Epoch 59, Training Loss: 0.8023614212086326\n",
      "Epoch 60, Training Loss: 0.801874888301792\n",
      "Epoch 61, Training Loss: 0.8016080562333415\n",
      "Epoch 62, Training Loss: 0.8013547960080598\n",
      "Epoch 63, Training Loss: 0.8011352245072673\n",
      "Epoch 64, Training Loss: 0.8014576896688992\n",
      "Epoch 65, Training Loss: 0.8011496212249412\n",
      "Epoch 66, Training Loss: 0.8012534334247273\n",
      "Epoch 67, Training Loss: 0.8014813960943007\n",
      "Epoch 68, Training Loss: 0.8007893318520453\n",
      "Epoch 69, Training Loss: 0.8006069948798732\n",
      "Epoch 70, Training Loss: 0.8008337152631659\n",
      "Epoch 71, Training Loss: 0.8000732549151084\n",
      "Epoch 72, Training Loss: 0.8010999570215555\n",
      "Epoch 73, Training Loss: 0.7998078799337373\n",
      "Epoch 74, Training Loss: 0.8001128266628523\n",
      "Epoch 75, Training Loss: 0.8002865742920037\n",
      "Epoch 76, Training Loss: 0.8004414543173367\n",
      "Epoch 77, Training Loss: 0.7993999718275285\n",
      "Epoch 78, Training Loss: 0.7995511370045798\n",
      "Epoch 79, Training Loss: 0.7994680515805581\n",
      "Epoch 80, Training Loss: 0.7990460957799639\n",
      "Epoch 81, Training Loss: 0.7996581697822513\n",
      "Epoch 82, Training Loss: 0.7993838362227705\n",
      "Epoch 83, Training Loss: 0.7991884107876541\n",
      "Epoch 84, Training Loss: 0.799119404204806\n",
      "Epoch 85, Training Loss: 0.7995360750004761\n",
      "Epoch 86, Training Loss: 0.7989820429256984\n",
      "Epoch 87, Training Loss: 0.7992977289328898\n",
      "Epoch 88, Training Loss: 0.7989913701114798\n",
      "Epoch 89, Training Loss: 0.7989352128559486\n",
      "Epoch 90, Training Loss: 0.7992742128838274\n",
      "Epoch 91, Training Loss: 0.7990143335851512\n",
      "Epoch 92, Training Loss: 0.7989357285033492\n",
      "Epoch 93, Training Loss: 0.7983960950284972\n",
      "Epoch 94, Training Loss: 0.7988568978202074\n",
      "Epoch 95, Training Loss: 0.7984488266751282\n",
      "Epoch 96, Training Loss: 0.7990579931359542\n",
      "Epoch 97, Training Loss: 0.7987156795379811\n",
      "Epoch 98, Training Loss: 0.798539153794597\n",
      "Epoch 99, Training Loss: 0.798800362052774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:10:48,084] Trial 371 finished with value: 0.6344 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7981786877589119\n",
      "Epoch 1, Training Loss: 0.8906437424788798\n",
      "Epoch 2, Training Loss: 0.8164658636078799\n",
      "Epoch 3, Training Loss: 0.8097841513784308\n",
      "Epoch 4, Training Loss: 0.8069516916920368\n",
      "Epoch 5, Training Loss: 0.8066930794178095\n",
      "Epoch 6, Training Loss: 0.8042236070883901\n",
      "Epoch 7, Training Loss: 0.803309278739126\n",
      "Epoch 8, Training Loss: 0.8023973733858955\n",
      "Epoch 9, Training Loss: 0.801298693606728\n",
      "Epoch 10, Training Loss: 0.8012281203628483\n",
      "Epoch 11, Training Loss: 0.8003222151806481\n",
      "Epoch 12, Training Loss: 0.8001562045032816\n",
      "Epoch 13, Training Loss: 0.8002884417548215\n",
      "Epoch 14, Training Loss: 0.8000492000938358\n",
      "Epoch 15, Training Loss: 0.7988681319064663\n",
      "Epoch 16, Training Loss: 0.7983608513846433\n",
      "Epoch 17, Training Loss: 0.7979039130354286\n",
      "Epoch 18, Training Loss: 0.798538217150179\n",
      "Epoch 19, Training Loss: 0.7975460756990246\n",
      "Epoch 20, Training Loss: 0.7975334621909865\n",
      "Epoch 21, Training Loss: 0.7969702993120465\n",
      "Epoch 22, Training Loss: 0.7962841737539248\n",
      "Epoch 23, Training Loss: 0.7960276509586134\n",
      "Epoch 24, Training Loss: 0.794998752443414\n",
      "Epoch 25, Training Loss: 0.7950272799434518\n",
      "Epoch 26, Training Loss: 0.7931733176224214\n",
      "Epoch 27, Training Loss: 0.7943019918929365\n",
      "Epoch 28, Training Loss: 0.7916376718901154\n",
      "Epoch 29, Training Loss: 0.791828315419362\n",
      "Epoch 30, Training Loss: 0.790671195750846\n",
      "Epoch 31, Training Loss: 0.7901076117852577\n",
      "Epoch 32, Training Loss: 0.7898652896845251\n",
      "Epoch 33, Training Loss: 0.7888241653155563\n",
      "Epoch 34, Training Loss: 0.7882867212582352\n",
      "Epoch 35, Training Loss: 0.7878973444601647\n",
      "Epoch 36, Training Loss: 0.7865557710479076\n",
      "Epoch 37, Training Loss: 0.7866783531088578\n",
      "Epoch 38, Training Loss: 0.7858086096613031\n",
      "Epoch 39, Training Loss: 0.7872635233671146\n",
      "Epoch 40, Training Loss: 0.7857508688941037\n",
      "Epoch 41, Training Loss: 0.7856334856578282\n",
      "Epoch 42, Training Loss: 0.7843573693494151\n",
      "Epoch 43, Training Loss: 0.7852737986951842\n",
      "Epoch 44, Training Loss: 0.7853818240022301\n",
      "Epoch 45, Training Loss: 0.7846437447949459\n",
      "Epoch 46, Training Loss: 0.7847122886127099\n",
      "Epoch 47, Training Loss: 0.7845256314241796\n",
      "Epoch 48, Training Loss: 0.7842172122539435\n",
      "Epoch 49, Training Loss: 0.7840850114822387\n",
      "Epoch 50, Training Loss: 0.783969209247962\n",
      "Epoch 51, Training Loss: 0.783964545117285\n",
      "Epoch 52, Training Loss: 0.7838486915244196\n",
      "Epoch 53, Training Loss: 0.7836762601271607\n",
      "Epoch 54, Training Loss: 0.783653680095099\n",
      "Epoch 55, Training Loss: 0.7832555669590943\n",
      "Epoch 56, Training Loss: 0.7833086585640011\n",
      "Epoch 57, Training Loss: 0.7834451850195576\n",
      "Epoch 58, Training Loss: 0.7834586628397605\n",
      "Epoch 59, Training Loss: 0.7824972476278033\n",
      "Epoch 60, Training Loss: 0.7827039468557315\n",
      "Epoch 61, Training Loss: 0.7828892050829149\n",
      "Epoch 62, Training Loss: 0.7832828357703704\n",
      "Epoch 63, Training Loss: 0.7833757916787514\n",
      "Epoch 64, Training Loss: 0.7830894518615608\n",
      "Epoch 65, Training Loss: 0.7826618804071183\n",
      "Epoch 66, Training Loss: 0.7830460795782562\n",
      "Epoch 67, Training Loss: 0.7822682866028376\n",
      "Epoch 68, Training Loss: 0.7819083084737448\n",
      "Epoch 69, Training Loss: 0.7829969164124109\n",
      "Epoch 70, Training Loss: 0.7825637978718694\n",
      "Epoch 71, Training Loss: 0.7823054846068074\n",
      "Epoch 72, Training Loss: 0.7822231860985434\n",
      "Epoch 73, Training Loss: 0.7821392881242852\n",
      "Epoch 74, Training Loss: 0.7827971575851728\n",
      "Epoch 75, Training Loss: 0.7819132767225566\n",
      "Epoch 76, Training Loss: 0.7821605524622408\n",
      "Epoch 77, Training Loss: 0.7821441725680702\n",
      "Epoch 78, Training Loss: 0.7817619412465203\n",
      "Epoch 79, Training Loss: 0.7819589217802636\n",
      "Epoch 80, Training Loss: 0.7818892559610812\n",
      "Epoch 81, Training Loss: 0.7816945372667528\n",
      "Epoch 82, Training Loss: 0.7821603102791578\n",
      "Epoch 83, Training Loss: 0.7819017213090022\n",
      "Epoch 84, Training Loss: 0.7814309369352527\n",
      "Epoch 85, Training Loss: 0.7813089079426644\n",
      "Epoch 86, Training Loss: 0.7814231652066224\n",
      "Epoch 87, Training Loss: 0.7817261170623894\n",
      "Epoch 88, Training Loss: 0.7812255032976767\n",
      "Epoch 89, Training Loss: 0.7821748562325213\n",
      "Epoch 90, Training Loss: 0.781125091072312\n",
      "Epoch 91, Training Loss: 0.7809846843991961\n",
      "Epoch 92, Training Loss: 0.7816618549196344\n",
      "Epoch 93, Training Loss: 0.7809324184754738\n",
      "Epoch 94, Training Loss: 0.7814576729795987\n",
      "Epoch 95, Training Loss: 0.781242255071052\n",
      "Epoch 96, Training Loss: 0.7813567591789073\n",
      "Epoch 97, Training Loss: 0.7817553617004165\n",
      "Epoch 98, Training Loss: 0.7808318208034773\n",
      "Epoch 99, Training Loss: 0.7813889821669213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:12:49,434] Trial 372 finished with value: 0.6414666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7816337604271738\n",
      "Epoch 1, Training Loss: 0.8913870550636062\n",
      "Epoch 2, Training Loss: 0.8166835533945184\n",
      "Epoch 3, Training Loss: 0.8115222848447642\n",
      "Epoch 4, Training Loss: 0.8084583182083933\n",
      "Epoch 5, Training Loss: 0.8078419305776295\n",
      "Epoch 6, Training Loss: 0.8047585032936325\n",
      "Epoch 7, Training Loss: 0.8022484885122543\n",
      "Epoch 8, Training Loss: 0.799247076681682\n",
      "Epoch 9, Training Loss: 0.7978209980448386\n",
      "Epoch 10, Training Loss: 0.7964488664964088\n",
      "Epoch 11, Training Loss: 0.794311195029352\n",
      "Epoch 12, Training Loss: 0.7936743658288081\n",
      "Epoch 13, Training Loss: 0.7935937214614753\n",
      "Epoch 14, Training Loss: 0.7921098408842445\n",
      "Epoch 15, Training Loss: 0.7918081379474554\n",
      "Epoch 16, Training Loss: 0.791750625201634\n",
      "Epoch 17, Training Loss: 0.7909315034859162\n",
      "Epoch 18, Training Loss: 0.7913348539431292\n",
      "Epoch 19, Training Loss: 0.7897413660709123\n",
      "Epoch 20, Training Loss: 0.7888989234329167\n",
      "Epoch 21, Training Loss: 0.787742998591043\n",
      "Epoch 22, Training Loss: 0.7886575937271119\n",
      "Epoch 23, Training Loss: 0.7874207078962397\n",
      "Epoch 24, Training Loss: 0.7878519235697008\n",
      "Epoch 25, Training Loss: 0.7876539258132303\n",
      "Epoch 26, Training Loss: 0.78851126790943\n",
      "Epoch 27, Training Loss: 0.7874896374860204\n",
      "Epoch 28, Training Loss: 0.788180320603507\n",
      "Epoch 29, Training Loss: 0.7865278935073909\n",
      "Epoch 30, Training Loss: 0.7866487622261047\n",
      "Epoch 31, Training Loss: 0.7863325187586304\n",
      "Epoch 32, Training Loss: 0.7866117457698162\n",
      "Epoch 33, Training Loss: 0.7872838035561984\n",
      "Epoch 34, Training Loss: 0.7855083466472482\n",
      "Epoch 35, Training Loss: 0.7850483851325243\n",
      "Epoch 36, Training Loss: 0.7860357476356334\n",
      "Epoch 37, Training Loss: 0.7859429859577265\n",
      "Epoch 38, Training Loss: 0.785666756208678\n",
      "Epoch 39, Training Loss: 0.7848563574310532\n",
      "Epoch 40, Training Loss: 0.7860315891136801\n",
      "Epoch 41, Training Loss: 0.7860164970383609\n",
      "Epoch 42, Training Loss: 0.7851585147076083\n",
      "Epoch 43, Training Loss: 0.7850062675045845\n",
      "Epoch 44, Training Loss: 0.7850188934713378\n",
      "Epoch 45, Training Loss: 0.784252112879789\n",
      "Epoch 46, Training Loss: 0.785217463342767\n",
      "Epoch 47, Training Loss: 0.7836526936158201\n",
      "Epoch 48, Training Loss: 0.7844562841537304\n",
      "Epoch 49, Training Loss: 0.7845874148203914\n",
      "Epoch 50, Training Loss: 0.7838784666886007\n",
      "Epoch 51, Training Loss: 0.7835876111697434\n",
      "Epoch 52, Training Loss: 0.7848364446396218\n",
      "Epoch 53, Training Loss: 0.7837945618127522\n",
      "Epoch 54, Training Loss: 0.7828402526396557\n",
      "Epoch 55, Training Loss: 0.7843742998919092\n",
      "Epoch 56, Training Loss: 0.7828650473652029\n",
      "Epoch 57, Training Loss: 0.7835181313349788\n",
      "Epoch 58, Training Loss: 0.7835490958134931\n",
      "Epoch 59, Training Loss: 0.7822748903941391\n",
      "Epoch 60, Training Loss: 0.7823379005704607\n",
      "Epoch 61, Training Loss: 0.7824871522143371\n",
      "Epoch 62, Training Loss: 0.7824115500414281\n",
      "Epoch 63, Training Loss: 0.7820050551927179\n",
      "Epoch 64, Training Loss: 0.7823395810629192\n",
      "Epoch 65, Training Loss: 0.7824923872947693\n",
      "Epoch 66, Training Loss: 0.7828452553964199\n",
      "Epoch 67, Training Loss: 0.7826775662881091\n",
      "Epoch 68, Training Loss: 0.7821646699331757\n",
      "Epoch 69, Training Loss: 0.7821885564273461\n",
      "Epoch 70, Training Loss: 0.7833005076064203\n",
      "Epoch 71, Training Loss: 0.7825540098032557\n",
      "Epoch 72, Training Loss: 0.7813746282032558\n",
      "Epoch 73, Training Loss: 0.7815869921132138\n",
      "Epoch 74, Training Loss: 0.7818961063722023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:14:17,847] Trial 373 finished with value: 0.6396666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7821562234620403\n",
      "Epoch 1, Training Loss: 0.9189911298190846\n",
      "Epoch 2, Training Loss: 0.8711208304938148\n",
      "Epoch 3, Training Loss: 0.8346860899644739\n",
      "Epoch 4, Training Loss: 0.8165535623185775\n",
      "Epoch 5, Training Loss: 0.8085965301710016\n",
      "Epoch 6, Training Loss: 0.805070886962554\n",
      "Epoch 7, Training Loss: 0.8033033222310684\n",
      "Epoch 8, Training Loss: 0.8021035196500667\n",
      "Epoch 9, Training Loss: 0.8016023164636948\n",
      "Epoch 10, Training Loss: 0.80103840898065\n",
      "Epoch 11, Training Loss: 0.8006674984623404\n",
      "Epoch 12, Training Loss: 0.8004706483728745\n",
      "Epoch 13, Training Loss: 0.800289149424609\n",
      "Epoch 14, Training Loss: 0.7998678339930142\n",
      "Epoch 15, Training Loss: 0.7997581364827998\n",
      "Epoch 16, Training Loss: 0.7991200982823091\n",
      "Epoch 17, Training Loss: 0.7991924266955431\n",
      "Epoch 18, Training Loss: 0.7990504977282356\n",
      "Epoch 19, Training Loss: 0.7988126086487489\n",
      "Epoch 20, Training Loss: 0.7987549292339998\n",
      "Epoch 21, Training Loss: 0.7983025789260865\n",
      "Epoch 22, Training Loss: 0.798106969314463\n",
      "Epoch 23, Training Loss: 0.7982006124889149\n",
      "Epoch 24, Training Loss: 0.7978890083817874\n",
      "Epoch 25, Training Loss: 0.7978454325479619\n",
      "Epoch 26, Training Loss: 0.797554025369532\n",
      "Epoch 27, Training Loss: 0.797499610045377\n",
      "Epoch 28, Training Loss: 0.797475375918781\n",
      "Epoch 29, Training Loss: 0.7972668973838581\n",
      "Epoch 30, Training Loss: 0.7970440180862651\n",
      "Epoch 31, Training Loss: 0.7971138658944298\n",
      "Epoch 32, Training Loss: 0.7969715961288003\n",
      "Epoch 33, Training Loss: 0.7967342352867126\n",
      "Epoch 34, Training Loss: 0.7967357674065758\n",
      "Epoch 35, Training Loss: 0.7966737277367536\n",
      "Epoch 36, Training Loss: 0.7962220830075881\n",
      "Epoch 37, Training Loss: 0.7961616712457994\n",
      "Epoch 38, Training Loss: 0.7959203248865464\n",
      "Epoch 39, Training Loss: 0.7955479499872993\n",
      "Epoch 40, Training Loss: 0.7952885913147646\n",
      "Epoch 41, Training Loss: 0.7952517376226538\n",
      "Epoch 42, Training Loss: 0.7948116291971767\n",
      "Epoch 43, Training Loss: 0.7943690191998202\n",
      "Epoch 44, Training Loss: 0.7943369562485639\n",
      "Epoch 45, Training Loss: 0.7938354118431316\n",
      "Epoch 46, Training Loss: 0.7933542426894693\n",
      "Epoch 47, Training Loss: 0.7926782954440398\n",
      "Epoch 48, Training Loss: 0.7924490123636583\n",
      "Epoch 49, Training Loss: 0.7923064823010388\n",
      "Epoch 50, Training Loss: 0.7920036122378181\n",
      "Epoch 51, Training Loss: 0.7917614376544952\n",
      "Epoch 52, Training Loss: 0.7916066324710846\n",
      "Epoch 53, Training Loss: 0.7914324599153856\n",
      "Epoch 54, Training Loss: 0.7913836401350358\n",
      "Epoch 55, Training Loss: 0.7914499201494105\n",
      "Epoch 56, Training Loss: 0.7910233881193048\n",
      "Epoch 57, Training Loss: 0.7909077430472654\n",
      "Epoch 58, Training Loss: 0.7908485092836268\n",
      "Epoch 59, Training Loss: 0.7907982487538282\n",
      "Epoch 60, Training Loss: 0.7906894315691555\n",
      "Epoch 61, Training Loss: 0.790443386961432\n",
      "Epoch 62, Training Loss: 0.7906207870034611\n",
      "Epoch 63, Training Loss: 0.7904652559757233\n",
      "Epoch 64, Training Loss: 0.7903645395531373\n",
      "Epoch 65, Training Loss: 0.7906259884553797\n",
      "Epoch 66, Training Loss: 0.7903379923455855\n",
      "Epoch 67, Training Loss: 0.7900657737255097\n",
      "Epoch 68, Training Loss: 0.7900155332509209\n",
      "Epoch 69, Training Loss: 0.7900062376611373\n",
      "Epoch 70, Training Loss: 0.7900847644665662\n",
      "Epoch 71, Training Loss: 0.7900666734050302\n",
      "Epoch 72, Training Loss: 0.7899468567090876\n",
      "Epoch 73, Training Loss: 0.7899308422032525\n",
      "Epoch 74, Training Loss: 0.7896433017534368\n",
      "Epoch 75, Training Loss: 0.7896979396483478\n",
      "Epoch 76, Training Loss: 0.7897478403764613\n",
      "Epoch 77, Training Loss: 0.7895994137315189\n",
      "Epoch 78, Training Loss: 0.7894745929100934\n",
      "Epoch 79, Training Loss: 0.7897520436960108\n",
      "Epoch 80, Training Loss: 0.7894468335544362\n",
      "Epoch 81, Training Loss: 0.7895210663010093\n",
      "Epoch 82, Training Loss: 0.7893437863798702\n",
      "Epoch 83, Training Loss: 0.7893538597752067\n",
      "Epoch 84, Training Loss: 0.789275200437097\n",
      "Epoch 85, Training Loss: 0.7894382726444917\n",
      "Epoch 86, Training Loss: 0.7893321210496566\n",
      "Epoch 87, Training Loss: 0.7890172061499428\n",
      "Epoch 88, Training Loss: 0.7890765414518468\n",
      "Epoch 89, Training Loss: 0.7892304298456977\n",
      "Epoch 90, Training Loss: 0.7892041477035073\n",
      "Epoch 91, Training Loss: 0.7891521871089935\n",
      "Epoch 92, Training Loss: 0.7888218288561877\n",
      "Epoch 93, Training Loss: 0.7887729637763079\n",
      "Epoch 94, Training Loss: 0.7889560509429259\n",
      "Epoch 95, Training Loss: 0.788724984070834\n",
      "Epoch 96, Training Loss: 0.7889816204940572\n",
      "Epoch 97, Training Loss: 0.7885043902958141\n",
      "Epoch 98, Training Loss: 0.7886354771081139\n",
      "Epoch 99, Training Loss: 0.7888316940560061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:16:10,794] Trial 374 finished with value: 0.6373333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7885421964000253\n",
      "Epoch 1, Training Loss: 0.8616184515812818\n",
      "Epoch 2, Training Loss: 0.8247719721934375\n",
      "Epoch 3, Training Loss: 0.8194031988873202\n",
      "Epoch 4, Training Loss: 0.8173538004650789\n",
      "Epoch 5, Training Loss: 0.8146462876656476\n",
      "Epoch 6, Training Loss: 0.8130993387278389\n",
      "Epoch 7, Training Loss: 0.8117890917553621\n",
      "Epoch 8, Training Loss: 0.8113490452485926\n",
      "Epoch 9, Training Loss: 0.8111488570886499\n",
      "Epoch 10, Training Loss: 0.8100001876494464\n",
      "Epoch 11, Training Loss: 0.8100304427567651\n",
      "Epoch 12, Training Loss: 0.8094535041556639\n",
      "Epoch 13, Training Loss: 0.808742526699515\n",
      "Epoch 14, Training Loss: 0.80832390981562\n",
      "Epoch 15, Training Loss: 0.808567619604223\n",
      "Epoch 16, Training Loss: 0.8079769185711356\n",
      "Epoch 17, Training Loss: 0.807471242091235\n",
      "Epoch 18, Training Loss: 0.8074935713936301\n",
      "Epoch 19, Training Loss: 0.8072991243530723\n",
      "Epoch 20, Training Loss: 0.8072040237398709\n",
      "Epoch 21, Training Loss: 0.8070326007113737\n",
      "Epoch 22, Training Loss: 0.8078160397445454\n",
      "Epoch 23, Training Loss: 0.8067937010877273\n",
      "Epoch 24, Training Loss: 0.8064508508233463\n",
      "Epoch 25, Training Loss: 0.8067376016869264\n",
      "Epoch 26, Training Loss: 0.8059371510673972\n",
      "Epoch 27, Training Loss: 0.8066000343070311\n",
      "Epoch 28, Training Loss: 0.8064007339758031\n",
      "Epoch 29, Training Loss: 0.8053051148442661\n",
      "Epoch 30, Training Loss: 0.8058795858831966\n",
      "Epoch 31, Training Loss: 0.8057917472895454\n",
      "Epoch 32, Training Loss: 0.8058409016272601\n",
      "Epoch 33, Training Loss: 0.8055130337967592\n",
      "Epoch 34, Training Loss: 0.8057796329610488\n",
      "Epoch 35, Training Loss: 0.8049355611380409\n",
      "Epoch 36, Training Loss: 0.8051088474778568\n",
      "Epoch 37, Training Loss: 0.8055120606983409\n",
      "Epoch 38, Training Loss: 0.8057579290866852\n",
      "Epoch 39, Training Loss: 0.8060558411654304\n",
      "Epoch 40, Training Loss: 0.8054877734184265\n",
      "Epoch 41, Training Loss: 0.8050486632655649\n",
      "Epoch 42, Training Loss: 0.805568737633088\n",
      "Epoch 43, Training Loss: 0.805238113192951\n",
      "Epoch 44, Training Loss: 0.805049209875219\n",
      "Epoch 45, Training Loss: 0.8049948292620042\n",
      "Epoch 46, Training Loss: 0.80481945290285\n",
      "Epoch 47, Training Loss: 0.8054288162203396\n",
      "Epoch 48, Training Loss: 0.8046097203563242\n",
      "Epoch 49, Training Loss: 0.8045357307265787\n",
      "Epoch 50, Training Loss: 0.8043833184242248\n",
      "Epoch 51, Training Loss: 0.8043114084355971\n",
      "Epoch 52, Training Loss: 0.8043633157365462\n",
      "Epoch 53, Training Loss: 0.8049598274511449\n",
      "Epoch 54, Training Loss: 0.8045220068623038\n",
      "Epoch 55, Training Loss: 0.8053257001848781\n",
      "Epoch 56, Training Loss: 0.8045736044294693\n",
      "Epoch 57, Training Loss: 0.8044023539739497\n",
      "Epoch 58, Training Loss: 0.8046491174838122\n",
      "Epoch 59, Training Loss: 0.8048150460860308\n",
      "Epoch 60, Training Loss: 0.8047212480797488\n",
      "Epoch 61, Training Loss: 0.8045446649719687\n",
      "Epoch 62, Training Loss: 0.8042815465085646\n",
      "Epoch 63, Training Loss: 0.8039807202535517\n",
      "Epoch 64, Training Loss: 0.8044111841566423\n",
      "Epoch 65, Training Loss: 0.8041192694972543\n",
      "Epoch 66, Training Loss: 0.8042710516733281\n",
      "Epoch 67, Training Loss: 0.8042692960711086\n",
      "Epoch 68, Training Loss: 0.8042026270137114\n",
      "Epoch 69, Training Loss: 0.80446035546415\n",
      "Epoch 70, Training Loss: 0.804525342057733\n",
      "Epoch 71, Training Loss: 0.8043558301645167\n",
      "Epoch 72, Training Loss: 0.803998446113923\n",
      "Epoch 73, Training Loss: 0.8038469750740949\n",
      "Epoch 74, Training Loss: 0.8036300982447232\n",
      "Epoch 75, Training Loss: 0.8038074902225943\n",
      "Epoch 76, Training Loss: 0.8037758497630849\n",
      "Epoch 77, Training Loss: 0.8036149619607365\n",
      "Epoch 78, Training Loss: 0.8034360763605903\n",
      "Epoch 79, Training Loss: 0.803553462379119\n",
      "Epoch 80, Training Loss: 0.8036394502836115\n",
      "Epoch 81, Training Loss: 0.803245042071623\n",
      "Epoch 82, Training Loss: 0.8027383724380942\n",
      "Epoch 83, Training Loss: 0.8028125318358926\n",
      "Epoch 84, Training Loss: 0.8021305559663212\n",
      "Epoch 85, Training Loss: 0.8021163910977981\n",
      "Epoch 86, Training Loss: 0.8021299573954414\n",
      "Epoch 87, Training Loss: 0.8017808770432192\n",
      "Epoch 88, Training Loss: 0.8016346242848564\n",
      "Epoch 89, Training Loss: 0.8027549976461074\n",
      "Epoch 90, Training Loss: 0.8016547650449416\n",
      "Epoch 91, Training Loss: 0.801661280113108\n",
      "Epoch 92, Training Loss: 0.8021304088480332\n",
      "Epoch 93, Training Loss: 0.800733512359507\n",
      "Epoch 94, Training Loss: 0.801341165304184\n",
      "Epoch 95, Training Loss: 0.8016344540960648\n",
      "Epoch 96, Training Loss: 0.8018329767619863\n",
      "Epoch 97, Training Loss: 0.8016049289703369\n",
      "Epoch 98, Training Loss: 0.8016029869107639\n",
      "Epoch 99, Training Loss: 0.8015931202383603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:18:03,346] Trial 375 finished with value: 0.632 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.801525178306243\n",
      "Epoch 1, Training Loss: 0.9127450427614656\n",
      "Epoch 2, Training Loss: 0.8296977167739008\n",
      "Epoch 3, Training Loss: 0.8213560341892386\n",
      "Epoch 4, Training Loss: 0.8168559253663945\n",
      "Epoch 5, Training Loss: 0.8147995679002059\n",
      "Epoch 6, Training Loss: 0.8131731738721518\n",
      "Epoch 7, Training Loss: 0.8107835476559804\n",
      "Epoch 8, Training Loss: 0.8093387316044112\n",
      "Epoch 9, Training Loss: 0.8089565027028994\n",
      "Epoch 10, Training Loss: 0.8079843536355442\n",
      "Epoch 11, Training Loss: 0.8073634827943673\n",
      "Epoch 12, Training Loss: 0.8069193934139453\n",
      "Epoch 13, Training Loss: 0.8073970192357114\n",
      "Epoch 14, Training Loss: 0.8064241671024408\n",
      "Epoch 15, Training Loss: 0.8050529728258463\n",
      "Epoch 16, Training Loss: 0.8046753328545649\n",
      "Epoch 17, Training Loss: 0.8049069747888953\n",
      "Epoch 18, Training Loss: 0.8033175325035152\n",
      "Epoch 19, Training Loss: 0.8042811390152551\n",
      "Epoch 20, Training Loss: 0.8037073039470759\n",
      "Epoch 21, Training Loss: 0.8028437254124118\n",
      "Epoch 22, Training Loss: 0.8028583985522277\n",
      "Epoch 23, Training Loss: 0.8020482297230483\n",
      "Epoch 24, Training Loss: 0.8022637707846505\n",
      "Epoch 25, Training Loss: 0.8011270182473319\n",
      "Epoch 26, Training Loss: 0.8016709495307808\n",
      "Epoch 27, Training Loss: 0.8012408589061938\n",
      "Epoch 28, Training Loss: 0.8005027598008178\n",
      "Epoch 29, Training Loss: 0.8002879075089792\n",
      "Epoch 30, Training Loss: 0.8002910216948144\n",
      "Epoch 31, Training Loss: 0.7994031275125374\n",
      "Epoch 32, Training Loss: 0.7992528613348653\n",
      "Epoch 33, Training Loss: 0.7998118802120812\n",
      "Epoch 34, Training Loss: 0.7988748796900412\n",
      "Epoch 35, Training Loss: 0.7982631586548081\n",
      "Epoch 36, Training Loss: 0.7984659077977776\n",
      "Epoch 37, Training Loss: 0.7979509214709576\n",
      "Epoch 38, Training Loss: 0.7983022247041974\n",
      "Epoch 39, Training Loss: 0.7977743194515544\n",
      "Epoch 40, Training Loss: 0.797332403265444\n",
      "Epoch 41, Training Loss: 0.7966503402344266\n",
      "Epoch 42, Training Loss: 0.7963570389532505\n",
      "Epoch 43, Training Loss: 0.7959940692535916\n",
      "Epoch 44, Training Loss: 0.7962507109892996\n",
      "Epoch 45, Training Loss: 0.7969657405874783\n",
      "Epoch 46, Training Loss: 0.7959198702546887\n",
      "Epoch 47, Training Loss: 0.7959781606394546\n",
      "Epoch 48, Training Loss: 0.7956236940577515\n",
      "Epoch 49, Training Loss: 0.7957498551311349\n",
      "Epoch 50, Training Loss: 0.7949451295953048\n",
      "Epoch 51, Training Loss: 0.7953942326674784\n",
      "Epoch 52, Training Loss: 0.7950567479420425\n",
      "Epoch 53, Training Loss: 0.7949784411523575\n",
      "Epoch 54, Training Loss: 0.7947099302048074\n",
      "Epoch 55, Training Loss: 0.7946528541414362\n",
      "Epoch 56, Training Loss: 0.7950294064399891\n",
      "Epoch 57, Training Loss: 0.7937047751774465\n",
      "Epoch 58, Training Loss: 0.7944884058228112\n",
      "Epoch 59, Training Loss: 0.7940033602535277\n",
      "Epoch 60, Training Loss: 0.7940238452495489\n",
      "Epoch 61, Training Loss: 0.7937916731475887\n",
      "Epoch 62, Training Loss: 0.7936470558768824\n",
      "Epoch 63, Training Loss: 0.7944133141883334\n",
      "Epoch 64, Training Loss: 0.7932896827396594\n",
      "Epoch 65, Training Loss: 0.7937292288120528\n",
      "Epoch 66, Training Loss: 0.7929383210221628\n",
      "Epoch 67, Training Loss: 0.7936305278225949\n",
      "Epoch 68, Training Loss: 0.7932958964118384\n",
      "Epoch 69, Training Loss: 0.7928464853673949\n",
      "Epoch 70, Training Loss: 0.792803822872334\n",
      "Epoch 71, Training Loss: 0.792747548200134\n",
      "Epoch 72, Training Loss: 0.7928962125814051\n",
      "Epoch 73, Training Loss: 0.7926541428816946\n",
      "Epoch 74, Training Loss: 0.79271050467527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:19:14,126] Trial 376 finished with value: 0.6142666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7930134257875887\n",
      "Epoch 1, Training Loss: 0.8881226401580008\n",
      "Epoch 2, Training Loss: 0.8186264176117747\n",
      "Epoch 3, Training Loss: 0.8065090236807229\n",
      "Epoch 4, Training Loss: 0.8038864276462928\n",
      "Epoch 5, Training Loss: 0.8013555909009804\n",
      "Epoch 6, Training Loss: 0.8000742644295656\n",
      "Epoch 7, Training Loss: 0.7986789556374227\n",
      "Epoch 8, Training Loss: 0.797059166521058\n",
      "Epoch 9, Training Loss: 0.7960233199865299\n",
      "Epoch 10, Training Loss: 0.7941776819247052\n",
      "Epoch 11, Training Loss: 0.7933878290025811\n",
      "Epoch 12, Training Loss: 0.7921818251896622\n",
      "Epoch 13, Training Loss: 0.7914704195538858\n",
      "Epoch 14, Training Loss: 0.7902437557851462\n",
      "Epoch 15, Training Loss: 0.7897398183668466\n",
      "Epoch 16, Training Loss: 0.7894057667345032\n",
      "Epoch 17, Training Loss: 0.7884209997671887\n",
      "Epoch 18, Training Loss: 0.788438009588342\n",
      "Epoch 19, Training Loss: 0.7878580748586727\n",
      "Epoch 20, Training Loss: 0.7874393588618228\n",
      "Epoch 21, Training Loss: 0.7871766812819287\n",
      "Epoch 22, Training Loss: 0.7865882497084768\n",
      "Epoch 23, Training Loss: 0.7863033391479263\n",
      "Epoch 24, Training Loss: 0.7864110494018497\n",
      "Epoch 25, Training Loss: 0.7859719864407876\n",
      "Epoch 26, Training Loss: 0.7857963873927755\n",
      "Epoch 27, Training Loss: 0.7852348696020313\n",
      "Epoch 28, Training Loss: 0.7856137402971884\n",
      "Epoch 29, Training Loss: 0.7851813622883388\n",
      "Epoch 30, Training Loss: 0.7846301086863181\n",
      "Epoch 31, Training Loss: 0.7844996120696678\n",
      "Epoch 32, Training Loss: 0.7842647359783488\n",
      "Epoch 33, Training Loss: 0.7846448412515167\n",
      "Epoch 34, Training Loss: 0.7836635805610427\n",
      "Epoch 35, Training Loss: 0.783314359546604\n",
      "Epoch 36, Training Loss: 0.7837016376337611\n",
      "Epoch 37, Training Loss: 0.7831081874388501\n",
      "Epoch 38, Training Loss: 0.7835203691532737\n",
      "Epoch 39, Training Loss: 0.7829359799399411\n",
      "Epoch 40, Training Loss: 0.7827540914815171\n",
      "Epoch 41, Training Loss: 0.7824753807003336\n",
      "Epoch 42, Training Loss: 0.7819755781862072\n",
      "Epoch 43, Training Loss: 0.7819496381551699\n",
      "Epoch 44, Training Loss: 0.7822105328839525\n",
      "Epoch 45, Training Loss: 0.781741009708634\n",
      "Epoch 46, Training Loss: 0.7822036143532373\n",
      "Epoch 47, Training Loss: 0.7819531490031938\n",
      "Epoch 48, Training Loss: 0.7818636328654182\n",
      "Epoch 49, Training Loss: 0.7815716060480677\n",
      "Epoch 50, Training Loss: 0.7817471979255963\n",
      "Epoch 51, Training Loss: 0.7814004393448507\n",
      "Epoch 52, Training Loss: 0.7812875216168569\n",
      "Epoch 53, Training Loss: 0.7807689420262673\n",
      "Epoch 54, Training Loss: 0.7810657009146267\n",
      "Epoch 55, Training Loss: 0.7809184074401856\n",
      "Epoch 56, Training Loss: 0.7802592152043393\n",
      "Epoch 57, Training Loss: 0.7802187002691111\n",
      "Epoch 58, Training Loss: 0.7803867147381144\n",
      "Epoch 59, Training Loss: 0.7799591199796002\n",
      "Epoch 60, Training Loss: 0.7802827611005396\n",
      "Epoch 61, Training Loss: 0.7805259240301032\n",
      "Epoch 62, Training Loss: 0.7798761311330293\n",
      "Epoch 63, Training Loss: 0.7801712676994783\n",
      "Epoch 64, Training Loss: 0.7802755380931653\n",
      "Epoch 65, Training Loss: 0.780159708790313\n",
      "Epoch 66, Training Loss: 0.7804590525483727\n",
      "Epoch 67, Training Loss: 0.7797264015764223\n",
      "Epoch 68, Training Loss: 0.7793828303204443\n",
      "Epoch 69, Training Loss: 0.7795450686512136\n",
      "Epoch 70, Training Loss: 0.7792920401221828\n",
      "Epoch 71, Training Loss: 0.7794428635360603\n",
      "Epoch 72, Training Loss: 0.7800627784621447\n",
      "Epoch 73, Training Loss: 0.7796684268721961\n",
      "Epoch 74, Training Loss: 0.779261865293173\n",
      "Epoch 75, Training Loss: 0.7792837143840646\n",
      "Epoch 76, Training Loss: 0.7795138666504308\n",
      "Epoch 77, Training Loss: 0.778747394389676\n",
      "Epoch 78, Training Loss: 0.7787432411559543\n",
      "Epoch 79, Training Loss: 0.7784860341172469\n",
      "Epoch 80, Training Loss: 0.7790981044446615\n",
      "Epoch 81, Training Loss: 0.7791632812722284\n",
      "Epoch 82, Training Loss: 0.7786525729007291\n",
      "Epoch 83, Training Loss: 0.7787309396535831\n",
      "Epoch 84, Training Loss: 0.7789312856537955\n",
      "Epoch 85, Training Loss: 0.7778297006635737\n",
      "Epoch 86, Training Loss: 0.7788423859087148\n",
      "Epoch 87, Training Loss: 0.7788021013252717\n",
      "Epoch 88, Training Loss: 0.7779044691781353\n",
      "Epoch 89, Training Loss: 0.7782956276621137\n",
      "Epoch 90, Training Loss: 0.7791005033299438\n",
      "Epoch 91, Training Loss: 0.7782580156075327\n",
      "Epoch 92, Training Loss: 0.7776831563254049\n",
      "Epoch 93, Training Loss: 0.7779274883126854\n",
      "Epoch 94, Training Loss: 0.778779857768152\n",
      "Epoch 95, Training Loss: 0.7778892226685259\n",
      "Epoch 96, Training Loss: 0.7783328044683413\n",
      "Epoch 97, Training Loss: 0.7782330417991581\n",
      "Epoch 98, Training Loss: 0.7784343971345657\n",
      "Epoch 99, Training Loss: 0.7777305538492991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:21:05,942] Trial 377 finished with value: 0.6397333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7777204632759094\n",
      "Epoch 1, Training Loss: 0.9712991950792425\n",
      "Epoch 2, Training Loss: 0.9419027222605313\n",
      "Epoch 3, Training Loss: 0.9235378937160268\n",
      "Epoch 4, Training Loss: 0.9020613222262439\n",
      "Epoch 5, Training Loss: 0.8799308292304768\n",
      "Epoch 6, Training Loss: 0.8596858982478871\n",
      "Epoch 7, Training Loss: 0.8434572480706608\n",
      "Epoch 8, Training Loss: 0.8313610219254213\n",
      "Epoch 9, Training Loss: 0.8236488670461318\n",
      "Epoch 10, Training Loss: 0.8186619947938358\n",
      "Epoch 11, Training Loss: 0.8154564073506524\n",
      "Epoch 12, Training Loss: 0.8133584369631375\n",
      "Epoch 13, Training Loss: 0.8119162019561319\n",
      "Epoch 14, Training Loss: 0.8109224914101993\n",
      "Epoch 15, Training Loss: 0.8100474371629602\n",
      "Epoch 16, Training Loss: 0.8095108492935406\n",
      "Epoch 17, Training Loss: 0.8093124823009267\n",
      "Epoch 18, Training Loss: 0.808715204631581\n",
      "Epoch 19, Training Loss: 0.8085379972177393\n",
      "Epoch 20, Training Loss: 0.8081735649529626\n",
      "Epoch 21, Training Loss: 0.8078488805714775\n",
      "Epoch 22, Training Loss: 0.8078666385482339\n",
      "Epoch 23, Training Loss: 0.8076389554668876\n",
      "Epoch 24, Training Loss: 0.8071505091470831\n",
      "Epoch 25, Training Loss: 0.807063518482096\n",
      "Epoch 26, Training Loss: 0.8067637508055743\n",
      "Epoch 27, Training Loss: 0.806745235709583\n",
      "Epoch 28, Training Loss: 0.8064182705738965\n",
      "Epoch 29, Training Loss: 0.8065609694228453\n",
      "Epoch 30, Training Loss: 0.8062322901978213\n",
      "Epoch 31, Training Loss: 0.8060670083410599\n",
      "Epoch 32, Training Loss: 0.8060391738835503\n",
      "Epoch 33, Training Loss: 0.8058814771736369\n",
      "Epoch 34, Training Loss: 0.8056414058629204\n",
      "Epoch 35, Training Loss: 0.805577517747879\n",
      "Epoch 36, Training Loss: 0.805362980856615\n",
      "Epoch 37, Training Loss: 0.8053095243958865\n",
      "Epoch 38, Training Loss: 0.8050849382316365\n",
      "Epoch 39, Training Loss: 0.805084212808048\n",
      "Epoch 40, Training Loss: 0.8051650482065538\n",
      "Epoch 41, Training Loss: 0.80479030567057\n",
      "Epoch 42, Training Loss: 0.8046310663924497\n",
      "Epoch 43, Training Loss: 0.8045418356446659\n",
      "Epoch 44, Training Loss: 0.8042792901572059\n",
      "Epoch 45, Training Loss: 0.804369179501253\n",
      "Epoch 46, Training Loss: 0.8043730510683621\n",
      "Epoch 47, Training Loss: 0.8041371085363276\n",
      "Epoch 48, Training Loss: 0.8039905063545003\n",
      "Epoch 49, Training Loss: 0.8038346477817087\n",
      "Epoch 50, Training Loss: 0.8036122607483583\n",
      "Epoch 51, Training Loss: 0.803390075599446\n",
      "Epoch 52, Training Loss: 0.8034467550586252\n",
      "Epoch 53, Training Loss: 0.8032684375959284\n",
      "Epoch 54, Training Loss: 0.8031792720626382\n",
      "Epoch 55, Training Loss: 0.8031871644889608\n",
      "Epoch 56, Training Loss: 0.8029056014734156\n",
      "Epoch 57, Training Loss: 0.8028043014161726\n",
      "Epoch 58, Training Loss: 0.8027561271190643\n",
      "Epoch 59, Training Loss: 0.8024901540139142\n",
      "Epoch 60, Training Loss: 0.802483843424741\n",
      "Epoch 61, Training Loss: 0.8024134058110854\n",
      "Epoch 62, Training Loss: 0.8021577569316415\n",
      "Epoch 63, Training Loss: 0.8018516560863046\n",
      "Epoch 64, Training Loss: 0.8019146835102754\n",
      "Epoch 65, Training Loss: 0.801865711071912\n",
      "Epoch 66, Training Loss: 0.8016920061672435\n",
      "Epoch 67, Training Loss: 0.8016322595932904\n",
      "Epoch 68, Training Loss: 0.8014919434575474\n",
      "Epoch 69, Training Loss: 0.8014608919620514\n",
      "Epoch 70, Training Loss: 0.8013051089819739\n",
      "Epoch 71, Training Loss: 0.8011322695367477\n",
      "Epoch 72, Training Loss: 0.8010382216818193\n",
      "Epoch 73, Training Loss: 0.8008892242347493\n",
      "Epoch 74, Training Loss: 0.8007469350450179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:22:30,679] Trial 378 finished with value: 0.6339333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8007583763318903\n",
      "Epoch 1, Training Loss: 1.018139279239318\n",
      "Epoch 2, Training Loss: 0.9431627939729129\n",
      "Epoch 3, Training Loss: 0.9270023111034842\n",
      "Epoch 4, Training Loss: 0.9175512693910037\n",
      "Epoch 5, Training Loss: 0.9094475242670844\n",
      "Epoch 6, Training Loss: 0.9010379997421714\n",
      "Epoch 7, Training Loss: 0.8910727204995997\n",
      "Epoch 8, Training Loss: 0.877974497570711\n",
      "Epoch 9, Training Loss: 0.8615924427789801\n",
      "Epoch 10, Training Loss: 0.8436487192967359\n",
      "Epoch 11, Training Loss: 0.8284129653257483\n",
      "Epoch 12, Training Loss: 0.8180938907931833\n",
      "Epoch 13, Training Loss: 0.8121064622261945\n",
      "Epoch 14, Training Loss: 0.808781003110549\n",
      "Epoch 15, Training Loss: 0.8063861484387341\n",
      "Epoch 16, Training Loss: 0.8052221885849448\n",
      "Epoch 17, Training Loss: 0.8039370275244994\n",
      "Epoch 18, Training Loss: 0.8032660077599918\n",
      "Epoch 19, Training Loss: 0.802794786902035\n",
      "Epoch 20, Training Loss: 0.8020528269515318\n",
      "Epoch 21, Training Loss: 0.8014198757620419\n",
      "Epoch 22, Training Loss: 0.8011559981458327\n",
      "Epoch 23, Training Loss: 0.8006700404952554\n",
      "Epoch 24, Training Loss: 0.8003114921205184\n",
      "Epoch 25, Training Loss: 0.8000083120430217\n",
      "Epoch 26, Training Loss: 0.799725606511621\n",
      "Epoch 27, Training Loss: 0.7993322510579053\n",
      "Epoch 28, Training Loss: 0.7990941388466779\n",
      "Epoch 29, Training Loss: 0.7986605379861944\n",
      "Epoch 30, Training Loss: 0.798481035232544\n",
      "Epoch 31, Training Loss: 0.7983450429579791\n",
      "Epoch 32, Training Loss: 0.7982178352159612\n",
      "Epoch 33, Training Loss: 0.797906399754917\n",
      "Epoch 34, Training Loss: 0.7975901228540084\n",
      "Epoch 35, Training Loss: 0.797377395770129\n",
      "Epoch 36, Training Loss: 0.7973330450759214\n",
      "Epoch 37, Training Loss: 0.7970189663241891\n",
      "Epoch 38, Training Loss: 0.7969349110827727\n",
      "Epoch 39, Training Loss: 0.7965623083535363\n",
      "Epoch 40, Training Loss: 0.7964630286833819\n",
      "Epoch 41, Training Loss: 0.7961691827633801\n",
      "Epoch 42, Training Loss: 0.7960204031186945\n",
      "Epoch 43, Training Loss: 0.7955933652204625\n",
      "Epoch 44, Training Loss: 0.7952709945510416\n",
      "Epoch 45, Training Loss: 0.795001306884429\n",
      "Epoch 46, Training Loss: 0.7949208470653085\n",
      "Epoch 47, Training Loss: 0.794662253926782\n",
      "Epoch 48, Training Loss: 0.7942925084338469\n",
      "Epoch 49, Training Loss: 0.7944848214177525\n",
      "Epoch 50, Training Loss: 0.7939998419845805\n",
      "Epoch 51, Training Loss: 0.7938784873485565\n",
      "Epoch 52, Training Loss: 0.7936059406224419\n",
      "Epoch 53, Training Loss: 0.7933946063238032\n",
      "Epoch 54, Training Loss: 0.7933185009395375\n",
      "Epoch 55, Training Loss: 0.7929551910652833\n",
      "Epoch 56, Training Loss: 0.7930868502925424\n",
      "Epoch 57, Training Loss: 0.7928670005237355\n",
      "Epoch 58, Training Loss: 0.7925442677385667\n",
      "Epoch 59, Training Loss: 0.7923717175511753\n",
      "Epoch 60, Training Loss: 0.7923051428794861\n",
      "Epoch 61, Training Loss: 0.7920284424108618\n",
      "Epoch 62, Training Loss: 0.7919877267585081\n",
      "Epoch 63, Training Loss: 0.7916556857613957\n",
      "Epoch 64, Training Loss: 0.7916226656997905\n",
      "Epoch 65, Training Loss: 0.7912406754493714\n",
      "Epoch 66, Training Loss: 0.7912166695735033\n",
      "Epoch 67, Training Loss: 0.7908385280300589\n",
      "Epoch 68, Training Loss: 0.790574605324689\n",
      "Epoch 69, Training Loss: 0.7905081543501685\n",
      "Epoch 70, Training Loss: 0.7903177204552819\n",
      "Epoch 71, Training Loss: 0.7903568890515495\n",
      "Epoch 72, Training Loss: 0.790143828532275\n",
      "Epoch 73, Training Loss: 0.7898522379117854\n",
      "Epoch 74, Training Loss: 0.7897909928069395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:23:52,633] Trial 379 finished with value: 0.6371333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.789709546846502\n",
      "Epoch 1, Training Loss: 0.945178302526474\n",
      "Epoch 2, Training Loss: 0.871886241085389\n",
      "Epoch 3, Training Loss: 0.8281783227359547\n",
      "Epoch 4, Training Loss: 0.815088417039198\n",
      "Epoch 5, Training Loss: 0.811279285585179\n",
      "Epoch 6, Training Loss: 0.809875445506152\n",
      "Epoch 7, Training Loss: 0.8085342416342567\n",
      "Epoch 8, Training Loss: 0.808122414560879\n",
      "Epoch 9, Training Loss: 0.807290688753128\n",
      "Epoch 10, Training Loss: 0.8066505370420568\n",
      "Epoch 11, Training Loss: 0.8061137876089881\n",
      "Epoch 12, Training Loss: 0.8057487984264599\n",
      "Epoch 13, Training Loss: 0.8050980379300959\n",
      "Epoch 14, Training Loss: 0.8048286775981679\n",
      "Epoch 15, Training Loss: 0.804843202829361\n",
      "Epoch 16, Training Loss: 0.804453985971563\n",
      "Epoch 17, Training Loss: 0.804124199923347\n",
      "Epoch 18, Training Loss: 0.8034436005003313\n",
      "Epoch 19, Training Loss: 0.8034037660851198\n",
      "Epoch 20, Training Loss: 0.8029179852149065\n",
      "Epoch 21, Training Loss: 0.8029912424087524\n",
      "Epoch 22, Training Loss: 0.8027305970472448\n",
      "Epoch 23, Training Loss: 0.8022962844371796\n",
      "Epoch 24, Training Loss: 0.8020458640070522\n",
      "Epoch 25, Training Loss: 0.8020218391278211\n",
      "Epoch 26, Training Loss: 0.8019541628220502\n",
      "Epoch 27, Training Loss: 0.8013827769896563\n",
      "Epoch 28, Training Loss: 0.8012410600045148\n",
      "Epoch 29, Training Loss: 0.8012869698159835\n",
      "Epoch 30, Training Loss: 0.8014250482531154\n",
      "Epoch 31, Training Loss: 0.8008055327219121\n",
      "Epoch 32, Training Loss: 0.8010549205892227\n",
      "Epoch 33, Training Loss: 0.8008129309906679\n",
      "Epoch 34, Training Loss: 0.8002959889524123\n",
      "Epoch 35, Training Loss: 0.8002098124167498\n",
      "Epoch 36, Training Loss: 0.800023856233148\n",
      "Epoch 37, Training Loss: 0.799946429939831\n",
      "Epoch 38, Training Loss: 0.7998911152166479\n",
      "Epoch 39, Training Loss: 0.7997883873126086\n",
      "Epoch 40, Training Loss: 0.7998076515338001\n",
      "Epoch 41, Training Loss: 0.7995894857715158\n",
      "Epoch 42, Training Loss: 0.7994229536196765\n",
      "Epoch 43, Training Loss: 0.7995677966931287\n",
      "Epoch 44, Training Loss: 0.7993991473141838\n",
      "Epoch 45, Training Loss: 0.7990216863155365\n",
      "Epoch 46, Training Loss: 0.7991804018441369\n",
      "Epoch 47, Training Loss: 0.7986997004817514\n",
      "Epoch 48, Training Loss: 0.7991501732433544\n",
      "Epoch 49, Training Loss: 0.7991543081928701\n",
      "Epoch 50, Training Loss: 0.7989508710889255\n",
      "Epoch 51, Training Loss: 0.7988965849315419\n",
      "Epoch 52, Training Loss: 0.798555421408485\n",
      "Epoch 53, Training Loss: 0.7986428564436295\n",
      "Epoch 54, Training Loss: 0.7986818962237414\n",
      "Epoch 55, Training Loss: 0.7986015451655668\n",
      "Epoch 56, Training Loss: 0.7985193211892072\n",
      "Epoch 57, Training Loss: 0.7985925006165224\n",
      "Epoch 58, Training Loss: 0.7982364461702459\n",
      "Epoch 59, Training Loss: 0.797993572249132\n",
      "Epoch 60, Training Loss: 0.7978707954462837\n",
      "Epoch 61, Training Loss: 0.7980780822389266\n",
      "Epoch 62, Training Loss: 0.7983348324018367\n",
      "Epoch 63, Training Loss: 0.7979564382749446\n",
      "Epoch 64, Training Loss: 0.7978981166727402\n",
      "Epoch 65, Training Loss: 0.7979402718824499\n",
      "Epoch 66, Training Loss: 0.7981178925317877\n",
      "Epoch 67, Training Loss: 0.797895366935169\n",
      "Epoch 68, Training Loss: 0.7978765996063457\n",
      "Epoch 69, Training Loss: 0.7980153748568366\n",
      "Epoch 70, Training Loss: 0.7976566485797658\n",
      "Epoch 71, Training Loss: 0.7977829161812278\n",
      "Epoch 72, Training Loss: 0.7975335732628317\n",
      "Epoch 73, Training Loss: 0.797635342724183\n",
      "Epoch 74, Training Loss: 0.7977161219540765\n",
      "Epoch 75, Training Loss: 0.7976964562780717\n",
      "Epoch 76, Training Loss: 0.797711211793563\n",
      "Epoch 77, Training Loss: 0.7976017633606406\n",
      "Epoch 78, Training Loss: 0.797272951743182\n",
      "Epoch 79, Training Loss: 0.797686653768315\n",
      "Epoch 80, Training Loss: 0.7976941626212176\n",
      "Epoch 81, Training Loss: 0.7975913578622481\n",
      "Epoch 82, Training Loss: 0.7974642709423514\n",
      "Epoch 83, Training Loss: 0.7975592437211205\n",
      "Epoch 84, Training Loss: 0.7974586508554571\n",
      "Epoch 85, Training Loss: 0.7972732263452866\n",
      "Epoch 86, Training Loss: 0.7974001550674439\n",
      "Epoch 87, Training Loss: 0.7974455200223362\n",
      "Epoch 88, Training Loss: 0.7976571677011602\n",
      "Epoch 89, Training Loss: 0.7975305499750025\n",
      "Epoch 90, Training Loss: 0.7972570962765637\n",
      "Epoch 91, Training Loss: 0.7970510066256804\n",
      "Epoch 92, Training Loss: 0.7971645516507766\n",
      "Epoch 93, Training Loss: 0.7969656412040486\n",
      "Epoch 94, Training Loss: 0.7968898173640756\n",
      "Epoch 95, Training Loss: 0.7969950194218579\n",
      "Epoch 96, Training Loss: 0.7972049656335045\n",
      "Epoch 97, Training Loss: 0.7971130896315856\n",
      "Epoch 98, Training Loss: 0.7971426611788133\n",
      "Epoch 99, Training Loss: 0.7967871656137354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:25:52,167] Trial 380 finished with value: 0.6346 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7967768504339106\n",
      "Epoch 1, Training Loss: 1.0586096148741873\n",
      "Epoch 2, Training Loss: 1.021868852744425\n",
      "Epoch 3, Training Loss: 0.9982134173687239\n",
      "Epoch 4, Training Loss: 0.9818256814677017\n",
      "Epoch 5, Training Loss: 0.9716485551425389\n",
      "Epoch 6, Training Loss: 0.964975286784925\n",
      "Epoch 7, Training Loss: 0.961119780594245\n",
      "Epoch 8, Training Loss: 0.9581716189707132\n",
      "Epoch 9, Training Loss: 0.9568604572375018\n",
      "Epoch 10, Training Loss: 0.9553728946169516\n",
      "Epoch 11, Training Loss: 0.9539562994376161\n",
      "Epoch 12, Training Loss: 0.9528011129314738\n",
      "Epoch 13, Training Loss: 0.9511332655311527\n",
      "Epoch 14, Training Loss: 0.9503698006608432\n",
      "Epoch 15, Training Loss: 0.9494387735101514\n",
      "Epoch 16, Training Loss: 0.9481170475034786\n",
      "Epoch 17, Training Loss: 0.9469268122113736\n",
      "Epoch 18, Training Loss: 0.9454781733061138\n",
      "Epoch 19, Training Loss: 0.9438214335226475\n",
      "Epoch 20, Training Loss: 0.9429470168020492\n",
      "Epoch 21, Training Loss: 0.9418066243480022\n",
      "Epoch 22, Training Loss: 0.9406754159389582\n",
      "Epoch 23, Training Loss: 0.9392919977804772\n",
      "Epoch 24, Training Loss: 0.9377270680621155\n",
      "Epoch 25, Training Loss: 0.9368326395077813\n",
      "Epoch 26, Training Loss: 0.9349572323318711\n",
      "Epoch 27, Training Loss: 0.9338125050516057\n",
      "Epoch 28, Training Loss: 0.9327314176057514\n",
      "Epoch 29, Training Loss: 0.9309867018147518\n",
      "Epoch 30, Training Loss: 0.9295044310110853\n",
      "Epoch 31, Training Loss: 0.9280923754649055\n",
      "Epoch 32, Training Loss: 0.9265909530166396\n",
      "Epoch 33, Training Loss: 0.9248996519504633\n",
      "Epoch 34, Training Loss: 0.9235625232072701\n",
      "Epoch 35, Training Loss: 0.9221631616578067\n",
      "Epoch 36, Training Loss: 0.9199188828468323\n",
      "Epoch 37, Training Loss: 0.9184505025246986\n",
      "Epoch 38, Training Loss: 0.916622605449275\n",
      "Epoch 39, Training Loss: 0.9147419894548287\n",
      "Epoch 40, Training Loss: 0.9131211002070204\n",
      "Epoch 41, Training Loss: 0.9118552442780115\n",
      "Epoch 42, Training Loss: 0.9098126020646633\n",
      "Epoch 43, Training Loss: 0.9077600628809821\n",
      "Epoch 44, Training Loss: 0.906118653412152\n",
      "Epoch 45, Training Loss: 0.9040394956904246\n",
      "Epoch 46, Training Loss: 0.9022732557210708\n",
      "Epoch 47, Training Loss: 0.9002753046222199\n",
      "Epoch 48, Training Loss: 0.8985070893639012\n",
      "Epoch 49, Training Loss: 0.8969063228234313\n",
      "Epoch 50, Training Loss: 0.8950175010172048\n",
      "Epoch 51, Training Loss: 0.8929552054046688\n",
      "Epoch 52, Training Loss: 0.8907850611478763\n",
      "Epoch 53, Training Loss: 0.8890846644129072\n",
      "Epoch 54, Training Loss: 0.8869374072641358\n",
      "Epoch 55, Training Loss: 0.8848883488124475\n",
      "Epoch 56, Training Loss: 0.8829332613407221\n",
      "Epoch 57, Training Loss: 0.8816326472095977\n",
      "Epoch 58, Training Loss: 0.8791098484419342\n",
      "Epoch 59, Training Loss: 0.8773584677760763\n",
      "Epoch 60, Training Loss: 0.8752459208768113\n",
      "Epoch 61, Training Loss: 0.8731992955494644\n",
      "Epoch 62, Training Loss: 0.8714380296549402\n",
      "Epoch 63, Training Loss: 0.8691539282637432\n",
      "Epoch 64, Training Loss: 0.8676376858151945\n",
      "Epoch 65, Training Loss: 0.8657940973912863\n",
      "Epoch 66, Training Loss: 0.8644271502817483\n",
      "Epoch 67, Training Loss: 0.8621945582834402\n",
      "Epoch 68, Training Loss: 0.860393422259424\n",
      "Epoch 69, Training Loss: 0.8589093919983484\n",
      "Epoch 70, Training Loss: 0.8574796898024423\n",
      "Epoch 71, Training Loss: 0.8555953395097775\n",
      "Epoch 72, Training Loss: 0.8544695237525424\n",
      "Epoch 73, Training Loss: 0.8524726088782002\n",
      "Epoch 74, Training Loss: 0.8512561258516813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:26:55,488] Trial 381 finished with value: 0.611 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8495493613687674\n",
      "Epoch 1, Training Loss: 1.0846632023418652\n",
      "Epoch 2, Training Loss: 1.041223679710837\n",
      "Epoch 3, Training Loss: 1.0136728162625257\n",
      "Epoch 4, Training Loss: 0.9948645777562085\n",
      "Epoch 5, Training Loss: 0.981942735489677\n",
      "Epoch 6, Training Loss: 0.9731633623908548\n",
      "Epoch 7, Training Loss: 0.9672296376088086\n",
      "Epoch 8, Training Loss: 0.9631745408563053\n",
      "Epoch 9, Training Loss: 0.960327927014407\n",
      "Epoch 10, Training Loss: 0.9582287898484398\n",
      "Epoch 11, Training Loss: 0.956586653134402\n",
      "Epoch 12, Training Loss: 0.9552276089612175\n",
      "Epoch 13, Training Loss: 0.9540323211866266\n",
      "Epoch 14, Training Loss: 0.952940620394314\n",
      "Epoch 15, Training Loss: 0.9519192582018235\n",
      "Epoch 16, Training Loss: 0.9509271076847525\n",
      "Epoch 17, Training Loss: 0.9499633599028868\n",
      "Epoch 18, Training Loss: 0.9489990899843328\n",
      "Epoch 19, Training Loss: 0.9480548478575314\n",
      "Epoch 20, Training Loss: 0.9471007928427528\n",
      "Epoch 21, Training Loss: 0.9461357462406158\n",
      "Epoch 22, Training Loss: 0.9451615643501282\n",
      "Epoch 23, Training Loss: 0.9441777443184572\n",
      "Epoch 24, Training Loss: 0.9431758872901692\n",
      "Epoch 25, Training Loss: 0.9421558838030871\n",
      "Epoch 26, Training Loss: 0.941116554666968\n",
      "Epoch 27, Training Loss: 0.9400576666523428\n",
      "Epoch 28, Training Loss: 0.9389737149547128\n",
      "Epoch 29, Training Loss: 0.937862569584566\n",
      "Epoch 30, Training Loss: 0.9367296883639167\n",
      "Epoch 31, Training Loss: 0.9355420176421895\n",
      "Epoch 32, Training Loss: 0.9343752633122837\n",
      "Epoch 33, Training Loss: 0.933144556424197\n",
      "Epoch 34, Training Loss: 0.9318861992218915\n",
      "Epoch 35, Training Loss: 0.9305958123066846\n",
      "Epoch 36, Training Loss: 0.9292683526347665\n",
      "Epoch 37, Training Loss: 0.9278989180396585\n",
      "Epoch 38, Training Loss: 0.9264975216809441\n",
      "Epoch 39, Training Loss: 0.9250518904012792\n",
      "Epoch 40, Training Loss: 0.9235598096426796\n",
      "Epoch 41, Training Loss: 0.9220376828137566\n",
      "Epoch 42, Training Loss: 0.9204707634449005\n",
      "Epoch 43, Training Loss: 0.9188533096453723\n",
      "Epoch 44, Training Loss: 0.9171967631929061\n",
      "Epoch 45, Training Loss: 0.9154892080671647\n",
      "Epoch 46, Training Loss: 0.9137434695748722\n",
      "Epoch 47, Training Loss: 0.9119339700306163\n",
      "Epoch 48, Training Loss: 0.9100990411814521\n",
      "Epoch 49, Training Loss: 0.90820950087379\n",
      "Epoch 50, Training Loss: 0.9062802115608665\n",
      "Epoch 51, Training Loss: 0.9042907524108886\n",
      "Epoch 52, Training Loss: 0.902272646707647\n",
      "Epoch 53, Training Loss: 0.9001984958788928\n",
      "Epoch 54, Training Loss: 0.8980974399342256\n",
      "Epoch 55, Training Loss: 0.8959420972010669\n",
      "Epoch 56, Training Loss: 0.8937519025802613\n",
      "Epoch 57, Training Loss: 0.891517361683004\n",
      "Epoch 58, Training Loss: 0.889290746310178\n",
      "Epoch 59, Training Loss: 0.8869979223083048\n",
      "Epoch 60, Training Loss: 0.884708707262488\n",
      "Epoch 61, Training Loss: 0.88239766219083\n",
      "Epoch 62, Training Loss: 0.8800619799950543\n",
      "Epoch 63, Training Loss: 0.8777091910558589\n",
      "Epoch 64, Training Loss: 0.8753714499052833\n",
      "Epoch 65, Training Loss: 0.8730334549090442\n",
      "Epoch 66, Training Loss: 0.870680149302763\n",
      "Epoch 67, Training Loss: 0.8683696041387671\n",
      "Epoch 68, Training Loss: 0.8660463409564074\n",
      "Epoch 69, Training Loss: 0.8637913652027355\n",
      "Epoch 70, Training Loss: 0.8615427611855899\n",
      "Epoch 71, Training Loss: 0.8593187431026907\n",
      "Epoch 72, Training Loss: 0.8571387534281787\n",
      "Epoch 73, Training Loss: 0.855024051175398\n",
      "Epoch 74, Training Loss: 0.8529200852618498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:28:16,474] Trial 382 finished with value: 0.6069333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8508860709386713\n",
      "Epoch 1, Training Loss: 0.9850386742984547\n",
      "Epoch 2, Training Loss: 0.9521923049758463\n",
      "Epoch 3, Training Loss: 0.9442411628190209\n",
      "Epoch 4, Training Loss: 0.9369075652431039\n",
      "Epoch 5, Training Loss: 0.929714176865185\n",
      "Epoch 6, Training Loss: 0.9223539091559018\n",
      "Epoch 7, Training Loss: 0.9145422469167148\n",
      "Epoch 8, Training Loss: 0.9063713456602658\n",
      "Epoch 9, Training Loss: 0.8980763180115644\n",
      "Epoch 10, Training Loss: 0.8895374549136442\n",
      "Epoch 11, Training Loss: 0.8808528465383193\n",
      "Epoch 12, Training Loss: 0.8721857776361354\n",
      "Epoch 13, Training Loss: 0.8638837117307326\n",
      "Epoch 14, Training Loss: 0.856022839826696\n",
      "Epoch 15, Training Loss: 0.8487860021871679\n",
      "Epoch 16, Training Loss: 0.8424293999812182\n",
      "Epoch 17, Training Loss: 0.8367334449992461\n",
      "Epoch 18, Training Loss: 0.8317393439657548\n",
      "Epoch 19, Training Loss: 0.8275006196779363\n",
      "Epoch 20, Training Loss: 0.8240779343072105\n",
      "Epoch 21, Training Loss: 0.8213353749583749\n",
      "Epoch 22, Training Loss: 0.8188470911979675\n",
      "Epoch 23, Training Loss: 0.8169229371407453\n",
      "Epoch 24, Training Loss: 0.8152443140394547\n",
      "Epoch 25, Training Loss: 0.8139610730900484\n",
      "Epoch 26, Training Loss: 0.8128502267949721\n",
      "Epoch 27, Training Loss: 0.8119232296242433\n",
      "Epoch 28, Training Loss: 0.8111135820080252\n",
      "Epoch 29, Training Loss: 0.8104590512023253\n",
      "Epoch 30, Training Loss: 0.8099317794687608\n",
      "Epoch 31, Training Loss: 0.8095170354141908\n",
      "Epoch 32, Training Loss: 0.8089227307544035\n",
      "Epoch 33, Training Loss: 0.808609802793054\n",
      "Epoch 34, Training Loss: 0.8081885872167699\n",
      "Epoch 35, Training Loss: 0.8079169234107523\n",
      "Epoch 36, Training Loss: 0.8075482151788824\n",
      "Epoch 37, Training Loss: 0.8072545120996587\n",
      "Epoch 38, Training Loss: 0.8069166581770953\n",
      "Epoch 39, Training Loss: 0.806696979859296\n",
      "Epoch 40, Training Loss: 0.806518788688323\n",
      "Epoch 41, Training Loss: 0.8061572016687955\n",
      "Epoch 42, Training Loss: 0.8060599288519691\n",
      "Epoch 43, Training Loss: 0.80566034190795\n",
      "Epoch 44, Training Loss: 0.8055268244182362\n",
      "Epoch 45, Training Loss: 0.8051836752190309\n",
      "Epoch 46, Training Loss: 0.8050226978694691\n",
      "Epoch 47, Training Loss: 0.8048098336949068\n",
      "Epoch 48, Training Loss: 0.8045059644474702\n",
      "Epoch 49, Training Loss: 0.8045178312413833\n",
      "Epoch 50, Training Loss: 0.8042509842620177\n",
      "Epoch 51, Training Loss: 0.8040012638007893\n",
      "Epoch 52, Training Loss: 0.8038749633817112\n",
      "Epoch 53, Training Loss: 0.8036716095138998\n",
      "Epoch 54, Training Loss: 0.8035714481157415\n",
      "Epoch 55, Training Loss: 0.803320491734673\n",
      "Epoch 56, Training Loss: 0.8031276353667764\n",
      "Epoch 57, Training Loss: 0.8030789678237017\n",
      "Epoch 58, Training Loss: 0.8029392683506011\n",
      "Epoch 59, Training Loss: 0.8027245997681337\n",
      "Epoch 60, Training Loss: 0.8025325408402612\n",
      "Epoch 61, Training Loss: 0.8025066581193139\n",
      "Epoch 62, Training Loss: 0.802225403364967\n",
      "Epoch 63, Training Loss: 0.8021838237958796\n",
      "Epoch 64, Training Loss: 0.8021290449535146\n",
      "Epoch 65, Training Loss: 0.8019715213775634\n",
      "Epoch 66, Training Loss: 0.8018018221855163\n",
      "Epoch 67, Training Loss: 0.8017960485290079\n",
      "Epoch 68, Training Loss: 0.8016708155940561\n",
      "Epoch 69, Training Loss: 0.8016238026057972\n",
      "Epoch 70, Training Loss: 0.801398706927019\n",
      "Epoch 71, Training Loss: 0.8013299107551575\n",
      "Epoch 72, Training Loss: 0.8011988294124603\n",
      "Epoch 73, Training Loss: 0.8010802980731515\n",
      "Epoch 74, Training Loss: 0.8010351425759933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:29:25,309] Trial 383 finished with value: 0.6318 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8010021957229165\n",
      "Epoch 1, Training Loss: 1.0107849951077226\n",
      "Epoch 2, Training Loss: 0.9601603124374734\n",
      "Epoch 3, Training Loss: 0.9502483609923743\n",
      "Epoch 4, Training Loss: 0.9411101995554185\n",
      "Epoch 5, Training Loss: 0.9305603433372382\n",
      "Epoch 6, Training Loss: 0.9182721490250494\n",
      "Epoch 7, Training Loss: 0.9034515328873369\n",
      "Epoch 8, Training Loss: 0.8871184470061969\n",
      "Epoch 9, Training Loss: 0.869235757060517\n",
      "Epoch 10, Training Loss: 0.8532763512511002\n",
      "Epoch 11, Training Loss: 0.8403345589350937\n",
      "Epoch 12, Training Loss: 0.8304460331013328\n",
      "Epoch 13, Training Loss: 0.8241476511596737\n",
      "Epoch 14, Training Loss: 0.8204019159302676\n",
      "Epoch 15, Training Loss: 0.8171383919572471\n",
      "Epoch 16, Training Loss: 0.8148493944254137\n",
      "Epoch 17, Training Loss: 0.8131093143520499\n",
      "Epoch 18, Training Loss: 0.8115091414379895\n",
      "Epoch 19, Training Loss: 0.8111351852130173\n",
      "Epoch 20, Training Loss: 0.8098709464969491\n",
      "Epoch 21, Training Loss: 0.8094380286403169\n",
      "Epoch 22, Training Loss: 0.8085738964546892\n",
      "Epoch 23, Training Loss: 0.807737097973214\n",
      "Epoch 24, Training Loss: 0.8075811759869855\n",
      "Epoch 25, Training Loss: 0.8081360626041441\n",
      "Epoch 26, Training Loss: 0.807043168598548\n",
      "Epoch 27, Training Loss: 0.8063282122289328\n",
      "Epoch 28, Training Loss: 0.8050633281245267\n",
      "Epoch 29, Training Loss: 0.804947705197155\n",
      "Epoch 30, Training Loss: 0.8050243271920914\n",
      "Epoch 31, Training Loss: 0.8046981740714912\n",
      "Epoch 32, Training Loss: 0.804614183777257\n",
      "Epoch 33, Training Loss: 0.8042030239463749\n",
      "Epoch 34, Training Loss: 0.8032979015120887\n",
      "Epoch 35, Training Loss: 0.8045286382051339\n",
      "Epoch 36, Training Loss: 0.803085318006071\n",
      "Epoch 37, Training Loss: 0.8032070008435643\n",
      "Epoch 38, Training Loss: 0.8027230974426843\n",
      "Epoch 39, Training Loss: 0.802299321862988\n",
      "Epoch 40, Training Loss: 0.8023211074951\n",
      "Epoch 41, Training Loss: 0.8017667377801766\n",
      "Epoch 42, Training Loss: 0.802016288086884\n",
      "Epoch 43, Training Loss: 0.801520461874797\n",
      "Epoch 44, Training Loss: 0.8016479269902509\n",
      "Epoch 45, Training Loss: 0.8016493471045243\n",
      "Epoch 46, Training Loss: 0.800640305585431\n",
      "Epoch 47, Training Loss: 0.8012812160907832\n",
      "Epoch 48, Training Loss: 0.8007365082439624\n",
      "Epoch 49, Training Loss: 0.8000939930292\n",
      "Epoch 50, Training Loss: 0.8003782528683655\n",
      "Epoch 51, Training Loss: 0.7999304169102719\n",
      "Epoch 52, Training Loss: 0.7996485484721966\n",
      "Epoch 53, Training Loss: 0.7996617366496782\n",
      "Epoch 54, Training Loss: 0.7994010699422736\n",
      "Epoch 55, Training Loss: 0.7999497531948233\n",
      "Epoch 56, Training Loss: 0.8001603411552601\n",
      "Epoch 57, Training Loss: 0.7992966156256827\n",
      "Epoch 58, Training Loss: 0.7993502896531184\n",
      "Epoch 59, Training Loss: 0.7994700217605534\n",
      "Epoch 60, Training Loss: 0.7993433222734838\n",
      "Epoch 61, Training Loss: 0.7991839453690034\n",
      "Epoch 62, Training Loss: 0.7986389979383999\n",
      "Epoch 63, Training Loss: 0.7986928841225187\n",
      "Epoch 64, Training Loss: 0.7987625376622479\n",
      "Epoch 65, Training Loss: 0.7985722630543817\n",
      "Epoch 66, Training Loss: 0.7984160330062522\n",
      "Epoch 67, Training Loss: 0.7991324016026088\n",
      "Epoch 68, Training Loss: 0.7985381996721254\n",
      "Epoch 69, Training Loss: 0.7988378875237658\n",
      "Epoch 70, Training Loss: 0.7992974074263322\n",
      "Epoch 71, Training Loss: 0.7980805885522886\n",
      "Epoch 72, Training Loss: 0.7980460832889815\n",
      "Epoch 73, Training Loss: 0.7982566929401311\n",
      "Epoch 74, Training Loss: 0.798461035319737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:30:34,198] Trial 384 finished with value: 0.6316666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7982297206283512\n",
      "Epoch 1, Training Loss: 1.0666850248505086\n",
      "Epoch 2, Training Loss: 1.035866769061369\n",
      "Epoch 3, Training Loss: 1.0129159029091106\n",
      "Epoch 4, Training Loss: 0.9952400612831116\n",
      "Epoch 5, Training Loss: 0.9821768018778633\n",
      "Epoch 6, Training Loss: 0.9729461991786956\n",
      "Epoch 7, Training Loss: 0.9666423932945027\n",
      "Epoch 8, Training Loss: 0.9623837784458609\n",
      "Epoch 9, Training Loss: 0.9594269349995781\n",
      "Epoch 10, Training Loss: 0.9572740609505598\n",
      "Epoch 11, Training Loss: 0.9556237511073842\n",
      "Epoch 12, Training Loss: 0.9542648220763487\n",
      "Epoch 13, Training Loss: 0.9530650077847873\n",
      "Epoch 14, Training Loss: 0.9519744368861703\n",
      "Epoch 15, Training Loss: 0.9509200399763444\n",
      "Epoch 16, Training Loss: 0.9499095921656665\n",
      "Epoch 17, Training Loss: 0.9489060551278732\n",
      "Epoch 18, Training Loss: 0.9479096041707431\n",
      "Epoch 19, Training Loss: 0.946908782860812\n",
      "Epoch 20, Training Loss: 0.9459039893571068\n",
      "Epoch 21, Training Loss: 0.9448870313868803\n",
      "Epoch 22, Training Loss: 0.9438726662187016\n",
      "Epoch 23, Training Loss: 0.942842581692864\n",
      "Epoch 24, Training Loss: 0.941805566479178\n",
      "Epoch 25, Training Loss: 0.9407433849923751\n",
      "Epoch 26, Training Loss: 0.9396743796853458\n",
      "Epoch 27, Training Loss: 0.9385828706096201\n",
      "Epoch 28, Training Loss: 0.9374770290010116\n",
      "Epoch 29, Training Loss: 0.936342407394858\n",
      "Epoch 30, Training Loss: 0.9351885476533104\n",
      "Epoch 31, Training Loss: 0.934021425597808\n",
      "Epoch 32, Training Loss: 0.9328137727344737\n",
      "Epoch 33, Training Loss: 0.9316051917216357\n",
      "Epoch 34, Training Loss: 0.9303505915753981\n",
      "Epoch 35, Training Loss: 0.929069609081044\n",
      "Epoch 36, Training Loss: 0.9277464800021228\n",
      "Epoch 37, Training Loss: 0.9264047709633322\n",
      "Epoch 38, Training Loss: 0.9250621449947357\n",
      "Epoch 39, Training Loss: 0.9236470573789933\n",
      "Epoch 40, Training Loss: 0.92220330119133\n",
      "Epoch 41, Training Loss: 0.9207259142398834\n",
      "Epoch 42, Training Loss: 0.9192229888018439\n",
      "Epoch 43, Training Loss: 0.9176681964537676\n",
      "Epoch 44, Training Loss: 0.9160808506432702\n",
      "Epoch 45, Training Loss: 0.9144464089589961\n",
      "Epoch 46, Training Loss: 0.9127970936719109\n",
      "Epoch 47, Training Loss: 0.9110740177771625\n",
      "Epoch 48, Training Loss: 0.9093359631650588\n",
      "Epoch 49, Training Loss: 0.9075346917264602\n",
      "Epoch 50, Training Loss: 0.9057136553175309\n",
      "Epoch 51, Training Loss: 0.9038509390634649\n",
      "Epoch 52, Training Loss: 0.9019479710214279\n",
      "Epoch 53, Training Loss: 0.9000068749399747\n",
      "Epoch 54, Training Loss: 0.898029504032696\n",
      "Epoch 55, Training Loss: 0.8960034660030813\n",
      "Epoch 56, Training Loss: 0.8939684060040642\n",
      "Epoch 57, Training Loss: 0.8919095852795769\n",
      "Epoch 58, Training Loss: 0.8898137652873993\n",
      "Epoch 59, Training Loss: 0.8877092975027421\n",
      "Epoch 60, Training Loss: 0.8855658191091874\n",
      "Epoch 61, Training Loss: 0.883412824378294\n",
      "Epoch 62, Training Loss: 0.8812659036411958\n",
      "Epoch 63, Training Loss: 0.8790693674368016\n",
      "Epoch 64, Training Loss: 0.8769139348058139\n",
      "Epoch 65, Training Loss: 0.8747309787133161\n",
      "Epoch 66, Training Loss: 0.872575743969749\n",
      "Epoch 67, Training Loss: 0.8704510880217833\n",
      "Epoch 68, Training Loss: 0.8682727850885952\n",
      "Epoch 69, Training Loss: 0.8661851201337927\n",
      "Epoch 70, Training Loss: 0.8640758996150073\n",
      "Epoch 71, Training Loss: 0.8620293341664707\n",
      "Epoch 72, Training Loss: 0.8599746594709509\n",
      "Epoch 73, Training Loss: 0.8579993277437546\n",
      "Epoch 74, Training Loss: 0.8560593978797688\n",
      "Epoch 75, Training Loss: 0.8541262358777664\n",
      "Epoch 76, Training Loss: 0.852285621025983\n",
      "Epoch 77, Training Loss: 0.8504498927733477\n",
      "Epoch 78, Training Loss: 0.8486996585481307\n",
      "Epoch 79, Training Loss: 0.84698911281193\n",
      "Epoch 80, Training Loss: 0.8453026171992807\n",
      "Epoch 81, Training Loss: 0.8437002753510194\n",
      "Epoch 82, Training Loss: 0.8421774545837851\n",
      "Epoch 83, Training Loss: 0.8406630636663998\n",
      "Epoch 84, Training Loss: 0.8391865436469808\n",
      "Epoch 85, Training Loss: 0.8377913244331584\n",
      "Epoch 86, Training Loss: 0.8365180390722612\n",
      "Epoch 87, Training Loss: 0.8351852524981779\n",
      "Epoch 88, Training Loss: 0.8340118509881637\n",
      "Epoch 89, Training Loss: 0.8328009039514205\n",
      "Epoch 90, Training Loss: 0.831686638804043\n",
      "Epoch 91, Training Loss: 0.8306395354691674\n",
      "Epoch 92, Training Loss: 0.8295999587283415\n",
      "Epoch 93, Training Loss: 0.8286464212221257\n",
      "Epoch 94, Training Loss: 0.8276982024837942\n",
      "Epoch 95, Training Loss: 0.8268342181514291\n",
      "Epoch 96, Training Loss: 0.8259640300273895\n",
      "Epoch 97, Training Loss: 0.8251848398937899\n",
      "Epoch 98, Training Loss: 0.8243829882144929\n",
      "Epoch 99, Training Loss: 0.8236887231995078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:32:25,007] Trial 385 finished with value: 0.6211333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8230079185962677\n",
      "Epoch 1, Training Loss: 0.998846312901553\n",
      "Epoch 2, Training Loss: 0.9246457170738893\n",
      "Epoch 3, Training Loss: 0.9046304672605852\n",
      "Epoch 4, Training Loss: 0.8819323157562929\n",
      "Epoch 5, Training Loss: 0.8503450307425331\n",
      "Epoch 6, Training Loss: 0.8220354004467235\n",
      "Epoch 7, Training Loss: 0.8102086045461543\n",
      "Epoch 8, Training Loss: 0.8064309381737429\n",
      "Epoch 9, Training Loss: 0.804463676073972\n",
      "Epoch 10, Training Loss: 0.8035666553413167\n",
      "Epoch 11, Training Loss: 0.8018388112853555\n",
      "Epoch 12, Training Loss: 0.801270994088229\n",
      "Epoch 13, Training Loss: 0.7999593881999745\n",
      "Epoch 14, Training Loss: 0.7992397659666398\n",
      "Epoch 15, Training Loss: 0.7985307908058167\n",
      "Epoch 16, Training Loss: 0.7981769366124097\n",
      "Epoch 17, Training Loss: 0.7977747561651117\n",
      "Epoch 18, Training Loss: 0.7966174570953145\n",
      "Epoch 19, Training Loss: 0.7965665677715751\n",
      "Epoch 20, Training Loss: 0.7959726001935846\n",
      "Epoch 21, Training Loss: 0.795723821555867\n",
      "Epoch 22, Training Loss: 0.7953754558983971\n",
      "Epoch 23, Training Loss: 0.7946727071790134\n",
      "Epoch 24, Training Loss: 0.7947805125573102\n",
      "Epoch 25, Training Loss: 0.7943312616909252\n",
      "Epoch 26, Training Loss: 0.7937154961333556\n",
      "Epoch 27, Training Loss: 0.7935133459287531\n",
      "Epoch 28, Training Loss: 0.7926220172994277\n",
      "Epoch 29, Training Loss: 0.7923022777192733\n",
      "Epoch 30, Training Loss: 0.7923675280458787\n",
      "Epoch 31, Training Loss: 0.7919006707387812\n",
      "Epoch 32, Training Loss: 0.7916412321960224\n",
      "Epoch 33, Training Loss: 0.790907463536543\n",
      "Epoch 34, Training Loss: 0.7903686503101798\n",
      "Epoch 35, Training Loss: 0.7901701327632455\n",
      "Epoch 36, Training Loss: 0.7903917063685024\n",
      "Epoch 37, Training Loss: 0.7897065910872291\n",
      "Epoch 38, Training Loss: 0.7894691225360422\n",
      "Epoch 39, Training Loss: 0.789386998274747\n",
      "Epoch 40, Training Loss: 0.7889534612964181\n",
      "Epoch 41, Training Loss: 0.7886758453004501\n",
      "Epoch 42, Training Loss: 0.7884450290483587\n",
      "Epoch 43, Training Loss: 0.7882434249625486\n",
      "Epoch 44, Training Loss: 0.7880640128079582\n",
      "Epoch 45, Training Loss: 0.787710623180165\n",
      "Epoch 46, Training Loss: 0.7874444841637331\n",
      "Epoch 47, Training Loss: 0.7872286371623769\n",
      "Epoch 48, Training Loss: 0.7867409769226523\n",
      "Epoch 49, Training Loss: 0.7867056843112497\n",
      "Epoch 50, Training Loss: 0.7866437478626476\n",
      "Epoch 51, Training Loss: 0.7864360866827124\n",
      "Epoch 52, Training Loss: 0.7864292416151832\n",
      "Epoch 53, Training Loss: 0.7861180100020241\n",
      "Epoch 54, Training Loss: 0.7858803556245916\n",
      "Epoch 55, Training Loss: 0.7858106158761418\n",
      "Epoch 56, Training Loss: 0.7855838946735157\n",
      "Epoch 57, Training Loss: 0.7854107638667611\n",
      "Epoch 58, Training Loss: 0.7858467512972215\n",
      "Epoch 59, Training Loss: 0.7853725338683409\n",
      "Epoch 60, Training Loss: 0.7853501146681169\n",
      "Epoch 61, Training Loss: 0.7849998659947339\n",
      "Epoch 62, Training Loss: 0.7848719681010526\n",
      "Epoch 63, Training Loss: 0.784651834263521\n",
      "Epoch 64, Training Loss: 0.7849165497106664\n",
      "Epoch 65, Training Loss: 0.7848054674793692\n",
      "Epoch 66, Training Loss: 0.7843990171656889\n",
      "Epoch 67, Training Loss: 0.7848724277580486\n",
      "Epoch 68, Training Loss: 0.7843860187951256\n",
      "Epoch 69, Training Loss: 0.7844919640877668\n",
      "Epoch 70, Training Loss: 0.7844145668254179\n",
      "Epoch 71, Training Loss: 0.7843060392491957\n",
      "Epoch 72, Training Loss: 0.7839043282761293\n",
      "Epoch 73, Training Loss: 0.7839043298188377\n",
      "Epoch 74, Training Loss: 0.7839734622310189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:33:44,944] Trial 386 finished with value: 0.6384666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7839732703040628\n",
      "Epoch 1, Training Loss: 0.8788768413487603\n",
      "Epoch 2, Training Loss: 0.8288112208422492\n",
      "Epoch 3, Training Loss: 0.8188859140171724\n",
      "Epoch 4, Training Loss: 0.8148965001807493\n",
      "Epoch 5, Training Loss: 0.8125231273034039\n",
      "Epoch 6, Training Loss: 0.8103332736912896\n",
      "Epoch 7, Training Loss: 0.8098116289166843\n",
      "Epoch 8, Training Loss: 0.8086735503112569\n",
      "Epoch 9, Training Loss: 0.8063099173237296\n",
      "Epoch 10, Training Loss: 0.8058630608109867\n",
      "Epoch 11, Training Loss: 0.8059455715908723\n",
      "Epoch 12, Training Loss: 0.8045848098923178\n",
      "Epoch 13, Training Loss: 0.8039906311736388\n",
      "Epoch 14, Training Loss: 0.804352291541941\n",
      "Epoch 15, Training Loss: 0.8044609087354997\n",
      "Epoch 16, Training Loss: 0.8042854335027583\n",
      "Epoch 17, Training Loss: 0.8034314337197472\n",
      "Epoch 18, Training Loss: 0.8030062762428732\n",
      "Epoch 19, Training Loss: 0.8024813949360567\n",
      "Epoch 20, Training Loss: 0.8026028877847335\n",
      "Epoch 21, Training Loss: 0.8030385100140291\n",
      "Epoch 22, Training Loss: 0.8024970562317792\n",
      "Epoch 23, Training Loss: 0.8028423626983867\n",
      "Epoch 24, Training Loss: 0.8019791110824136\n",
      "Epoch 25, Training Loss: 0.8021363926635069\n",
      "Epoch 26, Training Loss: 0.8021574602407567\n",
      "Epoch 27, Training Loss: 0.8019150766204385\n",
      "Epoch 28, Training Loss: 0.8020581374448889\n",
      "Epoch 29, Training Loss: 0.8021303689479828\n",
      "Epoch 30, Training Loss: 0.8014083036955665\n",
      "Epoch 31, Training Loss: 0.8017127887641682\n",
      "Epoch 32, Training Loss: 0.8010626571318683\n",
      "Epoch 33, Training Loss: 0.8012429191084469\n",
      "Epoch 34, Training Loss: 0.8013331409762887\n",
      "Epoch 35, Training Loss: 0.8010486308967366\n",
      "Epoch 36, Training Loss: 0.8009214857746573\n",
      "Epoch 37, Training Loss: 0.8008979254610398\n",
      "Epoch 38, Training Loss: 0.8005489175459918\n",
      "Epoch 39, Training Loss: 0.8000206659120672\n",
      "Epoch 40, Training Loss: 0.8007508202861338\n",
      "Epoch 41, Training Loss: 0.8014971160888672\n",
      "Epoch 42, Training Loss: 0.8010476904756882\n",
      "Epoch 43, Training Loss: 0.8007841171236599\n",
      "Epoch 44, Training Loss: 0.8015474998950958\n",
      "Epoch 45, Training Loss: 0.8007189531186047\n",
      "Epoch 46, Training Loss: 0.80071653008461\n",
      "Epoch 47, Training Loss: 0.8007007472655352\n",
      "Epoch 48, Training Loss: 0.8002913154574002\n",
      "Epoch 49, Training Loss: 0.800135175410439\n",
      "Epoch 50, Training Loss: 0.8002813792228699\n",
      "Epoch 51, Training Loss: 0.8000547571042005\n",
      "Epoch 52, Training Loss: 0.7997062103187337\n",
      "Epoch 53, Training Loss: 0.8001348910612218\n",
      "Epoch 54, Training Loss: 0.7995401163662181\n",
      "Epoch 55, Training Loss: 0.7999536514983457\n",
      "Epoch 56, Training Loss: 0.7999473684675553\n",
      "Epoch 57, Training Loss: 0.7997813599951127\n",
      "Epoch 58, Training Loss: 0.800258487252628\n",
      "Epoch 59, Training Loss: 0.79950825102189\n",
      "Epoch 60, Training Loss: 0.7996396115948172\n",
      "Epoch 61, Training Loss: 0.7993810728017021\n",
      "Epoch 62, Training Loss: 0.7997306018015917\n",
      "Epoch 63, Training Loss: 0.8001642278362723\n",
      "Epoch 64, Training Loss: 0.7988762971232919\n",
      "Epoch 65, Training Loss: 0.7992273317365085\n",
      "Epoch 66, Training Loss: 0.7986364765728221\n",
      "Epoch 67, Training Loss: 0.7985326234733358\n",
      "Epoch 68, Training Loss: 0.7997098208876217\n",
      "Epoch 69, Training Loss: 0.7992844688191133\n",
      "Epoch 70, Training Loss: 0.7990737275516285\n",
      "Epoch 71, Training Loss: 0.7994235857795267\n",
      "Epoch 72, Training Loss: 0.7990754606443293\n",
      "Epoch 73, Training Loss: 0.7986913713988136\n",
      "Epoch 74, Training Loss: 0.798687984382405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:35:22,698] Trial 387 finished with value: 0.6393333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7993798159150516\n",
      "Epoch 1, Training Loss: 1.0977025268669416\n",
      "Epoch 2, Training Loss: 1.0834011013346507\n",
      "Epoch 3, Training Loss: 1.0738177593489338\n",
      "Epoch 4, Training Loss: 1.065589529948127\n",
      "Epoch 5, Training Loss: 1.0575737288123683\n",
      "Epoch 6, Training Loss: 1.0496415787173392\n",
      "Epoch 7, Training Loss: 1.041443281424673\n",
      "Epoch 8, Training Loss: 1.0333826275696432\n",
      "Epoch 9, Training Loss: 1.0247558562379135\n",
      "Epoch 10, Training Loss: 1.0162168639046805\n",
      "Epoch 11, Training Loss: 1.0082841945770091\n",
      "Epoch 12, Training Loss: 0.999896252155304\n",
      "Epoch 13, Training Loss: 0.9927409135309377\n",
      "Epoch 14, Training Loss: 0.9859112598842248\n",
      "Epoch 15, Training Loss: 0.9795199689112212\n",
      "Epoch 16, Training Loss: 0.9741914824435586\n",
      "Epoch 17, Training Loss: 0.9694705017527243\n",
      "Epoch 18, Training Loss: 0.9656336832763557\n",
      "Epoch 19, Training Loss: 0.9619005516059417\n",
      "Epoch 20, Training Loss: 0.9589568807666463\n",
      "Epoch 21, Training Loss: 0.956452731171945\n",
      "Epoch 22, Training Loss: 0.9537245952993407\n",
      "Epoch 23, Training Loss: 0.9519654312528165\n",
      "Epoch 24, Training Loss: 0.9501337252165142\n",
      "Epoch 25, Training Loss: 0.9486073890126737\n",
      "Epoch 26, Training Loss: 0.9469489819125125\n",
      "Epoch 27, Training Loss: 0.9459807149449685\n",
      "Epoch 28, Training Loss: 0.9456055045127869\n",
      "Epoch 29, Training Loss: 0.9438040746782059\n",
      "Epoch 30, Training Loss: 0.9424640920825471\n",
      "Epoch 31, Training Loss: 0.9414776794892504\n",
      "Epoch 32, Training Loss: 0.9407186713433804\n",
      "Epoch 33, Training Loss: 0.9398677341920093\n",
      "Epoch 34, Training Loss: 0.939039627232946\n",
      "Epoch 35, Training Loss: 0.9380728544149184\n",
      "Epoch 36, Training Loss: 0.937080344669801\n",
      "Epoch 37, Training Loss: 0.9366799955081223\n",
      "Epoch 38, Training Loss: 0.9361373753475963\n",
      "Epoch 39, Training Loss: 0.9352856653973572\n",
      "Epoch 40, Training Loss: 0.9341885123037754\n",
      "Epoch 41, Training Loss: 0.933716999319263\n",
      "Epoch 42, Training Loss: 0.932784294723568\n",
      "Epoch 43, Training Loss: 0.9323967350156683\n",
      "Epoch 44, Training Loss: 0.9317907374604304\n",
      "Epoch 45, Training Loss: 0.9312242900518547\n",
      "Epoch 46, Training Loss: 0.9305309206023252\n",
      "Epoch 47, Training Loss: 0.9304718361761337\n",
      "Epoch 48, Training Loss: 0.92921273493229\n",
      "Epoch 49, Training Loss: 0.9290352962070838\n",
      "Epoch 50, Training Loss: 0.9281062393260181\n",
      "Epoch 51, Training Loss: 0.927840456747471\n",
      "Epoch 52, Training Loss: 0.927126669973359\n",
      "Epoch 53, Training Loss: 0.9266519855735893\n",
      "Epoch 54, Training Loss: 0.9255399469146155\n",
      "Epoch 55, Training Loss: 0.9249986924623188\n",
      "Epoch 56, Training Loss: 0.9241402823225896\n",
      "Epoch 57, Training Loss: 0.9246688632140482\n",
      "Epoch 58, Training Loss: 0.9242721279760948\n",
      "Epoch 59, Training Loss: 0.9230262389756683\n",
      "Epoch 60, Training Loss: 0.9227674960193778\n",
      "Epoch 61, Training Loss: 0.9224622374190423\n",
      "Epoch 62, Training Loss: 0.921652344922374\n",
      "Epoch 63, Training Loss: 0.9209764891997316\n",
      "Epoch 64, Training Loss: 0.9205620203699384\n",
      "Epoch 65, Training Loss: 0.9197964759697591\n",
      "Epoch 66, Training Loss: 0.9196553831709955\n",
      "Epoch 67, Training Loss: 0.9192955943874846\n",
      "Epoch 68, Training Loss: 0.9179416779288672\n",
      "Epoch 69, Training Loss: 0.9179136831957595\n",
      "Epoch 70, Training Loss: 0.9178472224034762\n",
      "Epoch 71, Training Loss: 0.9171029170652978\n",
      "Epoch 72, Training Loss: 0.9161548943447887\n",
      "Epoch 73, Training Loss: 0.9160199065853779\n",
      "Epoch 74, Training Loss: 0.9156571086188008\n",
      "Epoch 75, Training Loss: 0.9143410053468288\n",
      "Epoch 76, Training Loss: 0.914008154904932\n",
      "Epoch 77, Training Loss: 0.9141280969282738\n",
      "Epoch 78, Training Loss: 0.9126018465909743\n",
      "Epoch 79, Training Loss: 0.9125480186670346\n",
      "Epoch 80, Training Loss: 0.9119078171880621\n",
      "Epoch 81, Training Loss: 0.9115639070819195\n",
      "Epoch 82, Training Loss: 0.9118276718863867\n",
      "Epoch 83, Training Loss: 0.9105086115966166\n",
      "Epoch 84, Training Loss: 0.9102241580647633\n",
      "Epoch 85, Training Loss: 0.9093801517235606\n",
      "Epoch 86, Training Loss: 0.9085749112573781\n",
      "Epoch 87, Training Loss: 0.9085641669151479\n",
      "Epoch 88, Training Loss: 0.9075045012889948\n",
      "Epoch 89, Training Loss: 0.9067230817070581\n",
      "Epoch 90, Training Loss: 0.9063013079471158\n",
      "Epoch 91, Training Loss: 0.9056401037632075\n",
      "Epoch 92, Training Loss: 0.9051931317587544\n",
      "Epoch 93, Training Loss: 0.904588798920911\n",
      "Epoch 94, Training Loss: 0.9039702696907789\n",
      "Epoch 95, Training Loss: 0.9031559884996343\n",
      "Epoch 96, Training Loss: 0.9028056668159657\n",
      "Epoch 97, Training Loss: 0.9013421035350714\n",
      "Epoch 98, Training Loss: 0.9011458096647621\n",
      "Epoch 99, Training Loss: 0.9003412534419756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:36:54,469] Trial 388 finished with value: 0.574 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.89986613702057\n",
      "Epoch 1, Training Loss: 0.9186448625957264\n",
      "Epoch 2, Training Loss: 0.8501100474245408\n",
      "Epoch 3, Training Loss: 0.8216235950413873\n",
      "Epoch 4, Training Loss: 0.8138327553692986\n",
      "Epoch 5, Training Loss: 0.8115364253520966\n",
      "Epoch 6, Training Loss: 0.8103488522417405\n",
      "Epoch 7, Training Loss: 0.8096383880867678\n",
      "Epoch 8, Training Loss: 0.8088923187115613\n",
      "Epoch 9, Training Loss: 0.808321461467182\n",
      "Epoch 10, Training Loss: 0.807788852593478\n",
      "Epoch 11, Training Loss: 0.8074281299114228\n",
      "Epoch 12, Training Loss: 0.8070945211719064\n",
      "Epoch 13, Training Loss: 0.8069694282026851\n",
      "Epoch 14, Training Loss: 0.8067201340198517\n",
      "Epoch 15, Training Loss: 0.8063082727965186\n",
      "Epoch 16, Training Loss: 0.8062616290765651\n",
      "Epoch 17, Training Loss: 0.806059116896461\n",
      "Epoch 18, Training Loss: 0.8055834103331846\n",
      "Epoch 19, Training Loss: 0.805095973295324\n",
      "Epoch 20, Training Loss: 0.8050427326735328\n",
      "Epoch 21, Training Loss: 0.8046403439605937\n",
      "Epoch 22, Training Loss: 0.8046120995633742\n",
      "Epoch 23, Training Loss: 0.8043539179072661\n",
      "Epoch 24, Training Loss: 0.8040649160216836\n",
      "Epoch 25, Training Loss: 0.8037687477644752\n",
      "Epoch 26, Training Loss: 0.8036795761304744\n",
      "Epoch 27, Training Loss: 0.8033946279918446\n",
      "Epoch 28, Training Loss: 0.8031038205763873\n",
      "Epoch 29, Training Loss: 0.8029647494063658\n",
      "Epoch 30, Training Loss: 0.8024764245397904\n",
      "Epoch 31, Training Loss: 0.8022668970332426\n",
      "Epoch 32, Training Loss: 0.8023752352770637\n",
      "Epoch 33, Training Loss: 0.8020213638333713\n",
      "Epoch 34, Training Loss: 0.8017671810879426\n",
      "Epoch 35, Training Loss: 0.801828584951513\n",
      "Epoch 36, Training Loss: 0.8016367966988508\n",
      "Epoch 37, Training Loss: 0.8013380335359013\n",
      "Epoch 38, Training Loss: 0.8011740389992209\n",
      "Epoch 39, Training Loss: 0.8009753418669981\n",
      "Epoch 40, Training Loss: 0.8006310195782605\n",
      "Epoch 41, Training Loss: 0.8009382193930009\n",
      "Epoch 42, Training Loss: 0.8004260324730592\n",
      "Epoch 43, Training Loss: 0.8004021601115956\n",
      "Epoch 44, Training Loss: 0.8002539299516117\n",
      "Epoch 45, Training Loss: 0.8000065719380098\n",
      "Epoch 46, Training Loss: 0.7998440990027259\n",
      "Epoch 47, Training Loss: 0.7998959697695339\n",
      "Epoch 48, Training Loss: 0.7997822646533742\n",
      "Epoch 49, Training Loss: 0.7994778929037206\n",
      "Epoch 50, Training Loss: 0.7995741455695209\n",
      "Epoch 51, Training Loss: 0.7995475708035862\n",
      "Epoch 52, Training Loss: 0.7993249379186069\n",
      "Epoch 53, Training Loss: 0.7992691241993624\n",
      "Epoch 54, Training Loss: 0.7992928533694323\n",
      "Epoch 55, Training Loss: 0.7990531122684479\n",
      "Epoch 56, Training Loss: 0.7990061133749344\n",
      "Epoch 57, Training Loss: 0.7989915516096003\n",
      "Epoch 58, Training Loss: 0.7987603203689351\n",
      "Epoch 59, Training Loss: 0.7988006748872645\n",
      "Epoch 60, Training Loss: 0.7988534291351542\n",
      "Epoch 61, Training Loss: 0.7984213613762575\n",
      "Epoch 62, Training Loss: 0.7985435858894797\n",
      "Epoch 63, Training Loss: 0.7982554237281575\n",
      "Epoch 64, Training Loss: 0.798439835730721\n",
      "Epoch 65, Training Loss: 0.7985315789194668\n",
      "Epoch 66, Training Loss: 0.7983375988988315\n",
      "Epoch 67, Training Loss: 0.7983511543975157\n",
      "Epoch 68, Training Loss: 0.7983199532592998\n",
      "Epoch 69, Training Loss: 0.7981157253770267\n",
      "Epoch 70, Training Loss: 0.7979381495363572\n",
      "Epoch 71, Training Loss: 0.7980294075432945\n",
      "Epoch 72, Training Loss: 0.7976726456249461\n",
      "Epoch 73, Training Loss: 0.7978366993455326\n",
      "Epoch 74, Training Loss: 0.7976488723474391\n",
      "Epoch 75, Training Loss: 0.7977905194198384\n",
      "Epoch 76, Training Loss: 0.7976324202733881\n",
      "Epoch 77, Training Loss: 0.7978760597285103\n",
      "Epoch 78, Training Loss: 0.7980646802397335\n",
      "Epoch 79, Training Loss: 0.7978898836584652\n",
      "Epoch 80, Training Loss: 0.7976474474458134\n",
      "Epoch 81, Training Loss: 0.7977640599362991\n",
      "Epoch 82, Training Loss: 0.7976886362188003\n",
      "Epoch 83, Training Loss: 0.7976189286568586\n",
      "Epoch 84, Training Loss: 0.7975210106372833\n",
      "Epoch 85, Training Loss: 0.7976146824219648\n",
      "Epoch 86, Training Loss: 0.7974436738210566\n",
      "Epoch 87, Training Loss: 0.7974396671968348\n",
      "Epoch 88, Training Loss: 0.7971896790055668\n",
      "Epoch 89, Training Loss: 0.7975211452035343\n",
      "Epoch 90, Training Loss: 0.7974867104782778\n",
      "Epoch 91, Training Loss: 0.7969896477110245\n",
      "Epoch 92, Training Loss: 0.7972391498088837\n",
      "Epoch 93, Training Loss: 0.7973176790686215\n",
      "Epoch 94, Training Loss: 0.7972478698281681\n",
      "Epoch 95, Training Loss: 0.7972722032490899\n",
      "Epoch 96, Training Loss: 0.7972181105613708\n",
      "Epoch 97, Training Loss: 0.7972203531685997\n",
      "Epoch 98, Training Loss: 0.7973000935947194\n",
      "Epoch 99, Training Loss: 0.7971251822920407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:38:47,649] Trial 389 finished with value: 0.6341333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7968669232200174\n",
      "Epoch 1, Training Loss: 1.0722223351772566\n",
      "Epoch 2, Training Loss: 1.027552846679114\n",
      "Epoch 3, Training Loss: 0.9999526734638932\n",
      "Epoch 4, Training Loss: 0.9825865876405759\n",
      "Epoch 5, Training Loss: 0.971436754025911\n",
      "Epoch 6, Training Loss: 0.9650201040103024\n",
      "Epoch 7, Training Loss: 0.9610169935047178\n",
      "Epoch 8, Training Loss: 0.9591361196417557\n",
      "Epoch 9, Training Loss: 0.9570129472510259\n",
      "Epoch 10, Training Loss: 0.9565322140105685\n",
      "Epoch 11, Training Loss: 0.9552767667555271\n",
      "Epoch 12, Training Loss: 0.9542341268152222\n",
      "Epoch 13, Training Loss: 0.953500056894202\n",
      "Epoch 14, Training Loss: 0.95238371002943\n",
      "Epoch 15, Training Loss: 0.951796032970113\n",
      "Epoch 16, Training Loss: 0.9509749777334974\n",
      "Epoch 17, Training Loss: 0.9507371638054238\n",
      "Epoch 18, Training Loss: 0.9492211217270758\n",
      "Epoch 19, Training Loss: 0.9482769654209452\n",
      "Epoch 20, Training Loss: 0.9477236676036863\n",
      "Epoch 21, Training Loss: 0.9461899553026472\n",
      "Epoch 22, Training Loss: 0.9454115567350746\n",
      "Epoch 23, Training Loss: 0.9443569754299365\n",
      "Epoch 24, Training Loss: 0.9440097514848064\n",
      "Epoch 25, Training Loss: 0.9421339723400604\n",
      "Epoch 26, Training Loss: 0.9410815558935467\n",
      "Epoch 27, Training Loss: 0.9406211342130388\n",
      "Epoch 28, Training Loss: 0.9393515673795141\n",
      "Epoch 29, Training Loss: 0.9378154552072511\n",
      "Epoch 30, Training Loss: 0.9366523733712677\n",
      "Epoch 31, Training Loss: 0.9355866181223016\n",
      "Epoch 32, Training Loss: 0.9344757279955355\n",
      "Epoch 33, Training Loss: 0.9332264154477227\n",
      "Epoch 34, Training Loss: 0.9316178497515226\n",
      "Epoch 35, Training Loss: 0.9302718268301254\n",
      "Epoch 36, Training Loss: 0.9292426769894765\n",
      "Epoch 37, Training Loss: 0.927584941942889\n",
      "Epoch 38, Training Loss: 0.9267519361094425\n",
      "Epoch 39, Training Loss: 0.9253276956708808\n",
      "Epoch 40, Training Loss: 0.9233256115052934\n",
      "Epoch 41, Training Loss: 0.9213333690076843\n",
      "Epoch 42, Training Loss: 0.9201838271062177\n",
      "Epoch 43, Training Loss: 0.9185417987350234\n",
      "Epoch 44, Training Loss: 0.9170772666321662\n",
      "Epoch 45, Training Loss: 0.915119957565365\n",
      "Epoch 46, Training Loss: 0.9127652729364266\n",
      "Epoch 47, Training Loss: 0.9113896159301127\n",
      "Epoch 48, Training Loss: 0.9097655904920477\n",
      "Epoch 49, Training Loss: 0.9082071898575116\n",
      "Epoch 50, Training Loss: 0.906338291777704\n",
      "Epoch 51, Training Loss: 0.9047319334252436\n",
      "Epoch 52, Training Loss: 0.9024606801513443\n",
      "Epoch 53, Training Loss: 0.9013464715247764\n",
      "Epoch 54, Training Loss: 0.8990253141051845\n",
      "Epoch 55, Training Loss: 0.8970688858426603\n",
      "Epoch 56, Training Loss: 0.8946182761873518\n",
      "Epoch 57, Training Loss: 0.8933650570704524\n",
      "Epoch 58, Training Loss: 0.8915616092825295\n",
      "Epoch 59, Training Loss: 0.8891848997065895\n",
      "Epoch 60, Training Loss: 0.8869991018359823\n",
      "Epoch 61, Training Loss: 0.8854719908613907\n",
      "Epoch 62, Training Loss: 0.8835904829484179\n",
      "Epoch 63, Training Loss: 0.8818737017480951\n",
      "Epoch 64, Training Loss: 0.8797698174204145\n",
      "Epoch 65, Training Loss: 0.8775598927548057\n",
      "Epoch 66, Training Loss: 0.8758042640255806\n",
      "Epoch 67, Training Loss: 0.8730791285970158\n",
      "Epoch 68, Training Loss: 0.8718477967090177\n",
      "Epoch 69, Training Loss: 0.8698276366506305\n",
      "Epoch 70, Training Loss: 0.8676394086135061\n",
      "Epoch 71, Training Loss: 0.8672438565949748\n",
      "Epoch 72, Training Loss: 0.8639763170615175\n",
      "Epoch 73, Training Loss: 0.8628358961944294\n",
      "Epoch 74, Training Loss: 0.8612950168157879\n",
      "Epoch 75, Training Loss: 0.8591128978514133\n",
      "Epoch 76, Training Loss: 0.8577013754306879\n",
      "Epoch 77, Training Loss: 0.8553670296991678\n",
      "Epoch 78, Training Loss: 0.8543022620050531\n",
      "Epoch 79, Training Loss: 0.85185052640456\n",
      "Epoch 80, Training Loss: 0.8507367428084065\n",
      "Epoch 81, Training Loss: 0.8490109025983882\n",
      "Epoch 82, Training Loss: 0.847942212560123\n",
      "Epoch 83, Training Loss: 0.8463932299972476\n",
      "Epoch 84, Training Loss: 0.8446652286034778\n",
      "Epoch 85, Training Loss: 0.8439029128927934\n",
      "Epoch 86, Training Loss: 0.842407879166137\n",
      "Epoch 87, Training Loss: 0.8409461959860378\n",
      "Epoch 88, Training Loss: 0.8393320463653794\n",
      "Epoch 89, Training Loss: 0.8384840060893755\n",
      "Epoch 90, Training Loss: 0.8373646469044506\n",
      "Epoch 91, Training Loss: 0.8362679398149476\n",
      "Epoch 92, Training Loss: 0.8349378672757544\n",
      "Epoch 93, Training Loss: 0.8343769064523223\n",
      "Epoch 94, Training Loss: 0.8328959306379906\n",
      "Epoch 95, Training Loss: 0.8322172328941804\n",
      "Epoch 96, Training Loss: 0.831369321866143\n",
      "Epoch 97, Training Loss: 0.8305307604316482\n",
      "Epoch 98, Training Loss: 0.8291488423383325\n",
      "Epoch 99, Training Loss: 0.828467047214508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:40:10,732] Trial 390 finished with value: 0.6188666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8276952388591337\n",
      "Epoch 1, Training Loss: 0.9876992794345407\n",
      "Epoch 2, Training Loss: 0.9454847752346712\n",
      "Epoch 3, Training Loss: 0.9272038622463451\n",
      "Epoch 4, Training Loss: 0.9038402731278363\n",
      "Epoch 5, Training Loss: 0.878347190618515\n",
      "Epoch 6, Training Loss: 0.8560202320183025\n",
      "Epoch 7, Training Loss: 0.8395326846487382\n",
      "Epoch 8, Training Loss: 0.8286906197491815\n",
      "Epoch 9, Training Loss: 0.8218144971482894\n",
      "Epoch 10, Training Loss: 0.8177641892433166\n",
      "Epoch 11, Training Loss: 0.8151960368717418\n",
      "Epoch 12, Training Loss: 0.8133703741606544\n",
      "Epoch 13, Training Loss: 0.8122805692869074\n",
      "Epoch 14, Training Loss: 0.8111023076141581\n",
      "Epoch 15, Training Loss: 0.8105338119759279\n",
      "Epoch 16, Training Loss: 0.8101834228459527\n",
      "Epoch 17, Training Loss: 0.8095729060734019\n",
      "Epoch 18, Training Loss: 0.8091886349986581\n",
      "Epoch 19, Training Loss: 0.8086923778057098\n",
      "Epoch 20, Training Loss: 0.8083442700610441\n",
      "Epoch 21, Training Loss: 0.8081834460707272\n",
      "Epoch 22, Training Loss: 0.8078816134088179\n",
      "Epoch 23, Training Loss: 0.8075830811612746\n",
      "Epoch 24, Training Loss: 0.8071673577673295\n",
      "Epoch 25, Training Loss: 0.8069436494041892\n",
      "Epoch 26, Training Loss: 0.8067743556639727\n",
      "Epoch 27, Training Loss: 0.8067674169820898\n",
      "Epoch 28, Training Loss: 0.8063864116107716\n",
      "Epoch 29, Training Loss: 0.8062208347460803\n",
      "Epoch 30, Training Loss: 0.8059379604283501\n",
      "Epoch 31, Training Loss: 0.8059880118510302\n",
      "Epoch 32, Training Loss: 0.8056733975690954\n",
      "Epoch 33, Training Loss: 0.8055398662651286\n",
      "Epoch 34, Training Loss: 0.8053727831560022\n",
      "Epoch 35, Training Loss: 0.8050499094233793\n",
      "Epoch 36, Training Loss: 0.8051672443922828\n",
      "Epoch 37, Training Loss: 0.8049656013881459\n",
      "Epoch 38, Training Loss: 0.8048550974621492\n",
      "Epoch 39, Training Loss: 0.8047391735104954\n",
      "Epoch 40, Training Loss: 0.8046157740144169\n",
      "Epoch 41, Training Loss: 0.8043375760667464\n",
      "Epoch 42, Training Loss: 0.8040812009222367\n",
      "Epoch 43, Training Loss: 0.8039685446374557\n",
      "Epoch 44, Training Loss: 0.8038038760073045\n",
      "Epoch 45, Training Loss: 0.8036536342957441\n",
      "Epoch 46, Training Loss: 0.8036181808920467\n",
      "Epoch 47, Training Loss: 0.8033669640737421\n",
      "Epoch 48, Training Loss: 0.803261586077073\n",
      "Epoch 49, Training Loss: 0.8030241461361156\n",
      "Epoch 50, Training Loss: 0.8029702220243566\n",
      "Epoch 51, Training Loss: 0.8030225266428554\n",
      "Epoch 52, Training Loss: 0.8027611239517436\n",
      "Epoch 53, Training Loss: 0.8026248883499819\n",
      "Epoch 54, Training Loss: 0.8024216784449185\n",
      "Epoch 55, Training Loss: 0.8024420562211205\n",
      "Epoch 56, Training Loss: 0.8021344086001901\n",
      "Epoch 57, Training Loss: 0.8020639423763051\n",
      "Epoch 58, Training Loss: 0.8020130782267627\n",
      "Epoch 59, Training Loss: 0.8019455134868622\n",
      "Epoch 60, Training Loss: 0.8019907871414633\n",
      "Epoch 61, Training Loss: 0.8016873718710507\n",
      "Epoch 62, Training Loss: 0.8014886959861307\n",
      "Epoch 63, Training Loss: 0.8015734123482424\n",
      "Epoch 64, Training Loss: 0.8013917141100939\n",
      "Epoch 65, Training Loss: 0.8012187366625841\n",
      "Epoch 66, Training Loss: 0.8010970154930563\n",
      "Epoch 67, Training Loss: 0.8010553794748643\n",
      "Epoch 68, Training Loss: 0.801067424030865\n",
      "Epoch 69, Training Loss: 0.8007838881015777\n",
      "Epoch 70, Training Loss: 0.8006662132459529\n",
      "Epoch 71, Training Loss: 0.8007480581367717\n",
      "Epoch 72, Training Loss: 0.8006826864971834\n",
      "Epoch 73, Training Loss: 0.8006213622934678\n",
      "Epoch 74, Training Loss: 0.8002606006930856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:41:39,674] Trial 391 finished with value: 0.6338 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8005529524298275\n",
      "Epoch 1, Training Loss: 0.8683574776900442\n",
      "Epoch 2, Training Loss: 0.8154340588060537\n",
      "Epoch 3, Training Loss: 0.809544166765715\n",
      "Epoch 4, Training Loss: 0.8033174208232334\n",
      "Epoch 5, Training Loss: 0.7995649613832173\n",
      "Epoch 6, Training Loss: 0.797918178622884\n",
      "Epoch 7, Training Loss: 0.7961339614445105\n",
      "Epoch 8, Training Loss: 0.7968903411599927\n",
      "Epoch 9, Training Loss: 0.7946376454113121\n",
      "Epoch 10, Training Loss: 0.7921279784432032\n",
      "Epoch 11, Training Loss: 0.7930484864048492\n",
      "Epoch 12, Training Loss: 0.7929561084374449\n",
      "Epoch 13, Training Loss: 0.7930202880300077\n",
      "Epoch 14, Training Loss: 0.7924504848799311\n",
      "Epoch 15, Training Loss: 0.7904328268273433\n",
      "Epoch 16, Training Loss: 0.7925897383152094\n",
      "Epoch 17, Training Loss: 0.7901880244563396\n",
      "Epoch 18, Training Loss: 0.789621305017543\n",
      "Epoch 19, Training Loss: 0.7910211984376262\n",
      "Epoch 20, Training Loss: 0.790363385623559\n",
      "Epoch 21, Training Loss: 0.7907942478818105\n",
      "Epoch 22, Training Loss: 0.7901618252123209\n",
      "Epoch 23, Training Loss: 0.7895890316568819\n",
      "Epoch 24, Training Loss: 0.7895204317300839\n",
      "Epoch 25, Training Loss: 0.7890325279164135\n",
      "Epoch 26, Training Loss: 0.789670763876205\n",
      "Epoch 27, Training Loss: 0.7886489329481483\n",
      "Epoch 28, Training Loss: 0.7887552230877983\n",
      "Epoch 29, Training Loss: 0.7883254258256209\n",
      "Epoch 30, Training Loss: 0.7871107689420084\n",
      "Epoch 31, Training Loss: 0.7877340183222205\n",
      "Epoch 32, Training Loss: 0.7885949122278314\n",
      "Epoch 33, Training Loss: 0.7877904330877433\n",
      "Epoch 34, Training Loss: 0.7872447045225847\n",
      "Epoch 35, Training Loss: 0.7888443234271573\n",
      "Epoch 36, Training Loss: 0.786963833543591\n",
      "Epoch 37, Training Loss: 0.7871291931410481\n",
      "Epoch 38, Training Loss: 0.7871837689464254\n",
      "Epoch 39, Training Loss: 0.7881803055454913\n",
      "Epoch 40, Training Loss: 0.7858238913062819\n",
      "Epoch 41, Training Loss: 0.7864659231408198\n",
      "Epoch 42, Training Loss: 0.7859533946316941\n",
      "Epoch 43, Training Loss: 0.7859729683488832\n",
      "Epoch 44, Training Loss: 0.785411614582951\n",
      "Epoch 45, Training Loss: 0.7857225648442605\n",
      "Epoch 46, Training Loss: 0.7861372884054829\n",
      "Epoch 47, Training Loss: 0.785776152198476\n",
      "Epoch 48, Training Loss: 0.7850797355623174\n",
      "Epoch 49, Training Loss: 0.7853677508526279\n",
      "Epoch 50, Training Loss: 0.7861080216285877\n",
      "Epoch 51, Training Loss: 0.7855261361688599\n",
      "Epoch 52, Training Loss: 0.785309138154625\n",
      "Epoch 53, Training Loss: 0.7839232019912031\n",
      "Epoch 54, Training Loss: 0.7843243227865463\n",
      "Epoch 55, Training Loss: 0.7843674042171105\n",
      "Epoch 56, Training Loss: 0.7842604742014319\n",
      "Epoch 57, Training Loss: 0.7838363402768185\n",
      "Epoch 58, Training Loss: 0.7843906857913598\n",
      "Epoch 59, Training Loss: 0.7840701649959823\n",
      "Epoch 60, Training Loss: 0.784793358100088\n",
      "Epoch 61, Training Loss: 0.7846717037652668\n",
      "Epoch 62, Training Loss: 0.7846596734864372\n",
      "Epoch 63, Training Loss: 0.7837548994480219\n",
      "Epoch 64, Training Loss: 0.7835657830525161\n",
      "Epoch 65, Training Loss: 0.7833558465305127\n",
      "Epoch 66, Training Loss: 0.7834302451377525\n",
      "Epoch 67, Training Loss: 0.7830557860826192\n",
      "Epoch 68, Training Loss: 0.7835414093239863\n",
      "Epoch 69, Training Loss: 0.783191460415833\n",
      "Epoch 70, Training Loss: 0.7827390532744558\n",
      "Epoch 71, Training Loss: 0.7841799153421158\n",
      "Epoch 72, Training Loss: 0.7826839265966774\n",
      "Epoch 73, Training Loss: 0.7835559206797664\n",
      "Epoch 74, Training Loss: 0.7827728405930943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:43:09,667] Trial 392 finished with value: 0.6397333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7821617894826975\n",
      "Epoch 1, Training Loss: 0.930649686420665\n",
      "Epoch 2, Training Loss: 0.8556517293172724\n",
      "Epoch 3, Training Loss: 0.8232550630849951\n",
      "Epoch 4, Training Loss: 0.8145161582441891\n",
      "Epoch 5, Training Loss: 0.8114515010749592\n",
      "Epoch 6, Training Loss: 0.8098575826252208\n",
      "Epoch 7, Training Loss: 0.8090805276702432\n",
      "Epoch 8, Training Loss: 0.808706030775519\n",
      "Epoch 9, Training Loss: 0.8078273884689107\n",
      "Epoch 10, Training Loss: 0.8073257234517266\n",
      "Epoch 11, Training Loss: 0.8076184898264268\n",
      "Epoch 12, Training Loss: 0.8066373353144701\n",
      "Epoch 13, Training Loss: 0.8062185072197634\n",
      "Epoch 14, Training Loss: 0.8058390732372508\n",
      "Epoch 15, Training Loss: 0.8057047097121968\n",
      "Epoch 16, Training Loss: 0.80515685060445\n",
      "Epoch 17, Training Loss: 0.8053726180861978\n",
      "Epoch 18, Training Loss: 0.8046674773272346\n",
      "Epoch 19, Training Loss: 0.804464721399195\n",
      "Epoch 20, Training Loss: 0.8045930335100959\n",
      "Epoch 21, Training Loss: 0.8038282334804535\n",
      "Epoch 22, Training Loss: 0.8036086413439583\n",
      "Epoch 23, Training Loss: 0.8031793509511387\n",
      "Epoch 24, Training Loss: 0.8032383080089793\n",
      "Epoch 25, Training Loss: 0.8027364535892711\n",
      "Epoch 26, Training Loss: 0.8024482050362756\n",
      "Epoch 27, Training Loss: 0.8024272020424114\n",
      "Epoch 28, Training Loss: 0.8023181613052592\n",
      "Epoch 29, Training Loss: 0.8023077543342815\n",
      "Epoch 30, Training Loss: 0.80165270244374\n",
      "Epoch 31, Training Loss: 0.8016078277896432\n",
      "Epoch 32, Training Loss: 0.8019463206739986\n",
      "Epoch 33, Training Loss: 0.8010877229185666\n",
      "Epoch 34, Training Loss: 0.8010227363249834\n",
      "Epoch 35, Training Loss: 0.800629579670289\n",
      "Epoch 36, Training Loss: 0.8006223551666035\n",
      "Epoch 37, Training Loss: 0.8006049492078668\n",
      "Epoch 38, Training Loss: 0.8005855647956623\n",
      "Epoch 39, Training Loss: 0.8001028009021983\n",
      "Epoch 40, Training Loss: 0.8002848900065702\n",
      "Epoch 41, Training Loss: 0.8000450869167552\n",
      "Epoch 42, Training Loss: 0.7999242000018849\n",
      "Epoch 43, Training Loss: 0.7995622684675104\n",
      "Epoch 44, Training Loss: 0.7996077570494483\n",
      "Epoch 45, Training Loss: 0.7994493972553927\n",
      "Epoch 46, Training Loss: 0.7999004746184629\n",
      "Epoch 47, Training Loss: 0.7993696497468388\n",
      "Epoch 48, Training Loss: 0.7992918764142429\n",
      "Epoch 49, Training Loss: 0.7994131727078382\n",
      "Epoch 50, Training Loss: 0.7992390902603373\n",
      "Epoch 51, Training Loss: 0.7993434870944304\n",
      "Epoch 52, Training Loss: 0.7989105626414804\n",
      "Epoch 53, Training Loss: 0.7990049207210541\n",
      "Epoch 54, Training Loss: 0.7984663950695711\n",
      "Epoch 55, Training Loss: 0.7988080318535076\n",
      "Epoch 56, Training Loss: 0.7985577973197489\n",
      "Epoch 57, Training Loss: 0.7978806203954361\n",
      "Epoch 58, Training Loss: 0.7985100173950195\n",
      "Epoch 59, Training Loss: 0.7989605095106013\n",
      "Epoch 60, Training Loss: 0.798243345933802\n",
      "Epoch 61, Training Loss: 0.798347589338527\n",
      "Epoch 62, Training Loss: 0.7981056813632741\n",
      "Epoch 63, Training Loss: 0.7983308085974525\n",
      "Epoch 64, Training Loss: 0.7981097942941329\n",
      "Epoch 65, Training Loss: 0.7981219596722546\n",
      "Epoch 66, Training Loss: 0.7984044929812936\n",
      "Epoch 67, Training Loss: 0.7984517838674433\n",
      "Epoch 68, Training Loss: 0.7978177078331218\n",
      "Epoch 69, Training Loss: 0.7982753456340117\n",
      "Epoch 70, Training Loss: 0.7979184725004084\n",
      "Epoch 71, Training Loss: 0.7979247380705441\n",
      "Epoch 72, Training Loss: 0.798080967875088\n",
      "Epoch 73, Training Loss: 0.7974817333501928\n",
      "Epoch 74, Training Loss: 0.7977251016392427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:44:39,543] Trial 393 finished with value: 0.6342 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7975301356175366\n",
      "Epoch 1, Training Loss: 0.8464425493900041\n",
      "Epoch 2, Training Loss: 0.8194714663620282\n",
      "Epoch 3, Training Loss: 0.8137508588625972\n",
      "Epoch 4, Training Loss: 0.8136096014116043\n",
      "Epoch 5, Training Loss: 0.8128194353634254\n",
      "Epoch 6, Training Loss: 0.8100190466507933\n",
      "Epoch 7, Training Loss: 0.8100411247490044\n",
      "Epoch 8, Training Loss: 0.8078387741755723\n",
      "Epoch 9, Training Loss: 0.8085373977969463\n",
      "Epoch 10, Training Loss: 0.805718341447357\n",
      "Epoch 11, Training Loss: 0.8074067983412205\n",
      "Epoch 12, Training Loss: 0.8080382115858838\n",
      "Epoch 13, Training Loss: 0.8090296713929427\n",
      "Epoch 14, Training Loss: 0.8089736370215739\n",
      "Epoch 15, Training Loss: 0.806764500929897\n",
      "Epoch 16, Training Loss: 0.8068009828266345\n",
      "Epoch 17, Training Loss: 0.806559623811478\n",
      "Epoch 18, Training Loss: 0.8107670376175329\n",
      "Epoch 19, Training Loss: 0.8086438423708866\n",
      "Epoch 20, Training Loss: 0.8090373220300315\n",
      "Epoch 21, Training Loss: 0.8072752671134203\n",
      "Epoch 22, Training Loss: 0.8044147693124929\n",
      "Epoch 23, Training Loss: 0.8057595069247081\n",
      "Epoch 24, Training Loss: 0.8086552168193616\n",
      "Epoch 25, Training Loss: 0.805992839659067\n",
      "Epoch 26, Training Loss: 0.8061093722070967\n",
      "Epoch 27, Training Loss: 0.805859357163422\n",
      "Epoch 28, Training Loss: 0.8064940315440186\n",
      "Epoch 29, Training Loss: 0.8057868322931734\n",
      "Epoch 30, Training Loss: 0.8062873914725799\n",
      "Epoch 31, Training Loss: 0.8079912501170223\n",
      "Epoch 32, Training Loss: 0.8104239122311871\n",
      "Epoch 33, Training Loss: 0.8022035091443169\n",
      "Epoch 34, Training Loss: 0.8054571626777935\n",
      "Epoch 35, Training Loss: 0.8068520914343067\n",
      "Epoch 36, Training Loss: 0.8067458045213742\n",
      "Epoch 37, Training Loss: 0.8068087058856075\n",
      "Epoch 38, Training Loss: 0.8055722976985731\n",
      "Epoch 39, Training Loss: 0.8076249098419247\n",
      "Epoch 40, Training Loss: 0.805508139617461\n",
      "Epoch 41, Training Loss: 0.8046175666321489\n",
      "Epoch 42, Training Loss: 0.8072902978811048\n",
      "Epoch 43, Training Loss: 0.8032703526037976\n",
      "Epoch 44, Training Loss: 0.804113714542604\n",
      "Epoch 45, Training Loss: 0.80869318381288\n",
      "Epoch 46, Training Loss: 0.8049567437709723\n",
      "Epoch 47, Training Loss: 0.8075786120909497\n",
      "Epoch 48, Training Loss: 0.8061639592163545\n",
      "Epoch 49, Training Loss: 0.8039719539477412\n",
      "Epoch 50, Training Loss: 0.8073832657104149\n",
      "Epoch 51, Training Loss: 0.8048382984964471\n",
      "Epoch 52, Training Loss: 0.8065390187994879\n",
      "Epoch 53, Training Loss: 0.8066618454187436\n",
      "Epoch 54, Training Loss: 0.8065178336057448\n",
      "Epoch 55, Training Loss: 0.8104495958278054\n",
      "Epoch 56, Training Loss: 0.8057214347043432\n",
      "Epoch 57, Training Loss: 0.8050732130394842\n",
      "Epoch 58, Training Loss: 0.8053827266047772\n",
      "Epoch 59, Training Loss: 0.8049927220308691\n",
      "Epoch 60, Training Loss: 0.8072947203664851\n",
      "Epoch 61, Training Loss: 0.8083956505122938\n",
      "Epoch 62, Training Loss: 0.805875633712998\n",
      "Epoch 63, Training Loss: 0.8078139006643367\n",
      "Epoch 64, Training Loss: 0.8070650827615781\n",
      "Epoch 65, Training Loss: 0.8032858780004029\n",
      "Epoch 66, Training Loss: 0.8053630420139858\n",
      "Epoch 67, Training Loss: 0.8059415062567346\n",
      "Epoch 68, Training Loss: 0.8094628492244205\n",
      "Epoch 69, Training Loss: 0.805919221021179\n",
      "Epoch 70, Training Loss: 0.8063834859912556\n",
      "Epoch 71, Training Loss: 0.8036503658258826\n",
      "Epoch 72, Training Loss: 0.8087102749293908\n",
      "Epoch 73, Training Loss: 0.8089156235071053\n",
      "Epoch 74, Training Loss: 0.810272144643884\n",
      "Epoch 75, Training Loss: 0.8094146643366132\n",
      "Epoch 76, Training Loss: 0.8085097853402446\n",
      "Epoch 77, Training Loss: 0.8081116915645455\n",
      "Epoch 78, Training Loss: 0.8045187268938337\n",
      "Epoch 79, Training Loss: 0.8098850312537716\n",
      "Epoch 80, Training Loss: 0.80741127137851\n",
      "Epoch 81, Training Loss: 0.8065592833031389\n",
      "Epoch 82, Training Loss: 0.8073789129579874\n",
      "Epoch 83, Training Loss: 0.8062989344274191\n",
      "Epoch 84, Training Loss: 0.8092396139202261\n",
      "Epoch 85, Training Loss: 0.8083294152317191\n",
      "Epoch 86, Training Loss: 0.8088438910649235\n",
      "Epoch 87, Training Loss: 0.8069916208883874\n",
      "Epoch 88, Training Loss: 0.8070562621704618\n",
      "Epoch 89, Training Loss: 0.8087598527284493\n",
      "Epoch 90, Training Loss: 0.8081207742368368\n",
      "Epoch 91, Training Loss: 0.8064960549648543\n",
      "Epoch 92, Training Loss: 0.8077586541498514\n",
      "Epoch 93, Training Loss: 0.807730133372142\n",
      "Epoch 94, Training Loss: 0.8091366128813952\n",
      "Epoch 95, Training Loss: 0.8072184041030425\n",
      "Epoch 96, Training Loss: 0.8082051931467271\n",
      "Epoch 97, Training Loss: 0.8114811662444494\n",
      "Epoch 98, Training Loss: 0.8072384740176953\n",
      "Epoch 99, Training Loss: 0.809079971349329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:46:40,312] Trial 394 finished with value: 0.6212 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8075896169906273\n",
      "Epoch 1, Training Loss: 0.9721637995619523\n",
      "Epoch 2, Training Loss: 0.9442549572851425\n",
      "Epoch 3, Training Loss: 0.9272921363213905\n",
      "Epoch 4, Training Loss: 0.9073209871026806\n",
      "Epoch 5, Training Loss: 0.8852710280203282\n",
      "Epoch 6, Training Loss: 0.8647267728819883\n",
      "Epoch 7, Training Loss: 0.8477044792999898\n",
      "Epoch 8, Training Loss: 0.8351492839648311\n",
      "Epoch 9, Training Loss: 0.8267625591808692\n",
      "Epoch 10, Training Loss: 0.8213840799224108\n",
      "Epoch 11, Training Loss: 0.8170828383668025\n",
      "Epoch 12, Training Loss: 0.8144533680793934\n",
      "Epoch 13, Training Loss: 0.8128578885157306\n",
      "Epoch 14, Training Loss: 0.8123007064475153\n",
      "Epoch 15, Training Loss: 0.8108477193610113\n",
      "Epoch 16, Training Loss: 0.8108107771192278\n",
      "Epoch 17, Training Loss: 0.8098189058160423\n",
      "Epoch 18, Training Loss: 0.8098004049824593\n",
      "Epoch 19, Training Loss: 0.8088623642921448\n",
      "Epoch 20, Training Loss: 0.80849748096968\n",
      "Epoch 21, Training Loss: 0.8083488699188806\n",
      "Epoch 22, Training Loss: 0.8082618825417712\n",
      "Epoch 23, Training Loss: 0.8085727083055596\n",
      "Epoch 24, Training Loss: 0.807615369872043\n",
      "Epoch 25, Training Loss: 0.8071415069407987\n",
      "Epoch 26, Training Loss: 0.8074490544491244\n",
      "Epoch 27, Training Loss: 0.8075454131104892\n",
      "Epoch 28, Training Loss: 0.8071645355762396\n",
      "Epoch 29, Training Loss: 0.8063222970281329\n",
      "Epoch 30, Training Loss: 0.8064990767858978\n",
      "Epoch 31, Training Loss: 0.8062009961981522\n",
      "Epoch 32, Training Loss: 0.8057170135634286\n",
      "Epoch 33, Training Loss: 0.8069425527314494\n",
      "Epoch 34, Training Loss: 0.8060228475950715\n",
      "Epoch 35, Training Loss: 0.8058069244363254\n",
      "Epoch 36, Training Loss: 0.8059664614218518\n",
      "Epoch 37, Training Loss: 0.8057452067396694\n",
      "Epoch 38, Training Loss: 0.8051826542481444\n",
      "Epoch 39, Training Loss: 0.8050696783496025\n",
      "Epoch 40, Training Loss: 0.8051598200224396\n",
      "Epoch 41, Training Loss: 0.8053779190644286\n",
      "Epoch 42, Training Loss: 0.805328361073831\n",
      "Epoch 43, Training Loss: 0.8052766639487188\n",
      "Epoch 44, Training Loss: 0.804376889261088\n",
      "Epoch 45, Training Loss: 0.8039716737610954\n",
      "Epoch 46, Training Loss: 0.8049655282407775\n",
      "Epoch 47, Training Loss: 0.8041842329770998\n",
      "Epoch 48, Training Loss: 0.8044450238235015\n",
      "Epoch 49, Training Loss: 0.8034994842414569\n",
      "Epoch 50, Training Loss: 0.8038704829108446\n",
      "Epoch 51, Training Loss: 0.8041315857629131\n",
      "Epoch 52, Training Loss: 0.8039907528045482\n",
      "Epoch 53, Training Loss: 0.8038186517873205\n",
      "Epoch 54, Training Loss: 0.8035353564678278\n",
      "Epoch 55, Training Loss: 0.8049181561720998\n",
      "Epoch 56, Training Loss: 0.8034695406605427\n",
      "Epoch 57, Training Loss: 0.8034924914962367\n",
      "Epoch 58, Training Loss: 0.8031350428000429\n",
      "Epoch 59, Training Loss: 0.8025692989951686\n",
      "Epoch 60, Training Loss: 0.8030098568227955\n",
      "Epoch 61, Training Loss: 0.8026111462062463\n",
      "Epoch 62, Training Loss: 0.8031420970321598\n",
      "Epoch 63, Training Loss: 0.8022667696601467\n",
      "Epoch 64, Training Loss: 0.8028569594361729\n",
      "Epoch 65, Training Loss: 0.8020122321924769\n",
      "Epoch 66, Training Loss: 0.8021391410576669\n",
      "Epoch 67, Training Loss: 0.8019181120664554\n",
      "Epoch 68, Training Loss: 0.8016365942202116\n",
      "Epoch 69, Training Loss: 0.8023722420957752\n",
      "Epoch 70, Training Loss: 0.8019943925671111\n",
      "Epoch 71, Training Loss: 0.8014320744607681\n",
      "Epoch 72, Training Loss: 0.8018794216607746\n",
      "Epoch 73, Training Loss: 0.801212015456723\n",
      "Epoch 74, Training Loss: 0.8014204114899599\n",
      "Epoch 75, Training Loss: 0.8015439526479047\n",
      "Epoch 76, Training Loss: 0.8014077215266407\n",
      "Epoch 77, Training Loss: 0.8015423063048743\n",
      "Epoch 78, Training Loss: 0.8015329341243084\n",
      "Epoch 79, Training Loss: 0.8010626666527941\n",
      "Epoch 80, Training Loss: 0.80172493870097\n",
      "Epoch 81, Training Loss: 0.8011726525493135\n",
      "Epoch 82, Training Loss: 0.800803884527737\n",
      "Epoch 83, Training Loss: 0.8005883945558304\n",
      "Epoch 84, Training Loss: 0.800670065467519\n",
      "Epoch 85, Training Loss: 0.8003649763594892\n",
      "Epoch 86, Training Loss: 0.8002648349095108\n",
      "Epoch 87, Training Loss: 0.8002494124541605\n",
      "Epoch 88, Training Loss: 0.800231259120138\n",
      "Epoch 89, Training Loss: 0.8002845722033565\n",
      "Epoch 90, Training Loss: 0.8005117471056773\n",
      "Epoch 91, Training Loss: 0.8005751651928837\n",
      "Epoch 92, Training Loss: 0.800278761870879\n",
      "Epoch 93, Training Loss: 0.7999564499783337\n",
      "Epoch 94, Training Loss: 0.8004318326039421\n",
      "Epoch 95, Training Loss: 0.7997533696934693\n",
      "Epoch 96, Training Loss: 0.7993641267145487\n",
      "Epoch 97, Training Loss: 0.7994422444723602\n",
      "Epoch 98, Training Loss: 0.8001362545149667\n",
      "Epoch 99, Training Loss: 0.7996046196249195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:48:15,555] Trial 395 finished with value: 0.6357333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8000527815711229\n",
      "Epoch 1, Training Loss: 0.8479992572700276\n",
      "Epoch 2, Training Loss: 0.8214803516163546\n",
      "Epoch 3, Training Loss: 0.8212393466865315\n",
      "Epoch 4, Training Loss: 0.8159758595859303\n",
      "Epoch 5, Training Loss: 0.8160869474971996\n",
      "Epoch 6, Training Loss: 0.8124643017965204\n",
      "Epoch 7, Training Loss: 0.8157804617461036\n",
      "Epoch 8, Training Loss: 0.8142888486385346\n",
      "Epoch 9, Training Loss: 0.81217256917673\n",
      "Epoch 10, Training Loss: 0.8168448602451998\n",
      "Epoch 11, Training Loss: 0.8110770617513096\n",
      "Epoch 12, Training Loss: 0.8113542929116417\n",
      "Epoch 13, Training Loss: 0.8170147465257084\n",
      "Epoch 14, Training Loss: 0.8097282179664163\n",
      "Epoch 15, Training Loss: 0.8122674493929919\n",
      "Epoch 16, Training Loss: 0.8123106812729555\n",
      "Epoch 17, Training Loss: 0.8128852741858539\n",
      "Epoch 18, Training Loss: 0.8149547641417559\n",
      "Epoch 19, Training Loss: 0.8103102640544667\n",
      "Epoch 20, Training Loss: 0.8125474642304813\n",
      "Epoch 21, Training Loss: 0.8120054613842683\n",
      "Epoch 22, Training Loss: 0.8109998731753405\n",
      "Epoch 23, Training Loss: 0.813897888099446\n",
      "Epoch 24, Training Loss: 0.8128967365096597\n",
      "Epoch 25, Training Loss: 0.8135154763390037\n",
      "Epoch 26, Training Loss: 0.8135020274274489\n",
      "Epoch 27, Training Loss: 0.8098360084786135\n",
      "Epoch 28, Training Loss: 0.8142367442916422\n",
      "Epoch 29, Training Loss: 0.8125219082131105\n",
      "Epoch 30, Training Loss: 0.8119389050848344\n",
      "Epoch 31, Training Loss: 0.8154667223201079\n",
      "Epoch 32, Training Loss: 0.8133465094426099\n",
      "Epoch 33, Training Loss: 0.8191660084443934\n",
      "Epoch 34, Training Loss: 0.815883281511419\n",
      "Epoch 35, Training Loss: 0.8162380404332105\n",
      "Epoch 36, Training Loss: 0.8119953778912039\n",
      "Epoch 37, Training Loss: 0.8166025114059449\n",
      "Epoch 38, Training Loss: 0.8130080303023843\n",
      "Epoch 39, Training Loss: 0.8181395550335154\n",
      "Epoch 40, Training Loss: 0.8199064142563763\n",
      "Epoch 41, Training Loss: 0.8141115064480725\n",
      "Epoch 42, Training Loss: 0.819108984189875\n",
      "Epoch 43, Training Loss: 0.8184502910165226\n",
      "Epoch 44, Training Loss: 0.8226350719788496\n",
      "Epoch 45, Training Loss: 0.8236643394301919\n",
      "Epoch 46, Training Loss: 0.8315368606763728\n",
      "Epoch 47, Training Loss: 0.8388488456080941\n",
      "Epoch 48, Training Loss: 0.8359415564817541\n",
      "Epoch 49, Training Loss: 0.8363387523679172\n",
      "Epoch 50, Training Loss: 0.8349010755735286\n",
      "Epoch 51, Training Loss: 0.8359033418402952\n",
      "Epoch 52, Training Loss: 0.8380730230668012\n",
      "Epoch 53, Training Loss: 0.8336729753017426\n",
      "Epoch 54, Training Loss: 0.8303546786308289\n",
      "Epoch 55, Training Loss: 0.8325877719766953\n",
      "Epoch 56, Training Loss: 0.8330101962650523\n",
      "Epoch 57, Training Loss: 0.831576810724595\n",
      "Epoch 58, Training Loss: 0.8317489999182084\n",
      "Epoch 59, Training Loss: 0.8290588932878831\n",
      "Epoch 60, Training Loss: 0.8299748063087463\n",
      "Epoch 61, Training Loss: 0.8314098971731523\n",
      "Epoch 62, Training Loss: 0.830246081422357\n",
      "Epoch 63, Training Loss: 0.8317468643188477\n",
      "Epoch 64, Training Loss: 0.8283141746941735\n",
      "Epoch 65, Training Loss: 0.8309589802517611\n",
      "Epoch 66, Training Loss: 0.8271695402089287\n",
      "Epoch 67, Training Loss: 0.8317641182506785\n",
      "Epoch 68, Training Loss: 0.8314537375113543\n",
      "Epoch 69, Training Loss: 0.8284687585690442\n",
      "Epoch 70, Training Loss: 0.8327823611568003\n",
      "Epoch 71, Training Loss: 0.828694023104275\n",
      "Epoch 72, Training Loss: 0.829376289844513\n",
      "Epoch 73, Training Loss: 0.8281196590030895\n",
      "Epoch 74, Training Loss: 0.8305406227532555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:49:59,867] Trial 396 finished with value: 0.614 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.833662952745662\n",
      "Epoch 1, Training Loss: 0.8374611021880817\n",
      "Epoch 2, Training Loss: 0.8091686158251942\n",
      "Epoch 3, Training Loss: 0.8037842304186713\n",
      "Epoch 4, Training Loss: 0.800519781632531\n",
      "Epoch 5, Training Loss: 0.7979803465362778\n",
      "Epoch 6, Training Loss: 0.7978311363915752\n",
      "Epoch 7, Training Loss: 0.795726554375842\n",
      "Epoch 8, Training Loss: 0.7957106342889313\n",
      "Epoch 9, Training Loss: 0.7949311618518112\n",
      "Epoch 10, Training Loss: 0.7943802541359923\n",
      "Epoch 11, Training Loss: 0.7954601209862788\n",
      "Epoch 12, Training Loss: 0.7945112936478809\n",
      "Epoch 13, Training Loss: 0.794348674878142\n",
      "Epoch 14, Training Loss: 0.7928080872485512\n",
      "Epoch 15, Training Loss: 0.7927334756779492\n",
      "Epoch 16, Training Loss: 0.7918520987482\n",
      "Epoch 17, Training Loss: 0.7924829599552585\n",
      "Epoch 18, Training Loss: 0.7912241964411915\n",
      "Epoch 19, Training Loss: 0.792165706211463\n",
      "Epoch 20, Training Loss: 0.7911655241385438\n",
      "Epoch 21, Training Loss: 0.7912996053695679\n",
      "Epoch 22, Training Loss: 0.7919206241019686\n",
      "Epoch 23, Training Loss: 0.7903815681773021\n",
      "Epoch 24, Training Loss: 0.7924817828307474\n",
      "Epoch 25, Training Loss: 0.7906882507460458\n",
      "Epoch 26, Training Loss: 0.7911062576717004\n",
      "Epoch 27, Training Loss: 0.7901427965415152\n",
      "Epoch 28, Training Loss: 0.7908995267143824\n",
      "Epoch 29, Training Loss: 0.7904780797492292\n",
      "Epoch 30, Training Loss: 0.790071115368291\n",
      "Epoch 31, Training Loss: 0.7890901941105836\n",
      "Epoch 32, Training Loss: 0.7906042902989495\n",
      "Epoch 33, Training Loss: 0.7900935979714071\n",
      "Epoch 34, Training Loss: 0.7895642315534721\n",
      "Epoch 35, Training Loss: 0.789940399424474\n",
      "Epoch 36, Training Loss: 0.791389679012442\n",
      "Epoch 37, Training Loss: 0.7890834985819078\n",
      "Epoch 38, Training Loss: 0.7893743644083353\n",
      "Epoch 39, Training Loss: 0.7895174329442189\n",
      "Epoch 40, Training Loss: 0.7890072643308711\n",
      "Epoch 41, Training Loss: 0.7893271685542916\n",
      "Epoch 42, Training Loss: 0.7883230975696018\n",
      "Epoch 43, Training Loss: 0.7893136736145593\n",
      "Epoch 44, Training Loss: 0.7894005137278621\n",
      "Epoch 45, Training Loss: 0.7885568968335489\n",
      "Epoch 46, Training Loss: 0.7888589230695165\n",
      "Epoch 47, Training Loss: 0.7883342742919922\n",
      "Epoch 48, Training Loss: 0.787725559750894\n",
      "Epoch 49, Training Loss: 0.7887104290768616\n",
      "Epoch 50, Training Loss: 0.7886522583495406\n",
      "Epoch 51, Training Loss: 0.7886670019393577\n",
      "Epoch 52, Training Loss: 0.7885624294890496\n",
      "Epoch 53, Training Loss: 0.7891318229804362\n",
      "Epoch 54, Training Loss: 0.7879506860460553\n",
      "Epoch 55, Training Loss: 0.7885293127002573\n",
      "Epoch 56, Training Loss: 0.7880066094541909\n",
      "Epoch 57, Training Loss: 0.7893330799905878\n",
      "Epoch 58, Training Loss: 0.7882626908166068\n",
      "Epoch 59, Training Loss: 0.7878922627384501\n",
      "Epoch 60, Training Loss: 0.7895955659393081\n",
      "Epoch 61, Training Loss: 0.7870322292012379\n",
      "Epoch 62, Training Loss: 0.7878857038971177\n",
      "Epoch 63, Training Loss: 0.7874933204256502\n",
      "Epoch 64, Training Loss: 0.7877905752425803\n",
      "Epoch 65, Training Loss: 0.7874974331461397\n",
      "Epoch 66, Training Loss: 0.7871792409653054\n",
      "Epoch 67, Training Loss: 0.7866697021893092\n",
      "Epoch 68, Training Loss: 0.7884534424408934\n",
      "Epoch 69, Training Loss: 0.7887406368900959\n",
      "Epoch 70, Training Loss: 0.7883633118823059\n",
      "Epoch 71, Training Loss: 0.7877700993889256\n",
      "Epoch 72, Training Loss: 0.7882757436960264\n",
      "Epoch 73, Training Loss: 0.7878314543487435\n",
      "Epoch 74, Training Loss: 0.7861258893084705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:51:28,577] Trial 397 finished with value: 0.6387333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7884590752142713\n",
      "Epoch 1, Training Loss: 0.8846687420676737\n",
      "Epoch 2, Training Loss: 0.8256140437546898\n",
      "Epoch 3, Training Loss: 0.8190746494601755\n",
      "Epoch 4, Training Loss: 0.8163955907961902\n",
      "Epoch 5, Training Loss: 0.8137493139154771\n",
      "Epoch 6, Training Loss: 0.8122317914401783\n",
      "Epoch 7, Training Loss: 0.8106351897997015\n",
      "Epoch 8, Training Loss: 0.8103921876935398\n",
      "Epoch 9, Training Loss: 0.809041118551703\n",
      "Epoch 10, Training Loss: 0.8084627450914944\n",
      "Epoch 11, Training Loss: 0.8079093435932608\n",
      "Epoch 12, Training Loss: 0.8069767935837017\n",
      "Epoch 13, Training Loss: 0.8070254179309396\n",
      "Epoch 14, Training Loss: 0.8064264009980594\n",
      "Epoch 15, Training Loss: 0.8060935718171737\n",
      "Epoch 16, Training Loss: 0.8053902750155505\n",
      "Epoch 17, Training Loss: 0.8048722434043885\n",
      "Epoch 18, Training Loss: 0.8044628197305342\n",
      "Epoch 19, Training Loss: 0.8040836421181173\n",
      "Epoch 20, Training Loss: 0.8040292067387524\n",
      "Epoch 21, Training Loss: 0.8036390341730679\n",
      "Epoch 22, Training Loss: 0.8025662259494557\n",
      "Epoch 23, Training Loss: 0.8024283587932587\n",
      "Epoch 24, Training Loss: 0.8018863255136153\n",
      "Epoch 25, Training Loss: 0.8010907891918632\n",
      "Epoch 26, Training Loss: 0.8009401528975543\n",
      "Epoch 27, Training Loss: 0.8008458737766041\n",
      "Epoch 28, Training Loss: 0.8003628834556131\n",
      "Epoch 29, Training Loss: 0.8005009008856381\n",
      "Epoch 30, Training Loss: 0.8003652668700498\n",
      "Epoch 31, Training Loss: 0.7994964260914746\n",
      "Epoch 32, Training Loss: 0.7990847103034748\n",
      "Epoch 33, Training Loss: 0.7986699310471029\n",
      "Epoch 34, Training Loss: 0.7987495237238267\n",
      "Epoch 35, Training Loss: 0.7984497711237739\n",
      "Epoch 36, Training Loss: 0.7979499660519993\n",
      "Epoch 37, Training Loss: 0.7976790836979362\n",
      "Epoch 38, Training Loss: 0.7978655062002294\n",
      "Epoch 39, Training Loss: 0.7973124472533956\n",
      "Epoch 40, Training Loss: 0.7971463659931631\n",
      "Epoch 41, Training Loss: 0.7974566524870256\n",
      "Epoch 42, Training Loss: 0.7962079287977779\n",
      "Epoch 43, Training Loss: 0.7965454628888299\n",
      "Epoch 44, Training Loss: 0.7964761061528149\n",
      "Epoch 45, Training Loss: 0.7961637893845054\n",
      "Epoch 46, Training Loss: 0.7956493233933168\n",
      "Epoch 47, Training Loss: 0.7960265975138721\n",
      "Epoch 48, Training Loss: 0.7955011452646816\n",
      "Epoch 49, Training Loss: 0.7953260153882644\n",
      "Epoch 50, Training Loss: 0.7953772427755244\n",
      "Epoch 51, Training Loss: 0.7954940699128543\n",
      "Epoch 52, Training Loss: 0.7959045491499059\n",
      "Epoch 53, Training Loss: 0.7951972903223599\n",
      "Epoch 54, Training Loss: 0.795232425998239\n",
      "Epoch 55, Training Loss: 0.7948099159493166\n",
      "Epoch 56, Training Loss: 0.7954864604332867\n",
      "Epoch 57, Training Loss: 0.7942500363377963\n",
      "Epoch 58, Training Loss: 0.7949117418597726\n",
      "Epoch 59, Training Loss: 0.7950224546825184\n",
      "Epoch 60, Training Loss: 0.7947665775523466\n",
      "Epoch 61, Training Loss: 0.7946270450423746\n",
      "Epoch 62, Training Loss: 0.7945042231503655\n",
      "Epoch 63, Training Loss: 0.7945066352451549\n",
      "Epoch 64, Training Loss: 0.7938995588527006\n",
      "Epoch 65, Training Loss: 0.794343939037884\n",
      "Epoch 66, Training Loss: 0.794221660529866\n",
      "Epoch 67, Training Loss: 0.7940159780137679\n",
      "Epoch 68, Training Loss: 0.7936569335881402\n",
      "Epoch 69, Training Loss: 0.7936764088798972\n",
      "Epoch 70, Training Loss: 0.7937710140031927\n",
      "Epoch 71, Training Loss: 0.7935831210893743\n",
      "Epoch 72, Training Loss: 0.7933355082483853\n",
      "Epoch 73, Training Loss: 0.7934764247080859\n",
      "Epoch 74, Training Loss: 0.7935881213580861\n",
      "Epoch 75, Training Loss: 0.79291832131498\n",
      "Epoch 76, Training Loss: 0.7933518277897554\n",
      "Epoch 77, Training Loss: 0.7930622811878428\n",
      "Epoch 78, Training Loss: 0.7932310469711528\n",
      "Epoch 79, Training Loss: 0.7934834680136512\n",
      "Epoch 80, Training Loss: 0.7930037819637972\n",
      "Epoch 81, Training Loss: 0.793068608676686\n",
      "Epoch 82, Training Loss: 0.7932448613643647\n",
      "Epoch 83, Training Loss: 0.7932191278653986\n",
      "Epoch 84, Training Loss: 0.7926771842030917\n",
      "Epoch 85, Training Loss: 0.7926133692965788\n",
      "Epoch 86, Training Loss: 0.7931937804642846\n",
      "Epoch 87, Training Loss: 0.7931326704165514\n",
      "Epoch 88, Training Loss: 0.7926453722925747\n",
      "Epoch 89, Training Loss: 0.7922891656791463\n",
      "Epoch 90, Training Loss: 0.7923547448831446\n",
      "Epoch 91, Training Loss: 0.7920215481870314\n",
      "Epoch 92, Training Loss: 0.7927210424928104\n",
      "Epoch 93, Training Loss: 0.7922744362494525\n",
      "Epoch 94, Training Loss: 0.7924640293682322\n",
      "Epoch 95, Training Loss: 0.7919002766468946\n",
      "Epoch 96, Training Loss: 0.7919614205640905\n",
      "Epoch 97, Training Loss: 0.792142861029681\n",
      "Epoch 98, Training Loss: 0.7916170043103835\n",
      "Epoch 99, Training Loss: 0.7919184771706076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:53:20,304] Trial 398 finished with value: 0.6352 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7921737064333523\n",
      "Epoch 1, Training Loss: 0.8966195063030018\n",
      "Epoch 2, Training Loss: 0.8241227871530197\n",
      "Epoch 3, Training Loss: 0.8181345415115356\n",
      "Epoch 4, Training Loss: 0.8154381701525519\n",
      "Epoch 5, Training Loss: 0.8135156128687017\n",
      "Epoch 6, Training Loss: 0.8112443885382484\n",
      "Epoch 7, Training Loss: 0.8096327291516696\n",
      "Epoch 8, Training Loss: 0.808911064863205\n",
      "Epoch 9, Training Loss: 0.8084001659645754\n",
      "Epoch 10, Training Loss: 0.8073809101301082\n",
      "Epoch 11, Training Loss: 0.8065143007390639\n",
      "Epoch 12, Training Loss: 0.8066044804629158\n",
      "Epoch 13, Training Loss: 0.8056154925682966\n",
      "Epoch 14, Training Loss: 0.804867854048224\n",
      "Epoch 15, Training Loss: 0.804301128878313\n",
      "Epoch 16, Training Loss: 0.804693156340543\n",
      "Epoch 17, Training Loss: 0.8036532763873829\n",
      "Epoch 18, Training Loss: 0.8042599967648001\n",
      "Epoch 19, Training Loss: 0.8037646705964032\n",
      "Epoch 20, Training Loss: 0.8033066930490381\n",
      "Epoch 21, Training Loss: 0.8032645213603974\n",
      "Epoch 22, Training Loss: 0.8030889296531677\n",
      "Epoch 23, Training Loss: 0.8025127004174625\n",
      "Epoch 24, Training Loss: 0.8025859249339384\n",
      "Epoch 25, Training Loss: 0.8027353218022515\n",
      "Epoch 26, Training Loss: 0.8029314095833723\n",
      "Epoch 27, Training Loss: 0.8024943616109735\n",
      "Epoch 28, Training Loss: 0.8017522206727197\n",
      "Epoch 29, Training Loss: 0.801963813865886\n",
      "Epoch 30, Training Loss: 0.801653258379768\n",
      "Epoch 31, Training Loss: 0.8016018141718472\n",
      "Epoch 32, Training Loss: 0.8013231088133419\n",
      "Epoch 33, Training Loss: 0.8008190419393427\n",
      "Epoch 34, Training Loss: 0.8008204629140742\n",
      "Epoch 35, Training Loss: 0.8009528307353749\n",
      "Epoch 36, Training Loss: 0.8004325980298659\n",
      "Epoch 37, Training Loss: 0.8002868274380179\n",
      "Epoch 38, Training Loss: 0.8001331107756671\n",
      "Epoch 39, Training Loss: 0.8003373850093168\n",
      "Epoch 40, Training Loss: 0.799931711659712\n",
      "Epoch 41, Training Loss: 0.8001561554740457\n",
      "Epoch 42, Training Loss: 0.7995232082114501\n",
      "Epoch 43, Training Loss: 0.7994507535064922\n",
      "Epoch 44, Training Loss: 0.7990606645275565\n",
      "Epoch 45, Training Loss: 0.7992621857278487\n",
      "Epoch 46, Training Loss: 0.7987326243344475\n",
      "Epoch 47, Training Loss: 0.7986011420277989\n",
      "Epoch 48, Training Loss: 0.7984202908067142\n",
      "Epoch 49, Training Loss: 0.798672325891607\n",
      "Epoch 50, Training Loss: 0.7982592915787416\n",
      "Epoch 51, Training Loss: 0.7980768492642571\n",
      "Epoch 52, Training Loss: 0.7977425430802738\n",
      "Epoch 53, Training Loss: 0.7979780045677634\n",
      "Epoch 54, Training Loss: 0.7978595556231106\n",
      "Epoch 55, Training Loss: 0.7973761137092815\n",
      "Epoch 56, Training Loss: 0.7975012941220228\n",
      "Epoch 57, Training Loss: 0.7969887777637032\n",
      "Epoch 58, Training Loss: 0.7975033656288596\n",
      "Epoch 59, Training Loss: 0.7974178948823143\n",
      "Epoch 60, Training Loss: 0.7965107884126551\n",
      "Epoch 61, Training Loss: 0.7969935945202322\n",
      "Epoch 62, Training Loss: 0.7968538425950443\n",
      "Epoch 63, Training Loss: 0.7965439808368683\n",
      "Epoch 64, Training Loss: 0.7965315168745377\n",
      "Epoch 65, Training Loss: 0.7960795291732339\n",
      "Epoch 66, Training Loss: 0.7962118736435385\n",
      "Epoch 67, Training Loss: 0.7957937506367179\n",
      "Epoch 68, Training Loss: 0.7955978991003597\n",
      "Epoch 69, Training Loss: 0.795906150341034\n",
      "Epoch 70, Training Loss: 0.795593368025387\n",
      "Epoch 71, Training Loss: 0.7955220328359043\n",
      "Epoch 72, Training Loss: 0.7955133524361778\n",
      "Epoch 73, Training Loss: 0.795239318188499\n",
      "Epoch 74, Training Loss: 0.7952589064485887\n",
      "Epoch 75, Training Loss: 0.7952033105317284\n",
      "Epoch 76, Training Loss: 0.7949780462769901\n",
      "Epoch 77, Training Loss: 0.795041652987985\n",
      "Epoch 78, Training Loss: 0.7944070041880888\n",
      "Epoch 79, Training Loss: 0.7949818859380834\n",
      "Epoch 80, Training Loss: 0.7947781435882344\n",
      "Epoch 81, Training Loss: 0.7945131446333492\n",
      "Epoch 82, Training Loss: 0.794823417523328\n",
      "Epoch 83, Training Loss: 0.7941640582505395\n",
      "Epoch 84, Training Loss: 0.7940459138505599\n",
      "Epoch 85, Training Loss: 0.793877395251218\n",
      "Epoch 86, Training Loss: 0.793912483033012\n",
      "Epoch 87, Training Loss: 0.7939221848459804\n",
      "Epoch 88, Training Loss: 0.7936923111186308\n",
      "Epoch 89, Training Loss: 0.7940158454810872\n",
      "Epoch 90, Training Loss: 0.7937051996764015\n",
      "Epoch 91, Training Loss: 0.7933836404716267\n",
      "Epoch 92, Training Loss: 0.7933014144616969\n",
      "Epoch 93, Training Loss: 0.793508661775028\n",
      "Epoch 94, Training Loss: 0.7936079369573033\n",
      "Epoch 95, Training Loss: 0.7930807615027708\n",
      "Epoch 96, Training Loss: 0.7930005377180436\n",
      "Epoch 97, Training Loss: 0.792589610043694\n",
      "Epoch 98, Training Loss: 0.792904167175293\n",
      "Epoch 99, Training Loss: 0.7928604671534369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:55:12,419] Trial 399 finished with value: 0.6342 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7926670789017397\n",
      "Epoch 1, Training Loss: 0.8870405812824473\n",
      "Epoch 2, Training Loss: 0.8351915876304402\n",
      "Epoch 3, Training Loss: 0.8294580115290249\n",
      "Epoch 4, Training Loss: 0.8247620328734903\n",
      "Epoch 5, Training Loss: 0.8227351552598616\n",
      "Epoch 6, Training Loss: 0.8257700808609233\n",
      "Epoch 7, Training Loss: 0.8216488350138945\n",
      "Epoch 8, Training Loss: 0.8208190946719226\n",
      "Epoch 9, Training Loss: 0.8207758078855627\n",
      "Epoch 10, Training Loss: 0.8226111785804524\n",
      "Epoch 11, Training Loss: 0.8215547081302194\n",
      "Epoch 12, Training Loss: 0.8189194172270158\n",
      "Epoch 13, Training Loss: 0.8188939553849838\n",
      "Epoch 14, Training Loss: 0.818306624398512\n",
      "Epoch 15, Training Loss: 0.8200438796772677\n",
      "Epoch 16, Training Loss: 0.8169293500395383\n",
      "Epoch 17, Training Loss: 0.8183522571535672\n",
      "Epoch 18, Training Loss: 0.8173634568382712\n",
      "Epoch 19, Training Loss: 0.8172672576764051\n",
      "Epoch 20, Training Loss: 0.8184304255597732\n",
      "Epoch 21, Training Loss: 0.8192415588743547\n",
      "Epoch 22, Training Loss: 0.8173766310775982\n",
      "Epoch 23, Training Loss: 0.8167995987218969\n",
      "Epoch 24, Training Loss: 0.8163542276270249\n",
      "Epoch 25, Training Loss: 0.8168007774212781\n",
      "Epoch 26, Training Loss: 0.8182503414855283\n",
      "Epoch 27, Training Loss: 0.8156619857339298\n",
      "Epoch 28, Training Loss: 0.8163595221323126\n",
      "Epoch 29, Training Loss: 0.817308384811177\n",
      "Epoch 30, Training Loss: 0.8151621261063744\n",
      "Epoch 31, Training Loss: 0.8152068948745728\n",
      "Epoch 32, Training Loss: 0.8146889479020063\n",
      "Epoch 33, Training Loss: 0.8160207034559811\n",
      "Epoch 34, Training Loss: 0.8131780604755178\n",
      "Epoch 35, Training Loss: 0.8150770329727846\n",
      "Epoch 36, Training Loss: 0.81311367595897\n",
      "Epoch 37, Training Loss: 0.8142187938970677\n",
      "Epoch 38, Training Loss: 0.8154037904038148\n",
      "Epoch 39, Training Loss: 0.8150918317542356\n",
      "Epoch 40, Training Loss: 0.8132182924186482\n",
      "Epoch 41, Training Loss: 0.8149715063151192\n",
      "Epoch 42, Training Loss: 0.813016728513381\n",
      "Epoch 43, Training Loss: 0.8139765630750095\n",
      "Epoch 44, Training Loss: 0.8130318133270039\n",
      "Epoch 45, Training Loss: 0.8138026137211744\n",
      "Epoch 46, Training Loss: 0.8120124675947077\n",
      "Epoch 47, Training Loss: 0.8127950537905974\n",
      "Epoch 48, Training Loss: 0.8134157963360057\n",
      "Epoch 49, Training Loss: 0.8110999674656811\n",
      "Epoch 50, Training Loss: 0.8111406485473408\n",
      "Epoch 51, Training Loss: 0.81143137847676\n",
      "Epoch 52, Training Loss: 0.8124019414537094\n",
      "Epoch 53, Training Loss: 0.8094074843911564\n",
      "Epoch 54, Training Loss: 0.8125781399362227\n",
      "Epoch 55, Training Loss: 0.8137202905206119\n",
      "Epoch 56, Training Loss: 0.8090778951785144\n",
      "Epoch 57, Training Loss: 0.8097609332729788\n",
      "Epoch 58, Training Loss: 0.8093256795406342\n",
      "Epoch 59, Training Loss: 0.811227683389888\n",
      "Epoch 60, Training Loss: 0.8101971838053535\n",
      "Epoch 61, Training Loss: 0.8115032815231996\n",
      "Epoch 62, Training Loss: 0.8110881404315724\n",
      "Epoch 63, Training Loss: 0.8109164374716141\n",
      "Epoch 64, Training Loss: 0.8088062916082495\n",
      "Epoch 65, Training Loss: 0.8137574703553143\n",
      "Epoch 66, Training Loss: 0.8094287753806395\n",
      "Epoch 67, Training Loss: 0.808623934914084\n",
      "Epoch 68, Training Loss: 0.8097834195108975\n",
      "Epoch 69, Training Loss: 0.8084894792472614\n",
      "Epoch 70, Training Loss: 0.806281665984322\n",
      "Epoch 71, Training Loss: 0.8095105103885426\n",
      "Epoch 72, Training Loss: 0.8067429566383362\n",
      "Epoch 73, Training Loss: 0.8068833966816172\n",
      "Epoch 74, Training Loss: 0.8059733855023103\n",
      "Epoch 75, Training Loss: 0.8072533524036407\n",
      "Epoch 76, Training Loss: 0.8068481689340928\n",
      "Epoch 77, Training Loss: 0.8054385880161734\n",
      "Epoch 78, Training Loss: 0.8063861077673296\n",
      "Epoch 79, Training Loss: 0.8071662097818711\n",
      "Epoch 80, Training Loss: 0.8072610721167396\n",
      "Epoch 81, Training Loss: 0.8073586445696214\n",
      "Epoch 82, Training Loss: 0.8066149834324332\n",
      "Epoch 83, Training Loss: 0.8079672549752628\n",
      "Epoch 84, Training Loss: 0.8086157342265634\n",
      "Epoch 85, Training Loss: 0.8055388367877288\n",
      "Epoch 86, Training Loss: 0.8059487633144155\n",
      "Epoch 87, Training Loss: 0.8065520943613613\n",
      "Epoch 88, Training Loss: 0.8055079656488755\n",
      "Epoch 89, Training Loss: 0.8079424130215365\n",
      "Epoch 90, Training Loss: 0.8059557686132544\n",
      "Epoch 91, Training Loss: 0.8053706091992995\n",
      "Epoch 92, Training Loss: 0.8075990873224596\n",
      "Epoch 93, Training Loss: 0.8061720923816457\n",
      "Epoch 94, Training Loss: 0.8061138625705944\n",
      "Epoch 95, Training Loss: 0.8057977270378786\n",
      "Epoch 96, Training Loss: 0.8065566124635585\n",
      "Epoch 97, Training Loss: 0.8065183856908013\n",
      "Epoch 98, Training Loss: 0.8050328623547274\n",
      "Epoch 99, Training Loss: 0.8069603464883917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:57:05,554] Trial 400 finished with value: 0.6308666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.805301664015826\n",
      "Epoch 1, Training Loss: 0.8492501069728593\n",
      "Epoch 2, Training Loss: 0.8142236409330726\n",
      "Epoch 3, Training Loss: 0.8043249887631352\n",
      "Epoch 4, Training Loss: 0.8031057659844707\n",
      "Epoch 5, Training Loss: 0.8019270432622809\n",
      "Epoch 6, Training Loss: 0.7987034658740337\n",
      "Epoch 7, Training Loss: 0.7984337051111953\n",
      "Epoch 8, Training Loss: 0.7992866023142535\n",
      "Epoch 9, Training Loss: 0.798215835614312\n",
      "Epoch 10, Training Loss: 0.7999692385358022\n",
      "Epoch 11, Training Loss: 0.7973488572844886\n",
      "Epoch 12, Training Loss: 0.797361669773446\n",
      "Epoch 13, Training Loss: 0.7970485601210057\n",
      "Epoch 14, Training Loss: 0.7949349236667604\n",
      "Epoch 15, Training Loss: 0.7971757515032488\n",
      "Epoch 16, Training Loss: 0.7960315462341883\n",
      "Epoch 17, Training Loss: 0.7969290361368566\n",
      "Epoch 18, Training Loss: 0.7954834025605281\n",
      "Epoch 19, Training Loss: 0.7957582058763145\n",
      "Epoch 20, Training Loss: 0.7957859220361351\n",
      "Epoch 21, Training Loss: 0.7951521879748294\n",
      "Epoch 22, Training Loss: 0.7940192066637197\n",
      "Epoch 23, Training Loss: 0.7949884734207526\n",
      "Epoch 24, Training Loss: 0.7950291483922112\n",
      "Epoch 25, Training Loss: 0.7940756389072963\n",
      "Epoch 26, Training Loss: 0.7930577738392622\n",
      "Epoch 27, Training Loss: 0.7952785927550237\n",
      "Epoch 28, Training Loss: 0.7934272018590368\n",
      "Epoch 29, Training Loss: 0.7942663541413788\n",
      "Epoch 30, Training Loss: 0.7968142769390479\n",
      "Epoch 31, Training Loss: 0.7947296104036776\n",
      "Epoch 32, Training Loss: 0.7929210563351338\n",
      "Epoch 33, Training Loss: 0.7929333916284088\n",
      "Epoch 34, Training Loss: 0.794528481386658\n",
      "Epoch 35, Training Loss: 0.792676996288443\n",
      "Epoch 36, Training Loss: 0.7948663580686526\n",
      "Epoch 37, Training Loss: 0.793066188088037\n",
      "Epoch 38, Training Loss: 0.796533803742631\n",
      "Epoch 39, Training Loss: 0.7914065410320024\n",
      "Epoch 40, Training Loss: 0.7927292357710071\n",
      "Epoch 41, Training Loss: 0.7933100710237833\n",
      "Epoch 42, Training Loss: 0.7944530981823914\n",
      "Epoch 43, Training Loss: 0.7938866899425822\n",
      "Epoch 44, Training Loss: 0.7932999445979757\n",
      "Epoch 45, Training Loss: 0.793759778746985\n",
      "Epoch 46, Training Loss: 0.7919714513577913\n",
      "Epoch 47, Training Loss: 0.7923963641761838\n",
      "Epoch 48, Training Loss: 0.7927881002874303\n",
      "Epoch 49, Training Loss: 0.7925157679651017\n",
      "Epoch 50, Training Loss: 0.7921825181272694\n",
      "Epoch 51, Training Loss: 0.7932558433453839\n",
      "Epoch 52, Training Loss: 0.7925312761077308\n",
      "Epoch 53, Training Loss: 0.7934971846135935\n",
      "Epoch 54, Training Loss: 0.7924987748153227\n",
      "Epoch 55, Training Loss: 0.7932855376623626\n",
      "Epoch 56, Training Loss: 0.7931416603855621\n",
      "Epoch 57, Training Loss: 0.7918252434497489\n",
      "Epoch 58, Training Loss: 0.7927537634856718\n",
      "Epoch 59, Training Loss: 0.7927611783034819\n",
      "Epoch 60, Training Loss: 0.7931370494957257\n",
      "Epoch 61, Training Loss: 0.7929515279325328\n",
      "Epoch 62, Training Loss: 0.7916836744860599\n",
      "Epoch 63, Training Loss: 0.7925699482286783\n",
      "Epoch 64, Training Loss: 0.7932402157245722\n",
      "Epoch 65, Training Loss: 0.790309426838294\n",
      "Epoch 66, Training Loss: 0.7935139042094238\n",
      "Epoch 67, Training Loss: 0.7921816050110007\n",
      "Epoch 68, Training Loss: 0.7921400488767408\n",
      "Epoch 69, Training Loss: 0.7918426034145786\n",
      "Epoch 70, Training Loss: 0.7929009762025417\n",
      "Epoch 71, Training Loss: 0.7930069752205583\n",
      "Epoch 72, Training Loss: 0.7909442125406481\n",
      "Epoch 73, Training Loss: 0.7915947770713864\n",
      "Epoch 74, Training Loss: 0.792015352284998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:58:36,499] Trial 401 finished with value: 0.6314666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7918553592567157\n",
      "Epoch 1, Training Loss: 1.0223318805371908\n",
      "Epoch 2, Training Loss: 0.9554832183328786\n",
      "Epoch 3, Training Loss: 0.9395077037632017\n",
      "Epoch 4, Training Loss: 0.930028436237708\n",
      "Epoch 5, Training Loss: 0.9218545743397304\n",
      "Epoch 6, Training Loss: 0.9155235479648848\n",
      "Epoch 7, Training Loss: 0.9094221982740818\n",
      "Epoch 8, Training Loss: 0.9031331970279378\n",
      "Epoch 9, Training Loss: 0.8972212780687145\n",
      "Epoch 10, Training Loss: 0.8916649379228291\n",
      "Epoch 11, Training Loss: 0.885239355187667\n",
      "Epoch 12, Training Loss: 0.8788897150441219\n",
      "Epoch 13, Training Loss: 0.8729615997551079\n",
      "Epoch 14, Training Loss: 0.8664423472899243\n",
      "Epoch 15, Training Loss: 0.8598256515381032\n",
      "Epoch 16, Training Loss: 0.8536261713594422\n",
      "Epoch 17, Training Loss: 0.8476229523357592\n",
      "Epoch 18, Training Loss: 0.8419987012569169\n",
      "Epoch 19, Training Loss: 0.8367529083015327\n",
      "Epoch 20, Training Loss: 0.8324271226287785\n",
      "Epoch 21, Training Loss: 0.8277210526000288\n",
      "Epoch 22, Training Loss: 0.8249214706564308\n",
      "Epoch 23, Training Loss: 0.82134042485316\n",
      "Epoch 24, Training Loss: 0.8187216238867968\n",
      "Epoch 25, Training Loss: 0.816216242223754\n",
      "Epoch 26, Training Loss: 0.8140896400114648\n",
      "Epoch 27, Training Loss: 0.8127823495327082\n",
      "Epoch 28, Training Loss: 0.8108921104355862\n",
      "Epoch 29, Training Loss: 0.809876206853336\n",
      "Epoch 30, Training Loss: 0.8091411573546273\n",
      "Epoch 31, Training Loss: 0.8078763687520996\n",
      "Epoch 32, Training Loss: 0.8074366010221323\n",
      "Epoch 33, Training Loss: 0.8065491298983868\n",
      "Epoch 34, Training Loss: 0.8057833757615627\n",
      "Epoch 35, Training Loss: 0.804975466262129\n",
      "Epoch 36, Training Loss: 0.8047953879922852\n",
      "Epoch 37, Training Loss: 0.8042606514199335\n",
      "Epoch 38, Training Loss: 0.8037914801360969\n",
      "Epoch 39, Training Loss: 0.8033075980674055\n",
      "Epoch 40, Training Loss: 0.8028092424224194\n",
      "Epoch 41, Training Loss: 0.8025539120337121\n",
      "Epoch 42, Training Loss: 0.8020066160904734\n",
      "Epoch 43, Training Loss: 0.8025953567117676\n",
      "Epoch 44, Training Loss: 0.801823335841186\n",
      "Epoch 45, Training Loss: 0.8024335808323738\n",
      "Epoch 46, Training Loss: 0.8017354668531202\n",
      "Epoch 47, Training Loss: 0.8014192960316078\n",
      "Epoch 48, Training Loss: 0.8011924735585549\n",
      "Epoch 49, Training Loss: 0.8007150203661811\n",
      "Epoch 50, Training Loss: 0.8005966805873956\n",
      "Epoch 51, Training Loss: 0.8007007046749718\n",
      "Epoch 52, Training Loss: 0.8002038045933372\n",
      "Epoch 53, Training Loss: 0.8005711896078928\n",
      "Epoch 54, Training Loss: 0.8002930965638698\n",
      "Epoch 55, Training Loss: 0.8001085425678053\n",
      "Epoch 56, Training Loss: 0.8002984884089993\n",
      "Epoch 57, Training Loss: 0.7998083009755701\n",
      "Epoch 58, Training Loss: 0.7996849122800325\n",
      "Epoch 59, Training Loss: 0.7992387454760702\n",
      "Epoch 60, Training Loss: 0.7997718530489986\n",
      "Epoch 61, Training Loss: 0.7997243253808273\n",
      "Epoch 62, Training Loss: 0.7992729658471014\n",
      "Epoch 63, Training Loss: 0.7990675187648687\n",
      "Epoch 64, Training Loss: 0.7990329258423999\n",
      "Epoch 65, Training Loss: 0.7993226875039868\n",
      "Epoch 66, Training Loss: 0.7984604750360761\n",
      "Epoch 67, Training Loss: 0.79844879422869\n",
      "Epoch 68, Training Loss: 0.7986804764969905\n",
      "Epoch 69, Training Loss: 0.7989845688181713\n",
      "Epoch 70, Training Loss: 0.7990455939357443\n",
      "Epoch 71, Training Loss: 0.7981749237031865\n",
      "Epoch 72, Training Loss: 0.7985570749842135\n",
      "Epoch 73, Training Loss: 0.7986515241458003\n",
      "Epoch 74, Training Loss: 0.797841530321236\n",
      "Epoch 75, Training Loss: 0.7983014444659527\n",
      "Epoch 76, Training Loss: 0.798156175667182\n",
      "Epoch 77, Training Loss: 0.7976436526703655\n",
      "Epoch 78, Training Loss: 0.7978503771294329\n",
      "Epoch 79, Training Loss: 0.7975752774934123\n",
      "Epoch 80, Training Loss: 0.7977233057631585\n",
      "Epoch 81, Training Loss: 0.7978203751090774\n",
      "Epoch 82, Training Loss: 0.7974210838626202\n",
      "Epoch 83, Training Loss: 0.7978514233029874\n",
      "Epoch 84, Training Loss: 0.7973541688202019\n",
      "Epoch 85, Training Loss: 0.7976350864969698\n",
      "Epoch 86, Training Loss: 0.7978088189784746\n",
      "Epoch 87, Training Loss: 0.7971176647602167\n",
      "Epoch 88, Training Loss: 0.7979353266551082\n",
      "Epoch 89, Training Loss: 0.7977170995303563\n",
      "Epoch 90, Training Loss: 0.7979530064683211\n",
      "Epoch 91, Training Loss: 0.7978144971947921\n",
      "Epoch 92, Training Loss: 0.7976551635821063\n",
      "Epoch 93, Training Loss: 0.7964535504803621\n",
      "Epoch 94, Training Loss: 0.7974618332726615\n",
      "Epoch 95, Training Loss: 0.7972379457681699\n",
      "Epoch 96, Training Loss: 0.7970518275311119\n",
      "Epoch 97, Training Loss: 0.7969151194830586\n",
      "Epoch 98, Training Loss: 0.7963698445406175\n",
      "Epoch 99, Training Loss: 0.7966278881058657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 19:59:59,718] Trial 402 finished with value: 0.6339333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7971361162070941\n",
      "Epoch 1, Training Loss: 0.9724659721654161\n",
      "Epoch 2, Training Loss: 0.9421810030937194\n",
      "Epoch 3, Training Loss: 0.9280497230981526\n",
      "Epoch 4, Training Loss: 0.9131047541037538\n",
      "Epoch 5, Training Loss: 0.8978465549927905\n",
      "Epoch 6, Training Loss: 0.8818672566485585\n",
      "Epoch 7, Training Loss: 0.8676558655007441\n",
      "Epoch 8, Training Loss: 0.8550813567369504\n",
      "Epoch 9, Training Loss: 0.8443069451733639\n",
      "Epoch 10, Training Loss: 0.8358002275452578\n",
      "Epoch 11, Training Loss: 0.8291399443956246\n",
      "Epoch 12, Training Loss: 0.824055608502008\n",
      "Epoch 13, Training Loss: 0.8208516270594489\n",
      "Epoch 14, Training Loss: 0.8180479106150176\n",
      "Epoch 15, Training Loss: 0.8162196398677682\n",
      "Epoch 16, Training Loss: 0.8147662052534577\n",
      "Epoch 17, Training Loss: 0.8134934735477419\n",
      "Epoch 18, Training Loss: 0.8126033446842567\n",
      "Epoch 19, Training Loss: 0.8119680626948077\n",
      "Epoch 20, Training Loss: 0.8117671988064186\n",
      "Epoch 21, Training Loss: 0.8108712626579112\n",
      "Epoch 22, Training Loss: 0.8103937395533225\n",
      "Epoch 23, Training Loss: 0.810052658382215\n",
      "Epoch 24, Training Loss: 0.8100058327940174\n",
      "Epoch 25, Training Loss: 0.8094134083367828\n",
      "Epoch 26, Training Loss: 0.8087628983017198\n",
      "Epoch 27, Training Loss: 0.8091180087928486\n",
      "Epoch 28, Training Loss: 0.8088754670064252\n",
      "Epoch 29, Training Loss: 0.8083326485820282\n",
      "Epoch 30, Training Loss: 0.8081637790328577\n",
      "Epoch 31, Training Loss: 0.8082716361920637\n",
      "Epoch 32, Training Loss: 0.8079929931719501\n",
      "Epoch 33, Training Loss: 0.8065277312034951\n",
      "Epoch 34, Training Loss: 0.807200048561383\n",
      "Epoch 35, Training Loss: 0.8062175759695527\n",
      "Epoch 36, Training Loss: 0.8060944142198204\n",
      "Epoch 37, Training Loss: 0.806391527867855\n",
      "Epoch 38, Training Loss: 0.8057634160034639\n",
      "Epoch 39, Training Loss: 0.8053759349019904\n",
      "Epoch 40, Training Loss: 0.8060549706444704\n",
      "Epoch 41, Training Loss: 0.8046002135240942\n",
      "Epoch 42, Training Loss: 0.8048743701518927\n",
      "Epoch 43, Training Loss: 0.8044800367570462\n",
      "Epoch 44, Training Loss: 0.8044862607367953\n",
      "Epoch 45, Training Loss: 0.8044828098519404\n",
      "Epoch 46, Training Loss: 0.8043997949227355\n",
      "Epoch 47, Training Loss: 0.8033820695446846\n",
      "Epoch 48, Training Loss: 0.8038161170213742\n",
      "Epoch 49, Training Loss: 0.8029930978789366\n",
      "Epoch 50, Training Loss: 0.8038648054115755\n",
      "Epoch 51, Training Loss: 0.8036253191474685\n",
      "Epoch 52, Training Loss: 0.8026718972320843\n",
      "Epoch 53, Training Loss: 0.8028272540945756\n",
      "Epoch 54, Training Loss: 0.8025890844208854\n",
      "Epoch 55, Training Loss: 0.8020266622528994\n",
      "Epoch 56, Training Loss: 0.8019997001590585\n",
      "Epoch 57, Training Loss: 0.8012647842554221\n",
      "Epoch 58, Training Loss: 0.8018115902305546\n",
      "Epoch 59, Training Loss: 0.8013202433299301\n",
      "Epoch 60, Training Loss: 0.8011024958208988\n",
      "Epoch 61, Training Loss: 0.8014066460437345\n",
      "Epoch 62, Training Loss: 0.801464767563612\n",
      "Epoch 63, Training Loss: 0.8006767266675046\n",
      "Epoch 64, Training Loss: 0.8005525349674368\n",
      "Epoch 65, Training Loss: 0.8007721712714747\n",
      "Epoch 66, Training Loss: 0.8004520807051121\n",
      "Epoch 67, Training Loss: 0.8006621214680205\n",
      "Epoch 68, Training Loss: 0.8005123573138302\n",
      "Epoch 69, Training Loss: 0.8004755868947595\n",
      "Epoch 70, Training Loss: 0.8004496857635957\n",
      "Epoch 71, Training Loss: 0.8000397041327971\n",
      "Epoch 72, Training Loss: 0.8001118461888536\n",
      "Epoch 73, Training Loss: 0.7999226668723544\n",
      "Epoch 74, Training Loss: 0.7997233008979855\n",
      "Epoch 75, Training Loss: 0.7996808634664779\n",
      "Epoch 76, Training Loss: 0.7992004541526163\n",
      "Epoch 77, Training Loss: 0.7995295192065992\n",
      "Epoch 78, Training Loss: 0.7988269310248526\n",
      "Epoch 79, Training Loss: 0.7990462212634266\n",
      "Epoch 80, Training Loss: 0.7992645157010931\n",
      "Epoch 81, Training Loss: 0.798940062074733\n",
      "Epoch 82, Training Loss: 0.7988813103589797\n",
      "Epoch 83, Training Loss: 0.7989100880192634\n",
      "Epoch 84, Training Loss: 0.7985552597762947\n",
      "Epoch 85, Training Loss: 0.7985080211682427\n",
      "Epoch 86, Training Loss: 0.7990629664041046\n",
      "Epoch 87, Training Loss: 0.7982917718869403\n",
      "Epoch 88, Training Loss: 0.7986490259493204\n",
      "Epoch 89, Training Loss: 0.7983636207150338\n",
      "Epoch 90, Training Loss: 0.7983743042874157\n",
      "Epoch 91, Training Loss: 0.7981412591790794\n",
      "Epoch 92, Training Loss: 0.7986774007180579\n",
      "Epoch 93, Training Loss: 0.7981140705875884\n",
      "Epoch 94, Training Loss: 0.7984002665469521\n",
      "Epoch 95, Training Loss: 0.7978537758042041\n",
      "Epoch 96, Training Loss: 0.7981007573299839\n",
      "Epoch 97, Training Loss: 0.7982136438663741\n",
      "Epoch 98, Training Loss: 0.798241170546166\n",
      "Epoch 99, Training Loss: 0.7985365209722878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:01:22,870] Trial 403 finished with value: 0.6317333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7981503241940548\n",
      "Epoch 1, Training Loss: 0.9443481522395198\n",
      "Epoch 2, Training Loss: 0.8915490444441487\n",
      "Epoch 3, Training Loss: 0.8495375652062266\n",
      "Epoch 4, Training Loss: 0.8212358161022789\n",
      "Epoch 5, Training Loss: 0.810120575678976\n",
      "Epoch 6, Training Loss: 0.8056920323156773\n",
      "Epoch 7, Training Loss: 0.8034774693331324\n",
      "Epoch 8, Training Loss: 0.8027883451684077\n",
      "Epoch 9, Training Loss: 0.8014239901886847\n",
      "Epoch 10, Training Loss: 0.8007221293628664\n",
      "Epoch 11, Training Loss: 0.8006331500254179\n",
      "Epoch 12, Training Loss: 0.8002330564914789\n",
      "Epoch 13, Training Loss: 0.7994167246316609\n",
      "Epoch 14, Training Loss: 0.8000678902281855\n",
      "Epoch 15, Training Loss: 0.799081383880816\n",
      "Epoch 16, Training Loss: 0.7989366714219401\n",
      "Epoch 17, Training Loss: 0.7989100969823679\n",
      "Epoch 18, Training Loss: 0.7983182086532278\n",
      "Epoch 19, Training Loss: 0.7978544074789922\n",
      "Epoch 20, Training Loss: 0.7987250212439917\n",
      "Epoch 21, Training Loss: 0.7982139233359717\n",
      "Epoch 22, Training Loss: 0.7988215923309326\n",
      "Epoch 23, Training Loss: 0.7987683835782503\n",
      "Epoch 24, Training Loss: 0.7978501143312096\n",
      "Epoch 25, Training Loss: 0.7983462326508716\n",
      "Epoch 26, Training Loss: 0.7979788341916594\n",
      "Epoch 27, Training Loss: 0.797588562786131\n",
      "Epoch 28, Training Loss: 0.7984712737843507\n",
      "Epoch 29, Training Loss: 0.7976796154689072\n",
      "Epoch 30, Training Loss: 0.7973139488607421\n",
      "Epoch 31, Training Loss: 0.7978018646849725\n",
      "Epoch 32, Training Loss: 0.797697157071049\n",
      "Epoch 33, Training Loss: 0.7971507818179023\n",
      "Epoch 34, Training Loss: 0.7970330876515324\n",
      "Epoch 35, Training Loss: 0.7972628548629301\n",
      "Epoch 36, Training Loss: 0.7970688559058914\n",
      "Epoch 37, Training Loss: 0.796642587866102\n",
      "Epoch 38, Training Loss: 0.7964279531536246\n",
      "Epoch 39, Training Loss: 0.7963777458757386\n",
      "Epoch 40, Training Loss: 0.796430942707492\n",
      "Epoch 41, Training Loss: 0.7961319840940317\n",
      "Epoch 42, Training Loss: 0.7967012054041812\n",
      "Epoch 43, Training Loss: 0.7972018561865154\n",
      "Epoch 44, Training Loss: 0.7960501533701904\n",
      "Epoch 45, Training Loss: 0.7958387413419279\n",
      "Epoch 46, Training Loss: 0.7957290295371435\n",
      "Epoch 47, Training Loss: 0.7955727026874858\n",
      "Epoch 48, Training Loss: 0.7957598786605032\n",
      "Epoch 49, Training Loss: 0.7952247696711605\n",
      "Epoch 50, Training Loss: 0.7944973514492351\n",
      "Epoch 51, Training Loss: 0.7943978464693056\n",
      "Epoch 52, Training Loss: 0.7938118110025736\n",
      "Epoch 53, Training Loss: 0.7933746285904619\n",
      "Epoch 54, Training Loss: 0.7935056465012686\n",
      "Epoch 55, Training Loss: 0.7933882313563412\n",
      "Epoch 56, Training Loss: 0.7928812617646124\n",
      "Epoch 57, Training Loss: 0.7928922122582457\n",
      "Epoch 58, Training Loss: 0.793017552042366\n",
      "Epoch 59, Training Loss: 0.7923105129621979\n",
      "Epoch 60, Training Loss: 0.7916398354939052\n",
      "Epoch 61, Training Loss: 0.7913426481691518\n",
      "Epoch 62, Training Loss: 0.7915281599625609\n",
      "Epoch 63, Training Loss: 0.790885435459309\n",
      "Epoch 64, Training Loss: 0.7915409524637954\n",
      "Epoch 65, Training Loss: 0.7913192832380309\n",
      "Epoch 66, Training Loss: 0.7906477927265311\n",
      "Epoch 67, Training Loss: 0.7900893964713678\n",
      "Epoch 68, Training Loss: 0.7909304187710123\n",
      "Epoch 69, Training Loss: 0.7905729772453022\n",
      "Epoch 70, Training Loss: 0.7900170271557972\n",
      "Epoch 71, Training Loss: 0.7904960588405007\n",
      "Epoch 72, Training Loss: 0.7905661521101357\n",
      "Epoch 73, Training Loss: 0.7903023955517245\n",
      "Epoch 74, Training Loss: 0.7904879078829199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:02:38,065] Trial 404 finished with value: 0.639 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7898486125738101\n",
      "Epoch 1, Training Loss: 0.8946456395878511\n",
      "Epoch 2, Training Loss: 0.8193709673601038\n",
      "Epoch 3, Training Loss: 0.8140690903102651\n",
      "Epoch 4, Training Loss: 0.8112802236220416\n",
      "Epoch 5, Training Loss: 0.810275934864493\n",
      "Epoch 6, Training Loss: 0.809591409669203\n",
      "Epoch 7, Training Loss: 0.8074412272256963\n",
      "Epoch 8, Training Loss: 0.8074766024421243\n",
      "Epoch 9, Training Loss: 0.8053534962850458\n",
      "Epoch 10, Training Loss: 0.8058106894352857\n",
      "Epoch 11, Training Loss: 0.8054632248597987\n",
      "Epoch 12, Training Loss: 0.8049939915011911\n",
      "Epoch 13, Training Loss: 0.8055124775101157\n",
      "Epoch 14, Training Loss: 0.8037668981972863\n",
      "Epoch 15, Training Loss: 0.8041472765277414\n",
      "Epoch 16, Training Loss: 0.8041094259654774\n",
      "Epoch 17, Training Loss: 0.8043287099108977\n",
      "Epoch 18, Training Loss: 0.8035891392651726\n",
      "Epoch 19, Training Loss: 0.8024376072603113\n",
      "Epoch 20, Training Loss: 0.8024338578476625\n",
      "Epoch 21, Training Loss: 0.8033229931663064\n",
      "Epoch 22, Training Loss: 0.8035788390916937\n",
      "Epoch 23, Training Loss: 0.8025092416651108\n",
      "Epoch 24, Training Loss: 0.8025447677864748\n",
      "Epoch 25, Training Loss: 0.8028891658081728\n",
      "Epoch 26, Training Loss: 0.8012116681828219\n",
      "Epoch 27, Training Loss: 0.8014640663651859\n",
      "Epoch 28, Training Loss: 0.801093892139547\n",
      "Epoch 29, Training Loss: 0.8019225894703584\n",
      "Epoch 30, Training Loss: 0.8015819218579461\n",
      "Epoch 31, Training Loss: 0.8006470842221204\n",
      "Epoch 32, Training Loss: 0.800343131458058\n",
      "Epoch 33, Training Loss: 0.8000253246111029\n",
      "Epoch 34, Training Loss: 0.7999893886201522\n",
      "Epoch 35, Training Loss: 0.7999795245423037\n",
      "Epoch 36, Training Loss: 0.7993274532346164\n",
      "Epoch 37, Training Loss: 0.8000190980294172\n",
      "Epoch 38, Training Loss: 0.8001835064327015\n",
      "Epoch 39, Training Loss: 0.7997247045881608\n",
      "Epoch 40, Training Loss: 0.7994842621859383\n",
      "Epoch 41, Training Loss: 0.7993211715361651\n",
      "Epoch 42, Training Loss: 0.7985397649512571\n",
      "Epoch 43, Training Loss: 0.7986182127279394\n",
      "Epoch 44, Training Loss: 0.7983055694664226\n",
      "Epoch 45, Training Loss: 0.7985707552292768\n",
      "Epoch 46, Training Loss: 0.7989596591977512\n",
      "Epoch 47, Training Loss: 0.7980205574456383\n",
      "Epoch 48, Training Loss: 0.7985752739625819\n",
      "Epoch 49, Training Loss: 0.7980062889351565\n",
      "Epoch 50, Training Loss: 0.7978898685118732\n",
      "Epoch 51, Training Loss: 0.7972793151350582\n",
      "Epoch 52, Training Loss: 0.7974832406464745\n",
      "Epoch 53, Training Loss: 0.7972220164186814\n",
      "Epoch 54, Training Loss: 0.7966182809717515\n",
      "Epoch 55, Training Loss: 0.7976817338606891\n",
      "Epoch 56, Training Loss: 0.7968233026476468\n",
      "Epoch 57, Training Loss: 0.7966726235081167\n",
      "Epoch 58, Training Loss: 0.7970057960818796\n",
      "Epoch 59, Training Loss: 0.7963478482470793\n",
      "Epoch 60, Training Loss: 0.7957747513406417\n",
      "Epoch 61, Training Loss: 0.7965777231665219\n",
      "Epoch 62, Training Loss: 0.7964248570975135\n",
      "Epoch 63, Training Loss: 0.7965840428716996\n",
      "Epoch 64, Training Loss: 0.7955864258373485\n",
      "Epoch 65, Training Loss: 0.7954568102079279\n",
      "Epoch 66, Training Loss: 0.7951997731713688\n",
      "Epoch 67, Training Loss: 0.7955929249875686\n",
      "Epoch 68, Training Loss: 0.7956135169197531\n",
      "Epoch 69, Training Loss: 0.795025247966542\n",
      "Epoch 70, Training Loss: 0.7952918828936184\n",
      "Epoch 71, Training Loss: 0.7941201057854821\n",
      "Epoch 72, Training Loss: 0.7946708294924568\n",
      "Epoch 73, Training Loss: 0.794619533735163\n",
      "Epoch 74, Training Loss: 0.7942804894026588\n",
      "Epoch 75, Training Loss: 0.7942012376645032\n",
      "Epoch 76, Training Loss: 0.794034343326793\n",
      "Epoch 77, Training Loss: 0.7934051186898176\n",
      "Epoch 78, Training Loss: 0.7932994247885311\n",
      "Epoch 79, Training Loss: 0.7936485762455884\n",
      "Epoch 80, Training Loss: 0.7942089758901035\n",
      "Epoch 81, Training Loss: 0.7934773411470301\n",
      "Epoch 82, Training Loss: 0.7934191330741434\n",
      "Epoch 83, Training Loss: 0.7937478521992178\n",
      "Epoch 84, Training Loss: 0.7931056984031901\n",
      "Epoch 85, Training Loss: 0.792890084771549\n",
      "Epoch 86, Training Loss: 0.7932566971638624\n",
      "Epoch 87, Training Loss: 0.7924386911532458\n",
      "Epoch 88, Training Loss: 0.7931628019669477\n",
      "Epoch 89, Training Loss: 0.792023040266598\n",
      "Epoch 90, Training Loss: 0.7928234910964966\n",
      "Epoch 91, Training Loss: 0.7921021288983963\n",
      "Epoch 92, Training Loss: 0.7929388215261347\n",
      "Epoch 93, Training Loss: 0.7925736702890958\n",
      "Epoch 94, Training Loss: 0.7927065067431506\n",
      "Epoch 95, Training Loss: 0.7920375915134654\n",
      "Epoch 96, Training Loss: 0.7919685351147371\n",
      "Epoch 97, Training Loss: 0.7922936426892\n",
      "Epoch 98, Training Loss: 0.7916870036545922\n",
      "Epoch 99, Training Loss: 0.7924149729223813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:04:36,660] Trial 405 finished with value: 0.633 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7922042002397425\n",
      "Epoch 1, Training Loss: 1.0513321898264043\n",
      "Epoch 2, Training Loss: 0.9571456113282372\n",
      "Epoch 3, Training Loss: 0.9280054206006667\n",
      "Epoch 4, Training Loss: 0.9169723267414991\n",
      "Epoch 5, Training Loss: 0.9070407828863929\n",
      "Epoch 6, Training Loss: 0.8958525548261754\n",
      "Epoch 7, Training Loss: 0.8817886024362901\n",
      "Epoch 8, Training Loss: 0.8644738158057718\n",
      "Epoch 9, Training Loss: 0.8457052225225112\n",
      "Epoch 10, Training Loss: 0.8294329054215375\n",
      "Epoch 11, Training Loss: 0.8183745750258951\n",
      "Epoch 12, Training Loss: 0.8118324528020971\n",
      "Epoch 13, Training Loss: 0.8083394483257742\n",
      "Epoch 14, Training Loss: 0.8062667737988864\n",
      "Epoch 15, Training Loss: 0.8050391064671909\n",
      "Epoch 16, Training Loss: 0.803929980432286\n",
      "Epoch 17, Training Loss: 0.8031488855446086\n",
      "Epoch 18, Training Loss: 0.8022811679980334\n",
      "Epoch 19, Training Loss: 0.8016758639672223\n",
      "Epoch 20, Training Loss: 0.8010716589058147\n",
      "Epoch 21, Training Loss: 0.800719426169115\n",
      "Epoch 22, Training Loss: 0.8002119443697088\n",
      "Epoch 23, Training Loss: 0.7998404274267309\n",
      "Epoch 24, Training Loss: 0.7994322323799133\n",
      "Epoch 25, Training Loss: 0.7991198452079997\n",
      "Epoch 26, Training Loss: 0.798893756796332\n",
      "Epoch 27, Training Loss: 0.7984834708886988\n",
      "Epoch 28, Training Loss: 0.7981043244109435\n",
      "Epoch 29, Training Loss: 0.7980150476623984\n",
      "Epoch 30, Training Loss: 0.7975767994628233\n",
      "Epoch 31, Training Loss: 0.7972855921352611\n",
      "Epoch 32, Training Loss: 0.7967989944710451\n",
      "Epoch 33, Training Loss: 0.796595508070553\n",
      "Epoch 34, Training Loss: 0.7961648837959066\n",
      "Epoch 35, Training Loss: 0.7957830429077148\n",
      "Epoch 36, Training Loss: 0.7955723860684563\n",
      "Epoch 37, Training Loss: 0.795317078408073\n",
      "Epoch 38, Training Loss: 0.7951465439095217\n",
      "Epoch 39, Training Loss: 0.7948404805800494\n",
      "Epoch 40, Training Loss: 0.7944433123223922\n",
      "Epoch 41, Training Loss: 0.7940818836408503\n",
      "Epoch 42, Training Loss: 0.7939009229575886\n",
      "Epoch 43, Training Loss: 0.7937565178730909\n",
      "Epoch 44, Training Loss: 0.7933895874023438\n",
      "Epoch 45, Training Loss: 0.793177716030794\n",
      "Epoch 46, Training Loss: 0.7930062469314126\n",
      "Epoch 47, Training Loss: 0.7927746478249045\n",
      "Epoch 48, Training Loss: 0.7925957176965825\n",
      "Epoch 49, Training Loss: 0.7922196365805233\n",
      "Epoch 50, Training Loss: 0.7920759001900168\n",
      "Epoch 51, Training Loss: 0.7917886607085958\n",
      "Epoch 52, Training Loss: 0.7914335419149959\n",
      "Epoch 53, Training Loss: 0.7914760151330162\n",
      "Epoch 54, Training Loss: 0.7911746658998378\n",
      "Epoch 55, Training Loss: 0.7908680530155406\n",
      "Epoch 56, Training Loss: 0.7908064396241132\n",
      "Epoch 57, Training Loss: 0.7904298940125634\n",
      "Epoch 58, Training Loss: 0.7903801437686472\n",
      "Epoch 59, Training Loss: 0.7902614979884204\n",
      "Epoch 60, Training Loss: 0.7900120521994198\n",
      "Epoch 61, Training Loss: 0.7897833165000466\n",
      "Epoch 62, Training Loss: 0.7895720204185037\n",
      "Epoch 63, Training Loss: 0.7895291592093076\n",
      "Epoch 64, Training Loss: 0.7892811561332029\n",
      "Epoch 65, Training Loss: 0.7890182190782884\n",
      "Epoch 66, Training Loss: 0.7889964998469633\n",
      "Epoch 67, Training Loss: 0.7888286122855018\n",
      "Epoch 68, Training Loss: 0.78858268197845\n",
      "Epoch 69, Training Loss: 0.7886298921528985\n",
      "Epoch 70, Training Loss: 0.7884647364476148\n",
      "Epoch 71, Training Loss: 0.7884076400364146\n",
      "Epoch 72, Training Loss: 0.7881803801480461\n",
      "Epoch 73, Training Loss: 0.7880485477167017\n",
      "Epoch 74, Training Loss: 0.7879551314606386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:05:56,916] Trial 406 finished with value: 0.6392 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7877680834601907\n",
      "Epoch 1, Training Loss: 0.8735373698963839\n",
      "Epoch 2, Training Loss: 0.8153474096690907\n",
      "Epoch 3, Training Loss: 0.8110011970295625\n",
      "Epoch 4, Training Loss: 0.8079664375501521\n",
      "Epoch 5, Training Loss: 0.8055967394744649\n",
      "Epoch 6, Training Loss: 0.8038131762953366\n",
      "Epoch 7, Training Loss: 0.8026415376803454\n",
      "Epoch 8, Training Loss: 0.8018890569490545\n",
      "Epoch 9, Training Loss: 0.801333297070335\n",
      "Epoch 10, Training Loss: 0.8012117707028108\n",
      "Epoch 11, Training Loss: 0.8005726491703706\n",
      "Epoch 12, Training Loss: 0.8002046578070696\n",
      "Epoch 13, Training Loss: 0.7999055786693797\n",
      "Epoch 14, Training Loss: 0.7996423369996688\n",
      "Epoch 15, Training Loss: 0.7985671885574566\n",
      "Epoch 16, Training Loss: 0.7983187339586371\n",
      "Epoch 17, Training Loss: 0.7965557765960694\n",
      "Epoch 18, Training Loss: 0.7959814869656282\n",
      "Epoch 19, Training Loss: 0.7952274351260241\n",
      "Epoch 20, Training Loss: 0.7943068114448996\n",
      "Epoch 21, Training Loss: 0.7932754820935867\n",
      "Epoch 22, Training Loss: 0.7927636502770816\n",
      "Epoch 23, Training Loss: 0.7914572809023016\n",
      "Epoch 24, Training Loss: 0.7910462638209848\n",
      "Epoch 25, Training Loss: 0.7899851032565621\n",
      "Epoch 26, Training Loss: 0.7895678620478687\n",
      "Epoch 27, Training Loss: 0.7882407758516424\n",
      "Epoch 28, Training Loss: 0.7871915085175458\n",
      "Epoch 29, Training Loss: 0.787764000752393\n",
      "Epoch 30, Training Loss: 0.7867554995592903\n",
      "Epoch 31, Training Loss: 0.7865170171681573\n",
      "Epoch 32, Training Loss: 0.7863631190973169\n",
      "Epoch 33, Training Loss: 0.7860129971363965\n",
      "Epoch 34, Training Loss: 0.7854696096392239\n",
      "Epoch 35, Training Loss: 0.785071386309231\n",
      "Epoch 36, Training Loss: 0.7850615417957306\n",
      "Epoch 37, Training Loss: 0.7851243559051962\n",
      "Epoch 38, Training Loss: 0.7844204331846798\n",
      "Epoch 39, Training Loss: 0.7842888634345111\n",
      "Epoch 40, Training Loss: 0.7842036355944241\n",
      "Epoch 41, Training Loss: 0.7844462488679325\n",
      "Epoch 42, Training Loss: 0.7835879482942469\n",
      "Epoch 43, Training Loss: 0.7840853278075948\n",
      "Epoch 44, Training Loss: 0.7839270688505734\n",
      "Epoch 45, Training Loss: 0.783616881090052\n",
      "Epoch 46, Training Loss: 0.7835602404790766\n",
      "Epoch 47, Training Loss: 0.7836281358494478\n",
      "Epoch 48, Training Loss: 0.783632454030654\n",
      "Epoch 49, Training Loss: 0.7833253863979789\n",
      "Epoch 50, Training Loss: 0.7836201364853803\n",
      "Epoch 51, Training Loss: 0.7834250902428347\n",
      "Epoch 52, Training Loss: 0.7832259443226982\n",
      "Epoch 53, Training Loss: 0.7834244815041037\n",
      "Epoch 54, Training Loss: 0.7831532615773819\n",
      "Epoch 55, Training Loss: 0.7829688183700337\n",
      "Epoch 56, Training Loss: 0.7827885405456319\n",
      "Epoch 57, Training Loss: 0.782398140781066\n",
      "Epoch 58, Training Loss: 0.7827970513876746\n",
      "Epoch 59, Training Loss: 0.782601369899862\n",
      "Epoch 60, Training Loss: 0.782370805459864\n",
      "Epoch 61, Training Loss: 0.7826262575037339\n",
      "Epoch 62, Training Loss: 0.7825552795213812\n",
      "Epoch 63, Training Loss: 0.7829387064541088\n",
      "Epoch 64, Training Loss: 0.7818394057890948\n",
      "Epoch 65, Training Loss: 0.7821799471097834\n",
      "Epoch 66, Training Loss: 0.7820514413188485\n",
      "Epoch 67, Training Loss: 0.7819372874848983\n",
      "Epoch 68, Training Loss: 0.7818594839292414\n",
      "Epoch 69, Training Loss: 0.7820733202204985\n",
      "Epoch 70, Training Loss: 0.7824463529446546\n",
      "Epoch 71, Training Loss: 0.7820540457613328\n",
      "Epoch 72, Training Loss: 0.7820987079424017\n",
      "Epoch 73, Training Loss: 0.7820714786473443\n",
      "Epoch 74, Training Loss: 0.7822601698426639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:07:40,037] Trial 407 finished with value: 0.6412666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7817874737346874\n",
      "Epoch 1, Training Loss: 1.0430498597902411\n",
      "Epoch 2, Training Loss: 0.9899574379359974\n",
      "Epoch 3, Training Loss: 0.9672537772094503\n",
      "Epoch 4, Training Loss: 0.9597514322926016\n",
      "Epoch 5, Training Loss: 0.9563676482789657\n",
      "Epoch 6, Training Loss: 0.9538654852614683\n",
      "Epoch 7, Training Loss: 0.9514397607130163\n",
      "Epoch 8, Training Loss: 0.9490297884099623\n",
      "Epoch 9, Training Loss: 0.9462943997803857\n",
      "Epoch 10, Training Loss: 0.943638651230756\n",
      "Epoch 11, Training Loss: 0.9407075423352859\n",
      "Epoch 12, Training Loss: 0.9376893642369438\n",
      "Epoch 13, Training Loss: 0.9343785746658549\n",
      "Epoch 14, Training Loss: 0.9310025190605836\n",
      "Epoch 15, Training Loss: 0.9272606436645283\n",
      "Epoch 16, Training Loss: 0.9233252477645874\n",
      "Epoch 17, Training Loss: 0.9191849452607772\n",
      "Epoch 18, Training Loss: 0.9148865939589108\n",
      "Epoch 19, Training Loss: 0.9103403574578902\n",
      "Epoch 20, Training Loss: 0.9057246722894556\n",
      "Epoch 21, Training Loss: 0.9009766321322498\n",
      "Epoch 22, Training Loss: 0.8959445848885704\n",
      "Epoch 23, Training Loss: 0.8909402569602517\n",
      "Epoch 24, Training Loss: 0.8859545303793515\n",
      "Epoch 25, Training Loss: 0.8808679917980643\n",
      "Epoch 26, Training Loss: 0.8758637206694659\n",
      "Epoch 27, Training Loss: 0.8710162948159611\n",
      "Epoch 28, Training Loss: 0.8662845678189222\n",
      "Epoch 29, Training Loss: 0.861718781485277\n",
      "Epoch 30, Training Loss: 0.8573254421178033\n",
      "Epoch 31, Training Loss: 0.8530907907205469\n",
      "Epoch 32, Training Loss: 0.8490368115901947\n",
      "Epoch 33, Training Loss: 0.8453712762804593\n",
      "Epoch 34, Training Loss: 0.8419396122764139\n",
      "Epoch 35, Training Loss: 0.8387849660480724\n",
      "Epoch 36, Training Loss: 0.8358252700637369\n",
      "Epoch 37, Training Loss: 0.8330976750570185\n",
      "Epoch 38, Training Loss: 0.8307083648092607\n",
      "Epoch 39, Training Loss: 0.8284214366183561\n",
      "Epoch 40, Training Loss: 0.8264312649474425\n",
      "Epoch 41, Training Loss: 0.824562606531031\n",
      "Epoch 42, Training Loss: 0.8229763070274801\n",
      "Epoch 43, Training Loss: 0.8214834793174968\n",
      "Epoch 44, Training Loss: 0.820263598946964\n",
      "Epoch 45, Training Loss: 0.8190405003463521\n",
      "Epoch 46, Training Loss: 0.8180047939805424\n",
      "Epoch 47, Training Loss: 0.8169860395964454\n",
      "Epoch 48, Training Loss: 0.81625360047116\n",
      "Epoch 49, Training Loss: 0.8154923931290121\n",
      "Epoch 50, Training Loss: 0.8149200539027943\n",
      "Epoch 51, Training Loss: 0.814213245896732\n",
      "Epoch 52, Training Loss: 0.8136261983478771\n",
      "Epoch 53, Training Loss: 0.8132467504809885\n",
      "Epoch 54, Training Loss: 0.8129569201609668\n",
      "Epoch 55, Training Loss: 0.8124139584513271\n",
      "Epoch 56, Training Loss: 0.8121108505305122\n",
      "Epoch 57, Training Loss: 0.8119296990422642\n",
      "Epoch 58, Training Loss: 0.8114146601452547\n",
      "Epoch 59, Training Loss: 0.8112288843182957\n",
      "Epoch 60, Training Loss: 0.8109963872853447\n",
      "Epoch 61, Training Loss: 0.8108840830185834\n",
      "Epoch 62, Training Loss: 0.8106118191691006\n",
      "Epoch 63, Training Loss: 0.8105116051084855\n",
      "Epoch 64, Training Loss: 0.8102927038950078\n",
      "Epoch 65, Training Loss: 0.810049812372993\n",
      "Epoch 66, Training Loss: 0.8099513645031873\n",
      "Epoch 67, Training Loss: 0.8097474876572104\n",
      "Epoch 68, Training Loss: 0.8096158271677354\n",
      "Epoch 69, Training Loss: 0.8095859582985149\n",
      "Epoch 70, Training Loss: 0.8093545343595393\n",
      "Epoch 71, Training Loss: 0.8093130127121421\n",
      "Epoch 72, Training Loss: 0.8090953399153317\n",
      "Epoch 73, Training Loss: 0.8090467746117536\n",
      "Epoch 74, Training Loss: 0.8089042250549092\n",
      "Epoch 75, Training Loss: 0.8088632205654593\n",
      "Epoch 76, Training Loss: 0.8087698012239792\n",
      "Epoch 77, Training Loss: 0.808717291916118\n",
      "Epoch 78, Training Loss: 0.8085315978527069\n",
      "Epoch 79, Training Loss: 0.8084856668640585\n",
      "Epoch 80, Training Loss: 0.808373676889083\n",
      "Epoch 81, Training Loss: 0.8081908120127286\n",
      "Epoch 82, Training Loss: 0.808201311265721\n",
      "Epoch 83, Training Loss: 0.8080647510640762\n",
      "Epoch 84, Training Loss: 0.8080047275739558\n",
      "Epoch 85, Training Loss: 0.8079132721704595\n",
      "Epoch 86, Training Loss: 0.8077772700786591\n",
      "Epoch 87, Training Loss: 0.8076514375911039\n",
      "Epoch 88, Training Loss: 0.8075742707532995\n",
      "Epoch 89, Training Loss: 0.8076188941562877\n",
      "Epoch 90, Training Loss: 0.8074504877539243\n",
      "Epoch 91, Training Loss: 0.8073470236974604\n",
      "Epoch 92, Training Loss: 0.8073249455760507\n",
      "Epoch 93, Training Loss: 0.8071933533864862\n",
      "Epoch 94, Training Loss: 0.807051458569134\n",
      "Epoch 95, Training Loss: 0.8070074648716871\n",
      "Epoch 96, Training Loss: 0.8069159797359915\n",
      "Epoch 97, Training Loss: 0.8069081977535697\n",
      "Epoch 98, Training Loss: 0.8067690802321714\n",
      "Epoch 99, Training Loss: 0.806619564925923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:09:13,077] Trial 408 finished with value: 0.6295333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.806527902729371\n",
      "Epoch 1, Training Loss: 0.9854206733237532\n",
      "Epoch 2, Training Loss: 0.9495952528222162\n",
      "Epoch 3, Training Loss: 0.9296965746951282\n",
      "Epoch 4, Training Loss: 0.8978996835256877\n",
      "Epoch 5, Training Loss: 0.8580532402024234\n",
      "Epoch 6, Training Loss: 0.8295030625242936\n",
      "Epoch 7, Training Loss: 0.8174093722400809\n",
      "Epoch 8, Training Loss: 0.8130956865791091\n",
      "Epoch 9, Training Loss: 0.8110660914191626\n",
      "Epoch 10, Training Loss: 0.8096241748422608\n",
      "Epoch 11, Training Loss: 0.8085619813517521\n",
      "Epoch 12, Training Loss: 0.8082160505137049\n",
      "Epoch 13, Training Loss: 0.8077977285349279\n",
      "Epoch 14, Training Loss: 0.8066035381833413\n",
      "Epoch 15, Training Loss: 0.8064546439880715\n",
      "Epoch 16, Training Loss: 0.8055736815122734\n",
      "Epoch 17, Training Loss: 0.8046156954048271\n",
      "Epoch 18, Training Loss: 0.804915472080833\n",
      "Epoch 19, Training Loss: 0.8046410367004854\n",
      "Epoch 20, Training Loss: 0.8035301409269634\n",
      "Epoch 21, Training Loss: 0.8034469115106683\n",
      "Epoch 22, Training Loss: 0.802918029010744\n",
      "Epoch 23, Training Loss: 0.802728117318978\n",
      "Epoch 24, Training Loss: 0.803416145206394\n",
      "Epoch 25, Training Loss: 0.802554035993447\n",
      "Epoch 26, Training Loss: 0.8026736708511983\n",
      "Epoch 27, Training Loss: 0.8025532344230135\n",
      "Epoch 28, Training Loss: 0.8025211619255238\n",
      "Epoch 29, Training Loss: 0.8024697466004164\n",
      "Epoch 30, Training Loss: 0.8016327676916482\n",
      "Epoch 31, Training Loss: 0.8017705003121742\n",
      "Epoch 32, Training Loss: 0.8012308053504256\n",
      "Epoch 33, Training Loss: 0.8012492227375059\n",
      "Epoch 34, Training Loss: 0.8013224424276136\n",
      "Epoch 35, Training Loss: 0.8015186033750835\n",
      "Epoch 36, Training Loss: 0.8014014079158468\n",
      "Epoch 37, Training Loss: 0.8010639657651571\n",
      "Epoch 38, Training Loss: 0.8009357078631122\n",
      "Epoch 39, Training Loss: 0.8009410999771348\n",
      "Epoch 40, Training Loss: 0.8004351154305881\n",
      "Epoch 41, Training Loss: 0.7999352598100676\n",
      "Epoch 42, Training Loss: 0.8002177021557227\n",
      "Epoch 43, Training Loss: 0.8001247779767316\n",
      "Epoch 44, Training Loss: 0.7997768378795538\n",
      "Epoch 45, Training Loss: 0.7997500424994561\n",
      "Epoch 46, Training Loss: 0.7998603009639826\n",
      "Epoch 47, Training Loss: 0.8001447558403015\n",
      "Epoch 48, Training Loss: 0.8000447217683146\n",
      "Epoch 49, Training Loss: 0.800301136020431\n",
      "Epoch 50, Training Loss: 0.799837623234082\n",
      "Epoch 51, Training Loss: 0.799242635418598\n",
      "Epoch 52, Training Loss: 0.7995774771934165\n",
      "Epoch 53, Training Loss: 0.7988415572876321\n",
      "Epoch 54, Training Loss: 0.7992259755170434\n",
      "Epoch 55, Training Loss: 0.7986607416679985\n",
      "Epoch 56, Training Loss: 0.7988102805345578\n",
      "Epoch 57, Training Loss: 0.7994526019670013\n",
      "Epoch 58, Training Loss: 0.7988737410172484\n",
      "Epoch 59, Training Loss: 0.7982045646000625\n",
      "Epoch 60, Training Loss: 0.7983537379960368\n",
      "Epoch 61, Training Loss: 0.7984999642336279\n",
      "Epoch 62, Training Loss: 0.797744805068898\n",
      "Epoch 63, Training Loss: 0.7981913277081081\n",
      "Epoch 64, Training Loss: 0.7996161641034865\n",
      "Epoch 65, Training Loss: 0.7977983949775983\n",
      "Epoch 66, Training Loss: 0.7980037435553128\n",
      "Epoch 67, Training Loss: 0.798169722055134\n",
      "Epoch 68, Training Loss: 0.7983593202175054\n",
      "Epoch 69, Training Loss: 0.7976693740464691\n",
      "Epoch 70, Training Loss: 0.7974636781484561\n",
      "Epoch 71, Training Loss: 0.7970238269719863\n",
      "Epoch 72, Training Loss: 0.7969912484176177\n",
      "Epoch 73, Training Loss: 0.7974798923148249\n",
      "Epoch 74, Training Loss: 0.7967451882541627\n",
      "Epoch 75, Training Loss: 0.796795241814807\n",
      "Epoch 76, Training Loss: 0.7969146810079876\n",
      "Epoch 77, Training Loss: 0.7968596277380349\n",
      "Epoch 78, Training Loss: 0.7966203237834729\n",
      "Epoch 79, Training Loss: 0.7971079828147601\n",
      "Epoch 80, Training Loss: 0.7973467724663871\n",
      "Epoch 81, Training Loss: 0.7969718240257493\n",
      "Epoch 82, Training Loss: 0.796851358557106\n",
      "Epoch 83, Training Loss: 0.7966447312132756\n",
      "Epoch 84, Training Loss: 0.7964632668889555\n",
      "Epoch 85, Training Loss: 0.795861127770933\n",
      "Epoch 86, Training Loss: 0.7966401012320268\n",
      "Epoch 87, Training Loss: 0.7961941240425396\n",
      "Epoch 88, Training Loss: 0.7959562322699038\n",
      "Epoch 89, Training Loss: 0.7959044417044274\n",
      "Epoch 90, Training Loss: 0.7959855099369709\n",
      "Epoch 91, Training Loss: 0.795608460410197\n",
      "Epoch 92, Training Loss: 0.7958460424179421\n",
      "Epoch 93, Training Loss: 0.7955340492994265\n",
      "Epoch 94, Training Loss: 0.7956916896024144\n",
      "Epoch 95, Training Loss: 0.7958555540644137\n",
      "Epoch 96, Training Loss: 0.7959439984837869\n",
      "Epoch 97, Training Loss: 0.7956650673894954\n",
      "Epoch 98, Training Loss: 0.7956781095131895\n",
      "Epoch 99, Training Loss: 0.7950977333506247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:11:04,571] Trial 409 finished with value: 0.6260666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7947345662834053\n",
      "Epoch 1, Training Loss: 0.852967741840026\n",
      "Epoch 2, Training Loss: 0.8192189281828264\n",
      "Epoch 3, Training Loss: 0.8154162487562965\n",
      "Epoch 4, Training Loss: 0.8149340797873105\n",
      "Epoch 5, Training Loss: 0.8134497685292188\n",
      "Epoch 6, Training Loss: 0.816916703266256\n",
      "Epoch 7, Training Loss: 0.8124824758136974\n",
      "Epoch 8, Training Loss: 0.8170947880604688\n",
      "Epoch 9, Training Loss: 0.811498544216156\n",
      "Epoch 10, Training Loss: 0.8121924994973575\n",
      "Epoch 11, Training Loss: 0.8129005411091973\n",
      "Epoch 12, Training Loss: 0.8100925015000736\n",
      "Epoch 13, Training Loss: 0.8121439869263593\n",
      "Epoch 14, Training Loss: 0.8138893034878899\n",
      "Epoch 15, Training Loss: 0.8138669976767372\n",
      "Epoch 16, Training Loss: 0.809683108329773\n",
      "Epoch 17, Training Loss: 0.810231826024897\n",
      "Epoch 18, Training Loss: 0.8105704233926885\n",
      "Epoch 19, Training Loss: 0.8134720791087431\n",
      "Epoch 20, Training Loss: 0.8140894121983472\n",
      "Epoch 21, Training Loss: 0.814234084522023\n",
      "Epoch 22, Training Loss: 0.8145536494956297\n",
      "Epoch 23, Training Loss: 0.810942637569764\n",
      "Epoch 24, Training Loss: 0.8123506012383629\n",
      "Epoch 25, Training Loss: 0.8156845054906957\n",
      "Epoch 26, Training Loss: 0.8147148360925562\n",
      "Epoch 27, Training Loss: 0.8125913958689746\n",
      "Epoch 28, Training Loss: 0.8136596909691306\n",
      "Epoch 29, Training Loss: 0.8146845500609454\n",
      "Epoch 30, Training Loss: 0.8140491188974941\n",
      "Epoch 31, Training Loss: 0.8124701824608971\n",
      "Epoch 32, Training Loss: 0.8232705512467553\n",
      "Epoch 33, Training Loss: 0.8195107361148385\n",
      "Epoch 34, Training Loss: 0.8198138276268454\n",
      "Epoch 35, Training Loss: 0.8183699229184319\n",
      "Epoch 36, Training Loss: 0.821420303513022\n",
      "Epoch 37, Training Loss: 0.8184745527716244\n",
      "Epoch 38, Training Loss: 0.8241032377411337\n",
      "Epoch 39, Training Loss: 0.8273119657179888\n",
      "Epoch 40, Training Loss: 0.8292994499206543\n",
      "Epoch 41, Training Loss: 0.8286500727429109\n",
      "Epoch 42, Training Loss: 0.8260106150542988\n",
      "Epoch 43, Training Loss: 0.825454160045175\n",
      "Epoch 44, Training Loss: 0.8283125483288485\n",
      "Epoch 45, Training Loss: 0.8281126764241387\n",
      "Epoch 46, Training Loss: 0.8292252687145681\n",
      "Epoch 47, Training Loss: 0.825706312936895\n",
      "Epoch 48, Training Loss: 0.829393745099797\n",
      "Epoch 49, Training Loss: 0.8231622960286982\n",
      "Epoch 50, Training Loss: 0.8303559160933776\n",
      "Epoch 51, Training Loss: 0.824312859843759\n",
      "Epoch 52, Training Loss: 0.8274944265449748\n",
      "Epoch 53, Training Loss: 0.8249211784671334\n",
      "Epoch 54, Training Loss: 0.8234505458439098\n",
      "Epoch 55, Training Loss: 0.8258527010328629\n",
      "Epoch 56, Training Loss: 0.8239820532237783\n",
      "Epoch 57, Training Loss: 0.8245517654278699\n",
      "Epoch 58, Training Loss: 0.8274873733520508\n",
      "Epoch 59, Training Loss: 0.8272043174154619\n",
      "Epoch 60, Training Loss: 0.8243598442217883\n",
      "Epoch 61, Training Loss: 0.8256776621762444\n",
      "Epoch 62, Training Loss: 0.821508777071448\n",
      "Epoch 63, Training Loss: 0.8208588438875535\n",
      "Epoch 64, Training Loss: 0.826713236991097\n",
      "Epoch 65, Training Loss: 0.8253002602212569\n",
      "Epoch 66, Training Loss: 0.8216055960514966\n",
      "Epoch 67, Training Loss: 0.8234131383194643\n",
      "Epoch 68, Training Loss: 0.8269551315728356\n",
      "Epoch 69, Training Loss: 0.8202322551783393\n",
      "Epoch 70, Training Loss: 0.8248578982493456\n",
      "Epoch 71, Training Loss: 0.8228834958637462\n",
      "Epoch 72, Training Loss: 0.8226398963787976\n",
      "Epoch 73, Training Loss: 0.8244217812313753\n",
      "Epoch 74, Training Loss: 0.822880688344731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:12:47,677] Trial 410 finished with value: 0.6234666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.824962258198682\n",
      "Epoch 1, Training Loss: 1.1007883053973204\n",
      "Epoch 2, Training Loss: 1.0923704536337602\n",
      "Epoch 3, Training Loss: 1.0920513386116888\n",
      "Epoch 4, Training Loss: 1.0918634457695753\n",
      "Epoch 5, Training Loss: 1.0915618781756637\n",
      "Epoch 6, Training Loss: 1.0914313217751066\n",
      "Epoch 7, Training Loss: 1.0913069312733814\n",
      "Epoch 8, Training Loss: 1.090893152602633\n",
      "Epoch 9, Training Loss: 1.0907579032998336\n",
      "Epoch 10, Training Loss: 1.09051811533763\n",
      "Epoch 11, Training Loss: 1.0903957539034965\n",
      "Epoch 12, Training Loss: 1.090015146786109\n",
      "Epoch 13, Training Loss: 1.0898154932753483\n",
      "Epoch 14, Training Loss: 1.0896036160619635\n",
      "Epoch 15, Training Loss: 1.0894766208820774\n",
      "Epoch 16, Training Loss: 1.089263729941576\n",
      "Epoch 17, Training Loss: 1.0889540980633041\n",
      "Epoch 18, Training Loss: 1.0887385784235215\n",
      "Epoch 19, Training Loss: 1.0885505943370044\n",
      "Epoch 20, Training Loss: 1.088278837670061\n",
      "Epoch 21, Training Loss: 1.0880458417691683\n",
      "Epoch 22, Training Loss: 1.0879154242967304\n",
      "Epoch 23, Training Loss: 1.0876565300432364\n",
      "Epoch 24, Training Loss: 1.0874795112394748\n",
      "Epoch 25, Training Loss: 1.087218763774499\n",
      "Epoch 26, Training Loss: 1.0868637959759935\n",
      "Epoch 27, Training Loss: 1.0866999997232194\n",
      "Epoch 28, Training Loss: 1.0865173750353936\n",
      "Epoch 29, Training Loss: 1.0863765917326276\n",
      "Epoch 30, Training Loss: 1.0860417337345898\n",
      "Epoch 31, Training Loss: 1.0858467854951557\n",
      "Epoch 32, Training Loss: 1.085481015363134\n",
      "Epoch 33, Training Loss: 1.085309716633388\n",
      "Epoch 34, Training Loss: 1.085068789281343\n",
      "Epoch 35, Training Loss: 1.084718751011038\n",
      "Epoch 36, Training Loss: 1.0845337552235539\n",
      "Epoch 37, Training Loss: 1.0842539740684338\n",
      "Epoch 38, Training Loss: 1.0840356098978143\n",
      "Epoch 39, Training Loss: 1.083908856721749\n",
      "Epoch 40, Training Loss: 1.083652010896152\n",
      "Epoch 41, Training Loss: 1.0832823204814939\n",
      "Epoch 42, Training Loss: 1.0830547533537211\n",
      "Epoch 43, Training Loss: 1.0826032446739369\n",
      "Epoch 44, Training Loss: 1.082368078446926\n",
      "Epoch 45, Training Loss: 1.0821411405290877\n",
      "Epoch 46, Training Loss: 1.0818933115865952\n",
      "Epoch 47, Training Loss: 1.0815266774113017\n",
      "Epoch 48, Training Loss: 1.081227091201266\n",
      "Epoch 49, Training Loss: 1.080974767082616\n",
      "Epoch 50, Training Loss: 1.080566560594659\n",
      "Epoch 51, Training Loss: 1.0802756768420227\n",
      "Epoch 52, Training Loss: 1.0800609857516181\n",
      "Epoch 53, Training Loss: 1.0796010714724549\n",
      "Epoch 54, Training Loss: 1.079371940641475\n",
      "Epoch 55, Training Loss: 1.0789514536248115\n",
      "Epoch 56, Training Loss: 1.0786300953169514\n",
      "Epoch 57, Training Loss: 1.0782703702611134\n",
      "Epoch 58, Training Loss: 1.0778955961528578\n",
      "Epoch 59, Training Loss: 1.0775123983397519\n",
      "Epoch 60, Training Loss: 1.0772217906507335\n",
      "Epoch 61, Training Loss: 1.0768552534562303\n",
      "Epoch 62, Training Loss: 1.0765203927692615\n",
      "Epoch 63, Training Loss: 1.0759712749854067\n",
      "Epoch 64, Training Loss: 1.0756952909598674\n",
      "Epoch 65, Training Loss: 1.0752565903771192\n",
      "Epoch 66, Training Loss: 1.0748621619733654\n",
      "Epoch 67, Training Loss: 1.0743491405831245\n",
      "Epoch 68, Training Loss: 1.0739519391741073\n",
      "Epoch 69, Training Loss: 1.0734724141601333\n",
      "Epoch 70, Training Loss: 1.0730550364444131\n",
      "Epoch 71, Training Loss: 1.0726414295067463\n",
      "Epoch 72, Training Loss: 1.0721282623764268\n",
      "Epoch 73, Training Loss: 1.071759090925518\n",
      "Epoch 74, Training Loss: 1.0711963800559368\n",
      "Epoch 75, Training Loss: 1.0706712489737604\n",
      "Epoch 76, Training Loss: 1.0701672252855803\n",
      "Epoch 77, Training Loss: 1.0696010153992732\n",
      "Epoch 78, Training Loss: 1.0690948070440078\n",
      "Epoch 79, Training Loss: 1.068523479583568\n",
      "Epoch 80, Training Loss: 1.0679808543140727\n",
      "Epoch 81, Training Loss: 1.067616866047221\n",
      "Epoch 82, Training Loss: 1.067008940438579\n",
      "Epoch 83, Training Loss: 1.0663524737035421\n",
      "Epoch 84, Training Loss: 1.0656471015815447\n",
      "Epoch 85, Training Loss: 1.0651138112061005\n",
      "Epoch 86, Training Loss: 1.0645599605445575\n",
      "Epoch 87, Training Loss: 1.0637734993956143\n",
      "Epoch 88, Training Loss: 1.063278409592191\n",
      "Epoch 89, Training Loss: 1.0624529311531468\n",
      "Epoch 90, Training Loss: 1.0618360429778135\n",
      "Epoch 91, Training Loss: 1.0611968409746213\n",
      "Epoch 92, Training Loss: 1.0605463528095331\n",
      "Epoch 93, Training Loss: 1.0597393464325067\n",
      "Epoch 94, Training Loss: 1.0588938603723856\n",
      "Epoch 95, Training Loss: 1.0582839938931\n",
      "Epoch 96, Training Loss: 1.057337408496025\n",
      "Epoch 97, Training Loss: 1.0566867315679564\n",
      "Epoch 98, Training Loss: 1.0559498403305398\n",
      "Epoch 99, Training Loss: 1.055113761048568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:14:21,690] Trial 411 finished with value: 0.4674 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.054385345143483\n",
      "Epoch 1, Training Loss: 0.932446302876753\n",
      "Epoch 2, Training Loss: 0.8705778928364024\n",
      "Epoch 3, Training Loss: 0.8287559225278742\n",
      "Epoch 4, Training Loss: 0.8125396727113163\n",
      "Epoch 5, Training Loss: 0.8064909883106456\n",
      "Epoch 6, Training Loss: 0.8037817192778868\n",
      "Epoch 7, Training Loss: 0.8020867978124058\n",
      "Epoch 8, Training Loss: 0.8011779567073373\n",
      "Epoch 9, Training Loss: 0.8005451927465551\n",
      "Epoch 10, Training Loss: 0.7999196666829727\n",
      "Epoch 11, Training Loss: 0.7994683066536399\n",
      "Epoch 12, Training Loss: 0.7987927182281719\n",
      "Epoch 13, Training Loss: 0.7986571800007539\n",
      "Epoch 14, Training Loss: 0.7981794657426722\n",
      "Epoch 15, Training Loss: 0.7982428593495313\n",
      "Epoch 16, Training Loss: 0.7977061054987066\n",
      "Epoch 17, Training Loss: 0.797827933956595\n",
      "Epoch 18, Training Loss: 0.7976210071760066\n",
      "Epoch 19, Training Loss: 0.79722387769643\n",
      "Epoch 20, Training Loss: 0.7973992104390089\n",
      "Epoch 21, Training Loss: 0.7971133553280549\n",
      "Epoch 22, Training Loss: 0.7968078031960656\n",
      "Epoch 23, Training Loss: 0.796914832311518\n",
      "Epoch 24, Training Loss: 0.7966132553885965\n",
      "Epoch 25, Training Loss: 0.7961706343819114\n",
      "Epoch 26, Training Loss: 0.7963634896278381\n",
      "Epoch 27, Training Loss: 0.7961511934504789\n",
      "Epoch 28, Training Loss: 0.795965651624343\n",
      "Epoch 29, Training Loss: 0.7955697039295645\n",
      "Epoch 30, Training Loss: 0.79548304754145\n",
      "Epoch 31, Training Loss: 0.7950726543454563\n",
      "Epoch 32, Training Loss: 0.7946975256414974\n",
      "Epoch 33, Training Loss: 0.7945725190639495\n",
      "Epoch 34, Training Loss: 0.7943122619741103\n",
      "Epoch 35, Training Loss: 0.7937380949188682\n",
      "Epoch 36, Training Loss: 0.7936917529386632\n",
      "Epoch 37, Training Loss: 0.7935567912634681\n",
      "Epoch 38, Training Loss: 0.7932062467406777\n",
      "Epoch 39, Training Loss: 0.7928737765901229\n",
      "Epoch 40, Training Loss: 0.7927112001531265\n",
      "Epoch 41, Training Loss: 0.7926461723271538\n",
      "Epoch 42, Training Loss: 0.7920418485473184\n",
      "Epoch 43, Training Loss: 0.7917506782447591\n",
      "Epoch 44, Training Loss: 0.7914486277103424\n",
      "Epoch 45, Training Loss: 0.7914701293496524\n",
      "Epoch 46, Training Loss: 0.7912810038819033\n",
      "Epoch 47, Training Loss: 0.7912536243130179\n",
      "Epoch 48, Training Loss: 0.791147726984585\n",
      "Epoch 49, Training Loss: 0.7907109848891988\n",
      "Epoch 50, Training Loss: 0.7904373699777266\n",
      "Epoch 51, Training Loss: 0.7902212253037622\n",
      "Epoch 52, Training Loss: 0.7904593895463382\n",
      "Epoch 53, Training Loss: 0.789861132958356\n",
      "Epoch 54, Training Loss: 0.7901317203044891\n",
      "Epoch 55, Training Loss: 0.7896151717270122\n",
      "Epoch 56, Training Loss: 0.7898820119745591\n",
      "Epoch 57, Training Loss: 0.7895308864116669\n",
      "Epoch 58, Training Loss: 0.7893979780814226\n",
      "Epoch 59, Training Loss: 0.7895698286505306\n",
      "Epoch 60, Training Loss: 0.7891878414855283\n",
      "Epoch 61, Training Loss: 0.7888117633146399\n",
      "Epoch 62, Training Loss: 0.788898959790959\n",
      "Epoch 63, Training Loss: 0.7894513462571536\n",
      "Epoch 64, Training Loss: 0.7887356719549965\n",
      "Epoch 65, Training Loss: 0.7886145523716421\n",
      "Epoch 66, Training Loss: 0.7882765982431524\n",
      "Epoch 67, Training Loss: 0.7882073526522693\n",
      "Epoch 68, Training Loss: 0.7881742338573231\n",
      "Epoch 69, Training Loss: 0.7883628281425027\n",
      "Epoch 70, Training Loss: 0.7882781811321483\n",
      "Epoch 71, Training Loss: 0.7880816929480609\n",
      "Epoch 72, Training Loss: 0.7879822961021872\n",
      "Epoch 73, Training Loss: 0.7878818805778728\n",
      "Epoch 74, Training Loss: 0.7873773165310131\n",
      "Epoch 75, Training Loss: 0.7874643057935378\n",
      "Epoch 76, Training Loss: 0.7875347465627334\n",
      "Epoch 77, Training Loss: 0.7871538420284495\n",
      "Epoch 78, Training Loss: 0.7875341675562018\n",
      "Epoch 79, Training Loss: 0.7869899183161119\n",
      "Epoch 80, Training Loss: 0.7871261166825014\n",
      "Epoch 81, Training Loss: 0.7868988089701708\n",
      "Epoch 82, Training Loss: 0.7870838311139275\n",
      "Epoch 83, Training Loss: 0.7870127807645236\n",
      "Epoch 84, Training Loss: 0.7868938145216774\n",
      "Epoch 85, Training Loss: 0.7870872296305264\n",
      "Epoch 86, Training Loss: 0.7866982361148386\n",
      "Epoch 87, Training Loss: 0.7867875824956333\n",
      "Epoch 88, Training Loss: 0.7864975812154658\n",
      "Epoch 89, Training Loss: 0.7866832530498504\n",
      "Epoch 90, Training Loss: 0.7864832678261925\n",
      "Epoch 91, Training Loss: 0.7865327728495878\n",
      "Epoch 92, Training Loss: 0.7861175929798799\n",
      "Epoch 93, Training Loss: 0.7860672685679267\n",
      "Epoch 94, Training Loss: 0.7862664444306318\n",
      "Epoch 95, Training Loss: 0.7861746342041913\n",
      "Epoch 96, Training Loss: 0.7859491653302136\n",
      "Epoch 97, Training Loss: 0.7857606371711282\n",
      "Epoch 98, Training Loss: 0.7856573826425216\n",
      "Epoch 99, Training Loss: 0.7862006785589106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:16:20,665] Trial 412 finished with value: 0.6409333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7857838384544148\n",
      "Epoch 1, Training Loss: 0.9684040000859429\n",
      "Epoch 2, Training Loss: 0.9364216524713179\n",
      "Epoch 3, Training Loss: 0.9183331550570095\n",
      "Epoch 4, Training Loss: 0.8992888560014612\n",
      "Epoch 5, Training Loss: 0.8802051481078653\n",
      "Epoch 6, Training Loss: 0.8624828103710623\n",
      "Epoch 7, Training Loss: 0.8479848919896519\n",
      "Epoch 8, Training Loss: 0.8366670989990235\n",
      "Epoch 9, Training Loss: 0.8284127950668335\n",
      "Epoch 10, Training Loss: 0.8224600528268253\n",
      "Epoch 11, Training Loss: 0.8185031131435843\n",
      "Epoch 12, Training Loss: 0.8153388951806461\n",
      "Epoch 13, Training Loss: 0.8131340575218201\n",
      "Epoch 14, Training Loss: 0.8116772426577176\n",
      "Epoch 15, Training Loss: 0.8104865063639248\n",
      "Epoch 16, Training Loss: 0.8092877135557287\n",
      "Epoch 17, Training Loss: 0.8083274745941162\n",
      "Epoch 18, Training Loss: 0.807596340600182\n",
      "Epoch 19, Training Loss: 0.8067979041969074\n",
      "Epoch 20, Training Loss: 0.8064276116735795\n",
      "Epoch 21, Training Loss: 0.8058633962799521\n",
      "Epoch 22, Training Loss: 0.8052346985480364\n",
      "Epoch 23, Training Loss: 0.8048618311741773\n",
      "Epoch 24, Training Loss: 0.8045038748488706\n",
      "Epoch 25, Training Loss: 0.8040520835624022\n",
      "Epoch 26, Training Loss: 0.80355625853819\n",
      "Epoch 27, Training Loss: 0.803464600128286\n",
      "Epoch 28, Training Loss: 0.8030299381648793\n",
      "Epoch 29, Training Loss: 0.8026448663543252\n",
      "Epoch 30, Training Loss: 0.8023918021426482\n",
      "Epoch 31, Training Loss: 0.8022977773582234\n",
      "Epoch 32, Training Loss: 0.8019008351073545\n",
      "Epoch 33, Training Loss: 0.8017649365873898\n",
      "Epoch 34, Training Loss: 0.8016534078121186\n",
      "Epoch 35, Training Loss: 0.8012928929749658\n",
      "Epoch 36, Training Loss: 0.801150409824708\n",
      "Epoch 37, Training Loss: 0.8009824318044326\n",
      "Epoch 38, Training Loss: 0.8006731782941258\n",
      "Epoch 39, Training Loss: 0.8008936154842377\n",
      "Epoch 40, Training Loss: 0.8006525386081023\n",
      "Epoch 41, Training Loss: 0.8006025005789364\n",
      "Epoch 42, Training Loss: 0.800247186983333\n",
      "Epoch 43, Training Loss: 0.8002171864930321\n",
      "Epoch 44, Training Loss: 0.8001241797559402\n",
      "Epoch 45, Training Loss: 0.7999663580866421\n",
      "Epoch 46, Training Loss: 0.7997851029564352\n",
      "Epoch 47, Training Loss: 0.7998073059671066\n",
      "Epoch 48, Training Loss: 0.7995153220260844\n",
      "Epoch 49, Training Loss: 0.7996024252386654\n",
      "Epoch 50, Training Loss: 0.799452172798269\n",
      "Epoch 51, Training Loss: 0.7993782701913048\n",
      "Epoch 52, Training Loss: 0.7994710158600526\n",
      "Epoch 53, Training Loss: 0.7992716172863455\n",
      "Epoch 54, Training Loss: 0.7992191627446343\n",
      "Epoch 55, Training Loss: 0.7990849258619196\n",
      "Epoch 56, Training Loss: 0.7992003329361186\n",
      "Epoch 57, Training Loss: 0.7990757219931659\n",
      "Epoch 58, Training Loss: 0.7988751891781302\n",
      "Epoch 59, Training Loss: 0.7989977050528807\n",
      "Epoch 60, Training Loss: 0.7987437875831829\n",
      "Epoch 61, Training Loss: 0.7988098885031307\n",
      "Epoch 62, Training Loss: 0.7986240803494173\n",
      "Epoch 63, Training Loss: 0.7985348019880407\n",
      "Epoch 64, Training Loss: 0.7984586515847374\n",
      "Epoch 65, Training Loss: 0.798647267257466\n",
      "Epoch 66, Training Loss: 0.7983976049984203\n",
      "Epoch 67, Training Loss: 0.7985517974460826\n",
      "Epoch 68, Training Loss: 0.7984130347476286\n",
      "Epoch 69, Training Loss: 0.7984794041689705\n",
      "Epoch 70, Training Loss: 0.7984133697257323\n",
      "Epoch 71, Training Loss: 0.7982090562231401\n",
      "Epoch 72, Training Loss: 0.79812919160899\n",
      "Epoch 73, Training Loss: 0.7981622867023244\n",
      "Epoch 74, Training Loss: 0.7982216525779051\n",
      "Epoch 75, Training Loss: 0.7982252693176269\n",
      "Epoch 76, Training Loss: 0.7982579701087054\n",
      "Epoch 77, Training Loss: 0.7979881725591772\n",
      "Epoch 78, Training Loss: 0.7979471804113949\n",
      "Epoch 79, Training Loss: 0.7979607837340411\n",
      "Epoch 80, Training Loss: 0.7979436349868775\n",
      "Epoch 81, Training Loss: 0.7979130495295805\n",
      "Epoch 82, Training Loss: 0.798035628304762\n",
      "Epoch 83, Training Loss: 0.7979425703076756\n",
      "Epoch 84, Training Loss: 0.7979277305042043\n",
      "Epoch 85, Training Loss: 0.7978466291287366\n",
      "Epoch 86, Training Loss: 0.7978038649699267\n",
      "Epoch 87, Training Loss: 0.797774121761322\n",
      "Epoch 88, Training Loss: 0.7977476976198309\n",
      "Epoch 89, Training Loss: 0.7977814499069663\n",
      "Epoch 90, Training Loss: 0.7977409056354972\n",
      "Epoch 91, Training Loss: 0.7977528640101937\n",
      "Epoch 92, Training Loss: 0.7977048226665048\n",
      "Epoch 93, Training Loss: 0.7976589550691492\n",
      "Epoch 94, Training Loss: 0.7976067791966831\n",
      "Epoch 95, Training Loss: 0.7976971751802108\n",
      "Epoch 96, Training Loss: 0.7976296243246864\n",
      "Epoch 97, Training Loss: 0.7974998188720029\n",
      "Epoch 98, Training Loss: 0.7974932328392478\n",
      "Epoch 99, Training Loss: 0.7974561450060677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:17:58,560] Trial 413 finished with value: 0.6334666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7973297027279349\n",
      "Epoch 1, Training Loss: 0.8843085475528941\n",
      "Epoch 2, Training Loss: 0.8219240478908314\n",
      "Epoch 3, Training Loss: 0.8151076906568864\n",
      "Epoch 4, Training Loss: 0.8097891681334551\n",
      "Epoch 5, Training Loss: 0.8050822244672214\n",
      "Epoch 6, Training Loss: 0.8019726885767544\n",
      "Epoch 7, Training Loss: 0.7998008778515984\n",
      "Epoch 8, Training Loss: 0.7980299019112307\n",
      "Epoch 9, Training Loss: 0.7970823982883902\n",
      "Epoch 10, Training Loss: 0.7959081278829013\n",
      "Epoch 11, Training Loss: 0.7950009104784798\n",
      "Epoch 12, Training Loss: 0.7936473432007958\n",
      "Epoch 13, Training Loss: 0.7937726413502413\n",
      "Epoch 14, Training Loss: 0.7924528621926027\n",
      "Epoch 15, Training Loss: 0.7922421957464779\n",
      "Epoch 16, Training Loss: 0.7916541907366584\n",
      "Epoch 17, Training Loss: 0.7909644180886886\n",
      "Epoch 18, Training Loss: 0.790756909496644\n",
      "Epoch 19, Training Loss: 0.7902816982830272\n",
      "Epoch 20, Training Loss: 0.790150785446167\n",
      "Epoch 21, Training Loss: 0.7901028754430659\n",
      "Epoch 22, Training Loss: 0.7900181456874399\n",
      "Epoch 23, Training Loss: 0.7892849626260645\n",
      "Epoch 24, Training Loss: 0.7891573568652658\n",
      "Epoch 25, Training Loss: 0.7886260876936071\n",
      "Epoch 26, Training Loss: 0.7886035375034108\n",
      "Epoch 27, Training Loss: 0.7882384502186495\n",
      "Epoch 28, Training Loss: 0.7887299086065853\n",
      "Epoch 29, Training Loss: 0.7878826432368334\n",
      "Epoch 30, Training Loss: 0.7876809528294731\n",
      "Epoch 31, Training Loss: 0.7877022492184359\n",
      "Epoch 32, Training Loss: 0.787363029648276\n",
      "Epoch 33, Training Loss: 0.7875021107056561\n",
      "Epoch 34, Training Loss: 0.786978567908792\n",
      "Epoch 35, Training Loss: 0.7865539153183208\n",
      "Epoch 36, Training Loss: 0.7870149963042316\n",
      "Epoch 37, Training Loss: 0.7866442233674666\n",
      "Epoch 38, Training Loss: 0.7865751775573282\n",
      "Epoch 39, Training Loss: 0.7862674643011655\n",
      "Epoch 40, Training Loss: 0.7865791890200446\n",
      "Epoch 41, Training Loss: 0.7860606955079471\n",
      "Epoch 42, Training Loss: 0.7863855014829074\n",
      "Epoch 43, Training Loss: 0.7859277007860296\n",
      "Epoch 44, Training Loss: 0.785875779320212\n",
      "Epoch 45, Training Loss: 0.7856041025414187\n",
      "Epoch 46, Training Loss: 0.7855327890901005\n",
      "Epoch 47, Training Loss: 0.7854198121323305\n",
      "Epoch 48, Training Loss: 0.7853763328580295\n",
      "Epoch 49, Training Loss: 0.7851226894294514\n",
      "Epoch 50, Training Loss: 0.7851837888184716\n",
      "Epoch 51, Training Loss: 0.7854509911116432\n",
      "Epoch 52, Training Loss: 0.785237716366263\n",
      "Epoch 53, Training Loss: 0.7849979133465711\n",
      "Epoch 54, Training Loss: 0.7844900523213779\n",
      "Epoch 55, Training Loss: 0.7848093584004571\n",
      "Epoch 56, Training Loss: 0.7845290028347689\n",
      "Epoch 57, Training Loss: 0.7844381824661704\n",
      "Epoch 58, Training Loss: 0.7845864145194783\n",
      "Epoch 59, Training Loss: 0.7847864741437576\n",
      "Epoch 60, Training Loss: 0.7839083144244026\n",
      "Epoch 61, Training Loss: 0.7841120071270886\n",
      "Epoch 62, Training Loss: 0.7843813767853905\n",
      "Epoch 63, Training Loss: 0.7840445845267352\n",
      "Epoch 64, Training Loss: 0.7839014214627883\n",
      "Epoch 65, Training Loss: 0.7844793306378757\n",
      "Epoch 66, Training Loss: 0.7840750374513514\n",
      "Epoch 67, Training Loss: 0.7841561285888448\n",
      "Epoch 68, Training Loss: 0.7839689113813288\n",
      "Epoch 69, Training Loss: 0.7839771195018993\n",
      "Epoch 70, Training Loss: 0.7840865972462823\n",
      "Epoch 71, Training Loss: 0.7838952940351823\n",
      "Epoch 72, Training Loss: 0.7842157505540287\n",
      "Epoch 73, Training Loss: 0.7840688899685355\n",
      "Epoch 74, Training Loss: 0.7838186558555155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:19:34,903] Trial 414 finished with value: 0.6396666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7839534342288971\n",
      "Epoch 1, Training Loss: 0.9994059945407666\n",
      "Epoch 2, Training Loss: 0.9507164680868163\n",
      "Epoch 3, Training Loss: 0.9377022283417838\n",
      "Epoch 4, Training Loss: 0.9209582633541938\n",
      "Epoch 5, Training Loss: 0.9005216948968128\n",
      "Epoch 6, Training Loss: 0.8795335206770359\n",
      "Epoch 7, Training Loss: 0.8598738737572404\n",
      "Epoch 8, Training Loss: 0.8447157708325781\n",
      "Epoch 9, Training Loss: 0.8334738110241137\n",
      "Epoch 10, Training Loss: 0.8255844338495929\n",
      "Epoch 11, Training Loss: 0.8200658573243851\n",
      "Epoch 12, Training Loss: 0.8168698530448111\n",
      "Epoch 13, Training Loss: 0.8138396335723704\n",
      "Epoch 14, Training Loss: 0.8128788550097243\n",
      "Epoch 15, Training Loss: 0.8122646001048555\n",
      "Epoch 16, Training Loss: 0.8110173107089853\n",
      "Epoch 17, Training Loss: 0.8100304816898547\n",
      "Epoch 18, Training Loss: 0.809493893429749\n",
      "Epoch 19, Training Loss: 0.8087857714272979\n",
      "Epoch 20, Training Loss: 0.8085766729555632\n",
      "Epoch 21, Training Loss: 0.8080416942001285\n",
      "Epoch 22, Training Loss: 0.8080943230399512\n",
      "Epoch 23, Training Loss: 0.8080085634288932\n",
      "Epoch 24, Training Loss: 0.8069073336016863\n",
      "Epoch 25, Training Loss: 0.8072227346269708\n",
      "Epoch 26, Training Loss: 0.8069596972680629\n",
      "Epoch 27, Training Loss: 0.8065156873007466\n",
      "Epoch 28, Training Loss: 0.8060350244206593\n",
      "Epoch 29, Training Loss: 0.8054768816869061\n",
      "Epoch 30, Training Loss: 0.8056115621014646\n",
      "Epoch 31, Training Loss: 0.8055838011261216\n",
      "Epoch 32, Training Loss: 0.8055061741879112\n",
      "Epoch 33, Training Loss: 0.8049040339943162\n",
      "Epoch 34, Training Loss: 0.8047725575310843\n",
      "Epoch 35, Training Loss: 0.8048413844933188\n",
      "Epoch 36, Training Loss: 0.8049233202647446\n",
      "Epoch 37, Training Loss: 0.8043157057654589\n",
      "Epoch 38, Training Loss: 0.8043047404827032\n",
      "Epoch 39, Training Loss: 0.8041112469551258\n",
      "Epoch 40, Training Loss: 0.8041035204901731\n",
      "Epoch 41, Training Loss: 0.804193914115877\n",
      "Epoch 42, Training Loss: 0.8038906602931202\n",
      "Epoch 43, Training Loss: 0.8036971012452492\n",
      "Epoch 44, Training Loss: 0.8033061761605113\n",
      "Epoch 45, Training Loss: 0.8031954198851621\n",
      "Epoch 46, Training Loss: 0.8028302903462173\n",
      "Epoch 47, Training Loss: 0.8033538060977047\n",
      "Epoch 48, Training Loss: 0.8025567675891675\n",
      "Epoch 49, Training Loss: 0.8025412207259272\n",
      "Epoch 50, Training Loss: 0.8032767728755349\n",
      "Epoch 51, Training Loss: 0.8025914182340292\n",
      "Epoch 52, Training Loss: 0.8021023470656317\n",
      "Epoch 53, Training Loss: 0.8016839351869167\n",
      "Epoch 54, Training Loss: 0.8018236126218523\n",
      "Epoch 55, Training Loss: 0.8022434940015463\n",
      "Epoch 56, Training Loss: 0.8018111742528757\n",
      "Epoch 57, Training Loss: 0.801900363326969\n",
      "Epoch 58, Training Loss: 0.8012677734059499\n",
      "Epoch 59, Training Loss: 0.8008363948728805\n",
      "Epoch 60, Training Loss: 0.8010712364562472\n",
      "Epoch 61, Training Loss: 0.8012588111081518\n",
      "Epoch 62, Training Loss: 0.8009451477151168\n",
      "Epoch 63, Training Loss: 0.8013027101531065\n",
      "Epoch 64, Training Loss: 0.8004612764917818\n",
      "Epoch 65, Training Loss: 0.8007079871077286\n",
      "Epoch 66, Training Loss: 0.8006649813257662\n",
      "Epoch 67, Training Loss: 0.800484756061009\n",
      "Epoch 68, Training Loss: 0.800338924200015\n",
      "Epoch 69, Training Loss: 0.8008795415548454\n",
      "Epoch 70, Training Loss: 0.8001435632992507\n",
      "Epoch 71, Training Loss: 0.8001873821244204\n",
      "Epoch 72, Training Loss: 0.800076290987488\n",
      "Epoch 73, Training Loss: 0.799943158769966\n",
      "Epoch 74, Training Loss: 0.7997960980673482\n",
      "Epoch 75, Training Loss: 0.7998173450168811\n",
      "Epoch 76, Training Loss: 0.8012554279843668\n",
      "Epoch 77, Training Loss: 0.7995974664401291\n",
      "Epoch 78, Training Loss: 0.7994531759642121\n",
      "Epoch 79, Training Loss: 0.7995412952021549\n",
      "Epoch 80, Training Loss: 0.7993624275788329\n",
      "Epoch 81, Training Loss: 0.7995796856127287\n",
      "Epoch 82, Training Loss: 0.7986419078998996\n",
      "Epoch 83, Training Loss: 0.7995611531393868\n",
      "Epoch 84, Training Loss: 0.798836608578388\n",
      "Epoch 85, Training Loss: 0.7992820380325604\n",
      "Epoch 86, Training Loss: 0.7990591663166993\n",
      "Epoch 87, Training Loss: 0.7984185887906784\n",
      "Epoch 88, Training Loss: 0.7990459945865144\n",
      "Epoch 89, Training Loss: 0.7992062507715441\n",
      "Epoch 90, Training Loss: 0.7983226136157388\n",
      "Epoch 91, Training Loss: 0.7992450068767806\n",
      "Epoch 92, Training Loss: 0.7989580981713489\n",
      "Epoch 93, Training Loss: 0.7987182047134055\n",
      "Epoch 94, Training Loss: 0.7989627411490993\n",
      "Epoch 95, Training Loss: 0.7982885206552376\n",
      "Epoch 96, Training Loss: 0.798425651313667\n",
      "Epoch 97, Training Loss: 0.7984151199347991\n",
      "Epoch 98, Training Loss: 0.7986556653689622\n",
      "Epoch 99, Training Loss: 0.7981839723156807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:21:14,548] Trial 415 finished with value: 0.635 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7977951885166025\n",
      "Epoch 1, Training Loss: 0.9763543291199476\n",
      "Epoch 2, Training Loss: 0.927271776988094\n",
      "Epoch 3, Training Loss: 0.9144534463272955\n",
      "Epoch 4, Training Loss: 0.9041984912148096\n",
      "Epoch 5, Training Loss: 0.8928126984969118\n",
      "Epoch 6, Training Loss: 0.8819458117162374\n",
      "Epoch 7, Training Loss: 0.8704254372675616\n",
      "Epoch 8, Training Loss: 0.8582108570220776\n",
      "Epoch 9, Training Loss: 0.8463430856403552\n",
      "Epoch 10, Training Loss: 0.8358631853770493\n",
      "Epoch 11, Training Loss: 0.8276608957383865\n",
      "Epoch 12, Training Loss: 0.8212804658968645\n",
      "Epoch 13, Training Loss: 0.8163455396666562\n",
      "Epoch 14, Training Loss: 0.8126608293755611\n",
      "Epoch 15, Training Loss: 0.8102828916750456\n",
      "Epoch 16, Training Loss: 0.8090043464101346\n",
      "Epoch 17, Training Loss: 0.8063793892250921\n",
      "Epoch 18, Training Loss: 0.8061354241873089\n",
      "Epoch 19, Training Loss: 0.8048749711280478\n",
      "Epoch 20, Training Loss: 0.8040675073637998\n",
      "Epoch 21, Training Loss: 0.8040937841386724\n",
      "Epoch 22, Training Loss: 0.8033389865903926\n",
      "Epoch 23, Training Loss: 0.8029378412361432\n",
      "Epoch 24, Training Loss: 0.8030815536814525\n",
      "Epoch 25, Training Loss: 0.8021038478478453\n",
      "Epoch 26, Training Loss: 0.8020560631178375\n",
      "Epoch 27, Training Loss: 0.8018967502099231\n",
      "Epoch 28, Training Loss: 0.8016116792098024\n",
      "Epoch 29, Training Loss: 0.8014565620207249\n",
      "Epoch 30, Training Loss: 0.8012633369381267\n",
      "Epoch 31, Training Loss: 0.8011520373193841\n",
      "Epoch 32, Training Loss: 0.800649448803493\n",
      "Epoch 33, Training Loss: 0.8003896751798185\n",
      "Epoch 34, Training Loss: 0.8008919487322184\n",
      "Epoch 35, Training Loss: 0.8005077838897705\n",
      "Epoch 36, Training Loss: 0.8000948830654747\n",
      "Epoch 37, Training Loss: 0.800352178121868\n",
      "Epoch 38, Training Loss: 0.7993992560788205\n",
      "Epoch 39, Training Loss: 0.7995696848496459\n",
      "Epoch 40, Training Loss: 0.7993779056054309\n",
      "Epoch 41, Training Loss: 0.8001605132468661\n",
      "Epoch 42, Training Loss: 0.7994597700305451\n",
      "Epoch 43, Training Loss: 0.7992463163863447\n",
      "Epoch 44, Training Loss: 0.7986593441855638\n",
      "Epoch 45, Training Loss: 0.7986851323816113\n",
      "Epoch 46, Training Loss: 0.7987313863926364\n",
      "Epoch 47, Training Loss: 0.7984241332326617\n",
      "Epoch 48, Training Loss: 0.7990460183387412\n",
      "Epoch 49, Training Loss: 0.7988402753844297\n",
      "Epoch 50, Training Loss: 0.7976948471893942\n",
      "Epoch 51, Training Loss: 0.7981910443843756\n",
      "Epoch 52, Training Loss: 0.7979634870263866\n",
      "Epoch 53, Training Loss: 0.798344537996708\n",
      "Epoch 54, Training Loss: 0.7981182420164122\n",
      "Epoch 55, Training Loss: 0.7980980005479397\n",
      "Epoch 56, Training Loss: 0.7980753082081787\n",
      "Epoch 57, Training Loss: 0.7977288868194236\n",
      "Epoch 58, Training Loss: 0.7975932101558025\n",
      "Epoch 59, Training Loss: 0.7972892633954385\n",
      "Epoch 60, Training Loss: 0.7972253092249534\n",
      "Epoch 61, Training Loss: 0.7978453193392072\n",
      "Epoch 62, Training Loss: 0.7974317835685902\n",
      "Epoch 63, Training Loss: 0.7971327632889712\n",
      "Epoch 64, Training Loss: 0.7967200390826491\n",
      "Epoch 65, Training Loss: 0.7970970904020439\n",
      "Epoch 66, Training Loss: 0.7966794531148179\n",
      "Epoch 67, Training Loss: 0.7963704368225614\n",
      "Epoch 68, Training Loss: 0.7967183078142037\n",
      "Epoch 69, Training Loss: 0.7973310032285246\n",
      "Epoch 70, Training Loss: 0.7966829661139868\n",
      "Epoch 71, Training Loss: 0.7969917438980332\n",
      "Epoch 72, Training Loss: 0.7963502039586691\n",
      "Epoch 73, Training Loss: 0.7968799334719665\n",
      "Epoch 74, Training Loss: 0.7965895735231557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:22:17,309] Trial 416 finished with value: 0.6346666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.795915934421066\n",
      "Epoch 1, Training Loss: 0.9974276140156915\n",
      "Epoch 2, Training Loss: 0.9255413585550645\n",
      "Epoch 3, Training Loss: 0.9062785730642431\n",
      "Epoch 4, Training Loss: 0.8819493008361143\n",
      "Epoch 5, Training Loss: 0.8479952355693369\n",
      "Epoch 6, Training Loss: 0.8214633231303271\n",
      "Epoch 7, Training Loss: 0.8108951577018289\n",
      "Epoch 8, Training Loss: 0.8071537275875316\n",
      "Epoch 9, Training Loss: 0.8052666740557727\n",
      "Epoch 10, Training Loss: 0.8040938245548922\n",
      "Epoch 11, Training Loss: 0.8025397856095258\n",
      "Epoch 12, Training Loss: 0.8020504579123329\n",
      "Epoch 13, Training Loss: 0.8016518212767209\n",
      "Epoch 14, Training Loss: 0.8006573632184197\n",
      "Epoch 15, Training Loss: 0.7997831435764537\n",
      "Epoch 16, Training Loss: 0.7993049043767593\n",
      "Epoch 17, Training Loss: 0.7993978638508741\n",
      "Epoch 18, Training Loss: 0.7984514488192166\n",
      "Epoch 19, Training Loss: 0.7983056910598979\n",
      "Epoch 20, Training Loss: 0.7972815791999592\n",
      "Epoch 21, Training Loss: 0.7970839765492608\n",
      "Epoch 22, Training Loss: 0.7968822207170374\n",
      "Epoch 23, Training Loss: 0.7964967625281391\n",
      "Epoch 24, Training Loss: 0.7959663178640254\n",
      "Epoch 25, Training Loss: 0.7956711554527283\n",
      "Epoch 26, Training Loss: 0.7951882100105285\n",
      "Epoch 27, Training Loss: 0.7946423511645373\n",
      "Epoch 28, Training Loss: 0.7942326292570899\n",
      "Epoch 29, Training Loss: 0.7939847035969005\n",
      "Epoch 30, Training Loss: 0.7937192099935868\n",
      "Epoch 31, Training Loss: 0.7931952498940861\n",
      "Epoch 32, Training Loss: 0.7927052708233104\n",
      "Epoch 33, Training Loss: 0.7925104674170999\n",
      "Epoch 34, Training Loss: 0.7921808240694158\n",
      "Epoch 35, Training Loss: 0.7913453395226423\n",
      "Epoch 36, Training Loss: 0.7912331074125627\n",
      "Epoch 37, Training Loss: 0.79087880190681\n",
      "Epoch 38, Training Loss: 0.7904052844468286\n",
      "Epoch 39, Training Loss: 0.7901266256500693\n",
      "Epoch 40, Training Loss: 0.7895518725759842\n",
      "Epoch 41, Training Loss: 0.7893557095527649\n",
      "Epoch 42, Training Loss: 0.7889952157525455\n",
      "Epoch 43, Training Loss: 0.7887781319898718\n",
      "Epoch 44, Training Loss: 0.7884920602686265\n",
      "Epoch 45, Training Loss: 0.7880671618966495\n",
      "Epoch 46, Training Loss: 0.7877639013178208\n",
      "Epoch 47, Training Loss: 0.7873512427245869\n",
      "Epoch 48, Training Loss: 0.7870598724309136\n",
      "Epoch 49, Training Loss: 0.7868558257467606\n",
      "Epoch 50, Training Loss: 0.7867432507346658\n",
      "Epoch 51, Training Loss: 0.7863644974371966\n",
      "Epoch 52, Training Loss: 0.7860671357547535\n",
      "Epoch 53, Training Loss: 0.786152888396207\n",
      "Epoch 54, Training Loss: 0.785803821367376\n",
      "Epoch 55, Training Loss: 0.7856375473387102\n",
      "Epoch 56, Training Loss: 0.7854928745241726\n",
      "Epoch 57, Training Loss: 0.7854312044732711\n",
      "Epoch 58, Training Loss: 0.7852854213293861\n",
      "Epoch 59, Training Loss: 0.7850652725556317\n",
      "Epoch 60, Training Loss: 0.7846686029434204\n",
      "Epoch 61, Training Loss: 0.7847839152111726\n",
      "Epoch 62, Training Loss: 0.7845417789150687\n",
      "Epoch 63, Training Loss: 0.7842773510428036\n",
      "Epoch 64, Training Loss: 0.7843801858845879\n",
      "Epoch 65, Training Loss: 0.7842849885014926\n",
      "Epoch 66, Training Loss: 0.7838683832392973\n",
      "Epoch 67, Training Loss: 0.7840405863874099\n",
      "Epoch 68, Training Loss: 0.7841184600661782\n",
      "Epoch 69, Training Loss: 0.7838057823040906\n",
      "Epoch 70, Training Loss: 0.7835558938980103\n",
      "Epoch 71, Training Loss: 0.7834901205231162\n",
      "Epoch 72, Training Loss: 0.7834839149783639\n",
      "Epoch 73, Training Loss: 0.7833223141642178\n",
      "Epoch 74, Training Loss: 0.7834162728225483\n",
      "Epoch 75, Training Loss: 0.7831093153532813\n",
      "Epoch 76, Training Loss: 0.783113930646111\n",
      "Epoch 77, Training Loss: 0.7829765177474303\n",
      "Epoch 78, Training Loss: 0.7829631109798656\n",
      "Epoch 79, Training Loss: 0.7829636289792903\n",
      "Epoch 80, Training Loss: 0.7827435516609865\n",
      "Epoch 81, Training Loss: 0.782755854620653\n",
      "Epoch 82, Training Loss: 0.7827511478171629\n",
      "Epoch 83, Training Loss: 0.7825419979235705\n",
      "Epoch 84, Training Loss: 0.7824773261827581\n",
      "Epoch 85, Training Loss: 0.7824166615570293\n",
      "Epoch 86, Training Loss: 0.7822313832535464\n",
      "Epoch 87, Training Loss: 0.7822074946936439\n",
      "Epoch 88, Training Loss: 0.7822274069926318\n",
      "Epoch 89, Training Loss: 0.7820475398091709\n",
      "Epoch 90, Training Loss: 0.7821735133143032\n",
      "Epoch 91, Training Loss: 0.7820452237129212\n",
      "Epoch 92, Training Loss: 0.7818292686518501\n",
      "Epoch 93, Training Loss: 0.7817503880753237\n",
      "Epoch 94, Training Loss: 0.7817346877911512\n",
      "Epoch 95, Training Loss: 0.7819510120504043\n",
      "Epoch 96, Training Loss: 0.7818259032333599\n",
      "Epoch 97, Training Loss: 0.7815874845841352\n",
      "Epoch 98, Training Loss: 0.7814787988802966\n",
      "Epoch 99, Training Loss: 0.7816912836888257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:24:06,592] Trial 417 finished with value: 0.6400666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7814445062244639\n",
      "Epoch 1, Training Loss: 0.8417336099129871\n",
      "Epoch 2, Training Loss: 0.814924451014153\n",
      "Epoch 3, Training Loss: 0.8113192325247858\n",
      "Epoch 4, Training Loss: 0.8108091766224768\n",
      "Epoch 5, Training Loss: 0.8094799913858113\n",
      "Epoch 6, Training Loss: 0.8075933815841388\n",
      "Epoch 7, Training Loss: 0.8060658508673646\n",
      "Epoch 8, Training Loss: 0.8063851251638026\n",
      "Epoch 9, Training Loss: 0.8060694714237873\n",
      "Epoch 10, Training Loss: 0.8060935780966192\n",
      "Epoch 11, Training Loss: 0.8055409547081567\n",
      "Epoch 12, Training Loss: 0.8038766246989257\n",
      "Epoch 13, Training Loss: 0.8036450847647244\n",
      "Epoch 14, Training Loss: 0.803054328161971\n",
      "Epoch 15, Training Loss: 0.8037902503085316\n",
      "Epoch 16, Training Loss: 0.8020464155010711\n",
      "Epoch 17, Training Loss: 0.8031759783737642\n",
      "Epoch 18, Training Loss: 0.8022855675310121\n",
      "Epoch 19, Training Loss: 0.8018878943938061\n",
      "Epoch 20, Training Loss: 0.8033275021646256\n",
      "Epoch 21, Training Loss: 0.8016735764374411\n",
      "Epoch 22, Training Loss: 0.8040724415528147\n",
      "Epoch 23, Training Loss: 0.803829896898198\n",
      "Epoch 24, Training Loss: 0.801205583353688\n",
      "Epoch 25, Training Loss: 0.8008424015869772\n",
      "Epoch 26, Training Loss: 0.8006258802306383\n",
      "Epoch 27, Training Loss: 0.8008444046615658\n",
      "Epoch 28, Training Loss: 0.8001474916486812\n",
      "Epoch 29, Training Loss: 0.8005125894582361\n",
      "Epoch 30, Training Loss: 0.8013382517305532\n",
      "Epoch 31, Training Loss: 0.7996699815405939\n",
      "Epoch 32, Training Loss: 0.7999377014941739\n",
      "Epoch 33, Training Loss: 0.8003908698720144\n",
      "Epoch 34, Training Loss: 0.8002262643405369\n",
      "Epoch 35, Training Loss: 0.7993819676843801\n",
      "Epoch 36, Training Loss: 0.79976494666329\n",
      "Epoch 37, Training Loss: 0.7993686945814835\n",
      "Epoch 38, Training Loss: 0.8004685552496659\n",
      "Epoch 39, Training Loss: 0.8001310951727674\n",
      "Epoch 40, Training Loss: 0.7985521110376917\n",
      "Epoch 41, Training Loss: 0.7997204029470458\n",
      "Epoch 42, Training Loss: 0.7978066063465032\n",
      "Epoch 43, Training Loss: 0.7990748948620674\n",
      "Epoch 44, Training Loss: 0.7995572114349307\n",
      "Epoch 45, Training Loss: 0.7993637450655601\n",
      "Epoch 46, Training Loss: 0.7988988054426093\n",
      "Epoch 47, Training Loss: 0.7997625488087647\n",
      "Epoch 48, Training Loss: 0.7990895309842618\n",
      "Epoch 49, Training Loss: 0.7976527447987319\n",
      "Epoch 50, Training Loss: 0.7970159962661284\n",
      "Epoch 51, Training Loss: 0.7977579063938972\n",
      "Epoch 52, Training Loss: 0.7986489223358326\n",
      "Epoch 53, Training Loss: 0.7970606826301804\n",
      "Epoch 54, Training Loss: 0.7971550942363596\n",
      "Epoch 55, Training Loss: 0.7977905169465488\n",
      "Epoch 56, Training Loss: 0.7994642349114095\n",
      "Epoch 57, Training Loss: 0.7984766040529524\n",
      "Epoch 58, Training Loss: 0.7974397940743239\n",
      "Epoch 59, Training Loss: 0.7979761257207483\n",
      "Epoch 60, Training Loss: 0.7980523822899152\n",
      "Epoch 61, Training Loss: 0.7988822790913116\n",
      "Epoch 62, Training Loss: 0.7983984361913867\n",
      "Epoch 63, Training Loss: 0.7980580167662829\n",
      "Epoch 64, Training Loss: 0.7975777596459352\n",
      "Epoch 65, Training Loss: 0.7990327317015569\n",
      "Epoch 66, Training Loss: 0.7981945926085451\n",
      "Epoch 67, Training Loss: 0.7981284811084431\n",
      "Epoch 68, Training Loss: 0.7987262058078795\n",
      "Epoch 69, Training Loss: 0.7966902526697718\n",
      "Epoch 70, Training Loss: 0.796997963127337\n",
      "Epoch 71, Training Loss: 0.7978863431098766\n",
      "Epoch 72, Training Loss: 0.7965914459156811\n",
      "Epoch 73, Training Loss: 0.7956665993633126\n",
      "Epoch 74, Training Loss: 0.7974897591691268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:25:21,393] Trial 418 finished with value: 0.6293333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.798221046404731\n",
      "Epoch 1, Training Loss: 1.0907819799815908\n",
      "Epoch 2, Training Loss: 1.076838334307951\n",
      "Epoch 3, Training Loss: 1.0703610616571764\n",
      "Epoch 4, Training Loss: 1.0642475081892575\n",
      "Epoch 5, Training Loss: 1.0584705432723551\n",
      "Epoch 6, Training Loss: 1.0529855205031002\n",
      "Epoch 7, Training Loss: 1.0477463189293357\n",
      "Epoch 8, Training Loss: 1.042799482275458\n",
      "Epoch 9, Training Loss: 1.038070887116825\n",
      "Epoch 10, Training Loss: 1.033561747564989\n",
      "Epoch 11, Training Loss: 1.0292639217657202\n",
      "Epoch 12, Training Loss: 1.0251739389756147\n",
      "Epoch 13, Training Loss: 1.0212612933271072\n",
      "Epoch 14, Training Loss: 1.0175448517238392\n",
      "Epoch 15, Training Loss: 1.0140132080807405\n",
      "Epoch 16, Training Loss: 1.0106463980674745\n",
      "Epoch 17, Training Loss: 1.0074453914866728\n",
      "Epoch 18, Training Loss: 1.004424553197973\n",
      "Epoch 19, Training Loss: 1.0015493814384235\n",
      "Epoch 20, Training Loss: 0.9988264696037068\n",
      "Epoch 21, Training Loss: 0.9962337036693797\n",
      "Epoch 22, Training Loss: 0.9938240540027619\n",
      "Epoch 23, Training Loss: 0.9915201976720025\n",
      "Epoch 24, Training Loss: 0.9893413151712979\n",
      "Epoch 25, Training Loss: 0.987302787584417\n",
      "Epoch 26, Training Loss: 0.9853522652738235\n",
      "Epoch 27, Training Loss: 0.9835774523370406\n",
      "Epoch 28, Training Loss: 0.9818459423850564\n",
      "Epoch 29, Training Loss: 0.9802571487426758\n",
      "Epoch 30, Training Loss: 0.978757016027675\n",
      "Epoch 31, Training Loss: 0.9773390010525198\n",
      "Epoch 32, Training Loss: 0.9760312413468081\n",
      "Epoch 33, Training Loss: 0.9747899701314814\n",
      "Epoch 34, Training Loss: 0.9736129837176379\n",
      "Epoch 35, Training Loss: 0.9725437669894275\n",
      "Epoch 36, Training Loss: 0.9715223522046033\n",
      "Epoch 37, Training Loss: 0.970582444597693\n",
      "Epoch 38, Training Loss: 0.9696843549784492\n",
      "Epoch 39, Training Loss: 0.9688711374647477\n",
      "Epoch 40, Training Loss: 0.9680864540969625\n",
      "Epoch 41, Training Loss: 0.9673515363300548\n",
      "Epoch 42, Training Loss: 0.9666663165653453\n",
      "Epoch 43, Training Loss: 0.9660503092232873\n",
      "Epoch 44, Training Loss: 0.965428616439595\n",
      "Epoch 45, Training Loss: 0.9648565833708819\n",
      "Epoch 46, Training Loss: 0.9643552479323219\n",
      "Epoch 47, Training Loss: 0.9638827260101542\n",
      "Epoch 48, Training Loss: 0.9634175009587231\n",
      "Epoch 49, Training Loss: 0.9629848372235018\n",
      "Epoch 50, Training Loss: 0.9625787803706001\n",
      "Epoch 51, Training Loss: 0.9621932629276725\n",
      "Epoch 52, Training Loss: 0.9618359941594741\n",
      "Epoch 53, Training Loss: 0.9614978531528922\n",
      "Epoch 54, Training Loss: 0.9611682148540721\n",
      "Epoch 55, Training Loss: 0.9608625583087697\n",
      "Epoch 56, Training Loss: 0.96058933671783\n",
      "Epoch 57, Training Loss: 0.9603206337900723\n",
      "Epoch 58, Training Loss: 0.9600498520626741\n",
      "Epoch 59, Training Loss: 0.959799263827941\n",
      "Epoch 60, Training Loss: 0.9595819619122673\n",
      "Epoch 61, Training Loss: 0.9593667207745945\n",
      "Epoch 62, Training Loss: 0.959140248228522\n",
      "Epoch 63, Training Loss: 0.9589483020586126\n",
      "Epoch 64, Training Loss: 0.9587524710683262\n",
      "Epoch 65, Training Loss: 0.9585690412801855\n",
      "Epoch 66, Training Loss: 0.9583649749615614\n",
      "Epoch 67, Training Loss: 0.9581953193159665\n",
      "Epoch 68, Training Loss: 0.9580404327897465\n",
      "Epoch 69, Training Loss: 0.9578823278230779\n",
      "Epoch 70, Training Loss: 0.9577377538120045\n",
      "Epoch 71, Training Loss: 0.9575711610036738\n",
      "Epoch 72, Training Loss: 0.9574358663138222\n",
      "Epoch 73, Training Loss: 0.9572968703858993\n",
      "Epoch 74, Training Loss: 0.957145704872468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:26:32,102] Trial 419 finished with value: 0.5295333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9570073167716756\n",
      "Epoch 1, Training Loss: 0.8758528392117723\n",
      "Epoch 2, Training Loss: 0.8281425025230064\n",
      "Epoch 3, Training Loss: 0.8198917851412206\n",
      "Epoch 4, Training Loss: 0.8147329606507954\n",
      "Epoch 5, Training Loss: 0.8127334332107601\n",
      "Epoch 6, Training Loss: 0.8094350130934465\n",
      "Epoch 7, Training Loss: 0.8086330553642789\n",
      "Epoch 8, Training Loss: 0.8074780754576948\n",
      "Epoch 9, Training Loss: 0.8067519884360465\n",
      "Epoch 10, Training Loss: 0.8037246504224332\n",
      "Epoch 11, Training Loss: 0.8045654409810117\n",
      "Epoch 12, Training Loss: 0.8036911037631501\n",
      "Epoch 13, Training Loss: 0.8039616030857976\n",
      "Epoch 14, Training Loss: 0.8026350496406842\n",
      "Epoch 15, Training Loss: 0.801751154437101\n",
      "Epoch 16, Training Loss: 0.8016838948529466\n",
      "Epoch 17, Training Loss: 0.801677958230327\n",
      "Epoch 18, Training Loss: 0.8002168103268272\n",
      "Epoch 19, Training Loss: 0.801336033362195\n",
      "Epoch 20, Training Loss: 0.8009923448239951\n",
      "Epoch 21, Training Loss: 0.8000524757499982\n",
      "Epoch 22, Training Loss: 0.799792766750307\n",
      "Epoch 23, Training Loss: 0.7997338659781262\n",
      "Epoch 24, Training Loss: 0.800368146519912\n",
      "Epoch 25, Training Loss: 0.8003576477667442\n",
      "Epoch 26, Training Loss: 0.7991426022429216\n",
      "Epoch 27, Training Loss: 0.7988673114238825\n",
      "Epoch 28, Training Loss: 0.797224683958785\n",
      "Epoch 29, Training Loss: 0.7981611231215915\n",
      "Epoch 30, Training Loss: 0.7982437797955104\n",
      "Epoch 31, Training Loss: 0.7990438403043532\n",
      "Epoch 32, Training Loss: 0.7983821077454359\n",
      "Epoch 33, Training Loss: 0.7977247989267335\n",
      "Epoch 34, Training Loss: 0.7972425103187561\n",
      "Epoch 35, Training Loss: 0.7976994654289762\n",
      "Epoch 36, Training Loss: 0.7974673679000452\n",
      "Epoch 37, Training Loss: 0.7971170307998371\n",
      "Epoch 38, Training Loss: 0.7961540016016566\n",
      "Epoch 39, Training Loss: 0.7965012232163795\n",
      "Epoch 40, Training Loss: 0.7971248098782131\n",
      "Epoch 41, Training Loss: 0.7966585250277268\n",
      "Epoch 42, Training Loss: 0.7964069428300499\n",
      "Epoch 43, Training Loss: 0.7977099407884412\n",
      "Epoch 44, Training Loss: 0.795914049704272\n",
      "Epoch 45, Training Loss: 0.7964264580181667\n",
      "Epoch 46, Training Loss: 0.7961494381266428\n",
      "Epoch 47, Training Loss: 0.7947824721945855\n",
      "Epoch 48, Training Loss: 0.7956028547502102\n",
      "Epoch 49, Training Loss: 0.7955780102794332\n",
      "Epoch 50, Training Loss: 0.7954350772656893\n",
      "Epoch 51, Training Loss: 0.7949276726048692\n",
      "Epoch 52, Training Loss: 0.7963322740748413\n",
      "Epoch 53, Training Loss: 0.7959404909521117\n",
      "Epoch 54, Training Loss: 0.7957975667222101\n",
      "Epoch 55, Training Loss: 0.7951293939934637\n",
      "Epoch 56, Training Loss: 0.7963600840783657\n",
      "Epoch 57, Training Loss: 0.7974622244225409\n",
      "Epoch 58, Training Loss: 0.7951359398382947\n",
      "Epoch 59, Training Loss: 0.7953957409786999\n",
      "Epoch 60, Training Loss: 0.7952948917123608\n",
      "Epoch 61, Training Loss: 0.7940735595566886\n",
      "Epoch 62, Training Loss: 0.7950131592893959\n",
      "Epoch 63, Training Loss: 0.7952555620580688\n",
      "Epoch 64, Training Loss: 0.7954682902285927\n",
      "Epoch 65, Training Loss: 0.7959470604595386\n",
      "Epoch 66, Training Loss: 0.7959244840127185\n",
      "Epoch 67, Training Loss: 0.7957500165566466\n",
      "Epoch 68, Training Loss: 0.7949982237995119\n",
      "Epoch 69, Training Loss: 0.7954902473248934\n",
      "Epoch 70, Training Loss: 0.7957300844049096\n",
      "Epoch 71, Training Loss: 0.7948783553632578\n",
      "Epoch 72, Training Loss: 0.7940015881581414\n",
      "Epoch 73, Training Loss: 0.794003210928207\n",
      "Epoch 74, Training Loss: 0.7939703616880833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:27:55,531] Trial 420 finished with value: 0.573 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7947998029845101\n",
      "Epoch 1, Training Loss: 1.0041067885276966\n",
      "Epoch 2, Training Loss: 0.9601663800110495\n",
      "Epoch 3, Training Loss: 0.9506340405098478\n",
      "Epoch 4, Training Loss: 0.941194528655002\n",
      "Epoch 5, Training Loss: 0.9314189576564875\n",
      "Epoch 6, Training Loss: 0.9186745473316738\n",
      "Epoch 7, Training Loss: 0.90341628126632\n",
      "Epoch 8, Training Loss: 0.8858156715120588\n",
      "Epoch 9, Training Loss: 0.867031232665356\n",
      "Epoch 10, Training Loss: 0.8503778159170222\n",
      "Epoch 11, Training Loss: 0.8378595068042439\n",
      "Epoch 12, Training Loss: 0.828623786635865\n",
      "Epoch 13, Training Loss: 0.8225947852421523\n",
      "Epoch 14, Training Loss: 0.8188378100108383\n",
      "Epoch 15, Training Loss: 0.8159043109506593\n",
      "Epoch 16, Training Loss: 0.8145835611156951\n",
      "Epoch 17, Training Loss: 0.8135507065550726\n",
      "Epoch 18, Training Loss: 0.8118060094969614\n",
      "Epoch 19, Training Loss: 0.8124561890623623\n",
      "Epoch 20, Training Loss: 0.8114843771869975\n",
      "Epoch 21, Training Loss: 0.8098453426719608\n",
      "Epoch 22, Training Loss: 0.8090540055941818\n",
      "Epoch 23, Training Loss: 0.8087187200560606\n",
      "Epoch 24, Training Loss: 0.8085707695860612\n",
      "Epoch 25, Training Loss: 0.8082463113885177\n",
      "Epoch 26, Training Loss: 0.8073351350942053\n",
      "Epoch 27, Training Loss: 0.8067926490217223\n",
      "Epoch 28, Training Loss: 0.8066858381257022\n",
      "Epoch 29, Training Loss: 0.8062402280649744\n",
      "Epoch 30, Training Loss: 0.8064315893596277\n",
      "Epoch 31, Training Loss: 0.8056920280133871\n",
      "Epoch 32, Training Loss: 0.8058605745322722\n",
      "Epoch 33, Training Loss: 0.8046486564148637\n",
      "Epoch 34, Training Loss: 0.8044872295587583\n",
      "Epoch 35, Training Loss: 0.8039938438207583\n",
      "Epoch 36, Training Loss: 0.8035354312201192\n",
      "Epoch 37, Training Loss: 0.8033443195479256\n",
      "Epoch 38, Training Loss: 0.8029285821699559\n",
      "Epoch 39, Training Loss: 0.8030116950658928\n",
      "Epoch 40, Training Loss: 0.8019115479817068\n",
      "Epoch 41, Training Loss: 0.8025959263170572\n",
      "Epoch 42, Training Loss: 0.8026604391578445\n",
      "Epoch 43, Training Loss: 0.8013735037997253\n",
      "Epoch 44, Training Loss: 0.8011004290186373\n",
      "Epoch 45, Training Loss: 0.8015210096101115\n",
      "Epoch 46, Training Loss: 0.8013112236682634\n",
      "Epoch 47, Training Loss: 0.8004582666812983\n",
      "Epoch 48, Training Loss: 0.8014304320615037\n",
      "Epoch 49, Training Loss: 0.8004601480369281\n",
      "Epoch 50, Training Loss: 0.799886555510356\n",
      "Epoch 51, Training Loss: 0.7997728301170177\n",
      "Epoch 52, Training Loss: 0.8001405427330419\n",
      "Epoch 53, Training Loss: 0.7994417667837072\n",
      "Epoch 54, Training Loss: 0.799852350690311\n",
      "Epoch 55, Training Loss: 0.7994941486451859\n",
      "Epoch 56, Training Loss: 0.7997907091800431\n",
      "Epoch 57, Training Loss: 0.799610894515102\n",
      "Epoch 58, Training Loss: 0.7989982569128051\n",
      "Epoch 59, Training Loss: 0.7993453120826779\n",
      "Epoch 60, Training Loss: 0.79876255585735\n",
      "Epoch 61, Training Loss: 0.798742533626413\n",
      "Epoch 62, Training Loss: 0.7990041977480838\n",
      "Epoch 63, Training Loss: 0.7994983062708289\n",
      "Epoch 64, Training Loss: 0.7984073539425556\n",
      "Epoch 65, Training Loss: 0.7984864580003839\n",
      "Epoch 66, Training Loss: 0.7987514105058254\n",
      "Epoch 67, Training Loss: 0.7982659171398421\n",
      "Epoch 68, Training Loss: 0.7978842916345238\n",
      "Epoch 69, Training Loss: 0.7985688638866396\n",
      "Epoch 70, Training Loss: 0.7985913733790692\n",
      "Epoch 71, Training Loss: 0.7981808593398646\n",
      "Epoch 72, Training Loss: 0.7983242361169113\n",
      "Epoch 73, Training Loss: 0.7984057014149831\n",
      "Epoch 74, Training Loss: 0.7983887632090346\n",
      "Epoch 75, Training Loss: 0.7976055587144723\n",
      "Epoch 76, Training Loss: 0.7982422411889958\n",
      "Epoch 77, Training Loss: 0.7976122237685928\n",
      "Epoch 78, Training Loss: 0.7980272387203418\n",
      "Epoch 79, Training Loss: 0.7981024902566035\n",
      "Epoch 80, Training Loss: 0.7977401965542843\n",
      "Epoch 81, Training Loss: 0.7977729304392535\n",
      "Epoch 82, Training Loss: 0.7976610788725372\n",
      "Epoch 83, Training Loss: 0.7976644386026196\n",
      "Epoch 84, Training Loss: 0.7968791301089122\n",
      "Epoch 85, Training Loss: 0.7977461347902628\n",
      "Epoch 86, Training Loss: 0.7973133987053893\n",
      "Epoch 87, Training Loss: 0.7978789046294708\n",
      "Epoch 88, Training Loss: 0.7976991053810694\n",
      "Epoch 89, Training Loss: 0.7971594735195763\n",
      "Epoch 90, Training Loss: 0.7969452311221819\n",
      "Epoch 91, Training Loss: 0.797479192117103\n",
      "Epoch 92, Training Loss: 0.7967681889247177\n",
      "Epoch 93, Training Loss: 0.7966105997114253\n",
      "Epoch 94, Training Loss: 0.7970188001044711\n",
      "Epoch 95, Training Loss: 0.7972307142458464\n",
      "Epoch 96, Training Loss: 0.7965538215816469\n",
      "Epoch 97, Training Loss: 0.7971206341470991\n",
      "Epoch 98, Training Loss: 0.7969367702204482\n",
      "Epoch 99, Training Loss: 0.7966259018819135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:29:27,083] Trial 421 finished with value: 0.6302 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7972657922515296\n",
      "Epoch 1, Training Loss: 0.9757987553933087\n",
      "Epoch 2, Training Loss: 0.9308787457381977\n",
      "Epoch 3, Training Loss: 0.8899328658160042\n",
      "Epoch 4, Training Loss: 0.8511221421466154\n",
      "Epoch 5, Training Loss: 0.8301566476681653\n",
      "Epoch 6, Training Loss: 0.8200559115409851\n",
      "Epoch 7, Training Loss: 0.8149625147791469\n",
      "Epoch 8, Training Loss: 0.8125684188393986\n",
      "Epoch 9, Training Loss: 0.8105989123091978\n",
      "Epoch 10, Training Loss: 0.8095805350471945\n",
      "Epoch 11, Training Loss: 0.8084840484927682\n",
      "Epoch 12, Training Loss: 0.8078872486423043\n",
      "Epoch 13, Training Loss: 0.8069595501703375\n",
      "Epoch 14, Training Loss: 0.805719418665942\n",
      "Epoch 15, Training Loss: 0.8050304067836088\n",
      "Epoch 16, Training Loss: 0.8042262708439546\n",
      "Epoch 17, Training Loss: 0.8038210272789001\n",
      "Epoch 18, Training Loss: 0.8030416385566487\n",
      "Epoch 19, Training Loss: 0.8022208415760713\n",
      "Epoch 20, Training Loss: 0.8016418261387769\n",
      "Epoch 21, Training Loss: 0.8010960425348843\n",
      "Epoch 22, Training Loss: 0.8005727014822118\n",
      "Epoch 23, Training Loss: 0.8000565638261683\n",
      "Epoch 24, Training Loss: 0.7999657678604126\n",
      "Epoch 25, Training Loss: 0.7995285714373869\n",
      "Epoch 26, Training Loss: 0.7996103066556594\n",
      "Epoch 27, Training Loss: 0.7989642806614147\n",
      "Epoch 28, Training Loss: 0.7988783171597649\n",
      "Epoch 29, Training Loss: 0.7985888700625475\n",
      "Epoch 30, Training Loss: 0.7989514334061566\n",
      "Epoch 31, Training Loss: 0.7984305045885198\n",
      "Epoch 32, Training Loss: 0.7983414377885706\n",
      "Epoch 33, Training Loss: 0.7982833799895118\n",
      "Epoch 34, Training Loss: 0.7980085829426261\n",
      "Epoch 35, Training Loss: 0.7977037520268384\n",
      "Epoch 36, Training Loss: 0.7975798744313857\n",
      "Epoch 37, Training Loss: 0.7972095427793615\n",
      "Epoch 38, Training Loss: 0.7973271834850312\n",
      "Epoch 39, Training Loss: 0.7972449219928068\n",
      "Epoch 40, Training Loss: 0.7970298874378204\n",
      "Epoch 41, Training Loss: 0.7969955210124745\n",
      "Epoch 42, Training Loss: 0.7968898854536168\n",
      "Epoch 43, Training Loss: 0.7966069070731893\n",
      "Epoch 44, Training Loss: 0.7963623329471139\n",
      "Epoch 45, Training Loss: 0.7964290894480313\n",
      "Epoch 46, Training Loss: 0.7959914623288548\n",
      "Epoch 47, Training Loss: 0.7957823835400974\n",
      "Epoch 48, Training Loss: 0.795652630890117\n",
      "Epoch 49, Training Loss: 0.795401701225954\n",
      "Epoch 50, Training Loss: 0.7951698949056513\n",
      "Epoch 51, Training Loss: 0.7952356905796949\n",
      "Epoch 52, Training Loss: 0.7944413366037256\n",
      "Epoch 53, Training Loss: 0.7946446812854093\n",
      "Epoch 54, Training Loss: 0.7944298575204961\n",
      "Epoch 55, Training Loss: 0.7940747467209311\n",
      "Epoch 56, Training Loss: 0.7939451094935922\n",
      "Epoch 57, Training Loss: 0.7938361031167648\n",
      "Epoch 58, Training Loss: 0.7934541655989255\n",
      "Epoch 59, Training Loss: 0.7931211078166962\n",
      "Epoch 60, Training Loss: 0.793131545641843\n",
      "Epoch 61, Training Loss: 0.7927263456933639\n",
      "Epoch 62, Training Loss: 0.792558766042485\n",
      "Epoch 63, Training Loss: 0.7922218657241148\n",
      "Epoch 64, Training Loss: 0.7918966951790978\n",
      "Epoch 65, Training Loss: 0.7914846838221831\n",
      "Epoch 66, Training Loss: 0.7913430358381832\n",
      "Epoch 67, Training Loss: 0.7911660552024842\n",
      "Epoch 68, Training Loss: 0.7908131842052235\n",
      "Epoch 69, Training Loss: 0.7907251640628365\n",
      "Epoch 70, Training Loss: 0.7900901233448702\n",
      "Epoch 71, Training Loss: 0.78999999053338\n",
      "Epoch 72, Training Loss: 0.7896462298140806\n",
      "Epoch 73, Training Loss: 0.7895754403226516\n",
      "Epoch 74, Training Loss: 0.7891245094467612\n",
      "Epoch 75, Training Loss: 0.7890696017882404\n",
      "Epoch 76, Training Loss: 0.7888139329237096\n",
      "Epoch 77, Training Loss: 0.7884649042522206\n",
      "Epoch 78, Training Loss: 0.7882157663738026\n",
      "Epoch 79, Training Loss: 0.7881519987302668\n",
      "Epoch 80, Training Loss: 0.7876771451445187\n",
      "Epoch 81, Training Loss: 0.787542085016475\n",
      "Epoch 82, Training Loss: 0.787503431264092\n",
      "Epoch 83, Training Loss: 0.7875093729355755\n",
      "Epoch 84, Training Loss: 0.7871236836209017\n",
      "Epoch 85, Training Loss: 0.7866297056394465\n",
      "Epoch 86, Training Loss: 0.786513217827853\n",
      "Epoch 87, Training Loss: 0.7867305951258715\n",
      "Epoch 88, Training Loss: 0.7864605668713065\n",
      "Epoch 89, Training Loss: 0.7861443462091334\n",
      "Epoch 90, Training Loss: 0.786057593051125\n",
      "Epoch 91, Training Loss: 0.7857404722185696\n",
      "Epoch 92, Training Loss: 0.7858144414424896\n",
      "Epoch 93, Training Loss: 0.7855639172301573\n",
      "Epoch 94, Training Loss: 0.7855746131083544\n",
      "Epoch 95, Training Loss: 0.7855421773125144\n",
      "Epoch 96, Training Loss: 0.7852132692757775\n",
      "Epoch 97, Training Loss: 0.7851013716529397\n",
      "Epoch 98, Training Loss: 0.7846736838537104\n",
      "Epoch 99, Training Loss: 0.7849684398314533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:31:16,934] Trial 422 finished with value: 0.6396 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7848728944974787\n",
      "Epoch 1, Training Loss: 0.8443165771400227\n",
      "Epoch 2, Training Loss: 0.8108253945322598\n",
      "Epoch 3, Training Loss: 0.8094818269505221\n",
      "Epoch 4, Training Loss: 0.8060922368835001\n",
      "Epoch 5, Training Loss: 0.8040570292753332\n",
      "Epoch 6, Training Loss: 0.8025632580588845\n",
      "Epoch 7, Training Loss: 0.8016193243335276\n",
      "Epoch 8, Training Loss: 0.8020395665309008\n",
      "Epoch 9, Training Loss: 0.801279610675924\n",
      "Epoch 10, Training Loss: 0.8004169750213623\n",
      "Epoch 11, Training Loss: 0.8002611665164723\n",
      "Epoch 12, Training Loss: 0.8001913848344018\n",
      "Epoch 13, Training Loss: 0.799534747951171\n",
      "Epoch 14, Training Loss: 0.7995971986826729\n",
      "Epoch 15, Training Loss: 0.7984328097455642\n",
      "Epoch 16, Training Loss: 0.7983118534789366\n",
      "Epoch 17, Training Loss: 0.7989769040837007\n",
      "Epoch 18, Training Loss: 0.7990471973138698\n",
      "Epoch 19, Training Loss: 0.7988485291424919\n",
      "Epoch 20, Training Loss: 0.7985790151708266\n",
      "Epoch 21, Training Loss: 0.7979553846751942\n",
      "Epoch 22, Training Loss: 0.7993288024032817\n",
      "Epoch 23, Training Loss: 0.7977690601348877\n",
      "Epoch 24, Training Loss: 0.7976653319246628\n",
      "Epoch 25, Training Loss: 0.797592358939788\n",
      "Epoch 26, Training Loss: 0.7973976917827831\n",
      "Epoch 27, Training Loss: 0.7976886460360358\n",
      "Epoch 28, Training Loss: 0.7974858810621149\n",
      "Epoch 29, Training Loss: 0.7970054290575139\n",
      "Epoch 30, Training Loss: 0.7965228976922877\n",
      "Epoch 31, Training Loss: 0.7966286899061764\n",
      "Epoch 32, Training Loss: 0.7963650892061346\n",
      "Epoch 33, Training Loss: 0.7958349756633534\n",
      "Epoch 34, Training Loss: 0.7969762219400967\n",
      "Epoch 35, Training Loss: 0.7961657787070555\n",
      "Epoch 36, Training Loss: 0.7964331985221189\n",
      "Epoch 37, Training Loss: 0.7964962856909809\n",
      "Epoch 38, Training Loss: 0.7962213021867416\n",
      "Epoch 39, Training Loss: 0.7962744571180904\n",
      "Epoch 40, Training Loss: 0.7954630853849299\n",
      "Epoch 41, Training Loss: 0.7951524730289684\n",
      "Epoch 42, Training Loss: 0.7946775554909425\n",
      "Epoch 43, Training Loss: 0.7946603984692517\n",
      "Epoch 44, Training Loss: 0.7949525061074425\n",
      "Epoch 45, Training Loss: 0.7944929388691397\n",
      "Epoch 46, Training Loss: 0.7941392095649944\n",
      "Epoch 47, Training Loss: 0.7952549334834603\n",
      "Epoch 48, Training Loss: 0.7939650330824011\n",
      "Epoch 49, Training Loss: 0.7945789650608511\n",
      "Epoch 50, Training Loss: 0.7940513583491831\n",
      "Epoch 51, Training Loss: 0.794044705769595\n",
      "Epoch 52, Training Loss: 0.7938663157294779\n",
      "Epoch 53, Training Loss: 0.7939866150126738\n",
      "Epoch 54, Training Loss: 0.7937209221896003\n",
      "Epoch 55, Training Loss: 0.7934034937269547\n",
      "Epoch 56, Training Loss: 0.7931691952312694\n",
      "Epoch 57, Training Loss: 0.7935488231743083\n",
      "Epoch 58, Training Loss: 0.7926086532368379\n",
      "Epoch 59, Training Loss: 0.7937338191621444\n",
      "Epoch 60, Training Loss: 0.7935021903234369\n",
      "Epoch 61, Training Loss: 0.7934951045232661\n",
      "Epoch 62, Training Loss: 0.7933772376705619\n",
      "Epoch 63, Training Loss: 0.7932157783648547\n",
      "Epoch 64, Training Loss: 0.7932344742382273\n",
      "Epoch 65, Training Loss: 0.793000532879549\n",
      "Epoch 66, Training Loss: 0.7928437282057369\n",
      "Epoch 67, Training Loss: 0.7926580457827624\n",
      "Epoch 68, Training Loss: 0.792841254122117\n",
      "Epoch 69, Training Loss: 0.7925788091911989\n",
      "Epoch 70, Training Loss: 0.7936190643030054\n",
      "Epoch 71, Training Loss: 0.792481440445956\n",
      "Epoch 72, Training Loss: 0.7925793023670421\n",
      "Epoch 73, Training Loss: 0.7916735740970163\n",
      "Epoch 74, Training Loss: 0.7925250053405761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:32:47,493] Trial 423 finished with value: 0.6388666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7926225320731892\n",
      "Epoch 1, Training Loss: 0.8867557675698224\n",
      "Epoch 2, Training Loss: 0.8197317153566024\n",
      "Epoch 3, Training Loss: 0.8108395645898931\n",
      "Epoch 4, Training Loss: 0.8065913306264316\n",
      "Epoch 5, Training Loss: 0.8033369116923388\n",
      "Epoch 6, Training Loss: 0.8020469703393824\n",
      "Epoch 7, Training Loss: 0.7994629785593819\n",
      "Epoch 8, Training Loss: 0.798025138167774\n",
      "Epoch 9, Training Loss: 0.797138378199409\n",
      "Epoch 10, Training Loss: 0.7963821749827441\n",
      "Epoch 11, Training Loss: 0.7965315967447617\n",
      "Epoch 12, Training Loss: 0.7952478378660539\n",
      "Epoch 13, Training Loss: 0.7943480448161855\n",
      "Epoch 14, Training Loss: 0.7934311830296236\n",
      "Epoch 15, Training Loss: 0.7935797617014717\n",
      "Epoch 16, Training Loss: 0.7929204739542568\n",
      "Epoch 17, Training Loss: 0.7927035186571233\n",
      "Epoch 18, Training Loss: 0.7919804378116833\n",
      "Epoch 19, Training Loss: 0.7914752975632162\n",
      "Epoch 20, Training Loss: 0.7918716155080234\n",
      "Epoch 21, Training Loss: 0.7905499315963072\n",
      "Epoch 22, Training Loss: 0.7903452689507429\n",
      "Epoch 23, Training Loss: 0.7908542446529164\n",
      "Epoch 24, Training Loss: 0.7897027596305398\n",
      "Epoch 25, Training Loss: 0.7905983377905453\n",
      "Epoch 26, Training Loss: 0.7903409987337449\n",
      "Epoch 27, Training Loss: 0.7898026950219098\n",
      "Epoch 28, Training Loss: 0.7894877376977135\n",
      "Epoch 29, Training Loss: 0.788684249344994\n",
      "Epoch 30, Training Loss: 0.7892368018627167\n",
      "Epoch 31, Training Loss: 0.7891337891185984\n",
      "Epoch 32, Training Loss: 0.7886899012677809\n",
      "Epoch 33, Training Loss: 0.7885602366223055\n",
      "Epoch 34, Training Loss: 0.7885890461416806\n",
      "Epoch 35, Training Loss: 0.7880426976961248\n",
      "Epoch 36, Training Loss: 0.7886500837522394\n",
      "Epoch 37, Training Loss: 0.7879337658601648\n",
      "Epoch 38, Training Loss: 0.7886245075394126\n",
      "Epoch 39, Training Loss: 0.7878538546141456\n",
      "Epoch 40, Training Loss: 0.7880229705922743\n",
      "Epoch 41, Training Loss: 0.7873689117151148\n",
      "Epoch 42, Training Loss: 0.7882108271823209\n",
      "Epoch 43, Training Loss: 0.7873813693663653\n",
      "Epoch 44, Training Loss: 0.7878203288947835\n",
      "Epoch 45, Training Loss: 0.7870392096743865\n",
      "Epoch 46, Training Loss: 0.7875956475734711\n",
      "Epoch 47, Training Loss: 0.7867283148625318\n",
      "Epoch 48, Training Loss: 0.7874473313724294\n",
      "Epoch 49, Training Loss: 0.7871407726231743\n",
      "Epoch 50, Training Loss: 0.7871651582156911\n",
      "Epoch 51, Training Loss: 0.7874918605299557\n",
      "Epoch 52, Training Loss: 0.7874757227476905\n",
      "Epoch 53, Training Loss: 0.787928650449304\n",
      "Epoch 54, Training Loss: 0.787717767533134\n",
      "Epoch 55, Training Loss: 0.7881654226779937\n",
      "Epoch 56, Training Loss: 0.787847927458146\n",
      "Epoch 57, Training Loss: 0.7875430100104388\n",
      "Epoch 58, Training Loss: 0.7872619027249953\n",
      "Epoch 59, Training Loss: 0.7875835362602682\n",
      "Epoch 60, Training Loss: 0.7872824340006884\n",
      "Epoch 61, Training Loss: 0.7864747778107138\n",
      "Epoch 62, Training Loss: 0.787529386842952\n",
      "Epoch 63, Training Loss: 0.7868615319448359\n",
      "Epoch 64, Training Loss: 0.7868325537793777\n",
      "Epoch 65, Training Loss: 0.7873600908587961\n",
      "Epoch 66, Training Loss: 0.7873488706700942\n",
      "Epoch 67, Training Loss: 0.7872270955057705\n",
      "Epoch 68, Training Loss: 0.7873470128985012\n",
      "Epoch 69, Training Loss: 0.7870493142745074\n",
      "Epoch 70, Training Loss: 0.7869861372779398\n",
      "Epoch 71, Training Loss: 0.7873087882294374\n",
      "Epoch 72, Training Loss: 0.787470974220949\n",
      "Epoch 73, Training Loss: 0.787423023336074\n",
      "Epoch 74, Training Loss: 0.7872463613397935\n",
      "Epoch 75, Training Loss: 0.7873875043672673\n",
      "Epoch 76, Training Loss: 0.7871103318298565\n",
      "Epoch 77, Training Loss: 0.7873625758114983\n",
      "Epoch 78, Training Loss: 0.7885549499708063\n",
      "Epoch 79, Training Loss: 0.7882964329158558\n",
      "Epoch 80, Training Loss: 0.7877932666329777\n",
      "Epoch 81, Training Loss: 0.7878725248224595\n",
      "Epoch 82, Training Loss: 0.7879666493920718\n",
      "Epoch 83, Training Loss: 0.788766604311326\n",
      "Epoch 84, Training Loss: 0.7884671599023483\n",
      "Epoch 85, Training Loss: 0.7885868606847876\n",
      "Epoch 86, Training Loss: 0.7885996152372922\n",
      "Epoch 87, Training Loss: 0.7880278060716741\n",
      "Epoch 88, Training Loss: 0.7881105105315938\n",
      "Epoch 89, Training Loss: 0.7882280878459706\n",
      "Epoch 90, Training Loss: 0.7890891070926891\n",
      "Epoch 91, Training Loss: 0.7891171386662652\n",
      "Epoch 92, Training Loss: 0.7892808839152841\n",
      "Epoch 93, Training Loss: 0.7891949071603663\n",
      "Epoch 94, Training Loss: 0.7903909965122448\n",
      "Epoch 95, Training Loss: 0.7887562728629393\n",
      "Epoch 96, Training Loss: 0.7885936773524564\n",
      "Epoch 97, Training Loss: 0.7892530341709362\n",
      "Epoch 98, Training Loss: 0.7890746129260343\n",
      "Epoch 99, Training Loss: 0.7888571599651786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:34:57,255] Trial 424 finished with value: 0.6392 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7885737092354719\n",
      "Epoch 1, Training Loss: 0.8493060184226316\n",
      "Epoch 2, Training Loss: 0.8217236084096572\n",
      "Epoch 3, Training Loss: 0.8205505596890169\n",
      "Epoch 4, Training Loss: 0.8168501217225018\n",
      "Epoch 5, Training Loss: 0.8142096370107987\n",
      "Epoch 6, Training Loss: 0.8135645640597624\n",
      "Epoch 7, Training Loss: 0.8120296659890344\n",
      "Epoch 8, Training Loss: 0.8135698800928453\n",
      "Epoch 9, Training Loss: 0.8113860349795398\n",
      "Epoch 10, Training Loss: 0.8131017707375919\n",
      "Epoch 11, Training Loss: 0.811208564674153\n",
      "Epoch 12, Training Loss: 0.8115105456464431\n",
      "Epoch 13, Training Loss: 0.8104595838574802\n",
      "Epoch 14, Training Loss: 0.8124088709494647\n",
      "Epoch 15, Training Loss: 0.8114657704269185\n",
      "Epoch 16, Training Loss: 0.8093087567301358\n",
      "Epoch 17, Training Loss: 0.8094372806829565\n",
      "Epoch 18, Training Loss: 0.8086475050449371\n",
      "Epoch 19, Training Loss: 0.8092052851704991\n",
      "Epoch 20, Training Loss: 0.8083018795882955\n",
      "Epoch 21, Training Loss: 0.8078309988274294\n",
      "Epoch 22, Training Loss: 0.8087157378477209\n",
      "Epoch 23, Training Loss: 0.8078989256830776\n",
      "Epoch 24, Training Loss: 0.8069569936219384\n",
      "Epoch 25, Training Loss: 0.8066733629563275\n",
      "Epoch 26, Training Loss: 0.8073473243853625\n",
      "Epoch 27, Training Loss: 0.806396645377664\n",
      "Epoch 28, Training Loss: 0.8075254885589376\n",
      "Epoch 29, Training Loss: 0.8075188692878275\n",
      "Epoch 30, Training Loss: 0.8047800554247464\n",
      "Epoch 31, Training Loss: 0.8055583118691164\n",
      "Epoch 32, Training Loss: 0.8052333897702835\n",
      "Epoch 33, Training Loss: 0.8044951160515056\n",
      "Epoch 34, Training Loss: 0.8038775131281685\n",
      "Epoch 35, Training Loss: 0.8030199021451614\n",
      "Epoch 36, Training Loss: 0.8040810685298022\n",
      "Epoch 37, Training Loss: 0.8055706552898183\n",
      "Epoch 38, Training Loss: 0.8034239297053393\n",
      "Epoch 39, Training Loss: 0.8032518831421347\n",
      "Epoch 40, Training Loss: 0.8030447576326483\n",
      "Epoch 41, Training Loss: 0.8041255744765786\n",
      "Epoch 42, Training Loss: 0.8012986523964826\n",
      "Epoch 43, Training Loss: 0.8021046332752003\n",
      "Epoch 44, Training Loss: 0.8014709293842316\n",
      "Epoch 45, Training Loss: 0.8035907675939448\n",
      "Epoch 46, Training Loss: 0.8025420465890098\n",
      "Epoch 47, Training Loss: 0.8010117197036744\n",
      "Epoch 48, Training Loss: 0.8035252629308139\n",
      "Epoch 49, Training Loss: 0.8014904870706446\n",
      "Epoch 50, Training Loss: 0.8016468049498165\n",
      "Epoch 51, Training Loss: 0.8009425065797918\n",
      "Epoch 52, Training Loss: 0.8010762668357176\n",
      "Epoch 53, Training Loss: 0.8023666186893688\n",
      "Epoch 54, Training Loss: 0.7989460774029002\n",
      "Epoch 55, Training Loss: 0.8006417587925406\n",
      "Epoch 56, Training Loss: 0.802484683008755\n",
      "Epoch 57, Training Loss: 0.7997230458259582\n",
      "Epoch 58, Training Loss: 0.8009969438524808\n",
      "Epoch 59, Training Loss: 0.8003992688655853\n",
      "Epoch 60, Training Loss: 0.7996165661250844\n",
      "Epoch 61, Training Loss: 0.7995765028981602\n",
      "Epoch 62, Training Loss: 0.8004372662656447\n",
      "Epoch 63, Training Loss: 0.8003948346306295\n",
      "Epoch 64, Training Loss: 0.8005503266699174\n",
      "Epoch 65, Training Loss: 0.7990033809577718\n",
      "Epoch 66, Training Loss: 0.7998673109447255\n",
      "Epoch 67, Training Loss: 0.8004020170604481\n",
      "Epoch 68, Training Loss: 0.7997370398044586\n",
      "Epoch 69, Training Loss: 0.7995139165485606\n",
      "Epoch 70, Training Loss: 0.7990540458875544\n",
      "Epoch 71, Training Loss: 0.8004046628755682\n",
      "Epoch 72, Training Loss: 0.8010062052221859\n",
      "Epoch 73, Training Loss: 0.7997706266010509\n",
      "Epoch 74, Training Loss: 0.8000119501702926\n",
      "Epoch 75, Training Loss: 0.7995354421699749\n",
      "Epoch 76, Training Loss: 0.8006552912207211\n",
      "Epoch 77, Training Loss: 0.7982764952323016\n",
      "Epoch 78, Training Loss: 0.7988541689339806\n",
      "Epoch 79, Training Loss: 0.7986556361703312\n",
      "Epoch 80, Training Loss: 0.7983144804309397\n",
      "Epoch 81, Training Loss: 0.7990721851937911\n",
      "Epoch 82, Training Loss: 0.8002369854029487\n",
      "Epoch 83, Training Loss: 0.7975091075195986\n",
      "Epoch 84, Training Loss: 0.8002320424949422\n",
      "Epoch 85, Training Loss: 0.79762328098802\n",
      "Epoch 86, Training Loss: 0.7993351563285379\n",
      "Epoch 87, Training Loss: 0.7979537282270543\n",
      "Epoch 88, Training Loss: 0.7997190794524025\n",
      "Epoch 89, Training Loss: 0.7984453494408551\n",
      "Epoch 90, Training Loss: 0.7979440707318923\n",
      "Epoch 91, Training Loss: 0.8002093500249526\n",
      "Epoch 92, Training Loss: 0.7988279524270226\n",
      "Epoch 93, Training Loss: 0.7984678879204918\n",
      "Epoch 94, Training Loss: 0.798641826615614\n",
      "Epoch 95, Training Loss: 0.7996696401343626\n",
      "Epoch 96, Training Loss: 0.7978810432377984\n",
      "Epoch 97, Training Loss: 0.7980640827207004\n",
      "Epoch 98, Training Loss: 0.79956383543856\n",
      "Epoch 99, Training Loss: 0.7988145562480478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:36:57,494] Trial 425 finished with value: 0.6291333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7988010534819434\n",
      "Epoch 1, Training Loss: 0.9974904528237823\n",
      "Epoch 2, Training Loss: 0.8830730938373652\n",
      "Epoch 3, Training Loss: 0.8726229786872863\n",
      "Epoch 4, Training Loss: 0.8713720243676264\n",
      "Epoch 5, Training Loss: 0.8631767633266019\n",
      "Epoch 6, Training Loss: 0.8624860689156038\n",
      "Epoch 7, Training Loss: 0.8617082057142617\n",
      "Epoch 8, Training Loss: 0.8594650377008252\n",
      "Epoch 9, Training Loss: 0.8594517190653579\n",
      "Epoch 10, Training Loss: 0.8560531022853421\n",
      "Epoch 11, Training Loss: 0.8582392172705858\n",
      "Epoch 12, Training Loss: 0.8551452258475741\n",
      "Epoch 13, Training Loss: 0.8544251298545895\n",
      "Epoch 14, Training Loss: 0.8548839527861516\n",
      "Epoch 15, Training Loss: 0.8536805992735956\n",
      "Epoch 16, Training Loss: 0.8535816474964745\n",
      "Epoch 17, Training Loss: 0.8512662468996263\n",
      "Epoch 18, Training Loss: 0.8519496621045851\n",
      "Epoch 19, Training Loss: 0.8520061915530298\n",
      "Epoch 20, Training Loss: 0.8526323673420383\n",
      "Epoch 21, Training Loss: 0.8549966285103245\n",
      "Epoch 22, Training Loss: 0.849379496422029\n",
      "Epoch 23, Training Loss: 0.8532219981788692\n",
      "Epoch 24, Training Loss: 0.8494818795892529\n",
      "Epoch 25, Training Loss: 0.8524844733395971\n",
      "Epoch 26, Training Loss: 0.8494294692699175\n",
      "Epoch 27, Training Loss: 0.8521782472617644\n",
      "Epoch 28, Training Loss: 0.8500884870837505\n",
      "Epoch 29, Training Loss: 0.849106997027433\n",
      "Epoch 30, Training Loss: 0.8489408887418589\n",
      "Epoch 31, Training Loss: 0.8495738944612947\n",
      "Epoch 32, Training Loss: 0.8489451695205574\n",
      "Epoch 33, Training Loss: 0.8483835230196329\n",
      "Epoch 34, Training Loss: 0.848574539862181\n",
      "Epoch 35, Training Loss: 0.8488641001228103\n",
      "Epoch 36, Training Loss: 0.8474222921787348\n",
      "Epoch 37, Training Loss: 0.8457571744022513\n",
      "Epoch 38, Training Loss: 0.8483821406400294\n",
      "Epoch 39, Training Loss: 0.8511121989192819\n",
      "Epoch 40, Training Loss: 0.8504570359574225\n",
      "Epoch 41, Training Loss: 0.8488707519115362\n",
      "Epoch 42, Training Loss: 0.8488785128844412\n",
      "Epoch 43, Training Loss: 0.8474522965294974\n",
      "Epoch 44, Training Loss: 0.8479402521499118\n",
      "Epoch 45, Training Loss: 0.8467250460968878\n",
      "Epoch 46, Training Loss: 0.8484979512099933\n",
      "Epoch 47, Training Loss: 0.8451186411362842\n",
      "Epoch 48, Training Loss: 0.8463765303891404\n",
      "Epoch 49, Training Loss: 0.8480845631513381\n",
      "Epoch 50, Training Loss: 0.8456913150342783\n",
      "Epoch 51, Training Loss: 0.8484433967368047\n",
      "Epoch 52, Training Loss: 0.8473341724926368\n",
      "Epoch 53, Training Loss: 0.8458339153375841\n",
      "Epoch 54, Training Loss: 0.8464374423923349\n",
      "Epoch 55, Training Loss: 0.8482260152809602\n",
      "Epoch 56, Training Loss: 0.8491199845210053\n",
      "Epoch 57, Training Loss: 0.8476569063681408\n",
      "Epoch 58, Training Loss: 0.8467160811101584\n",
      "Epoch 59, Training Loss: 0.8477206303661031\n",
      "Epoch 60, Training Loss: 0.8508483788124601\n",
      "Epoch 61, Training Loss: 0.8476293671400027\n",
      "Epoch 62, Training Loss: 0.8486265829631261\n",
      "Epoch 63, Training Loss: 0.8445347847795128\n",
      "Epoch 64, Training Loss: 0.8454835681090678\n",
      "Epoch 65, Training Loss: 0.8465560704245603\n",
      "Epoch 66, Training Loss: 0.8475519019858282\n",
      "Epoch 67, Training Loss: 0.8450843900666201\n",
      "Epoch 68, Training Loss: 0.8479513161164477\n",
      "Epoch 69, Training Loss: 0.8468572988545984\n",
      "Epoch 70, Training Loss: 0.8435189833318381\n",
      "Epoch 71, Training Loss: 0.8465443886312327\n",
      "Epoch 72, Training Loss: 0.8430367552248159\n",
      "Epoch 73, Training Loss: 0.8454017682183058\n",
      "Epoch 74, Training Loss: 0.8451737728334011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:38:23,520] Trial 426 finished with value: 0.5700666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.845488871398725\n",
      "Epoch 1, Training Loss: 0.890637378131642\n",
      "Epoch 2, Training Loss: 0.8229965637010687\n",
      "Epoch 3, Training Loss: 0.8132532122555901\n",
      "Epoch 4, Training Loss: 0.8072714051078348\n",
      "Epoch 5, Training Loss: 0.8041186862833359\n",
      "Epoch 6, Training Loss: 0.8022557022291071\n",
      "Epoch 7, Training Loss: 0.8006007977794198\n",
      "Epoch 8, Training Loss: 0.7989076105286094\n",
      "Epoch 9, Training Loss: 0.7978383697481717\n",
      "Epoch 10, Training Loss: 0.7977902460799497\n",
      "Epoch 11, Training Loss: 0.796020415951224\n",
      "Epoch 12, Training Loss: 0.7958886243315304\n",
      "Epoch 13, Training Loss: 0.7955813417715185\n",
      "Epoch 14, Training Loss: 0.794866952124764\n",
      "Epoch 15, Training Loss: 0.7947057707870708\n",
      "Epoch 16, Training Loss: 0.7935060559300815\n",
      "Epoch 17, Training Loss: 0.7931354080929476\n",
      "Epoch 18, Training Loss: 0.7928416395187378\n",
      "Epoch 19, Training Loss: 0.7929626135265126\n",
      "Epoch 20, Training Loss: 0.7924308861003203\n",
      "Epoch 21, Training Loss: 0.7916164397492128\n",
      "Epoch 22, Training Loss: 0.7924880060728858\n",
      "Epoch 23, Training Loss: 0.7914162003292757\n",
      "Epoch 24, Training Loss: 0.7911164048138787\n",
      "Epoch 25, Training Loss: 0.7910304137538461\n",
      "Epoch 26, Training Loss: 0.7906156268540551\n",
      "Epoch 27, Training Loss: 0.7904399142545813\n",
      "Epoch 28, Training Loss: 0.790597522539251\n",
      "Epoch 29, Training Loss: 0.7901837612600887\n",
      "Epoch 30, Training Loss: 0.7898300583923564\n",
      "Epoch 31, Training Loss: 0.7907493864087497\n",
      "Epoch 32, Training Loss: 0.7904574878776774\n",
      "Epoch 33, Training Loss: 0.790045319515116\n",
      "Epoch 34, Training Loss: 0.7895168265875648\n",
      "Epoch 35, Training Loss: 0.7894674604780534\n",
      "Epoch 36, Training Loss: 0.7893570865603055\n",
      "Epoch 37, Training Loss: 0.7900903400252847\n",
      "Epoch 38, Training Loss: 0.7889878566826091\n",
      "Epoch 39, Training Loss: 0.7888981699943542\n",
      "Epoch 40, Training Loss: 0.7898964262709898\n",
      "Epoch 41, Training Loss: 0.7891670952824985\n",
      "Epoch 42, Training Loss: 0.7887007929297054\n",
      "Epoch 43, Training Loss: 0.7888533003189985\n",
      "Epoch 44, Training Loss: 0.7888071673757889\n",
      "Epoch 45, Training Loss: 0.7886842029936173\n",
      "Epoch 46, Training Loss: 0.7885772306077621\n",
      "Epoch 47, Training Loss: 0.7895721395576701\n",
      "Epoch 48, Training Loss: 0.78804018721861\n",
      "Epoch 49, Training Loss: 0.7891418705267065\n",
      "Epoch 50, Training Loss: 0.7887393332930173\n",
      "Epoch 51, Training Loss: 0.7888683984560125\n",
      "Epoch 52, Training Loss: 0.7883038667370291\n",
      "Epoch 53, Training Loss: 0.7881381043265847\n",
      "Epoch 54, Training Loss: 0.7886621499762816\n",
      "Epoch 55, Training Loss: 0.788385009765625\n",
      "Epoch 56, Training Loss: 0.7884837671588449\n",
      "Epoch 57, Training Loss: 0.7881240907136132\n",
      "Epoch 58, Training Loss: 0.7884721668327556\n",
      "Epoch 59, Training Loss: 0.7887767593299642\n",
      "Epoch 60, Training Loss: 0.7888135095904855\n",
      "Epoch 61, Training Loss: 0.7879563950791079\n",
      "Epoch 62, Training Loss: 0.7886891673592961\n",
      "Epoch 63, Training Loss: 0.7887696568404927\n",
      "Epoch 64, Training Loss: 0.788802842392641\n",
      "Epoch 65, Training Loss: 0.78937448999461\n",
      "Epoch 66, Training Loss: 0.7887653744921965\n",
      "Epoch 67, Training Loss: 0.7879660618305206\n",
      "Epoch 68, Training Loss: 0.7883994009214289\n",
      "Epoch 69, Training Loss: 0.7886534319905674\n",
      "Epoch 70, Training Loss: 0.7881393933997435\n",
      "Epoch 71, Training Loss: 0.7878956730225507\n",
      "Epoch 72, Training Loss: 0.7881562723832972\n",
      "Epoch 73, Training Loss: 0.7879868244423586\n",
      "Epoch 74, Training Loss: 0.7887172268418705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 20:40:00,706] Trial 427 finished with value: 0.6350666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7893997334732729\n",
      "Epoch 1, Training Loss: 0.8516489648117739\n",
      "Epoch 2, Training Loss: 0.8211145945156322\n",
      "Epoch 3, Training Loss: 0.8179297476656296\n",
      "Epoch 4, Training Loss: 0.8142611439789043\n",
      "Epoch 5, Training Loss: 0.8129793961609111\n",
      "Epoch 6, Training Loss: 0.8150138783454896\n",
      "Epoch 7, Training Loss: 0.8126852191195768\n",
      "Epoch 8, Training Loss: 0.8124223182481878\n",
      "Epoch 9, Training Loss: 0.8108477137369268\n",
      "Epoch 10, Training Loss: 0.8122699797854704\n",
      "Epoch 11, Training Loss: 0.8119438808104571\n",
      "Epoch 12, Training Loss: 0.8096904150878682\n",
      "Epoch 13, Training Loss: 0.8111961850699256\n",
      "Epoch 14, Training Loss: 0.8104343871509327\n",
      "Epoch 15, Training Loss: 0.809576691459207\n",
      "Epoch 16, Training Loss: 0.8085576668206383\n",
      "Epoch 17, Training Loss: 0.8091981108749614\n",
      "Epoch 18, Training Loss: 0.8091401799987344\n",
      "Epoch 19, Training Loss: 0.8080629956021028\n",
      "Epoch 20, Training Loss: 0.8071658350439632\n",
      "Epoch 21, Training Loss: 0.8073055917375228\n",
      "Epoch 22, Training Loss: 0.8076320658010595\n",
      "Epoch 23, Training Loss: 0.806204915747923\n",
      "Epoch 24, Training Loss: 0.8090805062826942\n",
      "Epoch 25, Training Loss: 0.8051555596379673\n",
      "Epoch 26, Training Loss: 0.807854639081394\n",
      "Epoch 27, Training Loss: 0.8034584005439983\n",
      "Epoch 28, Training Loss: 0.804771375726251\n",
      "Epoch 29, Training Loss: 0.8056772323215708\n",
      "Epoch 30, Training Loss: 0.8048140508988324\n",
      "Epoch 31, Training Loss: 0.8051007320600397\n",
      "Epoch 32, Training Loss: 0.8051143315259148\n",
      "Epoch 33, Training Loss: 0.8036928660028121\n",
      "Epoch 34, Training Loss: 0.8043184758635128\n",
      "Epoch 35, Training Loss: 0.8027230172297534\n",
      "Epoch 36, Training Loss: 0.8046652741993174\n",
      "Epoch 37, Training Loss: 0.8042893815040588\n",
      "Epoch 38, Training Loss: 0.8037686232258292\n",
      "Epoch 39, Training Loss: 0.803292461493436\n",
      "Epoch 40, Training Loss: 0.8039279902682585\n",
      "Epoch 41, Training Loss: 0.8026414547948276\n",
      "Epoch 42, Training Loss: 0.803633769680472\n",
      "Epoch 43, Training Loss: 0.8018005651586196\n",
      "Epoch 44, Training Loss: 0.8018511283397675\n",
      "Epoch 45, Training Loss: 0.8005619335174561\n",
      "Epoch 46, Training Loss: 0.8025233027514289\n",
      "Epoch 47, Training Loss: 0.801267438916599\n",
      "Epoch 48, Training Loss: 0.800675970175687\n",
      "Epoch 49, Training Loss: 0.8012746378954719\n",
      "Epoch 50, Training Loss: 0.8006398985666388\n",
      "Epoch 51, Training Loss: 0.8006778888141408\n",
      "Epoch 52, Training Loss: 0.8026033149747287\n",
      "Epoch 53, Training Loss: 0.8016480192717383\n",
      "Epoch 54, Training Loss: 0.7996558710406808\n",
      "Epoch 55, Training Loss: 0.8013582320774303\n",
      "Epoch 56, Training Loss: 0.8004300218469956\n",
      "Epoch 57, Training Loss: 0.8015780650868135\n",
      "Epoch 58, Training Loss: 0.8009875647460714\n",
      "Epoch 59, Training Loss: 0.8007566198881935\n",
      "Epoch 60, Training Loss: 0.8003944603134604\n",
      "Epoch 61, Training Loss: 0.8009548054723179\n",
      "Epoch 62, Training Loss: 0.8000400854559506\n",
      "Epoch 63, Training Loss: 0.798719944673426\n",
      "Epoch 64, Training Loss: 0.8005341494784636\n",
      "Epoch 65, Training Loss: 0.801487144722658\n",
      "Epoch 66, Training Loss: 0.8008185028328615\n",
      "Epoch 67, Training Loss: 0.7999828304262723\n",
      "Epoch 68, Training Loss: 0.8002964046422173\n",
      "Epoch 69, Training Loss: 0.798218618070378\n",
      "Epoch 70, Training Loss: 0.7988746825386496\n",
      "Epoch 71, Training Loss: 0.8000107870382421\n",
      "Epoch 72, Training Loss: 0.7978646856897017\n",
      "Epoch 73, Training Loss: 0.7998789141458623\n",
      "Epoch 74, Training Loss: 0.7987797897002277\n",
      "Epoch 75, Training Loss: 0.7990436941034653\n",
      "Epoch 76, Training Loss: 0.7986365577753852\n",
      "Epoch 77, Training Loss: 0.7985450950089623\n",
      "Epoch 78, Training Loss: 0.7984411985032699\n",
      "Epoch 79, Training Loss: 0.7980774099686566\n",
      "Epoch 80, Training Loss: 0.7985903537273407\n",
      "Epoch 81, Training Loss: 0.7989144469008727\n",
      "Epoch 82, Training Loss: 0.7982289726593915\n",
      "Epoch 83, Training Loss: 0.7996774340377134\n",
      "Epoch 84, Training Loss: 0.7986247137714835\n",
      "Epoch 85, Training Loss: 0.7972980608659632\n",
      "Epoch 86, Training Loss: 0.79936831537415\n",
      "Epoch 87, Training Loss: 0.7980931984677034\n",
      "Epoch 88, Training Loss: 0.7982336362670449\n",
      "Epoch 89, Training Loss: 0.7997882474871243\n",
      "Epoch 90, Training Loss: 0.7992810231096604\n",
      "Epoch 91, Training Loss: 0.7982659337801091\n",
      "Epoch 92, Training Loss: 0.7974820125804228\n",
      "Epoch 93, Training Loss: 0.7972060749811285\n",
      "Epoch 94, Training Loss: 0.798958057866377\n",
      "Epoch 95, Training Loss: 0.7984945161202375\n",
      "Epoch 96, Training Loss: 0.7977833166543176\n",
      "Epoch 97, Training Loss: 0.7993469196908615\n",
      "Epoch 98, Training Loss: 0.7987233523060294\n",
      "Epoch 99, Training Loss: 0.7989672319328084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 14:01:26,251] Trial 428 finished with value: 0.6269333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7975284089761622\n",
      "Epoch 1, Training Loss: 0.8525425237820561\n",
      "Epoch 2, Training Loss: 0.8134175247715828\n",
      "Epoch 3, Training Loss: 0.8096360955023227\n",
      "Epoch 4, Training Loss: 0.8059226977197748\n",
      "Epoch 5, Training Loss: 0.8051834726692142\n",
      "Epoch 6, Training Loss: 0.8036265245057587\n",
      "Epoch 7, Training Loss: 0.8030492904490995\n",
      "Epoch 8, Training Loss: 0.8025868830824257\n",
      "Epoch 9, Training Loss: 0.8019854430865524\n",
      "Epoch 10, Training Loss: 0.7997816028451561\n",
      "Epoch 11, Training Loss: 0.7994234872043581\n",
      "Epoch 12, Training Loss: 0.7997586480656961\n",
      "Epoch 13, Training Loss: 0.7979002619148197\n",
      "Epoch 14, Training Loss: 0.7976470800270712\n",
      "Epoch 15, Training Loss: 0.7974280056200529\n",
      "Epoch 16, Training Loss: 0.7988317175019056\n",
      "Epoch 17, Training Loss: 0.7976665629031963\n",
      "Epoch 18, Training Loss: 0.7976549263287308\n",
      "Epoch 19, Training Loss: 0.7959079657282148\n",
      "Epoch 20, Training Loss: 0.7965282678604126\n",
      "Epoch 21, Training Loss: 0.7970757802626244\n",
      "Epoch 22, Training Loss: 0.7972965284397727\n",
      "Epoch 23, Training Loss: 0.796005027724388\n",
      "Epoch 24, Training Loss: 0.7961925123867236\n",
      "Epoch 25, Training Loss: 0.7953709684816518\n",
      "Epoch 26, Training Loss: 0.7956201495084547\n",
      "Epoch 27, Training Loss: 0.7955889429364885\n",
      "Epoch 28, Training Loss: 0.7940380293623845\n",
      "Epoch 29, Training Loss: 0.7952628394714871\n",
      "Epoch 30, Training Loss: 0.7949280590043032\n",
      "Epoch 31, Training Loss: 0.7955753152531789\n",
      "Epoch 32, Training Loss: 0.7945975503527132\n",
      "Epoch 33, Training Loss: 0.7944548270756141\n",
      "Epoch 34, Training Loss: 0.7938490658774412\n",
      "Epoch 35, Training Loss: 0.7938120770275144\n",
      "Epoch 36, Training Loss: 0.7939772299357823\n",
      "Epoch 37, Training Loss: 0.7935820705012271\n",
      "Epoch 38, Training Loss: 0.7925006608765824\n",
      "Epoch 39, Training Loss: 0.793058863439058\n",
      "Epoch 40, Training Loss: 0.7934788126694529\n",
      "Epoch 41, Training Loss: 0.7932666710444859\n",
      "Epoch 42, Training Loss: 0.7919996196166017\n",
      "Epoch 43, Training Loss: 0.7934664123040392\n",
      "Epoch 44, Training Loss: 0.793466768228918\n",
      "Epoch 45, Training Loss: 0.7944937277557258\n",
      "Epoch 46, Training Loss: 0.7928029943229561\n",
      "Epoch 47, Training Loss: 0.7921511056727932\n",
      "Epoch 48, Training Loss: 0.7939445549384095\n",
      "Epoch 49, Training Loss: 0.7921745936673387\n",
      "Epoch 50, Training Loss: 0.7925324034870119\n",
      "Epoch 51, Training Loss: 0.79248831765096\n",
      "Epoch 52, Training Loss: 0.7927214153727195\n",
      "Epoch 53, Training Loss: 0.7932081332780365\n",
      "Epoch 54, Training Loss: 0.7930172633407707\n",
      "Epoch 55, Training Loss: 0.7929443277810749\n",
      "Epoch 56, Training Loss: 0.7932014066473883\n",
      "Epoch 57, Training Loss: 0.7909613730315875\n",
      "Epoch 58, Training Loss: 0.7920021496321026\n",
      "Epoch 59, Training Loss: 0.7921235649209274\n",
      "Epoch 60, Training Loss: 0.7927937239632571\n",
      "Epoch 61, Training Loss: 0.7924226983149248\n",
      "Epoch 62, Training Loss: 0.7924896997616704\n",
      "Epoch 63, Training Loss: 0.7928641425935845\n",
      "Epoch 64, Training Loss: 0.7927630982900921\n",
      "Epoch 65, Training Loss: 0.7925911565472309\n",
      "Epoch 66, Training Loss: 0.7918265862572462\n",
      "Epoch 67, Training Loss: 0.792593807116487\n",
      "Epoch 68, Training Loss: 0.7924819816324048\n",
      "Epoch 69, Training Loss: 0.7935068199509069\n",
      "Epoch 70, Training Loss: 0.7919936786020608\n",
      "Epoch 71, Training Loss: 0.7915801040212015\n",
      "Epoch 72, Training Loss: 0.7908079179606043\n",
      "Epoch 73, Training Loss: 0.7914544289273427\n",
      "Epoch 74, Training Loss: 0.7925839875873767\n",
      "Epoch 75, Training Loss: 0.7915071140554615\n",
      "Epoch 76, Training Loss: 0.7908192669538627\n",
      "Epoch 77, Training Loss: 0.7913789858495383\n",
      "Epoch 78, Training Loss: 0.790734443001281\n",
      "Epoch 79, Training Loss: 0.791915706315435\n",
      "Epoch 80, Training Loss: 0.791115803557231\n",
      "Epoch 81, Training Loss: 0.7906386828064023\n",
      "Epoch 82, Training Loss: 0.7907635655618251\n",
      "Epoch 83, Training Loss: 0.7915633163954082\n",
      "Epoch 84, Training Loss: 0.7912153670662327\n",
      "Epoch 85, Training Loss: 0.7897941194530717\n",
      "Epoch 86, Training Loss: 0.7905306368842161\n",
      "Epoch 87, Training Loss: 0.7902045586055383\n",
      "Epoch 88, Training Loss: 0.7912324999508105\n",
      "Epoch 89, Training Loss: 0.7907247370347045\n",
      "Epoch 90, Training Loss: 0.7906842608200876\n",
      "Epoch 91, Training Loss: 0.7907652204646204\n",
      "Epoch 92, Training Loss: 0.7914276296931102\n",
      "Epoch 93, Training Loss: 0.7915328428261262\n",
      "Epoch 94, Training Loss: 0.7909551733418515\n",
      "Epoch 95, Training Loss: 0.7914540066754907\n",
      "Epoch 96, Training Loss: 0.7914638441308101\n",
      "Epoch 97, Training Loss: 0.7903491820607866\n",
      "Epoch 98, Training Loss: 0.7906866644558154\n",
      "Epoch 99, Training Loss: 0.7901759458215614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 14:03:14,818] Trial 429 finished with value: 0.6371333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7906261744355797\n",
      "Epoch 1, Training Loss: 0.8509948058689342\n",
      "Epoch 2, Training Loss: 0.8182503159607158\n",
      "Epoch 3, Training Loss: 0.8136221283323625\n",
      "Epoch 4, Training Loss: 0.8105220390067381\n",
      "Epoch 5, Training Loss: 0.8078810607685762\n",
      "Epoch 6, Training Loss: 0.806120294823366\n",
      "Epoch 7, Training Loss: 0.8057725378345041\n",
      "Epoch 8, Training Loss: 0.8036627230223488\n",
      "Epoch 9, Training Loss: 0.80330983561628\n",
      "Epoch 10, Training Loss: 0.8029561044889338\n",
      "Epoch 11, Training Loss: 0.8011170746298397\n",
      "Epoch 12, Training Loss: 0.8012037028985864\n",
      "Epoch 13, Training Loss: 0.8006659469884985\n",
      "Epoch 14, Training Loss: 0.8001236189814175\n",
      "Epoch 15, Training Loss: 0.7995428923298331\n",
      "Epoch 16, Training Loss: 0.7994039709427777\n",
      "Epoch 17, Training Loss: 0.7986459016799927\n",
      "Epoch 18, Training Loss: 0.7993264336445752\n",
      "Epoch 19, Training Loss: 0.7981684910549837\n",
      "Epoch 20, Training Loss: 0.7980235811542062\n",
      "Epoch 21, Training Loss: 0.7975949674494126\n",
      "Epoch 22, Training Loss: 0.7973296791665694\n",
      "Epoch 23, Training Loss: 0.797364737777149\n",
      "Epoch 24, Training Loss: 0.7968254988333758\n",
      "Epoch 25, Training Loss: 0.7968483964835896\n",
      "Epoch 26, Training Loss: 0.7968619592750774\n",
      "Epoch 27, Training Loss: 0.7964480294900782\n",
      "Epoch 28, Training Loss: 0.7956638924514546\n",
      "Epoch 29, Training Loss: 0.7960548485727871\n",
      "Epoch 30, Training Loss: 0.7958428056099836\n",
      "Epoch 31, Training Loss: 0.7958382566536174\n",
      "Epoch 32, Training Loss: 0.7952136247298297\n",
      "Epoch 33, Training Loss: 0.7955049472696641\n",
      "Epoch 34, Training Loss: 0.7952994168505949\n",
      "Epoch 35, Training Loss: 0.7948898603635676\n",
      "Epoch 36, Training Loss: 0.794925123312894\n",
      "Epoch 37, Training Loss: 0.7947656977176666\n",
      "Epoch 38, Training Loss: 0.7951295929796556\n",
      "Epoch 39, Training Loss: 0.7951269352436066\n",
      "Epoch 40, Training Loss: 0.7943062865032869\n",
      "Epoch 41, Training Loss: 0.7945059827496024\n",
      "Epoch 42, Training Loss: 0.7943047209346996\n",
      "Epoch 43, Training Loss: 0.7948164946191452\n",
      "Epoch 44, Training Loss: 0.794481514972799\n",
      "Epoch 45, Training Loss: 0.7945082702356226\n",
      "Epoch 46, Training Loss: 0.7941168209384469\n",
      "Epoch 47, Training Loss: 0.7940325563795426\n",
      "Epoch 48, Training Loss: 0.7935670095331528\n",
      "Epoch 49, Training Loss: 0.7941095435619354\n",
      "Epoch 50, Training Loss: 0.7937782330372755\n",
      "Epoch 51, Training Loss: 0.7942762527045082\n",
      "Epoch 52, Training Loss: 0.7934111990648157\n",
      "Epoch 53, Training Loss: 0.7940739987878238\n",
      "Epoch 54, Training Loss: 0.7932531953559202\n",
      "Epoch 55, Training Loss: 0.7935052013397217\n",
      "Epoch 56, Training Loss: 0.7933828984288608\n",
      "Epoch 57, Training Loss: 0.7935392923916087\n",
      "Epoch 58, Training Loss: 0.7933729981674867\n",
      "Epoch 59, Training Loss: 0.7932039262967951\n",
      "Epoch 60, Training Loss: 0.7935009379947887\n",
      "Epoch 61, Training Loss: 0.7931936004582574\n",
      "Epoch 62, Training Loss: 0.7934632311849034\n",
      "Epoch 63, Training Loss: 0.7930944122987635\n",
      "Epoch 64, Training Loss: 0.7931466725994559\n",
      "Epoch 65, Training Loss: 0.7931869906537673\n",
      "Epoch 66, Training Loss: 0.7928949999809265\n",
      "Epoch 67, Training Loss: 0.7927026729022756\n",
      "Epoch 68, Training Loss: 0.7931823030640097\n",
      "Epoch 69, Training Loss: 0.7927110360650456\n",
      "Epoch 70, Training Loss: 0.792907320541494\n",
      "Epoch 71, Training Loss: 0.7927281764675589\n",
      "Epoch 72, Training Loss: 0.7929654590522541\n",
      "Epoch 73, Training Loss: 0.7924567256955539\n",
      "Epoch 74, Training Loss: 0.7925003746677848\n",
      "Epoch 75, Training Loss: 0.7927283270218793\n",
      "Epoch 76, Training Loss: 0.7924323176636415\n",
      "Epoch 77, Training Loss: 0.7926520508177141\n",
      "Epoch 78, Training Loss: 0.7929797856246724\n",
      "Epoch 79, Training Loss: 0.7925863262485056\n",
      "Epoch 80, Training Loss: 0.7931029424947851\n",
      "Epoch 81, Training Loss: 0.7926728594303131\n",
      "Epoch 82, Training Loss: 0.7922541447246776\n",
      "Epoch 83, Training Loss: 0.7926870430918301\n",
      "Epoch 84, Training Loss: 0.7925845515727997\n",
      "Epoch 85, Training Loss: 0.792069786155925\n",
      "Epoch 86, Training Loss: 0.7922922900845023\n",
      "Epoch 87, Training Loss: 0.7924097364089068\n",
      "Epoch 88, Training Loss: 0.7922200362121358\n",
      "Epoch 89, Training Loss: 0.7922286030124216\n",
      "Epoch 90, Training Loss: 0.7924966373864342\n",
      "Epoch 91, Training Loss: 0.7917324669922099\n",
      "Epoch 92, Training Loss: 0.7924152921929078\n",
      "Epoch 93, Training Loss: 0.7927937706779031\n",
      "Epoch 94, Training Loss: 0.7922707258953767\n",
      "Epoch 95, Training Loss: 0.7925831472873688\n",
      "Epoch 96, Training Loss: 0.7919708409028895\n",
      "Epoch 97, Training Loss: 0.7921027055908652\n",
      "Epoch 98, Training Loss: 0.7925496477940503\n",
      "Epoch 99, Training Loss: 0.7920102671314688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 14:05:08,719] Trial 430 finished with value: 0.6348666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7917741657004637\n",
      "Epoch 1, Training Loss: 0.9899403440952301\n",
      "Epoch 2, Training Loss: 0.9547745639436385\n",
      "Epoch 3, Training Loss: 0.9455190798815559\n",
      "Epoch 4, Training Loss: 0.9345590904880973\n",
      "Epoch 5, Training Loss: 0.9206916673043195\n",
      "Epoch 6, Training Loss: 0.9030666266469395\n",
      "Epoch 7, Training Loss: 0.8820226107625401\n",
      "Epoch 8, Training Loss: 0.8602201050169328\n",
      "Epoch 9, Training Loss: 0.8416359733833986\n",
      "Epoch 10, Training Loss: 0.8288622862451217\n",
      "Epoch 11, Training Loss: 0.8210312955519732\n",
      "Epoch 12, Training Loss: 0.816484430327135\n",
      "Epoch 13, Training Loss: 0.8133530131508322\n",
      "Epoch 14, Training Loss: 0.811861881017685\n",
      "Epoch 15, Training Loss: 0.8104865346235387\n",
      "Epoch 16, Training Loss: 0.8095628495777355\n",
      "Epoch 17, Training Loss: 0.808753355811624\n",
      "Epoch 18, Training Loss: 0.8079720903845394\n",
      "Epoch 19, Training Loss: 0.8074536201533149\n",
      "Epoch 20, Training Loss: 0.8067423468477586\n",
      "Epoch 21, Training Loss: 0.8062642401106217\n",
      "Epoch 22, Training Loss: 0.8057071171788608\n",
      "Epoch 23, Training Loss: 0.8053310352914473\n",
      "Epoch 24, Training Loss: 0.8051090652802412\n",
      "Epoch 25, Training Loss: 0.8044163629587959\n",
      "Epoch 26, Training Loss: 0.8040944428303662\n",
      "Epoch 27, Training Loss: 0.8036012676182915\n",
      "Epoch 28, Training Loss: 0.8032206733787761\n",
      "Epoch 29, Training Loss: 0.8028293482696309\n",
      "Epoch 30, Training Loss: 0.802347634119146\n",
      "Epoch 31, Training Loss: 0.8021823714059942\n",
      "Epoch 32, Training Loss: 0.8016221764508416\n",
      "Epoch 33, Training Loss: 0.801641931884429\n",
      "Epoch 34, Training Loss: 0.8011863755478579\n",
      "Epoch 35, Training Loss: 0.8010210301595576\n",
      "Epoch 36, Training Loss: 0.8006844384530011\n",
      "Epoch 37, Training Loss: 0.8006390612265643\n",
      "Epoch 38, Training Loss: 0.8001579431926503\n",
      "Epoch 39, Training Loss: 0.8001328853298636\n",
      "Epoch 40, Training Loss: 0.7999967267934014\n",
      "Epoch 41, Training Loss: 0.799880480485804\n",
      "Epoch 42, Training Loss: 0.7996006846427918\n",
      "Epoch 43, Training Loss: 0.7995753859071171\n",
      "Epoch 44, Training Loss: 0.7994823249648599\n",
      "Epoch 45, Training Loss: 0.7992045488778282\n",
      "Epoch 46, Training Loss: 0.7990304098409765\n",
      "Epoch 47, Training Loss: 0.7990451025261599\n",
      "Epoch 48, Training Loss: 0.7987395275340361\n",
      "Epoch 49, Training Loss: 0.7987761064136729\n",
      "Epoch 50, Training Loss: 0.7985910987152772\n",
      "Epoch 51, Training Loss: 0.7985669550474952\n",
      "Epoch 52, Training Loss: 0.7984792554378509\n",
      "Epoch 53, Training Loss: 0.7982051530305077\n",
      "Epoch 54, Training Loss: 0.7984573816551882\n",
      "Epoch 55, Training Loss: 0.7981737096169416\n",
      "Epoch 56, Training Loss: 0.7982308781848234\n",
      "Epoch 57, Training Loss: 0.7983755109590642\n",
      "Epoch 58, Training Loss: 0.7981691452334909\n",
      "Epoch 59, Training Loss: 0.7979599368572236\n",
      "Epoch 60, Training Loss: 0.7981699034746955\n",
      "Epoch 61, Training Loss: 0.7979109717817867\n",
      "Epoch 62, Training Loss: 0.7977694711965673\n",
      "Epoch 63, Training Loss: 0.7977406314541312\n",
      "Epoch 64, Training Loss: 0.7977925428923438\n",
      "Epoch 65, Training Loss: 0.7978389010008644\n",
      "Epoch 66, Training Loss: 0.7976805000445422\n",
      "Epoch 67, Training Loss: 0.7976769314793979\n",
      "Epoch 68, Training Loss: 0.7974400702644797\n",
      "Epoch 69, Training Loss: 0.7974797206065234\n",
      "Epoch 70, Training Loss: 0.7974335312843323\n",
      "Epoch 71, Training Loss: 0.7974579167366028\n",
      "Epoch 72, Training Loss: 0.7974053645835203\n",
      "Epoch 73, Training Loss: 0.7973596522387336\n",
      "Epoch 74, Training Loss: 0.7972786279986886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-23 14:06:32,750] Trial 431 finished with value: 0.6323333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7971297147694756\n"
     ]
    }
   ],
   "source": [
    "sampler = GridSampler(search_space)\n",
    "study= optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fe765ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layers': 2,\n",
       " 'act_functn': 'relu',\n",
       " 'optimizer_name': 'Adam',\n",
       " 'learning_rate': 0.001,\n",
       " 'batch_size': 128,\n",
       " 'epochs': 100,\n",
       " 'neurons_per_layer': 60}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As per the runs sor far, the best parameters are \n",
    "#[I 2025-06-22 15:57:08,640] Trial 248 finished with value: 0.6423333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 67 with value: 0.6433333333333333.\n",
    "\n",
    "\n",
    "best_params={'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51640015",
   "metadata": {},
   "source": [
    "# Improved Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63701a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layers': 2,\n",
       " 'act_functn': 'tanh',\n",
       " 'optimizer_name': 'Adam',\n",
       " 'learning_rate': 0.001,\n",
       " 'batch_size': 100,\n",
       " 'epochs': 100,\n",
       " 'neurons_per_layer': 50}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#improved model parameters are \n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "795dae95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=60, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=60, out_features=60, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=60, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape=X_train.shape[1]\n",
    "output_shape=3\n",
    "model=MyNN(input_shape,output_shape,2,60,'relu')\n",
    "optimizer =  optim.Adam(model.parameters(),lr=0.001)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f2d1e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8924137649679542\n",
      "Epoch 2, Loss: 0.8094131228619053\n",
      "Epoch 3, Loss: 0.8028215173491858\n",
      "Epoch 4, Loss: 0.7999953100555821\n",
      "Epoch 5, Loss: 0.7974972752700175\n",
      "Epoch 6, Loss: 0.7971822495747329\n",
      "Epoch 7, Loss: 0.796303589720475\n",
      "Epoch 8, Loss: 0.7942301460674831\n",
      "Epoch 9, Loss: 0.7921473362392053\n",
      "Epoch 10, Loss: 0.7912153379361432\n",
      "Epoch 11, Loss: 0.7899144125612159\n",
      "Epoch 12, Loss: 0.7895507474590961\n",
      "Epoch 13, Loss: 0.7884973827161287\n",
      "Epoch 14, Loss: 0.7884536172214307\n",
      "Epoch 15, Loss: 0.7876118168795019\n",
      "Epoch 16, Loss: 0.7879627255568827\n",
      "Epoch 17, Loss: 0.7871719724253604\n",
      "Epoch 18, Loss: 0.7862346641103127\n",
      "Epoch 19, Loss: 0.7857225551641077\n",
      "Epoch 20, Loss: 0.7856638842059257\n",
      "Epoch 21, Loss: 0.7856699832399985\n",
      "Epoch 22, Loss: 0.7847743262025647\n",
      "Epoch 23, Loss: 0.7849690671253922\n",
      "Epoch 24, Loss: 0.7841363992009844\n",
      "Epoch 25, Loss: 0.7846272470359515\n",
      "Epoch 26, Loss: 0.7839738779498222\n",
      "Epoch 27, Loss: 0.7838096928775758\n",
      "Epoch 28, Loss: 0.7831885321696002\n",
      "Epoch 29, Loss: 0.7828824619601543\n",
      "Epoch 30, Loss: 0.7826866984367371\n",
      "Epoch 31, Loss: 0.7834000953158041\n",
      "Epoch 32, Loss: 0.7823836646581951\n",
      "Epoch 33, Loss: 0.7832893452249972\n",
      "Epoch 34, Loss: 0.7820251891487523\n",
      "Epoch 35, Loss: 0.7826411004353286\n",
      "Epoch 36, Loss: 0.781907780546891\n",
      "Epoch 37, Loss: 0.7816358860274006\n",
      "Epoch 38, Loss: 0.7814733214844438\n",
      "Epoch 39, Loss: 0.7819765441399767\n",
      "Epoch 40, Loss: 0.7813094511067957\n",
      "Epoch 41, Loss: 0.780834122170183\n",
      "Epoch 42, Loss: 0.7822420578253897\n",
      "Epoch 43, Loss: 0.781333901649131\n",
      "Epoch 44, Loss: 0.7812444746942449\n",
      "Epoch 45, Loss: 0.780695382903393\n",
      "Epoch 46, Loss: 0.7809545733874902\n",
      "Epoch 47, Loss: 0.7811080429787026\n",
      "Epoch 48, Loss: 0.7799884969130495\n",
      "Epoch 49, Loss: 0.7811612131004046\n",
      "Epoch 50, Loss: 0.7803875417637646\n",
      "Epoch 51, Loss: 0.7797471721369521\n",
      "Epoch 52, Loss: 0.7796839386000669\n",
      "Epoch 53, Loss: 0.780045653733992\n",
      "Epoch 54, Loss: 0.7799947891020237\n",
      "Epoch 55, Loss: 0.7792920518638496\n",
      "Epoch 56, Loss: 0.7792154761185324\n",
      "Epoch 57, Loss: 0.7800223158714467\n",
      "Epoch 58, Loss: 0.7786803560149401\n",
      "Epoch 59, Loss: 0.779232614918759\n",
      "Epoch 60, Loss: 0.7788454926103577\n",
      "Epoch 61, Loss: 0.7801947592792654\n",
      "Epoch 62, Loss: 0.7786109939553684\n",
      "Epoch 63, Loss: 0.7785488213811602\n",
      "Epoch 64, Loss: 0.7783030823657388\n",
      "Epoch 65, Loss: 0.7789843833536134\n",
      "Epoch 66, Loss: 0.7788088076993038\n",
      "Epoch 67, Loss: 0.7778784364685977\n",
      "Epoch 68, Loss: 0.7785415119694588\n",
      "Epoch 69, Loss: 0.7787972683297064\n",
      "Epoch 70, Loss: 0.7778505575387997\n",
      "Epoch 71, Loss: 0.7777467856729837\n",
      "Epoch 72, Loss: 0.7779044053608314\n",
      "Epoch 73, Loss: 0.778231079685957\n",
      "Epoch 74, Loss: 0.7777744459926634\n",
      "Epoch 75, Loss: 0.7775463702087115\n",
      "Epoch 76, Loss: 0.7781756259444961\n",
      "Epoch 77, Loss: 0.7772142256112923\n",
      "Epoch 78, Loss: 0.7775280372540754\n",
      "Epoch 79, Loss: 0.7772889068252162\n",
      "Epoch 80, Loss: 0.7771822723230921\n",
      "Epoch 81, Loss: 0.7782683073129869\n",
      "Epoch 82, Loss: 0.7776188852195453\n",
      "Epoch 83, Loss: 0.7776110165101245\n",
      "Epoch 84, Loss: 0.7767189286705246\n",
      "Epoch 85, Loss: 0.7769267202319955\n",
      "Epoch 86, Loss: 0.7777731116552998\n",
      "Epoch 87, Loss: 0.777562083247909\n",
      "Epoch 88, Loss: 0.7763626196330651\n",
      "Epoch 89, Loss: 0.777598364191844\n",
      "Epoch 90, Loss: 0.7762956102091567\n",
      "Epoch 91, Loss: 0.7764929162828546\n",
      "Epoch 92, Loss: 0.7769404970613637\n",
      "Epoch 93, Loss: 0.7774100061646081\n",
      "Epoch 94, Loss: 0.7766992688179016\n",
      "Epoch 95, Loss: 0.7764650177238579\n",
      "Epoch 96, Loss: 0.7768655512565957\n",
      "Epoch 97, Loss: 0.7771754011175687\n",
      "Epoch 98, Loss: 0.775752629059598\n",
      "Epoch 99, Loss: 0.7767134480906609\n",
      "Epoch 100, Loss: 0.7763520022083942\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=128,shuffle=True,pin_memory=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=128,shuffle=False,pin_memory=True)\n",
    "epochs=100\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "total=0\n",
    "correct=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_epoch_loss=0\n",
    "\n",
    "    for batch_features,batch_labels in train_loader:\n",
    "\n",
    "        #move data to gpu\n",
    "        batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        outputs=model(batch_features)\n",
    "\n",
    "        #Calculate loss\n",
    "        loss= criterion(outputs,batch_labels)\n",
    "\n",
    "        #backpass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #update grads\n",
    "        optimizer.step()\n",
    "\n",
    "        total_epoch_loss= total_epoch_loss+loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "        # print(f'predicted:{predicted}Labels {batch_labels}')\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss=total_epoch_loss/len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    print(f'Epoch {epoch +1 }, Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a78221a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27ec97a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy with model parameters is : 0.6388666666666667\n"
     ]
    }
   ],
   "source": [
    "#Set model to eval model\n",
    "model.eval()\n",
    "\n",
    "#Evaluation Code\n",
    "\n",
    "total=0\n",
    "correct=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "        outputs=model(batch_features)\n",
    "        _, predicted=torch.max(outputs,1)\n",
    "\n",
    "        total = total + batch_labels.shape[0]\n",
    "        correct= correct+(predicted==batch_labels).sum().item()\n",
    "accuracy=correct/total\n",
    "\n",
    "print(f'Evaluation accuracy with model parameters is : {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89187b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7149f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772be97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5d902f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandman_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f9b369",
   "metadata": {},
   "source": [
    "# Predicting Chess Game Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec065bc6",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a00f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim  as optim\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6351132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for processing\n"
     ]
    }
   ],
   "source": [
    "#Select GPu if available for processing\n",
    "\n",
    "device=torch.device('gpu' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using {device} for processing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8370aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "df = pd.read_csv('matmob_data_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fe77ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print shape and one example row\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a181de92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w.mat.mean</th>\n",
       "      <th>w.mat.sd</th>\n",
       "      <th>b.mat.mean</th>\n",
       "      <th>b.mat.sd</th>\n",
       "      <th>w.mob.mean</th>\n",
       "      <th>w.mob.sd</th>\n",
       "      <th>b.mob.mean</th>\n",
       "      <th>b.mob.sd</th>\n",
       "      <th>result</th>\n",
       "      <th>valid.games.row</th>\n",
       "      <th>w.mat.vector</th>\n",
       "      <th>b.mat.vector</th>\n",
       "      <th>w.mob.vector</th>\n",
       "      <th>b.mob.vector</th>\n",
       "      <th>pgn</th>\n",
       "      <th>half.moves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.166667</td>\n",
       "      <td>8.850414</td>\n",
       "      <td>28.135417</td>\n",
       "      <td>9.671031</td>\n",
       "      <td>39.3125</td>\n",
       "      <td>12.951885</td>\n",
       "      <td>27.125</td>\n",
       "      <td>11.296064</td>\n",
       "      <td>1-0</td>\n",
       "      <td>1343406</td>\n",
       "      <td>39, 39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38...</td>\n",
       "      <td>39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38, 38...</td>\n",
       "      <td>20, 28, 30, 32, 30, 40, 40, 45, 49, 47, 49, 51...</td>\n",
       "      <td>20, 22, 28, 34, 34, 35, 31, 32, 33, 30, 30, 28...</td>\n",
       "      <td>1.d4 Nf6 2.c4 e6 3.Nf3 d5 4.cxd5 exd5 5.Qc2 Be...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w.mat.mean  w.mat.sd  b.mat.mean  b.mat.sd  w.mob.mean   w.mob.sd  \\\n",
       "0   28.166667  8.850414   28.135417  9.671031     39.3125  12.951885   \n",
       "\n",
       "   b.mob.mean   b.mob.sd result  valid.games.row  \\\n",
       "0      27.125  11.296064    1-0          1343406   \n",
       "\n",
       "                                        w.mat.vector  \\\n",
       "0  39, 39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38...   \n",
       "\n",
       "                                        b.mat.vector  \\\n",
       "0  39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38, 38...   \n",
       "\n",
       "                                        w.mob.vector  \\\n",
       "0  20, 28, 30, 32, 30, 40, 40, 45, 49, 47, 49, 51...   \n",
       "\n",
       "                                        b.mob.vector  \\\n",
       "0  20, 22, 28, 34, 34, 35, 31, 32, 33, 30, 30, 28...   \n",
       "\n",
       "                                                 pgn  half.moves  \n",
       "0  1.d4 Nf6 2.c4 e6 3.Nf3 d5 4.cxd5 exd5 5.Qc2 Be...          95  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad82fbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w.mat.mean</th>\n",
       "      <th>w.mat.sd</th>\n",
       "      <th>b.mat.mean</th>\n",
       "      <th>b.mat.sd</th>\n",
       "      <th>w.mob.mean</th>\n",
       "      <th>w.mob.sd</th>\n",
       "      <th>b.mob.mean</th>\n",
       "      <th>b.mob.sd</th>\n",
       "      <th>valid.games.row</th>\n",
       "      <th>half.moves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.471540</td>\n",
       "      <td>7.518266</td>\n",
       "      <td>29.456648</td>\n",
       "      <td>7.556172</td>\n",
       "      <td>33.083914</td>\n",
       "      <td>9.300182</td>\n",
       "      <td>30.808188</td>\n",
       "      <td>8.824375</td>\n",
       "      <td>8.475512e+05</td>\n",
       "      <td>80.524530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.700711</td>\n",
       "      <td>3.590136</td>\n",
       "      <td>5.722296</td>\n",
       "      <td>3.608866</td>\n",
       "      <td>4.899840</td>\n",
       "      <td>2.659605</td>\n",
       "      <td>4.488267</td>\n",
       "      <td>2.583139</td>\n",
       "      <td>4.892308e+05</td>\n",
       "      <td>33.794235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.768683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.127049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.312500</td>\n",
       "      <td>1.707825</td>\n",
       "      <td>12.272727</td>\n",
       "      <td>2.387467</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.206546</td>\n",
       "      <td>4.674827</td>\n",
       "      <td>25.148607</td>\n",
       "      <td>4.691699</td>\n",
       "      <td>29.653061</td>\n",
       "      <td>7.244109</td>\n",
       "      <td>27.731616</td>\n",
       "      <td>6.879109</td>\n",
       "      <td>4.246160e+05</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.317267</td>\n",
       "      <td>7.677324</td>\n",
       "      <td>30.308824</td>\n",
       "      <td>7.716147</td>\n",
       "      <td>33.390244</td>\n",
       "      <td>9.153638</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>8.740191</td>\n",
       "      <td>8.492815e+05</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>10.550320</td>\n",
       "      <td>34.017938</td>\n",
       "      <td>10.608270</td>\n",
       "      <td>36.700000</td>\n",
       "      <td>11.164435</td>\n",
       "      <td>33.978723</td>\n",
       "      <td>10.618308</td>\n",
       "      <td>1.268899e+06</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>16.043718</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>16.394856</td>\n",
       "      <td>50.842105</td>\n",
       "      <td>21.449271</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>20.940381</td>\n",
       "      <td>1.696627e+06</td>\n",
       "      <td>369.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          w.mat.mean       w.mat.sd     b.mat.mean       b.mat.sd  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       29.471540       7.518266      29.456648       7.556172   \n",
       "std         5.700711       3.590136       5.722296       3.608866   \n",
       "min         7.768683       0.000000       9.127049       0.000000   \n",
       "25%        25.206546       4.674827      25.148607       4.691699   \n",
       "50%        30.317267       7.677324      30.308824       7.716147   \n",
       "75%        34.000000      10.550320      34.017938      10.608270   \n",
       "max        39.000000      16.043718      39.000000      16.394856   \n",
       "\n",
       "          w.mob.mean       w.mob.sd     b.mob.mean       b.mob.sd  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       33.083914       9.300182      30.808188       8.824375   \n",
       "std         4.899840       2.659605       4.488267       2.583139   \n",
       "min        11.312500       1.707825      12.272727       2.387467   \n",
       "25%        29.653061       7.244109      27.731616       6.879109   \n",
       "50%        33.390244       9.153638      31.000000       8.740191   \n",
       "75%        36.700000      11.164435      33.978723      10.618308   \n",
       "max        50.842105      21.449271      48.250000      20.940381   \n",
       "\n",
       "       valid.games.row     half.moves  \n",
       "count     1.000000e+05  100000.000000  \n",
       "mean      8.475512e+05      80.524530  \n",
       "std       4.892308e+05      33.794235  \n",
       "min       7.000000e+00       6.000000  \n",
       "25%       4.246160e+05      58.000000  \n",
       "50%       8.492815e+05      77.000000  \n",
       "75%       1.268899e+06     100.000000  \n",
       "max       1.696627e+06     369.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f796a2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w.mat.mean         float64\n",
       "w.mat.sd           float64\n",
       "b.mat.mean         float64\n",
       "b.mat.sd           float64\n",
       "w.mob.mean         float64\n",
       "w.mob.sd           float64\n",
       "b.mob.mean         float64\n",
       "b.mob.sd           float64\n",
       "result              object\n",
       "valid.games.row      int64\n",
       "w.mat.vector        object\n",
       "b.mat.vector        object\n",
       "w.mob.vector        object\n",
       "b.mob.vector        object\n",
       "pgn                 object\n",
       "half.moves           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba23078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result\n",
       "1-0        36485\n",
       "1/2-1/2    35810\n",
       "0-1        27705\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Distribution of the result variable using value_counts()\n",
    "df['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40cd370a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAHqCAYAAACZcdjsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUExJREFUeJzt3XlcF/Xe//8noCyKgBugiYhiCiaiuFFmqCQqp/REtp5Cc7n0gkq40qLL3FrseMqlRKmvC56McmkXQxFTj4mmJLklpWFYCmQmuILC5/dHF/PzE+A2KKCP++02t+PM+/WZec3nHI48nXnP2FgsFosAAAAAwATb6m4AAAAAQO1HsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAFwxGxsbTZky5YYeMyQkRCEhITfkWH89vylTpsjGxkbHjh27Icdv1aqVhg0bdkOOBQBVjWABADVEYmKibGxsjKVOnTq67bbbNGzYMP3666/V3V6FtmzZoilTpujEiRNXVD9s2DCrc3R2dlbr1q314IMP6qOPPlJpaWm19HUj1eTeAMCMOtXdAADA2rRp0+Tj46Nz585p69atSkxM1ObNm7Vnzx45OjpWd3tWtmzZoqlTp2rYsGFyc3O7os84ODhowYIFkqSzZ8/q559/1hdffKEHH3xQISEh+uyzz+Ti4mLUr1279ob0VdZPnTrX96/GS/WWlZUlW1v+zQ9A7USwAIAaZuDAgerataskaeTIkWrSpIn++c9/6vPPP9dDDz1Uzd2ZV6dOHf3jH/+w2vbKK6/o9ddfV1xcnEaNGqVly5YZY/b29te1n9LSUhUXF8vR0bHag5uDg0O1Hh8AzOCfRQCghrv77rslSQcPHrTavn//fj344INq1KiRHB0d1bVrV33++edWNefPn9fUqVPVtm1bOTo6qnHjxurVq5dSU1ONmsrmMAwbNkytWrWqtK8pU6Zo/PjxkiQfHx/j9qZDhw5d03m+8MIL6t+/v1asWKEffvjhkv29/fbb6tChg+rVq6eGDRuqa9euSkpKuqK+bGxsFB0drffff18dOnSQg4ODUlJSjLGK5pAcO3ZMDz30kFxcXNS4cWM9++yzOnfunDF+6NAh2djYKDExsdxnL97n5XqraI7FTz/9pKFDh6pRo0aqV6+eevbsqeTkZKuaDRs2yMbGRsuXL9err76qFi1ayNHRUf369dOBAwcq/c4BoCpxxQIAariyXzobNmxobNu7d6/uuusu3XbbbXrhhRdUv359LV++XEOGDNFHH32kv//975L+/EV2+vTpGjlypLp3767CwkLt2LFD3377re69915TfT3wwAP64Ycf9MEHH2jWrFlq0qSJJKlp06bXvM8nnnhCa9euVWpqqm6//fYKa/7f//t/euaZZ/Tggw8av+Dv2rVL27Zt02OPPXZFfa1fv17Lly9XdHS0mjRpcskAJUkPPfSQWrVqpenTp2vr1q1666239Mcff+jf//73VZ3f1X5neXl5uvPOO3XmzBk988wzaty4sZYsWaL7779fK1euNP57LvP666/L1tZWzz33nAoKCjRjxgw9/vjj2rZt21X1CQDXgmABADVMQUGBjh07pnPnzmnbtm2aOnWqHBwc9Le//c2oefbZZ9WyZUtt377duH3mv//7v9WrVy89//zzxi+cycnJGjRokN59990q7zMgIEBdunTRBx98oCFDhlz2l/Mrcccdd0gqf3XmYsnJyerQoYNWrFhxzX1lZWVp9+7d8vf3v6K+fHx89Nlnn0mSoqKi5OLionnz5um5555TQEDAFe3jSnu72Ouvv668vDz95z//Ua9evSRJo0aNUkBAgGJjYzV48GCrORnnzp1TZmamcftYw4YN9eyzz2rPnj3GdwsA1wu3QgFADRMaGqqmTZvKy8tLDz74oOrXr6/PP/9cLVq0kCQdP35c69ev10MPPaSTJ0/q2LFjOnbsmH7//XeFhYXpxx9/NJ4i5ebmpr179+rHH3+szlO6Ys7OzpKkkydPVlrj5uamX375Rdu3b7/m49xzzz1XHCqkP8PExZ5++mlJ0urVq6+5hyuxevVqde/e3QgV0p/f0ejRo3Xo0CHt27fPqn748OFWc1LKbqP76aefrmufACARLACgxomPj1dqaqpWrlypQYMG6dixY1aTeg8cOCCLxaKXXnpJTZs2tVomT54sScrPz5f05xOmTpw4odtvv10dO3bU+PHjtWvXrmo5rytx6tQpSVKDBg0qrXn++efl7Oys7t27q23btoqKitLXX399Vcfx8fG5qvq2bdtarbdp00a2trbXPJ/kSv38889q165due1+fn7G+MVatmxptV52+9wff/xxnToEgP8ft0IBQA3TvXt346lQQ4YMUa9evfTYY48pKytLzs7OxrsennvuOYWFhVW4D19fX0lS7969dfDgQX322Wdau3atFixYoFmzZikhIUEjR46U9OfkYovFUm4fJSUl1+P0LmnPnj2S/v/+K+Ln56esrCytWrVKKSkp+uijjzRv3jxNmjRJU6dOvaLjODk5merTxsbmkutlbvR3aGdnV+H2iv77BYCqxhULAKjB7OzsNH36dB05ckRz586VJLVu3VqSVLduXYWGhla4XPwv/o0aNdLw4cP1wQcf6PDhwwoICLB68lHDhg0rfFnbX/81vCKV/UJ9rd577z3Z2NhcdmJ5/fr19fDDD2vx4sXKyclReHi4Xn31VeNJTVXd119vJTtw4IBKS0uNORJlVwb++j1W9B1eTW/e3t7Kysoqt33//v3GOADUFAQLAKjhQkJC1L17d82ePVvnzp2Tu7u7QkJC9M477+jo0aPl6n/77Tfjz7///rvVmLOzs3x9fVVUVGRsa9Omjfbv32/1ue++++6Kbi+qX7++pPK/UF+L119/XWvXrtXDDz9c7taji/31nOzt7eXv7y+LxaLz589XeV/Sn7enXeztt9+W9Oc7RyTJxcVFTZo00aZNm6zq5s2bV25fV9PboEGD9M033yg9Pd3Ydvr0ab377rtq1arVVc0TAYDrjVuhAKAWGD9+vIYOHarExESNGTNG8fHx6tWrlzp27KhRo0apdevWysvLU3p6un755Rd99913kiR/f3+FhIQoKChIjRo10o4dO7Ry5UpFR0cb+37qqac0c+ZMhYWFacSIEcrPz1dCQoI6dOigwsLCS/YVFBQkSfrf//1fPfLII6pbt67uu+8+45fnily4cEFLly6V9OdTjH7++Wd9/vnn2rVrl/r06XPZJ1j1799fnp6euuuuu+Th4aHvv/9ec+fOVXh4uHGl5lr6upTs7Gzdf//9GjBggNLT07V06VI99thj6tSpk1EzcuRIvf766xo5cqS6du2qTZs2Wb2Po8zV9PbCCy/ogw8+0MCBA/XMM8+oUaNGWrJkibKzs/XRRx/xlm4ANYsFAFAjLF682CLJsn379nJjJSUlljZt2ljatGljuXDhgsVisVgOHjxoefLJJy2enp6WunXrWm677TbL3/72N8vKlSuNz73yyiuW7t27W9zc3CxOTk6W9u3bW1599VVLcXGx1f6XLl1qad26tcXe3t4SGBhoWbNmjSUyMtLi7e1tVSfJMnnyZKttL7/8suW2226z2NraWiRZsrOzKz3HyMhIiyRjqVevnqVVq1aWiIgIy8qVKy0lJSXlPnPPPfdY7rnnHmP9nXfesfTu3dvSuHFji4ODg6VNmzaW8ePHWwoKCq6oL0mWqKioCvv76/lNnjzZIsmyb98+y4MPPmhp0KCBpWHDhpbo6GjL2bNnrT575swZy4gRIyyurq6WBg0aWB566CFLfn7+VX1n3t7elsjISKvagwcPWh588EGLm5ubxdHR0dK9e3fLqlWrrGq++uoriyTLihUrrLZnZ2dbJFkWL15c4fkCQFWysViY0QUAAADAHK6hAgAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0XpBXRUpLS3XkyBE1aNBANjY21d0OAAAAYJrFYtHJkyfVvHnzy76Uk2BRRY4cOSIvL6/qbgMAAACococPH1aLFi0uWUOwqCINGjSQ9OeX7uLiUs3dAAAAAOYVFhbKy8vL+F33UggWVaTs9icXFxeCBQAAAG4qV3KrP5O3AQAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESxwU5g/f74CAgKMyfPBwcH68ssvrWrS09PVt29f1a9fXy4uLurdu7fOnj1bbl9FRUUKDAyUjY2NMjMzrcbWrFmjnj17qkGDBmratKkiIiJ06NAhY3zDhg2ysbEpt+Tm5l6P0wYAAKgxCBa4KbRo0UKvv/66MjIytGPHDvXt21eDBw/W3r17Jf0ZKgYMGKD+/fvrm2++0fbt2xUdHV3hi14mTJig5s2bl9uenZ2twYMHq2/fvsrMzNSaNWt07NgxPfDAA+Vqs7KydPToUWNxd3ev+pMGAACoQXjcLG4K9913n9X6q6++qvnz52vr1q3q0KGDYmJi9Mwzz+iFF14watq1a1duP19++aXWrl2rjz76qNwVj4yMDJWUlOiVV14xAslzzz2nwYMH6/z586pbt65R6+7uLjc3tyo8QwAAgJqNKxa46ZSUlOjDDz/U6dOnFRwcrPz8fG3btk3u7u6688475eHhoXvuuUebN2+2+lxeXp5GjRql9957T/Xq1Su336CgINna2mrx4sUqKSlRQUGB3nvvPYWGhlqFCkkKDAxUs2bNdO+99+rrr7++rucLAABQExAscNPYvXu3nJ2d5eDgoDFjxuiTTz6Rv7+/fvrpJ0nSlClTNGrUKKWkpKhLly7q16+ffvzxR0mSxWLRsGHDNGbMGHXt2rXC/fv4+Gjt2rV68cUX5eDgIDc3N/3yyy9avny5UdOsWTMlJCToo48+0kcffSQvLy+FhITo22+/vf5fAAAAQDWysVgslupu4mZQWFgoV1dXFRQU8ObtalJcXKycnBwVFBRo5cqVWrBggTZu3KgTJ07orrvuUlxcnF577TWjPiAgQOHh4Zo+fbreeustLV++XBs3bpSdnZ0OHTokHx8f7dy5U4GBgZKk3Nxc9e7dW0OGDNGjjz6qkydPatKkSapTp45SU1MrfSPlPffco5YtW+q99967EV8DAABAlbma33GZY4Gbhr29vXx9fSX9edvS9u3bNWfOHGNehb+/v1W9n5+fcnJyJEnr169Xenq6HBwcrGq6du2qxx9/XEuWLFF8fLxcXV01Y8YMY3zp0qXy8vLStm3b1LNnzwr76t69e7nbrgAAAG423AqFm1ZpaamKiorUqlUrNW/eXFlZWVbjP/zwg7y9vSVJb731lr777jtlZmYqMzNTq1evliQtW7ZMr776qiTpzJkz5Z4iZWdnZxyrMpmZmWrWrFmVnRdQG13ukdAhISHlHtM8ZswYq31s375d/fr1k5ubmxo2bKiwsDB99913xvi5c+c0bNgwdezYUXXq1NGQIUMq7GXDhg3q0qWLHBwc5Ovrq8TExOtxygBwy+GKBW4KcXFxGjhwoFq2bKmTJ08qKSlJGzZs0Jo1a2RjY6Px48dr8uTJ6tSpkwIDA7VkyRLt379fK1eulCS1bNnSan/Ozs6SpDZt2qhFixaSpPDwcM2aNUvTpk0zboV68cUX5e3trc6dO0uSZs+eLR8fH3Xo0EHnzp3TggULtH79eq1du/YGfhtAzVP2SOi2bdvKYrFoyZIlGjx4sHbu3KkOHTpIkkaNGqVp06YZn7n4IQqnTp3SgAEDdP/992vevHm6cOGCJk+erLCwMB0+fFh169ZVSUmJnJyc9Mwzz+ijjz6qsI/s7GyFh4drzJgxev/995WWlqaRI0eqWbNmCgsLu75fAgDc5AgWuCnk5+frySef1NGjR+Xq6qqAgACtWbNG9957ryRp3LhxOnfunGJiYnT8+HF16tRJqampatOmzRUfo2/fvkpKStKMGTM0Y8YM1atXT8HBwUpJSZGTk5OkP+d5/M///I9+/fVX1atXTwEBAVq3bp369OlzXc4bqC0u90ho6c8g4enpWeHn9+/fr+PHj2vatGny8vKSJE2ePFkBAQH6+eef5evrq/r162v+/PmSpK+//lonTpwot5+EhAT5+PjozTfflPTnLZGbN2/WrFmzCBYAYBKTt6sIk7cB4MqUlJRoxYoVioyM1M6dO+Xv76+QkBDt3btXFotFnp6euu+++/TSSy8ZVy1OnjwpHx8fRUdH68UXX1RJSYni4uK0du1a7dq1S3XqWP872bBhw3TixAl9+umnVtt79+6tLl26aPbs2ca2xYsXa9y4cSooKLjepw4AtQ6TtwEANc7u3bsVHBysc+fOydnZ2XgktCQ99thj8vb2VvPmzbVr1y49//zzysrK0scffyxJatCggTZs2KAhQ4bo5ZdfliS1bdtWa9asKRcqLiU3N1ceHh5W2zw8PFRYWKizZ88aVx8BAFePYFFL5OTk6NixY9XdBnBdNWnSpNx8F9w82rVrp8zMTOOR0JGRkdq4caP8/f01evRoo65jx45q1qyZ+vXrp4MHD6pNmzY6e/asRowYobvuuksffPCBSkpK9MYbbyg8PFzbt28nEABADUCwqAVycnLU3s9PZ8+cqe5WgOvKqV497f/+e8LFTaqyR0K/88475Wp79OghSTpw4IDatGmjpKQkHTp0SOnp6cbT2ZKSktSwYUN99tlneuSRR66oB09PT+Xl5Vlty8vLk4uLC+EEAEwiWNQCx44d09kzZ/TM1Hm6rdXt1d0OcF38eugHvTX5v3Xs2DGCxS2i7JHQFcnMzJQk41HNZY97vvhFlGXrl3rc818FBwcbj5Muk5qaquDg4KvsHgDwVwSLWuS2VrerdfuA6m4DAK7apR4JffDgQSUlJWnQoEFq3Lixdu3apZiYGPXu3VsBAX/+f969996r8ePHKyoqSk8//bRKS0v1+uuvq06dOlZPXdu3b5+Ki4t1/PhxnTx50ggogYGBkqQxY8Zo7ty5mjBhgp566imtX79ey5cvV3Jy8o3+SgDgpkOwAABcd5d6JPThw4e1bt06zZ49W6dPn5aXl5ciIiI0ceJE4/Pt27fXF198oalTpyo4OFi2trbq3LmzUlJSrF5AOWjQIP3888/Getk7ZsoegOjj46Pk5GTFxMRozpw5atGihRYsWMCjZgGgChAsAADX3cKFCysd8/Ly0saNGy+7j3vvvdd4N01lDh06dNn9hISEaOfOnZetAwBcHdvqbgAAAABA7ccVCwCoAjwSGjc7HgcN4HIIFgBgUk5Ojvz82uvMmbPV3Qpw3dSr56Tvv99PuABQKYIFAJh07NgxnTlzVu+Mf0LtWnpWdztAlcvKydV//es9HgcN4JIIFgBQRdq19FQnX6/qbgMAgGrB5G0AAAAAplVrsJg/f74CAgLk4uIiFxcXBQcH68svvzTGQ0JCZGNjY7WMGTPGah85OTkKDw9XvXr15O7urvHjx+vChQtWNRs2bFCXLl3k4OAgX19fJSYmluslPj5erVq1kqOjo3r06KFvvvnmupwzAAAAcDOq1mDRokULvf7668rIyNCOHTvUt29fDR48WHv37jVqRo0apaNHjxrLjBkzjLGSkhKFh4eruLhYW7Zs0ZIlS5SYmKhJkyYZNdnZ2QoPD1efPn2UmZmpcePGaeTIkVqzZo1Rs2zZMsXGxmry5Mn69ttv1alTJ4WFhSk/P//GfBEAAABALVetweK+++7ToEGD1LZtW91+++169dVX5ezsrK1btxo19erVk6enp7G4uLgYY2vXrtW+ffu0dOlSBQYGauDAgXr55ZcVHx+v4uJiSVJCQoJ8fHz05ptvys/PT9HR0XrwwQc1a9YsYz8zZ87UqFGjNHz4cPn7+yshIUH16tXTokWLbtyXAQAAANRiNWaORUlJiT788EOdPn1awcHBxvb3339fTZo00R133KG4uDidOXPGGEtPT1fHjh3l4eFhbAsLC1NhYaFx1SM9PV2hoaFWxwoLC1N6erokqbi4WBkZGVY1tra2Cg0NNWoqUlRUpMLCQqsFAAAAuFVV+1Ohdu/ereDgYJ07d07Ozs765JNP5O/vL0l67LHH5O3trebNm2vXrl16/vnnlZWVpY8//liSlJubaxUqJBnrubm5l6wpLCzU2bNn9ccff6ikpKTCmv3791fa9/Tp0zV16lRzJw8AAADcJKo9WLRr106ZmZkqKCjQypUrFRkZqY0bN8rf31+jR4826jp27KhmzZqpX79+OnjwoNq0aVONXUtxcXGKjY011gsLC+XlxWMmAQAAcGuq9mBhb28vX19fSVJQUJC2b9+uOXPm6J133ilX26NHD0nSgQMH1KZNG3l6epZ7elNeXp4kydPT0/jPsm0X17i4uMjJyUl2dnays7OrsKZsHxVxcHCQg4PDVZ4tAAAAcHOqMXMsypSWlqqoqKjCsczMTElSs2bNJEnBwcHavXu31dObUlNT5eLiYtxOFRwcrLS0NKv9pKamGvM47O3tFRQUZFVTWlqqtLQ0q7keAAAAACpXrVcs4uLiNHDgQLVs2VInT55UUlKSNmzYoDVr1ujgwYNKSkrSoEGD1LhxY+3atUsxMTHq3bu3AgICJEn9+/eXv7+/nnjiCc2YMUO5ubmaOHGioqKijKsJY8aM0dy5czVhwgQ99dRTWr9+vZYvX67k5GSjj9jYWEVGRqpr167q3r27Zs+erdOnT2v48OHV8r0AAAAAtU21Bov8/Hw9+eSTOnr0qFxdXRUQEKA1a9bo3nvv1eHDh7Vu3Trjl3wvLy9FRERo4sSJxuft7Oy0atUqjR07VsHBwapfv74iIyM1bdo0o8bHx0fJycmKiYnRnDlz1KJFCy1YsEBhYWFGzcMPP6zffvtNkyZNUm5urgIDA5WSklJuQjcAAACAilVrsFi4cGGlY15eXtq4ceNl9+Ht7a3Vq1dfsiYkJEQ7d+68ZE10dLSio6MvezwAAAAA5dW4ORYAAAAAah+CBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANOqNVjMnz9fAQEBcnFxkYuLi4KDg/Xll18a4+fOnVNUVJQaN24sZ2dnRUREKC8vz2ofOTk5Cg8PV7169eTu7q7x48frwoULVjUbNmxQly5d5ODgIF9fXyUmJpbrJT4+Xq1atZKjo6N69Oihb7755rqcMwAAAHAzqtZg0aJFC73++uvKyMjQjh071LdvXw0ePFh79+6VJMXExOiLL77QihUrtHHjRh05ckQPPPCA8fmSkhKFh4eruLhYW7Zs0ZIlS5SYmKhJkyYZNdnZ2QoPD1efPn2UmZmpcePGaeTIkVqzZo1Rs2zZMsXGxmry5Mn69ttv1alTJ4WFhSk/P//GfRkAAABALVatweK+++7ToEGD1LZtW91+++169dVX5ezsrK1bt6qgoEALFy7UzJkz1bdvXwUFBWnx4sXasmWLtm7dKklau3at9u3bp6VLlyowMFADBw7Uyy+/rPj4eBUXF0uSEhIS5OPjozfffFN+fn6Kjo7Wgw8+qFmzZhl9zJw5U6NGjdLw4cPl7++vhIQE1atXT4sWLaqW7wUAAACobWrMHIuSkhJ9+OGHOn36tIKDg5WRkaHz588rNDTUqGnfvr1atmyp9PR0SVJ6ero6duwoDw8PoyYsLEyFhYXGVY/09HSrfZTVlO2juLhYGRkZVjW2trYKDQ01aipSVFSkwsJCqwUAAAC4VVV7sNi9e7ecnZ3l4OCgMWPG6JNPPpG/v79yc3Nlb28vNzc3q3oPDw/l5uZKknJzc61CRdl42dilagoLC3X27FkdO3ZMJSUlFdaU7aMi06dPl6urq7F4eXld0/kDAAAAN4NqDxbt2rVTZmamtm3bprFjxyoyMlL79u2r7rYuKy4uTgUFBcZy+PDh6m4JAAAAqDZ1qrsBe3t7+fr6SpKCgoK0fft2zZkzRw8//LCKi4t14sQJq6sWeXl58vT0lCR5enqWe3pT2VOjLq7565Ok8vLy5OLiIicnJ9nZ2cnOzq7CmrJ9VMTBwUEODg7XdtIAAADATabar1j8VWlpqYqKihQUFKS6desqLS3NGMvKylJOTo6Cg4MlScHBwdq9e7fV05tSU1Pl4uIif39/o+bifZTVlO3D3t5eQUFBVjWlpaVKS0szagAAAABcWrVesYiLi9PAgQPVsmVLnTx5UklJSdqwYYPWrFkjV1dXjRgxQrGxsWrUqJFcXFz09NNPKzg4WD179pQk9e/fX/7+/nriiSc0Y8YM5ebmauLEiYqKijKuJowZM0Zz587VhAkT9NRTT2n9+vVavny5kpOTjT5iY2MVGRmprl27qnv37po9e7ZOnz6t4cOHV8v3AgAAANQ21Ros8vPz9eSTT+ro0aNydXVVQECA1qxZo3vvvVeSNGvWLNna2ioiIkJFRUUKCwvTvHnzjM/b2dlp1apVGjt2rIKDg1W/fn1FRkZq2rRpRo2Pj4+Sk5MVExOjOXPmqEWLFlqwYIHCwsKMmocffli//fabJk2apNzcXAUGBiolJaXchG4AAAAAFavWYLFw4cJLjjs6Oio+Pl7x8fGV1nh7e2v16tWX3E9ISIh27tx5yZro6GhFR0dfsgYAAABAxWrcHAsAAAAAtQ/BAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAALiFTZ8+Xd26dVODBg3k7u6uIUOGKCsryxg/dOiQbGxsKlxWrFghSUpMTKy05uL3jW3YsEFdunSRg4ODfH19lZiYaNXLlClTyn2+ffv2N+R7gHkECwAAgFvYxo0bFRUVpa1btyo1NVXnz59X//79dfr0aUmSl5eXjh49arVMnTpVzs7OGjhwoKQ/H93/15qwsDDdc889cnd3lyRlZ2crPDxcffr0UWZmpsaNG6eRI0dqzZo1Vv106NDBaj+bN2++sV8Irlm1Pm4WAAAA1SslJcVqPTExUe7u7srIyFDv3r1lZ2cnT09Pq5pPPvlEDz30kJydnSVJTk5OcnJyMsZ/++03rV+/3urVAgkJCfLx8dGbb74pSfLz89PmzZs1a9Ysq/eL1alTp9zxUDtwxQIAAACGgoICSVKjRo0qHM/IyFBmZqZGjBhR6T7+/e9/q169enrwwQeNbenp6QoNDbWqCwsLU3p6utW2H3/8Uc2bN1fr1q31+OOPKycn51pPBTcYwQIAAACSpNLSUo0bN0533XWX7rjjjgprFi5cKD8/P915552V7mfhwoV67LHHrK5i5ObmysPDw6rOw8NDhYWFOnv2rCSpR48eSkxMVEpKiubPn6/s7GzdfffdOnnyZBWcHa43boUCAACAJCkqKkp79uypdF7D2bNnlZSUpJdeeqnSfaSnp+v777/Xe++9d9XHL5uzIUkBAQHq0aOHvL29tXz58kteIUHNQLAAAACAoqOjtWrVKm3atEktWrSosGblypU6c+aMnnzyyUr3s2DBAgUGBiooKMhqu6enp/Ly8qy25eXlycXFxerKxsXc3Nx0++2368CBA1d5NqgO3AoFAABwC7NYLIqOjtYnn3yi9evXy8fHp9LahQsX6v7771fTpk0rHD916lSlVxeCg4OVlpZmtS01NVXBwcGVHu/UqVM6ePCgmjVrdoVng+pEsAAAALiFRUVFaenSpUpKSlKDBg2Um5ur3NxcY95DmQMHDmjTpk0aOXJkpftatmyZLly4oH/84x/lxsaMGaOffvpJEyZM0P79+zVv3jwtX75cMTExRs1zzz2njRs36tChQ9qyZYv+/ve/y87OTo8++mjVnTCuG26FAgAAuIXNnz9fkhQSEmK1ffHixRo2bJixvmjRIrVo0UL9+/evdF8LFy7UAw88IDc3t3JjPj4+Sk5OVkxMjObMmaMWLVpowYIFVo+a/eWXX/Too4/q999/V9OmTdWrVy9t3bq10iskqFkIFgAAALcwi8VyRXWvvfaaXnvttUvWbNmy5ZLjISEh2rlzZ6XjH3744RX1gpqJW6EAAAAAmMYVCwAAcFPLycnRsWPHqrsN4Lpq0qSJWrZsWa09ECwAAMBNKycnR+392uvsmbOXLwZqMad6Ttr//f5qDRcECwAAcNM6duyYzp45q+FvRKlZm9uqux3gujh68Fctfi5ex44dI1gAAABcT83a3KaWHSp/PwMA85i8DQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADAtGoNFtOnT1e3bt3UoEEDubu7a8iQIcrKyrKqCQkJkY2NjdUyZswYq5qcnByFh4erXr16cnd31/jx43XhwgWrmg0bNqhLly5ycHCQr6+vEhMTy/UTHx+vVq1aydHRUT169NA333xT5ecMAAAA3IyqNVhs3LhRUVFR2rp1q1JTU3X+/Hn1799fp0+ftqobNWqUjh49aiwzZswwxkpKShQeHq7i4mJt2bJFS5YsUWJioiZNmmTUZGdnKzw8XH369FFmZqbGjRunkSNHas2aNUbNsmXLFBsbq8mTJ+vbb79Vp06dFBYWpvz8/Ov/RQAAAAC1XJ3qPHhKSorVemJiotzd3ZWRkaHevXsb2+vVqydPT88K97F27Vrt27dP69atk4eHhwIDA/Xyyy/r+eef15QpU2Rvb6+EhAT5+PjozTfflCT5+flp8+bNmjVrlsLCwiRJM2fO1KhRozR8+HBJUkJCgpKTk7Vo0SK98MIL1+P0AQAAgJtGjZpjUVBQIElq1KiR1fb3339fTZo00R133KG4uDidOXPGGEtPT1fHjh3l4eFhbAsLC1NhYaH27t1r1ISGhlrtMywsTOnp6ZKk4uJiZWRkWNXY2toqNDTUqAEAAABQuWq9YnGx0tJSjRs3TnfddZfuuOMOY/tjjz0mb29vNW/eXLt27dLzzz+vrKwsffzxx5Kk3Nxcq1AhyVjPzc29ZE1hYaHOnj2rP/74QyUlJRXW7N+/v8J+i4qKVFRUZKwXFhZe45kDAAAAtV+NCRZRUVHas2ePNm/ebLV99OjRxp87duyoZs2aqV+/fjp48KDatGlzo9s0TJ8+XVOnTq224wMAAAA1SY24FSo6OlqrVq3SV199pRYtWlyytkePHpKkAwcOSJI8PT2Vl5dnVVO2XjYvo7IaFxcXOTk5qUmTJrKzs6uwprK5HXFxcSooKDCWw4cPX+HZAgAAADefag0WFotF0dHR+uSTT7R+/Xr5+Phc9jOZmZmSpGbNmkmSgoODtXv3bqunN6WmpsrFxUX+/v5GTVpamtV+UlNTFRwcLEmyt7dXUFCQVU1paanS0tKMmr9ycHCQi4uL1QIAAADcqqr1VqioqCglJSXps88+U4MGDYw5Ea6urnJyctLBgweVlJSkQYMGqXHjxtq1a5diYmLUu3dvBQQESJL69+8vf39/PfHEE5oxY4Zyc3M1ceJERUVFycHBQZI0ZswYzZ07VxMmTNBTTz2l9evXa/ny5UpOTjZ6iY2NVWRkpLp27aru3btr9uzZOn36tPGUKAAAAACVq9ZgMX/+fEl/vgTvYosXL9awYcNkb2+vdevWGb/ke3l5KSIiQhMnTjRq7ezstGrVKo0dO1bBwcGqX7++IiMjNW3aNKPGx8dHycnJiomJ0Zw5c9SiRQstWLDAeNSsJD388MP67bffNGnSJOXm5iowMFApKSnlJnQDAAAAKK9ag4XFYrnkuJeXlzZu3HjZ/Xh7e2v16tWXrAkJCdHOnTsvWRMdHa3o6OjLHg8AAACAtRoxeRsAAABA7UawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmHZNwaJ169b6/fffy20/ceKEWrdubbopAAAAALXLNQWLQ4cOqaSkpNz2oqIi/frrr6abAgAAAFC71Lma4s8//9z485o1a+Tq6mqsl5SUKC0tTa1ataqy5gAAAADUDlcVLIYMGSJJsrGxUWRkpNVY3bp11apVK7355ptV1hwAAACA2uGqgkVpaakkycfHR9u3b1eTJk2uS1MAAAAAapdrmmORnZ1dJaFi+vTp6tatmxo0aCB3d3cNGTJEWVlZVjXnzp1TVFSUGjduLGdnZ0VERCgvL8+qJicnR+Hh4apXr57c3d01fvx4Xbhwwapmw4YN6tKlixwcHOTr66vExMRy/cTHx6tVq1ZydHRUjx499M0335g+RwAAAOBWcFVXLC6WlpamtLQ05efnG1cyyixatOiK9rFx40ZFRUWpW7duunDhgl588UX1799f+/btU/369SVJMTExSk5O1ooVK+Tq6qro6Gg98MAD+vrrryX9ObcjPDxcnp6e2rJli44ePaonn3xSdevW1WuvvSbpzyAUHh6uMWPG6P3331daWppGjhypZs2aKSwsTJK0bNkyxcbGKiEhQT169NDs2bMVFhamrKwsubu7X+vXBAAAANwSrilYTJ06VdOmTVPXrl3VrFkz2djYXNPBU1JSrNYTExPl7u6ujIwM9e7dWwUFBVq4cKGSkpLUt29fSdLixYvl5+enrVu3qmfPnlq7dq327dundevWycPDQ4GBgXr55Zf1/PPPa8qUKbK3t1dCQoJ8fHyM+R9+fn7avHmzZs2aZQSLmTNnatSoURo+fLgkKSEhQcnJyVq0aJFeeOGFazo/AAAA4FZxTcEiISFBiYmJeuKJJ6q0mYKCAklSo0aNJEkZGRk6f/68QkNDjZr27durZcuWSk9PV8+ePZWenq6OHTvKw8PDqAkLC9PYsWO1d+9ede7cWenp6Vb7KKsZN26cJKm4uFgZGRmKi4szxm1tbRUaGqr09PQqPUcAAADgZnRNwaK4uFh33nlnlTZSWlqqcePG6a677tIdd9whScrNzZW9vb3c3Nysaj08PJSbm2vUXBwqysbLxi5VU1hYqLNnz+qPP/5QSUlJhTX79++vsN+ioiIVFRUZ64WFhVd5xgAAAMDN45omb48cOVJJSUlV2khUVJT27NmjDz/8sEr3e71Mnz5drq6uxuLl5VXdLQEAAADV5pquWJw7d07vvvuu1q1bp4CAANWtW9dqfObMmVe1v+joaK1atUqbNm1SixYtjO2enp4qLi7WiRMnrK5a5OXlydPT06j569Obyp4adXHNX58klZeXJxcXFzk5OcnOzk52dnYV1pTt46/i4uIUGxtrrBcWFhIuAAAAcMu6pisWu3btUmBgoGxtbbVnzx7t3LnTWDIzM694PxaLRdHR0frkk0+0fv16+fj4WI0HBQWpbt26SktLM7ZlZWUpJydHwcHBkqTg4GDt3r1b+fn5Rk1qaqpcXFzk7+9v1Fy8j7Kasn3Y29srKCjIqqa0tFRpaWlGzV85ODjIxcXFagEAAABuVdd0xeKrr76qkoNHRUUpKSlJn332mRo0aGDMiXB1dZWTk5NcXV01YsQIxcbGqlGjRnJxcdHTTz+t4OBg9ezZU5LUv39/+fv764knntCMGTOUm5uriRMnKioqSg4ODpKkMWPGaO7cuZowYYKeeuoprV+/XsuXL1dycrLRS2xsrCIjI9W1a1d1795ds2fP1unTp42nRAEAAACo3DW/x6IqzJ8/X5IUEhJitX3x4sUaNmyYJGnWrFmytbVVRESEioqKFBYWpnnz5hm1dnZ2WrVqlcaOHavg4GDVr19fkZGRmjZtmlHj4+Oj5ORkxcTEaM6cOWrRooUWLFhgPGpWkh5++GH99ttvmjRpknJzcxUYGKiUlJRyE7oBAAAAlHdNwaJPnz6XfHfF+vXrr2g/FovlsjWOjo6Kj49XfHx8pTXe3t5avXr1JfcTEhKinTt3XrImOjpa0dHRl+0JAAAAgLVrChaBgYFW6+fPn1dmZqb27NmjyMjIqugLAAAAQC1yTcFi1qxZFW6fMmWKTp06ZaohAAAAALXPNT0VqjL/+Mc/tGjRoqrcJQAAAIBaoEqDRXp6uhwdHatylwAAAABqgWu6FeqBBx6wWrdYLDp69Kh27Nihl156qUoaAwAAAFB7XFOwcHV1tVq3tbVVu3btNG3aNPXv379KGgMAAABQe1xTsFi8eHFV9wEAAACgFjP1gryMjAx9//33kqQOHTqoc+fOVdIUAAAAgNrlmoJFfn6+HnnkEW3YsEFubm6SpBMnTqhPnz768MMP1bRp06rsEQAAAEANd01PhXr66ad18uRJ7d27V8ePH9fx48e1Z88eFRYW6plnnqnqHgEAAADUcNd0xSIlJUXr1q2Tn5+fsc3f31/x8fFM3gYAAABuQdd0xaK0tFR169Ytt71u3boqLS013RQAAACA2uWagkXfvn317LPP6siRI8a2X3/9VTExMerXr1+VNQcAAACgdrimYDF37lwVFhaqVatWatOmjdq0aSMfHx8VFhbq7bffruoeAQAAANRw1zTHwsvLS99++63WrVun/fv3S5L8/PwUGhpapc0BAAAAqB2u6orF+vXr5e/vr8LCQtnY2Ojee+/V008/raefflrdunVThw4d9J///Od69QoAAACghrqqYDF79myNGjVKLi4u5cZcXV31X//1X5o5c2aVNQcAAACgdriqYPHdd99pwIABlY73799fGRkZppsCAAAAULtcVbDIy8ur8DGzZerUqaPffvvNdFMAAAAAaperCha33Xab9uzZU+n4rl271KxZM9NNAQAAAKhdripYDBo0SC+99JLOnTtXbuzs2bOaPHmy/va3v1VZcwAAAABqh6t63OzEiRP18ccf6/bbb1d0dLTatWsnSdq/f7/i4+NVUlKi//3f/70ujQIAAACoua4qWHh4eGjLli0aO3as4uLiZLFYJEk2NjYKCwtTfHy8PDw8rkujAAAAAGquq35Bnre3t1avXq0//vhDBw4ckMViUdu2bdWwYcPr0R8AAACAWuCa3rwtSQ0bNlS3bt2qshcAAAAAtdRVTd4GAAAAgIoQLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAplVrsNi0aZPuu+8+NW/eXDY2Nvr000+txocNGyYbGxurZcCAAVY1x48f1+OPPy4XFxe5ublpxIgROnXqlFXNrl27dPfdd8vR0VFeXl6aMWNGuV5WrFih9u3by9HRUR07dtTq1aur/HwBAACAm1W1BovTp0+rU6dOio+Pr7RmwIABOnr0qLF88MEHVuOPP/649u7dq9TUVK1atUqbNm3S6NGjjfHCwkL1799f3t7eysjI0L/+9S9NmTJF7777rlGzZcsWPfrooxoxYoR27typIUOGaMiQIdqzZ0/VnzQAAABwE6pTnQcfOHCgBg4ceMkaBwcHeXp6Vjj2/fffKyUlRdu3b1fXrl0lSW+//bYGDRqkN954Q82bN9f777+v4uJiLVq0SPb29urQoYMyMzM1c+ZMI4DMmTNHAwYM0Pjx4yVJL7/8slJTUzV37lwlJCRU4RkDAAAAN6caP8diw4YNcnd3V7t27TR27Fj9/vvvxlh6errc3NyMUCFJoaGhsrW11bZt24ya3r17y97e3qgJCwtTVlaW/vjjD6MmNDTU6rhhYWFKT0+vtK+ioiIVFhZaLQAAAMCtqkYHiwEDBujf//630tLS9M9//lMbN27UwIEDVVJSIknKzc2Vu7u71Wfq1KmjRo0aKTc316jx8PCwqilbv1xN2XhFpk+fLldXV2Px8vIyd7IAAABALVatt0JdziOPPGL8uWPHjgoICFCbNm20YcMG9evXrxo7k+Li4hQbG2usFxYWEi4AAABwy6rRVyz+qnXr1mrSpIkOHDggSfL09FR+fr5VzYULF3T8+HFjXoanp6fy8vKsasrWL1dT2dwO6c+5Hy4uLlYLAAAAcKuqVcHil19+0e+//65mzZpJkoKDg3XixAllZGQYNevXr1dpaal69Ohh1GzatEnnz583alJTU9WuXTs1bNjQqElLS7M6VmpqqoKDg6/3KQEAAAA3hWoNFqdOnVJmZqYyMzMlSdnZ2crMzFROTo5OnTql8ePHa+vWrTp06JDS0tI0ePBg+fr6KiwsTJLk5+enAQMGaNSoUfrmm2/09ddfKzo6Wo888oiaN28uSXrsscdkb2+vESNGaO/evVq2bJnmzJljdRvTs88+q5SUFL355pvav3+/pkyZoh07dig6OvqGfycAAABAbVStwWLHjh3q3LmzOnfuLEmKjY1V586dNWnSJNnZ2WnXrl26//77dfvtt2vEiBEKCgrSf/7zHzk4OBj7eP/999W+fXv169dPgwYNUq9evazeUeHq6qq1a9cqOztbQUFB+p//+R9NmjTJ6l0Xd955p5KSkvTuu++qU6dOWrlypT799FPdcccdN+7LAAAAAGqxap28HRISIovFUun4mjVrLruPRo0aKSkp6ZI1AQEB+s9//nPJmqFDh2ro0KGXPR4AAACA8mrVHAsAAAAANRPBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGnVGiw2bdqk++67T82bN5eNjY0+/fRTq3GLxaJJkyapWbNmcnJyUmhoqH788UermuPHj+vxxx+Xi4uL3NzcNGLECJ06dcqqZteuXbr77rvl6OgoLy8vzZgxo1wvK1asUPv27eXo6KiOHTtq9erVVX6+AAAAwM2qWoPF6dOn1alTJ8XHx1c4PmPGDL311ltKSEjQtm3bVL9+fYWFhencuXNGzeOPP669e/cqNTVVq1at0qZNmzR69GhjvLCwUP3795e3t7cyMjL0r3/9S1OmTNG7775r1GzZskWPPvqoRowYoZ07d2rIkCEaMmSI9uzZc/1OHgAAALiJ1KnOgw8cOFADBw6scMxisWj27NmaOHGiBg8eLEn697//LQ8PD3366ad65JFH9P333yslJUXbt29X165dJUlvv/22Bg0apDfeeEPNmzfX+++/r+LiYi1atEj29vbq0KGDMjMzNXPmTCOAzJkzRwMGDND48eMlSS+//LJSU1M1d+5cJSQk3IBvAgAAAKjdauwci+zsbOXm5io0NNTY5urqqh49eig9PV2SlJ6eLjc3NyNUSFJoaKhsbW21bds2o6Z3796yt7c3asLCwpSVlaU//vjDqLn4OGU1ZcepSFFRkQoLC60WAAAA4FZVY4NFbm6uJMnDw8Nqu4eHhzGWm5srd3d3q/E6deqoUaNGVjUV7ePiY1RWUzZekenTp8vV1dVYvLy8rvYUAQAAgJtGjQ0WNV1cXJwKCgqM5fDhw9XdEgAAAFBtamyw8PT0lCTl5eVZbc/LyzPGPD09lZ+fbzV+4cIFHT9+3Kqmon1cfIzKasrGK+Lg4CAXFxerBQAAALhV1dhg4ePjI09PT6WlpRnbCgsLtW3bNgUHB0uSgoODdeLECWVkZBg169evV2lpqXr06GHUbNq0SefPnzdqUlNT1a5dOzVs2NCoufg4ZTVlxwEAAABwadUaLE6dOqXMzExlZmZK+nPCdmZmpnJycmRjY6Nx48bplVde0eeff67du3frySefVPPmzTVkyBBJkp+fnwYMGKBRo0bpm2++0ddff63o6Gg98sgjat68uSTpsccek729vUaMGKG9e/dq2bJlmjNnjmJjY40+nn32WaWkpOjNN9/U/v37NWXKFO3YsUPR0dE3+isBAAAAaqVqfdzsjh071KdPH2O97Jf9yMhIJSYmasKECTp9+rRGjx6tEydOqFevXkpJSZGjo6Pxmffff1/R0dHq16+fbG1tFRERobfeessYd3V11dq1axUVFaWgoCA1adJEkyZNsnrXxZ133qmkpCRNnDhRL774otq2batPP/1Ud9xxxw34FgAAAIDar1qDRUhIiCwWS6XjNjY2mjZtmqZNm1ZpTaNGjZSUlHTJ4wQEBOg///nPJWuGDh2qoUOHXrphAAAAABWqsXMsAAAAANQeBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGk1OlhMmTJFNjY2Vkv79u2N8XPnzikqKkqNGzeWs7OzIiIilJeXZ7WPnJwchYeHq169enJ3d9f48eN14cIFq5oNGzaoS5cucnBwkK+vrxITE2/E6QEAAAA3jRodLCSpQ4cOOnr0qLFs3rzZGIuJidEXX3yhFStWaOPGjTpy5IgeeOABY7ykpETh4eEqLi7Wli1btGTJEiUmJmrSpElGTXZ2tsLDw9WnTx9lZmZq3LhxGjlypNasWXNDzxMAAACozepUdwOXU6dOHXl6epbbXlBQoIULFyopKUl9+/aVJC1evFh+fn7aunWrevbsqbVr12rfvn1at26dPDw8FBgYqJdfflnPP/+8pkyZInt7eyUkJMjHx0dvvvmmJMnPz0+bN2/WrFmzFBYWdkPPFQAAAKitavwVix9//FHNmzdX69at9fjjjysnJ0eSlJGRofPnzys0NNSobd++vVq2bKn09HRJUnp6ujp27CgPDw+jJiwsTIWFhdq7d69Rc/E+ymrK9gEAAADg8mr0FYsePXooMTFR7dq109GjRzV16lTdfffd2rNnj3Jzc2Vvby83Nzerz3h4eCg3N1eSlJubaxUqysbLxi5VU1hYqLNnz8rJyanC3oqKilRUVGSsFxYWmjpXAAAAoDar0cFi4MCBxp8DAgLUo0cPeXt7a/ny5ZX+wn+jTJ8+XVOnTq3WHgAAAICaosbfCnUxNzc33X777Tpw4IA8PT1VXFysEydOWNXk5eUZczI8PT3LPSWqbP1yNS4uLpcML3FxcSooKDCWw4cPmz09AAAAoNaqVcHi1KlTOnjwoJo1a6agoCDVrVtXaWlpxnhWVpZycnIUHBwsSQoODtbu3buVn59v1KSmpsrFxUX+/v5GzcX7KKsp20dlHBwc5OLiYrUAAAAAt6oaHSyee+45bdy4UYcOHdKWLVv097//XXZ2dnr00Ufl6uqqESNGKDY2Vl999ZUyMjI0fPhwBQcHq2fPnpKk/v37y9/fX0888YS+++47rVmzRhMnTlRUVJQcHBwkSWPGjNFPP/2kCRMmaP/+/Zo3b56WL1+umJiY6jx1AAAAoFap0XMsfvnlFz366KP6/fff1bRpU/Xq1Utbt25V06ZNJUmzZs2Sra2tIiIiVFRUpLCwMM2bN8/4vJ2dnVatWqWxY8cqODhY9evXV2RkpKZNm2bU+Pj4KDk5WTExMZozZ45atGihBQsW8KhZAAAA4CrU6GDx4YcfXnLc0dFR8fHxio+Pr7TG29tbq1evvuR+QkJCtHPnzmvqEQAAAEANvxUKAAAAQO1AsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAaweIv4uPj1apVKzk6OqpHjx765ptvqrslAAAAoMYjWFxk2bJlio2N1eTJk/Xtt9+qU6dOCgsLU35+fnW3BgAAANRoBIuLzJw5U6NGjdLw4cPl7++vhIQE1atXT4sWLaru1gAAAIAajWDxf4qLi5WRkaHQ0FBjm62trUJDQ5Wenl6NnQEAAAA1X53qbqCmOHbsmEpKSuTh4WG13cPDQ/v37y9XX1RUpKKiImO9oKBAklRYWFjlvZ06dUqS9NP+73Tu7Okq3z9QExz5+YCkP//3fj1+jq6nsp/RzB9zdPps0WWqgdrnx1/yJNXun8+f92ar6My5au4GuD5ys49Kuj4/o2X7s1gsl621sVxJ1S3gyJEjuu2227RlyxYFBwcb2ydMmKCNGzdq27ZtVvVTpkzR1KlTb3SbAAAAwA13+PBhtWjR4pI1XLH4P02aNJGdnZ3y8vKstufl5cnT07NcfVxcnGJjY4310tJSHT9+XI0bN5aNjc117xfXV2Fhoby8vHT48GG5uLhUdzsA/oKfUaDm4ufz5mKxWHTy5Ek1b978srUEi/9jb2+voKAgpaWlaciQIZL+DAtpaWmKjo4uV+/g4CAHBwerbW5ubjegU9xILi4u/J8iUIPxMwrUXPx83jxcXV2vqI5gcZHY2FhFRkaqa9eu6t69u2bPnq3Tp09r+PDh1d0aAAAAUKMRLC7y8MMP67ffftOkSZOUm5urwMBApaSklJvQDQAAAMAaweIvoqOjK7z1CbcWBwcHTZ48udztbgBqBn5GgZqLn89bF0+FAgAAAGAaL8gDAAAAYBrBAgAAAIBpBAsAAAAAphEscEvatGmT7rvvPjVv3lw2Njb69NNPL/uZc+fOKSoqSo0bN5azs7MiIiLKvVARAICbXXx8vFq1aiVHR0f16NFD33zzzSXrX331Vd15552qV68e7/y6yREscEs6ffq0OnXqpPj4+Cv+TExMjL744gutWLFCGzdu1JEjR/TAAw9cxy6B2u1KA3yfPn20YMECfffdd3r00Ufl5eUlJycn+fn5ac6cOZc9zt69exUREaFWrVrJxsZGs2fPrrR2+PDhmjhxog4dOqQRI0bIx8dHTk5OatOmjSZPnqzi4uJrPFvg1rBs2TLFxsZq8uTJ+vbbb9WpUyeFhYUpPz+/0s8UFxdr6NChGjt27A3sFNWBx83iljRw4EANHDjwiusLCgq0cOFCJSUlqW/fvpKkxYsXy8/PT1u3blXPnj2vV6tArVUW4J966qlKQ/jx48f19ddf68MPP1RycrLc3d21dOlSeXl5acuWLRo9erTs7Owu+RjwM2fOqHXr1ho6dKhiYmIqrSspKdGqVauUnJys/fv3q7S0VO+88458fX21Z88ejRo1SqdPn9Ybb7xh+tyBm9XMmTM1atQo4+XBCQkJSk5O1qJFi/TCCy9U+JmpU6dKkhITE29Um6gmBAvgCmRkZOj8+fMKDQ01trVv314tW7ZUeno6wQKowJUE+OTkZHXp0kUeHh566qmnrMZat26t9PR0ffzxx5cMFt26dVO3bt0kqdJfbCRpy5Ytqlu3rrp16yYbGxsNGDDA6lhZWVmaP38+wQKoRHFxsTIyMhQXF2dss7W1VWhoqNLT06uxM9QU3AoFXIHc3FzZ29uXuzfUw8NDubm51dMUcBP4/PPPNXjw4ErHCwoK1KhRoyo71n333ScbG5vrfizgZnTs2DGVlJTIw8PDajt/F6IMwQL4i9dee03Ozs7GkpOTU90tATeloqIipaSk6P77769wfMuWLVq2bJlGjx5dJcf77LPPKj3WgQMH9Pbbb+u//uu/quRYwK1ozJgxVn9/4tZDsAD+YsyYMcrMzDSW5s2by9PTU8XFxTpx4oRVbV5enjw9PaunUaCWW79+vdzd3dWhQ4dyY3v27NHgwYM1efJk9e/fX5KUk5Nj9UvLa6+9dsXH+v7773XkyBH169ev3Nivv/6qAQMGaOjQoRo1atS1nxBwk2vSpIns7OzKPRGx7O/CadOmWf39iVsPcyyAv2jUqFG52yGCgoJUt25dpaWlKSIiQpKUlZWlnJwcBQcHV0ebQK33+eefV3gFYd++ferXr59Gjx6tiRMnGtubN29u9cvK1dy29Pnnn+vee++Vo6Oj1fYjR46oT58+uvPOO/Xuu+9e/UkAtxB7e3sFBQUpLS1NQ4YMkSSVlpYqLS1N0dHRcnd3l7u7e/U2iWpFsMAt6dSpUzpw4ICxnp2drczMTDVq1EgtW7YsV+/q6qoRI0YoNjZWjRo1kouLi55++mkFBwczcRu4BhaLRV988YWWLl1qtX3v3r3q27evIiMj9eqrr1qN1alTR76+vtd0vM8++6zcLVW//vqr+vTpo6CgIC1evFi2tlzEBy4nNjZWkZGR6tq1q7p3767Zs2fr9OnTxlOiKpKTk6Pjx48rJydHJSUlxj8Q+Pr6csvUTYZggVvSjh071KdPH2M9NjZWkhQZGVnp4/BmzZolW1tbRUREqKioSGFhYZo3b96NaBeolS4V4PPz83XmzBn16tXLGN+zZ4/69u2rsLAwxcbGGpNB7ezs1LRp00qPU1xcrH379hl//vXXX5WZmSlnZ2f5+voqPz9fO3bs0Oeff2585tdff1VISIi8vb31xhtv6LfffjPGuL0RqNzDDz+s3377TZMmTVJubq4CAwOVkpJSbkL3xSZNmqQlS5YY6507d5YkffXVVwoJCbneLeMGsrFYLJbqbgIAcPPZsGGDVYAvExkZKS8vL2VnZ1tdsZgyZYrxvPuLeXt769ChQ5Ue59ChQ/Lx8Sm3/Z577tGGDRu0cOFCLV68WJs3bzbGEhMTK/0XVv5aBIBrQ7AAANxwAQEBmjhxoh566KHrfqz7779fvXr10oQJE677sQDgVsYNpQCAG6q4uFgRERGXfXleVenVq5ceffTRG3IsALiVccUCAAAAgGlcsQAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAcEtITEyUm5tbdbcBADctggUA4IYYNmyYbGxsZGNjo7p168rHx0cTJkzQuXPnqqWfKVOmKDAwsFqODQA3ozrV3QAA4NYxYMAALV68WOfPn1dGRoYiIyNlY2Ojf/7zn9XdGgDAJK5YAABuGAcHB3l6esrLy0tDhgxRaGioUlNTJUmlpaWaPn26fHx85OTkpE6dOmnlypXGZ//44w89/vjjatq0qZycnNS2bVstXrxYkrRhwwbZ2NjoxIkTRn1mZqZsbGx06NChcn0kJiZq6tSp+u6774yrKImJidfz1AHgpscVCwBAtdizZ4+2bNkib29vSdL06dO1dOlSJSQkqG3bttq0aZP+8Y9/qGnTprrnnnv00ksvad++ffryyy/VpEkTHThwQGfPnr2mYz/88MPas2ePUlJStG7dOkmSq6trlZ0bANyKCBYAgBtm1apVcnZ21oULF1RUVCRbW1vNnTtXRUVFeu2117Ru3ToFBwdLklq3bq3NmzfrnXfe0T333KOcnBx17txZXbt2lSS1atXqmvtwcnKSs7Oz6tSpI09Pz6o4NQC45REsAAA3TJ8+fTR//nydPn1as2bNUp06dRQREaG9e/fqzJkzuvfee63qi4uL1blzZ0nS2LFjFRERoW+//Vb9+/fXkCFDdOedd1bHaQAAKkCwAADcMPXr15evr68kadGiRerUqZMWLlyoO+64Q5KUnJys2267zeozDg4OkqSBAwfq559/1urVq5Wamqp+/fopKipKb7zxhmxt/5wyaLFYjM+dP3/+RpwSAOD/ECwAANXC1tZWL774omJjY/XDDz/IwcFBOTk5uueeeyr9TNOmTRUZGanIyEjdfffdGj9+vN544w01bdpUknT06FE1bNhQ0p+Tty/F3t5eJSUlVXY+AHCrI1gAAKrN0KFDNX78eL3zzjt67rnnFBMTo9LSUvXq1UsFBQX6+uuv5eLiosjISE2aNElBQUHq0KGDioqKtGrVKvn5+UmSfH195eXlpSlTpujVV1/VDz/8oDfffPOSx27VqpWys7OVmZmpFi1aqEGDBsbVEQDA1SNYAACqTZ06dRQdHa0ZM2YoOztbTZs21fTp0/XTTz/Jzc1NXbp00YsvvijpzysMcXFxOnTokJycnHT33Xfrww8/lCTVrVtXH3zwgcaOHauAgAB169ZNr7zyioYOHVrpsSMiIvTxxx+rT58+OnHihBYvXqxhw4bdiNMGgJuSjeXiG1IBAAAA4BrwgjwAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBp/x+yg/aOtygmkAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#A labeled bar chart of the result distribution\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "data=df['result'].value_counts().index\n",
    "values=list(df['result'].value_counts())\n",
    "ax = sns.barplot(x=data,y=values,palette='pastel', edgecolor='black')\n",
    "\n",
    "# Add labels on top of bars\n",
    "for i, row in enumerate(data):\n",
    "    ax.text(i, float(values[i])+ 0.1, values[i], ha='center', va='bottom')\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Result Distribution')\n",
    "plt.xlabel('Result')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bdfcc8",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5237225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w.mat.mean</th>\n",
       "      <th>b.mat.mean</th>\n",
       "      <th>w.mob.mean</th>\n",
       "      <th>b.mob.mean</th>\n",
       "      <th>half.moves</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.166667</td>\n",
       "      <td>28.135417</td>\n",
       "      <td>39.312500</td>\n",
       "      <td>27.125000</td>\n",
       "      <td>95</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.851852</td>\n",
       "      <td>33.703704</td>\n",
       "      <td>37.185185</td>\n",
       "      <td>33.074074</td>\n",
       "      <td>53</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.301587</td>\n",
       "      <td>31.920635</td>\n",
       "      <td>32.625000</td>\n",
       "      <td>37.451613</td>\n",
       "      <td>62</td>\n",
       "      <td>0-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.878261</td>\n",
       "      <td>23.260870</td>\n",
       "      <td>27.172414</td>\n",
       "      <td>32.035088</td>\n",
       "      <td>114</td>\n",
       "      <td>0-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.775510</td>\n",
       "      <td>34.959184</td>\n",
       "      <td>41.040000</td>\n",
       "      <td>32.958333</td>\n",
       "      <td>48</td>\n",
       "      <td>1/2-1/2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w.mat.mean  b.mat.mean  w.mob.mean  b.mob.mean  half.moves   result\n",
       "0   28.166667   28.135417   39.312500   27.125000          95      1-0\n",
       "1   33.851852   33.703704   37.185185   33.074074          53      1-0\n",
       "2   31.301587   31.920635   32.625000   37.451613          62      0-1\n",
       "3   21.878261   23.260870   27.172414   32.035088         114      0-1\n",
       "4   34.775510   34.959184   41.040000   32.958333          48  1/2-1/2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retain only the following 6 columns\n",
    "new_df=df[['w.mat.mean', 'b.mat.mean', 'w.mob.mean', 'b.mob.mean', 'half.moves', 'result']].copy()\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c4fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: w.mat.mean, dtype: float64)\n",
      "Series([], Name: b.mat.mean, dtype: float64)\n",
      "Series([], Name: w.mob.mean, dtype: float64)\n",
      "Series([], Name: b.mob.mean, dtype: float64)\n",
      "Series([], Name: half.moves, dtype: int64)\n",
      "Series([], Name: result, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "#Looking for nul values and empty values\n",
    "for cols in new_df.columns:\n",
    "    print(new_df[cols].loc[new_df[cols].isna()==True])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bad2aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the shape of new data \n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2da34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w.mat.mean    float64\n",
       "b.mat.mean    float64\n",
       "w.mob.mean    float64\n",
       "b.mob.mean    float64\n",
       "half.moves      int64\n",
       "result         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "494acda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w.mat.mean</th>\n",
       "      <th>b.mat.mean</th>\n",
       "      <th>w.mob.mean</th>\n",
       "      <th>b.mob.mean</th>\n",
       "      <th>half.moves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.471540</td>\n",
       "      <td>29.456648</td>\n",
       "      <td>33.083914</td>\n",
       "      <td>30.808188</td>\n",
       "      <td>80.524530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.700711</td>\n",
       "      <td>5.722296</td>\n",
       "      <td>4.899840</td>\n",
       "      <td>4.488267</td>\n",
       "      <td>33.794235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.768683</td>\n",
       "      <td>9.127049</td>\n",
       "      <td>11.312500</td>\n",
       "      <td>12.272727</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.206546</td>\n",
       "      <td>25.148607</td>\n",
       "      <td>29.653061</td>\n",
       "      <td>27.731616</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.317267</td>\n",
       "      <td>30.308824</td>\n",
       "      <td>33.390244</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.017938</td>\n",
       "      <td>36.700000</td>\n",
       "      <td>33.978723</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>50.842105</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>369.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          w.mat.mean     b.mat.mean     w.mob.mean     b.mob.mean  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       29.471540      29.456648      33.083914      30.808188   \n",
       "std         5.700711       5.722296       4.899840       4.488267   \n",
       "min         7.768683       9.127049      11.312500      12.272727   \n",
       "25%        25.206546      25.148607      29.653061      27.731616   \n",
       "50%        30.317267      30.308824      33.390244      31.000000   \n",
       "75%        34.000000      34.017938      36.700000      33.978723   \n",
       "max        39.000000      39.000000      50.842105      48.250000   \n",
       "\n",
       "          half.moves  \n",
       "count  100000.000000  \n",
       "mean       80.524530  \n",
       "std        33.794235  \n",
       "min         6.000000  \n",
       "25%        58.000000  \n",
       "50%        77.000000  \n",
       "75%       100.000000  \n",
       "max       369.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look for the data characteristics\n",
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "975c081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# looking for how the data is spread\n",
    "\n",
    "for cols in new_df.columns:\n",
    "    plt.figure(figsize=(6, 4))  # Create a new figure for each boxplot\n",
    "    sns.boxplot(data=new_df, x=cols)\n",
    "    plt.title(f'Boxplot of {cols}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d73125",
   "metadata": {},
   "source": [
    "Looks like the data is spread too much and this needs to be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ccf76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode result to numeric using the following mapping\n",
    "new_df.result=new_df.result.map({'1-0':int(1), '0-1':int(0), '1/2-1/2':int(2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9638530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an 85% training / 15% test split using train_test_split()\n",
    "#Train test split\n",
    "X_train,X_test,y_train,y_test =train_test_split(new_df.drop('result',axis=1),new_df['result'],test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca6fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the 5 numeric predictor columns using StandardScaler() \n",
    "# Test upload\n",
    "scalar=StandardScaler()\n",
    "\n",
    "X_train=scalar.fit_transform(X_train)\n",
    "X_test=scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97131dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Dataset and the Data Loader\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self,features,labels):\n",
    "        self.features=torch.tensor(features,dtype=torch.float32)\n",
    "        self.labels=torch.tensor(np.array(labels),dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len( self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return(self.features[index],self.labels[index])\n",
    "\n",
    "train_dataset=CustomDataSet(X_train,y_train)\n",
    "test_dataset=CustomDataSet(X_test,y_test)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=128,shuffle=True,pin_memory=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=128,shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f8c26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Neural network Class\n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self,input_featutes):\n",
    "        super().__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Linear(input_featutes,50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,3),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c8a1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Learning rate and Epochs\n",
    "learning_rate=0.05\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eec8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Deine the model with the criterion and optimizer\n",
    "\n",
    "model=MyNN(X_train.shape[1])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=learning_rate,weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1fb2595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9491725909978823\n",
      "Epoch 2, Loss: 0.9068320102261421\n",
      "Epoch 3, Loss: 0.8799159352044413\n",
      "Epoch 4, Loss: 0.8513026666820498\n",
      "Epoch 5, Loss: 0.829006440567791\n",
      "Epoch 6, Loss: 0.8170230410152808\n",
      "Epoch 7, Loss: 0.8106344945448681\n",
      "Epoch 8, Loss: 0.8074333405136166\n",
      "Epoch 9, Loss: 0.8055501458340122\n",
      "Epoch 10, Loss: 0.8045422957355814\n",
      "Epoch 11, Loss: 0.8042315989508665\n",
      "Epoch 12, Loss: 0.8029835375628077\n",
      "Epoch 13, Loss: 0.8037387213312593\n",
      "Epoch 14, Loss: 0.8023044422156829\n",
      "Epoch 15, Loss: 0.8019580115949301\n",
      "Epoch 16, Loss: 0.8014794917034923\n",
      "Epoch 17, Loss: 0.8002887307701254\n",
      "Epoch 18, Loss: 0.8007522508614046\n",
      "Epoch 19, Loss: 0.7998446827544305\n",
      "Epoch 20, Loss: 0.7997090780645385\n",
      "Epoch 21, Loss: 0.7996115420994006\n",
      "Epoch 22, Loss: 0.7991201379245385\n",
      "Epoch 23, Loss: 0.798770999549923\n",
      "Epoch 24, Loss: 0.7980339566567787\n",
      "Epoch 25, Loss: 0.7981360180933673\n",
      "Epoch 26, Loss: 0.7974063977263027\n",
      "Epoch 27, Loss: 0.7973398274945137\n",
      "Epoch 28, Loss: 0.7973339562129257\n",
      "Epoch 29, Loss: 0.7977182325563933\n",
      "Epoch 30, Loss: 0.7970144036120939\n",
      "Epoch 31, Loss: 0.7964130733246194\n",
      "Epoch 32, Loss: 0.795984156866719\n",
      "Epoch 33, Loss: 0.7962404794262764\n",
      "Epoch 34, Loss: 0.7955600026854895\n",
      "Epoch 35, Loss: 0.79535462193023\n",
      "Epoch 36, Loss: 0.7952216992700907\n",
      "Epoch 37, Loss: 0.7945467591285705\n",
      "Epoch 38, Loss: 0.7945633151925596\n",
      "Epoch 39, Loss: 0.7944141430962355\n",
      "Epoch 40, Loss: 0.7950300276727604\n",
      "Epoch 41, Loss: 0.7946236090552538\n",
      "Epoch 42, Loss: 0.7941962275289951\n",
      "Epoch 43, Loss: 0.794280765648175\n",
      "Epoch 44, Loss: 0.7938875028065273\n",
      "Epoch 45, Loss: 0.7937190996973138\n",
      "Epoch 46, Loss: 0.7938918571723135\n",
      "Epoch 47, Loss: 0.7929619769404705\n",
      "Epoch 48, Loss: 0.7933482231054091\n",
      "Epoch 49, Loss: 0.7933345639616027\n",
      "Epoch 50, Loss: 0.792985269091183\n",
      "Epoch 51, Loss: 0.7921987787225193\n",
      "Epoch 52, Loss: 0.7926052061238683\n",
      "Epoch 53, Loss: 0.7927429399992291\n",
      "Epoch 54, Loss: 0.7931356685502189\n",
      "Epoch 55, Loss: 0.792986219807675\n",
      "Epoch 56, Loss: 0.7916883042880467\n",
      "Epoch 57, Loss: 0.7928595379779213\n",
      "Epoch 58, Loss: 0.7920597071934463\n",
      "Epoch 59, Loss: 0.792306481447435\n",
      "Epoch 60, Loss: 0.7919260560121751\n",
      "Epoch 61, Loss: 0.7918920636177063\n",
      "Epoch 62, Loss: 0.7918298189801382\n",
      "Epoch 63, Loss: 0.7912288427352905\n",
      "Epoch 64, Loss: 0.7916127752540703\n",
      "Epoch 65, Loss: 0.7915370454465537\n",
      "Epoch 66, Loss: 0.7913371786139065\n",
      "Epoch 67, Loss: 0.7912489752124127\n",
      "Epoch 68, Loss: 0.7916807515280587\n",
      "Epoch 69, Loss: 0.791809404254856\n",
      "Epoch 70, Loss: 0.7917207536840798\n",
      "Epoch 71, Loss: 0.791510472172185\n",
      "Epoch 72, Loss: 0.7912459337621703\n",
      "Epoch 73, Loss: 0.7916830493991537\n",
      "Epoch 74, Loss: 0.79101549888912\n",
      "Epoch 75, Loss: 0.7913163972080202\n",
      "Epoch 76, Loss: 0.7911610806794991\n",
      "Epoch 77, Loss: 0.791206255353483\n",
      "Epoch 78, Loss: 0.7917343844148449\n",
      "Epoch 79, Loss: 0.7908129309801231\n",
      "Epoch 80, Loss: 0.7923454698763396\n",
      "Epoch 81, Loss: 0.7909973859786987\n",
      "Epoch 82, Loss: 0.7912076428420561\n",
      "Epoch 83, Loss: 0.7918070791359234\n",
      "Epoch 84, Loss: 0.7910559524270825\n",
      "Epoch 85, Loss: 0.7913471070447362\n",
      "Epoch 86, Loss: 0.7903742863719625\n",
      "Epoch 87, Loss: 0.7900972441623085\n",
      "Epoch 88, Loss: 0.7906254727141302\n",
      "Epoch 89, Loss: 0.7905809872132495\n",
      "Epoch 90, Loss: 0.7911816221430785\n",
      "Epoch 91, Loss: 0.790190628462268\n",
      "Epoch 92, Loss: 0.7908525306479375\n",
      "Epoch 93, Loss: 0.7904298195265289\n",
      "Epoch 94, Loss: 0.7906731877111851\n",
      "Epoch 95, Loss: 0.7901349833585266\n",
      "Epoch 96, Loss: 0.7904497367098815\n",
      "Epoch 97, Loss: 0.7906338061605181\n",
      "Epoch 98, Loss: 0.7900244800668014\n",
      "Epoch 99, Loss: 0.7903746154971589\n",
      "Epoch 100, Loss: 0.7905542427435853\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "total=0\n",
    "correct=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_epoch_loss=0\n",
    "\n",
    "    for batch_features,batch_labels in train_loader:\n",
    "\n",
    "        #move data to gpu\n",
    "        batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        outputs=model(batch_features)\n",
    "\n",
    "        #Calculate loss\n",
    "        loss= criterion(outputs,batch_labels)\n",
    "\n",
    "        #backpass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #update grads\n",
    "        optimizer.step()\n",
    "\n",
    "        total_epoch_loss= total_epoch_loss+loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "        # print(f'predicted:{predicted}Labels {batch_labels}')\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss=total_epoch_loss/len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    print(f'Epoch {epoch +1 }, Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b82cea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAo69JREFUeJzs3Xd8U/X+x/F3kjbdi05aC2XJlg0CCg60iqLgQEFlqOAAB7hAGYpXql5FVBSUn6BX4YIDvKiAAldEFNmiyN5QaKFA907y+6NtsJeyS0+Svp6PRx6Qk5OTz0Erx3c+388xORwOhwAAAAAAAIAqZDa6AAAAAAAAAFQ/hFIAAAAAAACocoRSAAAAAAAAqHKEUgAAAAAAAKhyhFIAAAAAAACocoRSAAAAAAAAqHKEUgAAAAAAAKhyhFIAAAAAAACocoRSAAAAAAAAqHKEUgDc1oABA5SQkHBe733xxRdlMpkqtyAAAIALwLUNgOqGUApApTOZTGf1WLp0qdGlGmLAgAEKDAw0ugwAAHCWuLY5e71795bJZNJzzz1ndCkA3IDJ4XA4jC4CgGf57LPPyj3/17/+pUWLFunTTz8tt/26665TdHT0eX9OUVGR7Ha7fHx8zvm9xcXFKi4ulq+v73l//vkaMGCAvvzyS2VnZ1f5ZwMAgHPHtc3ZyczMVHR0tGJiYmSz2bR37166twCclpfRBQDwPPfee2+557/99psWLVp00vb/lZubK39//7P+HG9v7/OqT5K8vLzk5cV/AgEAwJlxbXN2vvrqK9lsNk2bNk3XXHONli1bpq5duxpaU0UcDofy8/Pl5+dndClAtcfyPQCGuOqqq9SsWTOtXbtWXbp0kb+/v55//nlJ0n/+8x/ddNNNio2NlY+Pj+rVq6eXX35ZNput3DH+d+7Cnj17ZDKZ9MYbb+jDDz9UvXr15OPjo3bt2mn16tXl3lvR3AWTyaShQ4fq66+/VrNmzeTj46OmTZtq4cKFJ9W/dOlStW3bVr6+vqpXr54++OCDSp/l8MUXX6hNmzby8/NTRESE7r33XiUnJ5fbJyUlRQMHDtQll1wiHx8f1axZU7feeqv27Nnj3GfNmjVKTExURESE/Pz8VKdOHd1///2VVicAAODaRpJmzJih6667TldffbUaN26sGTNmVLjfli1b1Lt3b0VGRsrPz08NGzbUCy+8UG6f5ORkPfDAA84/szp16uiRRx5RYWHhKc9Xkj7++GOZTKZy10IJCQm6+eab9f3336tt27by8/PTBx98IEmaPn26rrnmGkVFRcnHx0dNmjTR5MmTK6x7wYIF6tq1q4KCghQcHKx27dpp5syZkqSxY8fK29tbR44cOel9gwcPVmhoqPLz88/8hwhUM7QJADDM0aNHdeONN+ruu+/Wvffe62x3//jjjxUYGKjhw4crMDBQ//3vfzVmzBhlZmbqn//85xmPO3PmTGVlZemhhx6SyWTS66+/rttuu027du064zeQy5cv15w5c/Too48qKChI77zzjm6//Xbt27dP4eHhkqT169frhhtuUM2aNfXSSy/JZrNp3LhxioyMvPA/lFIff/yxBg4cqHbt2ikpKUmpqal6++239csvv2j9+vUKDQ2VJN1+++3666+/9NhjjykhIUGHDx/WokWLtG/fPufz66+/XpGRkRoxYoRCQ0O1Z88ezZkzp9JqBQAAJarztc3Bgwf1448/6pNPPpEk9enTR2+99ZYmTZokq9Xq3O+PP/7QlVdeKW9vbw0ePFgJCQnauXOnvvnmG73yyivOY7Vv317p6ekaPHiwGjVqpOTkZH355ZfKzc0td7yztXXrVvXp00cPPfSQBg0apIYNG0qSJk+erKZNm+qWW26Rl5eXvvnmGz366KOy2+0aMmSI8/0ff/yx7r//fjVt2lQjR45UaGio1q9fr4ULF6pv37667777NG7cOM2ePVtDhw51vq+wsFBffvmlbr/9dkOXVgIuywEAF9mQIUMc//ufm65duzokOaZMmXLS/rm5uSdte+ihhxz+/v6O/Px857b+/fs7ateu7Xy+e/duhyRHeHi449ixY87t//nPfxySHN98841z29ixY0+qSZLDarU6duzY4dy2YcMGhyTHu+++69zWo0cPh7+/vyM5Odm5bfv27Q4vL6+TjlmR/v37OwICAk75emFhoSMqKsrRrFkzR15ennP7t99+65DkGDNmjMPhcDiOHz/ukOT45z//ecpjzZ071yHJsXr16jPWBQAAzg7XNid74403HH5+fo7MzEyHw+FwbNu2zSHJMXfu3HL7denSxREUFOTYu3dvue12u935+379+jnMZnOF1y9l+1V0vg6HwzF9+nSHJMfu3bud22rXru2Q5Fi4cOFJ+1f0zyYxMdFRt25d5/P09HRHUFCQo0OHDuWuzf637o4dOzo6dOhQ7vU5c+Y4JDl+/PHHkz4HgMPB8j0AhvHx8dHAgQNP2v739f1ZWVlKS0vTlVdeqdzcXG3ZsuWMx73rrrsUFhbmfH7llVdKknbt2nXG93br1k316tVzPr/ssssUHBzsfK/NZtPixYvVs2dPxcbGOverX7++brzxxjMe/2ysWbNGhw8f1qOPPlruG7WbbrpJjRo10nfffSep5M/JarVq6dKlOn78eIXHKuuo+vbbb1VUVFQp9QEAgIpV52ubGTNm6KabblJQUJAkqUGDBmrTpk25JXxHjhzRsmXLdP/996tWrVrl3l+2FM9ut+vrr79Wjx491LZt25M+53xHJdSpU0eJiYknbf/7P5uMjAylpaWpa9eu2rVrlzIyMiRJixYtUlZWlkaMGHFSt9Pf6+nXr59WrlypnTt3OrfNmDFD8fHxLjlbC3AFhFIADBMXF1dh+/Vff/2lXr16KSQkRMHBwYqMjHQOEi27ODid/73IKbuIO1Vwc7r3lr2/7L2HDx9WXl6e6tevf9J+FW07H3v37pUkZ1v53zVq1Mj5uo+Pj1577TUtWLBA0dHR6tKli15//XWlpKQ49+/atatuv/12vfTSS4qIiNCtt96q6dOnq6CgoFJqBQAAJ1TXa5vNmzdr/fr16ty5s3bs2OF8XHXVVfr222+VmZkp6USI1qxZs1Me68iRI8rMzDztPuejTp06FW7/5Zdf1K1bNwUEBCg0NFSRkZHOWWBl/2zKQqYz1XTXXXfJx8fHGcRlZGTo22+/1T333MNdCIFTIJQCYJiK7niSnp6url27asOGDRo3bpy++eYbLVq0SK+99pqkkm/PzsRisVS43eFwXNT3GuHJJ5/Utm3blJSUJF9fX40ePVqNGzfW+vXrJZV8e/fll19qxYoVGjp0qJKTk3X//ferTZs2ys7ONrh6AAA8S3W9tvnss88kScOGDVODBg2cjzfffFP5+fn66quvKu2zypwq5Pnf4fFlKvpns3PnTl177bVKS0vThAkT9N1332nRokUaNmyYpLP7Z/N3YWFhuvnmm52h1JdffqmCgoIz3qURqM4YdA7ApSxdulRHjx7VnDlz1KVLF+f23bt3G1jVCVFRUfL19dWOHTtOeq2ibeejdu3akkoGcl5zzTXlXtu6davz9TL16tXTU089paeeekrbt29Xy5Yt9eabbzovECXp8ssv1+WXX65XXnlFM2fO1D333KNZs2bpwQcfrJSaAQBAxTz92sbhcGjmzJm6+uqr9eijj570+ssvv6wZM2Zo4MCBqlu3riRp48aNpzxeZGSkgoODT7uPdKJbLD093TmuQDrRcX42vvnmGxUUFGjevHnlOsp+/PHHcvuVLX/cuHHjGbvH+vXrp1tvvVWrV6/WjBkz1KpVKzVt2vSsawKqGzqlALiUsm/z/v7tXWFhod5//32jSirHYrGoW7du+vrrr3Xw4EHn9h07dmjBggWV8hlt27ZVVFSUpkyZUm6Z3YIFC7R582bddNNNkqTc3NyTbi1cr149BQUFOd93/Pjxk74JbdmypSSxhA8AgCrg6dc2v/zyi/bs2aOBAwfqjjvuOOlx11136ccff9TBgwcVGRmpLl26aNq0adq3b1+545T9+ZjNZvXs2VPffPON1qxZc9Lnle1XFhQtW7bM+VpOTo7z7n9ne+5/P6ZUsuRu+vTp5fa7/vrrFRQUpKSkpJOuvf73OuvGG29URESEXnvtNf300090SQFnQKcUAJfSqVMnhYWFqX///nr88cdlMpn06aefutTyuRdffFE//PCDOnfurEceeUQ2m02TJk1Ss2bN9Pvvv5/VMYqKivSPf/zjpO01atTQo48+qtdee00DBw5U165d1adPH6Wmpurtt99WQkKCs6V827Ztuvbaa9W7d281adJEXl5emjt3rlJTU3X33XdLkj755BO9//776tWrl+rVq6esrCxNnTpVwcHB6t69e6X9mQAAgIp5+rXNjBkzZLFYnF+a/a9bbrlFL7zwgmbNmqXhw4frnXfe0RVXXKHWrVtr8ODBqlOnjvbs2aPvvvvO+Vnjx4/XDz/8oK5du2rw4MFq3LixDh06pC+++ELLly9XaGiorr/+etWqVUsPPPCAnnnmGVksFk2bNk2RkZEnBV6ncv3118tqtapHjx566KGHlJ2dralTpyoqKkqHDh1y7hccHKy33npLDz74oNq1a6e+ffsqLCxMGzZsUG5ubrkgzNvbW3fffbcmTZoki8WiPn36nFUtQHVFKAXApYSHh+vbb7/VU089pVGjRiksLEz33nuvrr322grvmGKENm3aaMGCBXr66ac1evRoxcfHa9y4cdq8efNZ3UFHKvmGdPTo0Sdtr1evnh599FENGDBA/v7+evXVV/Xcc88pICBAvXr10muvveZsUY+Pj1efPn20ZMkSffrpp/Ly8lKjRo30+eef6/bbb5dUMuh81apVmjVrllJTUxUSEqL27dtrxowZpxz4CQAAKo8nX9sUFRXpiy++UKdOnVSjRo0K92nWrJnq1Kmjzz77TMOHD1eLFi3022+/afTo0Zo8ebLy8/NVu3Zt9e7d2/meuLg4rVy5UqNHj9aMGTOUmZmpuLg43XjjjfL395dUEv7MnTtXjz76qEaPHq2YmBg9+eSTCgsLq/AOiBVp2LChvvzyS40aNUpPP/20YmJi9MgjjygyMlL3339/uX0feOABRUVF6dVXX9XLL78sb29vNWrUyPll4d/169dPkyZN0rXXXquaNWueVS1AdWVyuFJEDwBurGfPnvrrr7+0fft2o0sBAAC4YFzbnJ8NGzaoZcuW+te//qX77rvP6HIAl8ZMKQA4D3l5eeWeb9++XfPnz9dVV11lTEEAAAAXgGubyjN16lQFBgbqtttuM7oUwOWxfA8AzkPdunU1YMAA1a1bV3v37tXkyZNltVr17LPPGl0aAADAOePa5sJ988032rRpkz788EMNHTpUAQEBRpcEuDyW7wHAeRg4cKB+/PFHpaSkyMfHRx07dtT48ePVunVro0sDAAA4Z1zbXLiEhASlpqYqMTFRn376qYKCgowuCXB5hFIAAAAAAACocsyUAgAAAAAAQJUjlAIAAAAAAECVY9B5Bex2uw4ePKigoCCZTCajywEAAC7I4XAoKytLsbGxMpurz/d8XCcBAIAzOdvrJEKpChw8eFDx8fFGlwEAANzA/v37dckllxhdRpXhOgkAAJytM10nEUpVoOwuCfv371dwcLDB1QAAAFeUmZmp+Pj4and3Ja6TAADAmZztdRKhVAXKWtGDg4O52AIAAKdV3ZawcZ0EAADO1pmuk6rPAAQAAAAAAAC4DEIpAAAAAAAAVDlCKQAAAAAAAFQ5ZkoBAKotu92uwsJCo8uAi/L29pbFYjG6DLdls9lUVFRkdBnAKfEzDgDGI5QCAFRLhYWF2r17t+x2u9GlwIWFhoYqJiam2g0zvxAOh0MpKSlKT083uhTgjPgZBwBjEUoBAKodh8OhQ4cOyWKxKD4+XmYzq9lRnsPhUG5urg4fPixJqlmzpsEVuY+yQCoqKkr+/v78zz5cEj/jAOAaCKUAANVOcXGxcnNzFRsbK39/f6PLgYvy8/OTJB0+fFhRUVFus8wnOTlZzz33nBYsWKDc3FzVr19f06dPV9u2bU/a9+GHH9YHH3ygt956S08++eQFf7bNZnMGUuHh4Rd8POBictefcQDwJIRSAIBqx2azSZKsVqvBlcDVlYWWRUVFbvE/rMePH1fnzp119dVXa8GCBYqMjNT27dsVFhZ20r5z587Vb7/9ptjY2Er7/LIZUoS9cBfu9jMOAJ6GUAoAUG2xrAhn4m7/jrz22muKj4/X9OnTndvq1Klz0n7Jycl67LHH9P333+umm26q9Drc7c8N1Rf/rgKAsRiiAQAA4CHmzZuntm3b6s4771RUVJRatWqlqVOnltvHbrfrvvvu0zPPPKOmTZue8ZgFBQXKzMws9wAAAKgMhFIAAFRjCQkJmjhx4lnvv3TpUplMJu6s5qJ27dqlyZMnq0GDBvr+++/1yCOP6PHHH9cnn3zi3Oe1116Tl5eXHn/88bM6ZlJSkkJCQpyP+Pj4i1W+R+FnCwCAMyOUAgDADZhMptM+XnzxxfM67urVqzV48OCz3r9Tp046dOiQQkJCzuvzzhb/g35+7Ha7WrdurfHjx6tVq1YaPHiwBg0apClTpkiS1q5dq7ffflsff/zxWS9bGjlypDIyMpyP/fv3X8xTqHLV7Wfr7xo1aiQfHx+lpKRU2WcCAPB3zJQCAMANHDp0yPn72bNna8yYMdq6datzW2BgoPP3DodDNptNXl5n/ms+MjLynOqwWq2KiYk5p/eg6tSsWVNNmjQpt61x48b66quvJEk///yzDh8+rFq1ajlft9lseuqppzRx4kTt2bPnpGP6+PjIx8fnotZtpOr6s7V8+XLl5eXpjjvu0CeffKLnnnuuyj67IkVFRfL29ja0BgBA1aNTCgAANxATE+N8hISEyGQyOZ9v2bJFQUFBWrBggdq0aSMfHx8tX75cO3fu1K233qro6GgFBgaqXbt2Wrx4cbnj/u8SI5PJpP/7v/9Tr1695O/vrwYNGmjevHnO1/+3g+njjz9WaGiovv/+ezVu3FiBgYG64YYbyv2PfnFxsR5//HGFhoYqPDxczz33nPr376+ePXue95/H8ePH1a9fP4WFhcnf31833nijtm/f7nx979696tGjh8LCwhQQEKCmTZtq/vz5zvfec889ioyMlJ+fnxo0aFBuMLg769y5c7lARZK2bdum2rVrS5Luu+8+/fHHH/r999+dj9jYWD3zzDP6/vvvjSjZcNX1Z+ujjz5S3759dd9992natGknvX7gwAH16dNHNWrUUEBAgNq2bauVK1c6X//mm2/Url07+fr6KiIiQr169Sp3rl9//XW544WGhurjjz+WJO3Zs0cmk0mzZ89W165d5evrqxkzZujo0aPq06eP4uLi5O/vr+bNm+vf//53uePY7Xa9/vrrql+/vnx8fFSrVi298sorkqRrrrlGQ4cOLbf/kSNHZLVatWTJkjP+mQAAqh6hVBXLL7Lph79SNG/DQaNLAQCUcjgcyi0sNuThcDgq7TxGjBihV199VZs3b9Zll12m7Oxsde/eXUuWLNH69et1ww03qEePHtq3b99pj/PSSy+pd+/e+uOPP9S9e3fdc889Onbs2Cn3z83N1RtvvKFPP/1Uy5Yt0759+/T00087X3/ttdc0Y8YMTZ8+Xb/88osyMzNP+h/WczVgwACtWbNG8+bN04oVK+RwONS9e3cVFRVJkoYMGaKCggItW7ZMf/75p1577TVnx8vo0aO1adMmLViwQJs3b9bkyZMVERFxQfW4imHDhum3337T+PHjtWPHDs2cOVMffvihhgwZIkkKDw9Xs2bNyj28vb0VExOjhg0bXpSajPr54mfr1LKysvTFF1/o3nvv1XXXXaeMjAz9/PPPztezs7PVtWtXJScna968edqwYYOeffZZ2e12SdJ3332nXr16qXv37lq/fr2WLFmi9u3bn/Fz/9eIESP0xBNPaPPmzUpMTFR+fr7atGmj7777Ths3btTgwYN13333adWqVc73jBw5Uq+++qrz53jmzJmKjo6WJD344IOaOXOmCgoKnPt/9tlniouL0zXXXHPO9QGAO3M4HMovsulYTqGS0/O043C2NiZnaM2eY/p5+xEt2pSqbzYcVHpuoaF1snyvimUXFGvwp2slSTc3rymzmdvQAoDR8opsajLGmC6RTeMS5W+tnL+Ox40bp+uuu875vEaNGmrRooXz+csvv6y5c+dq3rx5J3UT/N2AAQPUp08fSdL48eP1zjvvaNWqVbrhhhsq3L+oqEhTpkxRvXr1JElDhw7VuHHjnK+/++67GjlypLOTYtKkSc6upfOxfft2zZs3T7/88os6deokSZoxY4bi4+P19ddf684779S+fft0++23q3nz5pKkunXrOt+/b98+tWrVSm3btpVU0tHiKdq1a6e5c+dq5MiRGjdunOrUqaOJEyfqnnvuMawmo36++Nk6tVmzZqlBgwbOuy/efffd+uijj3TllVdKkmbOnKkjR45o9erVqlGjhiSpfv36zve/8soruvvuu/XSSy85t/39z+NsPfnkk7rtttvKbft76PbYY4/p+++/1+eff6727dsrKytLb7/9tiZNmqT+/ftLkurVq6crrrhCknTbbbdp6NCh+s9//qPevXtLKuk4GzBgwFnPUAOAquZwOFRQbFdOQbFyC23KKSxWToFNueV+LVZOoc25T16RTfmlv+YWntj37+/NK7LpbL6fmftoJ7WqZb34J3oKhFJVLNDnxB95XpFNAT78IwAAVI6ykKVMdna2XnzxRX333Xc6dOiQiouLlZeXd8Zujssuu8z5+4CAAAUHB+vw4cOn3N/f39/5P81SyVyjsv0zMjKUmpparovCYrGoTZs2zq6Lc7V582Z5eXmpQ4cOzm3h4eFq2LChNm/eLEl6/PHH9cgjj+iHH35Qt27ddPvttzvP65FHHtHtt9+udevW6frrr1fPnj2d4ZYnuPnmm3XzzTef9f4VzZFCeZ72szVt2jTde++9zuf33nuvunbtqnfffVdBQUH6/fff1apVK2cg9b9+//13DRo06LSfcTb+98/VZrNp/Pjx+vzzz5WcnKzCwkIVFBTI399fUsnPfkFBga699toKj+fr6+tcjti7d2+tW7dOGzduLLdMEgAuhMPhUF6RTdkFxcotDYHyCsuCIZvyi8qCoWJlF9iUW1CsnMJi5+t5pQFSbmHJMbILSvYtslVed29FrF5m+XqZ5We1yM/bIl/nwyxfb8tF/ewzIRGpYj5eZplNkt0h5RQUE0oBgAvw87Zo07hEwz67sgQEBJR7/vTTT2vRokV64403VL9+ffn5+emOO+5QYeHp27T/d9iwyWQ67f/kVrR/ZS6dOh8PPvigEhMT9d133+mHH35QUlKS3nzzTT322GO68cYbtXfvXs2fP1+LFi3StddeqyFDhuiNN94wtGZPZdTPFz9bFdu0aZN+++03rVq1qtxwc5vNplmzZmnQoEHy8/M77THO9HpFdZYtrf27//1z/ec//6m3335bEydOVPPmzRUQEKAnn3zS+ed6ps+VSn72W7ZsqQMHDmj69Om65pprnDPVAFRfhcV2ZeYXKSu/WFn5RSWBUH6xMxjKKvt9/onnOaWBUllwVNaJdDEvcfy8LQrwscjf6iV/q0X+VosCfLwUYPUq+bX0NT9vi/ysZmfAVPKalwKsJa8797Na5OtllpfFdSc3kYhUMZPJpAAfL+e/9FFGFwQAkMlkqrRlPq7kl19+0YABA5xLe7Kzs6u8KyYkJETR0dFavXq1unTpIqnkf37XrVunli1bntcxGzdurOLiYq1cudLZ4XT06FFt3bq13J3n4uPj9fDDD+vhhx/WyJEjNXXqVD322GOSSu6M1r9/f/Xv319XXnmlnnnmGUKpi8QTf77c+Wfro48+UpcuXfTee++V2z59+nR99NFHGjRokC677DL93//9n44dO1Zht9Rll12mJUuWaODAgRV+RmRkZLmB7Nu3b1dubu4Zz+mXX37Rrbfe6uzistvt2rZtm/PnukGDBvLz89OSJUv04IMPVniM5s2bq23btpo6dapmzpypSZMmnfFzAbi2ktlIJaFSRl7JIzOvNGAqKAmZysKmzLxiZeaXvJ6ZX1z6a5Hyi86vO/tUTCYp4G/BkV/p7/28S54H+njJ3+dEoORvPRE0+VlP7BPkWxImBfp4yd/qJUs1HO/jWVcIbiLAWhJK5RbajC4FAODBGjRooDlz5qhHjx4ymUwaPXr0eS+ZuxCPPfaYkpKSVL9+fTVq1Ejvvvuujh8/flYzXv78808FBQU5n5tMJrVo0UK33nqrBg0apA8++EBBQUEaMWKE4uLidOutt0oqmVVz44036tJLL9Xx48f1448/qnHjxpKkMWPGqE2bNmratKkKCgr07bffOl8Dzoa7/mwVFRXp008/1bhx49SsWbNyrz344IOaMGGC/vrrL/Xp00fjx49Xz549lZSUpJo1a2r9+vWKjY1Vx44dNXbsWF177bWqV6+e7r77bhUXF2v+/PnOzqtrrrlGkyZNUseOHWWz2fTcc8+d1PVVkQYNGujLL7/Ur7/+qrCwME2YMEGpqanOUMrX11fPPfecnn32WVmtVnXu3FlHjhzRX3/9pQceeKDcuQwdOlQBAQHl7goIwBjFNruyC4qVmVes9LxCpeeWBEvpeUXO0CirNEAqa97IKSjfoVRoq5z/xgaVBkGBfwuDysKhQB9vBfp6KcjnRFdSoM/fQ6MTz/28LcyHriSEUgYI8ClpJ88uKDa4EgCAJ5swYYLuv/9+derUSREREXruueeUmZlZ5XU899xzSklJUb9+/WSxWDR48GAlJibKYjnz8qqyDpAyFotFxcXFmj59up544gndfPPNKiwsVJcuXTR//nzn//jabDYNGTJEBw4cUHBwsG644Qa99dZbkiSr1aqRI0dqz5498vPz05VXXqlZs2ZV/onDY7nrz9a8efN09OjRCoOaxo0bq3Hjxvroo480YcIE/fDDD3rqqafUvXt3FRcXq0mTJs7uqquuukpffPGFXn75Zb366qsKDg4u97P65ptvauDAgbryyisVGxurt99+W2vXrj3j+YwaNUq7du1SYmKi/P39NXjwYPXs2VMZGRnOfUaPHi0vLy+NGTNGBw8eVM2aNfXwww+XO06fPn305JNPqk+fPvL19T2rP0sAp1dQbFNGbkmQlJFXpIzconKdS3/vYEovfS0zr6R7KaeSmjEsZpOCfb0U4uetYD9vBfl6KcjH2xkyBfl6l3s92LfktbLngT7VsxPJ1ZkcRg99cEGZmZkKCQlRRkaGgoODK/34t05arg0HMjRtQFtd0yi60o8PADi9/Px87d69W3Xq1OF/WAxgt9vVuHFj9e7dWy+//LLR5ZzW6f5dudjXC67qdOfNz5ax3Oln62Las2eP6tWrp9WrV6t169an3Zd/Z1HdOBwOZRcU61hOoY7lFOp4bqGO5RQpPbfseZEy8gp1PKdIx3MLSzqacouUV3ThwZKft0Wh/t4K8St5hPqXBEdlAVKwX0mwVNa9FOBzYlZSiJ+3AqwW7qTpRs72OolOKQOUzVXILmD5HgDA8+3du1c//PCDunbtqoKCAk2aNEm7d+9W3759jS4NcGv8bJVXVFSko0ePatSoUbr88svPGEgBnsDhcCgzr1hpOQU6llOoo9llQVNpyJRTqKOlvz+aXaC0nEIVFp/fUjizSc5AydmN9Lfn5V7zLfk1yNfL2dXk7cLDtmEcQikDlN1xL4flewCAasBsNuvjjz/W008/LYfDoWbNmmnx4sXMcQIuED9b5f3yyy+6+uqrdemll+rLL780uhzgvNjsjpIAKadAx7ILlVYaJh0rDZecHU05RTqWWxI6FdvPffGTn7dFNQKsqhFgVViAVWH+3grztyr0b7+G+pdsD/WzKsTfW0E+XsxRQqUjlDJA2UwpQikAQHUQHx+vX375xegyAI/Dz1Z5V111lZhMAldTWGzXsZxCpWUXlHYsFZRbLpeeW+QMoY5mF+pYbqHO51/jQB8vhQeWhEzhAVaF+VtVI9CqGv4loVNEoFXhAT4KL/3Vz3rmuY5AVSCUMsCJTimW7wEAAACAOykotiktu1BHsgqUllWgI9kFJb/PLn1kFTp/n5l/7o0IJpMU5l8SLtUIsCoi0MfZ0RT+P51N4YElAZSvNyET3BOhlAECSlPp3EI6pQAAAADAFTgcDqXnFulQRr4OZeQ5f03JKNDhrHwdziz59Xhu0Tkd12I2OTuYykKkGgFWhfpbVcPfu7ST6UQXU40AK3eJQ7VBKGWAsk6pbJbvAYChWOaBM7Hbz28YbHXHnxvcBf+uVg9lw8BTS4Ol1Mx8Hc46ETSlZuYrNStfqZkFZz0E3NtiUmSgjyKDfBTxt18jAq0KDzzx+4hAH4X4eTOLCTgFQikDBDLoHAAM5e3tLZPJpCNHjigyMpLbC+MkDodDhYWFOnLkiMxms6xWq9EluQWr1Sqz2ayDBw8qMjJSVquVny+4JH7GPYfN7tCRrAIdzMjTofQTHU4pmfk6nFkSNKVm5qvgHO44Fx5gVUyIr2qG+KlmiK9iQnwVHeyrqCAf56+h/t789w2oBIRSBvC3loZShcyUAgAjWCwWXXLJJTpw4ID27NljdDlwYf7+/qpVq5bMZm5jfTbMZrPq1KmjQ4cO6eDBg0aXA5wRP+OuL7ugWIfS83QwI18pGXk6mJ6vA8fzlJyeq+T0kiDqbO8+F+LnXS5Yigz2UXRQSeAUHVyyPTLIh/lMQBUilDIAd98DAOMFBgaqQYMGKio6t7kQqD4sFou8vLz4JvwcWa1W1apVS8XFxbLZ+AIOroufcePlFBQrJTNfqRn5zvlNBzPydbA0bDqYkaessxgUbjGbFBPsW9rd5KvYUD9FB/sqhrAJcHmEUgYIoFMKAFyCxWKRxcIFKlDZTCaTvL295e3tbXQpAAzicDh0PLdIe47maP+xkq6mg+l5Sj5e0u10toGTJAX7epUspQstCZ3iQv0UF+anS8L8FVcaQDEYHHBPLhFKvffee/rnP/+plJQUtWjRQu+++67at29f4b5FRUVKSkrSJ598ouTkZDVs2FCvvfaabrjhhgr3f/XVVzVy5Eg98cQTmjhx4kU8i7MXwEwpAAAAAG6uLHjanZajPWk52p2Wo91Hc7T3aI72Hs09q9ApwGpRzVA/xQSXBE41Q/0UW9rtFBvqq5gQP+dMXgCex/Cf7tmzZ2v48OGaMmWKOnTooIkTJyoxMVFbt25VVFTUSfuPGjVKn332maZOnapGjRrp+++/V69evfTrr7+qVatW5fZdvXq1PvjgA1122WVVdTpnhUHnAAAAANxBkc2ug+l52ncs1/k4cOzE84y80y+Drxniq1o1/BUX5qe4UL/SsKkkeIoJ8VWQLx2VQHVmeCg1YcIEDRo0SAMHDpQkTZkyRd99952mTZumESNGnLT/p59+qhdeeEHdu3eXJD3yyCNavHix3nzzTX322WfO/bKzs3XPPfdo6tSp+sc//lE1J3OW/JkpBQAAAMBFOBwOHczI187D2dpxOFs7j2Rrz9Ec7TuWq4Pp+bKdYZB4bIivEiIClBARoDrhJb8mhPsrvoY/c5wAnJahoVRhYaHWrl2rkSNHOreZzWZ169ZNK1asqPA9BQUF8vX1LbfNz89Py5cvL7dtyJAhuummm9StWzeXC6WcnVKFNjkcDoYrAgAAALjo7HaHDmbkaXtqtralZmlbara2H87SjsPZyj3NvFsfL7Pia/irdo2SoCm+hr9q1fBXfA0/1a4RID8rwROA82NoKJWWliabzabo6Ohy26Ojo7Vly5YK35OYmKgJEyaoS5cuqlevnpYsWaI5c+aUu7vLrFmztG7dOq1evfqs6igoKFBBQYHzeWZm5nmczdnzL/2Pts3uUEGxnW8PAAAAAFSawmK79h3L0Y7DOdp5pKT7qawD6lThk5fZpISIANWLDFD9qEAlhAeodniAaof7KzLQR2YGiQO4CAxfvneu3n77bQ0aNEiNGjWSyWRSvXr1NHDgQE2bNk2StH//fj3xxBNatGjRSR1Vp5KUlKSXXnrpYpZdTtnd96SSJXyEUgAAAADOVX6RTTsOl3Q9bU3N0s7D2dp1JEd7j+Wecsmdl9mkupEBahAdpIbRQbo0OlD1o4JUO9xf3hZzFZ8BgOrO0FAqIiJCFotFqamp5banpqYqJiamwvdERkbq66+/Vn5+vo4eParY2FiNGDFCdevWlSStXbtWhw8fVuvWrZ3vsdlsWrZsmSZNmqSCgoKTbv89cuRIDR8+3Pk8MzNT8fHxlXWaJzGbTfK3WpRbaFNOgU3hgRftowAAAAC4uYJim3an5ZQst0vN0rbULG1PLZn7dKpxT/5Wi+pGBqh+ZKDqR5UET/WjAgmfALgUQ0Mpq9WqNm3aaMmSJerZs6ckyW63a8mSJRo6dOhp3+vr66u4uDgVFRXpq6++Uu/evSVJ1157rf78889y+w4cOFCNGjXSc889d1IgJUk+Pj7y8fGpnJM6S/5Wr5JQqpBh5wAAAABKBo6nZhZoc0qmNh/K1OZDWdpyKFO70nJO2fkU5u+tS6OD1DAmSA2iAlU3MlD1IgMVHezD7FoALs/w5XvDhw9X//791bZtW7Vv314TJ05UTk6O8258/fr1U1xcnJKSkiRJK1euVHJyslq2bKnk5GS9+OKLstvtevbZZyVJQUFBatasWbnPCAgIUHh4+EnbjRToY1FaNnfgAwAAAKoju92h3Udz9NfBTP11MEObDmbqr4OZOpZTWOH+Qb5eujS6JHhqULrsrmFMkCIDCZ8AuC/DQ6m77rpLR44c0ZgxY5SSkqKWLVtq4cKFzuHn+/btk9l8or00Pz9fo0aN0q5duxQYGKju3bvr008/VWhoqEFncH78rSfuwAcAAADAczkcDiWn5+mPAxnacCBdf+zP0MbkDGVV8AW1xWxS3YgANa4ZrEY1g0p+jQlSTLAv4RMAj2N4KCVJQ4cOPeVyvaVLl5Z73rVrV23atOmcjv+/x3AFgT6loRSdUgAAAIDHcDgcOnA8T38mZ+jP5JLwaWNyho7nFp20r6+3WY1igtU0NlhNY0PUNDZYDWOCuBESgGrDJUKp6ijAp+QvmmxCKQAAAMBtZeYXacP+dP2+L12/70/XhgPpSss+eQmel9mkhjFBuuySULWMD9Fll4SqQVSgvBg6DqAaI5QyiH9pp1QuoRQAAADgNpLT87RmzzGt2XNcq/cc09bULDn+ZwZ5WQDVPC5EzeJC1DwuhA4oAKgAoZRBApkpBQAAALg0m92hbalZWrPnmFbvOa41e47pYEb+SfvVquGvlvGhahkfqhbxoWoaG0wABQBngVDKIP6ly/eYKQUAAAC4BofDoR2Hs7V8R5p+2ZGmlbuPKSu//PW6l9mkprHBaptQQ+0SwtSmdg1FBvkYVDEAuDdCKYMw6BwAAAAw3oHjufp151Gt2HlUv+xI0+GsgnKvB1gtal07TO0SaqhtQphaxoc676QNALgw/NfUIAGloVR2Acv3AAAAgKqSmpmv33Yd1a87jmrFrqPadyy33Os+Xma1r1NDnetHqHO9CDWuGcQwcgC4SAilDBJgLVm+l1tIpxQAAABwsRzOzNeKXUf1266j+m3XMe1Oyyn3usVsUotLQtSxXrg6149Q61phzIMCgCpCKGWQE51ShFIAAABAZSkotmntnuP6adsR/bTtiLakZJV73WySmsaG6PK6NdSpfoTaJdRwjtYAAFQt/utrkLJ16LncfQ8AAAC4IEeyCvTjlsNatDlVy7enKa/oxDW2ySQ1jQ1Wx7rhurxuuNom1FCIn7eB1QIAyhBKGYRB5wAAAMD5KbtL3uLNh7VoU4rW70+Xw3Hi9cggH3VpEKkul0boygaRqhFgNa5YAMApEUoZJMCnZJ06y/cAAACAMyuy2bVq9zEt3pyqJZsPnzSgvHlciLo1jta1jaPUNDZYJpPJoEoBAGeLUMogZTOlWL4HAAAAVCwrv0g/bTuiH/5K1Y9bDysr/8QXulaLWZfXC9d1TaLVrXGUaob4GVgpAOB8EEoZhEHnAAAAwMmOZhfo+79S9cOmFP2646gKbXbna+EBVl3dKErdGkfpigaRDCgHADfHf8UNEmAtWb5XWGxXkc0ub4vZ4IoAAAAAY5QFUd/9eVC/7Tomm/3EgKg6EQG6vmm0rm8SrZbxYbKYWZYHAJ6CUMogZXffk6TcAptC/AmlAAAAUH1kFxTr+40p+vr3ZP2682i5IKpZXLBubFZTiU2jVS8ykPlQAOChCKUMYvUyy2oxq9BmV3ZhsUL8uS0tAAAAPFuRza7l29M0d32yftiUovyiE0vzmsUFq3vzmrqpeU3VDg8wsEoAQFUhlDJQgI9Fhbl25TJXCgAAAB5sx+FsfbFmv75al6y07ALn9roRAerZKk63tIhVQgRBFABUN4RSBgrw8dLx3CKGnQMAAMDj5BQU69s/DurzNQe0du9x5/aIQKt6tIhVr1Zxah4XwtI8AKjGCKUMFFA6Vyq30GZwJQAAAEDl2JOWo3+t2Ksv1u5XVn7Jl68Ws0lXN4xU77bxurpRFDf5AQBIIpQyVIBPyR346JQCAACAO7PbHVq2/Yg++XWPftx6xLk9Idxfd7Wrpdtbxykq2NfACgEArohQykABPiV//DmEUgAAAHBD+UU2zVmXrI+W79LOIznO7Vc1jFT/Tgnq2iBSZjPL8wAAFSOUMlDZ8r0clu8BAADAjRzOytenK/bqs9/26nhukSQpyMdLd7aNV7+OtRlaDgA4K4RSBqJTCgAAAO5kx+FsTV22S3PXJ6vQZpckXRLmp4Gd66h320sU5OttcIUAAHdCKGWgsplSuYRSAAAAcGFr9hzTlJ92afHmVOe21rVCNejKurquSbS8GFwOADgPhFIGKuuUyi5g+R4AAABcz+o9x/T6wi1avee4JMlkkq5rHK2HutZVm9o1DK4OAODuCKUMFGAt7ZQqpFMKAAAArmPH4Sy9tnCrFm0q6YyyWsy6rXWcHryyrupHBRpcHQDAUxBKGehEpxShFAAAAIx3ODNfby3ertmr98nukCxmk3q3jdeT3RooOtjX6PIAAB6GUMpADDoHAACAKygstmvaL7v17pLtzjtDX9ckWs/d0FD1o4IMrg4A4KkIpQwUYC0NpQqZKQUAAABj/LTtiF6a95d2peVIklrEh2rUTY3VLoGZUQCAi4tQykBld9+jUwoAAABVbf+xXI37dpNzblREoFXP3dBIt7e+RGazyeDqAADVAaGUgcqW7+XSKQUAAIAqUlBs09Rlu/Tuf3eooNgui9mkAZ0S9ES3Bgr29Ta6PABANUIoZaCy5XsMOgcAAEBV+HVnmkZ9vVG7jpQs1etYN1zjbm2qBtHMjQIAVD1CKQMFMugcAAAAVeBodoFe/naTvv79oKSSpXqjbmqiW1vGymRiqR4AwBiEUgbyL50plVtok93uYO0+AAAAKt2PWw7rmS83KC27UCaTdG+H2no6saFC/FiqBwAwFqGUgco6pSQpt8hW7jkAAABwIfIKbRo/f7M+/W2vJOnS6ED9844WahEfamxhAACUIgUxkI+XWWaTZHdIuQXFhFIAAACoFBuTM/TErPXaWTo7amDnBD13QyP5elsMrgwAgBPMRhcgSe+9954SEhLk6+urDh06aNWqVafct6ioSOPGjVO9evXk6+urFi1aaOHCheX2SUpKUrt27RQUFKSoqCj17NlTW7duvdincc5MJpPzDnwMOwcAAJUhOTlZ9957r8LDw+Xn56fmzZtrzZo1kkquo5577jk1b95cAQEBio2NVb9+/XTw4EGDq0ZlcTgc+r+fd6nX+79o55EcRQb56F/3t9fYHk0JpAAALsfwUGr27NkaPny4xo4dq3Xr1qlFixZKTEzU4cOHK9x/1KhR+uCDD/Tuu+9q06ZNevjhh9WrVy+tX7/euc9PP/2kIUOG6LffftOiRYtUVFSk66+/Xjk5OVV1WmftxLBzm8GVAAAAd3f8+HF17txZ3t7eWrBggTZt2qQ333xTYWFhkqTc3FytW7dOo0eP1rp16zRnzhxt3bpVt9xyi8GVozKk5xZq0L/W6h/fbVaRzaHrm0Tr+ye7qMulkUaXBgBAhUwOh8NhZAEdOnRQu3btNGnSJEmS3W5XfHy8HnvsMY0YMeKk/WNjY/XCCy9oyJAhzm233367/Pz89Nlnn1X4GUeOHFFUVJR++ukndenS5Yw1ZWZmKiQkRBkZGQoODj7PMzs71765VDuP5GjW4Mt1ed3wi/pZAACg8lTl9cLZGjFihH755Rf9/PPPZ/2e1atXq3379tq7d69q1ap1xv1d8bwhrdt3XI/NXK/k9DxZLWaNurmx7ru8NnfWAwAY4myvFwztlCosLNTatWvVrVs35zaz2axu3bppxYoVFb6noKBAvr6+5bb5+flp+fLlp/ycjIwMSVKNGjVOeczMzMxyj6pyolOK5XsAAODCzJs3T23bttWdd96pqKgotWrVSlOnTj3tezIyMmQymRQaGlrh60ZeJ+HMHA6Hpi7bpd5TVig5PU+1w/0159FO6tcxgUAKAODyDA2l0tLSZLPZFB0dXW57dHS0UlJSKnxPYmKiJkyYoO3bt8tut2vRokWaM2eODh06VOH+drtdTz75pDp37qxmzZpVuE9SUpJCQkKcj/j4+As7sXPgby0NpQpZvgcAAC7Mrl27NHnyZDVo0EDff/+9HnnkET3++OP65JNPKtw/Pz9fzz33nPr06XPKbzGNvE7C6RUW2/X0F3/olfmbVWx36KbLaurbx65Qs7gQo0sDAOCsGD5T6ly9/fbbatCggRo1aiSr1aqhQ4dq4MCBMpsrPpUhQ4Zo48aNmjVr1imPOXLkSGVkZDgf+/fvv1jlnySATikAAFBJ7Ha7WrdurfHjx6tVq1YaPHiwBg0apClTppy0b1FRkXr37i2Hw6HJkyef8phGXifh1DJyi9R/2ip9te6ALGaTXrqlqSb1aaUgX2+jSwMA4Kx5GfnhERERslgsSk1NLbc9NTVVMTExFb4nMjJSX3/9tfLz83X06FHFxsZqxIgRqlu37kn7Dh06VN9++62WLVumSy655JR1+Pj4yMfH58JO5jwF+JTcBYVQCgAAXKiaNWuqSZMm5bY1btxYX331VbltZYHU3r179d///ve0sx6MvE5CxfYfy9WA6au080iOAqwWvXdPa13VMMrosgAAOGeGdkpZrVa1adNGS5YscW6z2+1asmSJOnbseNr3+vr6Ki4uTsXFxfrqq6906623Ol9zOBwaOnSo5s6dq//+97+qU6fORTuHCxXA3fcAAEAl6dy5s7Zu3Vpu27Zt21S7dm3n87JAavv27Vq8eLHCw7nRijtZv++4er73i3YeyVFMsK++eLgTgRQAwG0Z2iklScOHD1f//v3Vtm1btW/fXhMnTlROTo4GDhwoSerXr5/i4uKUlJQkSVq5cqWSk5PVsmVLJScn68UXX5Tdbtezzz7rPOaQIUM0c+ZM/ec//1FQUJBzPlVISIj8/Pyq/iRPwznovJBOKQAAcGGGDRumTp06afz48erdu7dWrVqlDz/8UB9++KGkkkDqjjvu0Lp16/Ttt9/KZrM5r5Nq1Kghq9VqZPk4g/9uSdWjM9Ypv8iuprHB+qh/O8WE+J75jQAAuCjDQ6m77rpLR44c0ZgxY5SSkqKWLVtq4cKFzuHn+/btKzcvKj8/X6NGjdKuXbsUGBio7t2769NPPy13x5iyuQhXXXVVuc+aPn26BgwYcLFP6Zz4W1m+BwAAKke7du00d+5cjRw5UuPGjVOdOnU0ceJE3XPPPZKk5ORkzZs3T5LUsmXLcu/98ccfT7p2guv4au0BPfvVH7LZHbqqYaTe69va2XEPAIC7MjkcDofRRbiazMxMhYSEKCMj47QzFirD//28S//4brN6tozVxLtbXdTPAgAAlacqrxdcSXU9byNNXbZLr8zfLEm6rVWcXrvjMnlb3O5+RQCAauRsrxf4esVg/taSfwTZzJQCAADA3zgcDr26YIs+WLZLkvTgFXX0fPfGMptNBlcGAEDlIJQyWNnd93KZKQUAAIBSdrtDz8/9U7NW75ckjbixkR7qUlcmE4EUAMBzEEoZzDnonJlSAAAAUEmH1Ivf/KVZq/fLbJJevf0y9W4bb3RZAABUOkIpg5Ut38spZPkeAABAdVe2ZO9fK/bKZJLe7N1CvVpdYnRZAABcFExINBidUgAAACjz9pLtzhlSr/RsTiAFAPBohFIG8y+dKZVNKAUAAFCtffDTTk1cvF2SNPrmJurboZbBFQEAcHERShmsrFMqt9Amh8NhcDUAAAAwwsyV+5S0YIsk6ZnEhnrgijoGVwQAwMVHKGWwgNJQymZ3qKDYbnA1AAAAqGo/bz+i0f/ZKEkacnU9Dbm6vsEVAQBQNQilDObvbXH+nrlSAAAA1cuOw1l6dMY62ewO3dY6Tk9f39DokgAAqDKEUgYzm03yt5YEUzkF3IEPAACgujiWU6j7P16jrPxitUsIU9JtzWUymYwuCwCAKkMo5QL8rSVL+Bh2DgAAUD0UFNv08Kdrte9YruJr+GnKvW3k42U58xsBAPAghFIuILD0Dny5hYRSAAAAns7hcOj5ORu1as8xBfl4aVr/dgoP9DG6LAAAqhyhlAsoG3ZOpxQAAIDnm/7LHn217oDMJmnSPa3VIDrI6JIAADAEoZQLCChdvpdbyEwpAAAAT/bngQwlLdgsSRp1UxN1vTTS4IoAADAOoZQLCChdvkenFAAAgOfKLijWY/9epyKbQ9c3idbAzglGlwQAgKEIpVyAf+nyvRxCKQAAAI81+uuN2nM0V7Ehvnr9jsu40x4AoNojlHIBgSzfAwAA8GhfrT2gueuTZTZJb/dppVB/q9ElAQBgOEIpF8CgcwAAAM+160i2Rv9noyRpWLdL1S6hhsEVAQDgGgilXEDZTKlcQikAAACPUlBs09CZ65VbaNPldWvo0avrG10SAAAug1DKBZzolGL5HgAAgCf54Kdd2nQoUzUCrHr77laymJkjBQBAGUIpFxBgLemUYtA5AACA59h/LFfv/bhDkjS2RxNFB/saXBEAAK6FUMoFlHVK5RQSSgEAAHgCh8OhsfP+UkGxXZ3qheuWFrFGlwQAgMshlHIBzlCKTikAAACPsGhTqv675bC8LSaNu7WZTCaW7QEA8L8IpVxAIHffAwAA8Bh5hTa99M0mSdKDV9ZV/ahAgysCAMA1EUq5gFB/b0lSem6RwZUAAADgQk36cbuS0/MUF+qnx67hbnsAAJwKoZQLCPO3SpKO5xbK4XAYXA0AAADO147D2fpw2S5J0pgeTeRv9TK4IgAAXBehlAsoC6WKbA6W8AEAALipkuHmG1Vkc+jqhpG6vkm00SUBAODSCKVcgJ/VIj9viyTpeA5L+AAAANzR0q1H9MuOo7J6mfXSLQw3BwDgTAilXESNgJJuqWO5hQZXAgAAgHNltzv0z++3SpIGdEpQrXB/gysCAMD1EUq5iLCAkmHnxwmlAAAA3M78jYe06VCmAn289EjXekaXAwCAWyCUchHOYec5hFIAAADupNhm14QftkmSBl1ZV2GlHfAAAOD0CKVcRFkodYxQCgAAwK18te6AdqXlqEaAVQ9cWcfocgAAcBuEUi6ibKYUy/cAAADcR36RTW8v3i5JevSqegr08TK4IgAA3AehlIs40SnF3fcAAADcxYyV+3QwI181Q3x17+W1jS4HAAC3QijlImqUDTpn+R4AAIBbyC4o1vs/7pAkPXFtA/l6WwyuCAAA9+ISodR7772nhIQE+fr6qkOHDlq1atUp9y0qKtK4ceNUr149+fr6qkWLFlq4cOEFHdMVhPqzfA8AAMCdTF++W0dzClUnIkB3tLnE6HIAAHA7hodSs2fP1vDhwzV27FitW7dOLVq0UGJiog4fPlzh/qNGjdIHH3ygd999V5s2bdLDDz+sXr16af369ed9TFfATCkAAAD3kVNQrKk/75IkDbvuUnlZDL+sBgDA7Rj+t+eECRM0aNAgDRw4UE2aNNGUKVPk7++vadOmVbj/p59+queff17du3dX3bp19cgjj6h79+568803z/uYroCZUgAAAO5jzroDyswvVkK4v25uXtPocgAAcEuGhlKFhYVau3atunXr5txmNpvVrVs3rVixosL3FBQUyNfXt9w2Pz8/LV++/IKOmZmZWe5R1f7eKeVwOKr88wEAAHB27HaHpv2yR5I0sHMdmc0mYwsCAMBNGRpKpaWlyWazKTo6utz26OhopaSkVPiexMRETZgwQdu3b5fdbteiRYs0Z84cHTp06LyPmZSUpJCQEOcjPj6+Es7u3IT6lww6t9kdyswvrvLPBwAAwNn5ceth7U7LUZCvF7OkAAC4AIYv3ztXb7/9tho0aKBGjRrJarVq6NChGjhwoMzm8z+VkSNHKiMjw/nYv39/JVZ8dny9LfK3ltyxJZ25UgAAAC5r2i+7JUl92tdSgI+XwdUAAOC+DA2lIiIiZLFYlJqaWm57amqqYmJiKnxPZGSkvv76a+Xk5Gjv3r3asmWLAgMDVbdu3fM+po+Pj4KDg8s9jHBirhShFAAAgCvafChTv+w4KovZpP6dEowuBwAAt2ZoKGW1WtWmTRstWbLEuc1ut2vJkiXq2LHjad/r6+uruLg4FRcX66uvvtKtt956wcc0GnfgAwAAcG3Tlpd0Sd3QNEZxoX4GVwMAgHszvN94+PDh6t+/v9q2bav27dtr4sSJysnJ0cCBAyVJ/fr1U1xcnJKSkiRJK1euVHJyslq2bKnk5GS9+OKLstvtevbZZ8/6mK4qLIA78AEAALiqtOwC/ef3g5Kk+6+oY3A1AAC4P8NDqbvuuktHjhzRmDFjlJKSopYtW2rhwoXOQeX79u0rNy8qPz9fo0aN0q5duxQYGKju3bvr008/VWho6Fkf01XVKB12fpzlewAAAC7ns9/2qtBmV4v4ULWuFWp0OQAAuD3DQylJGjp0qIYOHVrha0uXLi33vGvXrtq0adMFHdNVOTulWL4HAADgUgqKbfrst72SpAeuqCOTyWRwRQAAuD+3u/ueJysbdM7d9wAAAFzLvN8PKi27UDVDfHVjs4pvngMAAM4NoZQLOTFTilAKAADAlXy2cp8k6b6OteVt4RIaAIDKwN+oLqRGaafUcQadAwAAuIxdR7K1YX+6LGaT7mwTb3Q5AAB4DEIpFxIWUDLonJlSAAAAruPr9cmSpCsbRCgyyMfgagAA8ByEUi6kRkBZpxShFAAAgCtwOBya+3tJKNWrVZzB1QAA4FkIpVyIc9B5XpHsdofB1QAAAGDt3uPafyxPAVaLrm/CgHMAACoToZQLCfUvWb5nszuUlV9scDUAAACYU7p074ZmNeVntRhcDQAAnoVQyoX4eFkU6OMliblSAAAARisotum7Pw5Jkm5rzdI9AAAqG6GUi3EOO2euFAAAgKF+3HJEGXlFig720eV1w40uBwAAj0Mo5WJq+DPsHAAAwBXMXX9AktSzZZwsZpPB1QAA4HkIpVxMaGkoxfI9AAAA46TnFuq/Ww5LknqxdA8AgIuCUMrF1AgovQMfoRQAAIBhvv3jkIpsDjWuGaxGMcFGlwMAgEcilHIxYWWdUjlFBlcCAABQfX1dete9Xq1iDa4EAADPRSjlYmqUDjpnphQAAIAx9h3N1Zq9x2U2Sbe2ZOkeAAAXC6GUiwkLYKYUAACAkeZtKOmS6lw/QtHBvgZXAwCA5yKUcjHcfQ8AAMBYizeXDDjv3rymwZUAAODZCKVcTNnd947TKQUAAM5DcnKy7r33XoWHh8vPz0/NmzfXmjVrnK87HA6NGTNGNWvWlJ+fn7p166bt27cbWLFrOZJVoA0H0iVJ1zSKMrYYAAA8HKGUiym7+97xXAadAwCAc3P8+HF17txZ3t7eWrBggTZt2qQ333xTYWFhzn1ef/11vfPOO5oyZYpWrlypgIAAJSYmKj8/38DKXcfSrYflcEjN4oJZugcAwEXmZXQBKC+sdNB5em6hbHaHLGaTwRUBAAB38dprryk+Pl7Tp093bqtTp47z9w6HQxMnTtSoUaN06623SpL+9a9/KTo6Wl9//bXuvvvuKq/Z1fy4tWTp3jWNog2uBAAAz0co5WLCSpfv2R1SZl6Rc/A5AADwPHa7XT/99JN+/vln7d27V7m5uYqMjFSrVq3UrVs3xcfHn9Px5s2bp8TERN1555366aefFBcXp0cffVSDBg2SJO3evVspKSnq1q2b8z0hISHq0KGDVqxYUWEoVVBQoIKCAufzzMzM8zxb11dYbNeybWmSpGtZugcAwEXH8j0X420xK8i3JCvkDnwAAHimvLw8/eMf/1B8fLy6d++uBQsWKD09XRaLRTt27NDYsWNVp04dde/eXb/99ttZH3fXrl2aPHmyGjRooO+//16PPPKIHn/8cX3yySeSpJSUFElSdHT5LqDo6Gjna/8rKSlJISEhzse5BmXuZPWeY8ouKFZEoI+ax4UYXQ4AAB6PTikXFOZvVVZ+cckd+CKNrgYAAFS2Sy+9VB07dtTUqVN13XXXydvb+6R99u7dq5kzZ+ruu+/WCy+84Ox2Oh273a62bdtq/PjxkqRWrVpp48aNmjJlivr3739etY4cOVLDhw93Ps/MzPTYYGpJ6V33rm4YKTMjFAAAuOgIpVxQWIBV+47lMuwcAAAP9cMPP6hx48an3ad27doaOXKknn76ae3bt++sjluzZk01adKk3LbGjRvrq6++kiTFxMRIklJTU1WzZk3nPqmpqWrZsmWFx/Tx8ZGPj89Zfb67K5sndW1jlu4BAFAVWL7ngmr4l3xbejyH5XsAAHiiMwVSf+ft7a169eqd1b6dO3fW1q1by23btm2bateuLalk6HlMTIyWLFnifD0zM1MrV65Ux44dz7omT7TrSLZ2p+XI22LSFQ1oVQcAoCrQKeWCyoabM1MKAIDqo7i4WB988IGWLl0qm82mzp07a8iQIfL19T3rYwwbNkydOnXS+PHj1bt3b61atUoffvihPvzwQ0mSyWTSk08+qX/84x9q0KCB6tSpo9GjRys2NlY9e/a8SGfmHv67paRL6vK64Qr04RIZAICqwN+4LqhG6R346JQCAKD6ePzxx7Vt2zbddtttKioq0r/+9S+tWbNG//73v8/6GO3atdPcuXM1cuRIjRs3TnXq1NHEiRN1zz33OPd59tlnlZOTo8GDBys9PV1XXHGFFi5ceE7hlyc6MU+KpXsAAFQVQikX5OyUIpQCAMBjzZ07V7169XI+/+GHH7R161ZZLBZJUmJioi6//PJzPu7NN9+sm2+++ZSvm0wmjRs3TuPGjTv3oj1UZn6RVu85Jol5UgAAVCVmSrmgsLJOKQadAwDgsaZNm6aePXvq4MGDkqTWrVvr4Ycf1sKFC/XNN9/o2WefVbt27Qyusnr4eVuaiu0O1YsMUO3wAKPLAQCg2iCUckE1AkoHnTNTCgAAj/XNN9+oT58+uuqqq/Tuu+/qww8/VHBwsF544QWNHj1a8fHxmjlzptFlVgtLtqRKkq5tHG1wJQAAVC8s33NBYcyUAgCgWrjrrruUmJioZ599VomJiZoyZYrefPNNo8uqVmx2h5ZuPSKJeVIAAFQ1OqVcUA3uvgcAQLURGhqqDz/8UP/85z/Vr18/PfPMM8rPzze6rGpjw4F0HcspVJCvl9omhBldDgAA1QqhlAsqG3SekVekYpvd4GoAAMDFsG/fPvXu3VvNmzfXPffcowYNGmjt2rXy9/dXixYttGDBAqNLrBZW7ioZcN65XoS8LVwaAwBQlfib1wWF+pXMlHI4SoIpAADgefr16yez2ax//vOfioqK0kMPPSSr1aqXXnpJX3/9tZKSktS7d2+jy/R4a/celyS6pAAAMAAzpVyQl8WsYF8vZeYX63hukcIDfYwuCQAAVLI1a9Zow4YNqlevnhITE1WnTh3na40bN9ayZcv04YcfGlih53M4HFq3rySUalObUAoAgKpGKOWiagRYS0Mp5koBAOCJ2rRpozFjxqh///5avHixmjdvftI+gwcPNqCy6mN3Wo6O5RTK6mVW09gQo8sBAKDaMXz53nvvvaeEhAT5+vqqQ4cOWrVq1Wn3nzhxoho2bCg/Pz/Fx8dr2LBh5YaB2mw2jR49WnXq1JGfn5/q1aunl19+WQ6H42KfSqUqmyt1jDvwAQDgkf71r3+poKBAw4YNU3Jysj744AOjS6p2ypbutbgkRFYvwy+LAQCodgztlJo9e7aGDx+uKVOmqEOHDpo4caISExO1detWRUWdfEvemTNnasSIEZo2bZo6deqkbdu2acCAATKZTJowYYIk6bXXXtPkyZP1ySefqGnTplqzZo0GDhyokJAQPf7441V9iuethn9JKHWcUAoAAI9Uu3Ztffnll0aXUa2VhVJtatcwuBIAAKonQ78SmjBhggYNGqSBAweqSZMmmjJlivz9/TVt2rQK9//111/VuXNn9e3bVwkJCbr++uvVp0+fct1Vv/76q2699VbddNNNSkhI0B133KHrr7/+jB1Yria0NJQ6xvI9AAA8Tk5OzkXdH2fnRCjFPCkAAIxgWChVWFiotWvXqlu3bieKMZvVrVs3rVixosL3dOrUSWvXrnUGTLt27dL8+fPVvXv3cvssWbJE27ZtkyRt2LBBy5cv14033ngRz6by1QgouQNfei533wMAwNPUr19fr776qg4dOnTKfRwOhxYtWqQbb7xR77zzThVWVz1k5BZp++FsSVLrWqHGFgMAQDVl2PK9tLQ02Ww2RUdHl9seHR2tLVu2VPievn37Ki0tTVdccYUcDoeKi4v18MMP6/nnn3fuM2LECGVmZqpRo0ayWCyy2Wx65ZVXdM8995yyloKCAhUUFDifZ2ZmXuDZXbiymVJHs+mUAgDA0yxdulTPP/+8XnzxRbVo0UJt27ZVbGysfH19dfz4cW3atEkrVqyQl5eXRo4cqYceesjokj1O2V336kYEcKdjAAAM4lZ331u6dKnGjx+v999/Xx06dNCOHTv0xBNP6OWXX9bo0aMlSZ9//rlmzJihmTNnqmnTpvr999/15JNPKjY2Vv3796/wuElJSXrppZeq8lTOKKx0+V5GHqEUAACepmHDhvrqq6+0b98+ffHFF/r555/166+/Ki8vTxEREWrVqpWmTp2qG2+8URaLxehyPVLZ0r3WLN0DAMAwhoVSERERslgsSk1NLbc9NTVVMTExFb5n9OjRuu+++/Tggw9Kkpo3b66cnBwNHjxYL7zwgsxms5555hmNGDFCd999t3OfvXv3Kikp6ZSh1MiRIzV8+HDn88zMTMXHx1fGaZ63UL+S5XvHWb4HAIDHqlWrlp566ik99dRTRpdS7TBPCgAA4xk2U8pqtapNmzZasmSJc5vdbteSJUvUsWPHCt+Tm5srs7l8yWXfHjocjtPuY7fbT1mLj4+PgoODyz2MVjbo/DiDzgEAACpVkc2u3/enS5LaEkoBAGAYQ5fvDR8+XP3791fbtm3Vvn17TZw4UTk5ORo4cKAkqV+/foqLi1NSUpIkqUePHpowYYJatWrlXL43evRo9ejRwxlO9ejRQ6+88opq1aqlpk2bav369ZowYYLuv/9+w87zfISVDjrPoFMKAACgUm05lKW8IpuCfb1ULzLQ6HIAAKi2DA2l7rrrLh05ckRjxoxRSkqKWrZsqYULFzqHn+/bt69c19OoUaNkMpk0atQoJScnKzIy0hlClXn33Xc1evRoPfroozp8+LBiY2P10EMPacyYMVV+fhci1K+kUyo9r0gOh0Mmk8ngigAAADzD2r3HJJUs3TObucYCAMAoJkfZujc4ZWZmKiQkRBkZGYYt5csvsqnR6IWSpD9evF7Bvt6G1AEAACrmCtcLRvCE8x46c52+/eOQnr7+Ug29poHR5QAA4HHO9nrBsJlSOD1fb4v8vEuWJKbnsIQPAACgsqzjznsAALgEQikXFupfdgc+hp0DAOCpEhISNG7cOO3bt8/oUqqFg+l5OpiRL4vZpJbxoUaXAwBAtUYo5cLK7sCXnkenFAAAnurJJ5/UnDlzVLduXV133XWaNWuWCgoKjC7LY60t7ZJqUjNY/lZDx6sCAFDtEUq5sFC/kk6pdDqlAADwWE8++aR+//13rVq1So0bN9Zjjz2mmjVraujQoVq3bp3R5XmcslCqDUv3AAAwHKGUCwsLKF2+l0MoBQCAp2vdurXeeecdHTx4UGPHjtX//d//qV27dmrZsqWmTZsm7k1TOdbtI5QCAMBV0LPswli+BwBA9VFUVKS5c+dq+vTpWrRokS6//HI98MADOnDggJ5//nktXrxYM2fONLpMt5ZbWKy/DmZKIpQCAMAVnFcotX//fplMJl1yySWSpFWrVmnmzJlq0qSJBg8eXKkFVmcnlu8RSgEA4KnWrVun6dOn69///rfMZrP69eunt956S40aNXLu06tXL7Vr187AKj3D5kNZstkdigryUWyon9HlAABQ7Z3X8r2+ffvqxx9/lCSlpKTouuuu06pVq/TCCy9o3LhxlVpgdRZW1inFTCkAADxWu3bttH37dk2ePFnJycl64403ygVSklSnTh3dfffdBlXoOTYfKumSahIbbHAlAABAOs9OqY0bN6p9+/aSpM8//1zNmjXTL7/8oh9++EEPP/ywxowZU6lFVleh/qUzpeiUAgDAY+3atUu1a9c+7T4BAQGaPn16FVXkubaklIRSjWIIpQAAcAXn1SlVVFQkHx8fSdLixYt1yy23SJIaNWqkQ4cOVV511VwonVIAAHi8w4cPa+XKlSdtX7lypdasWWNARZ5r86EsSVLjmkEGVwIAAKTzDKWaNm2qKVOm6Oeff9aiRYt0ww03SJIOHjyo8PDwSi2wOgsr7ZRi0DkAAJ5ryJAh2r9//0nbk5OTNWTIEAMq8kx2u0NbU8pCKTqlAABwBecVSr322mv64IMPdNVVV6lPnz5q0aKFJGnevHnOZX24cGWdUsdz6JQCAMBTbdq0Sa1btz5pe6tWrbRp0yYDKvJMB47nKbugWFaLWXUjAowuBwAA6DxnSl111VVKS0tTZmamwsJO3E538ODB8vf3r7TiqruymVKZ+cUqttnlZTmvDBEAALgwHx8fpaamqm7duuW2Hzp0SF5e53WphgpsLp0n1SA6kGsqAABcxHn9jZyXl6eCggJnILV3715NnDhRW7duVVRUVKUWWJ2F+nk7f5+ZX2xgJQAA4GK5/vrrNXLkSGVkZDi3paen6/nnn9d1111nYGWepezOeyzdAwDAdZzX12+33nqrbrvtNj388MNKT09Xhw4d5O3trbS0NE2YMEGPPPJIZddZLXlZzAry8VJWQbGO5xaqRoDV6JIAAEAle+ONN9SlSxfVrl1brVq1kiT9/vvvio6O1qeffmpwdZ5jS+mQ80YxDDkHAMBVnFen1Lp163TllVdKkr788ktFR0dr7969+te//qV33nmnUgus7kIDSoed5zLsHAAATxQXF6c//vhDr7/+upo0aaI2bdro7bff1p9//qn4+Hijy/MYZcv3mtApBQCAyzivTqnc3FwFBZV8y/TDDz/otttuk9ls1uWXX669e/dWaoHVXZi/VfuP5Sk9l2HnAAB4qoCAAA0ePNjoMjxWTkGx9h7NlSQ1IpQCAMBlnFcoVb9+fX399dfq1auXvv/+ew0bNkySdPjwYQUH8xd9ZQopnSt1nE4pAAA82qZNm7Rv3z4VFpb/IuqWW24xqCLPsTW1ZOledLAP4xAAAHAh5xVKjRkzRn379tWwYcN0zTXXqGPHjpJKuqbKZiGgcoT5l1w40SkFAIBn2rVrl3r16qU///xTJpNJDodDkmQymSRJNpvNyPI8QtmQ80YxfHkKAIArOa+ZUnfccYf27dunNWvW6Pvvv3duv/baa/XWW29VWnGQwvyZKQUAgCd74oknVKdOHR0+fFj+/v7666+/tGzZMrVt21ZLly41ujyP4BxyXpMh5wAAuJLz6pSSpJiYGMXExOjAgQOSpEsuuUTt27evtMJQIqS0U+o4nVIAAHikFStW6L///a8iIiJkNptlNpt1xRVXKCkpSY8//rjWr19vdIlur6xTiiHnAAC4lvPqlLLb7Ro3bpxCQkJUu3Zt1a5dW6GhoXr55Zdlt9sru8ZqzdkplUenFAAAnshmszlvIBMREaGDBw9KkmrXrq2tW7caWZpHcDgc2pJS2inF8j0AAFzKeXVKvfDCC/roo4/06quvqnPnzpKk5cuX68UXX1R+fr5eeeWVSi2yOmOmFAAAnq1Zs2basGGD6tSpow4dOuj111+X1WrVhx9+qLp16xpdnts7cDxP2QXFslrMqhsZYHQ5AADgb84rlPrkk0/0f//3f+XuBnPZZZcpLi5Ojz76KKFUJQphphQAAB5t1KhRysnJkSSNGzdON998s6688kqFh4dr9uzZBlfn/sqW7tWPCpS35bwWCQAAgIvkvEKpY8eOqVGjRidtb9SokY4dO3bBReGEE51ShFIAAHiixMRE5+/r16+vLVu26NixYwoLC3PegQ/nb3PpkPPGzJMCAMDlnNfXRS1atNCkSZNO2j5p0iRddtllF1wUTiibKcWgcwAAPE9RUZG8vLy0cePGcttr1KhBIFVJtqSUdEo15s57AAC4nPPqlHr99dd10003afHixerYsaOkkjvH7N+/X/Pnz6/UAqu7UL+STqncQpsKim3y8bIYXBEAAKgs3t7eqlWrlmw2m9GleKyy5Xt0SgEA4HrOq1Oqa9eu2rZtm3r16qX09HSlp6frtttu019//aVPP/20smus1oJ8vWQu/aI0gyV8AAB4nBdeeEHPP/88IxAugpyCYu09litJahRDpxQAAK7mvDqlJCk2NvakgeYbNmzQRx99pA8//PCCC0MJs9mkED9vHc8t0vHcIkUF+xpdEgAAqESTJk3Sjh07FBsbq9q1aysgoPwd4tatW2dQZe5va2qWHA4pKshH4YE+RpcDAAD+x3mHUqg6Yf5WHc8tUjpzpQAA8Dg9e/Y0ugSPtaV0yHkjlu4BAOCSCKXcQKhz2DnL9wAA8DRjx441ugSPdWKeFEv3AABwRec1UwpVK9S/ZNh5Rh6dUgAAAGfLeee9GDqlAABwRefUKXXbbbed9vX09PQLqQWnQKcUAACey2w2y2QynfJ17sx3fhwOh7aklC3fo1MKAABXdE6hVEhIyBlf79ev3wUVhJOFlXZKHWemFAAAHmfu3LnlnhcVFWn9+vX65JNP9NJLLxlUlfs7mlOorPximUxSQnjAmd8AAACq3DmFUtOnT79YdeA0Qv1KOqUy6JQCAMDj3HrrrSdtu+OOO9S0aVPNnj1bDzzwgAFVub89aTmSpNgQP/l6WwyuBgAAVMTwmVLvvfeeEhIS5Ovrqw4dOmjVqlWn3X/ixIlq2LCh/Pz8FB8fr2HDhik/P7/cPsnJybr33nsVHh4uPz8/NW/eXGvWrLmYp3FRhQbQKQUAQHVz+eWXa8mSJUaX4bZ2l4ZSdSLokgIAwFUZeve92bNna/jw4ZoyZYo6dOigiRMnKjExUVu3blVUVNRJ+8+cOVMjRozQtGnT1KlTJ23btk0DBgyQyWTShAkTJEnHjx9X586ddfXVV2vBggWKjIzU9u3bFRYWVtWnV2nCmCkFAEC1kpeXp3feeUdxcXFGl+K29hwtCaVqh/sbXAkAADgVQ0OpCRMmaNCgQRo4cKAkacqUKfruu+80bdo0jRgx4qT9f/31V3Xu3Fl9+/aVJCUkJKhPnz5auXKlc5/XXntN8fHx5ZYa1qlT5yKfycUV6ld69z1CKQAAPE5YWFi5QecOh0NZWVny9/fXZ599ZmBl7m3P0VxJdEoBAODKDAulCgsLtXbtWo0cOdK5zWw2q1u3blqxYkWF7+nUqZM+++wzrVq1Su3bt9euXbs0f/583Xfffc595s2bp8TERN1555366aefFBcXp0cffVSDBg266Od0sZy4+x7L9wAA8DRvvfVWuVDKbDYrMjJSHTp0cOtOb6OVzZRiyDkAAK7LsFAqLS1NNptN0dHR5bZHR0dry5YtFb6nb9++SktL0xVXXCGHw6Hi4mI9/PDDev7555377Nq1S5MnT9bw4cP1/PPPa/Xq1Xr88cdltVrVv3//Co9bUFCggoIC5/PMzMxKOMPKUxZKpecVyeFwnPa20QAAwL0MGDDA6BI8jsPhOBFK0SkFAIDLMnzQ+blYunSpxo8fr/fff1/r1q3TnDlz9N133+nll1927mO329W6dWuNHz9erVq10uDBgzVo0CBNmTLllMdNSkpSSEiI8xEfH18Vp3PWwvxLlu8VFtuVV2QzuBoAAFCZpk+fri+++OKk7V988YU++eQTAypyf0eyC5RTaJPZJMXX8DO6HAAAcAqGhVIRERGyWCxKTU0ttz01NVUxMTEVvmf06NG677779OCDD6p58+bq1auXxo8fr6SkJNntdklSzZo11aRJk3Lva9y4sfbt23fKWkaOHKmMjAznY//+/Rd4dpXL32qR1VLyj4ph5wAAeJakpCRFRESctD0qKkrjx483oCL3t7d0nlRsqJ98vCwGVwMAAE7FsFDKarWqTZs25W51bLfbtWTJEnXs2LHC9+Tm5spsLl+yxVJyoeFwOCRJnTt31tatW8vts23bNtWuXfuUtfj4+Cg4OLjcw5WYTCaFlC3hY64UAAAeZd++fRXelKV27dqn/VINp7a7dOkeQ84BAHBthi7fGz58uKZOnapPPvlEmzdv1iOPPKKcnBzn3fj69etXbhB6jx49NHnyZM2aNUu7d+/WokWLNHr0aPXo0cMZTg0bNky//fabxo8frx07dmjmzJn68MMPNWTIEEPOsbKEOUMpOqUAAPAkUVFR+uOPP07avmHDBoWHh5/TsV588UWZTKZyj0aNGjlfT0lJ0X333aeYmBgFBASodevW+uqrry74HFxN2Typ2uH+BlcCAABOx7BB55J011136ciRIxozZoxSUlLUsmVLLVy40Dn8fN++feU6o0aNGiWTyaRRo0YpOTlZkZGR6tGjh1555RXnPu3atdPcuXM1cuRIjRs3TnXq1NHEiRN1zz33VPn5VabQ0rlS3IEPAADP0qdPHz3++OMKCgpSly5dJEk//fSTnnjiCd19993nfLymTZtq8eLFzudeXicu9/r166f09HTNmzdPERERmjlzpnr37q01a9aoVatWF34yLqJs+R533gMAwLUZGkpJ0tChQzV06NAKX1u6dGm5515eXho7dqzGjh172mPefPPNuvnmmyurRJcQ6kenFAAAnujll1/Wnj17dO211zoDJLvdrn79+p3XTCkvL69Tzuf89ddfNXnyZLVv315SyRd+b731ltauXetRoRTL9wAAcA9udfe96qzsDnzMlAIAwLNYrVbNnj1bW7du1YwZMzRnzhzt3LlT06ZNk9VqPefjbd++XbGxsapbt67uueeecnOpOnXqpNmzZ+vYsWOy2+2aNWuW8vPzddVVV53yeAUFBcrMzCz3cGUOh0N7jpaEUgmEUgAAuDTDO6VwdkIDSjqluPseAACeqUGDBmrQoMEFHaNDhw76+OOP1bBhQx06dEgvvfSSrrzySm3cuFFBQUH6/PPPdddddyk8PFxeXl7y9/fX3LlzVb9+/VMeMykpSS+99NIF1VWVjmQVKLfQJrNJig9jphQAAK6MTik3EepX1ilFKAUAgCe5/fbb9dprr520/fXXX9edd955Tse68cYbdeedd+qyyy5TYmKi5s+fr/T0dH3++eeSpNGjRys9PV2LFy/WmjVrNHz4cPXu3Vt//vnnKY85cuRIZWRkOB/79+8/txOsYntK50nFhfnJ6sWlLgAAroxOKTdx4u57LN8DAMCTLFu2TC+++OJJ22+88Ua9+eabF3Ts0NBQXXrppdqxY4d27typSZMmaePGjWratKkkqUWLFvr555/13nvvacqUKRUew8fHRz4+PhdUR1Uqu/MeQ84BAHB9fH3kJkLLQqk8OqUAAPAk2dnZFc6O8vb2vuD5TdnZ2dq5c6dq1qyp3NySDqK/39lYkiwWi+x2+wV9jivZfZRQCgAAd0Eo5SZCSwedH6dTCgAAj9K8eXPNnj37pO2zZs1SkyZNzulYTz/9tH766Sft2bNHv/76q3r16iWLxaI+ffqoUaNGql+/vh566CGtWrVKO3fu1JtvvqlFixapZ8+elXQ2xnN2SjHkHAAAl8fyPTdx4u57dEoBAOBJRo8erdtuu007d+7UNddcI0lasmSJ/v3vf+uLL744p2MdOHBAffr00dGjRxUZGakrrrhCv/32myIjIyVJ8+fP14gRI9SjRw9lZ2erfv36+uSTT9S9e/dKPy+jlM2UqhPBkHMAAFwdoZSbCP3bTCm73SGz2WRwRQAAoDL06NFDX3/9tcaPH68vv/xSfn5+uuyyy7R48WJ17dr1nI41a9as077eoEEDffXVVxdSrktzOBzay/I9AADcBqGUmygLpewOKaugWCF+3gZXBAAAKstNN92km2666aTtGzduVLNmzQyoyD0dzipQbqFNZpN0SRidUgAAuDpmSrkJHy+L/K0WSdyBDwAAT5aVlaUPP/xQ7du3V4sWLYwux63sLp0ndUmYv6xeXOYCAODq+NvajYT6lS3hY64UAACeZtmyZerXr59q1qypN954Q9dcc41+++03o8tyK86leww5BwDALbB8z42E+lt1MCOfO/ABAOAhUlJS9PHHH+ujjz5SZmamevfurYKCAn399dfnfOc9SLvTSoacJ4SzdA8AAHdAp5QbCQso6ZTKyKNTCgAAd9ejRw81bNhQf/zxhyZOnKiDBw/q3XffNbost7YnjSHnAAC4Ezql3Eion1WSdCyHTikAANzdggUL9Pjjj+uRRx5RgwYNjC7HI+wpXb5Xh+V7AAC4BTql3Eh4YEkodTSbUAoAAHe3fPlyZWVlqU2bNurQoYMmTZqktLQ0o8tyWw6HwxlKMVMKAAD3QCjlRiIDfSRJR7IKDK4EAABcqMsvv1xTp07VoUOH9NBDD2nWrFmKjY2V3W7XokWLlJWVZXSJbiU1s0D5RXZZzCZdEuZndDkAAOAsEEq5kcigklAqLZtQCgAATxEQEKD7779fy5cv159//qmnnnpKr776qqKionTLLbcYXZ7b2F06T+qSMD95W7jEBQDAHfA3thspC6WOEEoBAOCRGjZsqNdff10HDhzQv//9b6PLcSvOpXsMOQcAwG0QSrkRZyjF8j0AADyaxWJRz549NW/ePKNLcRsnQil/gysBAABni1DKjfx9+Z7d7jC4GgAAANexJ40h5wAAuBtCKTcSHlASShXZHMrIKzK4GgAAANex71ieJKk2nVIAALgNQik3YvUyK8zfWxJzpQAAAP7uwPFcSVJ8GKEUAADuglDKzTBXCgAAoLyM3CJl5RdLki4hlAIAwG0QSrkZQikAAIDy9pd2SUUEWuVntRhcDQAAOFuEUm4mMpBQCgAA4O8OHC+ZJxVHlxQAAG6FUMrNRJSFUsyUAgAAkHRintQlYX4GVwIAAM4FoZSbYfkeAABAeWWdUoRSAAC4F0IpN0MoBQAAUN6JUIrlewAAuBNCKTdDKAUAAFBe2fK9eDqlAABwK4RSbqYslEpjphQAAIAcDoeS6ZQCAMAtEUq5mbK77x3LLVSRzW5wNQAAAMbKyCtSVkGxJGZKAQDgbgil3EyYv1UWs0kOh3Qsp9DocgAAAAxVNk8qItBHvt4Wg6sBAADnglDKzZjNJkUEWiUxVwoAAKBsnhRdUgAAuB9CKTfEsHMAAIASJ+68RygFAIC7IZRyQ2VzpQilAABAdXeAIecAALgtlwil3nvvPSUkJMjX11cdOnTQqlWrTrv/xIkT1bBhQ/n5+Sk+Pl7Dhg1Tfn5+hfu++uqrMplMevLJJy9C5cZwdkpxBz4AAFDNlS3fi69BpxQAAO7G8FBq9uzZGj58uMaOHat169apRYsWSkxM1OHDhyvcf+bMmRoxYoTGjh2rzZs366OPPtLs2bP1/PPPn7Tv6tWr9cEHH+iyyy672KdRpVi+BwAAUIJOKQAA3JfhodSECRM0aNAgDRw4UE2aNNGUKVPk7++vadOmVbj/r7/+qs6dO6tv375KSEjQ9ddfrz59+pzUXZWdna177rlHU6dOVVhYWFWcSpWJYPkeAACAHA4HM6UAAHBjhoZShYWFWrt2rbp16+bcZjab1a1bN61YsaLC93Tq1Elr1651hlC7du3S/Pnz1b1793L7DRkyRDfddFO5Y3sKOqUAAACk9NwiZRcUS5LiQgmlAABwN15GfnhaWppsNpuio6PLbY+OjtaWLVsqfE/fvn2VlpamK664Qg6HQ8XFxXr44YfLLd+bNWuW1q1bp9WrV59VHQUFBSooOBHwZGZmnsfZVB3noHNmSgEAgGqsrEsqMshHvt4Wg6sBAADnyvDle+dq6dKlGj9+vN5//32tW7dOc+bM0XfffaeXX35ZkrR//3498cQTmjFjhnx9fc/qmElJSQoJCXE+4uPjL+YpXDA6pQAAAE4MOWfpHgAA7snQTqmIiAhZLBalpqaW256amqqYmJgK3zN69Gjdd999evDBByVJzZs3V05OjgYPHqwXXnhBa9eu1eHDh9W6dWvne2w2m5YtW6ZJkyapoKBAFkv5b9JGjhyp4cOHO59nZma6dDBVFkplFxQrr9AmPyvfDAIAgOqHIecAALg3QzulrFar2rRpoyVLlji32e12LVmyRB07dqzwPbm5uTKby5ddFjI5HA5de+21+vPPP/X77787H23bttU999yj33///aRASpJ8fHwUHBxc7uHKAn285Otd8meQxhI+AABQTZV1SsXTKQUAgFsytFNKkoYPH67+/furbdu2at++vSZOnKicnBwNHDhQktSvXz/FxcUpKSlJktSjRw9NmDBBrVq1UocOHbRjxw6NHj1aPXr0kMViUVBQkJo1a1buMwICAhQeHn7SdndlMpkUGeSj/cfydDirQPE1+HYQAABUP3RKAQDg3gwPpe666y4dOXJEY8aMUUpKilq2bKmFCxc6h5/v27evXGfUqFGjZDKZNGrUKCUnJysyMlI9evTQK6+8YtQpGCIysCSUYq4UAACork6EUnRKAQDgjgwPpSRp6NChGjp0aIWvLV26tNxzLy8vjR07VmPHjj3r4//vMTyBc9g5y/cAAEA15HA4GHQOAICbc7u776EEd+ADAADV2fHcIuUU2iRJsaGEUgAAuCNCKTcVGegriVAKAABUT2VdUlFBPvL15k7EAAC4I0IpNxURZJVEKAUAAKon5kkBAOD+CKXcVGQgM6UAAED1VdYpxV2IAQBwX4RSbqpsplQanVIAAKAaolMKAAD3Ryjlpv4+6NzhcBhcDQAAQNU6EUrRKQUAgLsilHJTEaXL9wptdmXmFRtcDQAAQNUqW75HpxQAAO6LUMpN+XpbFOzrJUk6kp1vcDUAAABVx+Fw0CkFAIAHIJRyY2VL+A4zVwoAAFQjx3IKlVtokyTFhvoaXA0AADhfhFJu7O9zpQAAAKqLsi6p6GAf+XhZDK4GAACcL0IpNxYZVPLNYFp2ocGVAAAAVJ2yUCqepXsAALg1Qik3FhlIpxQAAKh+GHIOAIBnIJRyYyzfAwAA1VFZp1QcoRQAAG6NUMqNOUOpbEIpAABQfRxMLw2lQlm+BwCAOyOUcmMRgVZJdEoBAIDqJTmdTikAADwBoZQbY/keAACojpyhVKivwZUAAIALQSjlxspCqWM5BbLZHQZXAwAAcPFl5hcpK79YkhQbSqcUAADujFDKjYUH+MhskuwO6WgO3VIAAMDzlc2TCvP3lr/Vy+BqAADAhSCUcmMWs0k1AljCBwAAqo+yUIouKQAA3B+hlJuLCSkJpcpujQwAAODJko+XzZMilAIAwN0RSrm5S6OCJEnbU7MMrgQAAODiS07Pl0SnFAAAnoBQys1dGlMSSm1JIZQCAACer+zOe5eEEUoBAODuCKXcXMPSUGobnVIAAKAaYKYUAACeg1DKzTWMLgmldh3JUWGx3eBqAAAALi5CKQAAPAehlJurGeKrIF8vFdsd2pWWbXQ5AADAQC+++KJMJlO5R6NGjcrts2LFCl1zzTUKCAhQcHCwunTporw897hhSpHNrtTMkplSDDoHAMD9eRldAC6MyWRSw+ggrdl7XFtTstQoJtjokgAAgIGaNm2qxYsXO597eZ243FuxYoVuuOEGjRw5Uu+++668vLy0YcMGmc3u8T1lSka+7A7J6mVWeIDV6HIAAMAFIpTyAA1jToRSAACgevPy8lJMTEyFrw0bNkyPP/64RowY4dzWsGHDqirtgpUNOY8L9ZPZbDK4GgAAcKHc42sxnFbZsHNCKQAAsH37dsXGxqpu3bq65557tG/fPknS4cOHtXLlSkVFRalTp06Kjo5W165dtXz58tMer6CgQJmZmeUeRjkxT8rXsBoAAEDlIZTyAGXDzrdyBz4AAKq1Dh066OOPP9bChQs1efJk7d69W1deeaWysrK0a9cuSSVzpwYNGqSFCxeqdevWuvbaa7V9+/ZTHjMpKUkhISHOR3x8fFWdzkmcoVQI86QAAPAEhFIeoKxT6sDxPGUXFBtcDQAAMMqNN96oO++8U5dddpkSExM1f/58paen6/PPP5fdXnKX3oceekgDBw5Uq1at9NZbb6lhw4aaNm3aKY85cuRIZWRkOB/79++vqtM5iXP5XhihFAAAnoBQygOE+lsVHewjSdpGtxQAACgVGhqqSy+9VDt27FDNmjUlSU2aNCm3T+PGjZ1L/Cri4+Oj4ODgcg+jJKeX3HkvljvvAQDgEQilPMSl0cyVAgAA5WVnZ2vnzp2qWbOmEhISFBsbq61bt5bbZ9u2bapdu7ZBFZ6b5OO5kqRLCKUAAPAIhFIeoiGhFAAA1d7TTz+tn376SXv27NGvv/6qXr16yWKxqE+fPjKZTHrmmWf0zjvv6Msvv9SOHTs0evRobdmyRQ888IDRpZ+Rw+HQQTqlAADwKF5GF4DKwR34AADAgQMH1KdPHx09elSRkZG64oor9NtvvykyMlKS9OSTTyo/P1/Dhg3TsWPH1KJFCy1atEj16tUzuPIzS88tUl6RTZJUk7vvAQDgEQilPERZKMVMKQAAqq9Zs2adcZ8RI0ZoxIgRVVBN5Sobch4Z5CMfL4vB1QAAgMrA8j0P0SAqSCaTdDSnUEeyCowuBwAAoFKVhVIs3QMAwHO4RCj13nvvKSEhQb6+vurQoYNWrVp12v0nTpyohg0bys/PT/Hx8Ro2bJjy8/OdryclJaldu3YKCgpSVFSUevbsedJQT0/jZ7Wodg1/SXRLAQAAz5N8vCSUYsg5AACew/BQavbs2Ro+fLjGjh2rdevWqUWLFkpMTNThw4cr3H/mzJkaMWKExo4dq82bN+ujjz7S7Nmz9fzzzzv3+emnnzRkyBD99ttvWrRokYqKinT99dcrJyenqk7LEGVL+LYwVwoAAHiYg85OKeZJAQDgKQyfKTVhwgQNGjRIAwcOlCRNmTJF3333naZNm1bhvINff/1VnTt3Vt++fSVJCQkJ6tOnj1auXOncZ+HCheXe8/HHHysqKkpr165Vly5dLuLZGKthdJC+/ytV2wilAACAhzmYURJKxdEpBQCAxzC0U6qwsFBr165Vt27dnNvMZrO6deumFStWVPieTp06ae3atc4lfrt27dL8+fPVvXv3U35ORkaGJKlGjRoVvl5QUKDMzMxyD3fUMCZYkrSF5XsAAMDDlC3fY6YUAACew9BOqbS0NNlsNkVHR5fbHh0drS1btlT4nr59+yotLU1XXHGFHA6HiouL9fDDD5dbvvd3drtdTz75pDp37qxmzZpVuE9SUpJeeumlCzsZF9AwJlCStD01S3a7Q2azyeCKAAAAKkdyesn8UEIpAAA8h+Ezpc7V0qVLNX78eL3//vtat26d5syZo++++04vv/xyhfsPGTJEGzduPO0tkkeOHKmMjAznY//+/Rer/IsqITxAVotZuYU25x1qAAAA3F1+kU1p2SV3F74kjFAKAABPYWinVEREhCwWi1JTU8ttT01NVUxMTIXvGT16tO677z49+OCDkqTmzZsrJydHgwcP1gsvvCCz+UTONnToUH377bdatmyZLrnkklPW4ePjIx8fn0o4I2N5WcyqFxWozYcytSUlS/Gld+MDAABwZ4cySrqk/K0Whfh5G1wNAACoLIZ2SlmtVrVp00ZLlixxbrPb7VqyZIk6duxY4Xtyc3PLBU+SZLFYJEkOh8P569ChQzV37lz997//VZ06dS7SGbieRqV34NvGXCkAAOAhyu68FxfqJ5OJ8QQAAHgKw+++N3z4cPXv319t27ZV+/btNXHiROXk5DjvxtevXz/FxcUpKSlJktSjRw9NmDBBrVq1UocOHbRjxw6NHj1aPXr0cIZTQ4YM0cyZM/Wf//xHQUFBSklJkSSFhITIz8+zW74vjS4JpbZwBz4AAOAhGHIOAIBnMjyUuuuuu3TkyBGNGTNGKSkpatmypRYuXOgcfr5v375ynVGjRo2SyWTSqFGjlJycrMjISPXo0UOvvPKKc5/JkydLkq666qpynzV9+nQNGDDgop+TkZydUoRSAADAQ5TNyiSUAgDAsxgeSkkls5+GDh1a4WtLly4t99zLy0tjx47V2LFjT3m8smV81dGlpaHUziPZyi+yydfbYnBFAAAAF6YslGLIOQAAnsXt7r6H04sN8VV0sI+K7Q79ujPN6HIAAAAu2EFnp5SvwZUAAIDKRCjlYUwmk25oWnLnwgV/phhcDQAAwIU7MeicOwsDAOBJCKU80A3NakqSftiUqiKb3eBqAAAAzp/d7tDB9HxJdEoBAOBpCKU8UPs6NRQeYFVGXpF+23XU6HIAAADOW1pOgQptdplNUnQwoRQAAJ6EUMoDWcwmXV+2hG8jS/gAAID7Sj5esnQvJthX3hYuXQEA8CT8ze6hujcvCaV++CtFNnv1vRshAABwbyeW7nHnPQAAPA2hlIe6vG64Qvy8lZZdqNV7jhldDgAAwHk5nFUSSkWHsHQPAABPQyjlobwtZl3fJFqStODPQwZXAwAAcH6OZhdKkiICrAZXAgAAKhuhlAe7sXQJ38K/UmRnCR8AAHBDR3MKJEnhgT4GVwIAACoboZQH61w/QkE+XkrNLND6/ceNLgcAAOCcpZV2SoUH0ikFAICnIZTyYD5eFl3bOEqStOBP7sIHAADcz9Hs0k6pADqlAADwNIRSHu6GZjUlSQs2psjhYAkfAABwL0dzSmdK0SkFAIDHIZTycFc1jJS/1aLk9Dz9mZxhdDkAAADn5Khz+R6dUgAAeBpCKQ/n623R1Q1LlvDNZwkfAABwI/lFNmUXFEtiphQAAJ6IUKoaKLsL39z1B5wXdgAAAK7uWOnSPavFrCAfL4OrAQAAlY1Qqhro1jha8TX8lJpZoNcXbjG6HAAAgLNStnSvRoBVJpPJ4GoAAEBlI5SqBny9LXr1tsskSf9asVerdh8zuCIAAIAzS8spvfMeS/cAAPBIhFLVROf6Ebq7Xbwk6bmv/lB+kc3gigAAAE6PIecAAHg2QqlqZGT3xooO9tHutBxNXLzd6HIAAABO62h2SadURACdUgAAeCJCqWokxM9b/+jZXJI09edd+vNAhsEVAQAAnNrRnLJOKUIpAAA8EaFUNXNdk2jdfFlN2ewOPfPlBhXZ7EaXBAAAUKG07LKZUizfAwDAExFKVUMv3dJUYf7e2pKSpVcXbJHD4TC6JAAAgJMcK+uUYvkeAAAeiVCqGgoP9NFLtzaTJH20fLdGzvlTxXRMAQAAF3Ni0DmhFAAAnohQqpq6pUWsxvdqLrNJmrV6vx7+bB135AMAAC6lbNB5eADL9wAA8ESEUtVY3w61NPneNrJ6mbV4c6ru+2ilMnKLjC4LAABADodDaQw6BwDAoxFKVXOJTWP06f3tFeTrpdV7juvOD37VpoOZRpcFAACqueyCYhUWl4wXoFMKAADPRCgFdagbri8e7qjoYB9tS81W93d+1oOfrNb6fceNLg0AAFRTZfOkAqwW+VktBlcDAAAuBkIpSJIaxQRr7qOd1aNFrMwmafHmw+r1/q+69/9W6qdtR5SeW2h0iQAAoBo5mlM6TyqQLikAADyVl9EFwHXEhvrp3T6tNKxbA01eulNz1ydr+Y40Ld+RJkmqEWBVnYgA1YkIUJvaYbqjzSXytpBrAgCAypfGnfcAAPB4JAo4Sd3IQP3zzhb68emrdO/ltRQT7CtJOpZTqLV7j+vLtQc0cs6fuumdn7Vi59EKj5FdUKzv/0pRSkZ+VZYOAAA8xLGyIefMkwIAwGPRKYVTiq/hr3/0bK5/9JRyCoq1Oy1Hu9NytD01S5/+tlfbUrPVZ+pvuqVFrF64qbHCA6z6eUea5q5L1g+bUpRfZFeQj5devKWpbmsdJ5PJZPQpAQAAN3E0u3T5XgCdUgAAeCpCKZyVAB8vNYsLUbO4EEnS/VfU0Rs/bNWMlfs0b8NBLdmcKj+rxdlqL0lBvl7Kyi/WU19s0A+bUjS+V3PmQgAAgLPC8j0AADwfy/dwXkL9rfpHz+aaN+QKtYwPVU6hTWnZhQoPsGpApwT9Z0hnrR99nZ5JbChvi0nf/5WqxInL9MNfKXI4HEaXDwAAXNzRsuV7fKEFAIDHolMKF6T5JSGa80gnLdt+RJLUuX5EueHnQ66ur6saRmr47A3ampqlwZ+uVZi/txpEB+nS6EA1iApSw5ggXXZJiPyt/OsIAABKlC3fi6BTCgAAj0UKgAtmNpt0VcOoU77eNDZE8x7rrAk/bNO0X3breG6RVu0+plW7jzn3sZhNalIzWG1qh6lN7TB1qFtDUUG+VVE+AABwQUezGXQOAICnc4nle++9954SEhLk6+urDh06aNWqVafdf+LEiWrYsKH8/PwUHx+vYcOGKT+//F3ezvWYuLh8vCwa2b2x/hibqG8fu0Jv3dVCj1xVT90aRys2xFc2u0N/Jmfo41/36LF/r1fnV/+r8fM3KzO/yOjSAQCAAY7mlA46p1MKAACPZXin1OzZszV8+HBNmTJFHTp00MSJE5WYmKitW7cqKurk7puZM2dqxIgRmjZtmjp16qRt27ZpwIABMplMmjBhwnkdE1XHz2opNzC9zMH0PK3de1xr9x7Xyt3HtPlQpj5ctktfrT2g4ddfqrvb1ZLFXHL3vvwimzYmZ2jToUzVjwpUx7rh3NkPAAAPYrc7dCyHQecAAHg6k8PgqdMdOnRQu3btNGnSJEmS3W5XfHy8HnvsMY0YMeKk/YcOHarNmzdryZIlzm1PPfWUVq5cqeXLl5/XMf9XZmamQkJClJGRoeDg4Mo4TZyjH7ce1j++3aSdR3IkSY1ignR53XCt35+uTQczVGQ78a9to5ggDeycoFtbxsnX22JUyQCAaqa6Xi9UxXkfyylU65cXSZJ2vHKjvCwu0dwPAADO0tleLxj6N3xhYaHWrl2rbt26ObeZzWZ169ZNK1asqPA9nTp10tq1a53L8Xbt2qX58+ere/fu533MgoICZWZmlnvAWFc3jNLCJ7voxR5NFOLnrS0pWfr41z3asD9dRTaHIgKturJBhPy8LdqSkqXnvvpTnV79r/75/RZ998chLd+epj8PZGjv0RwdzylUYbHd6FMCAABnqWzIeai/N4EUAAAezNDle2lpabLZbIqOji63PTo6Wlu2bKnwPX379lVaWpquuOIKORwOFRcX6+GHH9bzzz9/3sdMSkrSSy+9VAlnhMrkbTFrQOc6urVlnKb/ukcZuYVqVStMrWuFKb6Gn0wmkzJyizR7zT598uteJafn6b0fd57meCb5W70UYLUoIshHrWuFqW1CmNol1FB0MEPVAQBwFWnOIecs3QMAwJMZPlPqXC1dulTjx4/X+++/rw4dOmjHjh164okn9PLLL2v06NHndcyRI0dq+PDhzueZmZmKj4+vrJJxgcICrBp+3aUVvhbi763BXerp/s51tGhTquZtOKi07AJl5hUrI69IGXlFyiuySZKKbA7ntoMZ+frjQMlgdUmqVcNfLeNDdWl0oBpEB+nS6CDVquHvnGMFAACqzokh59x5DwAAT2ZoKBURESGLxaLU1NRy21NTUxUTE1Phe0aPHq377rtPDz74oCSpefPmysnJ0eDBg/XCCy+c1zF9fHzk48NFjzvzsph1Y/OaurF5zZNeK7LZlVtoU25hsXIKSn79//buPLyq+twX+HfPU/aQaWcOCYMhYRKZGgEtwiOg5YjSWm1qU20vVwWLWrWeKoLHQ1F7pVbrxdrT6vOcWmnpEatY8SKjKPMcQgJClEw7c/Y879/9I7BllyhBkrVC8v08z35C1lr7l996l8SXd//Wu860+7Dv8w7sqWnHcYcLZ9p9ONPuS3ifTq3E5MIU/Nu4bMwenQmLXiPV6RAREQ1qbWdXSqWxyTkREdGAJmtRSqvVYsKECdi0aRPmz58PoKsp+aZNm7B48eJu3+Pz+aBUJvYWUKm6mlsLIb7RmDSwaVRKWA1KWA1fFpXG5trwnbHZAABXIIwDX3SgstGFk00enGhy47NmD4KRGD4+2YqPT7biiXcqMKvYjnljs2EzauELReAJdhW5YkJg+og0DEk1yXWKREREA8q5nlKpJn5oSERENJDJfvveww8/jPLyckycOBGTJ0/Giy++CK/Xi7vvvhsA8KMf/Qg5OTlYuXIlAGDevHlYtWoVxo8fH799b+nSpZg3b168OHWxMYnOZ9Fr8O0iO75dZI9vi8YEalq92FDRiHcONeCzZg/+edSBfx51fOU4V+fZcMvV2bh5bBbs5kvrUdXiDuJ4owunWzy4KsOMyYUpbOxKRESDVqv3bE8prpQiIiIa0GQvSn3/+99HS0sLnnrqKTgcDlx99dXYsGFDvFH5mTNnElZGPfnkk1AoFHjyySdRX1+P9PR0zJs3DytWrOjxmEQXo1IqMNyehMU3jMCiGcNR2ejCPw41YHNVM4QQSNKpYdKpYdSq4Q1GsLumDYdqO3GothPPrK/E2FwbrAYNdGoldBoV9Gol1CoFAAUU57Wpqm334XijG61nPxE+Jy1JizmjM/GdsdmYVJDC3lZERDSoxFdKsacUERHRgKYQQgi5J9HfuFwuWK1WOJ1OWCwWuadDV4AWdxDvH2nAPw434OCZzkt+v0IBFKaaUJBmwoEzHej0heP70s06zBzZtZJr2og0JOlkryUTEREGb74gxXl/79VPsffzDvzfsmtwUzf9IomIiKh/62m+wH/dEvWCdLMOP55aiB9PLcSZNh8O13UiGIkhGIkiGI4hGIkhEo3hXAVYCEBAIN2sQ0mWBUWZZhi1XX8dw9EYPj3VhvWHG/DhMQda3EGs2VuLNXtroVEpMKkgBdddlY7xeTaMzrHC9BVFqnA0BpVCASVXWRER0RXmXKPzVBNv3yMiIhrIWJQi6mX5qUbkpxq/8fs1KiWuvyod11+VjhW3jsHO023YWt2MrdUtqGn14tNTbfj0VBsAQKkAhtuTMDbXhnSzDvUdftR3+lHX4UOzO4hkoxY3lmRgzuhMXDssDVo1+1QREVH/18rb94iIiAYFFqWI+jGt+ssC1bJ5QE2rF1urm7HzVBuO1DnhcAVwosmDE02ebt/f7g3FV1lZ9GrMKsnAuFwb7GYd7BYd7GY90s066DUqic+MiIioe6FIDK5ABEBXj0UiIiIauFiUIrqCFKaZUJhWiLunFgIAml0BHKlz4khdJ5z+MHKSDcixGZGbbEC2zYCTTW78s6IRGyqa0OoJ4u0D9Xj7QP0F4+o1SlgNGlj0GlgNGqQl6XDdVem4cVQG0i7jU+pgJIrjjW5kWHTItOihUPBWQiIi+nrtZ5+8p1YqYNFrZJ4NERER9SUWpYiuYHaLHrNK9JhV0v2TJdPNOlw7PA1P/9to7P+iAx8db0Jtuw9NrgCa3UE0u4MIRWIIhGMIhINocn35FMANxxx48p2jmFSQgrmjM1E6LA0qJRATQEwIxGJdK7lsRg1sBg3Uqq5bA5tcAWypasbmqmbs+KwVvlAUAJBi0mJUtgWjc6wYnW3F2FwrcpMNLFQREfWi5cuX4+mnn07YVlRUhKqqqoRtQgjcdNNN2LBhA9atW4f58+dLOMuvd+7WvRSTln0RiYiIBjgWpYgGAZVSgcmFKZhcmJKwXQgBlz8CVyAMpz8Ml7/r6+lWLzZUOHC03ondNe3YXdN+0Z9h1qmRpFej0RlI2J5s1MAdiKDdG8LHJ1vx8cnWhH1jcm0Yl2tFYZoJoUgM/nAUvlAUgXAURq0axVlmlGRbYDfreycYREQD3KhRo/DRRx/Fv1erL0z3XnzxxX77oUDb2ZVS7CdFREQ08LEoRTSIKRQKWI0aWI0a5P3LvkUzhqO23YcPjznwQYUDJ5vcUCoVUCoUUCq63hsIR+E+2/fDHYzAHYxAoQDG5towc6QdN4y0Y1S2BcFIDCea3Kiod6GiwYmjdU5UOVzo8IWx/UQLtp9ouehczz2pMD/FiBSTFqlJWqSYtLAZtPCFImjzhtDuDaHNE0IgEsWNJRm4/qr0fvuPLiKivqJWq5GZmfmV+w8dOoQXXngB+/btQ1ZWloQz65m2syul2E+KiIho4GNRioi+Ul6KET+dPhQ/nT70K4+JRLsa0nb6QnD6w8hNNiLdnPjptl6jwthcG8bm2uLbgpEoqhrdOFLvxJHaTjQ4/TBoVNBrVDBqVTBoVGj3hVHZ4MTpVi9a3EFsc1+8eHXOX3afwQh7En46vRC3XJ0Tb+YuhECzO4jKRhda3EEoFQooACiVgFKhQKpJhxEZSbCbdSxoEdEV6eTJk8jOzoZer0dpaSlWrlyJ/Px8AIDP58MPfvADvPLKK19buDpfMBhEMPjl7d0ul6tP5n3OuZ5SKSYWpYiIiAY6FqWI6LKoVUqkmLSX/I8HnVqFcXk2jMuzAd8a8rXH+kIRVDncqGxwweEMoN0XQrsnhHZfCJ2+EIxaNVLPziElSQtfMIp1B+txstmDX/zPUTy/oRo3jspAbbsfxxtd8VtDvo5Fr8ZVGWaMyDDjuhFpuKHYDp360p5S6PSFcazRicoGFyobXahscCEQjsJq1CL5bC8um1GLmcV2TB+R/pXjBCNR1HX4MSTFGO/dRUTUnSlTpuCNN95AUVERGhsb8fTTT2P69OmoqKiA2WzGQw89hGuvvRa33HJLj8dcuXLlBX2q+lKr5+zteybevkdERDTQKYQQQu5J9DculwtWqxVOpxMWi0Xu6RDRN+AKhPHXPbV4/ZMaNPxLnyulAhianoQcmwEAINC1giomBBo7A/i8zYvYv/xmTDZqcMvVOfjuhFyMyu76vVDX4UdFvRMVDU6cbvGi09fVk+tcfy53MNLj+d48JgtPzStBhuXL3llCCPzzqAPPbjiO2nY/Miw6LLgmF9+bmIfCNNM3CwxJquu/q66+bjTwXAn5QmdnJ4YMGYJVq1YhPT0dP//5z3Hw4EEkJSUB6LoV+2KNzrtbKZWXl9dn5/3o2sNYu78Oj84uwqIZw3t9fCIiIup7Pc2TWJTqxpWQZBJRz0SiMXxQ4UBFvROFaSYUZ1lQlGmO387XnUA4itMtXpxsduNonRPvHWlIeDJhYZoJbZ4gXIGLF53yUgwoybKgJMuKkmwLrAYNnP4wOnwhOH1hnGrxYO3+OkRjAmadGo/OKULZlCE4Wu/Ef66vxL4vOrodd3JBCq4vSkerJ4j6Dj8anH40dAagVChQnGVGcZYl/jU9SQe1UgmVSgG1UgGVsusrb0/sW8canHhwzSF0+sN46jsl+M7YLMZ8gLlS8oVJkyZh1qxZ8Pv9eOmll6BUfrniMhqNQqlUYvr06di6dWuPxuvr877njb3YXNWM5xaMwfcn5ff6+ERERNT3WJS6DFdKkklE0ojGBD4+2YK1++uw8VgTQtEYAECjUqAo04zR2VYUZZqRYtLCatDEX2lmHSx6zUXHP9bgxC/XVeBwbSeArkJWbbsfAKDXKPG/rxuGe6YW4tNTrfjrvlpsP9FywUquS6VQAFqVEjq1EjqNCiatChMLUjCjyI5pI9JgNXw5byEEWs724er0haHXdL1Hr1bBoFXBatDAbtbBpLv4HeHRmMBnzR4cruvEqRYPXP5w/AmQrkAEoUgMBo0SRq0aBm1XfzGbQQO7RY8Mix4ZFh0yLHpk2wxI6ubnxWICp1s9OFTrhMPpx7eGpuKa/ORee6x8LCZQ2ejCrtNt+LzNi5kjuxrqnz++EAL/vesL/Of7xxGKxOLbZxXb8cz80ciyGr5y/EA4ip2n27CtugXHGpyYMCQFt47PQVGmuVfmT73rSsgXPB4P8vPzsXz5ctx+++1obW1N2D9mzBj89re/xbx581BYWNijMfv6vG/53Q4crnPiv340EbNKMnp9fCIiIup7LEpdhishySQieXT6QthT045smwFXZZihVfdOj6doTOAvu7/A8xuq408xvG18Lh6dXYRMqz7h2EanH28fqMeJJjcyLXrkJBuQbTUgJ9mAYCSG442u+KvK4Y4/IbGnVEoFJgxJRkmWBZ81e3rch8uoVcFu1iHdrINZr4HhbMN6o1YFBYDjjW5UNDjhC0UvaT5fJdmoQW6yEbnJBqSbdfis2YOjdc4LbptMN+twY0kG5ozOxNgcG2o7fPi8zYvPW72oafUhGIki+VyfL6MWySYNVEolwpEYIrEYQlEBbzCCA190YHdNO5z+cML4w9JNuGdaIW4bn4tgJIrH/n4E/6+yCQAwc6Qdo3KsWL31M4SjXavhHr9pJL47IRetnhCaXAE0uwKo6/Djk89a8empNgTPK2SdU5Jlwa3jc3B9UTpa3EF83ubFmbau89BrVLjrW0MwsSClV+J6TjQmcLiuE/5QFN8amspbELvRH/OFRx55BPPmzcOQIUPQ0NCAZcuW4dChQ6isrER6+oW963py+96/6uvznvrsZtR3+rHu/msxPj+518cnIiKivsei1GXoj0kmEQ0OTa4A/udAHa4bkY7ROdZeGVMIgWhMIBI7+zUqEIrGEIxEEYzEEIrE0OwO4uMTLdhS3YxTLd4LxlAqgII0E7KsegTDMQQiUfhDUQTCMXT6QvBeQqHJqFVhdI4VJVkWpJq0MOvVsBg0sOg10KiV8Iei8Icj8IW6fkaHL4QmV/BsAScIhytwQWHofHqNEmNyrEg36/DxydZLLspdTJJOjUkFyci0GrD+cEO8CJZs1ECnVsHhCkCjUuDf5xbj7qkFUCgUONHkxmN/P4JDZ1fDfZ1sqx7XF9kxJseKrdXN2FLdjHD04v+rnlSQjPu+PQwziuw9uk3Q6QsjJgQ0aiU0KgU0SiWc/jC2n2zBlqpmbDvRgg5fV5wLUo249/phuPWanISG/5FoDHs/78DWE83Qq1UYm2vFmFwr7ObEQmosJtDqCcLpD6MgzQTNJTTsj8YEnP4wknTqb1wEFkL0ya2T/TFfuOOOO7B9+3a0tbUhPT0d06ZNw4oVKzBs2LBuj+9vRSkhBIqf2oBAOIaPH5uBvBRjr45PRERE0mBR6jL0xySTiEgqte0+bKluxpk2H4bZk7r6cGWYYdB+dR8ubzCCFncQze4gWtxBeIMR+EIR+MJdhaVQNIbh6UkYl2fDsPSky1514w6EUdfhR12HH7XtPjS5AyhMNWFcng0j7EnxpxSGIjHsPN2GDRUObKx0oNUTQopJi4JUIwrSTChMNcGoU6PTF0KHL4QOXxidvhBiMXQVa5QKaFRKaNVKjMwyo3RoKsbkWOPjuwNhrN1Xh9c/rYnfclmQasTLd16DMbmJRcVoTOCNTz/H//mwGv5wFBqVAnZz1y2JdrMeV+fbMKPIjqsykhIKKB3eEN4/2oh1B+tR2eBClk2PglQThqQaMSTFiCqHG28fqI/fVlqUYcat1+TEj8lLMSJJp0azK4Cdp9uw63Qbdp5qw+dtvovG2axXQ6lQxIuAmRY9/td1Q1GQasSHxxzYWNkUL1ydL8uqR3GWBb5QBA2dATQ6/fHCWpJOjcmFKbh2WCpKh6ViWHoS6jp8ONXiRU2rF6dbPGjoDKDVE0SrJ4R2bxAxAZi0KkwbkYYbRtoxo8gO+9mHAgTCUXzW7MGJJjdqWr1ocXe9r80bRJsnhDZPEL+9Y3yf3AY2WPOFvjxvbzCCUcs+BABU/sdsGLV8UDQREdGViEWpyzBYk0wiooEsGhPwh6Pd9qLqjbE3VjbhdKsHPyot+Nqf0bUSLIpko6bXVu80uQL4044avLn7DDzdPPXRolf3qDE/ABRnWfDtonTMKLLjmnwbgpEY3tpzBn/4+HRCw/9zbEYNZo7MgIDA0TonPmvxoLvMQqVUQKdW9trtmyMzzQiEo/ii3dftzzvf8wvG4vZJeb3yc883WPOFvjzv2nYfpj+/BXqNElXPzO3VsYmIiEg6Pc0X+PETERENCiqlok8KUufGnjM6s0fHGrSqr1119k1kWPT495uKcf+M4fjb3locrutEbbsPZ9p96PB1NZFXKLp6U5UO7VqhNKkwBSatGuFo7OxLQKVQwGpMbM6vVinx0+lDcVfpELx9oB5/2lEDXyiKWcV2zB6VicmFKfGVY0DXSpdjDS5UN7lh0auRYzMg22aA3ayDUqFAZaMLO0+14dNTrdhT0w5vKAqTVoXCdBOGpiVhaLoJeclGpJl1SEvSIi1Jh2SjFlUOF7ZUtWBzdTOO1HWiyuGO/8xkowYjMswYbk9CpkWP1CQtUk1d709N0iHTkng7IfVfrZ6uwmeqSSfzTIiIiEgKXCnVjcH6yScREQ08rkAY9R1+ZFn1sBm1ck8nQTgag9MfRqpJe0mrxlo9QeypaYfN0FWMSku6tPf3lsGaL/Tlebd5gvj4ZNcTAuePz+nVsYmIiEg6XClFREREsOg1sGRpLn6gDDQqJdKSLn1FTFqSDjeNyeqDGZHcUpN0LEYRERENIr3zLHMiIiIiIiIiIqJLwKIUERERERERERFJjkUpIiIiIiIiIiKSHItSREREREREREQkORaliIiIiIiIiIhIcixKERERERERERGR5FiUIiIiIiIiIiIiybEoRUREREREREREkmNRioiIiIiIiIiIJMeiFBERERERERERSY5FKSIiIiIiIiIikhyLUkREREREREREJDkWpYiIiIiIiIiISHIsShERERERERERkeRYlCIiIiIiIiIiIsmp5Z5AfySEAAC4XC6ZZ0JERET91bk84VzeMFgwTyIiIqKL6WmexKJUN9xuNwAgLy9P5pkQERFRf+d2u2G1WuWehmSYJxEREVFPXSxPUojB9vFeD8RiMTQ0NMBsNkOhUPT6+C6XC3l5eaitrYXFYun18enrMf7yYvzlx2sgL8ZfXr0ZfyEE3G43srOzoVQOno4IzJMGNsZfXoy//HgN5MX4y0uOPIkrpbqhVCqRm5vb5z/HYrHwL5qMGH95Mf7y4zWQF+Mvr96K/2BaIXUO86TBgfGXF+MvP14DeTH+8pIyTxo8H+sREREREREREVG/waIUERERERERERFJjkUpGeh0Oixbtgw6nU7uqQxKjL+8GH/58RrIi/GXF+Pf//EayYvxlxfjLz9eA3kx/vKSI/5sdE5ERERERERERJLjSikiIiIiIiIiIpIci1JERERERERERCQ5FqWIiIiIiIiIiEhyLEpJ7JVXXkFBQQH0ej2mTJmCPXv2yD2lAWnlypWYNGkSzGYz7HY75s+fj+rq6oRjAoEAFi1ahNTUVCQlJWHBggVoamqSacYD27PPPguFQoEHH3wwvo3x73v19fX44Q9/iNTUVBgMBowZMwb79u2L7xdC4KmnnkJWVhYMBgNmzZqFkydPyjjjgSMajWLp0qUoLCyEwWDAsGHD8Mwzz+D8No6Mf+/Zvn075s2bh+zsbCgUCrzzzjsJ+3sS6/b2dpSVlcFiscBms+EnP/kJPB6PhGdBAPMkqTBP6l+YJ8mDeZJ8mCdJq7/nSSxKSeivf/0rHn74YSxbtgwHDhzAuHHjMHv2bDQ3N8s9tQFn27ZtWLRoEXbt2oWNGzciHA7jxhtvhNfrjR/z0EMP4b333sPatWuxbds2NDQ04LbbbpNx1gPT3r178fvf/x5jx45N2M74962Ojg5MnToVGo0GH3zwASorK/HCCy8gOTk5fszzzz+Pl156Ca+++ip2794Nk8mE2bNnIxAIyDjzgeG5557D6tWr8bvf/Q7Hjx/Hc889h+effx4vv/xy/BjGv/d4vV6MGzcOr7zySrf7exLrsrIyHDt2DBs3bsT69euxfft2LFy4UKpTIDBPkhLzpP6DeZI8mCfJi3mStPp9niRIMpMnTxaLFi2Kfx+NRkV2drZYuXKljLMaHJqbmwUAsW3bNiGEEJ2dnUKj0Yi1a9fGjzl+/LgAIHbu3CnXNAcct9stRowYITZu3Ciuv/56sWTJEiEE4y+FX/ziF2LatGlfuT8Wi4nMzEzx61//Or6ts7NT6HQ68dZbb0kxxQHt5ptvFvfcc0/Ctttuu02UlZUJIRj/vgRArFu3Lv59T2JdWVkpAIi9e/fGj/nggw+EQqEQ9fX1ks19sGOeJB/mSfJgniQf5knyYp4kn/6YJ3GllERCoRD279+PWbNmxbcplUrMmjULO3fulHFmg4PT6QQApKSkAAD279+PcDiccD1GjhyJ/Px8Xo9etGjRItx8880JcQYYfym8++67mDhxIr73ve/Bbrdj/Pjx+MMf/hDfX1NTA4fDkXANrFYrpkyZwmvQC6699lps2rQJJ06cAAAcPnwYO3bswNy5cwEw/lLqSax37twJm82GiRMnxo+ZNWsWlEoldu/eLfmcByPmSfJiniQP5knyYZ4kL+ZJ/Ud/yJPUlz0C9Uhrayui0SgyMjIStmdkZKCqqkqmWQ0OsVgMDz74IKZOnYrRo0cDABwOB7RaLWw2W8KxGRkZcDgcMsxy4FmzZg0OHDiAvXv3XrCP8e97p0+fxurVq/Hwww/jl7/8Jfbu3Yuf/exn0Gq1KC8vj8e5u99JvAaX7/HHH4fL5cLIkSOhUqkQjUaxYsUKlJWVAQDjL6GexNrhcMButyfsV6vVSElJ4fWQCPMk+TBPkgfzJHkxT5IX86T+oz/kSSxK0YC3aNEiVFRUYMeOHXJPZdCora3FkiVLsHHjRuj1ermnMyjFYjFMnDgRv/rVrwAA48ePR0VFBV599VWUl5fLPLuB729/+xvefPNN/OUvf8GoUaNw6NAhPPjgg8jOzmb8iahfYZ4kPeZJ8mOeJC/mSXQ+3r4nkbS0NKhUqguemtHU1ITMzEyZZjXwLV68GOvXr8eWLVuQm5sb356ZmYlQKITOzs6E43k9esf+/fvR3NyMa665Bmq1Gmq1Gtu2bcNLL70EtVqNjIwMxr+PZWVloaSkJGFbcXExzpw5AwDxOPN3Ut949NFH8fjjj+OOO+7AmDFjcNddd+Ghhx7CypUrATD+UupJrDMzMy9oph2JRNDe3s7rIRHmSfJgniQP5knyY54kL+ZJ/Ud/yJNYlJKIVqvFhAkTsGnTpvi2WCyGTZs2obS0VMaZDUxCCCxevBjr1q3D5s2bUVhYmLB/woQJ0Gg0CdejuroaZ86c4fXoBTNnzsTRo0dx6NCh+GvixIkoKyuL/5nx71tTp0694PHeJ06cwJAhQwAAhYWFyMzMTLgGLpcLu3fv5jXoBT6fD0pl4v9iVSoVYrEYAMZfSj2JdWlpKTo7O7F///74MZs3b0YsFsOUKVMkn/NgxDxJWsyT5MU8SX7Mk+TFPKn/6Bd50mW3SqceW7NmjdDpdOKNN94QlZWVYuHChcJmswmHwyH31Aac++67T1itVrF161bR2NgYf/l8vvgx9957r8jPzxebN28W+/btE6WlpaK0tFTGWQ9s5z9VRgjGv6/t2bNHqNVqsWLFCnHy5Enx5ptvCqPRKP785z/Hj3n22WeFzWYT//jHP8SRI0fELbfcIgoLC4Xf75dx5gNDeXm5yMnJEevXrxc1NTXi7bffFmlpaeKxxx6LH8P49x632y0OHjwoDh48KACIVatWiYMHD4ovvvhCCNGzWM+ZM0eMHz9e7N69W+zYsUOMGDFC3HnnnXKd0qDEPEk6zJP6H+ZJ0mKeJC/mSdLq73kSi1ISe/nll0V+fr7QarVi8uTJYteuXXJPaUAC0O3r9ddfjx/j9/vF/fffL5KTk4XRaBS33nqraGxslG/SA9y/JluMf9977733xOjRo4VOpxMjR44Ur732WsL+WCwmli5dKjIyMoROpxMzZ84U1dXVMs12YHG5XGLJkiUiPz9f6PV6MXToUPHEE0+IYDAYP4bx7z1btmzp9nd+eXm5EKJnsW5raxN33nmnSEpKEhaLRdx9993C7XbLcDaDG/MkaTBP6n+YJ0mPeZJ8mCdJq7/nSQohhLj89VZEREREREREREQ9x55SREREREREREQkORaliIiIiIiIiIhIcixKERERERERERGR5FiUIiIiIiIiIiIiybEoRUREREREREREkmNRioiIiIiIiIiIJMeiFBERERERERERSY5FKSIiIiIiIiIikhyLUkREfUihUOCdd96RexpERERE/Q7zJCJiUYqIBqwf//jHUCgUF7zmzJkj99SIiIiIZMU8iYj6A7XcEyAi6ktz5szB66+/nrBNp9PJNBsiIiKi/oN5EhHJjSuliGhA0+l0yMzMTHglJycD6Foyvnr1asydOxcGgwFDhw7F3//+94T3Hz16FDfccAMMBgNSU1OxcOFCeDyehGP+9Kc/YdSoUdDpdMjKysLixYsT9re2tuLWW2+F0WjEiBEj8O677/btSRMRERH1APMkIpIbi1JENKgtXboUCxYswOHDh1FWVoY77rgDx48fBwB4vV7Mnj0bycnJ2Lt3L9auXYuPPvooIZlavXo1Fi1ahIULF+Lo0aN49913MXz48ISf8fTTT+P222/HkSNHcNNNN6GsrAzt7e2SnicRERHRpWKeRER9ThARDVDl5eVCpVIJk8mU8FqxYoUQQggA4t577014z5QpU8R9990nhBDitddeE8nJycLj8cT3v//++0KpVAqHwyGEECI7O1s88cQTXzkHAOLJJ5+Mf+/xeAQA8cEHH/TaeRIRERFdKuZJRNQfsKcUEQ1oM2bMwOrVqxO2paSkxP9cWlqasK+0tBSHDh0CABw/fhzjxo2DyWSK7586dSpisRiqq6uhUCjQ0NCAmTNnfu0cxo4dG/+zyWSCxWJBc3PzNz0lIiIiol7BPImI5MaiFBENaCaT6YJl4r3FYDD06DiNRpPwvUKhQCwW64spEREREfUY8yQikht7ShHRoLZr164Lvi8uLgYAFBcX4/Dhw/B6vfH9n3zyCZRKJYqKimA2m1FQUIBNmzZJOmciIiIiKTBPIqK+xpVSRDSgBYNBOByOhG1qtRppaWkAgLVr12LixImYNm0a3nzzTezZswd//OMfAQBlZWVYtmwZysvLsXz5crS0tOCBBx7AXXfdhYyMDADA8uXLce+998Jut2Pu3Llwu9345JNP8MADD0h7okRERESXiHkSEcmNRSkiGtA2bNiArKyshG1FRUWoqqoC0PXElzVr1uD+++9HVlYW3nrrLZSUlAAAjEYjPvzwQyxZsgSTJk2C0WjEggULsGrVqvhY5eXlCAQC+M1vfoNHHnkEaWlp+O53vyvdCRIRERF9Q8yTiEhuCiGEkHsSRERyUCgUWLduHebPny/3VIiIiIj6FeZJRCQF9pQiIiIiIiIiIiLJsShFRERERERERESS4+17REREREREREQkOa6UIiIiIiIiIiIiybEoRUREREREREREkmNRioiIiIiIiIiIJMeiFBERERERERERSY5FKSIiIiIiIiIikhyLUkREREREREREJDkWpYiIiIiIiIiISHIsShERERERERERkeRYlCIiIiIiIiIiIsn9f1BpYSvdCbk/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51b7d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy 0.6319333333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and print test accuracy.\n",
    "\n",
    "#Set model to eval model\n",
    "model.eval()\n",
    "\n",
    "#Evaluation Code\n",
    "\n",
    "total=0\n",
    "correct=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "        outputs=model(batch_features)\n",
    "        _, predicted=torch.max(outputs,1)\n",
    "\n",
    "        total = total + batch_labels.shape[0]\n",
    "        correct= correct+(predicted==batch_labels).sum().item()\n",
    "print(f'Evaluation accuracy {correct/total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9501c31",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning via Manual Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc7eb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,hidden_layers,neurons_per_layer,act_functn):\n",
    "        super().__init__()\n",
    "\n",
    "        layers =[]\n",
    "        \n",
    "        for i in range(hidden_layers):\n",
    "            layers.append(nn.Linear(input_dim,neurons_per_layer))\n",
    "            if act_functn=='relu':\n",
    "                   layers.append(nn.ReLU())\n",
    "            elif act_functn=='tanh':\n",
    "                   layers.append(nn.Tanh())\n",
    "            elif act_functn=='sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            # layers.append(nn.Dropout(drop_out_rate))\n",
    "            input_dim=neurons_per_layer\n",
    "        layers.append(nn.Linear(neurons_per_layer,output_dim))\n",
    "        # layers.append(nn.Softmax())\n",
    "\n",
    "        self.model=nn.Sequential(*layers)\n",
    "\n",
    "    def forward (self,x):\n",
    "        x=self.model(x)\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d636f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space={'hidden_layers':[1,2,3],'neurons_per_layer':[40,50,60],'act_functn':['relu','tanh','sigmoid'],'optimizer_name':['Adam','SGD','RMSprop'],'learning_rate':[0.001, 0.01,0.02],'batch_size':[100,128,],'epochs':[75,100]}\n",
    "\n",
    "#objective function\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_layers=trial.suggest_categorical('hidden_layers',search_space['hidden_layers'])\n",
    "    act_functn=trial.suggest_categorical('act_functn',search_space['act_functn'])\n",
    "    optimizer_name=trial.suggest_categorical('optimizer_name',search_space['optimizer_name'])\n",
    "    learning_rate=trial.suggest_categorical('learning_rate',search_space['learning_rate'])\n",
    "    batch_size=trial.suggest_categorical('batch_size',search_space['batch_size'])\n",
    "    epochs=trial.suggest_categorical('epochs',search_space['epochs'])\n",
    "    neurons_per_layer=trial.suggest_categorical('neurons_per_layer',search_space['neurons_per_layer'])\n",
    "    # dropout=trial.suggest_categorical('dropout',search_space['dropout'])\n",
    "\n",
    "    input_shape=X_train.shape[1]\n",
    "    output_shape=3\n",
    "\n",
    "    model=MyNN(input_shape,output_shape,hidden_layers,neurons_per_layer,act_functn)\n",
    "    model.to(device)\n",
    "\n",
    "    #Loss Function\n",
    "    criterion= nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    if optimizer_name=='Adam':\n",
    "        optimizer =  optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "    elif optimizer_name=='SGD':\n",
    "        optimizer =  optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "    elif optimizer_name=='RMSprop':\n",
    "        optimizer =  optim.RMSprop(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    # print(optimizer)\n",
    "\n",
    "\n",
    "    train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "    test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_epoch_loss=0\n",
    "        total=0\n",
    "        correct=0\n",
    "\n",
    "        for batch_features,batch_labels in train_loader:\n",
    "\n",
    "            #move data to gpu\n",
    "            batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
    "\n",
    "            #Forward pass\n",
    "            outputs=model(batch_features)\n",
    "\n",
    "            #Calculate loss\n",
    "            loss= criterion(outputs,batch_labels)\n",
    "\n",
    "            #backpass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #update grads\n",
    "            optimizer.step()\n",
    "\n",
    "            total_epoch_loss= total_epoch_loss+loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "            # print(f'predicted:{predicted}Labels {batch_labels}')\n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        avg_loss=total_epoch_loss/len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        print(f'Epoch {epoch +1 }, Training Loss: {avg_loss}')\n",
    "\n",
    "        # # Plot\n",
    "        # plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.plot(train_losses, label='Training Loss')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.title('Training Loss')\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.plot(train_accuracies, label='Training Accuracy')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Accuracy (%)')\n",
    "        # plt.title('Training Accuracy')\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        # Evaluate and print test accuracy.\n",
    "\n",
    "    #Set model to eval model\n",
    "    model.eval()\n",
    "\n",
    "    #Evaluation Code\n",
    "\n",
    "    total=0\n",
    "    correct=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "            outputs=model(batch_features)\n",
    "            _, predicted=torch.max(outputs,1)\n",
    "\n",
    "            total = total + batch_labels.shape[0]\n",
    "            correct= correct+(predicted==batch_labels).sum().item()\n",
    "    accuracy=correct/total\n",
    "    \n",
    "    # print(f'Evaluation accuracy with model parameters:  Learning rate: {learning_rate} batchsize : {batch_size} is : {accuracy}')\n",
    "\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 19:50:07,180] A new study created in memory with name: no-name-1138e45b-7014-4cb7-9146-c1d7b1b6497f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.8970520065243083\n",
      "Epoch 2, Training Loss: 0.8239295634111964\n",
      "Epoch 3, Training Loss: 0.8153982796167073\n",
      "Epoch 4, Training Loss: 0.8106156603734296\n",
      "Epoch 5, Training Loss: 0.8061953522208938\n",
      "Epoch 6, Training Loss: 0.8038895324656837\n",
      "Epoch 7, Training Loss: 0.8008328090036722\n",
      "Epoch 8, Training Loss: 0.7988826145803122\n",
      "Epoch 9, Training Loss: 0.7977007387275983\n",
      "Epoch 10, Training Loss: 0.796214593353128\n",
      "Epoch 11, Training Loss: 0.7938896713400245\n",
      "Epoch 12, Training Loss: 0.7938253042393161\n",
      "Epoch 13, Training Loss: 0.7937515718596322\n",
      "Epoch 14, Training Loss: 0.7926653149432705\n",
      "Epoch 15, Training Loss: 0.7915581531094429\n",
      "Epoch 16, Training Loss: 0.7920465119799277\n",
      "Epoch 17, Training Loss: 0.790984418607296\n",
      "Epoch 18, Training Loss: 0.791664175073007\n",
      "Epoch 19, Training Loss: 0.7908638093704567\n",
      "Epoch 20, Training Loss: 0.7901545426899329\n",
      "Epoch 21, Training Loss: 0.7896836128449978\n",
      "Epoch 22, Training Loss: 0.7896314208668874\n",
      "Epoch 23, Training Loss: 0.7891108289697116\n",
      "Epoch 24, Training Loss: 0.7895772854188331\n",
      "Epoch 25, Training Loss: 0.7886336092662094\n",
      "Epoch 26, Training Loss: 0.7885453559402237\n",
      "Epoch 27, Training Loss: 0.7884978599118111\n",
      "Epoch 28, Training Loss: 0.7882202359070455\n",
      "Epoch 29, Training Loss: 0.7882687750615571\n",
      "Epoch 30, Training Loss: 0.7885969062496845\n",
      "Epoch 31, Training Loss: 0.7879773060181984\n",
      "Epoch 32, Training Loss: 0.7872428029103387\n",
      "Epoch 33, Training Loss: 0.7880194988465847\n",
      "Epoch 34, Training Loss: 0.7889078225408281\n",
      "Epoch 35, Training Loss: 0.7880183222598599\n",
      "Epoch 36, Training Loss: 0.7878484151417151\n",
      "Epoch 37, Training Loss: 0.7871973036823416\n",
      "Epoch 38, Training Loss: 0.7869729414918369\n",
      "Epoch 39, Training Loss: 0.7863346298834435\n",
      "Epoch 40, Training Loss: 0.7873030583661301\n",
      "Epoch 41, Training Loss: 0.7867988786303011\n",
      "Epoch 42, Training Loss: 0.7861593570028033\n",
      "Epoch 43, Training Loss: 0.7858639170352678\n",
      "Epoch 44, Training Loss: 0.7865923298928971\n",
      "Epoch 45, Training Loss: 0.7859978953698524\n",
      "Epoch 46, Training Loss: 0.7864209279081875\n",
      "Epoch 47, Training Loss: 0.7856944493781355\n",
      "Epoch 48, Training Loss: 0.7857196936930032\n",
      "Epoch 49, Training Loss: 0.7860449354451402\n",
      "Epoch 50, Training Loss: 0.7855159317640433\n",
      "Epoch 51, Training Loss: 0.7853230525676469\n",
      "Epoch 52, Training Loss: 0.7850355493394952\n",
      "Epoch 53, Training Loss: 0.7851336143966904\n",
      "Epoch 54, Training Loss: 0.785130963110386\n",
      "Epoch 55, Training Loss: 0.7854766517653501\n",
      "Epoch 56, Training Loss: 0.78461374187828\n",
      "Epoch 57, Training Loss: 0.785664028780801\n",
      "Epoch 58, Training Loss: 0.7857890547666334\n",
      "Epoch 59, Training Loss: 0.7850511693416682\n",
      "Epoch 60, Training Loss: 0.7853420657322819\n",
      "Epoch 61, Training Loss: 0.7846588021382354\n",
      "Epoch 62, Training Loss: 0.7849118852973881\n",
      "Epoch 63, Training Loss: 0.7850754307624989\n",
      "Epoch 64, Training Loss: 0.7842120552421512\n",
      "Epoch 65, Training Loss: 0.784269336470984\n",
      "Epoch 66, Training Loss: 0.7842249057346717\n",
      "Epoch 67, Training Loss: 0.7844475689687227\n",
      "Epoch 68, Training Loss: 0.7839898817521289\n",
      "Epoch 69, Training Loss: 0.7846192422666047\n",
      "Epoch 70, Training Loss: 0.7839868568836298\n",
      "Epoch 71, Training Loss: 0.7841036845866899\n",
      "Epoch 72, Training Loss: 0.7837939334094972\n",
      "Epoch 73, Training Loss: 0.7844264230333773\n",
      "Epoch 74, Training Loss: 0.783802536734961\n",
      "Epoch 75, Training Loss: 0.7837268299626229\n",
      "Epoch 76, Training Loss: 0.7832526051908507\n",
      "Epoch 77, Training Loss: 0.7840599028687728\n",
      "Epoch 78, Training Loss: 0.7836434461120376\n",
      "Epoch 79, Training Loss: 0.7840233319684079\n",
      "Epoch 80, Training Loss: 0.7839121869632176\n",
      "Epoch 81, Training Loss: 0.7835272087190385\n",
      "Epoch 82, Training Loss: 0.7837488517725378\n",
      "Epoch 83, Training Loss: 0.7832636802716363\n",
      "Epoch 84, Training Loss: 0.783726740241947\n",
      "Epoch 85, Training Loss: 0.7846190489324412\n",
      "Epoch 86, Training Loss: 0.7841451454879647\n",
      "Epoch 87, Training Loss: 0.7835345777353846\n",
      "Epoch 88, Training Loss: 0.7831112423337492\n",
      "Epoch 89, Training Loss: 0.7832246260535448\n",
      "Epoch 90, Training Loss: 0.7835425143851373\n",
      "Epoch 91, Training Loss: 0.7834137961380464\n",
      "Epoch 92, Training Loss: 0.7841484805695096\n",
      "Epoch 93, Training Loss: 0.7838332509635982\n",
      "Epoch 94, Training Loss: 0.7829768436295645\n",
      "Epoch 95, Training Loss: 0.7830904083144395\n",
      "Epoch 96, Training Loss: 0.7832145797578912\n",
      "Epoch 97, Training Loss: 0.7832738637027884\n",
      "Epoch 98, Training Loss: 0.7825367950855341\n",
      "Epoch 99, Training Loss: 0.7837381520665678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 19:51:54,397] Trial 0 finished with value: 0.6303333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 0 with value: 0.6303333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7824159939038126\n",
      "Epoch 1, Training Loss: 0.8387569329792396\n",
      "Epoch 2, Training Loss: 0.8092809567774149\n",
      "Epoch 3, Training Loss: 0.8023941652219099\n",
      "Epoch 4, Training Loss: 0.7994877603717316\n",
      "Epoch 5, Training Loss: 0.7972087045361225\n",
      "Epoch 6, Training Loss: 0.7987153480823775\n",
      "Epoch 7, Training Loss: 0.7963238350430826\n",
      "Epoch 8, Training Loss: 0.7965555171321209\n",
      "Epoch 9, Training Loss: 0.7951837360410762\n",
      "Epoch 10, Training Loss: 0.7948780337670692\n",
      "Epoch 11, Training Loss: 0.7955051086899033\n",
      "Epoch 12, Training Loss: 0.7935089769219994\n",
      "Epoch 13, Training Loss: 0.793039123725174\n",
      "Epoch 14, Training Loss: 0.7929877187972678\n",
      "Epoch 15, Training Loss: 0.7949486247578957\n",
      "Epoch 16, Training Loss: 0.7923774960345792\n",
      "Epoch 17, Training Loss: 0.7919384722422836\n",
      "Epoch 18, Training Loss: 0.7918863327879655\n",
      "Epoch 19, Training Loss: 0.7916340065181704\n",
      "Epoch 20, Training Loss: 0.7922198449758658\n",
      "Epoch 21, Training Loss: 0.7912349440997705\n",
      "Epoch 22, Training Loss: 0.7907847552371204\n",
      "Epoch 23, Training Loss: 0.790552011154648\n",
      "Epoch 24, Training Loss: 0.7905469360208153\n",
      "Epoch 25, Training Loss: 0.7905861262988327\n",
      "Epoch 26, Training Loss: 0.7907214132466711\n",
      "Epoch 27, Training Loss: 0.7907055963699082\n",
      "Epoch 28, Training Loss: 0.7906230494492036\n",
      "Epoch 29, Training Loss: 0.7913771872233627\n",
      "Epoch 30, Training Loss: 0.7899846368266228\n",
      "Epoch 31, Training Loss: 0.7910292439890984\n",
      "Epoch 32, Training Loss: 0.7900996257487992\n",
      "Epoch 33, Training Loss: 0.7900086903034296\n",
      "Epoch 34, Training Loss: 0.7898272706153697\n",
      "Epoch 35, Training Loss: 0.7903881466478333\n",
      "Epoch 36, Training Loss: 0.7882795038079857\n",
      "Epoch 37, Training Loss: 0.7905041393480803\n",
      "Epoch 38, Training Loss: 0.7893088217068436\n",
      "Epoch 39, Training Loss: 0.7884997398333442\n",
      "Epoch 40, Training Loss: 0.7883180550166539\n",
      "Epoch 41, Training Loss: 0.7884182653032747\n",
      "Epoch 42, Training Loss: 0.7891998354653667\n",
      "Epoch 43, Training Loss: 0.7884296054678752\n",
      "Epoch 44, Training Loss: 0.7889186507777164\n",
      "Epoch 45, Training Loss: 0.7882267679486956\n",
      "Epoch 46, Training Loss: 0.7889155265083887\n",
      "Epoch 47, Training Loss: 0.787531400443916\n",
      "Epoch 48, Training Loss: 0.7876232076408272\n",
      "Epoch 49, Training Loss: 0.7873513818683481\n",
      "Epoch 50, Training Loss: 0.7885185381523648\n",
      "Epoch 51, Training Loss: 0.7882439512059205\n",
      "Epoch 52, Training Loss: 0.7880446082667301\n",
      "Epoch 53, Training Loss: 0.7875842831188575\n",
      "Epoch 54, Training Loss: 0.787391417994535\n",
      "Epoch 55, Training Loss: 0.7881113669926063\n",
      "Epoch 56, Training Loss: 0.7881920922071414\n",
      "Epoch 57, Training Loss: 0.7882574538539227\n",
      "Epoch 58, Training Loss: 0.788581138894074\n",
      "Epoch 59, Training Loss: 0.7875963770357289\n",
      "Epoch 60, Training Loss: 0.7882214304199793\n",
      "Epoch 61, Training Loss: 0.7880243645574814\n",
      "Epoch 62, Training Loss: 0.7873060954244513\n",
      "Epoch 63, Training Loss: 0.7870214148571617\n",
      "Epoch 64, Training Loss: 0.7876448978158764\n",
      "Epoch 65, Training Loss: 0.7876085564606172\n",
      "Epoch 66, Training Loss: 0.786785732534595\n",
      "Epoch 67, Training Loss: 0.7877046602112906\n",
      "Epoch 68, Training Loss: 0.7881063305345694\n",
      "Epoch 69, Training Loss: 0.7886215607026466\n",
      "Epoch 70, Training Loss: 0.7879063320339175\n",
      "Epoch 71, Training Loss: 0.7878162123206863\n",
      "Epoch 72, Training Loss: 0.7867307694334733\n",
      "Epoch 73, Training Loss: 0.7877225019878015\n",
      "Epoch 74, Training Loss: 0.7863044277169651\n",
      "Epoch 75, Training Loss: 0.7869028717055356\n",
      "Epoch 76, Training Loss: 0.7873264437331293\n",
      "Epoch 77, Training Loss: 0.7869127907251057\n",
      "Epoch 78, Training Loss: 0.7877360811807159\n",
      "Epoch 79, Training Loss: 0.7867776472765701\n",
      "Epoch 80, Training Loss: 0.7859799237179577\n",
      "Epoch 81, Training Loss: 0.7859804838223565\n",
      "Epoch 82, Training Loss: 0.7873134975146531\n",
      "Epoch 83, Training Loss: 0.7873047505106244\n",
      "Epoch 84, Training Loss: 0.7873648096744279\n",
      "Epoch 85, Training Loss: 0.7879206453050885\n",
      "Epoch 86, Training Loss: 0.7865210869258508\n",
      "Epoch 87, Training Loss: 0.7858824211851995\n",
      "Epoch 88, Training Loss: 0.7858793221918264\n",
      "Epoch 89, Training Loss: 0.7870045194052215\n",
      "Epoch 90, Training Loss: 0.787083311457383\n",
      "Epoch 91, Training Loss: 0.7869164954450794\n",
      "Epoch 92, Training Loss: 0.7865790538321761\n",
      "Epoch 93, Training Loss: 0.7865559274988964\n",
      "Epoch 94, Training Loss: 0.7872060209288633\n",
      "Epoch 95, Training Loss: 0.7861475073305287\n",
      "Epoch 96, Training Loss: 0.786775403184102\n",
      "Epoch 97, Training Loss: 0.7865455631026648\n",
      "Epoch 98, Training Loss: 0.7849930147031197\n",
      "Epoch 99, Training Loss: 0.7865314070443462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 19:53:46,478] Trial 1 finished with value: 0.6359333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7866457136053788\n",
      "Epoch 1, Training Loss: 0.8476737241885242\n",
      "Epoch 2, Training Loss: 0.8188646682570963\n",
      "Epoch 3, Training Loss: 0.8144354329389685\n",
      "Epoch 4, Training Loss: 0.811435117300819\n",
      "Epoch 5, Training Loss: 0.8095071416742662\n",
      "Epoch 6, Training Loss: 0.8059965398031123\n",
      "Epoch 7, Training Loss: 0.8052741707072538\n",
      "Epoch 8, Training Loss: 0.8033474967058967\n",
      "Epoch 9, Training Loss: 0.8030135427503025\n",
      "Epoch 10, Training Loss: 0.8023244657235987\n",
      "Epoch 11, Training Loss: 0.8019973506646998\n",
      "Epoch 12, Training Loss: 0.8007178033099455\n",
      "Epoch 13, Training Loss: 0.8013947516329148\n",
      "Epoch 14, Training Loss: 0.799403123995837\n",
      "Epoch 15, Training Loss: 0.7998054341007681\n",
      "Epoch 16, Training Loss: 0.799372454180437\n",
      "Epoch 17, Training Loss: 0.798741902884315\n",
      "Epoch 18, Training Loss: 0.7991880604800056\n",
      "Epoch 19, Training Loss: 0.797943820883246\n",
      "Epoch 20, Training Loss: 0.798230082357631\n",
      "Epoch 21, Training Loss: 0.798059563777026\n",
      "Epoch 22, Training Loss: 0.7970234301510979\n",
      "Epoch 23, Training Loss: 0.7971657602927265\n",
      "Epoch 24, Training Loss: 0.7965225241464727\n",
      "Epoch 25, Training Loss: 0.7970857261910158\n",
      "Epoch 26, Training Loss: 0.7969977756107555\n",
      "Epoch 27, Training Loss: 0.79634043637444\n",
      "Epoch 28, Training Loss: 0.796748847470564\n",
      "Epoch 29, Training Loss: 0.7963699008436764\n",
      "Epoch 30, Training Loss: 0.7957799937444575\n",
      "Epoch 31, Training Loss: 0.7960057904439813\n",
      "Epoch 32, Training Loss: 0.7959026636095607\n",
      "Epoch 33, Training Loss: 0.7956463360786438\n",
      "Epoch 34, Training Loss: 0.7963753405739279\n",
      "Epoch 35, Training Loss: 0.7959768892035765\n",
      "Epoch 36, Training Loss: 0.7955412352786345\n",
      "Epoch 37, Training Loss: 0.7954179875289693\n",
      "Epoch 38, Training Loss: 0.7960168541178984\n",
      "Epoch 39, Training Loss: 0.7950862091429093\n",
      "Epoch 40, Training Loss: 0.7948829726611867\n",
      "Epoch 41, Training Loss: 0.7955545341267305\n",
      "Epoch 42, Training Loss: 0.7947494673728943\n",
      "Epoch 43, Training Loss: 0.7945144093036651\n",
      "Epoch 44, Training Loss: 0.7952303067375632\n",
      "Epoch 45, Training Loss: 0.7947902631759644\n",
      "Epoch 46, Training Loss: 0.7940490819426144\n",
      "Epoch 47, Training Loss: 0.7948198357049157\n",
      "Epoch 48, Training Loss: 0.7944444106606876\n",
      "Epoch 49, Training Loss: 0.794911186765222\n",
      "Epoch 50, Training Loss: 0.7942119916747598\n",
      "Epoch 51, Training Loss: 0.7942520160534803\n",
      "Epoch 52, Training Loss: 0.7939729421279009\n",
      "Epoch 53, Training Loss: 0.7936523433993844\n",
      "Epoch 54, Training Loss: 0.7942840914165272\n",
      "Epoch 55, Training Loss: 0.7941790683129254\n",
      "Epoch 56, Training Loss: 0.7938254441233242\n",
      "Epoch 57, Training Loss: 0.7935670532899745\n",
      "Epoch 58, Training Loss: 0.7939042591347414\n",
      "Epoch 59, Training Loss: 0.7942264047790976\n",
      "Epoch 60, Training Loss: 0.7939942439163432\n",
      "Epoch 61, Training Loss: 0.7942481780753416\n",
      "Epoch 62, Training Loss: 0.793696403503418\n",
      "Epoch 63, Training Loss: 0.7936023286510916\n",
      "Epoch 64, Training Loss: 0.7936712163336137\n",
      "Epoch 65, Training Loss: 0.7932095761860118\n",
      "Epoch 66, Training Loss: 0.7929964963828816\n",
      "Epoch 67, Training Loss: 0.7935951665569754\n",
      "Epoch 68, Training Loss: 0.7931926370368284\n",
      "Epoch 69, Training Loss: 0.793028040002374\n",
      "Epoch 70, Training Loss: 0.7930327245067148\n",
      "Epoch 71, Training Loss: 0.792763215303421\n",
      "Epoch 72, Training Loss: 0.7932058775424957\n",
      "Epoch 73, Training Loss: 0.7929405998482424\n",
      "Epoch 74, Training Loss: 0.7933438355081222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 19:55:06,784] Trial 2 finished with value: 0.6342 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7929684786936816\n",
      "Epoch 1, Training Loss: 1.0997589985062095\n",
      "Epoch 2, Training Loss: 1.0919822248290567\n",
      "Epoch 3, Training Loss: 1.0917056522649877\n",
      "Epoch 4, Training Loss: 1.091445624547846\n",
      "Epoch 5, Training Loss: 1.091177908112021\n",
      "Epoch 6, Training Loss: 1.090905271137462\n",
      "Epoch 7, Training Loss: 1.0906395010387195\n",
      "Epoch 8, Training Loss: 1.0903885635207682\n",
      "Epoch 9, Training Loss: 1.0901296917129966\n",
      "Epoch 10, Training Loss: 1.0898601830706878\n",
      "Epoch 11, Training Loss: 1.0895930966208962\n",
      "Epoch 12, Training Loss: 1.0893385183109956\n",
      "Epoch 13, Training Loss: 1.0890733257461997\n",
      "Epoch 14, Training Loss: 1.0888041640730466\n",
      "Epoch 15, Training Loss: 1.0885455594343298\n",
      "Epoch 16, Training Loss: 1.088277618043563\n",
      "Epoch 17, Training Loss: 1.0880047374613144\n",
      "Epoch 18, Training Loss: 1.0877537655830383\n",
      "Epoch 19, Training Loss: 1.0874648217593923\n",
      "Epoch 20, Training Loss: 1.0872000677445355\n",
      "Epoch 21, Training Loss: 1.0869022764879115\n",
      "Epoch 22, Training Loss: 1.0866287495108211\n",
      "Epoch 23, Training Loss: 1.0863522998024435\n",
      "Epoch 24, Training Loss: 1.0860626426865072\n",
      "Epoch 25, Training Loss: 1.0857645988464355\n",
      "Epoch 26, Training Loss: 1.0854710703737596\n",
      "Epoch 27, Training Loss: 1.0851808029062608\n",
      "Epoch 28, Training Loss: 1.0848716838219588\n",
      "Epoch 29, Training Loss: 1.0845651505975162\n",
      "Epoch 30, Training Loss: 1.0842411689197315\n",
      "Epoch 31, Training Loss: 1.0839219680954428\n",
      "Epoch 32, Training Loss: 1.0835997101839852\n",
      "Epoch 33, Training Loss: 1.0832648560580085\n",
      "Epoch 34, Training Loss: 1.0829320460207321\n",
      "Epoch 35, Training Loss: 1.0826025153608883\n",
      "Epoch 36, Training Loss: 1.0822327317911036\n",
      "Epoch 37, Training Loss: 1.0818955197053797\n",
      "Epoch 38, Training Loss: 1.0815188821624306\n",
      "Epoch 39, Training Loss: 1.0811540000578936\n",
      "Epoch 40, Training Loss: 1.080780475139618\n",
      "Epoch 41, Training Loss: 1.0803959340207716\n",
      "Epoch 42, Training Loss: 1.0799771700185887\n",
      "Epoch 43, Training Loss: 1.0796045193952672\n",
      "Epoch 44, Training Loss: 1.0791544689851649\n",
      "Epoch 45, Training Loss: 1.0787740049642676\n",
      "Epoch 46, Training Loss: 1.0783379832436057\n",
      "Epoch 47, Training Loss: 1.0778931087606094\n",
      "Epoch 48, Training Loss: 1.0774454098589281\n",
      "Epoch 49, Training Loss: 1.076952057165258\n",
      "Epoch 50, Training Loss: 1.076499938544105\n",
      "Epoch 51, Training Loss: 1.0760263960501726\n",
      "Epoch 52, Training Loss: 1.0755217562002295\n",
      "Epoch 53, Training Loss: 1.0750170777825747\n",
      "Epoch 54, Training Loss: 1.0744978370386011\n",
      "Epoch 55, Training Loss: 1.073957802548128\n",
      "Epoch 56, Training Loss: 1.0734220608542948\n",
      "Epoch 57, Training Loss: 1.0728414915589726\n",
      "Epoch 58, Training Loss: 1.072277240192189\n",
      "Epoch 59, Training Loss: 1.0716820450390085\n",
      "Epoch 60, Training Loss: 1.071060925511753\n",
      "Epoch 61, Training Loss: 1.0704720606523401\n",
      "Epoch 62, Training Loss: 1.0698293189441457\n",
      "Epoch 63, Training Loss: 1.0691790810753317\n",
      "Epoch 64, Training Loss: 1.0684916601461523\n",
      "Epoch 65, Training Loss: 1.0678203397638657\n",
      "Epoch 66, Training Loss: 1.0671175583671122\n",
      "Epoch 67, Training Loss: 1.0663932817122515\n",
      "Epoch 68, Training Loss: 1.065669625085943\n",
      "Epoch 69, Training Loss: 1.0649027580373427\n",
      "Epoch 70, Training Loss: 1.0641459840886733\n",
      "Epoch 71, Training Loss: 1.063338327407837\n",
      "Epoch 72, Training Loss: 1.0625232013534096\n",
      "Epoch 73, Training Loss: 1.0616976658035726\n",
      "Epoch 74, Training Loss: 1.060852636309231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 19:56:25,746] Trial 3 finished with value: 0.45326666666666665 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 1.0599716575005476\n",
      "Epoch 1, Training Loss: 0.9586070341222427\n",
      "Epoch 2, Training Loss: 0.881836282716078\n",
      "Epoch 3, Training Loss: 0.8751907443299013\n",
      "Epoch 4, Training Loss: 0.8706529236541075\n",
      "Epoch 5, Training Loss: 0.8691059955428628\n",
      "Epoch 6, Training Loss: 0.8649999024587519\n",
      "Epoch 7, Training Loss: 0.8645643719504861\n",
      "Epoch 8, Training Loss: 0.865520677987267\n",
      "Epoch 9, Training Loss: 0.863359901414198\n",
      "Epoch 10, Training Loss: 0.862202774847255\n",
      "Epoch 11, Training Loss: 0.8612418416668387\n",
      "Epoch 12, Training Loss: 0.8617032296517316\n",
      "Epoch 13, Training Loss: 0.8637326342218062\n",
      "Epoch 14, Training Loss: 0.8601436037175796\n",
      "Epoch 15, Training Loss: 0.8565851309018977\n",
      "Epoch 16, Training Loss: 0.8594903882812052\n",
      "Epoch 17, Training Loss: 0.8597786183918223\n",
      "Epoch 18, Training Loss: 0.859518067556269\n",
      "Epoch 19, Training Loss: 0.8577144129837261\n",
      "Epoch 20, Training Loss: 0.8583113065186669\n",
      "Epoch 21, Training Loss: 0.8556945915783153\n",
      "Epoch 22, Training Loss: 0.8579675016683691\n",
      "Epoch 23, Training Loss: 0.8566233123751248\n",
      "Epoch 24, Training Loss: 0.8570064841298496\n",
      "Epoch 25, Training Loss: 0.8574806562592001\n",
      "Epoch 26, Training Loss: 0.8557949203603408\n",
      "Epoch 27, Training Loss: 0.85603601287393\n",
      "Epoch 28, Training Loss: 0.8528144361692317\n",
      "Epoch 29, Training Loss: 0.8571772585896885\n",
      "Epoch 30, Training Loss: 0.8561874054459965\n",
      "Epoch 31, Training Loss: 0.8574932193054873\n",
      "Epoch 32, Training Loss: 0.8570567257264081\n",
      "Epoch 33, Training Loss: 0.8552140444867751\n",
      "Epoch 34, Training Loss: 0.8562016144920798\n",
      "Epoch 35, Training Loss: 0.85476321942666\n",
      "Epoch 36, Training Loss: 0.8553178411371568\n",
      "Epoch 37, Training Loss: 0.8555365707593806\n",
      "Epoch 38, Training Loss: 0.8542820673129138\n",
      "Epoch 39, Training Loss: 0.853880682482439\n",
      "Epoch 40, Training Loss: 0.8552912919661578\n",
      "Epoch 41, Training Loss: 0.852805889564402\n",
      "Epoch 42, Training Loss: 0.8518087288211373\n",
      "Epoch 43, Training Loss: 0.8539902374323677\n",
      "Epoch 44, Training Loss: 0.8540708521534415\n",
      "Epoch 45, Training Loss: 0.8536672328500187\n",
      "Epoch 46, Training Loss: 0.8544502662209903\n",
      "Epoch 47, Training Loss: 0.8537473381266875\n",
      "Epoch 48, Training Loss: 0.8517243087291717\n",
      "Epoch 49, Training Loss: 0.8531577085046207\n",
      "Epoch 50, Training Loss: 0.8514703553564408\n",
      "Epoch 51, Training Loss: 0.8503813779354096\n",
      "Epoch 52, Training Loss: 0.8528668685520396\n",
      "Epoch 53, Training Loss: 0.8534591967919294\n",
      "Epoch 54, Training Loss: 0.8528535199165345\n",
      "Epoch 55, Training Loss: 0.8528937950555016\n",
      "Epoch 56, Training Loss: 0.855311661047094\n",
      "Epoch 57, Training Loss: 0.8531491319572224\n",
      "Epoch 58, Training Loss: 0.8556620707932641\n",
      "Epoch 59, Training Loss: 0.8526959712365094\n",
      "Epoch 60, Training Loss: 0.853797094892053\n",
      "Epoch 61, Training Loss: 0.8508954265538384\n",
      "Epoch 62, Training Loss: 0.853661252190085\n",
      "Epoch 63, Training Loss: 0.8505019514700946\n",
      "Epoch 64, Training Loss: 0.8525176811218261\n",
      "Epoch 65, Training Loss: 0.8508761389816508\n",
      "Epoch 66, Training Loss: 0.854043159344617\n",
      "Epoch 67, Training Loss: 0.8536115715082954\n",
      "Epoch 68, Training Loss: 0.8528228774491479\n",
      "Epoch 69, Training Loss: 0.8521445701402777\n",
      "Epoch 70, Training Loss: 0.8508914681041941\n",
      "Epoch 71, Training Loss: 0.8502866341787226\n",
      "Epoch 72, Training Loss: 0.8515844435551587\n",
      "Epoch 73, Training Loss: 0.854438977171393\n",
      "Epoch 74, Training Loss: 0.8519916702719296\n",
      "Epoch 75, Training Loss: 0.8500356459617615\n",
      "Epoch 76, Training Loss: 0.8524562788009643\n",
      "Epoch 77, Training Loss: 0.8527535493934856\n",
      "Epoch 78, Training Loss: 0.8520113058651195\n",
      "Epoch 79, Training Loss: 0.853351940056857\n",
      "Epoch 80, Training Loss: 0.849431624622906\n",
      "Epoch 81, Training Loss: 0.8503583124805899\n",
      "Epoch 82, Training Loss: 0.8531649629508747\n",
      "Epoch 83, Training Loss: 0.852274932160097\n",
      "Epoch 84, Training Loss: 0.853808405679815\n",
      "Epoch 85, Training Loss: 0.8527730998572182\n",
      "Epoch 86, Training Loss: 0.8538667056139778\n",
      "Epoch 87, Training Loss: 0.8513431760142831\n",
      "Epoch 88, Training Loss: 0.8516018440442926\n",
      "Epoch 89, Training Loss: 0.8520716890166787\n",
      "Epoch 90, Training Loss: 0.8529052209152895\n",
      "Epoch 91, Training Loss: 0.8521974842688617\n",
      "Epoch 92, Training Loss: 0.8515672859023599\n",
      "Epoch 93, Training Loss: 0.8504752694157993\n",
      "Epoch 94, Training Loss: 0.8502284420237821\n",
      "Epoch 95, Training Loss: 0.8511849545030032\n",
      "Epoch 96, Training Loss: 0.8525672768144047\n",
      "Epoch 97, Training Loss: 0.8513052098190084\n",
      "Epoch 98, Training Loss: 0.8505269941161661\n",
      "Epoch 99, Training Loss: 0.8496199626782361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 19:58:31,274] Trial 4 finished with value: 0.6126 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8509169439708485\n",
      "Epoch 1, Training Loss: 0.8956807327971739\n",
      "Epoch 2, Training Loss: 0.8286908267526065\n",
      "Epoch 3, Training Loss: 0.820452001024695\n",
      "Epoch 4, Training Loss: 0.8170476494115941\n",
      "Epoch 5, Training Loss: 0.8141092313037199\n",
      "Epoch 6, Training Loss: 0.8124642787961399\n",
      "Epoch 7, Training Loss: 0.8102742366229787\n",
      "Epoch 8, Training Loss: 0.8093338793866774\n",
      "Epoch 9, Training Loss: 0.8086588771202985\n",
      "Epoch 10, Training Loss: 0.8078897171861985\n",
      "Epoch 11, Training Loss: 0.8072045104643878\n",
      "Epoch 12, Training Loss: 0.8072320788748124\n",
      "Epoch 13, Training Loss: 0.8064673993867987\n",
      "Epoch 14, Training Loss: 0.8057075086761923\n",
      "Epoch 15, Training Loss: 0.8050142628305098\n",
      "Epoch 16, Training Loss: 0.8054819508861093\n",
      "Epoch 17, Training Loss: 0.8045559362102958\n",
      "Epoch 18, Training Loss: 0.8044028390155119\n",
      "Epoch 19, Training Loss: 0.804317626322017\n",
      "Epoch 20, Training Loss: 0.803598920878242\n",
      "Epoch 21, Training Loss: 0.8035935636127697\n",
      "Epoch 22, Training Loss: 0.8026007021174711\n",
      "Epoch 23, Training Loss: 0.8022240272690269\n",
      "Epoch 24, Training Loss: 0.8013967973344466\n",
      "Epoch 25, Training Loss: 0.801071993673549\n",
      "Epoch 26, Training Loss: 0.8007524756123038\n",
      "Epoch 27, Training Loss: 0.8009570591589984\n",
      "Epoch 28, Training Loss: 0.7999400924935061\n",
      "Epoch 29, Training Loss: 0.7995107296635123\n",
      "Epoch 30, Training Loss: 0.7993045097238878\n",
      "Epoch 31, Training Loss: 0.7991407041689929\n",
      "Epoch 32, Training Loss: 0.7989760479506325\n",
      "Epoch 33, Training Loss: 0.7982905230802648\n",
      "Epoch 34, Training Loss: 0.7979706886936636\n",
      "Epoch 35, Training Loss: 0.7979426420436186\n",
      "Epoch 36, Training Loss: 0.7975105463056004\n",
      "Epoch 37, Training Loss: 0.7971826507764704\n",
      "Epoch 38, Training Loss: 0.7974419226365931\n",
      "Epoch 39, Training Loss: 0.7968007423597223\n",
      "Epoch 40, Training Loss: 0.7961250355664422\n",
      "Epoch 41, Training Loss: 0.796539463856641\n",
      "Epoch 42, Training Loss: 0.7962839755591224\n",
      "Epoch 43, Training Loss: 0.7961890896628885\n",
      "Epoch 44, Training Loss: 0.7956627785458285\n",
      "Epoch 45, Training Loss: 0.7960607881405775\n",
      "Epoch 46, Training Loss: 0.7955791272135342\n",
      "Epoch 47, Training Loss: 0.7954530075017143\n",
      "Epoch 48, Training Loss: 0.7953981606399312\n",
      "Epoch 49, Training Loss: 0.7956117245730232\n",
      "Epoch 50, Training Loss: 0.7954076400925132\n",
      "Epoch 51, Training Loss: 0.7952328933687771\n",
      "Epoch 52, Training Loss: 0.7945807709413416\n",
      "Epoch 53, Training Loss: 0.7944738711329068\n",
      "Epoch 54, Training Loss: 0.7948120946042678\n",
      "Epoch 55, Training Loss: 0.7942099024968988\n",
      "Epoch 56, Training Loss: 0.7948608267307281\n",
      "Epoch 57, Training Loss: 0.794111380366718\n",
      "Epoch 58, Training Loss: 0.7941348709779628\n",
      "Epoch 59, Training Loss: 0.793971827100305\n",
      "Epoch 60, Training Loss: 0.7942021750702577\n",
      "Epoch 61, Training Loss: 0.7940851754300735\n",
      "Epoch 62, Training Loss: 0.79368924407398\n",
      "Epoch 63, Training Loss: 0.7938735634439131\n",
      "Epoch 64, Training Loss: 0.7940694231145522\n",
      "Epoch 65, Training Loss: 0.7933493530049044\n",
      "Epoch 66, Training Loss: 0.794235969992245\n",
      "Epoch 67, Training Loss: 0.7933207774863523\n",
      "Epoch 68, Training Loss: 0.7931886285894058\n",
      "Epoch 69, Training Loss: 0.7935120130286497\n",
      "Epoch 70, Training Loss: 0.7936811751477859\n",
      "Epoch 71, Training Loss: 0.7930072633659139\n",
      "Epoch 72, Training Loss: 0.793041453221265\n",
      "Epoch 73, Training Loss: 0.793081728219986\n",
      "Epoch 74, Training Loss: 0.7932703809878405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 19:59:54,633] Trial 5 finished with value: 0.6342666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7924745570912081\n",
      "Epoch 1, Training Loss: 0.8870471697695115\n",
      "Epoch 2, Training Loss: 0.8198384574581595\n",
      "Epoch 3, Training Loss: 0.8146695209951962\n",
      "Epoch 4, Training Loss: 0.8099717455050525\n",
      "Epoch 5, Training Loss: 0.8068451384235831\n",
      "Epoch 6, Training Loss: 0.8032122735416188\n",
      "Epoch 7, Training Loss: 0.8006625575879041\n",
      "Epoch 8, Training Loss: 0.7975637564939612\n",
      "Epoch 9, Training Loss: 0.7957404913621791\n",
      "Epoch 10, Training Loss: 0.7946071809179642\n",
      "Epoch 11, Training Loss: 0.7929290637549232\n",
      "Epoch 12, Training Loss: 0.792027875956367\n",
      "Epoch 13, Training Loss: 0.7922313190908993\n",
      "Epoch 14, Training Loss: 0.7913271000806024\n",
      "Epoch 15, Training Loss: 0.7901261719535378\n",
      "Epoch 16, Training Loss: 0.7902688834246467\n",
      "Epoch 17, Training Loss: 0.7896846563675824\n",
      "Epoch 18, Training Loss: 0.7897601965595694\n",
      "Epoch 19, Training Loss: 0.7898507865737466\n",
      "Epoch 20, Training Loss: 0.7888060287167045\n",
      "Epoch 21, Training Loss: 0.7885656208851758\n",
      "Epoch 22, Training Loss: 0.7886554004164303\n",
      "Epoch 23, Training Loss: 0.7882133826087503\n",
      "Epoch 24, Training Loss: 0.7881080252282759\n",
      "Epoch 25, Training Loss: 0.788116339935976\n",
      "Epoch 26, Training Loss: 0.7872348737716675\n",
      "Epoch 27, Training Loss: 0.7877795322502361\n",
      "Epoch 28, Training Loss: 0.7871842184487511\n",
      "Epoch 29, Training Loss: 0.7869315645273994\n",
      "Epoch 30, Training Loss: 0.787047473262338\n",
      "Epoch 31, Training Loss: 0.7869259055923014\n",
      "Epoch 32, Training Loss: 0.7867063787404228\n",
      "Epoch 33, Training Loss: 0.7870597505569458\n",
      "Epoch 34, Training Loss: 0.7869092597680933\n",
      "Epoch 35, Training Loss: 0.7862902314522687\n",
      "Epoch 36, Training Loss: 0.7860877793676713\n",
      "Epoch 37, Training Loss: 0.7862426838454079\n",
      "Epoch 38, Training Loss: 0.7859356051332811\n",
      "Epoch 39, Training Loss: 0.7859088730812073\n",
      "Epoch 40, Training Loss: 0.7855882043698255\n",
      "Epoch 41, Training Loss: 0.7854714442701901\n",
      "Epoch 42, Training Loss: 0.7856954221164479\n",
      "Epoch 43, Training Loss: 0.7857961006024304\n",
      "Epoch 44, Training Loss: 0.7852729105949402\n",
      "Epoch 45, Training Loss: 0.7854418714607463\n",
      "Epoch 46, Training Loss: 0.7857069927804611\n",
      "Epoch 47, Training Loss: 0.7853410109351663\n",
      "Epoch 48, Training Loss: 0.7855546898701612\n",
      "Epoch 49, Training Loss: 0.7849991515103508\n",
      "Epoch 50, Training Loss: 0.7846773814453798\n",
      "Epoch 51, Training Loss: 0.7847753658014185\n",
      "Epoch 52, Training Loss: 0.7849336295969346\n",
      "Epoch 53, Training Loss: 0.7846170554441564\n",
      "Epoch 54, Training Loss: 0.7846784177948447\n",
      "Epoch 55, Training Loss: 0.7847955536842346\n",
      "Epoch 56, Training Loss: 0.7842907512889189\n",
      "Epoch 57, Training Loss: 0.7846044231863583\n",
      "Epoch 58, Training Loss: 0.7843206099201652\n",
      "Epoch 59, Training Loss: 0.7845765472159666\n",
      "Epoch 60, Training Loss: 0.7844844848969403\n",
      "Epoch 61, Training Loss: 0.7837426356007071\n",
      "Epoch 62, Training Loss: 0.7845679560829611\n",
      "Epoch 63, Training Loss: 0.7840337270848892\n",
      "Epoch 64, Training Loss: 0.7840483073627248\n",
      "Epoch 65, Training Loss: 0.7842728811853072\n",
      "Epoch 66, Training Loss: 0.7836204206242281\n",
      "Epoch 67, Training Loss: 0.7842684876918793\n",
      "Epoch 68, Training Loss: 0.7839403068318086\n",
      "Epoch 69, Training Loss: 0.7840399904812083\n",
      "Epoch 70, Training Loss: 0.7839001499204075\n",
      "Epoch 71, Training Loss: 0.7836622451333438\n",
      "Epoch 72, Training Loss: 0.7830970184242024\n",
      "Epoch 73, Training Loss: 0.7833423105408164\n",
      "Epoch 74, Training Loss: 0.7831400850239922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:01:29,036] Trial 6 finished with value: 0.6352 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7832052444710451\n",
      "Epoch 1, Training Loss: 0.9187073302448244\n",
      "Epoch 2, Training Loss: 0.8333399237546706\n",
      "Epoch 3, Training Loss: 0.8252241899196366\n",
      "Epoch 4, Training Loss: 0.8190339973994664\n",
      "Epoch 5, Training Loss: 0.8142380251920313\n",
      "Epoch 6, Training Loss: 0.8126901716217959\n",
      "Epoch 7, Training Loss: 0.8108289467661004\n",
      "Epoch 8, Training Loss: 0.8100789607915663\n",
      "Epoch 9, Training Loss: 0.8091871997467557\n",
      "Epoch 10, Training Loss: 0.8079677795108996\n",
      "Epoch 11, Training Loss: 0.8063698650750899\n",
      "Epoch 12, Training Loss: 0.8074353229730649\n",
      "Epoch 13, Training Loss: 0.8063604251782697\n",
      "Epoch 14, Training Loss: 0.8056899266135423\n",
      "Epoch 15, Training Loss: 0.8055447540785137\n",
      "Epoch 16, Training Loss: 0.8046162552403328\n",
      "Epoch 17, Training Loss: 0.804662727682214\n",
      "Epoch 18, Training Loss: 0.8041678849915813\n",
      "Epoch 19, Training Loss: 0.8034148553260287\n",
      "Epoch 20, Training Loss: 0.80258525130444\n",
      "Epoch 21, Training Loss: 0.8023285651565495\n",
      "Epoch 22, Training Loss: 0.8033281558438351\n",
      "Epoch 23, Training Loss: 0.8020207130819335\n",
      "Epoch 24, Training Loss: 0.8017578980080167\n",
      "Epoch 25, Training Loss: 0.8020714662128822\n",
      "Epoch 26, Training Loss: 0.8014768618390076\n",
      "Epoch 27, Training Loss: 0.8020220809413078\n",
      "Epoch 28, Training Loss: 0.8005883984995964\n",
      "Epoch 29, Training Loss: 0.8012488298846366\n",
      "Epoch 30, Training Loss: 0.8001635466303144\n",
      "Epoch 31, Training Loss: 0.8004540596689497\n",
      "Epoch 32, Training Loss: 0.8001478577914991\n",
      "Epoch 33, Training Loss: 0.8000479441836365\n",
      "Epoch 34, Training Loss: 0.7997581768752937\n",
      "Epoch 35, Training Loss: 0.7985573605487221\n",
      "Epoch 36, Training Loss: 0.7987164866655393\n",
      "Epoch 37, Training Loss: 0.7996043213328025\n",
      "Epoch 38, Training Loss: 0.7986832188484364\n",
      "Epoch 39, Training Loss: 0.7981803122319673\n",
      "Epoch 40, Training Loss: 0.7980786005357154\n",
      "Epoch 41, Training Loss: 0.7981726918005405\n",
      "Epoch 42, Training Loss: 0.7981160947254726\n",
      "Epoch 43, Training Loss: 0.7971189637829487\n",
      "Epoch 44, Training Loss: 0.7974164227793987\n",
      "Epoch 45, Training Loss: 0.7981753081307376\n",
      "Epoch 46, Training Loss: 0.7975311401195095\n",
      "Epoch 47, Training Loss: 0.7966487804749854\n",
      "Epoch 48, Training Loss: 0.7966198020411613\n",
      "Epoch 49, Training Loss: 0.7969854045631294\n",
      "Epoch 50, Training Loss: 0.7966551533319\n",
      "Epoch 51, Training Loss: 0.795955927389905\n",
      "Epoch 52, Training Loss: 0.7960805246704503\n",
      "Epoch 53, Training Loss: 0.7956346748466778\n",
      "Epoch 54, Training Loss: 0.7952663842000459\n",
      "Epoch 55, Training Loss: 0.795512528706314\n",
      "Epoch 56, Training Loss: 0.7954011823001661\n",
      "Epoch 57, Training Loss: 0.7956020684170544\n",
      "Epoch 58, Training Loss: 0.7948171211364574\n",
      "Epoch 59, Training Loss: 0.7947324828994006\n",
      "Epoch 60, Training Loss: 0.794193184465394\n",
      "Epoch 61, Training Loss: 0.7942661761341239\n",
      "Epoch 62, Training Loss: 0.7932284252536028\n",
      "Epoch 63, Training Loss: 0.7936337106209949\n",
      "Epoch 64, Training Loss: 0.7940735372385584\n",
      "Epoch 65, Training Loss: 0.7944094985051262\n",
      "Epoch 66, Training Loss: 0.7932315480440183\n",
      "Epoch 67, Training Loss: 0.7932505970610711\n",
      "Epoch 68, Training Loss: 0.7936263435765316\n",
      "Epoch 69, Training Loss: 0.7935214270326428\n",
      "Epoch 70, Training Loss: 0.7928193616687803\n",
      "Epoch 71, Training Loss: 0.7931215746958453\n",
      "Epoch 72, Training Loss: 0.7936743779289991\n",
      "Epoch 73, Training Loss: 0.7937502456786937\n",
      "Epoch 74, Training Loss: 0.792913648239652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:02:41,345] Trial 7 finished with value: 0.625 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.793144740825309\n",
      "Epoch 1, Training Loss: 1.104826642485226\n",
      "Epoch 2, Training Loss: 1.080380716604345\n",
      "Epoch 3, Training Loss: 1.0649471792052774\n",
      "Epoch 4, Training Loss: 1.0508666892612681\n",
      "Epoch 5, Training Loss: 1.0365746500211603\n",
      "Epoch 6, Training Loss: 1.0221865876983194\n",
      "Epoch 7, Training Loss: 1.0085104846253115\n",
      "Epoch 8, Training Loss: 0.9964745567125433\n",
      "Epoch 9, Training Loss: 0.9866450308350956\n",
      "Epoch 10, Training Loss: 0.9790638873156379\n",
      "Epoch 11, Training Loss: 0.9733993237860062\n",
      "Epoch 12, Training Loss: 0.96921723372796\n",
      "Epoch 13, Training Loss: 0.9660868437851177\n",
      "Epoch 14, Training Loss: 0.9637171607858994\n",
      "Epoch 15, Training Loss: 0.9618571050026837\n",
      "Epoch 16, Training Loss: 0.9603512935077443\n",
      "Epoch 17, Training Loss: 0.9590733191546272\n",
      "Epoch 18, Training Loss: 0.9579592021773843\n",
      "Epoch 19, Training Loss: 0.9569294754897847\n",
      "Epoch 20, Training Loss: 0.955979611452888\n",
      "Epoch 21, Training Loss: 0.9550368358808405\n",
      "Epoch 22, Training Loss: 0.9541209171800052\n",
      "Epoch 23, Training Loss: 0.9532042184998007\n",
      "Epoch 24, Training Loss: 0.9522652189170613\n",
      "Epoch 25, Training Loss: 0.9513331909039441\n",
      "Epoch 26, Training Loss: 0.9503568272029652\n",
      "Epoch 27, Training Loss: 0.9493754120434031\n",
      "Epoch 28, Training Loss: 0.9483579771658953\n",
      "Epoch 29, Training Loss: 0.9473091914373286\n",
      "Epoch 30, Training Loss: 0.9462038873223697\n",
      "Epoch 31, Training Loss: 0.9451154700447532\n",
      "Epoch 32, Training Loss: 0.9439501424396739\n",
      "Epoch 33, Training Loss: 0.9427475438398474\n",
      "Epoch 34, Training Loss: 0.9414946171115427\n",
      "Epoch 35, Training Loss: 0.9401873374686521\n",
      "Epoch 36, Training Loss: 0.9388203487676733\n",
      "Epoch 37, Training Loss: 0.9374010510304395\n",
      "Epoch 38, Training Loss: 0.9359093358937431\n",
      "Epoch 39, Training Loss: 0.9343501351160162\n",
      "Epoch 40, Training Loss: 0.9327095491745893\n",
      "Epoch 41, Training Loss: 0.9309879369595472\n",
      "Epoch 42, Training Loss: 0.9291668117747587\n",
      "Epoch 43, Training Loss: 0.9272637337796829\n",
      "Epoch 44, Training Loss: 0.9252470914756551\n",
      "Epoch 45, Training Loss: 0.9231353033991421\n",
      "Epoch 46, Training Loss: 0.9209054305974175\n",
      "Epoch 47, Training Loss: 0.9185323337947621\n",
      "Epoch 48, Training Loss: 0.9160706360901103\n",
      "Epoch 49, Training Loss: 0.9134558406998129\n",
      "Epoch 50, Training Loss: 0.9107195176096523\n",
      "Epoch 51, Training Loss: 0.9078391303735621\n",
      "Epoch 52, Training Loss: 0.9048785016115974\n",
      "Epoch 53, Training Loss: 0.901769027148976\n",
      "Epoch 54, Training Loss: 0.8985491925127366\n",
      "Epoch 55, Training Loss: 0.8952107776613797\n",
      "Epoch 56, Training Loss: 0.8917987448327681\n",
      "Epoch 57, Training Loss: 0.888332867341883\n",
      "Epoch 58, Training Loss: 0.8848383549381705\n",
      "Epoch 59, Training Loss: 0.881310689238941\n",
      "Epoch 60, Training Loss: 0.877800026991788\n",
      "Epoch 61, Training Loss: 0.874355495817521\n",
      "Epoch 62, Training Loss: 0.8709370141169605\n",
      "Epoch 63, Training Loss: 0.8676471838530372\n",
      "Epoch 64, Training Loss: 0.8644738736573387\n",
      "Epoch 65, Training Loss: 0.8614546430110931\n",
      "Epoch 66, Training Loss: 0.858586009460337\n",
      "Epoch 67, Training Loss: 0.8559105028825648\n",
      "Epoch 68, Training Loss: 0.8533854580626768\n",
      "Epoch 69, Training Loss: 0.8510931253433227\n",
      "Epoch 70, Training Loss: 0.8489491499171538\n",
      "Epoch 71, Training Loss: 0.8470070662919212\n",
      "Epoch 72, Training Loss: 0.8452209756654852\n",
      "Epoch 73, Training Loss: 0.8435606940353618\n",
      "Epoch 74, Training Loss: 0.8421005075819352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:04:07,150] Trial 8 finished with value: 0.6096 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8407768461984747\n",
      "Epoch 1, Training Loss: 0.8530972893076731\n",
      "Epoch 2, Training Loss: 0.8352926956083542\n",
      "Epoch 3, Training Loss: 0.822495656802242\n",
      "Epoch 4, Training Loss: 0.8231831123954372\n",
      "Epoch 5, Training Loss: 0.8268208259926703\n",
      "Epoch 6, Training Loss: 0.8261285776482489\n",
      "Epoch 7, Training Loss: 0.8297691270821077\n",
      "Epoch 8, Training Loss: 0.8273386460497864\n",
      "Epoch 9, Training Loss: 0.8323991774616385\n",
      "Epoch 10, Training Loss: 0.8296769013082175\n",
      "Epoch 11, Training Loss: 0.8295123821810673\n",
      "Epoch 12, Training Loss: 0.8327281966245264\n",
      "Epoch 13, Training Loss: 0.8339845712919881\n",
      "Epoch 14, Training Loss: 0.8325187277973146\n",
      "Epoch 15, Training Loss: 0.8343816266920333\n",
      "Epoch 16, Training Loss: 0.8351100771050705\n",
      "Epoch 17, Training Loss: 0.8335201180967173\n",
      "Epoch 18, Training Loss: 0.8375446488982753\n",
      "Epoch 19, Training Loss: 0.833743915790902\n",
      "Epoch 20, Training Loss: 0.8391711370389264\n",
      "Epoch 21, Training Loss: 0.8386286603777032\n",
      "Epoch 22, Training Loss: 0.8384652004206091\n",
      "Epoch 23, Training Loss: 0.8386978374387984\n",
      "Epoch 24, Training Loss: 0.8346316452313186\n",
      "Epoch 25, Training Loss: 0.8395912390902527\n",
      "Epoch 26, Training Loss: 0.8455086194482961\n",
      "Epoch 27, Training Loss: 0.8376082360296321\n",
      "Epoch 28, Training Loss: 0.8379539658252458\n",
      "Epoch 29, Training Loss: 0.8387060174368378\n",
      "Epoch 30, Training Loss: 0.8402754189376545\n",
      "Epoch 31, Training Loss: 0.835930557806689\n",
      "Epoch 32, Training Loss: 0.8375147428727687\n",
      "Epoch 33, Training Loss: 0.8366344409777706\n",
      "Epoch 34, Training Loss: 0.8386502555438451\n",
      "Epoch 35, Training Loss: 0.839675087767436\n",
      "Epoch 36, Training Loss: 0.840371693076944\n",
      "Epoch 37, Training Loss: 0.8391942920541404\n",
      "Epoch 38, Training Loss: 0.8381148622448283\n",
      "Epoch 39, Training Loss: 0.8366901201413091\n",
      "Epoch 40, Training Loss: 0.8439236080736146\n",
      "Epoch 41, Training Loss: 0.8441645555030134\n",
      "Epoch 42, Training Loss: 0.8370825437674845\n",
      "Epoch 43, Training Loss: 0.8380987109098219\n",
      "Epoch 44, Training Loss: 0.8375181360352308\n",
      "Epoch 45, Training Loss: 0.8358148702105185\n",
      "Epoch 46, Training Loss: 0.8364132675013147\n",
      "Epoch 47, Training Loss: 0.8335218626753729\n",
      "Epoch 48, Training Loss: 0.8375468428869893\n",
      "Epoch 49, Training Loss: 0.841636401907842\n",
      "Epoch 50, Training Loss: 0.8428790923348046\n",
      "Epoch 51, Training Loss: 0.8454906073728002\n",
      "Epoch 52, Training Loss: 0.8364037945754546\n",
      "Epoch 53, Training Loss: 0.8375922363503535\n",
      "Epoch 54, Training Loss: 0.8330894549090163\n",
      "Epoch 55, Training Loss: 0.8414182643245037\n",
      "Epoch 56, Training Loss: 0.8499327580731614\n",
      "Epoch 57, Training Loss: 0.8587597828162344\n",
      "Epoch 58, Training Loss: 0.8384364876532017\n",
      "Epoch 59, Training Loss: 0.8416322657040187\n",
      "Epoch 60, Training Loss: 0.8380746822608145\n",
      "Epoch 61, Training Loss: 0.8402414059280453\n",
      "Epoch 62, Training Loss: 0.8482116075386679\n",
      "Epoch 63, Training Loss: 0.8438730105421597\n",
      "Epoch 64, Training Loss: 0.8466545383733018\n",
      "Epoch 65, Training Loss: 0.8461597047354046\n",
      "Epoch 66, Training Loss: 0.8416379492085679\n",
      "Epoch 67, Training Loss: 0.8435669130848763\n",
      "Epoch 68, Training Loss: 0.8414646794921473\n",
      "Epoch 69, Training Loss: 0.8408750170155576\n",
      "Epoch 70, Training Loss: 0.8441187055487381\n",
      "Epoch 71, Training Loss: 0.840634422642844\n",
      "Epoch 72, Training Loss: 0.8402772984110323\n",
      "Epoch 73, Training Loss: 0.8360709655553775\n",
      "Epoch 74, Training Loss: 0.841366906452896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:05:42,167] Trial 9 finished with value: 0.6128666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8428413582923717\n",
      "Epoch 1, Training Loss: 1.0954412665086635\n",
      "Epoch 2, Training Loss: 1.0823180417453542\n",
      "Epoch 3, Training Loss: 1.0731082436617683\n",
      "Epoch 4, Training Loss: 1.0646026720720179\n",
      "Epoch 5, Training Loss: 1.055969704880434\n",
      "Epoch 6, Training Loss: 1.0469598899168127\n",
      "Epoch 7, Training Loss: 1.0375046906751746\n",
      "Epoch 8, Training Loss: 1.027680474659976\n",
      "Epoch 9, Training Loss: 1.017669490435544\n",
      "Epoch 10, Training Loss: 1.0077656356727376\n",
      "Epoch 11, Training Loss: 0.9982565931011649\n",
      "Epoch 12, Training Loss: 0.989420079834321\n",
      "Epoch 13, Training Loss: 0.9814496127296897\n",
      "Epoch 14, Training Loss: 0.9744631113024319\n",
      "Epoch 15, Training Loss: 0.9684633959040923\n",
      "Epoch 16, Training Loss: 0.9633527689821579\n",
      "Epoch 17, Training Loss: 0.9589967527109033\n",
      "Epoch 18, Training Loss: 0.9552482531351202\n",
      "Epoch 19, Training Loss: 0.9520066189064699\n",
      "Epoch 20, Training Loss: 0.9491805007878472\n",
      "Epoch 21, Training Loss: 0.9466604227178237\n",
      "Epoch 22, Training Loss: 0.9443994042452644\n",
      "Epoch 23, Training Loss: 0.942343827345792\n",
      "Epoch 24, Training Loss: 0.9404637249778299\n",
      "Epoch 25, Training Loss: 0.9387142607745003\n",
      "Epoch 26, Training Loss: 0.9370838873526629\n",
      "Epoch 27, Training Loss: 0.9355421914072598\n",
      "Epoch 28, Training Loss: 0.9340792815825518\n",
      "Epoch 29, Training Loss: 0.9326877309995539\n",
      "Epoch 30, Training Loss: 0.9313487754148595\n",
      "Epoch 31, Training Loss: 0.9300570776883293\n",
      "Epoch 32, Training Loss: 0.9287906082237468\n",
      "Epoch 33, Training Loss: 0.9275724701320424\n",
      "Epoch 34, Training Loss: 0.9263646639795864\n",
      "Epoch 35, Training Loss: 0.9251782960751478\n",
      "Epoch 36, Training Loss: 0.9240168646503897\n",
      "Epoch 37, Training Loss: 0.9228571834984948\n",
      "Epoch 38, Training Loss: 0.9217161177186405\n",
      "Epoch 39, Training Loss: 0.9205851146052866\n",
      "Epoch 40, Training Loss: 0.9194542260029737\n",
      "Epoch 41, Training Loss: 0.9183459722294527\n",
      "Epoch 42, Training Loss: 0.9172579272354351\n",
      "Epoch 43, Training Loss: 0.9161793108547435\n",
      "Epoch 44, Training Loss: 0.9151150490255917\n",
      "Epoch 45, Training Loss: 0.9140350415426142\n",
      "Epoch 46, Training Loss: 0.9129850338487064\n",
      "Epoch 47, Training Loss: 0.9119099347731646\n",
      "Epoch 48, Training Loss: 0.9108671520738041\n",
      "Epoch 49, Training Loss: 0.9098096984274248\n",
      "Epoch 50, Training Loss: 0.9087445254185621\n",
      "Epoch 51, Training Loss: 0.9076761437864864\n",
      "Epoch 52, Training Loss: 0.9066103874935824\n",
      "Epoch 53, Training Loss: 0.9055286584180944\n",
      "Epoch 54, Training Loss: 0.9044419353148516\n",
      "Epoch 55, Training Loss: 0.9033429181575775\n",
      "Epoch 56, Training Loss: 0.9022445992862477\n",
      "Epoch 57, Training Loss: 0.9011034418554867\n",
      "Epoch 58, Training Loss: 0.8999891380702748\n",
      "Epoch 59, Training Loss: 0.8988346916787765\n",
      "Epoch 60, Training Loss: 0.8976644657639896\n",
      "Epoch 61, Training Loss: 0.8964525287291583\n",
      "Epoch 62, Training Loss: 0.8952320991544163\n",
      "Epoch 63, Training Loss: 0.8940162596281837\n",
      "Epoch 64, Training Loss: 0.8927353357567507\n",
      "Epoch 65, Training Loss: 0.8914520828162923\n",
      "Epoch 66, Training Loss: 0.8901530086994172\n",
      "Epoch 67, Training Loss: 0.8888140378980076\n",
      "Epoch 68, Training Loss: 0.8874310694722568\n",
      "Epoch 69, Training Loss: 0.8860435851882486\n",
      "Epoch 70, Training Loss: 0.8845990945311154\n",
      "Epoch 71, Training Loss: 0.8831347433258505\n",
      "Epoch 72, Training Loss: 0.8816192417285021\n",
      "Epoch 73, Training Loss: 0.8800825766254874\n",
      "Epoch 74, Training Loss: 0.8785082556920893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:07:00,422] Trial 10 finished with value: 0.5867333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8768888932115891\n",
      "Epoch 1, Training Loss: 1.0913441864171423\n",
      "Epoch 2, Training Loss: 1.0911743171232984\n",
      "Epoch 3, Training Loss: 1.0911125597200895\n",
      "Epoch 4, Training Loss: 1.0910495379813632\n",
      "Epoch 5, Training Loss: 1.0910144409738984\n",
      "Epoch 6, Training Loss: 1.091010778649409\n",
      "Epoch 7, Training Loss: 1.0908400675407925\n",
      "Epoch 8, Training Loss: 1.0908526702034742\n",
      "Epoch 9, Training Loss: 1.0908130294398257\n",
      "Epoch 10, Training Loss: 1.0905802981297772\n",
      "Epoch 11, Training Loss: 1.090697311279469\n",
      "Epoch 12, Training Loss: 1.0904961670251718\n",
      "Epoch 13, Training Loss: 1.090600774162694\n",
      "Epoch 14, Training Loss: 1.090391454840065\n",
      "Epoch 15, Training Loss: 1.090233460225557\n",
      "Epoch 16, Training Loss: 1.0902808365068937\n",
      "Epoch 17, Training Loss: 1.0900955209158416\n",
      "Epoch 18, Training Loss: 1.090047700602309\n",
      "Epoch 19, Training Loss: 1.09000649667324\n",
      "Epoch 20, Training Loss: 1.0898593047507723\n",
      "Epoch 21, Training Loss: 1.08953822591251\n",
      "Epoch 22, Training Loss: 1.0893841169830552\n",
      "Epoch 23, Training Loss: 1.0892829737268892\n",
      "Epoch 24, Training Loss: 1.0889947984451638\n",
      "Epoch 25, Training Loss: 1.088792852710064\n",
      "Epoch 26, Training Loss: 1.0882873159602173\n",
      "Epoch 27, Training Loss: 1.088064397905106\n",
      "Epoch 28, Training Loss: 1.0876330617675207\n",
      "Epoch 29, Training Loss: 1.0872158470010398\n",
      "Epoch 30, Training Loss: 1.086502122520504\n",
      "Epoch 31, Training Loss: 1.085947142507797\n",
      "Epoch 32, Training Loss: 1.085116311661283\n",
      "Epoch 33, Training Loss: 1.083973305117815\n",
      "Epoch 34, Training Loss: 1.0828261153142256\n",
      "Epoch 35, Training Loss: 1.0810447902607738\n",
      "Epoch 36, Training Loss: 1.0789974762981098\n",
      "Epoch 37, Training Loss: 1.0763451527832146\n",
      "Epoch 38, Training Loss: 1.0724237350592936\n",
      "Epoch 39, Training Loss: 1.0673209014691805\n",
      "Epoch 40, Training Loss: 1.0599522508176646\n",
      "Epoch 41, Training Loss: 1.0496212334561168\n",
      "Epoch 42, Training Loss: 1.035352281161717\n",
      "Epoch 43, Training Loss: 1.0177891266973396\n",
      "Epoch 44, Training Loss: 1.0001727191129126\n",
      "Epoch 45, Training Loss: 0.9860574716912176\n",
      "Epoch 46, Training Loss: 0.9774785159225751\n",
      "Epoch 47, Training Loss: 0.9707791957640111\n",
      "Epoch 48, Training Loss: 0.9656995567164026\n",
      "Epoch 49, Training Loss: 0.9623091850065647\n",
      "Epoch 50, Training Loss: 0.958731560599535\n",
      "Epoch 51, Training Loss: 0.9560422554051966\n",
      "Epoch 52, Training Loss: 0.9539480140334682\n",
      "Epoch 53, Training Loss: 0.9521220823875943\n",
      "Epoch 54, Training Loss: 0.9509372667262429\n",
      "Epoch 55, Training Loss: 0.9494780933946595\n",
      "Epoch 56, Training Loss: 0.9475801188246649\n",
      "Epoch 57, Training Loss: 0.94589231982267\n",
      "Epoch 58, Training Loss: 0.9443512354578291\n",
      "Epoch 59, Training Loss: 0.9427016940331997\n",
      "Epoch 60, Training Loss: 0.9409884066510021\n",
      "Epoch 61, Training Loss: 0.9385568754117292\n",
      "Epoch 62, Training Loss: 0.9365593866298073\n",
      "Epoch 63, Training Loss: 0.9343713935156513\n",
      "Epoch 64, Training Loss: 0.9325128997178902\n",
      "Epoch 65, Training Loss: 0.9293847775100765\n",
      "Epoch 66, Training Loss: 0.9267918384164796\n",
      "Epoch 67, Training Loss: 0.9237077270235334\n",
      "Epoch 68, Training Loss: 0.9198220650952561\n",
      "Epoch 69, Training Loss: 0.9156304947415689\n",
      "Epoch 70, Training Loss: 0.9112241537051093\n",
      "Epoch 71, Training Loss: 0.9063388417537948\n",
      "Epoch 72, Training Loss: 0.9013677713566257\n",
      "Epoch 73, Training Loss: 0.8952243988675282\n",
      "Epoch 74, Training Loss: 0.8888772888291151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:08:14,694] Trial 11 finished with value: 0.5757333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8821230217030174\n",
      "Epoch 1, Training Loss: 0.9963370984658263\n",
      "Epoch 2, Training Loss: 0.9502772323170999\n",
      "Epoch 3, Training Loss: 0.9410609470274215\n",
      "Epoch 4, Training Loss: 0.9317529429170422\n",
      "Epoch 5, Training Loss: 0.9208326943834921\n",
      "Epoch 6, Training Loss: 0.9080958246288443\n",
      "Epoch 7, Training Loss: 0.8928530870523668\n",
      "Epoch 8, Training Loss: 0.8757372234996996\n",
      "Epoch 9, Training Loss: 0.8594372527043622\n",
      "Epoch 10, Training Loss: 0.8455883160569614\n",
      "Epoch 11, Training Loss: 0.8359509723527091\n",
      "Epoch 12, Training Loss: 0.829387785079784\n",
      "Epoch 13, Training Loss: 0.8239752406464483\n",
      "Epoch 14, Training Loss: 0.8207679591680828\n",
      "Epoch 15, Training Loss: 0.8182898031141524\n",
      "Epoch 16, Training Loss: 0.8167530121659874\n",
      "Epoch 17, Training Loss: 0.814595171293818\n",
      "Epoch 18, Training Loss: 0.8141136415022656\n",
      "Epoch 19, Training Loss: 0.8129194184353478\n",
      "Epoch 20, Training Loss: 0.8121669460956316\n",
      "Epoch 21, Training Loss: 0.8117820988920398\n",
      "Epoch 22, Training Loss: 0.811331804175126\n",
      "Epoch 23, Training Loss: 0.8105650624834505\n",
      "Epoch 24, Training Loss: 0.8092208211583303\n",
      "Epoch 25, Training Loss: 0.8091303626397499\n",
      "Epoch 26, Training Loss: 0.8091928086782757\n",
      "Epoch 27, Training Loss: 0.8081227772217944\n",
      "Epoch 28, Training Loss: 0.8078870110942009\n",
      "Epoch 29, Training Loss: 0.807188970433142\n",
      "Epoch 30, Training Loss: 0.8073319055084\n",
      "Epoch 31, Training Loss: 0.8065408907438579\n",
      "Epoch 32, Training Loss: 0.8066199687190522\n",
      "Epoch 33, Training Loss: 0.8054769642370984\n",
      "Epoch 34, Training Loss: 0.8051295793146119\n",
      "Epoch 35, Training Loss: 0.8052669671245087\n",
      "Epoch 36, Training Loss: 0.8048813136000382\n",
      "Epoch 37, Training Loss: 0.8038364847799889\n",
      "Epoch 38, Training Loss: 0.8036408264834182\n",
      "Epoch 39, Training Loss: 0.8030801726463146\n",
      "Epoch 40, Training Loss: 0.8035598126569189\n",
      "Epoch 41, Training Loss: 0.8030713376246\n",
      "Epoch 42, Training Loss: 0.8029986806381914\n",
      "Epoch 43, Training Loss: 0.8025451168978125\n",
      "Epoch 44, Training Loss: 0.8017790363247234\n",
      "Epoch 45, Training Loss: 0.8021058047624459\n",
      "Epoch 46, Training Loss: 0.8016367846861818\n",
      "Epoch 47, Training Loss: 0.8011040215205429\n",
      "Epoch 48, Training Loss: 0.8012512065414199\n",
      "Epoch 49, Training Loss: 0.8014149603090788\n",
      "Epoch 50, Training Loss: 0.8005398095998549\n",
      "Epoch 51, Training Loss: 0.8000657124626905\n",
      "Epoch 52, Training Loss: 0.8000230272013442\n",
      "Epoch 53, Training Loss: 0.8005476593074942\n",
      "Epoch 54, Training Loss: 0.7997348597175197\n",
      "Epoch 55, Training Loss: 0.8001251373075902\n",
      "Epoch 56, Training Loss: 0.7999077197304345\n",
      "Epoch 57, Training Loss: 0.7994075070646472\n",
      "Epoch 58, Training Loss: 0.7994565144517368\n",
      "Epoch 59, Training Loss: 0.7991558843089226\n",
      "Epoch 60, Training Loss: 0.7993817080232434\n",
      "Epoch 61, Training Loss: 0.7986744834068126\n",
      "Epoch 62, Training Loss: 0.7987900206917211\n",
      "Epoch 63, Training Loss: 0.7984299846161577\n",
      "Epoch 64, Training Loss: 0.7983091763983992\n",
      "Epoch 65, Training Loss: 0.798914671033845\n",
      "Epoch 66, Training Loss: 0.7982744712578623\n",
      "Epoch 67, Training Loss: 0.7984165894357782\n",
      "Epoch 68, Training Loss: 0.7981850151728866\n",
      "Epoch 69, Training Loss: 0.7984176469924754\n",
      "Epoch 70, Training Loss: 0.7980023550807982\n",
      "Epoch 71, Training Loss: 0.7990892429997151\n",
      "Epoch 72, Training Loss: 0.7979802137030695\n",
      "Epoch 73, Training Loss: 0.7981438943317958\n",
      "Epoch 74, Training Loss: 0.7979728503334791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:09:25,224] Trial 12 finished with value: 0.6348666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.797729490215617\n",
      "Epoch 1, Training Loss: 0.8979378280920141\n",
      "Epoch 2, Training Loss: 0.8236030943253461\n",
      "Epoch 3, Training Loss: 0.81659047554521\n",
      "Epoch 4, Training Loss: 0.8089742484513451\n",
      "Epoch 5, Training Loss: 0.8045957988851211\n",
      "Epoch 6, Training Loss: 0.8008356745804057\n",
      "Epoch 7, Training Loss: 0.7999219571842867\n",
      "Epoch 8, Training Loss: 0.7974643545291004\n",
      "Epoch 9, Training Loss: 0.7964552780459909\n",
      "Epoch 10, Training Loss: 0.7949295766213361\n",
      "Epoch 11, Training Loss: 0.7937259200741263\n",
      "Epoch 12, Training Loss: 0.7933999417108648\n",
      "Epoch 13, Training Loss: 0.7931438978279338\n",
      "Epoch 14, Training Loss: 0.7918508977048537\n",
      "Epoch 15, Training Loss: 0.7917565686562482\n",
      "Epoch 16, Training Loss: 0.7907070016159731\n",
      "Epoch 17, Training Loss: 0.7906033813252169\n",
      "Epoch 18, Training Loss: 0.7902264698112712\n",
      "Epoch 19, Training Loss: 0.7900801958056057\n",
      "Epoch 20, Training Loss: 0.7889561601246105\n",
      "Epoch 21, Training Loss: 0.7892312777042388\n",
      "Epoch 22, Training Loss: 0.78921678592177\n",
      "Epoch 23, Training Loss: 0.7888098662741044\n",
      "Epoch 24, Training Loss: 0.7885285369087668\n",
      "Epoch 25, Training Loss: 0.7887223956865422\n",
      "Epoch 26, Training Loss: 0.7877741094897776\n",
      "Epoch 27, Training Loss: 0.7877446762954488\n",
      "Epoch 28, Training Loss: 0.7868790977141437\n",
      "Epoch 29, Training Loss: 0.7871092162412756\n",
      "Epoch 30, Training Loss: 0.7871597017260159\n",
      "Epoch 31, Training Loss: 0.7870370374006384\n",
      "Epoch 32, Training Loss: 0.7865233344190261\n",
      "Epoch 33, Training Loss: 0.7863846222092123\n",
      "Epoch 34, Training Loss: 0.7863438929529751\n",
      "Epoch 35, Training Loss: 0.7858510447950924\n",
      "Epoch 36, Training Loss: 0.7856132919648114\n",
      "Epoch 37, Training Loss: 0.7854980369876413\n",
      "Epoch 38, Training Loss: 0.7852583660097683\n",
      "Epoch 39, Training Loss: 0.7853854726342594\n",
      "Epoch 40, Training Loss: 0.7852144455208497\n",
      "Epoch 41, Training Loss: 0.7848453287517323\n",
      "Epoch 42, Training Loss: 0.7850599203390234\n",
      "Epoch 43, Training Loss: 0.7845444131598753\n",
      "Epoch 44, Training Loss: 0.784827945022022\n",
      "Epoch 45, Training Loss: 0.7842875710655661\n",
      "Epoch 46, Training Loss: 0.7840518715802361\n",
      "Epoch 47, Training Loss: 0.7842623509378994\n",
      "Epoch 48, Training Loss: 0.7841129162031062\n",
      "Epoch 49, Training Loss: 0.7837315249443054\n",
      "Epoch 50, Training Loss: 0.7833960241429946\n",
      "Epoch 51, Training Loss: 0.7838701052525464\n",
      "Epoch 52, Training Loss: 0.7834796487583834\n",
      "Epoch 53, Training Loss: 0.7829980944885927\n",
      "Epoch 54, Training Loss: 0.783030673195334\n",
      "Epoch 55, Training Loss: 0.7833239460692686\n",
      "Epoch 56, Training Loss: 0.782538992727504\n",
      "Epoch 57, Training Loss: 0.7828842910598306\n",
      "Epoch 58, Training Loss: 0.783172520609463\n",
      "Epoch 59, Training Loss: 0.7822904971066643\n",
      "Epoch 60, Training Loss: 0.7825781663025126\n",
      "Epoch 61, Training Loss: 0.7825174019617193\n",
      "Epoch 62, Training Loss: 0.7822070368598489\n",
      "Epoch 63, Training Loss: 0.7825810019408955\n",
      "Epoch 64, Training Loss: 0.7813905863200917\n",
      "Epoch 65, Training Loss: 0.7817783298211939\n",
      "Epoch 66, Training Loss: 0.7819635824596181\n",
      "Epoch 67, Training Loss: 0.782015739959829\n",
      "Epoch 68, Training Loss: 0.7818342205356149\n",
      "Epoch 69, Training Loss: 0.7816470487678753\n",
      "Epoch 70, Training Loss: 0.7812715994610506\n",
      "Epoch 71, Training Loss: 0.7817443762106054\n",
      "Epoch 72, Training Loss: 0.7815588425187504\n",
      "Epoch 73, Training Loss: 0.7816971815333646\n",
      "Epoch 74, Training Loss: 0.7820230671237497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:11:06,878] Trial 13 finished with value: 0.6357333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7819114988691667\n",
      "Epoch 1, Training Loss: 1.0908852445378023\n",
      "Epoch 2, Training Loss: 1.0873445011587703\n",
      "Epoch 3, Training Loss: 1.0838506739279803\n",
      "Epoch 4, Training Loss: 1.0797414678685806\n",
      "Epoch 5, Training Loss: 1.0745343418682323\n",
      "Epoch 6, Training Loss: 1.067788290276247\n",
      "Epoch 7, Training Loss: 1.0593073659784653\n",
      "Epoch 8, Training Loss: 1.0484508397999932\n",
      "Epoch 9, Training Loss: 1.035416154651081\n",
      "Epoch 10, Training Loss: 1.0208879631407122\n",
      "Epoch 11, Training Loss: 1.0065924072966856\n",
      "Epoch 12, Training Loss: 0.994121946587282\n",
      "Epoch 13, Training Loss: 0.9845158837823307\n",
      "Epoch 14, Training Loss: 0.9775487145956825\n",
      "Epoch 15, Training Loss: 0.9723636247831232\n",
      "Epoch 16, Training Loss: 0.968365434478311\n",
      "Epoch 17, Training Loss: 0.9656146156787873\n",
      "Epoch 18, Training Loss: 0.9632089103670681\n",
      "Epoch 19, Training Loss: 0.9612436614317053\n",
      "Epoch 20, Training Loss: 0.9597570619863622\n",
      "Epoch 21, Training Loss: 0.9584338288447436\n",
      "Epoch 22, Training Loss: 0.9574603184531717\n",
      "Epoch 23, Training Loss: 0.9564705516310299\n",
      "Epoch 24, Training Loss: 0.9557203339829164\n",
      "Epoch 25, Training Loss: 0.9548381109097425\n",
      "Epoch 26, Training Loss: 0.9540418320543625\n",
      "Epoch 27, Training Loss: 0.9532096142628613\n",
      "Epoch 28, Training Loss: 0.9524137978694018\n",
      "Epoch 29, Training Loss: 0.9516728320542504\n",
      "Epoch 30, Training Loss: 0.950804978538962\n",
      "Epoch 31, Training Loss: 0.9499227006295148\n",
      "Epoch 32, Training Loss: 0.9490766626245836\n",
      "Epoch 33, Training Loss: 0.9481113758507896\n",
      "Epoch 34, Training Loss: 0.9472101236792172\n",
      "Epoch 35, Training Loss: 0.9462975116337047\n",
      "Epoch 36, Training Loss: 0.9452114474773406\n",
      "Epoch 37, Training Loss: 0.9441976950449102\n",
      "Epoch 38, Training Loss: 0.9431279980435091\n",
      "Epoch 39, Training Loss: 0.94201921154471\n",
      "Epoch 40, Training Loss: 0.9409225072580225\n",
      "Epoch 41, Training Loss: 0.9396771907806396\n",
      "Epoch 42, Training Loss: 0.9384196317195892\n",
      "Epoch 43, Training Loss: 0.9369738854380215\n",
      "Epoch 44, Training Loss: 0.9356804791618796\n",
      "Epoch 45, Training Loss: 0.93413257690037\n",
      "Epoch 46, Training Loss: 0.932656248106676\n",
      "Epoch 47, Training Loss: 0.9310286140441895\n",
      "Epoch 48, Training Loss: 0.9294380934799419\n",
      "Epoch 49, Training Loss: 0.9276209437145906\n",
      "Epoch 50, Training Loss: 0.925683669202468\n",
      "Epoch 51, Training Loss: 0.9238122447799234\n",
      "Epoch 52, Training Loss: 0.9216537423694835\n",
      "Epoch 53, Training Loss: 0.9194997619180119\n",
      "Epoch 54, Training Loss: 0.917281411114861\n",
      "Epoch 55, Training Loss: 0.9148888877560111\n",
      "Epoch 56, Training Loss: 0.912248210065505\n",
      "Epoch 57, Training Loss: 0.9096666211941663\n",
      "Epoch 58, Training Loss: 0.9069071045342614\n",
      "Epoch 59, Training Loss: 0.9040980395850013\n",
      "Epoch 60, Training Loss: 0.9010538564710056\n",
      "Epoch 61, Training Loss: 0.8978917741074282\n",
      "Epoch 62, Training Loss: 0.8947275150523466\n",
      "Epoch 63, Training Loss: 0.8915770689178916\n",
      "Epoch 64, Training Loss: 0.8882052443307988\n",
      "Epoch 65, Training Loss: 0.88471305734971\n",
      "Epoch 66, Training Loss: 0.8814116923949298\n",
      "Epoch 67, Training Loss: 0.8780094604632434\n",
      "Epoch 68, Training Loss: 0.8745877272241256\n",
      "Epoch 69, Training Loss: 0.8711366011815913\n",
      "Epoch 70, Training Loss: 0.8678758464841282\n",
      "Epoch 71, Training Loss: 0.8645260859236997\n",
      "Epoch 72, Training Loss: 0.8611710022477542\n",
      "Epoch 73, Training Loss: 0.8582984905383166\n",
      "Epoch 74, Training Loss: 0.8552040850414949\n",
      "Epoch 75, Training Loss: 0.8524263795684366\n",
      "Epoch 76, Training Loss: 0.8498020404927871\n",
      "Epoch 77, Training Loss: 0.8473323094844818\n",
      "Epoch 78, Training Loss: 0.844665868212195\n",
      "Epoch 79, Training Loss: 0.8425884564483866\n",
      "Epoch 80, Training Loss: 0.8403812428081737\n",
      "Epoch 81, Training Loss: 0.8385443292645847\n",
      "Epoch 82, Training Loss: 0.8366791805099039\n",
      "Epoch 83, Training Loss: 0.8349813582616694\n",
      "Epoch 84, Training Loss: 0.833334034541074\n",
      "Epoch 85, Training Loss: 0.8320087162887349\n",
      "Epoch 86, Training Loss: 0.8305262588052189\n",
      "Epoch 87, Training Loss: 0.8293175585129682\n",
      "Epoch 88, Training Loss: 0.8280063470672159\n",
      "Epoch 89, Training Loss: 0.8268132946771733\n",
      "Epoch 90, Training Loss: 0.8258428687207839\n",
      "Epoch 91, Training Loss: 0.8249918643867268\n",
      "Epoch 92, Training Loss: 0.8240751274193034\n",
      "Epoch 93, Training Loss: 0.823248733772951\n",
      "Epoch 94, Training Loss: 0.8225061040766098\n",
      "Epoch 95, Training Loss: 0.8216406428813934\n",
      "Epoch 96, Training Loss: 0.8209857626522289\n",
      "Epoch 97, Training Loss: 0.8204756555136512\n",
      "Epoch 98, Training Loss: 0.8198389043527491\n",
      "Epoch 99, Training Loss: 0.8192287987120012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:12:52,876] Trial 14 finished with value: 0.622 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 1 with value: 0.6359333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8186431520826676\n",
      "Epoch 1, Training Loss: 0.8937367458203259\n",
      "Epoch 2, Training Loss: 0.8251174194672528\n",
      "Epoch 3, Training Loss: 0.8182513580602758\n",
      "Epoch 4, Training Loss: 0.8154592842915479\n",
      "Epoch 5, Training Loss: 0.8132885624380672\n",
      "Epoch 6, Training Loss: 0.8111560580309699\n",
      "Epoch 7, Training Loss: 0.8100027075935813\n",
      "Epoch 8, Training Loss: 0.8089520391997169\n",
      "Epoch 9, Training Loss: 0.8084493120277629\n",
      "Epoch 10, Training Loss: 0.8078725997840657\n",
      "Epoch 11, Training Loss: 0.8063098439048318\n",
      "Epoch 12, Training Loss: 0.8060896223432877\n",
      "Epoch 13, Training Loss: 0.8057015989107245\n",
      "Epoch 14, Training Loss: 0.8055586211120381\n",
      "Epoch 15, Training Loss: 0.8046651447520536\n",
      "Epoch 16, Training Loss: 0.8045832900439992\n",
      "Epoch 17, Training Loss: 0.8042266870947445\n",
      "Epoch 18, Training Loss: 0.8038764114940867\n",
      "Epoch 19, Training Loss: 0.8032376025003546\n",
      "Epoch 20, Training Loss: 0.8034835199047538\n",
      "Epoch 21, Training Loss: 0.8035544557431165\n",
      "Epoch 22, Training Loss: 0.8031769152248607\n",
      "Epoch 23, Training Loss: 0.8028263639702516\n",
      "Epoch 24, Training Loss: 0.8028209129501792\n",
      "Epoch 25, Training Loss: 0.8026622436327093\n",
      "Epoch 26, Training Loss: 0.8022513305439668\n",
      "Epoch 27, Training Loss: 0.8017429834954879\n",
      "Epoch 28, Training Loss: 0.8019176556783564\n",
      "Epoch 29, Training Loss: 0.8018834431031171\n",
      "Epoch 30, Training Loss: 0.8015294853378745\n",
      "Epoch 31, Training Loss: 0.8015392352552975\n",
      "Epoch 32, Training Loss: 0.8005816868473502\n",
      "Epoch 33, Training Loss: 0.8012163514249465\n",
      "Epoch 34, Training Loss: 0.8010569787726682\n",
      "Epoch 35, Training Loss: 0.800289534260245\n",
      "Epoch 36, Training Loss: 0.8002590668902677\n",
      "Epoch 37, Training Loss: 0.7998854109820197\n",
      "Epoch 38, Training Loss: 0.8001051743591533\n",
      "Epoch 39, Training Loss: 0.7999735499830807\n",
      "Epoch 40, Training Loss: 0.7996259670397815\n",
      "Epoch 41, Training Loss: 0.799335323712405\n",
      "Epoch 42, Training Loss: 0.7989651771853952\n",
      "Epoch 43, Training Loss: 0.798859533702626\n",
      "Epoch 44, Training Loss: 0.7988579594387728\n",
      "Epoch 45, Training Loss: 0.7988223595478955\n",
      "Epoch 46, Training Loss: 0.7988727437047397\n",
      "Epoch 47, Training Loss: 0.7982770528512843\n",
      "Epoch 48, Training Loss: 0.7983424390063566\n",
      "Epoch 49, Training Loss: 0.7980882165712468\n",
      "Epoch 50, Training Loss: 0.797773007364834\n",
      "Epoch 51, Training Loss: 0.797415187849718\n",
      "Epoch 52, Training Loss: 0.7976479639726527\n",
      "Epoch 53, Training Loss: 0.7973975720125086\n",
      "Epoch 54, Training Loss: 0.797421375653323\n",
      "Epoch 55, Training Loss: 0.7969470744974473\n",
      "Epoch 56, Training Loss: 0.7971927174399881\n",
      "Epoch 57, Training Loss: 0.7967658458737766\n",
      "Epoch 58, Training Loss: 0.7970423995747286\n",
      "Epoch 59, Training Loss: 0.7965908902532914\n",
      "Epoch 60, Training Loss: 0.7963623250232024\n",
      "Epoch 61, Training Loss: 0.7962168109416962\n",
      "Epoch 62, Training Loss: 0.7961007005326888\n",
      "Epoch 63, Training Loss: 0.7961243316005258\n",
      "Epoch 64, Training Loss: 0.796241162033642\n",
      "Epoch 65, Training Loss: 0.7959476318780113\n",
      "Epoch 66, Training Loss: 0.7955303582724403\n",
      "Epoch 67, Training Loss: 0.7954033946990967\n",
      "Epoch 68, Training Loss: 0.794914216223885\n",
      "Epoch 69, Training Loss: 0.7951018133584191\n",
      "Epoch 70, Training Loss: 0.7948206424713135\n",
      "Epoch 71, Training Loss: 0.7951569592251497\n",
      "Epoch 72, Training Loss: 0.7947291632259593\n",
      "Epoch 73, Training Loss: 0.794675077901167\n",
      "Epoch 74, Training Loss: 0.7944839915107278\n",
      "Epoch 75, Training Loss: 0.7942182687450857\n",
      "Epoch 76, Training Loss: 0.7942733281500199\n",
      "Epoch 77, Training Loss: 0.7945150265272926\n",
      "Epoch 78, Training Loss: 0.7944626493313733\n",
      "Epoch 79, Training Loss: 0.7943494478394003\n",
      "Epoch 80, Training Loss: 0.7940096441437217\n",
      "Epoch 81, Training Loss: 0.793700192675871\n",
      "Epoch 82, Training Loss: 0.7939006060011247\n",
      "Epoch 83, Training Loss: 0.7939021339136011\n",
      "Epoch 84, Training Loss: 0.7938814291533302\n",
      "Epoch 85, Training Loss: 0.793367522464079\n",
      "Epoch 86, Training Loss: 0.793512260983972\n",
      "Epoch 87, Training Loss: 0.793491789523293\n",
      "Epoch 88, Training Loss: 0.793519463819616\n",
      "Epoch 89, Training Loss: 0.7935675522159128\n",
      "Epoch 90, Training Loss: 0.7933152965237112\n",
      "Epoch 91, Training Loss: 0.7932245403878829\n",
      "Epoch 92, Training Loss: 0.7930556163367103\n",
      "Epoch 93, Training Loss: 0.7929122546139885\n",
      "Epoch 94, Training Loss: 0.7927660691738129\n",
      "Epoch 95, Training Loss: 0.7926854800476747\n",
      "Epoch 96, Training Loss: 0.7928888004667619\n",
      "Epoch 97, Training Loss: 0.792713076577467\n",
      "Epoch 98, Training Loss: 0.7928359121434829\n",
      "Epoch 99, Training Loss: 0.7927985318268047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:14:43,065] Trial 15 finished with value: 0.6382 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.792736442790312\n",
      "Epoch 1, Training Loss: 1.0080608437341803\n",
      "Epoch 2, Training Loss: 0.9300204603812274\n",
      "Epoch 3, Training Loss: 0.8904402300189523\n",
      "Epoch 4, Training Loss: 0.8288710086485919\n",
      "Epoch 5, Training Loss: 0.813358710232903\n",
      "Epoch 6, Training Loss: 0.8113130204817828\n",
      "Epoch 7, Training Loss: 0.809894154141931\n",
      "Epoch 8, Training Loss: 0.8086804447454564\n",
      "Epoch 9, Training Loss: 0.8072158158526701\n",
      "Epoch 10, Training Loss: 0.8065881188476787\n",
      "Epoch 11, Training Loss: 0.8054130581547232\n",
      "Epoch 12, Training Loss: 0.8047074283571805\n",
      "Epoch 13, Training Loss: 0.8045059063154109\n",
      "Epoch 14, Training Loss: 0.803103234277052\n",
      "Epoch 15, Training Loss: 0.8022871959910673\n",
      "Epoch 16, Training Loss: 0.8022303719380323\n",
      "Epoch 17, Training Loss: 0.8018211922926062\n",
      "Epoch 18, Training Loss: 0.8018049713443307\n",
      "Epoch 19, Training Loss: 0.801310608106501\n",
      "Epoch 20, Training Loss: 0.800772761667476\n",
      "Epoch 21, Training Loss: 0.8005545739566579\n",
      "Epoch 22, Training Loss: 0.7997009073986726\n",
      "Epoch 23, Training Loss: 0.7999919511290158\n",
      "Epoch 24, Training Loss: 0.799809000562219\n",
      "Epoch 25, Training Loss: 0.799318721855388\n",
      "Epoch 26, Training Loss: 0.7989580731532153\n",
      "Epoch 27, Training Loss: 0.799177282557768\n",
      "Epoch 28, Training Loss: 0.7989406462977914\n",
      "Epoch 29, Training Loss: 0.7983511224915\n",
      "Epoch 30, Training Loss: 0.798920348812552\n",
      "Epoch 31, Training Loss: 0.7978082942962647\n",
      "Epoch 32, Training Loss: 0.797982133697061\n",
      "Epoch 33, Training Loss: 0.7978202787567588\n",
      "Epoch 34, Training Loss: 0.7977422468101277\n",
      "Epoch 35, Training Loss: 0.7978049954246073\n",
      "Epoch 36, Training Loss: 0.7976132427243625\n",
      "Epoch 37, Training Loss: 0.7975896340959212\n",
      "Epoch 38, Training Loss: 0.7967164812368505\n",
      "Epoch 39, Training Loss: 0.7967490817518795\n",
      "Epoch 40, Training Loss: 0.7961948900363025\n",
      "Epoch 41, Training Loss: 0.7962508550812216\n",
      "Epoch 42, Training Loss: 0.7960392524214351\n",
      "Epoch 43, Training Loss: 0.7956954709922566\n",
      "Epoch 44, Training Loss: 0.7958523656340206\n",
      "Epoch 45, Training Loss: 0.7958162345605738\n",
      "Epoch 46, Training Loss: 0.7957902661491842\n",
      "Epoch 47, Training Loss: 0.795303111076355\n",
      "Epoch 48, Training Loss: 0.7953716987020829\n",
      "Epoch 49, Training Loss: 0.794862742283765\n",
      "Epoch 50, Training Loss: 0.7948083498197444\n",
      "Epoch 51, Training Loss: 0.7945730799085954\n",
      "Epoch 52, Training Loss: 0.7949710953936857\n",
      "Epoch 53, Training Loss: 0.7947288792273578\n",
      "Epoch 54, Training Loss: 0.7946686213156756\n",
      "Epoch 55, Training Loss: 0.7941247983539805\n",
      "Epoch 56, Training Loss: 0.7943115660022286\n",
      "Epoch 57, Training Loss: 0.7936191935399\n",
      "Epoch 58, Training Loss: 0.7940756066406475\n",
      "Epoch 59, Training Loss: 0.7930598517025218\n",
      "Epoch 60, Training Loss: 0.7927992454697104\n",
      "Epoch 61, Training Loss: 0.7923463997420143\n",
      "Epoch 62, Training Loss: 0.7924519398633172\n",
      "Epoch 63, Training Loss: 0.7917179639199201\n",
      "Epoch 64, Training Loss: 0.7913378047241885\n",
      "Epoch 65, Training Loss: 0.791732988988652\n",
      "Epoch 66, Training Loss: 0.7910552061305327\n",
      "Epoch 67, Training Loss: 0.790379719804315\n",
      "Epoch 68, Training Loss: 0.7895779759042403\n",
      "Epoch 69, Training Loss: 0.7892301875703475\n",
      "Epoch 70, Training Loss: 0.7892133601974038\n",
      "Epoch 71, Training Loss: 0.7887966868456672\n",
      "Epoch 72, Training Loss: 0.7884225147611955\n",
      "Epoch 73, Training Loss: 0.7879914600007675\n",
      "Epoch 74, Training Loss: 0.7878407373147852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:16:32,443] Trial 16 finished with value: 0.6364666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7874928416224087\n",
      "Epoch 1, Training Loss: 0.86289586160416\n",
      "Epoch 2, Training Loss: 0.8164303312624307\n",
      "Epoch 3, Training Loss: 0.8099589303920144\n",
      "Epoch 4, Training Loss: 0.8075522766077429\n",
      "Epoch 5, Training Loss: 0.8047793164289088\n",
      "Epoch 6, Training Loss: 0.8036652066653832\n",
      "Epoch 7, Training Loss: 0.8012732450227092\n",
      "Epoch 8, Training Loss: 0.8000644924049091\n",
      "Epoch 9, Training Loss: 0.8002742404328254\n",
      "Epoch 10, Training Loss: 0.7988035644803728\n",
      "Epoch 11, Training Loss: 0.7982856132034072\n",
      "Epoch 12, Training Loss: 0.7973250688466811\n",
      "Epoch 13, Training Loss: 0.7963051993147772\n",
      "Epoch 14, Training Loss: 0.7975117877013701\n",
      "Epoch 15, Training Loss: 0.7961156743809693\n",
      "Epoch 16, Training Loss: 0.7948348533838315\n",
      "Epoch 17, Training Loss: 0.7937948742755374\n",
      "Epoch 18, Training Loss: 0.7941152614758427\n",
      "Epoch 19, Training Loss: 0.7950316071510315\n",
      "Epoch 20, Training Loss: 0.7936871260628664\n",
      "Epoch 21, Training Loss: 0.7947081670725256\n",
      "Epoch 22, Training Loss: 0.7929943074857382\n",
      "Epoch 23, Training Loss: 0.7942940735279169\n",
      "Epoch 24, Training Loss: 0.794724160298369\n",
      "Epoch 25, Training Loss: 0.7935496193125732\n",
      "Epoch 26, Training Loss: 0.7933764466665741\n",
      "Epoch 27, Training Loss: 0.7941021075822357\n",
      "Epoch 28, Training Loss: 0.7928881753656201\n",
      "Epoch 29, Training Loss: 0.7929030641577297\n",
      "Epoch 30, Training Loss: 0.7936767535998409\n",
      "Epoch 31, Training Loss: 0.7923447076539348\n",
      "Epoch 32, Training Loss: 0.7920674235300911\n",
      "Epoch 33, Training Loss: 0.7913667128050238\n",
      "Epoch 34, Training Loss: 0.7932229181877652\n",
      "Epoch 35, Training Loss: 0.7921492089902548\n",
      "Epoch 36, Training Loss: 0.7930547820894341\n",
      "Epoch 37, Training Loss: 0.7917089222965384\n",
      "Epoch 38, Training Loss: 0.7924341403452078\n",
      "Epoch 39, Training Loss: 0.7923229496281846\n",
      "Epoch 40, Training Loss: 0.7908005424908229\n",
      "Epoch 41, Training Loss: 0.792272723079624\n",
      "Epoch 42, Training Loss: 0.7920150873356295\n",
      "Epoch 43, Training Loss: 0.7916430300339721\n",
      "Epoch 44, Training Loss: 0.7916246008155937\n",
      "Epoch 45, Training Loss: 0.7918889470566485\n",
      "Epoch 46, Training Loss: 0.7913542586161678\n",
      "Epoch 47, Training Loss: 0.7921551939240076\n",
      "Epoch 48, Training Loss: 0.7915059898132668\n",
      "Epoch 49, Training Loss: 0.7909698310651277\n",
      "Epoch 50, Training Loss: 0.7914733219863777\n",
      "Epoch 51, Training Loss: 0.7914271819860416\n",
      "Epoch 52, Training Loss: 0.7903107897679609\n",
      "Epoch 53, Training Loss: 0.7909055653371309\n",
      "Epoch 54, Training Loss: 0.7900905256880854\n",
      "Epoch 55, Training Loss: 0.7923349319544054\n",
      "Epoch 56, Training Loss: 0.7904364858354841\n",
      "Epoch 57, Training Loss: 0.7913846952574594\n",
      "Epoch 58, Training Loss: 0.7908329096055569\n",
      "Epoch 59, Training Loss: 0.7903826170397881\n",
      "Epoch 60, Training Loss: 0.7908607157549463\n",
      "Epoch 61, Training Loss: 0.7902644907621513\n",
      "Epoch 62, Training Loss: 0.7909282669088894\n",
      "Epoch 63, Training Loss: 0.7901243471561518\n",
      "Epoch 64, Training Loss: 0.7904445864204177\n",
      "Epoch 65, Training Loss: 0.7907218698272132\n",
      "Epoch 66, Training Loss: 0.7905921166104481\n",
      "Epoch 67, Training Loss: 0.7905884905865318\n",
      "Epoch 68, Training Loss: 0.7907513328960963\n",
      "Epoch 69, Training Loss: 0.7902433577336763\n",
      "Epoch 70, Training Loss: 0.7909511038235255\n",
      "Epoch 71, Training Loss: 0.78977764809042\n",
      "Epoch 72, Training Loss: 0.7903598722658659\n",
      "Epoch 73, Training Loss: 0.790353698838026\n",
      "Epoch 74, Training Loss: 0.7907781687894262\n",
      "Epoch 75, Training Loss: 0.7897634933765669\n",
      "Epoch 76, Training Loss: 0.7895173862464446\n",
      "Epoch 77, Training Loss: 0.790367324997608\n",
      "Epoch 78, Training Loss: 0.7897848992419422\n",
      "Epoch 79, Training Loss: 0.7906960706065472\n",
      "Epoch 80, Training Loss: 0.7893189775316338\n",
      "Epoch 81, Training Loss: 0.7900432536476537\n",
      "Epoch 82, Training Loss: 0.790138754629551\n",
      "Epoch 83, Training Loss: 0.7900734669283817\n",
      "Epoch 84, Training Loss: 0.7898395931810365\n",
      "Epoch 85, Training Loss: 0.7903651289473799\n",
      "Epoch 86, Training Loss: 0.7896758607455663\n",
      "Epoch 87, Training Loss: 0.790552908854377\n",
      "Epoch 88, Training Loss: 0.7902408720855426\n",
      "Epoch 89, Training Loss: 0.7904214696776598\n",
      "Epoch 90, Training Loss: 0.7896591535188202\n",
      "Epoch 91, Training Loss: 0.7904648449187889\n",
      "Epoch 92, Training Loss: 0.7890178281113618\n",
      "Epoch 93, Training Loss: 0.7894866282778575\n",
      "Epoch 94, Training Loss: 0.7890666718769791\n",
      "Epoch 95, Training Loss: 0.7896788775472713\n",
      "Epoch 96, Training Loss: 0.7900759619877751\n",
      "Epoch 97, Training Loss: 0.7891233214758393\n",
      "Epoch 98, Training Loss: 0.7893294324552206\n",
      "Epoch 99, Training Loss: 0.7890459676434223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:18:34,888] Trial 17 finished with value: 0.6262666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7894921326995792\n",
      "Epoch 1, Training Loss: 1.035356332753834\n",
      "Epoch 2, Training Loss: 0.967357263349949\n",
      "Epoch 3, Training Loss: 0.9538981739739726\n",
      "Epoch 4, Training Loss: 0.9438483856674423\n",
      "Epoch 5, Training Loss: 0.9318290741820084\n",
      "Epoch 6, Training Loss: 0.9155218562685458\n",
      "Epoch 7, Training Loss: 0.8922137092826958\n",
      "Epoch 8, Training Loss: 0.8635029471906505\n",
      "Epoch 9, Training Loss: 0.8404298590538197\n",
      "Epoch 10, Training Loss: 0.8264219603144136\n",
      "Epoch 11, Training Loss: 0.8201450027917561\n",
      "Epoch 12, Training Loss: 0.8172247882176162\n",
      "Epoch 13, Training Loss: 0.8149443930253051\n",
      "Epoch 14, Training Loss: 0.8129374674388341\n",
      "Epoch 15, Training Loss: 0.8117526452344163\n",
      "Epoch 16, Training Loss: 0.8113327881447354\n",
      "Epoch 17, Training Loss: 0.8101178824453426\n",
      "Epoch 18, Training Loss: 0.8096342586933222\n",
      "Epoch 19, Training Loss: 0.8084153758852105\n",
      "Epoch 20, Training Loss: 0.8086782453651715\n",
      "Epoch 21, Training Loss: 0.8070351325479665\n",
      "Epoch 22, Training Loss: 0.8066303352664288\n",
      "Epoch 23, Training Loss: 0.8060958276117655\n",
      "Epoch 24, Training Loss: 0.805948473457107\n",
      "Epoch 25, Training Loss: 0.8052145283025011\n",
      "Epoch 26, Training Loss: 0.8045611450546666\n",
      "Epoch 27, Training Loss: 0.8037025600895846\n",
      "Epoch 28, Training Loss: 0.8042902665926998\n",
      "Epoch 29, Training Loss: 0.8048136331981286\n",
      "Epoch 30, Training Loss: 0.8033508613593596\n",
      "Epoch 31, Training Loss: 0.8023900984821463\n",
      "Epoch 32, Training Loss: 0.8028969468927025\n",
      "Epoch 33, Training Loss: 0.8024559764037454\n",
      "Epoch 34, Training Loss: 0.8025082054891084\n",
      "Epoch 35, Training Loss: 0.8011983859807925\n",
      "Epoch 36, Training Loss: 0.8013686156810674\n",
      "Epoch 37, Training Loss: 0.8018460072969136\n",
      "Epoch 38, Training Loss: 0.8006671425095178\n",
      "Epoch 39, Training Loss: 0.8007335743509737\n",
      "Epoch 40, Training Loss: 0.8004828136666376\n",
      "Epoch 41, Training Loss: 0.8004671631899095\n",
      "Epoch 42, Training Loss: 0.8008077793551567\n",
      "Epoch 43, Training Loss: 0.8005273243538419\n",
      "Epoch 44, Training Loss: 0.7997505097460926\n",
      "Epoch 45, Training Loss: 0.8000084792760979\n",
      "Epoch 46, Training Loss: 0.7989881059280912\n",
      "Epoch 47, Training Loss: 0.7994504579924103\n",
      "Epoch 48, Training Loss: 0.7994240150415808\n",
      "Epoch 49, Training Loss: 0.7993773603797856\n",
      "Epoch 50, Training Loss: 0.7989896711550261\n",
      "Epoch 51, Training Loss: 0.799157172486298\n",
      "Epoch 52, Training Loss: 0.7984828187110729\n",
      "Epoch 53, Training Loss: 0.7985467020730327\n",
      "Epoch 54, Training Loss: 0.7982595941177885\n",
      "Epoch 55, Training Loss: 0.7985422520709217\n",
      "Epoch 56, Training Loss: 0.7982996389381868\n",
      "Epoch 57, Training Loss: 0.7978904327951876\n",
      "Epoch 58, Training Loss: 0.7983035477480493\n",
      "Epoch 59, Training Loss: 0.7972520575487524\n",
      "Epoch 60, Training Loss: 0.7981140004064804\n",
      "Epoch 61, Training Loss: 0.7975786760337371\n",
      "Epoch 62, Training Loss: 0.7977430200218258\n",
      "Epoch 63, Training Loss: 0.7979258092722499\n",
      "Epoch 64, Training Loss: 0.7970766683270161\n",
      "Epoch 65, Training Loss: 0.7966604723069901\n",
      "Epoch 66, Training Loss: 0.7968813871082506\n",
      "Epoch 67, Training Loss: 0.7970008405527674\n",
      "Epoch 68, Training Loss: 0.7970330118236685\n",
      "Epoch 69, Training Loss: 0.7966140203906181\n",
      "Epoch 70, Training Loss: 0.797166715707994\n",
      "Epoch 71, Training Loss: 0.7967695628790031\n",
      "Epoch 72, Training Loss: 0.796980355108591\n",
      "Epoch 73, Training Loss: 0.796655837754558\n",
      "Epoch 74, Training Loss: 0.7961738436741936\n",
      "Epoch 75, Training Loss: 0.7961469583941582\n",
      "Epoch 76, Training Loss: 0.7961880248292048\n",
      "Epoch 77, Training Loss: 0.7961716015536086\n",
      "Epoch 78, Training Loss: 0.7961313889438945\n",
      "Epoch 79, Training Loss: 0.7961996722042113\n",
      "Epoch 80, Training Loss: 0.7958787488758116\n",
      "Epoch 81, Training Loss: 0.7960281277061405\n",
      "Epoch 82, Training Loss: 0.7947968067083143\n",
      "Epoch 83, Training Loss: 0.7952336400075066\n",
      "Epoch 84, Training Loss: 0.7951878088757508\n",
      "Epoch 85, Training Loss: 0.7954841439885304\n",
      "Epoch 86, Training Loss: 0.7951634244811266\n",
      "Epoch 87, Training Loss: 0.7948235091410185\n",
      "Epoch 88, Training Loss: 0.7943036615400386\n",
      "Epoch 89, Training Loss: 0.7946254933687081\n",
      "Epoch 90, Training Loss: 0.7945095310533853\n",
      "Epoch 91, Training Loss: 0.7944981308808005\n",
      "Epoch 92, Training Loss: 0.7947483227665263\n",
      "Epoch 93, Training Loss: 0.7938612240149563\n",
      "Epoch 94, Training Loss: 0.7940455179465444\n",
      "Epoch 95, Training Loss: 0.7939319373969745\n",
      "Epoch 96, Training Loss: 0.7940399230870986\n",
      "Epoch 97, Training Loss: 0.7944344741957529\n",
      "Epoch 98, Training Loss: 0.7934506410046628\n",
      "Epoch 99, Training Loss: 0.7932083203380269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:20:14,059] Trial 18 finished with value: 0.6283333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7930538652534772\n",
      "Epoch 1, Training Loss: 1.101733944416046\n",
      "Epoch 2, Training Loss: 1.0909638096304501\n",
      "Epoch 3, Training Loss: 1.090939277340384\n",
      "Epoch 4, Training Loss: 1.0909201484567979\n",
      "Epoch 5, Training Loss: 1.0909059085565456\n",
      "Epoch 6, Training Loss: 1.0909149109615999\n",
      "Epoch 7, Training Loss: 1.0909126152711757\n",
      "Epoch 8, Training Loss: 1.0909021205060623\n",
      "Epoch 9, Training Loss: 1.0909047867270076\n",
      "Epoch 10, Training Loss: 1.0908938103563646\n",
      "Epoch 11, Training Loss: 1.0908758599617903\n",
      "Epoch 12, Training Loss: 1.0908819850753335\n",
      "Epoch 13, Training Loss: 1.0908720792041104\n",
      "Epoch 14, Training Loss: 1.0908709280631121\n",
      "Epoch 15, Training Loss: 1.0908609429527731\n",
      "Epoch 16, Training Loss: 1.0908414095990797\n",
      "Epoch 17, Training Loss: 1.0908561773861156\n",
      "Epoch 18, Training Loss: 1.090838782787323\n",
      "Epoch 19, Training Loss: 1.0908122660132016\n",
      "Epoch 20, Training Loss: 1.090833815125858\n",
      "Epoch 21, Training Loss: 1.0908271943821626\n",
      "Epoch 22, Training Loss: 1.0908128755232867\n",
      "Epoch 23, Training Loss: 1.0908112113616046\n",
      "Epoch 24, Training Loss: 1.0907989900252397\n",
      "Epoch 25, Training Loss: 1.0907981498101178\n",
      "Epoch 26, Training Loss: 1.0907942341355716\n",
      "Epoch 27, Training Loss: 1.0907844689313104\n",
      "Epoch 28, Training Loss: 1.0907823118041544\n",
      "Epoch 29, Training Loss: 1.090777175286237\n",
      "Epoch 30, Training Loss: 1.090771524625666\n",
      "Epoch 31, Training Loss: 1.0907627342729007\n",
      "Epoch 32, Training Loss: 1.0907558659946217\n",
      "Epoch 33, Training Loss: 1.0907469384810504\n",
      "Epoch 34, Training Loss: 1.0907478378800786\n",
      "Epoch 35, Training Loss: 1.0907289892084457\n",
      "Epoch 36, Training Loss: 1.0907317888035495\n",
      "Epoch 37, Training Loss: 1.0907259179564084\n",
      "Epoch 38, Training Loss: 1.0907158745036405\n",
      "Epoch 39, Training Loss: 1.0907039253851947\n",
      "Epoch 40, Training Loss: 1.090693993287928\n",
      "Epoch 41, Training Loss: 1.0907049860673792\n",
      "Epoch 42, Training Loss: 1.09069379455903\n",
      "Epoch 43, Training Loss: 1.0906896769299226\n",
      "Epoch 44, Training Loss: 1.0906718955320471\n",
      "Epoch 45, Training Loss: 1.0906776987805087\n",
      "Epoch 46, Training Loss: 1.0906616938815397\n",
      "Epoch 47, Training Loss: 1.0906557393074037\n",
      "Epoch 48, Training Loss: 1.0906458269848542\n",
      "Epoch 49, Training Loss: 1.0906415591520422\n",
      "Epoch 50, Training Loss: 1.0906388073809006\n",
      "Epoch 51, Training Loss: 1.0906377719430362\n",
      "Epoch 52, Training Loss: 1.0906272832085104\n",
      "Epoch 53, Training Loss: 1.0906120656518374\n",
      "Epoch 54, Training Loss: 1.0906048506848953\n",
      "Epoch 55, Training Loss: 1.0906147990507238\n",
      "Epoch 56, Training Loss: 1.0905940592990202\n",
      "Epoch 57, Training Loss: 1.0906008661494535\n",
      "Epoch 58, Training Loss: 1.0905842636613285\n",
      "Epoch 59, Training Loss: 1.0905883918089025\n",
      "Epoch 60, Training Loss: 1.0905663167729098\n",
      "Epoch 61, Training Loss: 1.0905668391900905\n",
      "Epoch 62, Training Loss: 1.090541306663962\n",
      "Epoch 63, Training Loss: 1.0905657104884876\n",
      "Epoch 64, Training Loss: 1.0905311149709365\n",
      "Epoch 65, Training Loss: 1.0905491154334124\n",
      "Epoch 66, Training Loss: 1.0905314431470983\n",
      "Epoch 67, Training Loss: 1.09053423390669\n",
      "Epoch 68, Training Loss: 1.0905202568278594\n",
      "Epoch 69, Training Loss: 1.0905234981985654\n",
      "Epoch 70, Training Loss: 1.0905057606977575\n",
      "Epoch 71, Training Loss: 1.0905023557999556\n",
      "Epoch 72, Training Loss: 1.0904985893473906\n",
      "Epoch 73, Training Loss: 1.0904845314867355\n",
      "Epoch 74, Training Loss: 1.09047601629706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:21:39,381] Trial 19 finished with value: 0.3558 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 1.0904775120230281\n",
      "Epoch 1, Training Loss: 1.1048629154878504\n",
      "Epoch 2, Training Loss: 1.0934301684884464\n",
      "Epoch 3, Training Loss: 1.0869385926863726\n",
      "Epoch 4, Training Loss: 1.0823987416660084\n",
      "Epoch 5, Training Loss: 1.0785351294629715\n",
      "Epoch 6, Training Loss: 1.0747753667831421\n",
      "Epoch 7, Training Loss: 1.0708315336003023\n",
      "Epoch 8, Training Loss: 1.0664964321080377\n",
      "Epoch 9, Training Loss: 1.0616406632872188\n",
      "Epoch 10, Training Loss: 1.0562426114082337\n",
      "Epoch 11, Training Loss: 1.050270650807549\n",
      "Epoch 12, Training Loss: 1.0437138993599835\n",
      "Epoch 13, Training Loss: 1.03656708149349\n",
      "Epoch 14, Training Loss: 1.0288436026432934\n",
      "Epoch 15, Training Loss: 1.020568971633911\n",
      "Epoch 16, Training Loss: 1.0117785955176635\n",
      "Epoch 17, Training Loss: 1.00255677300341\n",
      "Epoch 18, Training Loss: 0.9930768350292655\n",
      "Epoch 19, Training Loss: 0.9836461782455445\n",
      "Epoch 20, Training Loss: 0.974667259945589\n",
      "Epoch 21, Training Loss: 0.9665300940064823\n",
      "Epoch 22, Training Loss: 0.9595401873308069\n",
      "Epoch 23, Training Loss: 0.9538449215187746\n",
      "Epoch 24, Training Loss: 0.9493644008215736\n",
      "Epoch 25, Training Loss: 0.945922055314569\n",
      "Epoch 26, Training Loss: 0.9432460892901701\n",
      "Epoch 27, Training Loss: 0.941117597467759\n",
      "Epoch 28, Training Loss: 0.9393647434430964\n",
      "Epoch 29, Training Loss: 0.9378525654007407\n",
      "Epoch 30, Training Loss: 0.9364836735585157\n",
      "Epoch 31, Training Loss: 0.9352291883440579\n",
      "Epoch 32, Training Loss: 0.9340320650970234\n",
      "Epoch 33, Training Loss: 0.9328443228497225\n",
      "Epoch 34, Training Loss: 0.9317404988934012\n",
      "Epoch 35, Training Loss: 0.9306080374296973\n",
      "Epoch 36, Training Loss: 0.9294996365378885\n",
      "Epoch 37, Training Loss: 0.9283854622700635\n",
      "Epoch 38, Training Loss: 0.9272844081766465\n",
      "Epoch 39, Training Loss: 0.9261725446757149\n",
      "Epoch 40, Training Loss: 0.925059634587344\n",
      "Epoch 41, Training Loss: 0.9239325378922855\n",
      "Epoch 42, Training Loss: 0.9228045960734872\n",
      "Epoch 43, Training Loss: 0.9216644767452689\n",
      "Epoch 44, Training Loss: 0.9205191172571743\n",
      "Epoch 45, Training Loss: 0.9193573733638315\n",
      "Epoch 46, Training Loss: 0.9181833921460545\n",
      "Epoch 47, Training Loss: 0.9169758911693797\n",
      "Epoch 48, Training Loss: 0.9157897015880135\n",
      "Epoch 49, Training Loss: 0.9145353867727167\n",
      "Epoch 50, Training Loss: 0.9132765013330123\n",
      "Epoch 51, Training Loss: 0.9120128524303436\n",
      "Epoch 52, Training Loss: 0.9107115460844601\n",
      "Epoch 53, Training Loss: 0.9093661817382364\n",
      "Epoch 54, Training Loss: 0.9080013076698079\n",
      "Epoch 55, Training Loss: 0.9065766946007223\n",
      "Epoch 56, Training Loss: 0.9051064043185291\n",
      "Epoch 57, Training Loss: 0.9036172044277191\n",
      "Epoch 58, Training Loss: 0.9020636754176196\n",
      "Epoch 59, Training Loss: 0.9004600846066194\n",
      "Epoch 60, Training Loss: 0.8987805329350864\n",
      "Epoch 61, Training Loss: 0.8970473969683928\n",
      "Epoch 62, Training Loss: 0.8951969370421241\n",
      "Epoch 63, Training Loss: 0.8933171082945431\n",
      "Epoch 64, Training Loss: 0.8913224084938274\n",
      "Epoch 65, Training Loss: 0.8892450142607969\n",
      "Epoch 66, Training Loss: 0.8870566248893738\n",
      "Epoch 67, Training Loss: 0.8848029659074895\n",
      "Epoch 68, Training Loss: 0.8824380668471842\n",
      "Epoch 69, Training Loss: 0.8799578950685614\n",
      "Epoch 70, Training Loss: 0.8773449847978704\n",
      "Epoch 71, Training Loss: 0.8746871106063618\n",
      "Epoch 72, Training Loss: 0.8718846267111161\n",
      "Epoch 73, Training Loss: 0.8689777480153477\n",
      "Epoch 74, Training Loss: 0.8660342140758739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:23:05,647] Trial 20 finished with value: 0.5946 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8629913720663857\n",
      "Epoch 1, Training Loss: 1.0914791687797099\n",
      "Epoch 2, Training Loss: 1.0910569443422204\n",
      "Epoch 3, Training Loss: 1.091036628414603\n",
      "Epoch 4, Training Loss: 1.090926748864791\n",
      "Epoch 5, Training Loss: 1.09077392578125\n",
      "Epoch 6, Training Loss: 1.09073540280847\n",
      "Epoch 7, Training Loss: 1.0906178578208474\n",
      "Epoch 8, Training Loss: 1.0906102066881516\n",
      "Epoch 9, Training Loss: 1.0904541321361767\n",
      "Epoch 10, Training Loss: 1.0904077315330505\n",
      "Epoch 11, Training Loss: 1.0902763565848856\n",
      "Epoch 12, Training Loss: 1.09003007552203\n",
      "Epoch 13, Training Loss: 1.0901211833953857\n",
      "Epoch 14, Training Loss: 1.0900140693608453\n",
      "Epoch 15, Training Loss: 1.0898784004940707\n",
      "Epoch 16, Training Loss: 1.089739684637855\n",
      "Epoch 17, Training Loss: 1.0895605055023643\n",
      "Epoch 18, Training Loss: 1.089503759356106\n",
      "Epoch 19, Training Loss: 1.0892176379876979\n",
      "Epoch 20, Training Loss: 1.0891998718766605\n",
      "Epoch 21, Training Loss: 1.088996668843662\n",
      "Epoch 22, Training Loss: 1.0887655725198633\n",
      "Epoch 23, Training Loss: 1.0885325121879577\n",
      "Epoch 24, Training Loss: 1.0882582716380849\n",
      "Epoch 25, Training Loss: 1.08798167032354\n",
      "Epoch 26, Training Loss: 1.087650116892422\n",
      "Epoch 27, Training Loss: 1.0872411356252782\n",
      "Epoch 28, Training Loss: 1.0868907896210165\n",
      "Epoch 29, Training Loss: 1.0863950186617235\n",
      "Epoch 30, Training Loss: 1.0858952828014599\n",
      "Epoch 31, Training Loss: 1.0852190521184135\n",
      "Epoch 32, Training Loss: 1.0844888891893274\n",
      "Epoch 33, Training Loss: 1.0836496294246\n",
      "Epoch 34, Training Loss: 1.082637167257421\n",
      "Epoch 35, Training Loss: 1.0814465982773724\n",
      "Epoch 36, Training Loss: 1.0799738404330086\n",
      "Epoch 37, Training Loss: 1.0783292560016409\n",
      "Epoch 38, Training Loss: 1.0760847567109502\n",
      "Epoch 39, Training Loss: 1.0734267201143153\n",
      "Epoch 40, Training Loss: 1.0701246145192314\n",
      "Epoch 41, Training Loss: 1.0658875773934757\n",
      "Epoch 42, Training Loss: 1.0605659333397361\n",
      "Epoch 43, Training Loss: 1.0538273274197298\n",
      "Epoch 44, Training Loss: 1.045483024260577\n",
      "Epoch 45, Training Loss: 1.0355842579813566\n",
      "Epoch 46, Training Loss: 1.024486202141818\n",
      "Epoch 47, Training Loss: 1.013304157186957\n",
      "Epoch 48, Training Loss: 1.0033104540320004\n",
      "Epoch 49, Training Loss: 0.9955121153242448\n",
      "Epoch 50, Training Loss: 0.9900463674348944\n",
      "Epoch 51, Training Loss: 0.9860535740151125\n",
      "Epoch 52, Training Loss: 0.9829688265744377\n",
      "Epoch 53, Training Loss: 0.9802538885088528\n",
      "Epoch 54, Training Loss: 0.9775534078654121\n",
      "Epoch 55, Training Loss: 0.9749809392760782\n",
      "Epoch 56, Training Loss: 0.9721088210274191\n",
      "Epoch 57, Training Loss: 0.9693231201873106\n",
      "Epoch 58, Training Loss: 0.9664890052991755\n",
      "Epoch 59, Training Loss: 0.963607996561948\n",
      "Epoch 60, Training Loss: 0.9608782074030708\n",
      "Epoch 61, Training Loss: 0.9583342361450196\n",
      "Epoch 62, Training Loss: 0.955804126402911\n",
      "Epoch 63, Training Loss: 0.9536637066392337\n",
      "Epoch 64, Training Loss: 0.9518293243997237\n",
      "Epoch 65, Training Loss: 0.9499984930543338\n",
      "Epoch 66, Training Loss: 0.9486083627448363\n",
      "Epoch 67, Training Loss: 0.9473210466609282\n",
      "Epoch 68, Training Loss: 0.9460680691634907\n",
      "Epoch 69, Training Loss: 0.9449214198308833\n",
      "Epoch 70, Training Loss: 0.9436367573457606\n",
      "Epoch 71, Training Loss: 0.9426078968188342\n",
      "Epoch 72, Training Loss: 0.9413560472516452\n",
      "Epoch 73, Training Loss: 0.9401761644728044\n",
      "Epoch 74, Training Loss: 0.9388333203512079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:24:32,354] Trial 21 finished with value: 0.5449333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9375464214998133\n",
      "Epoch 1, Training Loss: 0.862437156158335\n",
      "Epoch 2, Training Loss: 0.826407981970731\n",
      "Epoch 3, Training Loss: 0.822352135742412\n",
      "Epoch 4, Training Loss: 0.8192166935696321\n",
      "Epoch 5, Training Loss: 0.8163596900070414\n",
      "Epoch 6, Training Loss: 0.8136035347686095\n",
      "Epoch 7, Training Loss: 0.8124647905546076\n",
      "Epoch 8, Training Loss: 0.8117841097186593\n",
      "Epoch 9, Training Loss: 0.8104321883005254\n",
      "Epoch 10, Training Loss: 0.8097610310245963\n",
      "Epoch 11, Training Loss: 0.8095772219405455\n",
      "Epoch 12, Training Loss: 0.8087583169516395\n",
      "Epoch 13, Training Loss: 0.8086832275811364\n",
      "Epoch 14, Training Loss: 0.8085326433882993\n",
      "Epoch 15, Training Loss: 0.8082574373834274\n",
      "Epoch 16, Training Loss: 0.807278288602829\n",
      "Epoch 17, Training Loss: 0.8074330991857193\n",
      "Epoch 18, Training Loss: 0.8073896336555481\n",
      "Epoch 19, Training Loss: 0.8072929279944476\n",
      "Epoch 20, Training Loss: 0.8070792585260728\n",
      "Epoch 21, Training Loss: 0.8063293909325319\n",
      "Epoch 22, Training Loss: 0.8059643520327175\n",
      "Epoch 23, Training Loss: 0.806021000216989\n",
      "Epoch 24, Training Loss: 0.8060904537228977\n",
      "Epoch 25, Training Loss: 0.8055764180071213\n",
      "Epoch 26, Training Loss: 0.8058223699822146\n",
      "Epoch 27, Training Loss: 0.8056938716243295\n",
      "Epoch 28, Training Loss: 0.8054142298417933\n",
      "Epoch 29, Training Loss: 0.8051232624053956\n",
      "Epoch 30, Training Loss: 0.804768665117376\n",
      "Epoch 31, Training Loss: 0.8040930503957412\n",
      "Epoch 32, Training Loss: 0.804036425001481\n",
      "Epoch 33, Training Loss: 0.8041553366184234\n",
      "Epoch 34, Training Loss: 0.8042519626196692\n",
      "Epoch 35, Training Loss: 0.8039091384410858\n",
      "Epoch 36, Training Loss: 0.803632527028813\n",
      "Epoch 37, Training Loss: 0.8032358886213864\n",
      "Epoch 38, Training Loss: 0.8034783120015089\n",
      "Epoch 39, Training Loss: 0.8030000483989715\n",
      "Epoch 40, Training Loss: 0.8029022122130675\n",
      "Epoch 41, Training Loss: 0.8028153435622944\n",
      "Epoch 42, Training Loss: 0.8026403064587537\n",
      "Epoch 43, Training Loss: 0.8019274343462551\n",
      "Epoch 44, Training Loss: 0.8028491698293125\n",
      "Epoch 45, Training Loss: 0.8022289213012247\n",
      "Epoch 46, Training Loss: 0.8019885424305411\n",
      "Epoch 47, Training Loss: 0.8019976917435141\n",
      "Epoch 48, Training Loss: 0.8019741481191972\n",
      "Epoch 49, Training Loss: 0.8013665072356954\n",
      "Epoch 50, Training Loss: 0.8015026452260859\n",
      "Epoch 51, Training Loss: 0.8015871421028586\n",
      "Epoch 52, Training Loss: 0.8015199822538039\n",
      "Epoch 53, Training Loss: 0.8016377033205593\n",
      "Epoch 54, Training Loss: 0.8011541783108431\n",
      "Epoch 55, Training Loss: 0.8002837498749004\n",
      "Epoch 56, Training Loss: 0.8007059993463405\n",
      "Epoch 57, Training Loss: 0.8007772575406468\n",
      "Epoch 58, Training Loss: 0.8001456436690162\n",
      "Epoch 59, Training Loss: 0.8012271400760201\n",
      "Epoch 60, Training Loss: 0.8004805951258716\n",
      "Epoch 61, Training Loss: 0.800025283238467\n",
      "Epoch 62, Training Loss: 0.7995685217661016\n",
      "Epoch 63, Training Loss: 0.7997327732338625\n",
      "Epoch 64, Training Loss: 0.7998196041584015\n",
      "Epoch 65, Training Loss: 0.7990913938774782\n",
      "Epoch 66, Training Loss: 0.7996640004129971\n",
      "Epoch 67, Training Loss: 0.7993544358365676\n",
      "Epoch 68, Training Loss: 0.7990389240489286\n",
      "Epoch 69, Training Loss: 0.7988058579669279\n",
      "Epoch 70, Training Loss: 0.7989990401268006\n",
      "Epoch 71, Training Loss: 0.7986449506002314\n",
      "Epoch 72, Training Loss: 0.7986844283692976\n",
      "Epoch 73, Training Loss: 0.7983090973601622\n",
      "Epoch 74, Training Loss: 0.7982507805263295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:25:53,966] Trial 22 finished with value: 0.6328 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7984591215498307\n",
      "Epoch 1, Training Loss: 1.0738670219393338\n",
      "Epoch 2, Training Loss: 1.0236375516302445\n",
      "Epoch 3, Training Loss: 0.9972075026175555\n",
      "Epoch 4, Training Loss: 0.9811212904313031\n",
      "Epoch 5, Training Loss: 0.9711734618860133\n",
      "Epoch 6, Training Loss: 0.9650542696083293\n",
      "Epoch 7, Training Loss: 0.9611670385388768\n",
      "Epoch 8, Training Loss: 0.9585349695822772\n",
      "Epoch 9, Training Loss: 0.9565953188783982\n",
      "Epoch 10, Training Loss: 0.9550181059977587\n",
      "Epoch 11, Training Loss: 0.9536167167214786\n",
      "Epoch 12, Training Loss: 0.9523170697689056\n",
      "Epoch 13, Training Loss: 0.9510607739757089\n",
      "Epoch 14, Training Loss: 0.9498155764271231\n",
      "Epoch 15, Training Loss: 0.9485784667379716\n",
      "Epoch 16, Training Loss: 0.9473212120112251\n",
      "Epoch 17, Training Loss: 0.9460684755269219\n",
      "Epoch 18, Training Loss: 0.9447976749784807\n",
      "Epoch 19, Training Loss: 0.9434992075667662\n",
      "Epoch 20, Training Loss: 0.9421821104778964\n",
      "Epoch 21, Training Loss: 0.9408353381297168\n",
      "Epoch 22, Training Loss: 0.9394626475782956\n",
      "Epoch 23, Training Loss: 0.9380699971844169\n",
      "Epoch 24, Training Loss: 0.9366425258973066\n",
      "Epoch 25, Training Loss: 0.935179077316733\n",
      "Epoch 26, Training Loss: 0.93368506620912\n",
      "Epoch 27, Training Loss: 0.9321493621433482\n",
      "Epoch 28, Training Loss: 0.9305823704775642\n",
      "Epoch 29, Training Loss: 0.9289654123783112\n",
      "Epoch 30, Training Loss: 0.9273164702864254\n",
      "Epoch 31, Training Loss: 0.9256251486609964\n",
      "Epoch 32, Training Loss: 0.9238947606787962\n",
      "Epoch 33, Training Loss: 0.9221078098521513\n",
      "Epoch 34, Training Loss: 0.9202970672354979\n",
      "Epoch 35, Training Loss: 0.9184252031410441\n",
      "Epoch 36, Training Loss: 0.9165111738793991\n",
      "Epoch 37, Training Loss: 0.9145504675191991\n",
      "Epoch 38, Training Loss: 0.9125533293275272\n",
      "Epoch 39, Training Loss: 0.9105052056032069\n",
      "Epoch 40, Training Loss: 0.9084236801371854\n",
      "Epoch 41, Training Loss: 0.9062953899888432\n",
      "Epoch 42, Training Loss: 0.9041322202542249\n",
      "Epoch 43, Training Loss: 0.9019417440891266\n",
      "Epoch 44, Training Loss: 0.8996999151566449\n",
      "Epoch 45, Training Loss: 0.8974273730726803\n",
      "Epoch 46, Training Loss: 0.8951265037761015\n",
      "Epoch 47, Training Loss: 0.8928111964113572\n",
      "Epoch 48, Training Loss: 0.890497841835022\n",
      "Epoch 49, Training Loss: 0.8881396712275113\n",
      "Epoch 50, Training Loss: 0.885786657333374\n",
      "Epoch 51, Training Loss: 0.8834249270663542\n",
      "Epoch 52, Training Loss: 0.8810445885097279\n",
      "Epoch 53, Training Loss: 0.8786876173580394\n",
      "Epoch 54, Training Loss: 0.876345424932592\n",
      "Epoch 55, Training Loss: 0.8740140640034395\n",
      "Epoch 56, Training Loss: 0.871721608358271\n",
      "Epoch 57, Training Loss: 0.8694295124446645\n",
      "Epoch 58, Training Loss: 0.8671857515503378\n",
      "Epoch 59, Training Loss: 0.8649747358350193\n",
      "Epoch 60, Training Loss: 0.8627955368687125\n",
      "Epoch 61, Training Loss: 0.860635284465902\n",
      "Epoch 62, Training Loss: 0.8585919307961184\n",
      "Epoch 63, Training Loss: 0.8565726657474743\n",
      "Epoch 64, Training Loss: 0.8546008840729209\n",
      "Epoch 65, Training Loss: 0.8526859999404234\n",
      "Epoch 66, Training Loss: 0.8508417645622702\n",
      "Epoch 67, Training Loss: 0.849027686399572\n",
      "Epoch 68, Training Loss: 0.8473154771327972\n",
      "Epoch 69, Training Loss: 0.8456327690797694\n",
      "Epoch 70, Training Loss: 0.8440590999406927\n",
      "Epoch 71, Training Loss: 0.8424818764013403\n",
      "Epoch 72, Training Loss: 0.8410425490491531\n",
      "Epoch 73, Training Loss: 0.8395909293960122\n",
      "Epoch 74, Training Loss: 0.8382426318701576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:27:14,698] Trial 23 finished with value: 0.613 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8369217178400825\n",
      "Epoch 1, Training Loss: 0.976804845683715\n",
      "Epoch 2, Training Loss: 0.9368996491852929\n",
      "Epoch 3, Training Loss: 0.8990655312117408\n",
      "Epoch 4, Training Loss: 0.8502699988729814\n",
      "Epoch 5, Training Loss: 0.8238671628166647\n",
      "Epoch 6, Training Loss: 0.8161121619448942\n",
      "Epoch 7, Training Loss: 0.8134990318382488\n",
      "Epoch 8, Training Loss: 0.8118002154546625\n",
      "Epoch 9, Training Loss: 0.8105003925632028\n",
      "Epoch 10, Training Loss: 0.8089218716761645\n",
      "Epoch 11, Training Loss: 0.8080636448719922\n",
      "Epoch 12, Training Loss: 0.807433758132598\n",
      "Epoch 13, Training Loss: 0.8061697177325978\n",
      "Epoch 14, Training Loss: 0.8060611310425927\n",
      "Epoch 15, Training Loss: 0.8051344725664924\n",
      "Epoch 16, Training Loss: 0.8045726359591765\n",
      "Epoch 17, Training Loss: 0.8043374237593482\n",
      "Epoch 18, Training Loss: 0.8038655096643111\n",
      "Epoch 19, Training Loss: 0.8037746500968933\n",
      "Epoch 20, Training Loss: 0.8033638255736407\n",
      "Epoch 21, Training Loss: 0.8027856035092298\n",
      "Epoch 22, Training Loss: 0.8027591297205756\n",
      "Epoch 23, Training Loss: 0.8023265152819017\n",
      "Epoch 24, Training Loss: 0.8021192794687608\n",
      "Epoch 25, Training Loss: 0.8021579452121959\n",
      "Epoch 26, Training Loss: 0.8013617840935202\n",
      "Epoch 27, Training Loss: 0.8019439579458798\n",
      "Epoch 28, Training Loss: 0.8014463404346915\n",
      "Epoch 29, Training Loss: 0.8009264537166146\n",
      "Epoch 30, Training Loss: 0.8009547193611369\n",
      "Epoch 31, Training Loss: 0.8007705204627094\n",
      "Epoch 32, Training Loss: 0.8006267015373005\n",
      "Epoch 33, Training Loss: 0.8004039125582751\n",
      "Epoch 34, Training Loss: 0.8003281798082239\n",
      "Epoch 35, Training Loss: 0.7999012377682854\n",
      "Epoch 36, Training Loss: 0.7999790765958674\n",
      "Epoch 37, Training Loss: 0.7997191612860736\n",
      "Epoch 38, Training Loss: 0.7999076337674085\n",
      "Epoch 39, Training Loss: 0.7998984519874348\n",
      "Epoch 40, Training Loss: 0.799501814772101\n",
      "Epoch 41, Training Loss: 0.7996554175545187\n",
      "Epoch 42, Training Loss: 0.7990991309811087\n",
      "Epoch 43, Training Loss: 0.798833548672059\n",
      "Epoch 44, Training Loss: 0.7989271180769977\n",
      "Epoch 45, Training Loss: 0.7988769029869752\n",
      "Epoch 46, Training Loss: 0.7988830095880172\n",
      "Epoch 47, Training Loss: 0.798568085642422\n",
      "Epoch 48, Training Loss: 0.7985333028961631\n",
      "Epoch 49, Training Loss: 0.7986766373409945\n",
      "Epoch 50, Training Loss: 0.7985219593609081\n",
      "Epoch 51, Training Loss: 0.7979993800555959\n",
      "Epoch 52, Training Loss: 0.7979706508271834\n",
      "Epoch 53, Training Loss: 0.7978579717523911\n",
      "Epoch 54, Training Loss: 0.7981151588524089\n",
      "Epoch 55, Training Loss: 0.7976864758659812\n",
      "Epoch 56, Training Loss: 0.7977678670602686\n",
      "Epoch 57, Training Loss: 0.7976043088997111\n",
      "Epoch 58, Training Loss: 0.7976337355725905\n",
      "Epoch 59, Training Loss: 0.7973328358986799\n",
      "Epoch 60, Training Loss: 0.7973009024648106\n",
      "Epoch 61, Training Loss: 0.7971441427399131\n",
      "Epoch 62, Training Loss: 0.7970032743145438\n",
      "Epoch 63, Training Loss: 0.7968835585958818\n",
      "Epoch 64, Training Loss: 0.7969169235229492\n",
      "Epoch 65, Training Loss: 0.7966992784247678\n",
      "Epoch 66, Training Loss: 0.7966990641285392\n",
      "Epoch 67, Training Loss: 0.796650830226786\n",
      "Epoch 68, Training Loss: 0.7963067964946522\n",
      "Epoch 69, Training Loss: 0.7964607862163993\n",
      "Epoch 70, Training Loss: 0.7961493342764238\n",
      "Epoch 71, Training Loss: 0.796037402714\n",
      "Epoch 72, Training Loss: 0.7959913038506228\n",
      "Epoch 73, Training Loss: 0.795608973643359\n",
      "Epoch 74, Training Loss: 0.7957861235562493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:28:49,970] Trial 24 finished with value: 0.6289333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 15 with value: 0.6382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7958202094190261\n",
      "Epoch 1, Training Loss: 0.845485756451026\n",
      "Epoch 2, Training Loss: 0.8076210711235391\n",
      "Epoch 3, Training Loss: 0.8036711793196829\n",
      "Epoch 4, Training Loss: 0.7996031656301111\n",
      "Epoch 5, Training Loss: 0.7991526849287793\n",
      "Epoch 6, Training Loss: 0.7988443546725396\n",
      "Epoch 7, Training Loss: 0.7956980839707798\n",
      "Epoch 8, Training Loss: 0.7954755506121126\n",
      "Epoch 9, Training Loss: 0.7958700780581711\n",
      "Epoch 10, Training Loss: 0.7933354259433603\n",
      "Epoch 11, Training Loss: 0.7943199534165232\n",
      "Epoch 12, Training Loss: 0.7952441825902552\n",
      "Epoch 13, Training Loss: 0.7935010109628949\n",
      "Epoch 14, Training Loss: 0.7924716057633995\n",
      "Epoch 15, Training Loss: 0.7927568053840695\n",
      "Epoch 16, Training Loss: 0.7934760893197884\n",
      "Epoch 17, Training Loss: 0.7920219741369549\n",
      "Epoch 18, Training Loss: 0.7935296383119167\n",
      "Epoch 19, Training Loss: 0.7919197691114326\n",
      "Epoch 20, Training Loss: 0.7934652817876715\n",
      "Epoch 21, Training Loss: 0.7908781746276339\n",
      "Epoch 22, Training Loss: 0.7916471904381773\n",
      "Epoch 23, Training Loss: 0.7918610222357556\n",
      "Epoch 24, Training Loss: 0.7897644817380977\n",
      "Epoch 25, Training Loss: 0.7914496217455183\n",
      "Epoch 26, Training Loss: 0.7900634625352415\n",
      "Epoch 27, Training Loss: 0.79119864038955\n",
      "Epoch 28, Training Loss: 0.790251168093287\n",
      "Epoch 29, Training Loss: 0.7895214096944135\n",
      "Epoch 30, Training Loss: 0.789915755010189\n",
      "Epoch 31, Training Loss: 0.7898876980731362\n",
      "Epoch 32, Training Loss: 0.7910440845597059\n",
      "Epoch 33, Training Loss: 0.789105917278089\n",
      "Epoch 34, Training Loss: 0.7902774882495851\n",
      "Epoch 35, Training Loss: 0.789113319995708\n",
      "Epoch 36, Training Loss: 0.7890403604148922\n",
      "Epoch 37, Training Loss: 0.7891478340428575\n",
      "Epoch 38, Training Loss: 0.7892002214166455\n",
      "Epoch 39, Training Loss: 0.7891716843260859\n",
      "Epoch 40, Training Loss: 0.788266074209285\n",
      "Epoch 41, Training Loss: 0.7888091626920198\n",
      "Epoch 42, Training Loss: 0.7882675269492587\n",
      "Epoch 43, Training Loss: 0.7896131221513103\n",
      "Epoch 44, Training Loss: 0.7878457501418609\n",
      "Epoch 45, Training Loss: 0.7878467807644292\n",
      "Epoch 46, Training Loss: 0.7885473099866308\n",
      "Epoch 47, Training Loss: 0.7890259325952458\n",
      "Epoch 48, Training Loss: 0.7890584072672335\n",
      "Epoch 49, Training Loss: 0.7883313944465236\n",
      "Epoch 50, Training Loss: 0.7890727509233288\n",
      "Epoch 51, Training Loss: 0.7886353635250177\n",
      "Epoch 52, Training Loss: 0.7884900845979389\n",
      "Epoch 53, Training Loss: 0.7897824411105393\n",
      "Epoch 54, Training Loss: 0.7885411280438416\n",
      "Epoch 55, Training Loss: 0.7878975995501182\n",
      "Epoch 56, Training Loss: 0.787289139381925\n",
      "Epoch 57, Training Loss: 0.7888019504403709\n",
      "Epoch 58, Training Loss: 0.7880270975872986\n",
      "Epoch 59, Training Loss: 0.7885099782083268\n",
      "Epoch 60, Training Loss: 0.7888799593860941\n",
      "Epoch 61, Training Loss: 0.788396386096352\n",
      "Epoch 62, Training Loss: 0.787378051764983\n",
      "Epoch 63, Training Loss: 0.7881854938385182\n",
      "Epoch 64, Training Loss: 0.7883699668529338\n",
      "Epoch 65, Training Loss: 0.7874779104290152\n",
      "Epoch 66, Training Loss: 0.7876528972969916\n",
      "Epoch 67, Training Loss: 0.7871051753373971\n",
      "Epoch 68, Training Loss: 0.7871240213401336\n",
      "Epoch 69, Training Loss: 0.7872039149578353\n",
      "Epoch 70, Training Loss: 0.7881187386082528\n",
      "Epoch 71, Training Loss: 0.7883312047872328\n",
      "Epoch 72, Training Loss: 0.7878892255008669\n",
      "Epoch 73, Training Loss: 0.7869893159185137\n",
      "Epoch 74, Training Loss: 0.7874498096623815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:30:17,612] Trial 25 finished with value: 0.6383333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 25 with value: 0.6383333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7876853318142711\n",
      "Epoch 1, Training Loss: 0.9468834824131843\n",
      "Epoch 2, Training Loss: 0.8912701499193234\n",
      "Epoch 3, Training Loss: 0.8472911719092749\n",
      "Epoch 4, Training Loss: 0.825049684908157\n",
      "Epoch 5, Training Loss: 0.8161103020933338\n",
      "Epoch 6, Training Loss: 0.8128614441792767\n",
      "Epoch 7, Training Loss: 0.8116832727776434\n",
      "Epoch 8, Training Loss: 0.8106891691236567\n",
      "Epoch 9, Training Loss: 0.8094942982035472\n",
      "Epoch 10, Training Loss: 0.8087511140601079\n",
      "Epoch 11, Training Loss: 0.8085955923661253\n",
      "Epoch 12, Training Loss: 0.807540646352266\n",
      "Epoch 13, Training Loss: 0.8081376083811423\n",
      "Epoch 14, Training Loss: 0.8074894992032445\n",
      "Epoch 15, Training Loss: 0.8066395310530985\n",
      "Epoch 16, Training Loss: 0.8064154360527382\n",
      "Epoch 17, Training Loss: 0.8064971571578119\n",
      "Epoch 18, Training Loss: 0.8056560286005637\n",
      "Epoch 19, Training Loss: 0.8052151753490132\n",
      "Epoch 20, Training Loss: 0.8050571030243895\n",
      "Epoch 21, Training Loss: 0.8055138339673666\n",
      "Epoch 22, Training Loss: 0.8042353516234491\n",
      "Epoch 23, Training Loss: 0.80451400127626\n",
      "Epoch 24, Training Loss: 0.8038600657219277\n",
      "Epoch 25, Training Loss: 0.8041101554282626\n",
      "Epoch 26, Training Loss: 0.8035589593693726\n",
      "Epoch 27, Training Loss: 0.8031878334239013\n",
      "Epoch 28, Training Loss: 0.8029976358987335\n",
      "Epoch 29, Training Loss: 0.8029486563868988\n",
      "Epoch 30, Training Loss: 0.8029390066189873\n",
      "Epoch 31, Training Loss: 0.8025765730922384\n",
      "Epoch 32, Training Loss: 0.8022584883790267\n",
      "Epoch 33, Training Loss: 0.8016542751089971\n",
      "Epoch 34, Training Loss: 0.8017555574725445\n",
      "Epoch 35, Training Loss: 0.8014390847736732\n",
      "Epoch 36, Training Loss: 0.801619445470939\n",
      "Epoch 37, Training Loss: 0.8010198912226167\n",
      "Epoch 38, Training Loss: 0.8008450188134846\n",
      "Epoch 39, Training Loss: 0.8009240839714394\n",
      "Epoch 40, Training Loss: 0.8012566440087512\n",
      "Epoch 41, Training Loss: 0.8009753702278424\n",
      "Epoch 42, Training Loss: 0.8004949463937515\n",
      "Epoch 43, Training Loss: 0.8003850796168908\n",
      "Epoch 44, Training Loss: 0.8002136505636057\n",
      "Epoch 45, Training Loss: 0.7999957788259463\n",
      "Epoch 46, Training Loss: 0.7992855642971239\n",
      "Epoch 47, Training Loss: 0.7993822164105293\n",
      "Epoch 48, Training Loss: 0.7997062818448346\n",
      "Epoch 49, Training Loss: 0.7997094753093289\n",
      "Epoch 50, Training Loss: 0.7997229281224703\n",
      "Epoch 51, Training Loss: 0.7993934056812659\n",
      "Epoch 52, Training Loss: 0.7990775742925199\n",
      "Epoch 53, Training Loss: 0.7989342561341766\n",
      "Epoch 54, Training Loss: 0.7991517477465752\n",
      "Epoch 55, Training Loss: 0.7985474580660799\n",
      "Epoch 56, Training Loss: 0.7987537828603185\n",
      "Epoch 57, Training Loss: 0.7987555881191913\n",
      "Epoch 58, Training Loss: 0.7988830679341367\n",
      "Epoch 59, Training Loss: 0.7986720472350156\n",
      "Epoch 60, Training Loss: 0.7983940631823432\n",
      "Epoch 61, Training Loss: 0.7980135509842321\n",
      "Epoch 62, Training Loss: 0.7985830311488388\n",
      "Epoch 63, Training Loss: 0.7983192241281495\n",
      "Epoch 64, Training Loss: 0.7978799217625668\n",
      "Epoch 65, Training Loss: 0.7983915537819827\n",
      "Epoch 66, Training Loss: 0.7981842120787255\n",
      "Epoch 67, Training Loss: 0.7976117494411038\n",
      "Epoch 68, Training Loss: 0.7976278984008875\n",
      "Epoch 69, Training Loss: 0.7974185799297534\n",
      "Epoch 70, Training Loss: 0.7977385477912157\n",
      "Epoch 71, Training Loss: 0.7981466607043618\n",
      "Epoch 72, Training Loss: 0.7979161064427598\n",
      "Epoch 73, Training Loss: 0.7972490142610736\n",
      "Epoch 74, Training Loss: 0.797666839549416\n",
      "Epoch 75, Training Loss: 0.79760001736476\n",
      "Epoch 76, Training Loss: 0.7981785370891256\n",
      "Epoch 77, Training Loss: 0.7982691834743758\n",
      "Epoch 78, Training Loss: 0.7977213213318273\n",
      "Epoch 79, Training Loss: 0.7974123624034394\n",
      "Epoch 80, Training Loss: 0.797444258327771\n",
      "Epoch 81, Training Loss: 0.7970145257792078\n",
      "Epoch 82, Training Loss: 0.7979834851465727\n",
      "Epoch 83, Training Loss: 0.7971743837335056\n",
      "Epoch 84, Training Loss: 0.797470948481022\n",
      "Epoch 85, Training Loss: 0.7971745669393611\n",
      "Epoch 86, Training Loss: 0.7969286300185928\n",
      "Epoch 87, Training Loss: 0.7975132671513951\n",
      "Epoch 88, Training Loss: 0.7972797306856715\n",
      "Epoch 89, Training Loss: 0.7973340750636911\n",
      "Epoch 90, Training Loss: 0.796647359150693\n",
      "Epoch 91, Training Loss: 0.7971958554776988\n",
      "Epoch 92, Training Loss: 0.7969772592523044\n",
      "Epoch 93, Training Loss: 0.7970525221717089\n",
      "Epoch 94, Training Loss: 0.7975685521175987\n",
      "Epoch 95, Training Loss: 0.7971910443521084\n",
      "Epoch 96, Training Loss: 0.7968370347094715\n",
      "Epoch 97, Training Loss: 0.7973720260132524\n",
      "Epoch 98, Training Loss: 0.7968929149154433\n",
      "Epoch 99, Training Loss: 0.7965092823021394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:31:50,435] Trial 26 finished with value: 0.6344 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 25 with value: 0.6383333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7965945229494482\n",
      "Epoch 1, Training Loss: 0.8824911729733746\n",
      "Epoch 2, Training Loss: 0.8347473488714462\n",
      "Epoch 3, Training Loss: 0.8284740052725139\n",
      "Epoch 4, Training Loss: 0.8236984200943681\n",
      "Epoch 5, Training Loss: 0.8207422569282072\n",
      "Epoch 6, Training Loss: 0.8193313491075559\n",
      "Epoch 7, Training Loss: 0.8180953967840152\n",
      "Epoch 8, Training Loss: 0.8184808784857729\n",
      "Epoch 9, Training Loss: 0.8165675360457342\n",
      "Epoch 10, Training Loss: 0.818130556353949\n",
      "Epoch 11, Training Loss: 0.815820580676086\n",
      "Epoch 12, Training Loss: 0.817747548110503\n",
      "Epoch 13, Training Loss: 0.8155413574742195\n",
      "Epoch 14, Training Loss: 0.8162386951589943\n",
      "Epoch 15, Training Loss: 0.8142177959133808\n",
      "Epoch 16, Training Loss: 0.8130985493050482\n",
      "Epoch 17, Training Loss: 0.8131882577910459\n",
      "Epoch 18, Training Loss: 0.812893625668117\n",
      "Epoch 19, Training Loss: 0.8116979906433507\n",
      "Epoch 20, Training Loss: 0.8132281034512627\n",
      "Epoch 21, Training Loss: 0.8112550501536606\n",
      "Epoch 22, Training Loss: 0.8117036885784981\n",
      "Epoch 23, Training Loss: 0.8116555324174408\n",
      "Epoch 24, Training Loss: 0.8117143165796323\n",
      "Epoch 25, Training Loss: 0.8113636222997106\n",
      "Epoch 26, Training Loss: 0.8098916478623125\n",
      "Epoch 27, Training Loss: 0.8108928035076399\n",
      "Epoch 28, Training Loss: 0.8097079701889727\n",
      "Epoch 29, Training Loss: 0.8107074863928602\n",
      "Epoch 30, Training Loss: 0.8102446864422103\n",
      "Epoch 31, Training Loss: 0.8105736342587866\n",
      "Epoch 32, Training Loss: 0.8103646178890888\n",
      "Epoch 33, Training Loss: 0.8095452011079717\n",
      "Epoch 34, Training Loss: 0.8094166381018503\n",
      "Epoch 35, Training Loss: 0.8096735014055009\n",
      "Epoch 36, Training Loss: 0.8081889207201792\n",
      "Epoch 37, Training Loss: 0.8086244774044008\n",
      "Epoch 38, Training Loss: 0.8102907853018969\n",
      "Epoch 39, Training Loss: 0.8085745789054641\n",
      "Epoch 40, Training Loss: 0.8083320939451232\n",
      "Epoch 41, Training Loss: 0.808019362804585\n",
      "Epoch 42, Training Loss: 0.8106841493369942\n",
      "Epoch 43, Training Loss: 0.8079307594693693\n",
      "Epoch 44, Training Loss: 0.8093801209801121\n",
      "Epoch 45, Training Loss: 0.8075839773156589\n",
      "Epoch 46, Training Loss: 0.8083032958489612\n",
      "Epoch 47, Training Loss: 0.8073501906896893\n",
      "Epoch 48, Training Loss: 0.8083105228001014\n",
      "Epoch 49, Training Loss: 0.8083739565727406\n",
      "Epoch 50, Training Loss: 0.807706890697766\n",
      "Epoch 51, Training Loss: 0.8060052979261355\n",
      "Epoch 52, Training Loss: 0.8071549469367005\n",
      "Epoch 53, Training Loss: 0.8090701353281065\n",
      "Epoch 54, Training Loss: 0.806684697884366\n",
      "Epoch 55, Training Loss: 0.8065088589388625\n",
      "Epoch 56, Training Loss: 0.8079341581889561\n",
      "Epoch 57, Training Loss: 0.8064734536006039\n",
      "Epoch 58, Training Loss: 0.8090994922738326\n",
      "Epoch 59, Training Loss: 0.8067039349025353\n",
      "Epoch 60, Training Loss: 0.8058290303201604\n",
      "Epoch 61, Training Loss: 0.8048371585688197\n",
      "Epoch 62, Training Loss: 0.8040047793460071\n",
      "Epoch 63, Training Loss: 0.8054398640654141\n",
      "Epoch 64, Training Loss: 0.804804313451724\n",
      "Epoch 65, Training Loss: 0.8058578060981922\n",
      "Epoch 66, Training Loss: 0.803557382938557\n",
      "Epoch 67, Training Loss: 0.8038171534251449\n",
      "Epoch 68, Training Loss: 0.8057778766280727\n",
      "Epoch 69, Training Loss: 0.8048070325887293\n",
      "Epoch 70, Training Loss: 0.8034034737967011\n",
      "Epoch 71, Training Loss: 0.8033098709314389\n",
      "Epoch 72, Training Loss: 0.8037957095562067\n",
      "Epoch 73, Training Loss: 0.8035896648141675\n",
      "Epoch 74, Training Loss: 0.8029800733229271\n",
      "Epoch 75, Training Loss: 0.8032902059698463\n",
      "Epoch 76, Training Loss: 0.8039919741171643\n",
      "Epoch 77, Training Loss: 0.8041721853994785\n",
      "Epoch 78, Training Loss: 0.8048372976762012\n",
      "Epoch 79, Training Loss: 0.8031012138029686\n",
      "Epoch 80, Training Loss: 0.8034997680133447\n",
      "Epoch 81, Training Loss: 0.8027232038347345\n",
      "Epoch 82, Training Loss: 0.8034562779548473\n",
      "Epoch 83, Training Loss: 0.8031412813896524\n",
      "Epoch 84, Training Loss: 0.8035164225370364\n",
      "Epoch 85, Training Loss: 0.8027209601456061\n",
      "Epoch 86, Training Loss: 0.8017328496266128\n",
      "Epoch 87, Training Loss: 0.8037877005742009\n",
      "Epoch 88, Training Loss: 0.8037582387601523\n",
      "Epoch 89, Training Loss: 0.8023100671015287\n",
      "Epoch 90, Training Loss: 0.8037225708925635\n",
      "Epoch 91, Training Loss: 0.8013258432983456\n",
      "Epoch 92, Training Loss: 0.8053356268352135\n",
      "Epoch 93, Training Loss: 0.8015447624643942\n",
      "Epoch 94, Training Loss: 0.8039408040225954\n",
      "Epoch 95, Training Loss: 0.8010977062067591\n",
      "Epoch 96, Training Loss: 0.8032482845442636\n",
      "Epoch 97, Training Loss: 0.8034080922155452\n",
      "Epoch 98, Training Loss: 0.803549471744021\n",
      "Epoch 99, Training Loss: 0.8015300514106464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:33:23,992] Trial 27 finished with value: 0.6044666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 25 with value: 0.6383333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8021185707328911\n",
      "Epoch 1, Training Loss: 1.000786070267957\n",
      "Epoch 2, Training Loss: 0.9514377675558391\n",
      "Epoch 3, Training Loss: 0.9384539613150116\n",
      "Epoch 4, Training Loss: 0.922297039336728\n",
      "Epoch 5, Training Loss: 0.9031465507091436\n",
      "Epoch 6, Training Loss: 0.8831382109706564\n",
      "Epoch 7, Training Loss: 0.8641323589740839\n",
      "Epoch 8, Training Loss: 0.8489980625927\n",
      "Epoch 9, Training Loss: 0.8373145757761217\n",
      "Epoch 10, Training Loss: 0.8291390079304688\n",
      "Epoch 11, Training Loss: 0.8229850803999076\n",
      "Epoch 12, Training Loss: 0.8182472269337876\n",
      "Epoch 13, Training Loss: 0.8153558563469048\n",
      "Epoch 14, Training Loss: 0.8128049944576464\n",
      "Epoch 15, Training Loss: 0.8119457699302444\n",
      "Epoch 16, Training Loss: 0.8105690701563556\n",
      "Epoch 17, Training Loss: 0.8098672467963139\n",
      "Epoch 18, Training Loss: 0.8089694169230928\n",
      "Epoch 19, Training Loss: 0.808877790691261\n",
      "Epoch 20, Training Loss: 0.8084743793745687\n",
      "Epoch 21, Training Loss: 0.8081493010198263\n",
      "Epoch 22, Training Loss: 0.8079943725937291\n",
      "Epoch 23, Training Loss: 0.8074743447447181\n",
      "Epoch 24, Training Loss: 0.8077539814145942\n",
      "Epoch 25, Training Loss: 0.8070838094653939\n",
      "Epoch 26, Training Loss: 0.8069553467564117\n",
      "Epoch 27, Training Loss: 0.8074397618609264\n",
      "Epoch 28, Training Loss: 0.8065992907473916\n",
      "Epoch 29, Training Loss: 0.8066922629686226\n",
      "Epoch 30, Training Loss: 0.8060369634538664\n",
      "Epoch 31, Training Loss: 0.8065928171451827\n",
      "Epoch 32, Training Loss: 0.8061595576150077\n",
      "Epoch 33, Training Loss: 0.8063297670586664\n",
      "Epoch 34, Training Loss: 0.8061487873694054\n",
      "Epoch 35, Training Loss: 0.8058153536983003\n",
      "Epoch 36, Training Loss: 0.8056262373924256\n",
      "Epoch 37, Training Loss: 0.8053047429350085\n",
      "Epoch 38, Training Loss: 0.8057633275376227\n",
      "Epoch 39, Training Loss: 0.8056036660545751\n",
      "Epoch 40, Training Loss: 0.8051523931940695\n",
      "Epoch 41, Training Loss: 0.805447296809433\n",
      "Epoch 42, Training Loss: 0.8053967084203447\n",
      "Epoch 43, Training Loss: 0.8050306355146537\n",
      "Epoch 44, Training Loss: 0.8048008532452404\n",
      "Epoch 45, Training Loss: 0.8044222160389549\n",
      "Epoch 46, Training Loss: 0.8044912138379606\n",
      "Epoch 47, Training Loss: 0.8041016656653326\n",
      "Epoch 48, Training Loss: 0.8047954142541813\n",
      "Epoch 49, Training Loss: 0.803778356358521\n",
      "Epoch 50, Training Loss: 0.8039323857852391\n",
      "Epoch 51, Training Loss: 0.8040330094502385\n",
      "Epoch 52, Training Loss: 0.8042379392717117\n",
      "Epoch 53, Training Loss: 0.8035682434426215\n",
      "Epoch 54, Training Loss: 0.8031464156351591\n",
      "Epoch 55, Training Loss: 0.8032947784079645\n",
      "Epoch 56, Training Loss: 0.8035704722978119\n",
      "Epoch 57, Training Loss: 0.8031456448081741\n",
      "Epoch 58, Training Loss: 0.8024737026458396\n",
      "Epoch 59, Training Loss: 0.8026918041078668\n",
      "Epoch 60, Training Loss: 0.803094752569844\n",
      "Epoch 61, Training Loss: 0.8018848305806181\n",
      "Epoch 62, Training Loss: 0.8022217901129471\n",
      "Epoch 63, Training Loss: 0.8019840723589847\n",
      "Epoch 64, Training Loss: 0.8015340183910571\n",
      "Epoch 65, Training Loss: 0.8029746323599851\n",
      "Epoch 66, Training Loss: 0.8016503541989434\n",
      "Epoch 67, Training Loss: 0.8018443121049638\n",
      "Epoch 68, Training Loss: 0.8018197860932887\n",
      "Epoch 69, Training Loss: 0.8015995956901321\n",
      "Epoch 70, Training Loss: 0.8012010536695782\n",
      "Epoch 71, Training Loss: 0.8013228571504578\n",
      "Epoch 72, Training Loss: 0.8007744457488669\n",
      "Epoch 73, Training Loss: 0.8013820623096667\n",
      "Epoch 74, Training Loss: 0.8007027538199174\n",
      "Epoch 75, Training Loss: 0.8004019563359426\n",
      "Epoch 76, Training Loss: 0.8006722045124025\n",
      "Epoch 77, Training Loss: 0.800749677285216\n",
      "Epoch 78, Training Loss: 0.8002760467672707\n",
      "Epoch 79, Training Loss: 0.8006037070338887\n",
      "Epoch 80, Training Loss: 0.8002954708006149\n",
      "Epoch 81, Training Loss: 0.8004026206812465\n",
      "Epoch 82, Training Loss: 0.7997100351448346\n",
      "Epoch 83, Training Loss: 0.8004453409883313\n",
      "Epoch 84, Training Loss: 0.8004701916436504\n",
      "Epoch 85, Training Loss: 0.7998619084967706\n",
      "Epoch 86, Training Loss: 0.7992365735814088\n",
      "Epoch 87, Training Loss: 0.8000433142023875\n",
      "Epoch 88, Training Loss: 0.7994595671058597\n",
      "Epoch 89, Training Loss: 0.7998003904980825\n",
      "Epoch 90, Training Loss: 0.7992254835322388\n",
      "Epoch 91, Training Loss: 0.7997737640725042\n",
      "Epoch 92, Training Loss: 0.7990237014634268\n",
      "Epoch 93, Training Loss: 0.7999247022141192\n",
      "Epoch 94, Training Loss: 0.7991844062518356\n",
      "Epoch 95, Training Loss: 0.7990465490441573\n",
      "Epoch 96, Training Loss: 0.7996693318051503\n",
      "Epoch 97, Training Loss: 0.7990652179359493\n",
      "Epoch 98, Training Loss: 0.7996788850404266\n",
      "Epoch 99, Training Loss: 0.7988245471975857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:35:04,683] Trial 28 finished with value: 0.6339333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 25 with value: 0.6383333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7989267412881206\n",
      "Epoch 1, Training Loss: 1.0923220698973712\n",
      "Epoch 2, Training Loss: 1.0907228084171519\n",
      "Epoch 3, Training Loss: 1.090722778264214\n",
      "Epoch 4, Training Loss: 1.0906846141815185\n",
      "Epoch 5, Training Loss: 1.0906845866932589\n",
      "Epoch 6, Training Loss: 1.0906549303671893\n",
      "Epoch 7, Training Loss: 1.090658624312457\n",
      "Epoch 8, Training Loss: 1.0906461255690632\n",
      "Epoch 9, Training Loss: 1.0906393373713774\n",
      "Epoch 10, Training Loss: 1.0906150325606851\n",
      "Epoch 11, Training Loss: 1.0906060904615065\n",
      "Epoch 12, Training Loss: 1.0906023805281695\n",
      "Epoch 13, Training Loss: 1.090583888502682\n",
      "Epoch 14, Training Loss: 1.0905649089813232\n",
      "Epoch 15, Training Loss: 1.090547344824847\n",
      "Epoch 16, Training Loss: 1.0905371255033156\n",
      "Epoch 17, Training Loss: 1.0905235017047208\n",
      "Epoch 18, Training Loss: 1.0905121116077199\n",
      "Epoch 19, Training Loss: 1.0904837245099686\n",
      "Epoch 20, Training Loss: 1.090462735260234\n",
      "Epoch 21, Training Loss: 1.090483424242805\n",
      "Epoch 22, Training Loss: 1.0904677552335402\n",
      "Epoch 23, Training Loss: 1.0904441499710082\n",
      "Epoch 24, Training Loss: 1.0904309973997228\n",
      "Epoch 25, Training Loss: 1.0904095610450295\n",
      "Epoch 26, Training Loss: 1.0903954970135408\n",
      "Epoch 27, Training Loss: 1.0903924214138705\n",
      "Epoch 28, Training Loss: 1.0903753516253303\n",
      "Epoch 29, Training Loss: 1.0903662321146796\n",
      "Epoch 30, Training Loss: 1.0903406535877902\n",
      "Epoch 31, Training Loss: 1.090335449330947\n",
      "Epoch 32, Training Loss: 1.09031671369777\n",
      "Epoch 33, Training Loss: 1.0903041786306045\n",
      "Epoch 34, Training Loss: 1.0903043671215282\n",
      "Epoch 35, Training Loss: 1.0902786116039052\n",
      "Epoch 36, Training Loss: 1.0902707091499777\n",
      "Epoch 37, Training Loss: 1.090248273821438\n",
      "Epoch 38, Training Loss: 1.0902461813477908\n",
      "Epoch 39, Training Loss: 1.0902104736776912\n",
      "Epoch 40, Training Loss: 1.0902115805008832\n",
      "Epoch 41, Training Loss: 1.0901976666730993\n",
      "Epoch 42, Training Loss: 1.0901775967373568\n",
      "Epoch 43, Training Loss: 1.0901629999104667\n",
      "Epoch 44, Training Loss: 1.0901460553618039\n",
      "Epoch 45, Training Loss: 1.0901393508911132\n",
      "Epoch 46, Training Loss: 1.090084921612459\n",
      "Epoch 47, Training Loss: 1.0900973854345435\n",
      "Epoch 48, Training Loss: 1.0900865842314327\n",
      "Epoch 49, Training Loss: 1.0900747872801388\n",
      "Epoch 50, Training Loss: 1.090056390061098\n",
      "Epoch 51, Training Loss: 1.0900454171966103\n",
      "Epoch 52, Training Loss: 1.0900183578098521\n",
      "Epoch 53, Training Loss: 1.090014589954825\n",
      "Epoch 54, Training Loss: 1.0899889678113601\n",
      "Epoch 55, Training Loss: 1.0899920024591334\n",
      "Epoch 56, Training Loss: 1.0899631151031046\n",
      "Epoch 57, Training Loss: 1.0899481324588551\n",
      "Epoch 58, Training Loss: 1.089941649577197\n",
      "Epoch 59, Training Loss: 1.089910090670866\n",
      "Epoch 60, Training Loss: 1.0899121406499077\n",
      "Epoch 61, Training Loss: 1.0898848059598136\n",
      "Epoch 62, Training Loss: 1.0898425699682797\n",
      "Epoch 63, Training Loss: 1.0898576599008898\n",
      "Epoch 64, Training Loss: 1.089843577777638\n",
      "Epoch 65, Training Loss: 1.0898179354387172\n",
      "Epoch 66, Training Loss: 1.0897991247738108\n",
      "Epoch 67, Training Loss: 1.0897593952627742\n",
      "Epoch 68, Training Loss: 1.0897458176051869\n",
      "Epoch 69, Training Loss: 1.0897465460440692\n",
      "Epoch 70, Training Loss: 1.0897193385572994\n",
      "Epoch 71, Training Loss: 1.0897218391474555\n",
      "Epoch 72, Training Loss: 1.089687820743112\n",
      "Epoch 73, Training Loss: 1.08967129314647\n",
      "Epoch 74, Training Loss: 1.0896495862568125\n",
      "Epoch 75, Training Loss: 1.0896412341734942\n",
      "Epoch 76, Training Loss: 1.0896176643932567\n",
      "Epoch 77, Training Loss: 1.089593790699454\n",
      "Epoch 78, Training Loss: 1.0895832014083862\n",
      "Epoch 79, Training Loss: 1.0895613086924834\n",
      "Epoch 80, Training Loss: 1.0895595366814557\n",
      "Epoch 81, Training Loss: 1.0895348210895763\n",
      "Epoch 82, Training Loss: 1.089512765968547\n",
      "Epoch 83, Training Loss: 1.0894964484607472\n",
      "Epoch 84, Training Loss: 1.089452776067397\n",
      "Epoch 85, Training Loss: 1.0894580538132612\n",
      "Epoch 86, Training Loss: 1.0894314700014451\n",
      "Epoch 87, Training Loss: 1.089407812988057\n",
      "Epoch 88, Training Loss: 1.0893883085250855\n",
      "Epoch 89, Training Loss: 1.0893701940424303\n",
      "Epoch 90, Training Loss: 1.089347167856553\n",
      "Epoch 91, Training Loss: 1.0893229967005114\n",
      "Epoch 92, Training Loss: 1.0893143404231351\n",
      "Epoch 93, Training Loss: 1.0892915011854734\n",
      "Epoch 94, Training Loss: 1.089262347501867\n",
      "Epoch 95, Training Loss: 1.0892471878668841\n",
      "Epoch 96, Training Loss: 1.089216540701249\n",
      "Epoch 97, Training Loss: 1.0892027309361627\n",
      "Epoch 98, Training Loss: 1.089178441692801\n",
      "Epoch 99, Training Loss: 1.0891473735080046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:37:03,099] Trial 29 finished with value: 0.3558 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 25 with value: 0.6383333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.089133389977848\n",
      "Epoch 1, Training Loss: 0.8518865121038337\n",
      "Epoch 2, Training Loss: 0.8233124612865591\n",
      "Epoch 3, Training Loss: 0.8209759057912611\n",
      "Epoch 4, Training Loss: 0.8134761321813541\n",
      "Epoch 5, Training Loss: 0.8124699241236637\n",
      "Epoch 6, Training Loss: 0.812005545142898\n",
      "Epoch 7, Training Loss: 0.8116612182972126\n",
      "Epoch 8, Training Loss: 0.8115153318060968\n",
      "Epoch 9, Training Loss: 0.811807113841064\n",
      "Epoch 10, Training Loss: 0.8096099985273261\n",
      "Epoch 11, Training Loss: 0.8094627678842473\n",
      "Epoch 12, Training Loss: 0.8098357087687442\n",
      "Epoch 13, Training Loss: 0.8103111993549461\n",
      "Epoch 14, Training Loss: 0.8081119772186853\n",
      "Epoch 15, Training Loss: 0.80947317870936\n",
      "Epoch 16, Training Loss: 0.8085197853862791\n",
      "Epoch 17, Training Loss: 0.8093092357305656\n",
      "Epoch 18, Training Loss: 0.8098541378974915\n",
      "Epoch 19, Training Loss: 0.8084081678462208\n",
      "Epoch 20, Training Loss: 0.8086090313760858\n",
      "Epoch 21, Training Loss: 0.8082453552941631\n",
      "Epoch 22, Training Loss: 0.8087801781812108\n",
      "Epoch 23, Training Loss: 0.8063056005571122\n",
      "Epoch 24, Training Loss: 0.8057646733925755\n",
      "Epoch 25, Training Loss: 0.8083958297743833\n",
      "Epoch 26, Training Loss: 0.8063262355058713\n",
      "Epoch 27, Training Loss: 0.8051559959139143\n",
      "Epoch 28, Training Loss: 0.8058095862094621\n",
      "Epoch 29, Training Loss: 0.8059993005336675\n",
      "Epoch 30, Training Loss: 0.8044104343966434\n",
      "Epoch 31, Training Loss: 0.8043224185929263\n",
      "Epoch 32, Training Loss: 0.8055294492190942\n",
      "Epoch 33, Training Loss: 0.8033726278104281\n",
      "Epoch 34, Training Loss: 0.8040074069697157\n",
      "Epoch 35, Training Loss: 0.8032004408370284\n",
      "Epoch 36, Training Loss: 0.8032434598843854\n",
      "Epoch 37, Training Loss: 0.8041675122160661\n",
      "Epoch 38, Training Loss: 0.8018209093495419\n",
      "Epoch 39, Training Loss: 0.8024288721550676\n",
      "Epoch 40, Training Loss: 0.8017835965730193\n",
      "Epoch 41, Training Loss: 0.8011084979638121\n",
      "Epoch 42, Training Loss: 0.8022831095788712\n",
      "Epoch 43, Training Loss: 0.8008517971612457\n",
      "Epoch 44, Training Loss: 0.8019702335049336\n",
      "Epoch 45, Training Loss: 0.8017517478842484\n",
      "Epoch 46, Training Loss: 0.8008438448260601\n",
      "Epoch 47, Training Loss: 0.8014145483647971\n",
      "Epoch 48, Training Loss: 0.8016560475629075\n",
      "Epoch 49, Training Loss: 0.8002284274961715\n",
      "Epoch 50, Training Loss: 0.8002399116530454\n",
      "Epoch 51, Training Loss: 0.800396195927957\n",
      "Epoch 52, Training Loss: 0.8011268157708017\n",
      "Epoch 53, Training Loss: 0.8007718166014306\n",
      "Epoch 54, Training Loss: 0.7995758215287574\n",
      "Epoch 55, Training Loss: 0.8000086894608978\n",
      "Epoch 56, Training Loss: 0.801184210203644\n",
      "Epoch 57, Training Loss: 0.7999919936172945\n",
      "Epoch 58, Training Loss: 0.7983712304803662\n",
      "Epoch 59, Training Loss: 0.7993738871768005\n",
      "Epoch 60, Training Loss: 0.7985220777361016\n",
      "Epoch 61, Training Loss: 0.7992186847485994\n",
      "Epoch 62, Training Loss: 0.8006388006353737\n",
      "Epoch 63, Training Loss: 0.7993481200440485\n",
      "Epoch 64, Training Loss: 0.7991591857788258\n",
      "Epoch 65, Training Loss: 0.7990101212845709\n",
      "Epoch 66, Training Loss: 0.7985084129455394\n",
      "Epoch 67, Training Loss: 0.799691483131925\n",
      "Epoch 68, Training Loss: 0.7995708651112434\n",
      "Epoch 69, Training Loss: 0.7969994860484187\n",
      "Epoch 70, Training Loss: 0.798491799024711\n",
      "Epoch 71, Training Loss: 0.7986403721167629\n",
      "Epoch 72, Training Loss: 0.7979534841121587\n",
      "Epoch 73, Training Loss: 0.7995256065426016\n",
      "Epoch 74, Training Loss: 0.7974388308991167\n",
      "Epoch 75, Training Loss: 0.7979975503638275\n",
      "Epoch 76, Training Loss: 0.7987875931245044\n",
      "Epoch 77, Training Loss: 0.7985849971161749\n",
      "Epoch 78, Training Loss: 0.8006501328676267\n",
      "Epoch 79, Training Loss: 0.7989235667357767\n",
      "Epoch 80, Training Loss: 0.79704444515974\n",
      "Epoch 81, Training Loss: 0.7977111903348364\n",
      "Epoch 82, Training Loss: 0.799544540563024\n",
      "Epoch 83, Training Loss: 0.7981838395721034\n",
      "Epoch 84, Training Loss: 0.7987692187603255\n",
      "Epoch 85, Training Loss: 0.7974794516886087\n",
      "Epoch 86, Training Loss: 0.7988057257537555\n",
      "Epoch 87, Training Loss: 0.7976525278916037\n",
      "Epoch 88, Training Loss: 0.7979247919598916\n",
      "Epoch 89, Training Loss: 0.7975909439244665\n",
      "Epoch 90, Training Loss: 0.7977281094493722\n",
      "Epoch 91, Training Loss: 0.7972611179925445\n",
      "Epoch 92, Training Loss: 0.7974026676407434\n",
      "Epoch 93, Training Loss: 0.798292400962428\n",
      "Epoch 94, Training Loss: 0.7984882197882\n",
      "Epoch 95, Training Loss: 0.7973510130007464\n",
      "Epoch 96, Training Loss: 0.800033822543639\n",
      "Epoch 97, Training Loss: 0.7970046420742695\n",
      "Epoch 98, Training Loss: 0.7970376613444852\n",
      "Epoch 99, Training Loss: 0.7978741876164773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:38:42,342] Trial 30 finished with value: 0.6279333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 25 with value: 0.6383333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7985086987789413\n",
      "Epoch 1, Training Loss: 0.8721430153005263\n",
      "Epoch 2, Training Loss: 0.8169625957573161\n",
      "Epoch 3, Training Loss: 0.8131398297057433\n",
      "Epoch 4, Training Loss: 0.8108739439880147\n",
      "Epoch 5, Training Loss: 0.8090919957441443\n",
      "Epoch 6, Training Loss: 0.8069022699664621\n",
      "Epoch 7, Training Loss: 0.8056146531946519\n",
      "Epoch 8, Training Loss: 0.8044458318457884\n",
      "Epoch 9, Training Loss: 0.8030237988864675\n",
      "Epoch 10, Training Loss: 0.8025357530397528\n",
      "Epoch 11, Training Loss: 0.8024632190956789\n",
      "Epoch 12, Training Loss: 0.8020384092891918\n",
      "Epoch 13, Training Loss: 0.8013263956238241\n",
      "Epoch 14, Training Loss: 0.8007081890106201\n",
      "Epoch 15, Training Loss: 0.8001689581310047\n",
      "Epoch 16, Training Loss: 0.7994380386436687\n",
      "Epoch 17, Training Loss: 0.7993266446450177\n",
      "Epoch 18, Training Loss: 0.7985946677011602\n",
      "Epoch 19, Training Loss: 0.7985040265672347\n",
      "Epoch 20, Training Loss: 0.7974392611138961\n",
      "Epoch 21, Training Loss: 0.7971573448882383\n",
      "Epoch 22, Training Loss: 0.7967229615239536\n",
      "Epoch 23, Training Loss: 0.7958717690495883\n",
      "Epoch 24, Training Loss: 0.7953645365378436\n",
      "Epoch 25, Training Loss: 0.7949258557487937\n",
      "Epoch 26, Training Loss: 0.7936688154585221\n",
      "Epoch 27, Training Loss: 0.7933804251867183\n",
      "Epoch 28, Training Loss: 0.7924427197260016\n",
      "Epoch 29, Training Loss: 0.7913979850095861\n",
      "Epoch 30, Training Loss: 0.7907303529627183\n",
      "Epoch 31, Training Loss: 0.7903148523498984\n",
      "Epoch 32, Training Loss: 0.7892677681586322\n",
      "Epoch 33, Training Loss: 0.7890025481055765\n",
      "Epoch 34, Training Loss: 0.7884874770220588\n",
      "Epoch 35, Training Loss: 0.7881682914144853\n",
      "Epoch 36, Training Loss: 0.787461628072402\n",
      "Epoch 37, Training Loss: 0.787056692978915\n",
      "Epoch 38, Training Loss: 0.7869123036721174\n",
      "Epoch 39, Training Loss: 0.786525615383597\n",
      "Epoch 40, Training Loss: 0.7862016778833726\n",
      "Epoch 41, Training Loss: 0.786147212561439\n",
      "Epoch 42, Training Loss: 0.7861322018679451\n",
      "Epoch 43, Training Loss: 0.7855625895892873\n",
      "Epoch 44, Training Loss: 0.7855407723959754\n",
      "Epoch 45, Training Loss: 0.7847330273600186\n",
      "Epoch 46, Training Loss: 0.7854475444204667\n",
      "Epoch 47, Training Loss: 0.784822514407775\n",
      "Epoch 48, Training Loss: 0.7849912005312303\n",
      "Epoch 49, Training Loss: 0.7848270814559039\n",
      "Epoch 50, Training Loss: 0.7844421893007615\n",
      "Epoch 51, Training Loss: 0.7848697713543387\n",
      "Epoch 52, Training Loss: 0.7843138809063855\n",
      "Epoch 53, Training Loss: 0.784206000846975\n",
      "Epoch 54, Training Loss: 0.78407929483582\n",
      "Epoch 55, Training Loss: 0.7837376493566176\n",
      "Epoch 56, Training Loss: 0.7839505339370054\n",
      "Epoch 57, Training Loss: 0.7838670516014099\n",
      "Epoch 58, Training Loss: 0.7835816142839543\n",
      "Epoch 59, Training Loss: 0.7832753441614263\n",
      "Epoch 60, Training Loss: 0.7834003074028912\n",
      "Epoch 61, Training Loss: 0.7835595990629757\n",
      "Epoch 62, Training Loss: 0.7831950956933639\n",
      "Epoch 63, Training Loss: 0.7833615146665012\n",
      "Epoch 64, Training Loss: 0.7832138776779175\n",
      "Epoch 65, Training Loss: 0.7832300757660585\n",
      "Epoch 66, Training Loss: 0.7827385748835171\n",
      "Epoch 67, Training Loss: 0.7828583652131698\n",
      "Epoch 68, Training Loss: 0.783030622917063\n",
      "Epoch 69, Training Loss: 0.7826068178345176\n",
      "Epoch 70, Training Loss: 0.7827592574848848\n",
      "Epoch 71, Training Loss: 0.7830398953662199\n",
      "Epoch 72, Training Loss: 0.7825812233896816\n",
      "Epoch 73, Training Loss: 0.7829216690624461\n",
      "Epoch 74, Training Loss: 0.7824026214375216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:40:16,283] Trial 31 finished with value: 0.6411333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.78264997734743\n",
      "Epoch 1, Training Loss: 0.8668791865943966\n",
      "Epoch 2, Training Loss: 0.8185833193305739\n",
      "Epoch 3, Training Loss: 0.8110906837578107\n",
      "Epoch 4, Training Loss: 0.8078345111438207\n",
      "Epoch 5, Training Loss: 0.8060288954498176\n",
      "Epoch 6, Training Loss: 0.8041888176050401\n",
      "Epoch 7, Training Loss: 0.8026125239250356\n",
      "Epoch 8, Training Loss: 0.8009724361555917\n",
      "Epoch 9, Training Loss: 0.8004988172896823\n",
      "Epoch 10, Training Loss: 0.7985077693050069\n",
      "Epoch 11, Training Loss: 0.7974671297503594\n",
      "Epoch 12, Training Loss: 0.797432976199272\n",
      "Epoch 13, Training Loss: 0.7965621631844599\n",
      "Epoch 14, Training Loss: 0.7955707206761926\n",
      "Epoch 15, Training Loss: 0.7953015541671811\n",
      "Epoch 16, Training Loss: 0.7951139912569434\n",
      "Epoch 17, Training Loss: 0.7951513077083386\n",
      "Epoch 18, Training Loss: 0.7941199258754128\n",
      "Epoch 19, Training Loss: 0.7945880863003265\n",
      "Epoch 20, Training Loss: 0.7945046529734046\n",
      "Epoch 21, Training Loss: 0.7926024958827442\n",
      "Epoch 22, Training Loss: 0.7934743816691233\n",
      "Epoch 23, Training Loss: 0.7926724691140025\n",
      "Epoch 24, Training Loss: 0.7933833264766779\n",
      "Epoch 25, Training Loss: 0.7938041506853318\n",
      "Epoch 26, Training Loss: 0.7928714418769779\n",
      "Epoch 27, Training Loss: 0.7925196980175219\n",
      "Epoch 28, Training Loss: 0.7925054470399269\n",
      "Epoch 29, Training Loss: 0.7921976957106053\n",
      "Epoch 30, Training Loss: 0.7919255931574599\n",
      "Epoch 31, Training Loss: 0.7928885679495962\n",
      "Epoch 32, Training Loss: 0.7920901389050304\n",
      "Epoch 33, Training Loss: 0.79125867866932\n",
      "Epoch 34, Training Loss: 0.7922281708036151\n",
      "Epoch 35, Training Loss: 0.7918090633879927\n",
      "Epoch 36, Training Loss: 0.7932355878048374\n",
      "Epoch 37, Training Loss: 0.7902473778652965\n",
      "Epoch 38, Training Loss: 0.7904266660822962\n",
      "Epoch 39, Training Loss: 0.7911919308784313\n",
      "Epoch 40, Training Loss: 0.7904803386308197\n",
      "Epoch 41, Training Loss: 0.7912803883839371\n",
      "Epoch 42, Training Loss: 0.7907813220992124\n",
      "Epoch 43, Training Loss: 0.7909635092979087\n",
      "Epoch 44, Training Loss: 0.7904610193761668\n",
      "Epoch 45, Training Loss: 0.7908380056682386\n",
      "Epoch 46, Training Loss: 0.791520883535084\n",
      "Epoch 47, Training Loss: 0.7913272141513968\n",
      "Epoch 48, Training Loss: 0.7907428295988785\n",
      "Epoch 49, Training Loss: 0.7893003350810001\n",
      "Epoch 50, Training Loss: 0.7904145343859393\n",
      "Epoch 51, Training Loss: 0.7899388546334174\n",
      "Epoch 52, Training Loss: 0.7902365722154316\n",
      "Epoch 53, Training Loss: 0.790533200959514\n",
      "Epoch 54, Training Loss: 0.7898662715029896\n",
      "Epoch 55, Training Loss: 0.7893892347364497\n",
      "Epoch 56, Training Loss: 0.7895199095396171\n",
      "Epoch 57, Training Loss: 0.78873191928505\n",
      "Epoch 58, Training Loss: 0.7902717826957989\n",
      "Epoch 59, Training Loss: 0.790141615204345\n",
      "Epoch 60, Training Loss: 0.7903491097285336\n",
      "Epoch 61, Training Loss: 0.7902544264506577\n",
      "Epoch 62, Training Loss: 0.7900197807111238\n",
      "Epoch 63, Training Loss: 0.7893238174287897\n",
      "Epoch 64, Training Loss: 0.7905289762002184\n",
      "Epoch 65, Training Loss: 0.7896886872169667\n",
      "Epoch 66, Training Loss: 0.7898237032997877\n",
      "Epoch 67, Training Loss: 0.7900791031973703\n",
      "Epoch 68, Training Loss: 0.7891380352185184\n",
      "Epoch 69, Training Loss: 0.7893102065961164\n",
      "Epoch 70, Training Loss: 0.7893126248417044\n",
      "Epoch 71, Training Loss: 0.7903187069677768\n",
      "Epoch 72, Training Loss: 0.7888924725969931\n",
      "Epoch 73, Training Loss: 0.7895004198963481\n",
      "Epoch 74, Training Loss: 0.7896535123201242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:41:48,702] Trial 32 finished with value: 0.6102666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7889894501607221\n",
      "Epoch 1, Training Loss: 0.9250819261809041\n",
      "Epoch 2, Training Loss: 0.8500784442837077\n",
      "Epoch 3, Training Loss: 0.8403463391433085\n",
      "Epoch 4, Training Loss: 0.8342408822891407\n",
      "Epoch 5, Training Loss: 0.8321248522378448\n",
      "Epoch 6, Training Loss: 0.8298281020687935\n",
      "Epoch 7, Training Loss: 0.8261937702508797\n",
      "Epoch 8, Training Loss: 0.8267892042497047\n",
      "Epoch 9, Training Loss: 0.8226929140270205\n",
      "Epoch 10, Training Loss: 0.822552906570578\n",
      "Epoch 11, Training Loss: 0.8224001146796951\n",
      "Epoch 12, Training Loss: 0.8207536514988519\n",
      "Epoch 13, Training Loss: 0.8207762786320277\n",
      "Epoch 14, Training Loss: 0.8211923644058686\n",
      "Epoch 15, Training Loss: 0.8195732236804819\n",
      "Epoch 16, Training Loss: 0.8207643948999562\n",
      "Epoch 17, Training Loss: 0.8218950734998947\n",
      "Epoch 18, Training Loss: 0.8182462390204122\n",
      "Epoch 19, Training Loss: 0.8182672714828548\n",
      "Epoch 20, Training Loss: 0.8211712817500408\n",
      "Epoch 21, Training Loss: 0.8186538404091857\n",
      "Epoch 22, Training Loss: 0.820144792606956\n",
      "Epoch 23, Training Loss: 0.8189368042730747\n",
      "Epoch 24, Training Loss: 0.8203048843190186\n",
      "Epoch 25, Training Loss: 0.8184541664625469\n",
      "Epoch 26, Training Loss: 0.8195537988404582\n",
      "Epoch 27, Training Loss: 0.8185343163354056\n",
      "Epoch 28, Training Loss: 0.8175164914668951\n",
      "Epoch 29, Training Loss: 0.8176546420369829\n",
      "Epoch 30, Training Loss: 0.8177452701375001\n",
      "Epoch 31, Training Loss: 0.8161334038677072\n",
      "Epoch 32, Training Loss: 0.8171011710525455\n",
      "Epoch 33, Training Loss: 0.8170111281531197\n",
      "Epoch 34, Training Loss: 0.8172255888021082\n",
      "Epoch 35, Training Loss: 0.8159724918523229\n",
      "Epoch 36, Training Loss: 0.8175035928425036\n",
      "Epoch 37, Training Loss: 0.8157829370713772\n",
      "Epoch 38, Training Loss: 0.8164056172048239\n",
      "Epoch 39, Training Loss: 0.8165262293994875\n",
      "Epoch 40, Training Loss: 0.8154432569231306\n",
      "Epoch 41, Training Loss: 0.8150450744126972\n",
      "Epoch 42, Training Loss: 0.8181672265655116\n",
      "Epoch 43, Training Loss: 0.8167694761340779\n",
      "Epoch 44, Training Loss: 0.8165411529684425\n",
      "Epoch 45, Training Loss: 0.8172934934608919\n",
      "Epoch 46, Training Loss: 0.8173752424412204\n",
      "Epoch 47, Training Loss: 0.8145815844822647\n",
      "Epoch 48, Training Loss: 0.8159478587315495\n",
      "Epoch 49, Training Loss: 0.8177137466301595\n",
      "Epoch 50, Training Loss: 0.8159738483285546\n",
      "Epoch 51, Training Loss: 0.8185170102836494\n",
      "Epoch 52, Training Loss: 0.8170903611900215\n",
      "Epoch 53, Training Loss: 0.8171893845823475\n",
      "Epoch 54, Training Loss: 0.8177684426307679\n",
      "Epoch 55, Training Loss: 0.8174154866010623\n",
      "Epoch 56, Training Loss: 0.8168821011270796\n",
      "Epoch 57, Training Loss: 0.8172436509813581\n",
      "Epoch 58, Training Loss: 0.8164145899894543\n",
      "Epoch 59, Training Loss: 0.8161450275801179\n",
      "Epoch 60, Training Loss: 0.8191661849954075\n",
      "Epoch 61, Training Loss: 0.8179905844810313\n",
      "Epoch 62, Training Loss: 0.8156285615791952\n",
      "Epoch 63, Training Loss: 0.818122096527788\n",
      "Epoch 64, Training Loss: 0.8178433940822917\n",
      "Epoch 65, Training Loss: 0.8176284364291599\n",
      "Epoch 66, Training Loss: 0.8160686165766609\n",
      "Epoch 67, Training Loss: 0.814842507377603\n",
      "Epoch 68, Training Loss: 0.817444590159825\n",
      "Epoch 69, Training Loss: 0.8178765210890232\n",
      "Epoch 70, Training Loss: 0.8192010066563026\n",
      "Epoch 71, Training Loss: 0.8165650908212017\n",
      "Epoch 72, Training Loss: 0.8185043471200125\n",
      "Epoch 73, Training Loss: 0.816269390206588\n",
      "Epoch 74, Training Loss: 0.8172363235538167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:43:10,176] Trial 33 finished with value: 0.46973333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8171816998854615\n",
      "Epoch 1, Training Loss: 1.077972828176685\n",
      "Epoch 2, Training Loss: 1.0323962203542092\n",
      "Epoch 3, Training Loss: 0.9711886094925098\n",
      "Epoch 4, Training Loss: 0.9437032484470453\n",
      "Epoch 5, Training Loss: 0.9312842599431375\n",
      "Epoch 6, Training Loss: 0.9228131740613091\n",
      "Epoch 7, Training Loss: 0.9142189024982595\n",
      "Epoch 8, Training Loss: 0.9050676472204968\n",
      "Epoch 9, Training Loss: 0.8938905493657392\n",
      "Epoch 10, Training Loss: 0.8775529985141037\n",
      "Epoch 11, Training Loss: 0.8567049089230989\n",
      "Epoch 12, Training Loss: 0.8336263509173142\n",
      "Epoch 13, Training Loss: 0.819368590985922\n",
      "Epoch 14, Training Loss: 0.812514955656869\n",
      "Epoch 15, Training Loss: 0.809072312466184\n",
      "Epoch 16, Training Loss: 0.8072479665727543\n",
      "Epoch 17, Training Loss: 0.8062674198831831\n",
      "Epoch 18, Training Loss: 0.8050006239934075\n",
      "Epoch 19, Training Loss: 0.8042042027738757\n",
      "Epoch 20, Training Loss: 0.8036679175563325\n",
      "Epoch 21, Training Loss: 0.8026806596526526\n",
      "Epoch 22, Training Loss: 0.8026303646259738\n",
      "Epoch 23, Training Loss: 0.8018509915896824\n",
      "Epoch 24, Training Loss: 0.8013083270617893\n",
      "Epoch 25, Training Loss: 0.8011955210140773\n",
      "Epoch 26, Training Loss: 0.8008491597677532\n",
      "Epoch 27, Training Loss: 0.7999734582757592\n",
      "Epoch 28, Training Loss: 0.7998573899269104\n",
      "Epoch 29, Training Loss: 0.7993956612465077\n",
      "Epoch 30, Training Loss: 0.7984353166773803\n",
      "Epoch 31, Training Loss: 0.7992594043115028\n",
      "Epoch 32, Training Loss: 0.7987834743987349\n",
      "Epoch 33, Training Loss: 0.7980526257278328\n",
      "Epoch 34, Training Loss: 0.7982399504883845\n",
      "Epoch 35, Training Loss: 0.7973572633320227\n",
      "Epoch 36, Training Loss: 0.797378717329269\n",
      "Epoch 37, Training Loss: 0.7976896669631613\n",
      "Epoch 38, Training Loss: 0.7962580218350976\n",
      "Epoch 39, Training Loss: 0.7962925526432525\n",
      "Epoch 40, Training Loss: 0.7964173734636235\n",
      "Epoch 41, Training Loss: 0.7957996776229457\n",
      "Epoch 42, Training Loss: 0.7948180234521851\n",
      "Epoch 43, Training Loss: 0.7949302214428895\n",
      "Epoch 44, Training Loss: 0.7947975262663418\n",
      "Epoch 45, Training Loss: 0.794893295155432\n",
      "Epoch 46, Training Loss: 0.7942657407961393\n",
      "Epoch 47, Training Loss: 0.7937611811143115\n",
      "Epoch 48, Training Loss: 0.7942380574412812\n",
      "Epoch 49, Training Loss: 0.7937298183154343\n",
      "Epoch 50, Training Loss: 0.7933775883868225\n",
      "Epoch 51, Training Loss: 0.7924099536766683\n",
      "Epoch 52, Training Loss: 0.7929677022130865\n",
      "Epoch 53, Training Loss: 0.7921412813932376\n",
      "Epoch 54, Training Loss: 0.7920685276053006\n",
      "Epoch 55, Training Loss: 0.7915255057184319\n",
      "Epoch 56, Training Loss: 0.791727988702014\n",
      "Epoch 57, Training Loss: 0.7916293050113478\n",
      "Epoch 58, Training Loss: 0.7917660836886643\n",
      "Epoch 59, Training Loss: 0.7916424770104258\n",
      "Epoch 60, Training Loss: 0.7908730798197868\n",
      "Epoch 61, Training Loss: 0.790527189584603\n",
      "Epoch 62, Training Loss: 0.790426830868972\n",
      "Epoch 63, Training Loss: 0.7902160611367763\n",
      "Epoch 64, Training Loss: 0.7904052696729961\n",
      "Epoch 65, Training Loss: 0.7907237978806173\n",
      "Epoch 66, Training Loss: 0.7893044360598227\n",
      "Epoch 67, Training Loss: 0.789225054862804\n",
      "Epoch 68, Training Loss: 0.7894828741711781\n",
      "Epoch 69, Training Loss: 0.7883690426224157\n",
      "Epoch 70, Training Loss: 0.788653041904134\n",
      "Epoch 71, Training Loss: 0.7885469745872612\n",
      "Epoch 72, Training Loss: 0.7887316524534297\n",
      "Epoch 73, Training Loss: 0.7889063793017452\n",
      "Epoch 74, Training Loss: 0.7883128758659936\n",
      "Epoch 75, Training Loss: 0.7881166189236748\n",
      "Epoch 76, Training Loss: 0.7877208465024045\n",
      "Epoch 77, Training Loss: 0.7876757881695167\n",
      "Epoch 78, Training Loss: 0.7869405622769119\n",
      "Epoch 79, Training Loss: 0.7865705655481582\n",
      "Epoch 80, Training Loss: 0.7865875418024851\n",
      "Epoch 81, Training Loss: 0.7868829659949568\n",
      "Epoch 82, Training Loss: 0.7867684292614012\n",
      "Epoch 83, Training Loss: 0.7865953920479107\n",
      "Epoch 84, Training Loss: 0.7864951613254116\n",
      "Epoch 85, Training Loss: 0.7867665306966107\n",
      "Epoch 86, Training Loss: 0.7858334416733649\n",
      "Epoch 87, Training Loss: 0.7859897034508841\n",
      "Epoch 88, Training Loss: 0.7861334376765373\n",
      "Epoch 89, Training Loss: 0.7856250334503059\n",
      "Epoch 90, Training Loss: 0.785317407694078\n",
      "Epoch 91, Training Loss: 0.7852499655314854\n",
      "Epoch 92, Training Loss: 0.7848701927894937\n",
      "Epoch 93, Training Loss: 0.7851126484404829\n",
      "Epoch 94, Training Loss: 0.7844543551143847\n",
      "Epoch 95, Training Loss: 0.7848964820230814\n",
      "Epoch 96, Training Loss: 0.7848075611250741\n",
      "Epoch 97, Training Loss: 0.7850959097532402\n",
      "Epoch 98, Training Loss: 0.7850359590430008\n",
      "Epoch 99, Training Loss: 0.7846316485476673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:44:52,772] Trial 34 finished with value: 0.6378666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7842646802278389\n",
      "Epoch 1, Training Loss: 0.9116350833634684\n",
      "Epoch 2, Training Loss: 0.8217721688119989\n",
      "Epoch 3, Training Loss: 0.8114067434368277\n",
      "Epoch 4, Training Loss: 0.8066784994942802\n",
      "Epoch 5, Training Loss: 0.8043074363156368\n",
      "Epoch 6, Training Loss: 0.8021825907821942\n",
      "Epoch 7, Training Loss: 0.8012406432539001\n",
      "Epoch 8, Training Loss: 0.7995953792019894\n",
      "Epoch 9, Training Loss: 0.7989170016202711\n",
      "Epoch 10, Training Loss: 0.7982265648088958\n",
      "Epoch 11, Training Loss: 0.7977038013307671\n",
      "Epoch 12, Training Loss: 0.7976320731908755\n",
      "Epoch 13, Training Loss: 0.7961418613455349\n",
      "Epoch 14, Training Loss: 0.7944323463547499\n",
      "Epoch 15, Training Loss: 0.7947285929120573\n",
      "Epoch 16, Training Loss: 0.7938589140884859\n",
      "Epoch 17, Training Loss: 0.7939871534369045\n",
      "Epoch 18, Training Loss: 0.7939892611109225\n",
      "Epoch 19, Training Loss: 0.7934961573521894\n",
      "Epoch 20, Training Loss: 0.7922736927082664\n",
      "Epoch 21, Training Loss: 0.7926764318817541\n",
      "Epoch 22, Training Loss: 0.7925751108872263\n",
      "Epoch 23, Training Loss: 0.791777811910873\n",
      "Epoch 24, Training Loss: 0.7913942509127739\n",
      "Epoch 25, Training Loss: 0.7909753973322703\n",
      "Epoch 26, Training Loss: 0.7910838813710034\n",
      "Epoch 27, Training Loss: 0.7903321920480944\n",
      "Epoch 28, Training Loss: 0.7898264844614761\n",
      "Epoch 29, Training Loss: 0.7896275479094427\n",
      "Epoch 30, Training Loss: 0.7897335897710986\n",
      "Epoch 31, Training Loss: 0.7897425768070652\n",
      "Epoch 32, Training Loss: 0.7902441605589443\n",
      "Epoch 33, Training Loss: 0.7887670262415606\n",
      "Epoch 34, Training Loss: 0.7896304374350641\n",
      "Epoch 35, Training Loss: 0.7899040691834643\n",
      "Epoch 36, Training Loss: 0.7888391054662547\n",
      "Epoch 37, Training Loss: 0.790202041765801\n",
      "Epoch 38, Training Loss: 0.7891128594714\n",
      "Epoch 39, Training Loss: 0.7895912001903792\n",
      "Epoch 40, Training Loss: 0.7888884825365884\n",
      "Epoch 41, Training Loss: 0.7888454476693519\n",
      "Epoch 42, Training Loss: 0.7890905787173966\n",
      "Epoch 43, Training Loss: 0.7887160112086992\n",
      "Epoch 44, Training Loss: 0.7887609926381506\n",
      "Epoch 45, Training Loss: 0.7888355182525807\n",
      "Epoch 46, Training Loss: 0.7889490892116289\n",
      "Epoch 47, Training Loss: 0.7885205639932389\n",
      "Epoch 48, Training Loss: 0.7887964671715758\n",
      "Epoch 49, Training Loss: 0.7884222215727756\n",
      "Epoch 50, Training Loss: 0.7883404068480757\n",
      "Epoch 51, Training Loss: 0.7879447560561331\n",
      "Epoch 52, Training Loss: 0.7887052634604892\n",
      "Epoch 53, Training Loss: 0.7886730687062543\n",
      "Epoch 54, Training Loss: 0.7900932087037796\n",
      "Epoch 55, Training Loss: 0.7884060554038314\n",
      "Epoch 56, Training Loss: 0.7884986989928368\n",
      "Epoch 57, Training Loss: 0.7891092030625594\n",
      "Epoch 58, Training Loss: 0.7883152192696593\n",
      "Epoch 59, Training Loss: 0.788614198946415\n",
      "Epoch 60, Training Loss: 0.7900823821698812\n",
      "Epoch 61, Training Loss: 0.7889392070304182\n",
      "Epoch 62, Training Loss: 0.7888952705196868\n",
      "Epoch 63, Training Loss: 0.7887372605782702\n",
      "Epoch 64, Training Loss: 0.7889422147793878\n",
      "Epoch 65, Training Loss: 0.7893345655355238\n",
      "Epoch 66, Training Loss: 0.7886202836395206\n",
      "Epoch 67, Training Loss: 0.7885098512011363\n",
      "Epoch 68, Training Loss: 0.7900778993628079\n",
      "Epoch 69, Training Loss: 0.788101579641041\n",
      "Epoch 70, Training Loss: 0.788293170839324\n",
      "Epoch 71, Training Loss: 0.7891887085778373\n",
      "Epoch 72, Training Loss: 0.7905377737561563\n",
      "Epoch 73, Training Loss: 0.7895753267116117\n",
      "Epoch 74, Training Loss: 0.7893968491625966\n",
      "Epoch 75, Training Loss: 0.7895297737049877\n",
      "Epoch 76, Training Loss: 0.7893039397727278\n",
      "Epoch 77, Training Loss: 0.7888889146926708\n",
      "Epoch 78, Training Loss: 0.7894990540088568\n",
      "Epoch 79, Training Loss: 0.7895600656817731\n",
      "Epoch 80, Training Loss: 0.7895734302083353\n",
      "Epoch 81, Training Loss: 0.7891600733412836\n",
      "Epoch 82, Training Loss: 0.7884728209416669\n",
      "Epoch 83, Training Loss: 0.7893373716146426\n",
      "Epoch 84, Training Loss: 0.7896147440250655\n",
      "Epoch 85, Training Loss: 0.7892270151385687\n",
      "Epoch 86, Training Loss: 0.7892408385312647\n",
      "Epoch 87, Training Loss: 0.7898013447460376\n",
      "Epoch 88, Training Loss: 0.7906637595112163\n",
      "Epoch 89, Training Loss: 0.7887200228701857\n",
      "Epoch 90, Training Loss: 0.7892771985297813\n",
      "Epoch 91, Training Loss: 0.790161309027134\n",
      "Epoch 92, Training Loss: 0.7899068487317938\n",
      "Epoch 93, Training Loss: 0.7890018371711099\n",
      "Epoch 94, Training Loss: 0.7892479572081028\n",
      "Epoch 95, Training Loss: 0.7904954621666356\n",
      "Epoch 96, Training Loss: 0.7904344685991904\n",
      "Epoch 97, Training Loss: 0.792203374464709\n",
      "Epoch 98, Training Loss: 0.7905406272500978\n",
      "Epoch 99, Training Loss: 0.7906758154245247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:46:53,484] Trial 35 finished with value: 0.6181333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7903677660719792\n",
      "Epoch 1, Training Loss: 1.0156666798699172\n",
      "Epoch 2, Training Loss: 0.957987654298768\n",
      "Epoch 3, Training Loss: 0.9471953549779447\n",
      "Epoch 4, Training Loss: 0.9400220023958307\n",
      "Epoch 5, Training Loss: 0.931826056663255\n",
      "Epoch 6, Training Loss: 0.9219122322878444\n",
      "Epoch 7, Training Loss: 0.9110158897880325\n",
      "Epoch 8, Training Loss: 0.8968541197310713\n",
      "Epoch 9, Training Loss: 0.8818863044107766\n",
      "Epoch 10, Training Loss: 0.8658318182579556\n",
      "Epoch 11, Training Loss: 0.8504530176184231\n",
      "Epoch 12, Training Loss: 0.838433812076884\n",
      "Epoch 13, Training Loss: 0.8285566454543207\n",
      "Epoch 14, Training Loss: 0.8224493386153888\n",
      "Epoch 15, Training Loss: 0.8178479958297615\n",
      "Epoch 16, Training Loss: 0.8149162084536445\n",
      "Epoch 17, Training Loss: 0.813438253832939\n",
      "Epoch 18, Training Loss: 0.8116768294707277\n",
      "Epoch 19, Training Loss: 0.8110585662655364\n",
      "Epoch 20, Training Loss: 0.8093937864877228\n",
      "Epoch 21, Training Loss: 0.8093950163152881\n",
      "Epoch 22, Training Loss: 0.8088183712242241\n",
      "Epoch 23, Training Loss: 0.8081971007182186\n",
      "Epoch 24, Training Loss: 0.8073190943639081\n",
      "Epoch 25, Training Loss: 0.8076149771088048\n",
      "Epoch 26, Training Loss: 0.8067091949003979\n",
      "Epoch 27, Training Loss: 0.8064006490814954\n",
      "Epoch 28, Training Loss: 0.8060397313053447\n",
      "Epoch 29, Training Loss: 0.8058328872336481\n",
      "Epoch 30, Training Loss: 0.8053298682198489\n",
      "Epoch 31, Training Loss: 0.8054114360558359\n",
      "Epoch 32, Training Loss: 0.8052096531803447\n",
      "Epoch 33, Training Loss: 0.8055648326873779\n",
      "Epoch 34, Training Loss: 0.8044157497864917\n",
      "Epoch 35, Training Loss: 0.8043826070046962\n",
      "Epoch 36, Training Loss: 0.8041223934718541\n",
      "Epoch 37, Training Loss: 0.8035881936101985\n",
      "Epoch 38, Training Loss: 0.8037189390426291\n",
      "Epoch 39, Training Loss: 0.8031690347463565\n",
      "Epoch 40, Training Loss: 0.8025094370196636\n",
      "Epoch 41, Training Loss: 0.8027428118806136\n",
      "Epoch 42, Training Loss: 0.802066289392629\n",
      "Epoch 43, Training Loss: 0.8026161574779597\n",
      "Epoch 44, Training Loss: 0.8023169163474463\n",
      "Epoch 45, Training Loss: 0.8019112097589594\n",
      "Epoch 46, Training Loss: 0.800946298353654\n",
      "Epoch 47, Training Loss: 0.8015229898287838\n",
      "Epoch 48, Training Loss: 0.8016192268608208\n",
      "Epoch 49, Training Loss: 0.8006229829967471\n",
      "Epoch 50, Training Loss: 0.8007013796863699\n",
      "Epoch 51, Training Loss: 0.8014384463317412\n",
      "Epoch 52, Training Loss: 0.8006073634427293\n",
      "Epoch 53, Training Loss: 0.8005662210005566\n",
      "Epoch 54, Training Loss: 0.8003681711684493\n",
      "Epoch 55, Training Loss: 0.8001957754443463\n",
      "Epoch 56, Training Loss: 0.7998564829503684\n",
      "Epoch 57, Training Loss: 0.7998774884338665\n",
      "Epoch 58, Training Loss: 0.7989536558775078\n",
      "Epoch 59, Training Loss: 0.7994944812659931\n",
      "Epoch 60, Training Loss: 0.7988866535344519\n",
      "Epoch 61, Training Loss: 0.7997397230980091\n",
      "Epoch 62, Training Loss: 0.7989954669672744\n",
      "Epoch 63, Training Loss: 0.7982984089313593\n",
      "Epoch 64, Training Loss: 0.7988913449129664\n",
      "Epoch 65, Training Loss: 0.7984003266893831\n",
      "Epoch 66, Training Loss: 0.7985148913878247\n",
      "Epoch 67, Training Loss: 0.7981773637739339\n",
      "Epoch 68, Training Loss: 0.798310200970872\n",
      "Epoch 69, Training Loss: 0.7983676395021884\n",
      "Epoch 70, Training Loss: 0.7990371583995962\n",
      "Epoch 71, Training Loss: 0.7983422685386543\n",
      "Epoch 72, Training Loss: 0.7981446846983486\n",
      "Epoch 73, Training Loss: 0.7981020362753617\n",
      "Epoch 74, Training Loss: 0.7975335013149376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:48:03,857] Trial 36 finished with value: 0.6264 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7984299283278616\n",
      "Epoch 1, Training Loss: 1.0954436047632892\n",
      "Epoch 2, Training Loss: 1.0901544522522086\n",
      "Epoch 3, Training Loss: 1.0866924135308518\n",
      "Epoch 4, Training Loss: 1.0840319075978788\n",
      "Epoch 5, Training Loss: 1.0817050042905305\n",
      "Epoch 6, Training Loss: 1.0795392671025785\n",
      "Epoch 7, Training Loss: 1.0772095617495085\n",
      "Epoch 8, Training Loss: 1.0749628257034416\n",
      "Epoch 9, Training Loss: 1.0724780394618671\n",
      "Epoch 10, Training Loss: 1.069454008116758\n",
      "Epoch 11, Training Loss: 1.066327298673472\n",
      "Epoch 12, Training Loss: 1.0628628244973664\n",
      "Epoch 13, Training Loss: 1.0590479483281758\n",
      "Epoch 14, Training Loss: 1.0547643975207681\n",
      "Epoch 15, Training Loss: 1.050039821997621\n",
      "Epoch 16, Training Loss: 1.0449394095212894\n",
      "Epoch 17, Training Loss: 1.0392209325517927\n",
      "Epoch 18, Training Loss: 1.033044555581602\n",
      "Epoch 19, Training Loss: 1.0264718119363139\n",
      "Epoch 20, Training Loss: 1.0194529141698565\n",
      "Epoch 21, Training Loss: 1.01216566168276\n",
      "Epoch 22, Training Loss: 1.004823220582833\n",
      "Epoch 23, Training Loss: 0.997363029117871\n",
      "Epoch 24, Training Loss: 0.9901345574766174\n",
      "Epoch 25, Training Loss: 0.9832554516039397\n",
      "Epoch 26, Training Loss: 0.9772508104044692\n",
      "Epoch 27, Training Loss: 0.9710032320560369\n",
      "Epoch 28, Training Loss: 0.9652727791241237\n",
      "Epoch 29, Training Loss: 0.9611188773822067\n",
      "Epoch 30, Training Loss: 0.9568213550668013\n",
      "Epoch 31, Training Loss: 0.9535226480405133\n",
      "Epoch 32, Training Loss: 0.9506106505716654\n",
      "Epoch 33, Training Loss: 0.9476622234609791\n",
      "Epoch 34, Training Loss: 0.9456396803820044\n",
      "Epoch 35, Training Loss: 0.9438257800905328\n",
      "Epoch 36, Training Loss: 0.9417889108335166\n",
      "Epoch 37, Training Loss: 0.9397995965821403\n",
      "Epoch 38, Training Loss: 0.9389606137024729\n",
      "Epoch 39, Training Loss: 0.938159303826497\n",
      "Epoch 40, Training Loss: 0.9365434961211413\n",
      "Epoch 41, Training Loss: 0.9348638283578973\n",
      "Epoch 42, Training Loss: 0.9343363729634679\n",
      "Epoch 43, Training Loss: 0.9331339690022002\n",
      "Epoch 44, Training Loss: 0.9322782450152519\n",
      "Epoch 45, Training Loss: 0.9313694178609919\n",
      "Epoch 46, Training Loss: 0.9303826215571926\n",
      "Epoch 47, Training Loss: 0.9297124863567209\n",
      "Epoch 48, Training Loss: 0.928948302018015\n",
      "Epoch 49, Training Loss: 0.9281790397220985\n",
      "Epoch 50, Training Loss: 0.9273775820445297\n",
      "Epoch 51, Training Loss: 0.9272194147109986\n",
      "Epoch 52, Training Loss: 0.9257067380094887\n",
      "Epoch 53, Training Loss: 0.9254842634487869\n",
      "Epoch 54, Training Loss: 0.9243451680455889\n",
      "Epoch 55, Training Loss: 0.9239506708948235\n",
      "Epoch 56, Training Loss: 0.9234732460258599\n",
      "Epoch 57, Training Loss: 0.9221797400847414\n",
      "Epoch 58, Training Loss: 0.921755047669088\n",
      "Epoch 59, Training Loss: 0.9218590804508754\n",
      "Epoch 60, Training Loss: 0.9204563045860233\n",
      "Epoch 61, Training Loss: 0.9201476562292056\n",
      "Epoch 62, Training Loss: 0.9189364680670258\n",
      "Epoch 63, Training Loss: 0.9182885686257728\n",
      "Epoch 64, Training Loss: 0.9176870813047079\n",
      "Epoch 65, Training Loss: 0.916933052073744\n",
      "Epoch 66, Training Loss: 0.9161366919825847\n",
      "Epoch 67, Training Loss: 0.915717904012006\n",
      "Epoch 68, Training Loss: 0.9152504071257168\n",
      "Epoch 69, Training Loss: 0.9141624431861074\n",
      "Epoch 70, Training Loss: 0.9141025906218622\n",
      "Epoch 71, Training Loss: 0.9131602015710415\n",
      "Epoch 72, Training Loss: 0.912683435579888\n",
      "Epoch 73, Training Loss: 0.9116111879061936\n",
      "Epoch 74, Training Loss: 0.9112985720311789\n",
      "Epoch 75, Training Loss: 0.9107666282725514\n",
      "Epoch 76, Training Loss: 0.9090616118639036\n",
      "Epoch 77, Training Loss: 0.9088861705665302\n",
      "Epoch 78, Training Loss: 0.908367065200232\n",
      "Epoch 79, Training Loss: 0.9074483330088451\n",
      "Epoch 80, Training Loss: 0.906540046240154\n",
      "Epoch 81, Training Loss: 0.9058086997584293\n",
      "Epoch 82, Training Loss: 0.9049822480158698\n",
      "Epoch 83, Training Loss: 0.9046401895974812\n",
      "Epoch 84, Training Loss: 0.9036874171486474\n",
      "Epoch 85, Training Loss: 0.9023985733663229\n",
      "Epoch 86, Training Loss: 0.9012867874668953\n",
      "Epoch 87, Training Loss: 0.9007677296050509\n",
      "Epoch 88, Training Loss: 0.8999083255466662\n",
      "Epoch 89, Training Loss: 0.898785598654496\n",
      "Epoch 90, Training Loss: 0.8976498435314436\n",
      "Epoch 91, Training Loss: 0.896807657327867\n",
      "Epoch 92, Training Loss: 0.896255554202804\n",
      "Epoch 93, Training Loss: 0.8945595744857214\n",
      "Epoch 94, Training Loss: 0.8938965221096699\n",
      "Epoch 95, Training Loss: 0.8926160863467625\n",
      "Epoch 96, Training Loss: 0.890859333823498\n",
      "Epoch 97, Training Loss: 0.8897833490730228\n",
      "Epoch 98, Training Loss: 0.8884222278917643\n",
      "Epoch 99, Training Loss: 0.8873284567567639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:49:42,598] Trial 37 finished with value: 0.5818 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8854962604386466\n",
      "Epoch 1, Training Loss: 1.0609162106233485\n",
      "Epoch 2, Training Loss: 1.0147252188009375\n",
      "Epoch 3, Training Loss: 0.9868279312638676\n",
      "Epoch 4, Training Loss: 0.9711815751300139\n",
      "Epoch 5, Training Loss: 0.9629719944561229\n",
      "Epoch 6, Training Loss: 0.9586096786751467\n",
      "Epoch 7, Training Loss: 0.9560696816444397\n",
      "Epoch 8, Training Loss: 0.9544019455769482\n",
      "Epoch 9, Training Loss: 0.9530038301383748\n",
      "Epoch 10, Training Loss: 0.9517502138193916\n",
      "Epoch 11, Training Loss: 0.9504822641961714\n",
      "Epoch 12, Training Loss: 0.9493614576844608\n",
      "Epoch 13, Training Loss: 0.9482275117144865\n",
      "Epoch 14, Training Loss: 0.946919335267123\n",
      "Epoch 15, Training Loss: 0.9457618423069225\n",
      "Epoch 16, Training Loss: 0.9444282693722669\n",
      "Epoch 17, Training Loss: 0.94321573769345\n",
      "Epoch 18, Training Loss: 0.9418460853660808\n",
      "Epoch 19, Training Loss: 0.9404418399754693\n",
      "Epoch 20, Training Loss: 0.9390916461804334\n",
      "Epoch 21, Training Loss: 0.9376176600596484\n",
      "Epoch 22, Training Loss: 0.9360695291266722\n",
      "Epoch 23, Training Loss: 0.9345273398651797\n",
      "Epoch 24, Training Loss: 0.932942656699349\n",
      "Epoch 25, Training Loss: 0.9313321053981781\n",
      "Epoch 26, Training Loss: 0.9295753594005809\n",
      "Epoch 27, Training Loss: 0.9278121738574084\n",
      "Epoch 28, Training Loss: 0.9260183243891772\n",
      "Epoch 29, Training Loss: 0.9241508481783025\n",
      "Epoch 30, Training Loss: 0.9223490394564235\n",
      "Epoch 31, Training Loss: 0.9203602177255293\n",
      "Epoch 32, Training Loss: 0.9184088714683757\n",
      "Epoch 33, Training Loss: 0.9163733717273264\n",
      "Epoch 34, Training Loss: 0.9143000694583444\n",
      "Epoch 35, Training Loss: 0.9121010069987353\n",
      "Epoch 36, Training Loss: 0.9100170604621662\n",
      "Epoch 37, Training Loss: 0.9076966796201819\n",
      "Epoch 38, Training Loss: 0.9055943606881535\n",
      "Epoch 39, Training Loss: 0.9032995878948885\n",
      "Epoch 40, Training Loss: 0.9010155564897201\n",
      "Epoch 41, Training Loss: 0.8986653971672058\n",
      "Epoch 42, Training Loss: 0.896359330555972\n",
      "Epoch 43, Training Loss: 0.8939768052802366\n",
      "Epoch 44, Training Loss: 0.8916596416164847\n",
      "Epoch 45, Training Loss: 0.8893155254335965\n",
      "Epoch 46, Training Loss: 0.8868828795937931\n",
      "Epoch 47, Training Loss: 0.8845041521857766\n",
      "Epoch 48, Training Loss: 0.8820533834485447\n",
      "Epoch 49, Training Loss: 0.8797678533722373\n",
      "Epoch 50, Training Loss: 0.8774193648029777\n",
      "Epoch 51, Training Loss: 0.8750707758875454\n",
      "Epoch 52, Training Loss: 0.8727196840678945\n",
      "Epoch 53, Training Loss: 0.8705371808304506\n",
      "Epoch 54, Training Loss: 0.8682731796713437\n",
      "Epoch 55, Training Loss: 0.8660151343485888\n",
      "Epoch 56, Training Loss: 0.8638254824105431\n",
      "Epoch 57, Training Loss: 0.8616991925239563\n",
      "Epoch 58, Training Loss: 0.8595727755742915\n",
      "Epoch 59, Training Loss: 0.8574647837526658\n",
      "Epoch 60, Training Loss: 0.8555374834116768\n",
      "Epoch 61, Training Loss: 0.853570357280619\n",
      "Epoch 62, Training Loss: 0.8516587679526385\n",
      "Epoch 63, Training Loss: 0.8498351764678955\n",
      "Epoch 64, Training Loss: 0.8480044295507319\n",
      "Epoch 65, Training Loss: 0.8462347576898687\n",
      "Epoch 66, Training Loss: 0.8446020404030296\n",
      "Epoch 67, Training Loss: 0.8429200525844799\n",
      "Epoch 68, Training Loss: 0.8413781180802513\n",
      "Epoch 69, Training Loss: 0.8398751282691955\n",
      "Epoch 70, Training Loss: 0.8383932783323176\n",
      "Epoch 71, Training Loss: 0.8370208081077127\n",
      "Epoch 72, Training Loss: 0.835650199581595\n",
      "Epoch 73, Training Loss: 0.8343225002990049\n",
      "Epoch 74, Training Loss: 0.8331859504475313\n",
      "Epoch 75, Training Loss: 0.8319588129660662\n",
      "Epoch 76, Training Loss: 0.8308590362352484\n",
      "Epoch 77, Training Loss: 0.8297502878132988\n",
      "Epoch 78, Training Loss: 0.8286604495609508\n",
      "Epoch 79, Training Loss: 0.8276121144434985\n",
      "Epoch 80, Training Loss: 0.826768956675249\n",
      "Epoch 81, Training Loss: 0.8258664730015923\n",
      "Epoch 82, Training Loss: 0.8249862514523899\n",
      "Epoch 83, Training Loss: 0.8241501098520616\n",
      "Epoch 84, Training Loss: 0.8233805457283468\n",
      "Epoch 85, Training Loss: 0.8226381194591522\n",
      "Epoch 86, Training Loss: 0.8219670260653776\n",
      "Epoch 87, Training Loss: 0.8212797314980451\n",
      "Epoch 88, Training Loss: 0.8206137512010686\n",
      "Epoch 89, Training Loss: 0.8201304556341732\n",
      "Epoch 90, Training Loss: 0.8195117901353275\n",
      "Epoch 91, Training Loss: 0.8189543835555806\n",
      "Epoch 92, Training Loss: 0.8184540968782762\n",
      "Epoch 93, Training Loss: 0.8179649154578938\n",
      "Epoch 94, Training Loss: 0.8175444416438832\n",
      "Epoch 95, Training Loss: 0.8170595565964194\n",
      "Epoch 96, Training Loss: 0.81669696913046\n",
      "Epoch 97, Training Loss: 0.8163306409471175\n",
      "Epoch 98, Training Loss: 0.815931183871101\n",
      "Epoch 99, Training Loss: 0.8156003830012153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:51:17,539] Trial 38 finished with value: 0.6242666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8152592529268826\n",
      "Epoch 1, Training Loss: 1.0813516621028676\n",
      "Epoch 2, Training Loss: 1.0597293700891381\n",
      "Epoch 3, Training Loss: 1.0401917179191813\n",
      "Epoch 4, Training Loss: 1.0221852306057424\n",
      "Epoch 5, Training Loss: 1.0067550091182484\n",
      "Epoch 6, Training Loss: 0.9946829066557042\n",
      "Epoch 7, Training Loss: 0.9857153612725875\n",
      "Epoch 8, Training Loss: 0.9790069760995753\n",
      "Epoch 9, Training Loss: 0.9738030885948854\n",
      "Epoch 10, Training Loss: 0.9696306123453028\n",
      "Epoch 11, Training Loss: 0.9662457641433267\n",
      "Epoch 12, Training Loss: 0.9634922382410834\n",
      "Epoch 13, Training Loss: 0.9612503621157478\n",
      "Epoch 14, Training Loss: 0.9593960654735565\n",
      "Epoch 15, Training Loss: 0.9578344934828141\n",
      "Epoch 16, Training Loss: 0.956482182671042\n",
      "Epoch 17, Training Loss: 0.9552547683435327\n",
      "Epoch 18, Training Loss: 0.9540997121614568\n",
      "Epoch 19, Training Loss: 0.9529989848417394\n",
      "Epoch 20, Training Loss: 0.9519039919797112\n",
      "Epoch 21, Training Loss: 0.9508113067991594\n",
      "Epoch 22, Training Loss: 0.9497051424138686\n",
      "Epoch 23, Training Loss: 0.9485956258633558\n",
      "Epoch 24, Training Loss: 0.94744239358341\n",
      "Epoch 25, Training Loss: 0.9462717086427352\n",
      "Epoch 26, Training Loss: 0.9450615723694072\n",
      "Epoch 27, Training Loss: 0.9438105038334341\n",
      "Epoch 28, Training Loss: 0.9425149794887094\n",
      "Epoch 29, Training Loss: 0.9411742607986225\n",
      "Epoch 30, Training Loss: 0.9397781283715192\n",
      "Epoch 31, Training Loss: 0.9383269739852232\n",
      "Epoch 32, Training Loss: 0.9368215955706204\n",
      "Epoch 33, Training Loss: 0.9352443441222695\n",
      "Epoch 34, Training Loss: 0.9335855889320374\n",
      "Epoch 35, Training Loss: 0.9318516635894776\n",
      "Epoch 36, Training Loss: 0.9300534800922169\n",
      "Epoch 37, Training Loss: 0.928139297471327\n",
      "Epoch 38, Training Loss: 0.926130593734629\n",
      "Epoch 39, Training Loss: 0.9240148926482481\n",
      "Epoch 40, Training Loss: 0.9217998407868778\n",
      "Epoch 41, Training Loss: 0.9194363814241746\n",
      "Epoch 42, Training Loss: 0.9169766722707188\n",
      "Epoch 43, Training Loss: 0.9143653739199918\n",
      "Epoch 44, Training Loss: 0.9116214963969063\n",
      "Epoch 45, Training Loss: 0.9087485656317542\n",
      "Epoch 46, Training Loss: 0.9057239514238694\n",
      "Epoch 47, Training Loss: 0.9025708476234885\n",
      "Epoch 48, Training Loss: 0.8992690542866202\n",
      "Epoch 49, Training Loss: 0.8958295927328221\n",
      "Epoch 50, Training Loss: 0.892311282228021\n",
      "Epoch 51, Training Loss: 0.8886766687561484\n",
      "Epoch 52, Training Loss: 0.8849486632206861\n",
      "Epoch 53, Training Loss: 0.8812041066674625\n",
      "Epoch 54, Training Loss: 0.8773975647898281\n",
      "Epoch 55, Training Loss: 0.8736180618230034\n",
      "Epoch 56, Training Loss: 0.8698571444960201\n",
      "Epoch 57, Training Loss: 0.8661605170895071\n",
      "Epoch 58, Training Loss: 0.8625869117063635\n",
      "Epoch 59, Training Loss: 0.8591200034057392\n",
      "Epoch 60, Training Loss: 0.8558017260888043\n",
      "Epoch 61, Training Loss: 0.8526607308668249\n",
      "Epoch 62, Training Loss: 0.849690711848876\n",
      "Epoch 63, Training Loss: 0.8468875372409821\n",
      "Epoch 64, Training Loss: 0.844272482956157\n",
      "Epoch 65, Training Loss: 0.8419672904996311\n",
      "Epoch 66, Training Loss: 0.8397766168678508\n",
      "Epoch 67, Training Loss: 0.8377538175442639\n",
      "Epoch 68, Training Loss: 0.8359294466411367\n",
      "Epoch 69, Training Loss: 0.8342844907676472\n",
      "Epoch 70, Training Loss: 0.8327196119112127\n",
      "Epoch 71, Training Loss: 0.8314159340016982\n",
      "Epoch 72, Training Loss: 0.8301429364260505\n",
      "Epoch 73, Training Loss: 0.8289887135870316\n",
      "Epoch 74, Training Loss: 0.8279652127097634\n",
      "Epoch 75, Training Loss: 0.8269661845179165\n",
      "Epoch 76, Training Loss: 0.8261278838269851\n",
      "Epoch 77, Training Loss: 0.8253430339869331\n",
      "Epoch 78, Training Loss: 0.824544492258745\n",
      "Epoch 79, Training Loss: 0.8238766994897057\n",
      "Epoch 80, Training Loss: 0.823215320741429\n",
      "Epoch 81, Training Loss: 0.8226009544204264\n",
      "Epoch 82, Training Loss: 0.8220197758253883\n",
      "Epoch 83, Training Loss: 0.8213923579103807\n",
      "Epoch 84, Training Loss: 0.8208180857405943\n",
      "Epoch 85, Training Loss: 0.8203690380208633\n",
      "Epoch 86, Training Loss: 0.8198272478580475\n",
      "Epoch 87, Training Loss: 0.8192867325333988\n",
      "Epoch 88, Training Loss: 0.8188406699545243\n",
      "Epoch 89, Training Loss: 0.8184493146223181\n",
      "Epoch 90, Training Loss: 0.8179738471788519\n",
      "Epoch 91, Training Loss: 0.8175479104939629\n",
      "Epoch 92, Training Loss: 0.8171384572281557\n",
      "Epoch 93, Training Loss: 0.8166738906327415\n",
      "Epoch 94, Training Loss: 0.8163115476159488\n",
      "Epoch 95, Training Loss: 0.8159005507300882\n",
      "Epoch 96, Training Loss: 0.8155419609125922\n",
      "Epoch 97, Training Loss: 0.815136866499396\n",
      "Epoch 98, Training Loss: 0.8147661953112658\n",
      "Epoch 99, Training Loss: 0.8144608112643746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:53:15,871] Trial 39 finished with value: 0.6282 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8141510034308714\n",
      "Epoch 1, Training Loss: 0.9118318266438362\n",
      "Epoch 2, Training Loss: 0.8197410784269634\n",
      "Epoch 3, Training Loss: 0.8043939976315749\n",
      "Epoch 4, Training Loss: 0.8000558997007241\n",
      "Epoch 5, Training Loss: 0.7998979111363117\n",
      "Epoch 6, Training Loss: 0.7984138613356684\n",
      "Epoch 7, Training Loss: 0.7961445588366429\n",
      "Epoch 8, Training Loss: 0.7956108533350149\n",
      "Epoch 9, Training Loss: 0.7948954750720719\n",
      "Epoch 10, Training Loss: 0.7931238438850059\n",
      "Epoch 11, Training Loss: 0.7927687106275917\n",
      "Epoch 12, Training Loss: 0.7906081779558856\n",
      "Epoch 13, Training Loss: 0.7901909025988184\n",
      "Epoch 14, Training Loss: 0.7907489367893764\n",
      "Epoch 15, Training Loss: 0.7894231748760194\n",
      "Epoch 16, Training Loss: 0.7900774910037679\n",
      "Epoch 17, Training Loss: 0.7888695652323557\n",
      "Epoch 18, Training Loss: 0.7885035256694134\n",
      "Epoch 19, Training Loss: 0.7884892771118566\n",
      "Epoch 20, Training Loss: 0.7876343145406336\n",
      "Epoch 21, Training Loss: 0.7873499774395075\n",
      "Epoch 22, Training Loss: 0.7868958706246283\n",
      "Epoch 23, Training Loss: 0.7870655177231122\n",
      "Epoch 24, Training Loss: 0.7869186738379916\n",
      "Epoch 25, Training Loss: 0.7871053965468155\n",
      "Epoch 26, Training Loss: 0.7860159700974486\n",
      "Epoch 27, Training Loss: 0.7868052924486031\n",
      "Epoch 28, Training Loss: 0.7865702189897236\n",
      "Epoch 29, Training Loss: 0.7870838907428254\n",
      "Epoch 30, Training Loss: 0.7855393771838425\n",
      "Epoch 31, Training Loss: 0.786223678570941\n",
      "Epoch 32, Training Loss: 0.7853923880067983\n",
      "Epoch 33, Training Loss: 0.785273538137737\n",
      "Epoch 34, Training Loss: 0.7847508562238593\n",
      "Epoch 35, Training Loss: 0.784795776076783\n",
      "Epoch 36, Training Loss: 0.7848649805649779\n",
      "Epoch 37, Training Loss: 0.7840636239015967\n",
      "Epoch 38, Training Loss: 0.784607724229196\n",
      "Epoch 39, Training Loss: 0.783659731535087\n",
      "Epoch 40, Training Loss: 0.7840968649190171\n",
      "Epoch 41, Training Loss: 0.7840449958815611\n",
      "Epoch 42, Training Loss: 0.783158460505923\n",
      "Epoch 43, Training Loss: 0.7835642171981639\n",
      "Epoch 44, Training Loss: 0.7829066799995594\n",
      "Epoch 45, Training Loss: 0.7828047708909315\n",
      "Epoch 46, Training Loss: 0.7821091004332206\n",
      "Epoch 47, Training Loss: 0.7825887854834248\n",
      "Epoch 48, Training Loss: 0.7826349720022732\n",
      "Epoch 49, Training Loss: 0.7828425853772271\n",
      "Epoch 50, Training Loss: 0.7820052898019776\n",
      "Epoch 51, Training Loss: 0.7816231424647166\n",
      "Epoch 52, Training Loss: 0.7816417744285182\n",
      "Epoch 53, Training Loss: 0.780952000707612\n",
      "Epoch 54, Training Loss: 0.7815274319254366\n",
      "Epoch 55, Training Loss: 0.7807529080182987\n",
      "Epoch 56, Training Loss: 0.7813135922403264\n",
      "Epoch 57, Training Loss: 0.781632643803618\n",
      "Epoch 58, Training Loss: 0.7808232092319575\n",
      "Epoch 59, Training Loss: 0.7811006113998872\n",
      "Epoch 60, Training Loss: 0.7814131404224195\n",
      "Epoch 61, Training Loss: 0.780153048755531\n",
      "Epoch 62, Training Loss: 0.7813672371376726\n",
      "Epoch 63, Training Loss: 0.7805196492743671\n",
      "Epoch 64, Training Loss: 0.780002697921337\n",
      "Epoch 65, Training Loss: 0.78070965978436\n",
      "Epoch 66, Training Loss: 0.7800669598400145\n",
      "Epoch 67, Training Loss: 0.7797246902060688\n",
      "Epoch 68, Training Loss: 0.7802756681478114\n",
      "Epoch 69, Training Loss: 0.7800031832286289\n",
      "Epoch 70, Training Loss: 0.7807597674821553\n",
      "Epoch 71, Training Loss: 0.7807554573941051\n",
      "Epoch 72, Training Loss: 0.7797734048133506\n",
      "Epoch 73, Training Loss: 0.7808526936330293\n",
      "Epoch 74, Training Loss: 0.7794807935119572\n",
      "Epoch 75, Training Loss: 0.7797900078888226\n",
      "Epoch 76, Training Loss: 0.7807072729096377\n",
      "Epoch 77, Training Loss: 0.7791546681769809\n",
      "Epoch 78, Training Loss: 0.7797171605260749\n",
      "Epoch 79, Training Loss: 0.779036147343485\n",
      "Epoch 80, Training Loss: 0.7798614363921316\n",
      "Epoch 81, Training Loss: 0.7787487368386491\n",
      "Epoch 82, Training Loss: 0.7795552749382822\n",
      "Epoch 83, Training Loss: 0.7800169916081249\n",
      "Epoch 84, Training Loss: 0.7788411914854121\n",
      "Epoch 85, Training Loss: 0.7794940496745862\n",
      "Epoch 86, Training Loss: 0.7783207501683916\n",
      "Epoch 87, Training Loss: 0.779047447577455\n",
      "Epoch 88, Training Loss: 0.7792215609909\n",
      "Epoch 89, Training Loss: 0.7790790331991095\n",
      "Epoch 90, Training Loss: 0.779362336226872\n",
      "Epoch 91, Training Loss: 0.7791257801808809\n",
      "Epoch 92, Training Loss: 0.7785987357447918\n",
      "Epoch 93, Training Loss: 0.7789293535669943\n",
      "Epoch 94, Training Loss: 0.7784786783662954\n",
      "Epoch 95, Training Loss: 0.7785940255437579\n",
      "Epoch 96, Training Loss: 0.7797112868244487\n",
      "Epoch 97, Training Loss: 0.7789092692217432\n",
      "Epoch 98, Training Loss: 0.778873552415604\n",
      "Epoch 99, Training Loss: 0.778849678738673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:55:08,405] Trial 40 finished with value: 0.6385333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7781242671765779\n",
      "Epoch 1, Training Loss: 0.8713383176272973\n",
      "Epoch 2, Training Loss: 0.8279364624417814\n",
      "Epoch 3, Training Loss: 0.8219354463699169\n",
      "Epoch 4, Training Loss: 0.8186810638671531\n",
      "Epoch 5, Training Loss: 0.8163387554928773\n",
      "Epoch 6, Training Loss: 0.8149720083502002\n",
      "Epoch 7, Training Loss: 0.8126895920674604\n",
      "Epoch 8, Training Loss: 0.8110674610711578\n",
      "Epoch 9, Training Loss: 0.8097541913950353\n",
      "Epoch 10, Training Loss: 0.8097582172630424\n",
      "Epoch 11, Training Loss: 0.809599501028993\n",
      "Epoch 12, Training Loss: 0.8089467707433199\n",
      "Epoch 13, Training Loss: 0.8080537720730431\n",
      "Epoch 14, Training Loss: 0.8066887689712352\n",
      "Epoch 15, Training Loss: 0.8066233684245805\n",
      "Epoch 16, Training Loss: 0.8063497918889039\n",
      "Epoch 17, Training Loss: 0.805840133545094\n",
      "Epoch 18, Training Loss: 0.8053330661658954\n",
      "Epoch 19, Training Loss: 0.806187635794618\n",
      "Epoch 20, Training Loss: 0.8056295758799503\n",
      "Epoch 21, Training Loss: 0.8044591508413615\n",
      "Epoch 22, Training Loss: 0.8044911923265099\n",
      "Epoch 23, Training Loss: 0.8044182357035186\n",
      "Epoch 24, Training Loss: 0.8046639633357973\n",
      "Epoch 25, Training Loss: 0.8053163713082335\n",
      "Epoch 26, Training Loss: 0.8046381375843421\n",
      "Epoch 27, Training Loss: 0.8038277098110744\n",
      "Epoch 28, Training Loss: 0.8045652925520015\n",
      "Epoch 29, Training Loss: 0.8029786024774824\n",
      "Epoch 30, Training Loss: 0.8029408089200357\n",
      "Epoch 31, Training Loss: 0.8025008191739706\n",
      "Epoch 32, Training Loss: 0.8020431238009518\n",
      "Epoch 33, Training Loss: 0.8023935165620388\n",
      "Epoch 34, Training Loss: 0.8021337851545864\n",
      "Epoch 35, Training Loss: 0.80147006664061\n",
      "Epoch 36, Training Loss: 0.8011802831090482\n",
      "Epoch 37, Training Loss: 0.8006545488995717\n",
      "Epoch 38, Training Loss: 0.8005090026030863\n",
      "Epoch 39, Training Loss: 0.8011955912848164\n",
      "Epoch 40, Training Loss: 0.8014412738326797\n",
      "Epoch 41, Training Loss: 0.8010092744253632\n",
      "Epoch 42, Training Loss: 0.8006945522207963\n",
      "Epoch 43, Training Loss: 0.800606516070832\n",
      "Epoch 44, Training Loss: 0.8009636879863595\n",
      "Epoch 45, Training Loss: 0.8004412059497116\n",
      "Epoch 46, Training Loss: 0.8005765078659345\n",
      "Epoch 47, Training Loss: 0.7998125119316847\n",
      "Epoch 48, Training Loss: 0.8000410328234049\n",
      "Epoch 49, Training Loss: 0.8009698742314388\n",
      "Epoch 50, Training Loss: 0.7996178469263522\n",
      "Epoch 51, Training Loss: 0.79916800318804\n",
      "Epoch 52, Training Loss: 0.7995473985385178\n",
      "Epoch 53, Training Loss: 0.8001715078389734\n",
      "Epoch 54, Training Loss: 0.7997707413551502\n",
      "Epoch 55, Training Loss: 0.7990823554813413\n",
      "Epoch 56, Training Loss: 0.7988737238977188\n",
      "Epoch 57, Training Loss: 0.7993926645221566\n",
      "Epoch 58, Training Loss: 0.798344805321299\n",
      "Epoch 59, Training Loss: 0.7994209235772154\n",
      "Epoch 60, Training Loss: 0.7990322080769934\n",
      "Epoch 61, Training Loss: 0.8000126627154817\n",
      "Epoch 62, Training Loss: 0.7993773915713891\n",
      "Epoch 63, Training Loss: 0.7990703913502227\n",
      "Epoch 64, Training Loss: 0.7998688897692171\n",
      "Epoch 65, Training Loss: 0.7997651282109712\n",
      "Epoch 66, Training Loss: 0.7984037384951025\n",
      "Epoch 67, Training Loss: 0.7981654007632033\n",
      "Epoch 68, Training Loss: 0.7984465962962101\n",
      "Epoch 69, Training Loss: 0.7986089691183621\n",
      "Epoch 70, Training Loss: 0.798836382349631\n",
      "Epoch 71, Training Loss: 0.7977574264196525\n",
      "Epoch 72, Training Loss: 0.7981614300182888\n",
      "Epoch 73, Training Loss: 0.7987481179990267\n",
      "Epoch 74, Training Loss: 0.7985082756307789\n",
      "Epoch 75, Training Loss: 0.798666867277676\n",
      "Epoch 76, Training Loss: 0.7980011620019611\n",
      "Epoch 77, Training Loss: 0.7989762536565164\n",
      "Epoch 78, Training Loss: 0.7981837070974193\n",
      "Epoch 79, Training Loss: 0.7988986330821102\n",
      "Epoch 80, Training Loss: 0.7981601006554482\n",
      "Epoch 81, Training Loss: 0.798655439050574\n",
      "Epoch 82, Training Loss: 0.7991143046465136\n",
      "Epoch 83, Training Loss: 0.7982470147591785\n",
      "Epoch 84, Training Loss: 0.7976697640311449\n",
      "Epoch 85, Training Loss: 0.7983164157186236\n",
      "Epoch 86, Training Loss: 0.7976975224071876\n",
      "Epoch 87, Training Loss: 0.796460814718017\n",
      "Epoch 88, Training Loss: 0.7986296686014734\n",
      "Epoch 89, Training Loss: 0.7975438510564933\n",
      "Epoch 90, Training Loss: 0.7977138778320829\n",
      "Epoch 91, Training Loss: 0.7982392943891368\n",
      "Epoch 92, Training Loss: 0.7982527333094661\n",
      "Epoch 93, Training Loss: 0.7975962568942766\n",
      "Epoch 94, Training Loss: 0.7984010379117235\n",
      "Epoch 95, Training Loss: 0.7968371921912172\n",
      "Epoch 96, Training Loss: 0.798592025713813\n",
      "Epoch 97, Training Loss: 0.7989229087542771\n",
      "Epoch 98, Training Loss: 0.7975193837531527\n",
      "Epoch 99, Training Loss: 0.798041223941889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:56:44,637] Trial 41 finished with value: 0.6240666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.797134872845241\n",
      "Epoch 1, Training Loss: 0.891886307870535\n",
      "Epoch 2, Training Loss: 0.8178894469612523\n",
      "Epoch 3, Training Loss: 0.8078566376427959\n",
      "Epoch 4, Training Loss: 0.8042754908253376\n",
      "Epoch 5, Training Loss: 0.8025061384179538\n",
      "Epoch 6, Training Loss: 0.8013938350785047\n",
      "Epoch 7, Training Loss: 0.8003046485714447\n",
      "Epoch 8, Training Loss: 0.7985168556969865\n",
      "Epoch 9, Training Loss: 0.7978029829218871\n",
      "Epoch 10, Training Loss: 0.7970524381874199\n",
      "Epoch 11, Training Loss: 0.7948994005533089\n",
      "Epoch 12, Training Loss: 0.7940023415967038\n",
      "Epoch 13, Training Loss: 0.7929426292727765\n",
      "Epoch 14, Training Loss: 0.792920274125006\n",
      "Epoch 15, Training Loss: 0.7912235585370458\n",
      "Epoch 16, Training Loss: 0.790958335166587\n",
      "Epoch 17, Training Loss: 0.7903655545155804\n",
      "Epoch 18, Training Loss: 0.7906973297434642\n",
      "Epoch 19, Training Loss: 0.7895054084017761\n",
      "Epoch 20, Training Loss: 0.7892500710666628\n",
      "Epoch 21, Training Loss: 0.7889707924728107\n",
      "Epoch 22, Training Loss: 0.7884133819126545\n",
      "Epoch 23, Training Loss: 0.7885784512175653\n",
      "Epoch 24, Training Loss: 0.7882683696603416\n",
      "Epoch 25, Training Loss: 0.7884268891542477\n",
      "Epoch 26, Training Loss: 0.7876073732411951\n",
      "Epoch 27, Training Loss: 0.7878385013207457\n",
      "Epoch 28, Training Loss: 0.7874924663314246\n",
      "Epoch 29, Training Loss: 0.7870115428042591\n",
      "Epoch 30, Training Loss: 0.7861526262939424\n",
      "Epoch 31, Training Loss: 0.7860413375653719\n",
      "Epoch 32, Training Loss: 0.7857427863698256\n",
      "Epoch 33, Training Loss: 0.7856994417376985\n",
      "Epoch 34, Training Loss: 0.7858527961530184\n",
      "Epoch 35, Training Loss: 0.7856841988133308\n",
      "Epoch 36, Training Loss: 0.7850125163121331\n",
      "Epoch 37, Training Loss: 0.7867236011906674\n",
      "Epoch 38, Training Loss: 0.784595315169571\n",
      "Epoch 39, Training Loss: 0.7845847517027891\n",
      "Epoch 40, Training Loss: 0.784741349238202\n",
      "Epoch 41, Training Loss: 0.7842810353838412\n",
      "Epoch 42, Training Loss: 0.7844368650500936\n",
      "Epoch 43, Training Loss: 0.7838073840714935\n",
      "Epoch 44, Training Loss: 0.7844147621240831\n",
      "Epoch 45, Training Loss: 0.7839245897486694\n",
      "Epoch 46, Training Loss: 0.7836218086400426\n",
      "Epoch 47, Training Loss: 0.7832459698942371\n",
      "Epoch 48, Training Loss: 0.7835510039688053\n",
      "Epoch 49, Training Loss: 0.7828133976549134\n",
      "Epoch 50, Training Loss: 0.7838552641689329\n",
      "Epoch 51, Training Loss: 0.7827667437997976\n",
      "Epoch 52, Training Loss: 0.7824367110890553\n",
      "Epoch 53, Training Loss: 0.7820698081998897\n",
      "Epoch 54, Training Loss: 0.7830173160796775\n",
      "Epoch 55, Training Loss: 0.782454312385473\n",
      "Epoch 56, Training Loss: 0.7817931557956495\n",
      "Epoch 57, Training Loss: 0.7822306816739247\n",
      "Epoch 58, Training Loss: 0.7816256544643775\n",
      "Epoch 59, Training Loss: 0.7814855926018909\n",
      "Epoch 60, Training Loss: 0.781748274304813\n",
      "Epoch 61, Training Loss: 0.7811538923055605\n",
      "Epoch 62, Training Loss: 0.7812664543775688\n",
      "Epoch 63, Training Loss: 0.7817033980125772\n",
      "Epoch 64, Training Loss: 0.7807340208749126\n",
      "Epoch 65, Training Loss: 0.7808623519158902\n",
      "Epoch 66, Training Loss: 0.7805969763519173\n",
      "Epoch 67, Training Loss: 0.7816869142360257\n",
      "Epoch 68, Training Loss: 0.7809384529751943\n",
      "Epoch 69, Training Loss: 0.7822920662119872\n",
      "Epoch 70, Training Loss: 0.78029101154858\n",
      "Epoch 71, Training Loss: 0.7801014391551341\n",
      "Epoch 72, Training Loss: 0.7807526013008634\n",
      "Epoch 73, Training Loss: 0.7798629701137543\n",
      "Epoch 74, Training Loss: 0.7805976393527554\n",
      "Epoch 75, Training Loss: 0.780008005647731\n",
      "Epoch 76, Training Loss: 0.7802567265983811\n",
      "Epoch 77, Training Loss: 0.7800975423110159\n",
      "Epoch 78, Training Loss: 0.7799354983451671\n",
      "Epoch 79, Training Loss: 0.7807558928217206\n",
      "Epoch 80, Training Loss: 0.7798813574296192\n",
      "Epoch 81, Training Loss: 0.7802414374243944\n",
      "Epoch 82, Training Loss: 0.7804645350104884\n",
      "Epoch 83, Training Loss: 0.7795714445132061\n",
      "Epoch 84, Training Loss: 0.7797375082073356\n",
      "Epoch 85, Training Loss: 0.7801586267643406\n",
      "Epoch 86, Training Loss: 0.7798655891776981\n",
      "Epoch 87, Training Loss: 0.7795102684121383\n",
      "Epoch 88, Training Loss: 0.7799023394297836\n",
      "Epoch 89, Training Loss: 0.7796559972870619\n",
      "Epoch 90, Training Loss: 0.7799338950250382\n",
      "Epoch 91, Training Loss: 0.7795642272870343\n",
      "Epoch 92, Training Loss: 0.778988952833907\n",
      "Epoch 93, Training Loss: 0.7797184895752067\n",
      "Epoch 94, Training Loss: 0.7793194629195938\n",
      "Epoch 95, Training Loss: 0.7793461946616496\n",
      "Epoch 96, Training Loss: 0.7791302459580558\n",
      "Epoch 97, Training Loss: 0.7798915401437229\n",
      "Epoch 98, Training Loss: 0.7793972098737731\n",
      "Epoch 99, Training Loss: 0.7797785953471535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 20:58:30,823] Trial 42 finished with value: 0.6334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7787880973708361\n",
      "Epoch 1, Training Loss: 0.8603670067356941\n",
      "Epoch 2, Training Loss: 0.8267578730905862\n",
      "Epoch 3, Training Loss: 0.8230349455560957\n",
      "Epoch 4, Training Loss: 0.8187019332907254\n",
      "Epoch 5, Training Loss: 0.8161023122027404\n",
      "Epoch 6, Training Loss: 0.8146585335408835\n",
      "Epoch 7, Training Loss: 0.812843504525665\n",
      "Epoch 8, Training Loss: 0.8114733031817845\n",
      "Epoch 9, Training Loss: 0.8109247222878879\n",
      "Epoch 10, Training Loss: 0.8095020712766432\n",
      "Epoch 11, Training Loss: 0.8090980296296285\n",
      "Epoch 12, Training Loss: 0.8079342376916928\n",
      "Epoch 13, Training Loss: 0.8083991479156609\n",
      "Epoch 14, Training Loss: 0.8076757835266285\n",
      "Epoch 15, Training Loss: 0.807451725992045\n",
      "Epoch 16, Training Loss: 0.8068801161041833\n",
      "Epoch 17, Training Loss: 0.8077053096957673\n",
      "Epoch 18, Training Loss: 0.8071597760781309\n",
      "Epoch 19, Training Loss: 0.8063170878958882\n",
      "Epoch 20, Training Loss: 0.8064696838084916\n",
      "Epoch 21, Training Loss: 0.8062865065452748\n",
      "Epoch 22, Training Loss: 0.806225433833617\n",
      "Epoch 23, Training Loss: 0.8065675843030886\n",
      "Epoch 24, Training Loss: 0.8057578898013983\n",
      "Epoch 25, Training Loss: 0.8050139913881632\n",
      "Epoch 26, Training Loss: 0.8067145940952731\n",
      "Epoch 27, Training Loss: 0.8056136318615504\n",
      "Epoch 28, Training Loss: 0.8058941392073954\n",
      "Epoch 29, Training Loss: 0.8061653025168225\n",
      "Epoch 30, Training Loss: 0.805320165748883\n",
      "Epoch 31, Training Loss: 0.8052760093731988\n",
      "Epoch 32, Training Loss: 0.805279094563391\n",
      "Epoch 33, Training Loss: 0.8064528343372775\n",
      "Epoch 34, Training Loss: 0.805166556960658\n",
      "Epoch 35, Training Loss: 0.8045614913890237\n",
      "Epoch 36, Training Loss: 0.8049840850041325\n",
      "Epoch 37, Training Loss: 0.8051362049310727\n",
      "Epoch 38, Training Loss: 0.8052946205425979\n",
      "Epoch 39, Training Loss: 0.804658229637863\n",
      "Epoch 40, Training Loss: 0.8037254054743544\n",
      "Epoch 41, Training Loss: 0.8044804573059082\n",
      "Epoch 42, Training Loss: 0.8036786981095049\n",
      "Epoch 43, Training Loss: 0.8042315942004211\n",
      "Epoch 44, Training Loss: 0.8034902935637567\n",
      "Epoch 45, Training Loss: 0.8037021517753601\n",
      "Epoch 46, Training Loss: 0.8027922005581677\n",
      "Epoch 47, Training Loss: 0.8026582889090803\n",
      "Epoch 48, Training Loss: 0.8027933480148028\n",
      "Epoch 49, Training Loss: 0.8026050106923382\n",
      "Epoch 50, Training Loss: 0.8022819302135841\n",
      "Epoch 51, Training Loss: 0.8018031299562383\n",
      "Epoch 52, Training Loss: 0.801736179061402\n",
      "Epoch 53, Training Loss: 0.8015093167025343\n",
      "Epoch 54, Training Loss: 0.8013947390972224\n",
      "Epoch 55, Training Loss: 0.8009919579764058\n",
      "Epoch 56, Training Loss: 0.8010989667777728\n",
      "Epoch 57, Training Loss: 0.8016353211008517\n",
      "Epoch 58, Training Loss: 0.8011299217553963\n",
      "Epoch 59, Training Loss: 0.8004922811250041\n",
      "Epoch 60, Training Loss: 0.8005495571552362\n",
      "Epoch 61, Training Loss: 0.7998404540513692\n",
      "Epoch 62, Training Loss: 0.799491206057986\n",
      "Epoch 63, Training Loss: 0.8005728265396634\n",
      "Epoch 64, Training Loss: 0.7994203679543689\n",
      "Epoch 65, Training Loss: 0.7998951483489876\n",
      "Epoch 66, Training Loss: 0.7992587708889094\n",
      "Epoch 67, Training Loss: 0.7996249834397682\n",
      "Epoch 68, Training Loss: 0.7988977213551227\n",
      "Epoch 69, Training Loss: 0.7994781197461867\n",
      "Epoch 70, Training Loss: 0.7988407567927712\n",
      "Epoch 71, Training Loss: 0.7985777546588639\n",
      "Epoch 72, Training Loss: 0.7988932254619168\n",
      "Epoch 73, Training Loss: 0.7981980418800412\n",
      "Epoch 74, Training Loss: 0.7981902556759971\n",
      "Epoch 75, Training Loss: 0.7987311319301003\n",
      "Epoch 76, Training Loss: 0.7986224995519882\n",
      "Epoch 77, Training Loss: 0.7985094084775537\n",
      "Epoch 78, Training Loss: 0.7980497292109898\n",
      "Epoch 79, Training Loss: 0.7984980851187742\n",
      "Epoch 80, Training Loss: 0.7977447812718557\n",
      "Epoch 81, Training Loss: 0.7974843518178266\n",
      "Epoch 82, Training Loss: 0.7979998753483134\n",
      "Epoch 83, Training Loss: 0.7976154943157856\n",
      "Epoch 84, Training Loss: 0.7969053041666074\n",
      "Epoch 85, Training Loss: 0.7971190286758251\n",
      "Epoch 86, Training Loss: 0.7975662450145061\n",
      "Epoch 87, Training Loss: 0.7975766599626469\n",
      "Epoch 88, Training Loss: 0.7974712733935593\n",
      "Epoch 89, Training Loss: 0.7968152184235422\n",
      "Epoch 90, Training Loss: 0.7969393135013437\n",
      "Epoch 91, Training Loss: 0.7967412657307503\n",
      "Epoch 92, Training Loss: 0.7963945213117097\n",
      "Epoch 93, Training Loss: 0.796552162421377\n",
      "Epoch 94, Training Loss: 0.7965700883614389\n",
      "Epoch 95, Training Loss: 0.7967038012985\n",
      "Epoch 96, Training Loss: 0.796338807460957\n",
      "Epoch 97, Training Loss: 0.7956276672227042\n",
      "Epoch 98, Training Loss: 0.7955411286730515\n",
      "Epoch 99, Training Loss: 0.7961223928551925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:00:04,946] Trial 43 finished with value: 0.6061333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7958801039179465\n",
      "Epoch 1, Training Loss: 0.84992327115115\n",
      "Epoch 2, Training Loss: 0.818792088803123\n",
      "Epoch 3, Training Loss: 0.8152057474500993\n",
      "Epoch 4, Training Loss: 0.8116587224427392\n",
      "Epoch 5, Training Loss: 0.8166829051690944\n",
      "Epoch 6, Training Loss: 0.8151591743441189\n",
      "Epoch 7, Training Loss: 0.809773368064095\n",
      "Epoch 8, Training Loss: 0.8112325598211849\n",
      "Epoch 9, Training Loss: 0.8120298697667964\n",
      "Epoch 10, Training Loss: 0.8110375111944536\n",
      "Epoch 11, Training Loss: 0.8101337932137882\n",
      "Epoch 12, Training Loss: 0.8080161080640905\n",
      "Epoch 13, Training Loss: 0.8082910476011388\n",
      "Epoch 14, Training Loss: 0.8115570226136376\n",
      "Epoch 15, Training Loss: 0.8097106920270358\n",
      "Epoch 16, Training Loss: 0.8076356490219341\n",
      "Epoch 17, Training Loss: 0.809033416299259\n",
      "Epoch 18, Training Loss: 0.812702092703651\n",
      "Epoch 19, Training Loss: 0.8068540460923139\n",
      "Epoch 20, Training Loss: 0.8095953685395858\n",
      "Epoch 21, Training Loss: 0.8062773212264566\n",
      "Epoch 22, Training Loss: 0.8076335933629204\n",
      "Epoch 23, Training Loss: 0.8086686205162722\n",
      "Epoch 24, Training Loss: 0.8110891457164989\n",
      "Epoch 25, Training Loss: 0.8093640967677621\n",
      "Epoch 26, Training Loss: 0.8067500267309301\n",
      "Epoch 27, Training Loss: 0.8071745102545794\n",
      "Epoch 28, Training Loss: 0.8090981303243077\n",
      "Epoch 29, Training Loss: 0.8087155091061312\n",
      "Epoch 30, Training Loss: 0.8086839720782112\n",
      "Epoch 31, Training Loss: 0.810813548635034\n",
      "Epoch 32, Training Loss: 0.8068460190997404\n",
      "Epoch 33, Training Loss: 0.8064493990645689\n",
      "Epoch 34, Training Loss: 0.808777073691873\n",
      "Epoch 35, Training Loss: 0.8077389441518222\n",
      "Epoch 36, Training Loss: 0.8084769994371077\n",
      "Epoch 37, Training Loss: 0.8092665017352385\n",
      "Epoch 38, Training Loss: 0.8078728337848887\n",
      "Epoch 39, Training Loss: 0.8089629679567674\n",
      "Epoch 40, Training Loss: 0.8069470465183258\n",
      "Epoch 41, Training Loss: 0.8114007840437047\n",
      "Epoch 42, Training Loss: 0.8073991938198314\n",
      "Epoch 43, Training Loss: 0.8077207606680253\n",
      "Epoch 44, Training Loss: 0.8088451147079467\n",
      "Epoch 45, Training Loss: 0.8083724319233614\n",
      "Epoch 46, Training Loss: 0.80887221995522\n",
      "Epoch 47, Training Loss: 0.808256231546402\n",
      "Epoch 48, Training Loss: 0.8075678639552173\n",
      "Epoch 49, Training Loss: 0.8068414693018969\n",
      "Epoch 50, Training Loss: 0.8093061217139749\n",
      "Epoch 51, Training Loss: 0.8082668699937708\n",
      "Epoch 52, Training Loss: 0.8092588445018319\n",
      "Epoch 53, Training Loss: 0.8086963150080513\n",
      "Epoch 54, Training Loss: 0.8088630918194266\n",
      "Epoch 55, Training Loss: 0.8086341797604281\n",
      "Epoch 56, Training Loss: 0.8074905647951014\n",
      "Epoch 57, Training Loss: 0.8062428368540371\n",
      "Epoch 58, Training Loss: 0.812897628475638\n",
      "Epoch 59, Training Loss: 0.8106095821015975\n",
      "Epoch 60, Training Loss: 0.8105643191057093\n",
      "Epoch 61, Training Loss: 0.8092192364440245\n",
      "Epoch 62, Training Loss: 0.8085703396095949\n",
      "Epoch 63, Training Loss: 0.8079720284658319\n",
      "Epoch 64, Training Loss: 0.812151299013811\n",
      "Epoch 65, Training Loss: 0.8079360935968511\n",
      "Epoch 66, Training Loss: 0.8073270074058981\n",
      "Epoch 67, Training Loss: 0.8083632220941431\n",
      "Epoch 68, Training Loss: 0.8093759022740756\n",
      "Epoch 69, Training Loss: 0.8100574119652019\n",
      "Epoch 70, Training Loss: 0.8085181793745826\n",
      "Epoch 71, Training Loss: 0.8094557656260097\n",
      "Epoch 72, Training Loss: 0.8111111687912661\n",
      "Epoch 73, Training Loss: 0.8084333807580611\n",
      "Epoch 74, Training Loss: 0.8105894174295313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:01:45,586] Trial 44 finished with value: 0.608 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8132002907640794\n",
      "Epoch 1, Training Loss: 0.8420676433950438\n",
      "Epoch 2, Training Loss: 0.8175475566010726\n",
      "Epoch 3, Training Loss: 0.8120373817314779\n",
      "Epoch 4, Training Loss: 0.811042485022007\n",
      "Epoch 5, Training Loss: 0.8117919089202594\n",
      "Epoch 6, Training Loss: 0.808350790443277\n",
      "Epoch 7, Training Loss: 0.8050255862393774\n",
      "Epoch 8, Training Loss: 0.8056121326030645\n",
      "Epoch 9, Training Loss: 0.8051270885575087\n",
      "Epoch 10, Training Loss: 0.804545892001991\n",
      "Epoch 11, Training Loss: 0.8039058160064811\n",
      "Epoch 12, Training Loss: 0.8044962536123462\n",
      "Epoch 13, Training Loss: 0.8012567628595165\n",
      "Epoch 14, Training Loss: 0.8026077252581604\n",
      "Epoch 15, Training Loss: 0.8022046609032423\n",
      "Epoch 16, Training Loss: 0.8037891487429912\n",
      "Epoch 17, Training Loss: 0.8021834533913691\n",
      "Epoch 18, Training Loss: 0.8012574749781673\n",
      "Epoch 19, Training Loss: 0.8011877978654732\n",
      "Epoch 20, Training Loss: 0.8019863869910849\n",
      "Epoch 21, Training Loss: 0.802001714975314\n",
      "Epoch 22, Training Loss: 0.7996206945942756\n",
      "Epoch 23, Training Loss: 0.8000491749971432\n",
      "Epoch 24, Training Loss: 0.8002680545462701\n",
      "Epoch 25, Training Loss: 0.8005160087929633\n",
      "Epoch 26, Training Loss: 0.8007572368571633\n",
      "Epoch 27, Training Loss: 0.7996637031548005\n",
      "Epoch 28, Training Loss: 0.7997956690931679\n",
      "Epoch 29, Training Loss: 0.8003977310388608\n",
      "Epoch 30, Training Loss: 0.7995490426407721\n",
      "Epoch 31, Training Loss: 0.8013025646819207\n",
      "Epoch 32, Training Loss: 0.7982603653929288\n",
      "Epoch 33, Training Loss: 0.7992698536779648\n",
      "Epoch 34, Training Loss: 0.79998375742059\n",
      "Epoch 35, Training Loss: 0.7984306893850628\n",
      "Epoch 36, Training Loss: 0.7988865653375038\n",
      "Epoch 37, Training Loss: 0.7979472754593182\n",
      "Epoch 38, Training Loss: 0.7993207951237384\n",
      "Epoch 39, Training Loss: 0.7993655021925617\n",
      "Epoch 40, Training Loss: 0.7983907909321606\n",
      "Epoch 41, Training Loss: 0.7983745046128008\n",
      "Epoch 42, Training Loss: 0.7969154477119446\n",
      "Epoch 43, Training Loss: 0.7976006342952413\n",
      "Epoch 44, Training Loss: 0.7979975385773451\n",
      "Epoch 45, Training Loss: 0.797497932892993\n",
      "Epoch 46, Training Loss: 0.7969652941352443\n",
      "Epoch 47, Training Loss: 0.7993205816225898\n",
      "Epoch 48, Training Loss: 0.7974180077251635\n",
      "Epoch 49, Training Loss: 0.7966060897461453\n",
      "Epoch 50, Training Loss: 0.7974701302392142\n",
      "Epoch 51, Training Loss: 0.7979124534398989\n",
      "Epoch 52, Training Loss: 0.7957506830530955\n",
      "Epoch 53, Training Loss: 0.7978218633429448\n",
      "Epoch 54, Training Loss: 0.7964300753478717\n",
      "Epoch 55, Training Loss: 0.7954933088076742\n",
      "Epoch 56, Training Loss: 0.7966947544338112\n",
      "Epoch 57, Training Loss: 0.7962853662053445\n",
      "Epoch 58, Training Loss: 0.7972693376075056\n",
      "Epoch 59, Training Loss: 0.795402849706492\n",
      "Epoch 60, Training Loss: 0.7956712119561389\n",
      "Epoch 61, Training Loss: 0.796769480687335\n",
      "Epoch 62, Training Loss: 0.7971139912318467\n",
      "Epoch 63, Training Loss: 0.7965219091652032\n",
      "Epoch 64, Training Loss: 0.796651998498386\n",
      "Epoch 65, Training Loss: 0.7960730817981232\n",
      "Epoch 66, Training Loss: 0.7968838040990041\n",
      "Epoch 67, Training Loss: 0.7961213640700605\n",
      "Epoch 68, Training Loss: 0.7967019813401358\n",
      "Epoch 69, Training Loss: 0.7964937081014304\n",
      "Epoch 70, Training Loss: 0.7955327461536665\n",
      "Epoch 71, Training Loss: 0.7959609046018213\n",
      "Epoch 72, Training Loss: 0.7960938575572537\n",
      "Epoch 73, Training Loss: 0.7965382753458238\n",
      "Epoch 74, Training Loss: 0.7964735207701088\n",
      "Epoch 75, Training Loss: 0.7964495068205927\n",
      "Epoch 76, Training Loss: 0.7968025195867495\n",
      "Epoch 77, Training Loss: 0.7960951739684083\n",
      "Epoch 78, Training Loss: 0.795881252001999\n",
      "Epoch 79, Training Loss: 0.7969391466979694\n",
      "Epoch 80, Training Loss: 0.7968007601293406\n",
      "Epoch 81, Training Loss: 0.7963514092273282\n",
      "Epoch 82, Training Loss: 0.7950421321660952\n",
      "Epoch 83, Training Loss: 0.7966586653451274\n",
      "Epoch 84, Training Loss: 0.7970129688879601\n",
      "Epoch 85, Training Loss: 0.7951957066256301\n",
      "Epoch 86, Training Loss: 0.7966986558491126\n",
      "Epoch 87, Training Loss: 0.7948777560900925\n",
      "Epoch 88, Training Loss: 0.7966406734366166\n",
      "Epoch 89, Training Loss: 0.7951898336410522\n",
      "Epoch 90, Training Loss: 0.79472195645024\n",
      "Epoch 91, Training Loss: 0.795255158001319\n",
      "Epoch 92, Training Loss: 0.7959762716651859\n",
      "Epoch 93, Training Loss: 0.7965887945397456\n",
      "Epoch 94, Training Loss: 0.7956674718319026\n",
      "Epoch 95, Training Loss: 0.7958250380100165\n",
      "Epoch 96, Training Loss: 0.7967420865718583\n",
      "Epoch 97, Training Loss: 0.7955682538505784\n",
      "Epoch 98, Training Loss: 0.7946447773087294\n",
      "Epoch 99, Training Loss: 0.795565002215536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:03:26,045] Trial 45 finished with value: 0.6373333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7954543719614359\n",
      "Epoch 1, Training Loss: 0.8941116538468529\n",
      "Epoch 2, Training Loss: 0.8171536992577946\n",
      "Epoch 3, Training Loss: 0.8123393303506514\n",
      "Epoch 4, Training Loss: 0.809811787815655\n",
      "Epoch 5, Training Loss: 0.8076015480125651\n",
      "Epoch 6, Training Loss: 0.8072746933207793\n",
      "Epoch 7, Training Loss: 0.8070131517157835\n",
      "Epoch 8, Training Loss: 0.8061126991580514\n",
      "Epoch 9, Training Loss: 0.805586436215569\n",
      "Epoch 10, Training Loss: 0.8047669555860407\n",
      "Epoch 11, Training Loss: 0.8039993837300469\n",
      "Epoch 12, Training Loss: 0.8031739289620343\n",
      "Epoch 13, Training Loss: 0.8045048126052408\n",
      "Epoch 14, Training Loss: 0.8034074523869683\n",
      "Epoch 15, Training Loss: 0.8024444359190324\n",
      "Epoch 16, Training Loss: 0.8032914843980004\n",
      "Epoch 17, Training Loss: 0.8024200525704552\n",
      "Epoch 18, Training Loss: 0.8025953977949479\n",
      "Epoch 19, Training Loss: 0.802725127023809\n",
      "Epoch 20, Training Loss: 0.8016905161913703\n",
      "Epoch 21, Training Loss: 0.8024125543061424\n",
      "Epoch 22, Training Loss: 0.8016925110536464\n",
      "Epoch 23, Training Loss: 0.8021713705623851\n",
      "Epoch 24, Training Loss: 0.8013958768984851\n",
      "Epoch 25, Training Loss: 0.8009184636087978\n",
      "Epoch 26, Training Loss: 0.8005845968162312\n",
      "Epoch 27, Training Loss: 0.8014418292045593\n",
      "Epoch 28, Training Loss: 0.8010150773384992\n",
      "Epoch 29, Training Loss: 0.8005205113747541\n",
      "Epoch 30, Training Loss: 0.8007350096983068\n",
      "Epoch 31, Training Loss: 0.8006959452348597\n",
      "Epoch 32, Training Loss: 0.8001108442334568\n",
      "Epoch 33, Training Loss: 0.8000138251220479\n",
      "Epoch 34, Training Loss: 0.7995759018729715\n",
      "Epoch 35, Training Loss: 0.799563325994155\n",
      "Epoch 36, Training Loss: 0.7995475809013143\n",
      "Epoch 37, Training Loss: 0.799573604850208\n",
      "Epoch 38, Training Loss: 0.8001750159263611\n",
      "Epoch 39, Training Loss: 0.7988491227346308\n",
      "Epoch 40, Training Loss: 0.7995367008097032\n",
      "Epoch 41, Training Loss: 0.7995242120939142\n",
      "Epoch 42, Training Loss: 0.7992175327329074\n",
      "Epoch 43, Training Loss: 0.7989516980507795\n",
      "Epoch 44, Training Loss: 0.7997676453870886\n",
      "Epoch 45, Training Loss: 0.7984512425871456\n",
      "Epoch 46, Training Loss: 0.7981949205959544\n",
      "Epoch 47, Training Loss: 0.7975784744935878\n",
      "Epoch 48, Training Loss: 0.7979806509438683\n",
      "Epoch 49, Training Loss: 0.7972105141948251\n",
      "Epoch 50, Training Loss: 0.797486035192714\n",
      "Epoch 51, Training Loss: 0.797458635708865\n",
      "Epoch 52, Training Loss: 0.7969594529797049\n",
      "Epoch 53, Training Loss: 0.7970321262584014\n",
      "Epoch 54, Training Loss: 0.796636332133237\n",
      "Epoch 55, Training Loss: 0.79673368082327\n",
      "Epoch 56, Training Loss: 0.7967685867057127\n",
      "Epoch 57, Training Loss: 0.796592734561247\n",
      "Epoch 58, Training Loss: 0.7959221454928903\n",
      "Epoch 59, Training Loss: 0.7966591813985039\n",
      "Epoch 60, Training Loss: 0.7953685811687918\n",
      "Epoch 61, Training Loss: 0.7961263424508712\n",
      "Epoch 62, Training Loss: 0.7959933618938222\n",
      "Epoch 63, Training Loss: 0.7954767280466416\n",
      "Epoch 64, Training Loss: 0.7957163563896628\n",
      "Epoch 65, Training Loss: 0.7963148168255301\n",
      "Epoch 66, Training Loss: 0.7950782563405878\n",
      "Epoch 67, Training Loss: 0.7955202888039982\n",
      "Epoch 68, Training Loss: 0.7946526356304393\n",
      "Epoch 69, Training Loss: 0.7946201871423161\n",
      "Epoch 70, Training Loss: 0.7948834767061121\n",
      "Epoch 71, Training Loss: 0.7947114085449892\n",
      "Epoch 72, Training Loss: 0.7941464823835036\n",
      "Epoch 73, Training Loss: 0.7948926583458396\n",
      "Epoch 74, Training Loss: 0.7944284221705269\n",
      "Epoch 75, Training Loss: 0.7943460291974684\n",
      "Epoch 76, Training Loss: 0.7941813558690688\n",
      "Epoch 77, Training Loss: 0.7937640758822946\n",
      "Epoch 78, Training Loss: 0.7935272761653451\n",
      "Epoch 79, Training Loss: 0.7936589089561911\n",
      "Epoch 80, Training Loss: 0.7936993252529817\n",
      "Epoch 81, Training Loss: 0.793650645087747\n",
      "Epoch 82, Training Loss: 0.7938235676989835\n",
      "Epoch 83, Training Loss: 0.7934575631338008\n",
      "Epoch 84, Training Loss: 0.7932024267140557\n",
      "Epoch 85, Training Loss: 0.7938788481319652\n",
      "Epoch 86, Training Loss: 0.7930618930564207\n",
      "Epoch 87, Training Loss: 0.7926454321075889\n",
      "Epoch 88, Training Loss: 0.7932336934173808\n",
      "Epoch 89, Training Loss: 0.7926204421239741\n",
      "Epoch 90, Training Loss: 0.7930557795131907\n",
      "Epoch 91, Training Loss: 0.7927422286482418\n",
      "Epoch 92, Training Loss: 0.792947133919772\n",
      "Epoch 93, Training Loss: 0.792274376785054\n",
      "Epoch 94, Training Loss: 0.7920935980011435\n",
      "Epoch 95, Training Loss: 0.7922604570669286\n",
      "Epoch 96, Training Loss: 0.7923035507342394\n",
      "Epoch 97, Training Loss: 0.792147658221862\n",
      "Epoch 98, Training Loss: 0.7924935469206642\n",
      "Epoch 99, Training Loss: 0.7919198416962343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:05:19,787] Trial 46 finished with value: 0.6362666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7917295820572797\n",
      "Epoch 1, Training Loss: 0.9017731706898912\n",
      "Epoch 2, Training Loss: 0.8285176425948179\n",
      "Epoch 3, Training Loss: 0.8199384998558159\n",
      "Epoch 4, Training Loss: 0.8172732810328778\n",
      "Epoch 5, Training Loss: 0.8137768936336489\n",
      "Epoch 6, Training Loss: 0.8128645002393794\n",
      "Epoch 7, Training Loss: 0.8112797461954274\n",
      "Epoch 8, Training Loss: 0.808848408962551\n",
      "Epoch 9, Training Loss: 0.8094159098496114\n",
      "Epoch 10, Training Loss: 0.8079063465720728\n",
      "Epoch 11, Training Loss: 0.8078999050577781\n",
      "Epoch 12, Training Loss: 0.8065481120482423\n",
      "Epoch 13, Training Loss: 0.806148262041852\n",
      "Epoch 14, Training Loss: 0.8062247464531347\n",
      "Epoch 15, Training Loss: 0.805581335555342\n",
      "Epoch 16, Training Loss: 0.8047035989904763\n",
      "Epoch 17, Training Loss: 0.8046346672495505\n",
      "Epoch 18, Training Loss: 0.8042034738942196\n",
      "Epoch 19, Training Loss: 0.8041430724294563\n",
      "Epoch 20, Training Loss: 0.80317202053572\n",
      "Epoch 21, Training Loss: 0.8037556246707314\n",
      "Epoch 22, Training Loss: 0.8034882269407574\n",
      "Epoch 23, Training Loss: 0.8034530629789023\n",
      "Epoch 24, Training Loss: 0.8017374506570343\n",
      "Epoch 25, Training Loss: 0.8020088169807779\n",
      "Epoch 26, Training Loss: 0.8020854910513512\n",
      "Epoch 27, Training Loss: 0.80147335753405\n",
      "Epoch 28, Training Loss: 0.8013750716259606\n",
      "Epoch 29, Training Loss: 0.8013861841725227\n",
      "Epoch 30, Training Loss: 0.800547318709524\n",
      "Epoch 31, Training Loss: 0.8001365951129369\n",
      "Epoch 32, Training Loss: 0.7999289712511507\n",
      "Epoch 33, Training Loss: 0.8004867887138424\n",
      "Epoch 34, Training Loss: 0.7995668864787969\n",
      "Epoch 35, Training Loss: 0.7988160556420347\n",
      "Epoch 36, Training Loss: 0.7992185489575666\n",
      "Epoch 37, Training Loss: 0.7986550148268391\n",
      "Epoch 38, Training Loss: 0.7982699164770599\n",
      "Epoch 39, Training Loss: 0.7995244477924548\n",
      "Epoch 40, Training Loss: 0.798348857944173\n",
      "Epoch 41, Training Loss: 0.798677628918698\n",
      "Epoch 42, Training Loss: 0.797948648338031\n",
      "Epoch 43, Training Loss: 0.7970334231405329\n",
      "Epoch 44, Training Loss: 0.7974531539400718\n",
      "Epoch 45, Training Loss: 0.796729020920015\n",
      "Epoch 46, Training Loss: 0.796486070640105\n",
      "Epoch 47, Training Loss: 0.7970729130551331\n",
      "Epoch 48, Training Loss: 0.7963145932756869\n",
      "Epoch 49, Training Loss: 0.7966065042897275\n",
      "Epoch 50, Training Loss: 0.7957921742496634\n",
      "Epoch 51, Training Loss: 0.7955516306081213\n",
      "Epoch 52, Training Loss: 0.7959811128171763\n",
      "Epoch 53, Training Loss: 0.795748906476157\n",
      "Epoch 54, Training Loss: 0.7954035932856395\n",
      "Epoch 55, Training Loss: 0.7947736995560782\n",
      "Epoch 56, Training Loss: 0.7949657558498526\n",
      "Epoch 57, Training Loss: 0.7946422372545515\n",
      "Epoch 58, Training Loss: 0.7949091362773923\n",
      "Epoch 59, Training Loss: 0.7942522337562159\n",
      "Epoch 60, Training Loss: 0.7944469890200105\n",
      "Epoch 61, Training Loss: 0.7940745801853955\n",
      "Epoch 62, Training Loss: 0.7942304376372717\n",
      "Epoch 63, Training Loss: 0.7941397966298842\n",
      "Epoch 64, Training Loss: 0.7938948561374406\n",
      "Epoch 65, Training Loss: 0.7935672845159258\n",
      "Epoch 66, Training Loss: 0.7932802082004403\n",
      "Epoch 67, Training Loss: 0.7946106123744994\n",
      "Epoch 68, Training Loss: 0.793670023742475\n",
      "Epoch 69, Training Loss: 0.7937408332537887\n",
      "Epoch 70, Training Loss: 0.7934301411298881\n",
      "Epoch 71, Training Loss: 0.7935260173969699\n",
      "Epoch 72, Training Loss: 0.7933481114251273\n",
      "Epoch 73, Training Loss: 0.7933721731479902\n",
      "Epoch 74, Training Loss: 0.7928137731731386\n",
      "Epoch 75, Training Loss: 0.7933843531106648\n",
      "Epoch 76, Training Loss: 0.7930525483941673\n",
      "Epoch 77, Training Loss: 0.7931060388572234\n",
      "Epoch 78, Training Loss: 0.7922798828971117\n",
      "Epoch 79, Training Loss: 0.7924340352976232\n",
      "Epoch 80, Training Loss: 0.7926989699664869\n",
      "Epoch 81, Training Loss: 0.7922031597983569\n",
      "Epoch 82, Training Loss: 0.7920252546331936\n",
      "Epoch 83, Training Loss: 0.7914800909228791\n",
      "Epoch 84, Training Loss: 0.7919530336121867\n",
      "Epoch 85, Training Loss: 0.7918547240415014\n",
      "Epoch 86, Training Loss: 0.7929618929561816\n",
      "Epoch 87, Training Loss: 0.7921604433454069\n",
      "Epoch 88, Training Loss: 0.791542413091301\n",
      "Epoch 89, Training Loss: 0.7918671132030344\n",
      "Epoch 90, Training Loss: 0.7916427560318682\n",
      "Epoch 91, Training Loss: 0.7920095583550015\n",
      "Epoch 92, Training Loss: 0.7909244958619426\n",
      "Epoch 93, Training Loss: 0.7917612023819658\n",
      "Epoch 94, Training Loss: 0.7915005926799057\n",
      "Epoch 95, Training Loss: 0.7920825009059189\n",
      "Epoch 96, Training Loss: 0.7911687464642345\n",
      "Epoch 97, Training Loss: 0.7909649617689892\n",
      "Epoch 98, Training Loss: 0.790843679582266\n",
      "Epoch 99, Training Loss: 0.7913361989465871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:06:56,572] Trial 47 finished with value: 0.5954 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7911041579748455\n",
      "Epoch 1, Training Loss: 0.8620370610315997\n",
      "Epoch 2, Training Loss: 0.8279384583458864\n",
      "Epoch 3, Training Loss: 0.8246878329972576\n",
      "Epoch 4, Training Loss: 0.8200462639779973\n",
      "Epoch 5, Training Loss: 0.8196226137025016\n",
      "Epoch 6, Training Loss: 0.8154946573694846\n",
      "Epoch 7, Training Loss: 0.8136472474363513\n",
      "Epoch 8, Training Loss: 0.8119265554542828\n",
      "Epoch 9, Training Loss: 0.8111563267564414\n",
      "Epoch 10, Training Loss: 0.8096647944665493\n",
      "Epoch 11, Training Loss: 0.8092960709916022\n",
      "Epoch 12, Training Loss: 0.8083802474172492\n",
      "Epoch 13, Training Loss: 0.8079920762463619\n",
      "Epoch 14, Training Loss: 0.8080080991400812\n",
      "Epoch 15, Training Loss: 0.8070118616398115\n",
      "Epoch 16, Training Loss: 0.807108928207168\n",
      "Epoch 17, Training Loss: 0.8070371927175307\n",
      "Epoch 18, Training Loss: 0.8070262006350926\n",
      "Epoch 19, Training Loss: 0.8066946927766154\n",
      "Epoch 20, Training Loss: 0.8054269757934083\n",
      "Epoch 21, Training Loss: 0.8053849482446684\n",
      "Epoch 22, Training Loss: 0.8063102136877246\n",
      "Epoch 23, Training Loss: 0.8057434521223369\n",
      "Epoch 24, Training Loss: 0.8056937804795745\n",
      "Epoch 25, Training Loss: 0.806320277461432\n",
      "Epoch 26, Training Loss: 0.8051667915251022\n",
      "Epoch 27, Training Loss: 0.804264900110718\n",
      "Epoch 28, Training Loss: 0.8052019820177465\n",
      "Epoch 29, Training Loss: 0.8050537763681627\n",
      "Epoch 30, Training Loss: 0.8040322375028653\n",
      "Epoch 31, Training Loss: 0.8050575534203895\n",
      "Epoch 32, Training Loss: 0.8050638235601267\n",
      "Epoch 33, Training Loss: 0.8044717881016266\n",
      "Epoch 34, Training Loss: 0.8039909162019429\n",
      "Epoch 35, Training Loss: 0.8041701968451191\n",
      "Epoch 36, Training Loss: 0.8039406192930121\n",
      "Epoch 37, Training Loss: 0.8032019445770665\n",
      "Epoch 38, Training Loss: 0.8041172462298458\n",
      "Epoch 39, Training Loss: 0.8038560372546203\n",
      "Epoch 40, Training Loss: 0.8033779263496399\n",
      "Epoch 41, Training Loss: 0.8025637900918946\n",
      "Epoch 42, Training Loss: 0.8031344232702614\n",
      "Epoch 43, Training Loss: 0.8025959132757402\n",
      "Epoch 44, Training Loss: 0.8019758557914791\n",
      "Epoch 45, Training Loss: 0.8023977034970333\n",
      "Epoch 46, Training Loss: 0.8020664291274279\n",
      "Epoch 47, Training Loss: 0.8015642673449409\n",
      "Epoch 48, Training Loss: 0.8014625953552418\n",
      "Epoch 49, Training Loss: 0.8018420820845698\n",
      "Epoch 50, Training Loss: 0.8013924541329979\n",
      "Epoch 51, Training Loss: 0.8014055483323291\n",
      "Epoch 52, Training Loss: 0.800396638526056\n",
      "Epoch 53, Training Loss: 0.8009481050914391\n",
      "Epoch 54, Training Loss: 0.8007151661062599\n",
      "Epoch 55, Training Loss: 0.8008473731521377\n",
      "Epoch 56, Training Loss: 0.8006684137466259\n",
      "Epoch 57, Training Loss: 0.8012992951206694\n",
      "Epoch 58, Training Loss: 0.800466123290528\n",
      "Epoch 59, Training Loss: 0.8001524354282178\n",
      "Epoch 60, Training Loss: 0.799961631459401\n",
      "Epoch 61, Training Loss: 0.7996821714523143\n",
      "Epoch 62, Training Loss: 0.7998379734225739\n",
      "Epoch 63, Training Loss: 0.7998308923011436\n",
      "Epoch 64, Training Loss: 0.7992988699360898\n",
      "Epoch 65, Training Loss: 0.7999771775159621\n",
      "Epoch 66, Training Loss: 0.7995545484965905\n",
      "Epoch 67, Training Loss: 0.7992800760986214\n",
      "Epoch 68, Training Loss: 0.7999438102083994\n",
      "Epoch 69, Training Loss: 0.7990523671745358\n",
      "Epoch 70, Training Loss: 0.7995177789738304\n",
      "Epoch 71, Training Loss: 0.7993751247126357\n",
      "Epoch 72, Training Loss: 0.7992854747557102\n",
      "Epoch 73, Training Loss: 0.7992903425281209\n",
      "Epoch 74, Training Loss: 0.7986998715795073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:08:11,088] Trial 48 finished with value: 0.6024666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7979005811806011\n",
      "Epoch 1, Training Loss: 0.9508346915245056\n",
      "Epoch 2, Training Loss: 0.832166194019461\n",
      "Epoch 3, Training Loss: 0.8199221197823833\n",
      "Epoch 4, Training Loss: 0.8168890705682281\n",
      "Epoch 5, Training Loss: 0.8139178210631349\n",
      "Epoch 6, Training Loss: 0.8120630606672817\n",
      "Epoch 7, Training Loss: 0.8110477204609634\n",
      "Epoch 8, Training Loss: 0.8102578928596095\n",
      "Epoch 9, Training Loss: 0.8083727126731012\n",
      "Epoch 10, Training Loss: 0.806055277989323\n",
      "Epoch 11, Training Loss: 0.8077007672840492\n",
      "Epoch 12, Training Loss: 0.8057332213659932\n",
      "Epoch 13, Training Loss: 0.8057656452171784\n",
      "Epoch 14, Training Loss: 0.8052278381541259\n",
      "Epoch 15, Training Loss: 0.8044556131936553\n",
      "Epoch 16, Training Loss: 0.8040833224031262\n",
      "Epoch 17, Training Loss: 0.8053194915441643\n",
      "Epoch 18, Training Loss: 0.8050619797599047\n",
      "Epoch 19, Training Loss: 0.803362928924704\n",
      "Epoch 20, Training Loss: 0.8042226067162994\n",
      "Epoch 21, Training Loss: 0.8044004222504179\n",
      "Epoch 22, Training Loss: 0.804233778150458\n",
      "Epoch 23, Training Loss: 0.8017411151326689\n",
      "Epoch 24, Training Loss: 0.8027757495865786\n",
      "Epoch 25, Training Loss: 0.8009347220112507\n",
      "Epoch 26, Training Loss: 0.8020845413208008\n",
      "Epoch 27, Training Loss: 0.8018601962498256\n",
      "Epoch 28, Training Loss: 0.8011730206640143\n",
      "Epoch 29, Training Loss: 0.8022573962247461\n",
      "Epoch 30, Training Loss: 0.8010984704010469\n",
      "Epoch 31, Training Loss: 0.8024033604707933\n",
      "Epoch 32, Training Loss: 0.8015888094902038\n",
      "Epoch 33, Training Loss: 0.80090682139074\n",
      "Epoch 34, Training Loss: 0.801077105317797\n",
      "Epoch 35, Training Loss: 0.8011372160194512\n",
      "Epoch 36, Training Loss: 0.8013035823528032\n",
      "Epoch 37, Training Loss: 0.801093368153823\n",
      "Epoch 38, Training Loss: 0.8010996540686242\n",
      "Epoch 39, Training Loss: 0.8005404773511384\n",
      "Epoch 40, Training Loss: 0.8009895908205132\n",
      "Epoch 41, Training Loss: 0.7996778730163001\n",
      "Epoch 42, Training Loss: 0.8022942255314132\n",
      "Epoch 43, Training Loss: 0.8000388160684055\n",
      "Epoch 44, Training Loss: 0.8006638885440683\n",
      "Epoch 45, Training Loss: 0.7999880019883464\n",
      "Epoch 46, Training Loss: 0.8005295570631673\n",
      "Epoch 47, Training Loss: 0.8008888455261861\n",
      "Epoch 48, Training Loss: 0.7995986328985458\n",
      "Epoch 49, Training Loss: 0.8006447055286035\n",
      "Epoch 50, Training Loss: 0.8012937958079173\n",
      "Epoch 51, Training Loss: 0.7998607112949055\n",
      "Epoch 52, Training Loss: 0.8007632245694785\n",
      "Epoch 53, Training Loss: 0.7996352266548271\n",
      "Epoch 54, Training Loss: 0.8007702314764037\n",
      "Epoch 55, Training Loss: 0.8003760187249435\n",
      "Epoch 56, Training Loss: 0.7992527397951685\n",
      "Epoch 57, Training Loss: 0.7982266699461112\n",
      "Epoch 58, Training Loss: 0.7993146291352753\n",
      "Epoch 59, Training Loss: 0.7982409914633385\n",
      "Epoch 60, Training Loss: 0.7991953182041197\n",
      "Epoch 61, Training Loss: 0.8000941054265301\n",
      "Epoch 62, Training Loss: 0.7998158159112572\n",
      "Epoch 63, Training Loss: 0.8002599735009043\n",
      "Epoch 64, Training Loss: 0.7995281010642088\n",
      "Epoch 65, Training Loss: 0.7981364917934389\n",
      "Epoch 66, Training Loss: 0.8003420840528674\n",
      "Epoch 67, Training Loss: 0.8003603126769675\n",
      "Epoch 68, Training Loss: 0.7983640210072797\n",
      "Epoch 69, Training Loss: 0.7986680435955076\n",
      "Epoch 70, Training Loss: 0.7987768209966501\n",
      "Epoch 71, Training Loss: 0.798930661660388\n",
      "Epoch 72, Training Loss: 0.7990530193300176\n",
      "Epoch 73, Training Loss: 0.8000265028243675\n",
      "Epoch 74, Training Loss: 0.8001219255583627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:09:47,699] Trial 49 finished with value: 0.5558 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8000982624247558\n",
      "Epoch 1, Training Loss: 0.9037872005911435\n",
      "Epoch 2, Training Loss: 0.8134005047994501\n",
      "Epoch 3, Training Loss: 0.8065627848400789\n",
      "Epoch 4, Training Loss: 0.8036794939461877\n",
      "Epoch 5, Training Loss: 0.8021386672468747\n",
      "Epoch 6, Training Loss: 0.7998091296588673\n",
      "Epoch 7, Training Loss: 0.7984848399723278\n",
      "Epoch 8, Training Loss: 0.7963924264907837\n",
      "Epoch 9, Training Loss: 0.7943457951966454\n",
      "Epoch 10, Training Loss: 0.7934080544640036\n",
      "Epoch 11, Training Loss: 0.792010140208637\n",
      "Epoch 12, Training Loss: 0.7914860465246089\n",
      "Epoch 13, Training Loss: 0.790341979896321\n",
      "Epoch 14, Training Loss: 0.7895503798653097\n",
      "Epoch 15, Training Loss: 0.7886559975147247\n",
      "Epoch 16, Training Loss: 0.7888028841860154\n",
      "Epoch 17, Training Loss: 0.7881966031298918\n",
      "Epoch 18, Training Loss: 0.7875271991421194\n",
      "Epoch 19, Training Loss: 0.7872306409302879\n",
      "Epoch 20, Training Loss: 0.7871000656660865\n",
      "Epoch 21, Training Loss: 0.7862948457633748\n",
      "Epoch 22, Training Loss: 0.7857753252281862\n",
      "Epoch 23, Training Loss: 0.7856631736194386\n",
      "Epoch 24, Training Loss: 0.78545994218658\n",
      "Epoch 25, Training Loss: 0.785039537303588\n",
      "Epoch 26, Training Loss: 0.7853554138015298\n",
      "Epoch 27, Training Loss: 0.7847180485725402\n",
      "Epoch 28, Training Loss: 0.7849769542497748\n",
      "Epoch 29, Training Loss: 0.784002578679253\n",
      "Epoch 30, Training Loss: 0.7843370567349827\n",
      "Epoch 31, Training Loss: 0.7842303776039796\n",
      "Epoch 32, Training Loss: 0.784375665398205\n",
      "Epoch 33, Training Loss: 0.7839617070730994\n",
      "Epoch 34, Training Loss: 0.7839008262578179\n",
      "Epoch 35, Training Loss: 0.7835417292398565\n",
      "Epoch 36, Training Loss: 0.7838260271268732\n",
      "Epoch 37, Training Loss: 0.783974364084356\n",
      "Epoch 38, Training Loss: 0.7833001849230598\n",
      "Epoch 39, Training Loss: 0.7831395920585184\n",
      "Epoch 40, Training Loss: 0.7830479839268852\n",
      "Epoch 41, Training Loss: 0.7828428203919354\n",
      "Epoch 42, Training Loss: 0.7829158907778123\n",
      "Epoch 43, Training Loss: 0.7825606322288513\n",
      "Epoch 44, Training Loss: 0.7824208111622755\n",
      "Epoch 45, Training Loss: 0.7823924650164211\n",
      "Epoch 46, Training Loss: 0.7821275404621573\n",
      "Epoch 47, Training Loss: 0.7817113473836114\n",
      "Epoch 48, Training Loss: 0.7821134412989897\n",
      "Epoch 49, Training Loss: 0.7817730109831866\n",
      "Epoch 50, Training Loss: 0.7821441275231978\n",
      "Epoch 51, Training Loss: 0.7817146886096281\n",
      "Epoch 52, Training Loss: 0.7818970024585724\n",
      "Epoch 53, Training Loss: 0.7816894468840431\n",
      "Epoch 54, Training Loss: 0.7813809751061832\n",
      "Epoch 55, Training Loss: 0.7815741043932297\n",
      "Epoch 56, Training Loss: 0.781419143185896\n",
      "Epoch 57, Training Loss: 0.7814746571288389\n",
      "Epoch 58, Training Loss: 0.7811405659423155\n",
      "Epoch 59, Training Loss: 0.7812873272334828\n",
      "Epoch 60, Training Loss: 0.781214119756923\n",
      "Epoch 61, Training Loss: 0.7809019918301526\n",
      "Epoch 62, Training Loss: 0.7810231950703789\n",
      "Epoch 63, Training Loss: 0.7807423773933859\n",
      "Epoch 64, Training Loss: 0.7806757203971638\n",
      "Epoch 65, Training Loss: 0.780795685824226\n",
      "Epoch 66, Training Loss: 0.7806365594443153\n",
      "Epoch 67, Training Loss: 0.780624281799092\n",
      "Epoch 68, Training Loss: 0.7801786135925967\n",
      "Epoch 69, Training Loss: 0.7804275941848755\n",
      "Epoch 70, Training Loss: 0.7798403559712803\n",
      "Epoch 71, Training Loss: 0.780272133140003\n",
      "Epoch 72, Training Loss: 0.7802143272932838\n",
      "Epoch 73, Training Loss: 0.7798881130358752\n",
      "Epoch 74, Training Loss: 0.7803585048984079\n",
      "Epoch 75, Training Loss: 0.7799683718821582\n",
      "Epoch 76, Training Loss: 0.7798935889496523\n",
      "Epoch 77, Training Loss: 0.7795591374705819\n",
      "Epoch 78, Training Loss: 0.7795102374693926\n",
      "Epoch 79, Training Loss: 0.7794849249895881\n",
      "Epoch 80, Training Loss: 0.7793737622569589\n",
      "Epoch 81, Training Loss: 0.7800453771563137\n",
      "Epoch 82, Training Loss: 0.7794036398915684\n",
      "Epoch 83, Training Loss: 0.7792556292870465\n",
      "Epoch 84, Training Loss: 0.7792847108840942\n",
      "Epoch 85, Training Loss: 0.7793338002176846\n",
      "Epoch 86, Training Loss: 0.7793824897092931\n",
      "Epoch 87, Training Loss: 0.779428171971265\n",
      "Epoch 88, Training Loss: 0.7786790008404676\n",
      "Epoch 89, Training Loss: 0.7791272115707397\n",
      "Epoch 90, Training Loss: 0.7792029483178082\n",
      "Epoch 91, Training Loss: 0.7789999893132378\n",
      "Epoch 92, Training Loss: 0.7789484433566822\n",
      "Epoch 93, Training Loss: 0.7786781076122733\n",
      "Epoch 94, Training Loss: 0.7785854141852435\n",
      "Epoch 95, Training Loss: 0.7784914962684407\n",
      "Epoch 96, Training Loss: 0.7787850770529579\n",
      "Epoch 97, Training Loss: 0.7785631523412817\n",
      "Epoch 98, Training Loss: 0.7784384419637568\n",
      "Epoch 99, Training Loss: 0.7787016670142903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:12:01,396] Trial 50 finished with value: 0.6407333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7786268698467927\n",
      "Epoch 1, Training Loss: 0.8645040812211878\n",
      "Epoch 2, Training Loss: 0.8063661160889793\n",
      "Epoch 3, Training Loss: 0.7999063294074115\n",
      "Epoch 4, Training Loss: 0.7947841215834898\n",
      "Epoch 5, Training Loss: 0.7928108374511494\n",
      "Epoch 6, Training Loss: 0.790526168276282\n",
      "Epoch 7, Training Loss: 0.7896712585056529\n",
      "Epoch 8, Training Loss: 0.7884937790562124\n",
      "Epoch 9, Training Loss: 0.7873137854828554\n",
      "Epoch 10, Training Loss: 0.7864849915223963\n",
      "Epoch 11, Training Loss: 0.7863167212991153\n",
      "Epoch 12, Training Loss: 0.7859813312221976\n",
      "Epoch 13, Training Loss: 0.7844169017847846\n",
      "Epoch 14, Training Loss: 0.7842975052665262\n",
      "Epoch 15, Training Loss: 0.7848097179216497\n",
      "Epoch 16, Training Loss: 0.7843992050956278\n",
      "Epoch 17, Training Loss: 0.7834154219487134\n",
      "Epoch 18, Training Loss: 0.7840207324308508\n",
      "Epoch 19, Training Loss: 0.7832000217017006\n",
      "Epoch 20, Training Loss: 0.7832883667945861\n",
      "Epoch 21, Training Loss: 0.7825677899753346\n",
      "Epoch 22, Training Loss: 0.7822896334003\n",
      "Epoch 23, Training Loss: 0.7818005474875955\n",
      "Epoch 24, Training Loss: 0.7819778065821704\n",
      "Epoch 25, Training Loss: 0.7816656438042136\n",
      "Epoch 26, Training Loss: 0.7817557442188263\n",
      "Epoch 27, Training Loss: 0.7817445704516243\n",
      "Epoch 28, Training Loss: 0.7807279188492718\n",
      "Epoch 29, Training Loss: 0.7805551725976607\n",
      "Epoch 30, Training Loss: 0.7806771039261537\n",
      "Epoch 31, Training Loss: 0.7808504945390364\n",
      "Epoch 32, Training Loss: 0.7797751468069413\n",
      "Epoch 33, Training Loss: 0.7800510691895205\n",
      "Epoch 34, Training Loss: 0.7804877062404857\n",
      "Epoch 35, Training Loss: 0.7796791241449468\n",
      "Epoch 36, Training Loss: 0.7793969016215381\n",
      "Epoch 37, Training Loss: 0.7789948674510507\n",
      "Epoch 38, Training Loss: 0.779204105279025\n",
      "Epoch 39, Training Loss: 0.7784522360913894\n",
      "Epoch 40, Training Loss: 0.7788408418262706\n",
      "Epoch 41, Training Loss: 0.7787934865671046\n",
      "Epoch 42, Training Loss: 0.7779753083341262\n",
      "Epoch 43, Training Loss: 0.7782146404070013\n",
      "Epoch 44, Training Loss: 0.7778091676094953\n",
      "Epoch 45, Training Loss: 0.7782148872403537\n",
      "Epoch 46, Training Loss: 0.7774054450848523\n",
      "Epoch 47, Training Loss: 0.7775715002592872\n",
      "Epoch 48, Training Loss: 0.7770100276610431\n",
      "Epoch 49, Training Loss: 0.7773558643284966\n",
      "Epoch 50, Training Loss: 0.7772751798349268\n",
      "Epoch 51, Training Loss: 0.7767156164085164\n",
      "Epoch 52, Training Loss: 0.7769316135434543\n",
      "Epoch 53, Training Loss: 0.7770406204111436\n",
      "Epoch 54, Training Loss: 0.776768639087677\n",
      "Epoch 55, Training Loss: 0.7768792632748099\n",
      "Epoch 56, Training Loss: 0.7764693546996397\n",
      "Epoch 57, Training Loss: 0.7759440046899458\n",
      "Epoch 58, Training Loss: 0.776425127842847\n",
      "Epoch 59, Training Loss: 0.775788242115694\n",
      "Epoch 60, Training Loss: 0.7759545786941753\n",
      "Epoch 61, Training Loss: 0.7758924751421985\n",
      "Epoch 62, Training Loss: 0.7761227933098288\n",
      "Epoch 63, Training Loss: 0.775748155187158\n",
      "Epoch 64, Training Loss: 0.7751887925232158\n",
      "Epoch 65, Training Loss: 0.7755464881307939\n",
      "Epoch 66, Training Loss: 0.775244656310362\n",
      "Epoch 67, Training Loss: 0.775278059314279\n",
      "Epoch 68, Training Loss: 0.7748177613230313\n",
      "Epoch 69, Training Loss: 0.7745695128160365\n",
      "Epoch 70, Training Loss: 0.7744622471753289\n",
      "Epoch 71, Training Loss: 0.774275829932269\n",
      "Epoch 72, Training Loss: 0.7743415443336262\n",
      "Epoch 73, Training Loss: 0.7740440579722909\n",
      "Epoch 74, Training Loss: 0.7741222609491909\n",
      "Epoch 75, Training Loss: 0.7735492857764749\n",
      "Epoch 76, Training Loss: 0.7737992498453926\n",
      "Epoch 77, Training Loss: 0.7733655435898724\n",
      "Epoch 78, Training Loss: 0.773091838500079\n",
      "Epoch 79, Training Loss: 0.773744697711047\n",
      "Epoch 80, Training Loss: 0.7731778444963343\n",
      "Epoch 81, Training Loss: 0.7732830326697405\n",
      "Epoch 82, Training Loss: 0.7725176579811994\n",
      "Epoch 83, Training Loss: 0.7730869283395655\n",
      "Epoch 84, Training Loss: 0.7723479472188388\n",
      "Epoch 85, Training Loss: 0.772869277491289\n",
      "Epoch 86, Training Loss: 0.7719656984946307\n",
      "Epoch 87, Training Loss: 0.7723294310008778\n",
      "Epoch 88, Training Loss: 0.7718317393695607\n",
      "Epoch 89, Training Loss: 0.7725840774704428\n",
      "Epoch 90, Training Loss: 0.771449763844995\n",
      "Epoch 91, Training Loss: 0.7723049111927257\n",
      "Epoch 92, Training Loss: 0.7713844272669624\n",
      "Epoch 93, Training Loss: 0.7713000099097981\n",
      "Epoch 94, Training Loss: 0.7711182125175701\n",
      "Epoch 95, Training Loss: 0.7715163230194765\n",
      "Epoch 96, Training Loss: 0.7710573382237378\n",
      "Epoch 97, Training Loss: 0.7710797597380246\n",
      "Epoch 98, Training Loss: 0.7710843116395614\n",
      "Epoch 99, Training Loss: 0.7706101253453423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:14:37,393] Trial 51 finished with value: 0.6384666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.770746452107149\n",
      "Epoch 1, Training Loss: 1.0246566695378239\n",
      "Epoch 2, Training Loss: 0.9609803935638944\n",
      "Epoch 3, Training Loss: 0.9511939574005013\n",
      "Epoch 4, Training Loss: 0.9438611079875688\n",
      "Epoch 5, Training Loss: 0.9344628104589936\n",
      "Epoch 6, Training Loss: 0.92171432685135\n",
      "Epoch 7, Training Loss: 0.9042630718166667\n",
      "Epoch 8, Training Loss: 0.8790046782421886\n",
      "Epoch 9, Training Loss: 0.8535814326508601\n",
      "Epoch 10, Training Loss: 0.8365733213890764\n",
      "Epoch 11, Training Loss: 0.8272708715352797\n",
      "Epoch 12, Training Loss: 0.8222646574328717\n",
      "Epoch 13, Training Loss: 0.8187237684888051\n",
      "Epoch 14, Training Loss: 0.8159891319454164\n",
      "Epoch 15, Training Loss: 0.8144476644974902\n",
      "Epoch 16, Training Loss: 0.8129849929558604\n",
      "Epoch 17, Training Loss: 0.8112611346675042\n",
      "Epoch 18, Training Loss: 0.8106718886167483\n",
      "Epoch 19, Training Loss: 0.809735211483518\n",
      "Epoch 20, Training Loss: 0.8089975301484417\n",
      "Epoch 21, Training Loss: 0.8081432003723947\n",
      "Epoch 22, Training Loss: 0.8080969802419046\n",
      "Epoch 23, Training Loss: 0.8074551416518992\n",
      "Epoch 24, Training Loss: 0.8067433245199963\n",
      "Epoch 25, Training Loss: 0.8067622934965263\n",
      "Epoch 26, Training Loss: 0.8054618959140061\n",
      "Epoch 27, Training Loss: 0.8053368241267097\n",
      "Epoch 28, Training Loss: 0.8044335120602658\n",
      "Epoch 29, Training Loss: 0.8045713663101196\n",
      "Epoch 30, Training Loss: 0.8043439599804412\n",
      "Epoch 31, Training Loss: 0.8033385646970649\n",
      "Epoch 32, Training Loss: 0.8031743270113952\n",
      "Epoch 33, Training Loss: 0.8022149358925067\n",
      "Epoch 34, Training Loss: 0.8022335773123834\n",
      "Epoch 35, Training Loss: 0.8014099439283959\n",
      "Epoch 36, Training Loss: 0.8016836757050421\n",
      "Epoch 37, Training Loss: 0.8010475235774105\n",
      "Epoch 38, Training Loss: 0.8009196561978276\n",
      "Epoch 39, Training Loss: 0.8007183783932736\n",
      "Epoch 40, Training Loss: 0.8010043788673287\n",
      "Epoch 41, Training Loss: 0.7997357613161991\n",
      "Epoch 42, Training Loss: 0.7997815015620755\n",
      "Epoch 43, Training Loss: 0.7989561641126647\n",
      "Epoch 44, Training Loss: 0.799247183745965\n",
      "Epoch 45, Training Loss: 0.7987908085486046\n",
      "Epoch 46, Training Loss: 0.7991675219141451\n",
      "Epoch 47, Training Loss: 0.7989138087831942\n",
      "Epoch 48, Training Loss: 0.7985380922045027\n",
      "Epoch 49, Training Loss: 0.7984741554224402\n",
      "Epoch 50, Training Loss: 0.7979817532058945\n",
      "Epoch 51, Training Loss: 0.7982930300827313\n",
      "Epoch 52, Training Loss: 0.7980716875621251\n",
      "Epoch 53, Training Loss: 0.7975727457749217\n",
      "Epoch 54, Training Loss: 0.7975004774287231\n",
      "Epoch 55, Training Loss: 0.7977267625636624\n",
      "Epoch 56, Training Loss: 0.7972120268900592\n",
      "Epoch 57, Training Loss: 0.7972789098445634\n",
      "Epoch 58, Training Loss: 0.7966700005800205\n",
      "Epoch 59, Training Loss: 0.7966863720040572\n",
      "Epoch 60, Training Loss: 0.7970468550696409\n",
      "Epoch 61, Training Loss: 0.7968346557222811\n",
      "Epoch 62, Training Loss: 0.7967580285287441\n",
      "Epoch 63, Training Loss: 0.7963212637524856\n",
      "Epoch 64, Training Loss: 0.7965052223743353\n",
      "Epoch 65, Training Loss: 0.7971836282794637\n",
      "Epoch 66, Training Loss: 0.7965115551661728\n",
      "Epoch 67, Training Loss: 0.7967695414571834\n",
      "Epoch 68, Training Loss: 0.7965052830545526\n",
      "Epoch 69, Training Loss: 0.7958111971840822\n",
      "Epoch 70, Training Loss: 0.7961155085635364\n",
      "Epoch 71, Training Loss: 0.7965585607335084\n",
      "Epoch 72, Training Loss: 0.7961460320573104\n",
      "Epoch 73, Training Loss: 0.7958295790772689\n",
      "Epoch 74, Training Loss: 0.7960845872871858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:15:57,450] Trial 52 finished with value: 0.6346666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7954725190212852\n",
      "Epoch 1, Training Loss: 0.8621705049626968\n",
      "Epoch 2, Training Loss: 0.8335090781660641\n",
      "Epoch 3, Training Loss: 0.8331396670201245\n",
      "Epoch 4, Training Loss: 0.8374747126943924\n",
      "Epoch 5, Training Loss: 0.8365498925657834\n",
      "Epoch 6, Training Loss: 0.8390111231803894\n",
      "Epoch 7, Training Loss: 0.8357374752269072\n",
      "Epoch 8, Training Loss: 0.8366474371096667\n",
      "Epoch 9, Training Loss: 0.8388281015788808\n",
      "Epoch 10, Training Loss: 0.8407925889071296\n",
      "Epoch 11, Training Loss: 0.84217229310204\n",
      "Epoch 12, Training Loss: 0.846906475389705\n",
      "Epoch 13, Training Loss: 0.840782587668475\n",
      "Epoch 14, Training Loss: 0.8443904677559347\n",
      "Epoch 15, Training Loss: 0.8475307863600113\n",
      "Epoch 16, Training Loss: 0.8474631536708158\n",
      "Epoch 17, Training Loss: 0.84660601012847\n",
      "Epoch 18, Training Loss: 0.8432250043925117\n",
      "Epoch 19, Training Loss: 0.8447656019294963\n",
      "Epoch 20, Training Loss: 0.8446528785368975\n",
      "Epoch 21, Training Loss: 0.8432688846307642\n",
      "Epoch 22, Training Loss: 0.847542671736549\n",
      "Epoch 23, Training Loss: 0.8498161655313828\n",
      "Epoch 24, Training Loss: 0.8469447143638835\n",
      "Epoch 25, Training Loss: 0.8443962416929357\n",
      "Epoch 26, Training Loss: 0.8482011316804325\n",
      "Epoch 27, Training Loss: 0.8461837697730344\n",
      "Epoch 28, Training Loss: 0.843825334591024\n",
      "Epoch 29, Training Loss: 0.8508582073800705\n",
      "Epoch 30, Training Loss: 0.8491560742434333\n",
      "Epoch 31, Training Loss: 0.845286911796121\n",
      "Epoch 32, Training Loss: 0.8498809670700747\n",
      "Epoch 33, Training Loss: 0.8477047725986032\n",
      "Epoch 34, Training Loss: 0.8460962799717399\n",
      "Epoch 35, Training Loss: 0.8512744602736305\n",
      "Epoch 36, Training Loss: 0.8420127971733318\n",
      "Epoch 37, Training Loss: 0.8453609563322628\n",
      "Epoch 38, Training Loss: 0.8482751998480629\n",
      "Epoch 39, Training Loss: 0.8483725688737982\n",
      "Epoch 40, Training Loss: 0.8438868674811195\n",
      "Epoch 41, Training Loss: 0.8463278183516334\n",
      "Epoch 42, Training Loss: 0.8480236945432775\n",
      "Epoch 43, Training Loss: 0.8484641207666959\n",
      "Epoch 44, Training Loss: 0.8453179528432734\n",
      "Epoch 45, Training Loss: 0.8446500492095947\n",
      "Epoch 46, Training Loss: 0.8505096702014698\n",
      "Epoch 47, Training Loss: 0.8455014808738933\n",
      "Epoch 48, Training Loss: 0.8506349623904509\n",
      "Epoch 49, Training Loss: 0.845121850196053\n",
      "Epoch 50, Training Loss: 0.8542866239828222\n",
      "Epoch 51, Training Loss: 0.8550033158414504\n",
      "Epoch 52, Training Loss: 0.8475831012866076\n",
      "Epoch 53, Training Loss: 0.853146731783362\n",
      "Epoch 54, Training Loss: 0.8509081558620228\n",
      "Epoch 55, Training Loss: 0.8502768906425028\n",
      "Epoch 56, Training Loss: 0.8500754291871014\n",
      "Epoch 57, Training Loss: 0.8467858757692225\n",
      "Epoch 58, Training Loss: 0.8515015769706052\n",
      "Epoch 59, Training Loss: 0.8511163602155798\n",
      "Epoch 60, Training Loss: 0.8440265722835765\n",
      "Epoch 61, Training Loss: 0.8522491413004258\n",
      "Epoch 62, Training Loss: 0.8506163796957802\n",
      "Epoch 63, Training Loss: 0.8511459032227011\n",
      "Epoch 64, Training Loss: 0.8507014528442831\n",
      "Epoch 65, Training Loss: 0.8550324507320628\n",
      "Epoch 66, Training Loss: 0.8489213086577023\n",
      "Epoch 67, Training Loss: 0.8478015416509965\n",
      "Epoch 68, Training Loss: 0.8498123232056113\n",
      "Epoch 69, Training Loss: 0.8456878810770372\n",
      "Epoch 70, Training Loss: 0.8418821955428404\n",
      "Epoch 71, Training Loss: 0.8439102717006908\n",
      "Epoch 72, Training Loss: 0.8432680646812215\n",
      "Epoch 73, Training Loss: 0.8483194656231824\n",
      "Epoch 74, Training Loss: 0.8497269217407002\n",
      "Epoch 75, Training Loss: 0.8413957445761737\n",
      "Epoch 76, Training Loss: 0.8509503597371718\n",
      "Epoch 77, Training Loss: 0.8424577178674586\n",
      "Epoch 78, Training Loss: 0.8495714978610768\n",
      "Epoch 79, Training Loss: 0.846719893357333\n",
      "Epoch 80, Training Loss: 0.8449415096815894\n",
      "Epoch 81, Training Loss: 0.8581121669797336\n",
      "Epoch 82, Training Loss: 0.8496204453356125\n",
      "Epoch 83, Training Loss: 0.8568110802594353\n",
      "Epoch 84, Training Loss: 0.8491702670209548\n",
      "Epoch 85, Training Loss: 0.846698665268281\n",
      "Epoch 86, Training Loss: 0.8493424358788658\n",
      "Epoch 87, Training Loss: 0.8450298761620241\n",
      "Epoch 88, Training Loss: 0.8501891413155724\n",
      "Epoch 89, Training Loss: 0.8535734922745648\n",
      "Epoch 90, Training Loss: 0.8499884354366976\n",
      "Epoch 91, Training Loss: 0.8441061242888955\n",
      "Epoch 92, Training Loss: 0.8500664467671338\n",
      "Epoch 93, Training Loss: 0.8490981153179618\n",
      "Epoch 94, Training Loss: 0.8579109653304605\n",
      "Epoch 95, Training Loss: 0.8574525217448964\n",
      "Epoch 96, Training Loss: 0.8538304794535917\n",
      "Epoch 97, Training Loss: 0.8441936828809626\n",
      "Epoch 98, Training Loss: 0.8566851986857021\n",
      "Epoch 99, Training Loss: 0.8573973674633923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:18:28,940] Trial 53 finished with value: 0.5938666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8605345761775971\n",
      "Epoch 1, Training Loss: 0.9039904513753446\n",
      "Epoch 2, Training Loss: 0.8320056105914869\n",
      "Epoch 3, Training Loss: 0.8218776269962913\n",
      "Epoch 4, Training Loss: 0.8177217038950526\n",
      "Epoch 5, Training Loss: 0.8152724300112043\n",
      "Epoch 6, Training Loss: 0.8129234304105429\n",
      "Epoch 7, Training Loss: 0.8101325932301973\n",
      "Epoch 8, Training Loss: 0.8087911380861038\n",
      "Epoch 9, Training Loss: 0.8085040355087223\n",
      "Epoch 10, Training Loss: 0.8069232731833493\n",
      "Epoch 11, Training Loss: 0.807254073315097\n",
      "Epoch 12, Training Loss: 0.804904172967251\n",
      "Epoch 13, Training Loss: 0.8062983489574347\n",
      "Epoch 14, Training Loss: 0.8047011514355366\n",
      "Epoch 15, Training Loss: 0.8060860307593095\n",
      "Epoch 16, Training Loss: 0.8042802750616145\n",
      "Epoch 17, Training Loss: 0.8039887278599847\n",
      "Epoch 18, Training Loss: 0.8041881300452957\n",
      "Epoch 19, Training Loss: 0.804412147066647\n",
      "Epoch 20, Training Loss: 0.8027055276515789\n",
      "Epoch 21, Training Loss: 0.8031910630993377\n",
      "Epoch 22, Training Loss: 0.8038466604132402\n",
      "Epoch 23, Training Loss: 0.8027189853496122\n",
      "Epoch 24, Training Loss: 0.8029202757025123\n",
      "Epoch 25, Training Loss: 0.8024327431406294\n",
      "Epoch 26, Training Loss: 0.801647650837002\n",
      "Epoch 27, Training Loss: 0.8024889566844567\n",
      "Epoch 28, Training Loss: 0.8010432265754929\n",
      "Epoch 29, Training Loss: 0.8019944735935756\n",
      "Epoch 30, Training Loss: 0.8028517840500164\n",
      "Epoch 31, Training Loss: 0.801437312767918\n",
      "Epoch 32, Training Loss: 0.7997100446457254\n",
      "Epoch 33, Training Loss: 0.801277854657711\n",
      "Epoch 34, Training Loss: 0.8013976171500701\n",
      "Epoch 35, Training Loss: 0.8004328104786407\n",
      "Epoch 36, Training Loss: 0.800890000242936\n",
      "Epoch 37, Training Loss: 0.8005357420534119\n",
      "Epoch 38, Training Loss: 0.8015062419991744\n",
      "Epoch 39, Training Loss: 0.8011783014562793\n",
      "Epoch 40, Training Loss: 0.8011401515257986\n",
      "Epoch 41, Training Loss: 0.801805229473831\n",
      "Epoch 42, Training Loss: 0.7998047673836687\n",
      "Epoch 43, Training Loss: 0.8009349983437617\n",
      "Epoch 44, Training Loss: 0.8004570320136565\n",
      "Epoch 45, Training Loss: 0.7998570304167898\n",
      "Epoch 46, Training Loss: 0.800325766362642\n",
      "Epoch 47, Training Loss: 0.7997540520546131\n",
      "Epoch 48, Training Loss: 0.7994321901995437\n",
      "Epoch 49, Training Loss: 0.7996578844866358\n",
      "Epoch 50, Training Loss: 0.8004819089308717\n",
      "Epoch 51, Training Loss: 0.7997991878287236\n",
      "Epoch 52, Training Loss: 0.8004666564159824\n",
      "Epoch 53, Training Loss: 0.7997388377225488\n",
      "Epoch 54, Training Loss: 0.7989758954908615\n",
      "Epoch 55, Training Loss: 0.7995418592503196\n",
      "Epoch 56, Training Loss: 0.8001440656812567\n",
      "Epoch 57, Training Loss: 0.79962321546741\n",
      "Epoch 58, Training Loss: 0.7999046586509934\n",
      "Epoch 59, Training Loss: 0.8001138244356428\n",
      "Epoch 60, Training Loss: 0.7989707359694\n",
      "Epoch 61, Training Loss: 0.7992749799463086\n",
      "Epoch 62, Training Loss: 0.7995040218632921\n",
      "Epoch 63, Training Loss: 0.7988402649872285\n",
      "Epoch 64, Training Loss: 0.7989732163293021\n",
      "Epoch 65, Training Loss: 0.7991580244293787\n",
      "Epoch 66, Training Loss: 0.7990834172506978\n",
      "Epoch 67, Training Loss: 0.7987563610076904\n",
      "Epoch 68, Training Loss: 0.7990586963811315\n",
      "Epoch 69, Training Loss: 0.7992364956920308\n",
      "Epoch 70, Training Loss: 0.7992833547126081\n",
      "Epoch 71, Training Loss: 0.7994867037113448\n",
      "Epoch 72, Training Loss: 0.7981132466990248\n",
      "Epoch 73, Training Loss: 0.7984856420889833\n",
      "Epoch 74, Training Loss: 0.7994005637061327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:20:02,576] Trial 54 finished with value: 0.6056666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7994050013391595\n",
      "Epoch 1, Training Loss: 0.8432142187778214\n",
      "Epoch 2, Training Loss: 0.8137017415878468\n",
      "Epoch 3, Training Loss: 0.8062344141472552\n",
      "Epoch 4, Training Loss: 0.8070035194095813\n",
      "Epoch 5, Training Loss: 0.806106684440957\n",
      "Epoch 6, Training Loss: 0.8032331515971879\n",
      "Epoch 7, Training Loss: 0.8025540416402028\n",
      "Epoch 8, Training Loss: 0.8044015343027904\n",
      "Epoch 9, Training Loss: 0.8021541618763056\n",
      "Epoch 10, Training Loss: 0.8009154939113703\n",
      "Epoch 11, Training Loss: 0.7981462290412501\n",
      "Epoch 12, Training Loss: 0.7994849412064803\n",
      "Epoch 13, Training Loss: 0.8023103531141926\n",
      "Epoch 14, Training Loss: 0.7996314423424857\n",
      "Epoch 15, Training Loss: 0.7989994753572277\n",
      "Epoch 16, Training Loss: 0.8001394469935195\n",
      "Epoch 17, Training Loss: 0.7989101797118223\n",
      "Epoch 18, Training Loss: 0.797539883852005\n",
      "Epoch 19, Training Loss: 0.7977754214652499\n",
      "Epoch 20, Training Loss: 0.798040217026732\n",
      "Epoch 21, Training Loss: 0.7988981561553209\n",
      "Epoch 22, Training Loss: 0.7979180728582511\n",
      "Epoch 23, Training Loss: 0.7979538120721515\n",
      "Epoch 24, Training Loss: 0.7981799199168843\n",
      "Epoch 25, Training Loss: 0.7971753841952274\n",
      "Epoch 26, Training Loss: 0.798220309727174\n",
      "Epoch 27, Training Loss: 0.7984880587212125\n",
      "Epoch 28, Training Loss: 0.798203512510859\n",
      "Epoch 29, Training Loss: 0.7970474996961149\n",
      "Epoch 30, Training Loss: 0.7984741279953405\n",
      "Epoch 31, Training Loss: 0.7973127697643481\n",
      "Epoch 32, Training Loss: 0.7971447862180552\n",
      "Epoch 33, Training Loss: 0.7975191484716602\n",
      "Epoch 34, Training Loss: 0.797095042960088\n",
      "Epoch 35, Training Loss: 0.7956858988095047\n",
      "Epoch 36, Training Loss: 0.7962653305297508\n",
      "Epoch 37, Training Loss: 0.7950769697813164\n",
      "Epoch 38, Training Loss: 0.7954248781491043\n",
      "Epoch 39, Training Loss: 0.7953712092306381\n",
      "Epoch 40, Training Loss: 0.7968688946021231\n",
      "Epoch 41, Training Loss: 0.7950417359968773\n",
      "Epoch 42, Training Loss: 0.794512920406528\n",
      "Epoch 43, Training Loss: 0.7956599089436065\n",
      "Epoch 44, Training Loss: 0.7962168128866899\n",
      "Epoch 45, Training Loss: 0.7947866931893772\n",
      "Epoch 46, Training Loss: 0.79725278741435\n",
      "Epoch 47, Training Loss: 0.7963004269097981\n",
      "Epoch 48, Training Loss: 0.797206029139067\n",
      "Epoch 49, Training Loss: 0.7948952662317377\n",
      "Epoch 50, Training Loss: 0.7952568641282562\n",
      "Epoch 51, Training Loss: 0.7952919457191812\n",
      "Epoch 52, Training Loss: 0.7945707074681619\n",
      "Epoch 53, Training Loss: 0.7950977742223811\n",
      "Epoch 54, Training Loss: 0.7961851712456323\n",
      "Epoch 55, Training Loss: 0.7957564806579647\n",
      "Epoch 56, Training Loss: 0.7963978370329491\n",
      "Epoch 57, Training Loss: 0.7937296778635872\n",
      "Epoch 58, Training Loss: 0.7953448861165154\n",
      "Epoch 59, Training Loss: 0.795634034432863\n",
      "Epoch 60, Training Loss: 0.7963659813529567\n",
      "Epoch 61, Training Loss: 0.7948158609239678\n",
      "Epoch 62, Training Loss: 0.7954838850444421\n",
      "Epoch 63, Training Loss: 0.7960853971933064\n",
      "Epoch 64, Training Loss: 0.7957807426165817\n",
      "Epoch 65, Training Loss: 0.7947323560714722\n",
      "Epoch 66, Training Loss: 0.7956019198984132\n",
      "Epoch 67, Training Loss: 0.7952293808298899\n",
      "Epoch 68, Training Loss: 0.7941552403277921\n",
      "Epoch 69, Training Loss: 0.7949092704550664\n",
      "Epoch 70, Training Loss: 0.7941450290213851\n",
      "Epoch 71, Training Loss: 0.7950049627992444\n",
      "Epoch 72, Training Loss: 0.7958818918780277\n",
      "Epoch 73, Training Loss: 0.7953894559602092\n",
      "Epoch 74, Training Loss: 0.7941095405951478\n",
      "Epoch 75, Training Loss: 0.7947790036524148\n",
      "Epoch 76, Training Loss: 0.7955745125175419\n",
      "Epoch 77, Training Loss: 0.7950746765710357\n",
      "Epoch 78, Training Loss: 0.7948732300808555\n",
      "Epoch 79, Training Loss: 0.7969043006574301\n",
      "Epoch 80, Training Loss: 0.7946863635142046\n",
      "Epoch 81, Training Loss: 0.793319979287628\n",
      "Epoch 82, Training Loss: 0.7943056637183168\n",
      "Epoch 83, Training Loss: 0.7955158690760906\n",
      "Epoch 84, Training Loss: 0.794588629912613\n",
      "Epoch 85, Training Loss: 0.7956124830963021\n",
      "Epoch 86, Training Loss: 0.7947292640693205\n",
      "Epoch 87, Training Loss: 0.7959930367935869\n",
      "Epoch 88, Training Loss: 0.7959550359195336\n",
      "Epoch 89, Training Loss: 0.7950673429589522\n",
      "Epoch 90, Training Loss: 0.79475318050026\n",
      "Epoch 91, Training Loss: 0.7941595609923054\n",
      "Epoch 92, Training Loss: 0.7951877758915263\n",
      "Epoch 93, Training Loss: 0.7941442330977074\n",
      "Epoch 94, Training Loss: 0.7941503816081169\n",
      "Epoch 95, Training Loss: 0.7949883278151204\n",
      "Epoch 96, Training Loss: 0.7943308195673433\n",
      "Epoch 97, Training Loss: 0.7947533991103782\n",
      "Epoch 98, Training Loss: 0.7937160238287503\n",
      "Epoch 99, Training Loss: 0.7957243179020129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:21:59,893] Trial 55 finished with value: 0.6392 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7946126279078032\n",
      "Epoch 1, Training Loss: 0.842030758337867\n",
      "Epoch 2, Training Loss: 0.8178482820216875\n",
      "Epoch 3, Training Loss: 0.8133993555728655\n",
      "Epoch 4, Training Loss: 0.8097276459959216\n",
      "Epoch 5, Training Loss: 0.8098212022530406\n",
      "Epoch 6, Training Loss: 0.8088862991870794\n",
      "Epoch 7, Training Loss: 0.8074560497936449\n",
      "Epoch 8, Training Loss: 0.8058409847711262\n",
      "Epoch 9, Training Loss: 0.8066733561064068\n",
      "Epoch 10, Training Loss: 0.8056320027301186\n",
      "Epoch 11, Training Loss: 0.8058414857190355\n",
      "Epoch 12, Training Loss: 0.8047108874285132\n",
      "Epoch 13, Training Loss: 0.8023882640483684\n",
      "Epoch 14, Training Loss: 0.8045900069681325\n",
      "Epoch 15, Training Loss: 0.8035361721103352\n",
      "Epoch 16, Training Loss: 0.802220510629783\n",
      "Epoch 17, Training Loss: 0.8025931321588674\n",
      "Epoch 18, Training Loss: 0.8022805200483566\n",
      "Epoch 19, Training Loss: 0.8016901167711817\n",
      "Epoch 20, Training Loss: 0.8012755747576406\n",
      "Epoch 21, Training Loss: 0.8024613454825896\n",
      "Epoch 22, Training Loss: 0.801276255191717\n",
      "Epoch 23, Training Loss: 0.8020044743566584\n",
      "Epoch 24, Training Loss: 0.8007167211152557\n",
      "Epoch 25, Training Loss: 0.8001833182528503\n",
      "Epoch 26, Training Loss: 0.8022965700106514\n",
      "Epoch 27, Training Loss: 0.7998736970406726\n",
      "Epoch 28, Training Loss: 0.8007939498227342\n",
      "Epoch 29, Training Loss: 0.8007678325014903\n",
      "Epoch 30, Training Loss: 0.8005952907684154\n",
      "Epoch 31, Training Loss: 0.8000354725615423\n",
      "Epoch 32, Training Loss: 0.8001953962153958\n",
      "Epoch 33, Training Loss: 0.7981355270048729\n",
      "Epoch 34, Training Loss: 0.8000046848354483\n",
      "Epoch 35, Training Loss: 0.8010661624428025\n",
      "Epoch 36, Training Loss: 0.8002823880740575\n",
      "Epoch 37, Training Loss: 0.7991424347224988\n",
      "Epoch 38, Training Loss: 0.7982967516085259\n",
      "Epoch 39, Training Loss: 0.7999944235149182\n",
      "Epoch 40, Training Loss: 0.7996986639230771\n",
      "Epoch 41, Training Loss: 0.7985414755075497\n",
      "Epoch 42, Training Loss: 0.799875071084589\n",
      "Epoch 43, Training Loss: 0.7981946078458226\n",
      "Epoch 44, Training Loss: 0.7994898515536373\n",
      "Epoch 45, Training Loss: 0.7980906732100294\n",
      "Epoch 46, Training Loss: 0.7993904392522081\n",
      "Epoch 47, Training Loss: 0.7968189814485105\n",
      "Epoch 48, Training Loss: 0.7972560843130699\n",
      "Epoch 49, Training Loss: 0.7971915526945789\n",
      "Epoch 50, Training Loss: 0.7986274753298078\n",
      "Epoch 51, Training Loss: 0.7983753409600796\n",
      "Epoch 52, Training Loss: 0.7980010654693259\n",
      "Epoch 53, Training Loss: 0.7992363220766971\n",
      "Epoch 54, Training Loss: 0.7980955880387385\n",
      "Epoch 55, Training Loss: 0.7991327103815581\n",
      "Epoch 56, Training Loss: 0.7979158952720183\n",
      "Epoch 57, Training Loss: 0.8001595686252853\n",
      "Epoch 58, Training Loss: 0.7981116807550416\n",
      "Epoch 59, Training Loss: 0.7984166920633244\n",
      "Epoch 60, Training Loss: 0.798623095688067\n",
      "Epoch 61, Training Loss: 0.7979944162799003\n",
      "Epoch 62, Training Loss: 0.7971313209461987\n",
      "Epoch 63, Training Loss: 0.798405212836158\n",
      "Epoch 64, Training Loss: 0.7970934927015376\n",
      "Epoch 65, Training Loss: 0.7972293307010393\n",
      "Epoch 66, Training Loss: 0.7987493702343532\n",
      "Epoch 67, Training Loss: 0.7968167071055648\n",
      "Epoch 68, Training Loss: 0.7968367782750524\n",
      "Epoch 69, Training Loss: 0.7963873035029361\n",
      "Epoch 70, Training Loss: 0.7974404291999071\n",
      "Epoch 71, Training Loss: 0.7988070801684731\n",
      "Epoch 72, Training Loss: 0.7975767589153204\n",
      "Epoch 73, Training Loss: 0.7981780426842826\n",
      "Epoch 74, Training Loss: 0.7975974471945512\n",
      "Epoch 75, Training Loss: 0.7968657312536598\n",
      "Epoch 76, Training Loss: 0.7969894607264296\n",
      "Epoch 77, Training Loss: 0.7978476294897553\n",
      "Epoch 78, Training Loss: 0.7985842383893809\n",
      "Epoch 79, Training Loss: 0.7972621697680394\n",
      "Epoch 80, Training Loss: 0.7979797779169298\n",
      "Epoch 81, Training Loss: 0.7969977955172832\n",
      "Epoch 82, Training Loss: 0.7968687814877445\n",
      "Epoch 83, Training Loss: 0.796410089865663\n",
      "Epoch 84, Training Loss: 0.7974203930761581\n",
      "Epoch 85, Training Loss: 0.7971491504432564\n",
      "Epoch 86, Training Loss: 0.7970106011046503\n",
      "Epoch 87, Training Loss: 0.796770315331624\n",
      "Epoch 88, Training Loss: 0.7969585384641374\n",
      "Epoch 89, Training Loss: 0.7978608352797372\n",
      "Epoch 90, Training Loss: 0.7971248200065211\n",
      "Epoch 91, Training Loss: 0.7967328570839157\n",
      "Epoch 92, Training Loss: 0.7968011847115997\n",
      "Epoch 93, Training Loss: 0.7981526517330255\n",
      "Epoch 94, Training Loss: 0.7985517965223556\n",
      "Epoch 95, Training Loss: 0.7982319338877398\n",
      "Epoch 96, Training Loss: 0.797518593655493\n",
      "Epoch 97, Training Loss: 0.7971254594343945\n",
      "Epoch 98, Training Loss: 0.7975438954238605\n",
      "Epoch 99, Training Loss: 0.7970195551563922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:23:38,335] Trial 56 finished with value: 0.6309333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7961688061405842\n",
      "Epoch 1, Training Loss: 0.8464398155535074\n",
      "Epoch 2, Training Loss: 0.8116440571340403\n",
      "Epoch 3, Training Loss: 0.8075373794799461\n",
      "Epoch 4, Training Loss: 0.8060988025557726\n",
      "Epoch 5, Training Loss: 0.8045471351397665\n",
      "Epoch 6, Training Loss: 0.8035465245856378\n",
      "Epoch 7, Training Loss: 0.8018721351049897\n",
      "Epoch 8, Training Loss: 0.8024516819114972\n",
      "Epoch 9, Training Loss: 0.8022549719738781\n",
      "Epoch 10, Training Loss: 0.8007156392685453\n",
      "Epoch 11, Training Loss: 0.8006327700794191\n",
      "Epoch 12, Training Loss: 0.7986896135753259\n",
      "Epoch 13, Training Loss: 0.7984689635441715\n",
      "Epoch 14, Training Loss: 0.7983688128621955\n",
      "Epoch 15, Training Loss: 0.7984708174727017\n",
      "Epoch 16, Training Loss: 0.7999413259047314\n",
      "Epoch 17, Training Loss: 0.7965887821706614\n",
      "Epoch 18, Training Loss: 0.7965417624416208\n",
      "Epoch 19, Training Loss: 0.7967856334564382\n",
      "Epoch 20, Training Loss: 0.7973576165679702\n",
      "Epoch 21, Training Loss: 0.7961632968787861\n",
      "Epoch 22, Training Loss: 0.7953789816315012\n",
      "Epoch 23, Training Loss: 0.7963574848676983\n",
      "Epoch 24, Training Loss: 0.7955713169915336\n",
      "Epoch 25, Training Loss: 0.7940382860656968\n",
      "Epoch 26, Training Loss: 0.7950063195443691\n",
      "Epoch 27, Training Loss: 0.7966260188504269\n",
      "Epoch 28, Training Loss: 0.7952923713770128\n",
      "Epoch 29, Training Loss: 0.79419812905161\n",
      "Epoch 30, Training Loss: 0.7941297474660372\n",
      "Epoch 31, Training Loss: 0.7943815314680114\n",
      "Epoch 32, Training Loss: 0.7946321951715569\n",
      "Epoch 33, Training Loss: 0.7937306511671023\n",
      "Epoch 34, Training Loss: 0.7948101763438461\n",
      "Epoch 35, Training Loss: 0.793508103377837\n",
      "Epoch 36, Training Loss: 0.7941648589041\n",
      "Epoch 37, Training Loss: 0.7936974510214383\n",
      "Epoch 38, Training Loss: 0.79355042975648\n",
      "Epoch 39, Training Loss: 0.793473129792321\n",
      "Epoch 40, Training Loss: 0.7944042255107622\n",
      "Epoch 41, Training Loss: 0.793764458921619\n",
      "Epoch 42, Training Loss: 0.7930218551391945\n",
      "Epoch 43, Training Loss: 0.7924009879728905\n",
      "Epoch 44, Training Loss: 0.792614988366464\n",
      "Epoch 45, Training Loss: 0.793536987609433\n",
      "Epoch 46, Training Loss: 0.7925889095865695\n",
      "Epoch 47, Training Loss: 0.7930292413647013\n",
      "Epoch 48, Training Loss: 0.792680857683483\n",
      "Epoch 49, Training Loss: 0.7937213847511693\n",
      "Epoch 50, Training Loss: 0.7919375548685403\n",
      "Epoch 51, Training Loss: 0.7914391870785477\n",
      "Epoch 52, Training Loss: 0.7920697688160087\n",
      "Epoch 53, Training Loss: 0.7916176434746363\n",
      "Epoch 54, Training Loss: 0.7926584304723524\n",
      "Epoch 55, Training Loss: 0.7917953032299988\n",
      "Epoch 56, Training Loss: 0.7923560615769006\n",
      "Epoch 57, Training Loss: 0.7917470244536723\n",
      "Epoch 58, Training Loss: 0.7916488278181033\n",
      "Epoch 59, Training Loss: 0.7925447371669282\n",
      "Epoch 60, Training Loss: 0.7921828332700227\n",
      "Epoch 61, Training Loss: 0.7914922093090259\n",
      "Epoch 62, Training Loss: 0.792052037733838\n",
      "Epoch 63, Training Loss: 0.7912282496466673\n",
      "Epoch 64, Training Loss: 0.7908437147176355\n",
      "Epoch 65, Training Loss: 0.7915333886791889\n",
      "Epoch 66, Training Loss: 0.7910069099046234\n",
      "Epoch 67, Training Loss: 0.791535682965042\n",
      "Epoch 68, Training Loss: 0.7901447777461289\n",
      "Epoch 69, Training Loss: 0.7911156088786018\n",
      "Epoch 70, Training Loss: 0.791918496888383\n",
      "Epoch 71, Training Loss: 0.7915672460893043\n",
      "Epoch 72, Training Loss: 0.7911064366200813\n",
      "Epoch 73, Training Loss: 0.792156049631592\n",
      "Epoch 74, Training Loss: 0.7924083409452797\n",
      "Epoch 75, Training Loss: 0.7908295825908058\n",
      "Epoch 76, Training Loss: 0.7911359530642517\n",
      "Epoch 77, Training Loss: 0.7912736887322332\n",
      "Epoch 78, Training Loss: 0.789817692373032\n",
      "Epoch 79, Training Loss: 0.790829863225607\n",
      "Epoch 80, Training Loss: 0.7911025363699834\n",
      "Epoch 81, Training Loss: 0.791107858302898\n",
      "Epoch 82, Training Loss: 0.7911747014612184\n",
      "Epoch 83, Training Loss: 0.7903404697439724\n",
      "Epoch 84, Training Loss: 0.7908737314374823\n",
      "Epoch 85, Training Loss: 0.7913841265484802\n",
      "Epoch 86, Training Loss: 0.7907011789486821\n",
      "Epoch 87, Training Loss: 0.7900144590471024\n",
      "Epoch 88, Training Loss: 0.7905390027770423\n",
      "Epoch 89, Training Loss: 0.7915756561702355\n",
      "Epoch 90, Training Loss: 0.7910368009617454\n",
      "Epoch 91, Training Loss: 0.7912946568395859\n",
      "Epoch 92, Training Loss: 0.7902789932444579\n",
      "Epoch 93, Training Loss: 0.7910523172607995\n",
      "Epoch 94, Training Loss: 0.7912132472920238\n",
      "Epoch 95, Training Loss: 0.7911214002989289\n",
      "Epoch 96, Training Loss: 0.7908866389353473\n",
      "Epoch 97, Training Loss: 0.7906738708790083\n",
      "Epoch 98, Training Loss: 0.7908901380417043\n",
      "Epoch 99, Training Loss: 0.7901049924972362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:25:17,334] Trial 57 finished with value: 0.6337333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7910044660245565\n",
      "Epoch 1, Training Loss: 1.091357802089892\n",
      "Epoch 2, Training Loss: 1.091246845847682\n",
      "Epoch 3, Training Loss: 1.0911652107884113\n",
      "Epoch 4, Training Loss: 1.0909292896887413\n",
      "Epoch 5, Training Loss: 1.0910161272923748\n",
      "Epoch 6, Training Loss: 1.0907781749739682\n",
      "Epoch 7, Training Loss: 1.0905914428538845\n",
      "Epoch 8, Training Loss: 1.0906621941946504\n",
      "Epoch 9, Training Loss: 1.0905083270897542\n",
      "Epoch 10, Training Loss: 1.0903049176796935\n",
      "Epoch 11, Training Loss: 1.0901106956309843\n",
      "Epoch 12, Training Loss: 1.0899861767776031\n",
      "Epoch 13, Training Loss: 1.0897607918072465\n",
      "Epoch 14, Training Loss: 1.08945874594208\n",
      "Epoch 15, Training Loss: 1.0892151943723063\n",
      "Epoch 16, Training Loss: 1.0888773871543713\n",
      "Epoch 17, Training Loss: 1.088410308665799\n",
      "Epoch 18, Training Loss: 1.0879835116235834\n",
      "Epoch 19, Training Loss: 1.0872821097983454\n",
      "Epoch 20, Training Loss: 1.0864712374550956\n",
      "Epoch 21, Training Loss: 1.085501944212089\n",
      "Epoch 22, Training Loss: 1.0843255310130298\n",
      "Epoch 23, Training Loss: 1.0828237209104954\n",
      "Epoch 24, Training Loss: 1.0805254324934537\n",
      "Epoch 25, Training Loss: 1.0772950676150788\n",
      "Epoch 26, Training Loss: 1.0727805015736056\n",
      "Epoch 27, Training Loss: 1.0660947116694055\n",
      "Epoch 28, Training Loss: 1.0567224839576206\n",
      "Epoch 29, Training Loss: 1.0433536553741398\n",
      "Epoch 30, Training Loss: 1.0272981881199026\n",
      "Epoch 31, Training Loss: 1.0121075765530867\n",
      "Epoch 32, Training Loss: 1.0019997026687277\n",
      "Epoch 33, Training Loss: 0.9966538817362678\n",
      "Epoch 34, Training Loss: 0.9939635246319878\n",
      "Epoch 35, Training Loss: 0.9926270679423683\n",
      "Epoch 36, Training Loss: 0.9905156983468766\n",
      "Epoch 37, Training Loss: 0.9890124556713534\n",
      "Epoch 38, Training Loss: 0.9874239502992845\n",
      "Epoch 39, Training Loss: 0.9857681560337095\n",
      "Epoch 40, Training Loss: 0.9836322596198634\n",
      "Epoch 41, Training Loss: 0.9806482592919715\n",
      "Epoch 42, Training Loss: 0.9773674502408594\n",
      "Epoch 43, Training Loss: 0.9741096167636097\n",
      "Epoch 44, Training Loss: 0.9705935149264515\n",
      "Epoch 45, Training Loss: 0.9664697764511395\n",
      "Epoch 46, Training Loss: 0.9623136007696166\n",
      "Epoch 47, Training Loss: 0.958015986044604\n",
      "Epoch 48, Training Loss: 0.9540644469117759\n",
      "Epoch 49, Training Loss: 0.9503808004515512\n",
      "Epoch 50, Training Loss: 0.9473206341714787\n",
      "Epoch 51, Training Loss: 0.9444109206809137\n",
      "Epoch 52, Training Loss: 0.9422476455681306\n",
      "Epoch 53, Training Loss: 0.9403960608898249\n",
      "Epoch 54, Training Loss: 0.9373820804115525\n",
      "Epoch 55, Training Loss: 0.93474306656902\n",
      "Epoch 56, Training Loss: 0.9322916130374249\n",
      "Epoch 57, Training Loss: 0.9295440205954071\n",
      "Epoch 58, Training Loss: 0.925592599506665\n",
      "Epoch 59, Training Loss: 0.9219689192628502\n",
      "Epoch 60, Training Loss: 0.9178850052948285\n",
      "Epoch 61, Training Loss: 0.9131130783181441\n",
      "Epoch 62, Training Loss: 0.9086772230334748\n",
      "Epoch 63, Training Loss: 0.9034338645468977\n",
      "Epoch 64, Training Loss: 0.8964576261384146\n",
      "Epoch 65, Training Loss: 0.8903454466869957\n",
      "Epoch 66, Training Loss: 0.8836076671915843\n",
      "Epoch 67, Training Loss: 0.8770309862337614\n",
      "Epoch 68, Training Loss: 0.8704202515738351\n",
      "Epoch 69, Training Loss: 0.8646000292964447\n",
      "Epoch 70, Training Loss: 0.8587200085919603\n",
      "Epoch 71, Training Loss: 0.8546060571096894\n",
      "Epoch 72, Training Loss: 0.8505382763712029\n",
      "Epoch 73, Training Loss: 0.8474182442614907\n",
      "Epoch 74, Training Loss: 0.8444869542480411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:26:34,187] Trial 58 finished with value: 0.5993333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8421679746835752\n",
      "Epoch 1, Training Loss: 0.8978747304220845\n",
      "Epoch 2, Training Loss: 0.8243358589652786\n",
      "Epoch 3, Training Loss: 0.8162536278703159\n",
      "Epoch 4, Training Loss: 0.8123843787308026\n",
      "Epoch 5, Training Loss: 0.8092902302742004\n",
      "Epoch 6, Training Loss: 0.8062950609321881\n",
      "Epoch 7, Training Loss: 0.8036405604584773\n",
      "Epoch 8, Training Loss: 0.8008296642984662\n",
      "Epoch 9, Training Loss: 0.7992799669280088\n",
      "Epoch 10, Training Loss: 0.7969936458688033\n",
      "Epoch 11, Training Loss: 0.7961832831676742\n",
      "Epoch 12, Training Loss: 0.7945456658987174\n",
      "Epoch 13, Training Loss: 0.792972072040228\n",
      "Epoch 14, Training Loss: 0.7930365426199777\n",
      "Epoch 15, Training Loss: 0.7921284924772449\n",
      "Epoch 16, Training Loss: 0.7918632536006153\n",
      "Epoch 17, Training Loss: 0.7913682484985294\n",
      "Epoch 18, Training Loss: 0.7917208374891066\n",
      "Epoch 19, Training Loss: 0.7911606661359171\n",
      "Epoch 20, Training Loss: 0.789911393025764\n",
      "Epoch 21, Training Loss: 0.7895114928259885\n",
      "Epoch 22, Training Loss: 0.7897312505800921\n",
      "Epoch 23, Training Loss: 0.788644706485863\n",
      "Epoch 24, Training Loss: 0.7886276548966429\n",
      "Epoch 25, Training Loss: 0.7882308747535361\n",
      "Epoch 26, Training Loss: 0.7877388908002609\n",
      "Epoch 27, Training Loss: 0.7876025834477933\n",
      "Epoch 28, Training Loss: 0.7875125432372989\n",
      "Epoch 29, Training Loss: 0.7872036063581481\n",
      "Epoch 30, Training Loss: 0.7870813221859753\n",
      "Epoch 31, Training Loss: 0.7874101636105014\n",
      "Epoch 32, Training Loss: 0.7869032533545243\n",
      "Epoch 33, Training Loss: 0.7868394042316236\n",
      "Epoch 34, Training Loss: 0.7860422531913097\n",
      "Epoch 35, Training Loss: 0.7864884674997258\n",
      "Epoch 36, Training Loss: 0.7864128267854676\n",
      "Epoch 37, Training Loss: 0.7861993699145496\n",
      "Epoch 38, Training Loss: 0.7854964896252281\n",
      "Epoch 39, Training Loss: 0.786319042686233\n",
      "Epoch 40, Training Loss: 0.7867378485830206\n",
      "Epoch 41, Training Loss: 0.7869040185347536\n",
      "Epoch 42, Training Loss: 0.7854170821214977\n",
      "Epoch 43, Training Loss: 0.7855652200548272\n",
      "Epoch 44, Training Loss: 0.7857274633601196\n",
      "Epoch 45, Training Loss: 0.7857501675311784\n",
      "Epoch 46, Training Loss: 0.7859196449580945\n",
      "Epoch 47, Training Loss: 0.7846302638376565\n",
      "Epoch 48, Training Loss: 0.7856947577985606\n",
      "Epoch 49, Training Loss: 0.7851545309661923\n",
      "Epoch 50, Training Loss: 0.7850539017440681\n",
      "Epoch 51, Training Loss: 0.784776190438665\n",
      "Epoch 52, Training Loss: 0.7847957684581441\n",
      "Epoch 53, Training Loss: 0.7846726018683354\n",
      "Epoch 54, Training Loss: 0.7842280908634788\n",
      "Epoch 55, Training Loss: 0.7848363342141746\n",
      "Epoch 56, Training Loss: 0.7846972727237788\n",
      "Epoch 57, Training Loss: 0.784350295622546\n",
      "Epoch 58, Training Loss: 0.7842337385156101\n",
      "Epoch 59, Training Loss: 0.7847678184509277\n",
      "Epoch 60, Training Loss: 0.7845969900152737\n",
      "Epoch 61, Training Loss: 0.7841130711082229\n",
      "Epoch 62, Training Loss: 0.7841132905250205\n",
      "Epoch 63, Training Loss: 0.7839983602215473\n",
      "Epoch 64, Training Loss: 0.7839017267513992\n",
      "Epoch 65, Training Loss: 0.7838162657013513\n",
      "Epoch 66, Training Loss: 0.7841927264866076\n",
      "Epoch 67, Training Loss: 0.7845408747070715\n",
      "Epoch 68, Training Loss: 0.7836615121454225\n",
      "Epoch 69, Training Loss: 0.7832802123593209\n",
      "Epoch 70, Training Loss: 0.783867783833267\n",
      "Epoch 71, Training Loss: 0.7834693738392421\n",
      "Epoch 72, Training Loss: 0.7832791472736158\n",
      "Epoch 73, Training Loss: 0.7834705021148337\n",
      "Epoch 74, Training Loss: 0.783625624861036\n",
      "Epoch 75, Training Loss: 0.7835186286976463\n",
      "Epoch 76, Training Loss: 0.7837830183201266\n",
      "Epoch 77, Training Loss: 0.7829048644331165\n",
      "Epoch 78, Training Loss: 0.7830394544099507\n",
      "Epoch 79, Training Loss: 0.7834149506755341\n",
      "Epoch 80, Training Loss: 0.7831648662574309\n",
      "Epoch 81, Training Loss: 0.7828719670611216\n",
      "Epoch 82, Training Loss: 0.7833805337884372\n",
      "Epoch 83, Training Loss: 0.782824500700585\n",
      "Epoch 84, Training Loss: 0.7826838263891693\n",
      "Epoch 85, Training Loss: 0.7829082463020669\n",
      "Epoch 86, Training Loss: 0.7832470625863039\n",
      "Epoch 87, Training Loss: 0.783091440147027\n",
      "Epoch 88, Training Loss: 0.7825599960814741\n",
      "Epoch 89, Training Loss: 0.7829948080213447\n",
      "Epoch 90, Training Loss: 0.7824410203704261\n",
      "Epoch 91, Training Loss: 0.7830256822413968\n",
      "Epoch 92, Training Loss: 0.7825358046624894\n",
      "Epoch 93, Training Loss: 0.7825280787353229\n",
      "Epoch 94, Training Loss: 0.7828202240449146\n",
      "Epoch 95, Training Loss: 0.7824714255512208\n",
      "Epoch 96, Training Loss: 0.7829777567906487\n",
      "Epoch 97, Training Loss: 0.7821951023618081\n",
      "Epoch 98, Training Loss: 0.7828292803656786\n",
      "Epoch 99, Training Loss: 0.7831641585307014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:28:23,156] Trial 59 finished with value: 0.6329333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7828866422624516\n",
      "Epoch 1, Training Loss: 0.86189509352347\n",
      "Epoch 2, Training Loss: 0.8166445009690478\n",
      "Epoch 3, Training Loss: 0.8131875702313014\n",
      "Epoch 4, Training Loss: 0.8113700840706216\n",
      "Epoch 5, Training Loss: 0.808673910449322\n",
      "Epoch 6, Training Loss: 0.8064142687876422\n",
      "Epoch 7, Training Loss: 0.8052651053084466\n",
      "Epoch 8, Training Loss: 0.8035727463270489\n",
      "Epoch 9, Training Loss: 0.8028470279578875\n",
      "Epoch 10, Training Loss: 0.802845969773773\n",
      "Epoch 11, Training Loss: 0.8015814763262756\n",
      "Epoch 12, Training Loss: 0.8010082083537167\n",
      "Epoch 13, Training Loss: 0.8003824532480168\n",
      "Epoch 14, Training Loss: 0.8007086164072941\n",
      "Epoch 15, Training Loss: 0.7996917481709244\n",
      "Epoch 16, Training Loss: 0.7988716156859147\n",
      "Epoch 17, Training Loss: 0.7983030956490595\n",
      "Epoch 18, Training Loss: 0.7974930080256067\n",
      "Epoch 19, Training Loss: 0.796728711827357\n",
      "Epoch 20, Training Loss: 0.7963897221070483\n",
      "Epoch 21, Training Loss: 0.795343904387682\n",
      "Epoch 22, Training Loss: 0.7949884139505544\n",
      "Epoch 23, Training Loss: 0.7933647067026984\n",
      "Epoch 24, Training Loss: 0.7925417655392697\n",
      "Epoch 25, Training Loss: 0.792976266728308\n",
      "Epoch 26, Training Loss: 0.791485035240202\n",
      "Epoch 27, Training Loss: 0.7911545315183195\n",
      "Epoch 28, Training Loss: 0.7896215620793794\n",
      "Epoch 29, Training Loss: 0.7891839069531377\n",
      "Epoch 30, Training Loss: 0.7885364909817402\n",
      "Epoch 31, Training Loss: 0.7880660126083776\n",
      "Epoch 32, Training Loss: 0.788324052678015\n",
      "Epoch 33, Training Loss: 0.7883708932345971\n",
      "Epoch 34, Training Loss: 0.787363136262822\n",
      "Epoch 35, Training Loss: 0.7867376502295186\n",
      "Epoch 36, Training Loss: 0.7860889674129342\n",
      "Epoch 37, Training Loss: 0.7860313345615129\n",
      "Epoch 38, Training Loss: 0.7854764182764785\n",
      "Epoch 39, Training Loss: 0.7862891062758023\n",
      "Epoch 40, Training Loss: 0.7849534763429398\n",
      "Epoch 41, Training Loss: 0.7850206211993569\n",
      "Epoch 42, Training Loss: 0.7845462590231931\n",
      "Epoch 43, Training Loss: 0.7849942365087065\n",
      "Epoch 44, Training Loss: 0.7843209725573547\n",
      "Epoch 45, Training Loss: 0.7844699526191654\n",
      "Epoch 46, Training Loss: 0.7847282535151432\n",
      "Epoch 47, Training Loss: 0.7837478826816817\n",
      "Epoch 48, Training Loss: 0.7846096490558825\n",
      "Epoch 49, Training Loss: 0.7841233808295172\n",
      "Epoch 50, Training Loss: 0.7841391458547204\n",
      "Epoch 51, Training Loss: 0.7831419369331876\n",
      "Epoch 52, Training Loss: 0.7842192956379481\n",
      "Epoch 53, Training Loss: 0.7841294348688054\n",
      "Epoch 54, Training Loss: 0.7835881561264956\n",
      "Epoch 55, Training Loss: 0.7833606403573115\n",
      "Epoch 56, Training Loss: 0.7828714846668386\n",
      "Epoch 57, Training Loss: 0.7832135319709778\n",
      "Epoch 58, Training Loss: 0.783493080981692\n",
      "Epoch 59, Training Loss: 0.7826897474159872\n",
      "Epoch 60, Training Loss: 0.7829716127617915\n",
      "Epoch 61, Training Loss: 0.7829314812681729\n",
      "Epoch 62, Training Loss: 0.7828944385499883\n",
      "Epoch 63, Training Loss: 0.7831659778616482\n",
      "Epoch 64, Training Loss: 0.7825026838402999\n",
      "Epoch 65, Training Loss: 0.7824741897726417\n",
      "Epoch 66, Training Loss: 0.7826469848030492\n",
      "Epoch 67, Training Loss: 0.7824567008735542\n",
      "Epoch 68, Training Loss: 0.781977873726895\n",
      "Epoch 69, Training Loss: 0.7824098939286139\n",
      "Epoch 70, Training Loss: 0.7823171366426281\n",
      "Epoch 71, Training Loss: 0.7820799158031779\n",
      "Epoch 72, Training Loss: 0.7821014182908195\n",
      "Epoch 73, Training Loss: 0.7823371979527007\n",
      "Epoch 74, Training Loss: 0.7815667557985263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:29:46,674] Trial 60 finished with value: 0.6330666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7822355602020608\n",
      "Epoch 1, Training Loss: 0.8955137081005994\n",
      "Epoch 2, Training Loss: 0.8148737517525168\n",
      "Epoch 3, Training Loss: 0.8084799831053789\n",
      "Epoch 4, Training Loss: 0.8061559469559614\n",
      "Epoch 5, Training Loss: 0.8054853501740624\n",
      "Epoch 6, Training Loss: 0.7999678486936233\n",
      "Epoch 7, Training Loss: 0.7968086814880371\n",
      "Epoch 8, Training Loss: 0.7962619413347806\n",
      "Epoch 9, Training Loss: 0.7945041242066552\n",
      "Epoch 10, Training Loss: 0.795259366386077\n",
      "Epoch 11, Training Loss: 0.792781640291214\n",
      "Epoch 12, Training Loss: 0.7919813909250147\n",
      "Epoch 13, Training Loss: 0.7928084426066455\n",
      "Epoch 14, Training Loss: 0.7895788056008956\n",
      "Epoch 15, Training Loss: 0.7905230419776019\n",
      "Epoch 16, Training Loss: 0.7903387641906738\n",
      "Epoch 17, Training Loss: 0.7900729388349197\n",
      "Epoch 18, Training Loss: 0.7888319762314067\n",
      "Epoch 19, Training Loss: 0.7890163844473221\n",
      "Epoch 20, Training Loss: 0.7884341817042407\n",
      "Epoch 21, Training Loss: 0.7877289425625521\n",
      "Epoch 22, Training Loss: 0.7893572594137753\n",
      "Epoch 23, Training Loss: 0.7868431435613071\n",
      "Epoch 24, Training Loss: 0.787200354828554\n",
      "Epoch 25, Training Loss: 0.7869590136584114\n",
      "Epoch 26, Training Loss: 0.786560746010612\n",
      "Epoch 27, Training Loss: 0.7866516711431392\n",
      "Epoch 28, Training Loss: 0.7867903052358066\n",
      "Epoch 29, Training Loss: 0.786863808631897\n",
      "Epoch 30, Training Loss: 0.7858308077559751\n",
      "Epoch 31, Training Loss: 0.7865464123557596\n",
      "Epoch 32, Training Loss: 0.7859545775722054\n",
      "Epoch 33, Training Loss: 0.7860918792556314\n",
      "Epoch 34, Training Loss: 0.7857494455926558\n",
      "Epoch 35, Training Loss: 0.7855635349890765\n",
      "Epoch 36, Training Loss: 0.7849706679933212\n",
      "Epoch 37, Training Loss: 0.7852467377045576\n",
      "Epoch 38, Training Loss: 0.7840268123851103\n",
      "Epoch 39, Training Loss: 0.7846453109208276\n",
      "Epoch 40, Training Loss: 0.7839672783543082\n",
      "Epoch 41, Training Loss: 0.7842455898312961\n",
      "Epoch 42, Training Loss: 0.7841934747555677\n",
      "Epoch 43, Training Loss: 0.7837387502193451\n",
      "Epoch 44, Training Loss: 0.7833662896997788\n",
      "Epoch 45, Training Loss: 0.7838731150066152\n",
      "Epoch 46, Training Loss: 0.7837380151888903\n",
      "Epoch 47, Training Loss: 0.7829817999110502\n",
      "Epoch 48, Training Loss: 0.7833790186573477\n",
      "Epoch 49, Training Loss: 0.7829282870713402\n",
      "Epoch 50, Training Loss: 0.7831348540502436\n",
      "Epoch 51, Training Loss: 0.7827290612108567\n",
      "Epoch 52, Training Loss: 0.7829322501491098\n",
      "Epoch 53, Training Loss: 0.7827765520179973\n",
      "Epoch 54, Training Loss: 0.7821522539503434\n",
      "Epoch 55, Training Loss: 0.7823726195447586\n",
      "Epoch 56, Training Loss: 0.7817698866479537\n",
      "Epoch 57, Training Loss: 0.782118952554815\n",
      "Epoch 58, Training Loss: 0.7823692438181709\n",
      "Epoch 59, Training Loss: 0.7815400923700894\n",
      "Epoch 60, Training Loss: 0.7813065211913165\n",
      "Epoch 61, Training Loss: 0.7813966922900256\n",
      "Epoch 62, Training Loss: 0.7815340731424444\n",
      "Epoch 63, Training Loss: 0.7815792480637046\n",
      "Epoch 64, Training Loss: 0.781109537377077\n",
      "Epoch 65, Training Loss: 0.7815667829092812\n",
      "Epoch 66, Training Loss: 0.7805871462120729\n",
      "Epoch 67, Training Loss: 0.7805913141194512\n",
      "Epoch 68, Training Loss: 0.780524192277123\n",
      "Epoch 69, Training Loss: 0.7807795948140761\n",
      "Epoch 70, Training Loss: 0.7803502507770763\n",
      "Epoch 71, Training Loss: 0.7806596933392917\n",
      "Epoch 72, Training Loss: 0.7803992268618415\n",
      "Epoch 73, Training Loss: 0.7802432848425472\n",
      "Epoch 74, Training Loss: 0.7800351518743178\n",
      "Epoch 75, Training Loss: 0.7803183316483218\n",
      "Epoch 76, Training Loss: 0.7801278082062216\n",
      "Epoch 77, Training Loss: 0.7795304327852586\n",
      "Epoch 78, Training Loss: 0.7798810904166278\n",
      "Epoch 79, Training Loss: 0.7794149296423968\n",
      "Epoch 80, Training Loss: 0.7790707862377166\n",
      "Epoch 81, Training Loss: 0.7791963272234973\n",
      "Epoch 82, Training Loss: 0.7789579365533941\n",
      "Epoch 83, Training Loss: 0.7790154373645782\n",
      "Epoch 84, Training Loss: 0.7791285998681012\n",
      "Epoch 85, Training Loss: 0.7785998558296877\n",
      "Epoch 86, Training Loss: 0.7786987234564389\n",
      "Epoch 87, Training Loss: 0.7783666906637304\n",
      "Epoch 88, Training Loss: 0.7780665337338167\n",
      "Epoch 89, Training Loss: 0.778442914205439\n",
      "Epoch 90, Training Loss: 0.7779692083947799\n",
      "Epoch 91, Training Loss: 0.7784018207998836\n",
      "Epoch 92, Training Loss: 0.7786138735799228\n",
      "Epoch 93, Training Loss: 0.7775613151578342\n",
      "Epoch 94, Training Loss: 0.7782167008343865\n",
      "Epoch 95, Training Loss: 0.7778329964245067\n",
      "Epoch 96, Training Loss: 0.777829157184152\n",
      "Epoch 97, Training Loss: 0.7778067572677837\n",
      "Epoch 98, Training Loss: 0.7772565335385939\n",
      "Epoch 99, Training Loss: 0.7778128728445839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:32:15,065] Trial 61 finished with value: 0.6379333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7777518227521111\n",
      "Epoch 1, Training Loss: 0.9773657161348006\n",
      "Epoch 2, Training Loss: 0.9353904792140512\n",
      "Epoch 3, Training Loss: 0.9266819007256452\n",
      "Epoch 4, Training Loss: 0.920235300765318\n",
      "Epoch 5, Training Loss: 0.914478839004741\n",
      "Epoch 6, Training Loss: 0.9088744682424209\n",
      "Epoch 7, Training Loss: 0.90324475218268\n",
      "Epoch 8, Training Loss: 0.8972679432700662\n",
      "Epoch 9, Training Loss: 0.8909301837752847\n",
      "Epoch 10, Training Loss: 0.8839890887456782\n",
      "Epoch 11, Training Loss: 0.8765730387323043\n",
      "Epoch 12, Training Loss: 0.8685950986076804\n",
      "Epoch 13, Training Loss: 0.8603241502537446\n",
      "Epoch 14, Training Loss: 0.8525914138204911\n",
      "Epoch 15, Training Loss: 0.84517393098158\n",
      "Epoch 16, Training Loss: 0.8388237420250387\n",
      "Epoch 17, Training Loss: 0.8330209051160251\n",
      "Epoch 18, Training Loss: 0.8280198911358329\n",
      "Epoch 19, Training Loss: 0.8236617270637961\n",
      "Epoch 20, Training Loss: 0.8200580103958355\n",
      "Epoch 21, Training Loss: 0.817035560537787\n",
      "Epoch 22, Training Loss: 0.8145154582752901\n",
      "Epoch 23, Training Loss: 0.8124453489219441\n",
      "Epoch 24, Training Loss: 0.8107822390163646\n",
      "Epoch 25, Training Loss: 0.8092024021288928\n",
      "Epoch 26, Training Loss: 0.8080071350406198\n",
      "Epoch 27, Training Loss: 0.8069481478017919\n",
      "Epoch 28, Training Loss: 0.8060139733202317\n",
      "Epoch 29, Training Loss: 0.805218166253146\n",
      "Epoch 30, Training Loss: 0.8046382781337289\n",
      "Epoch 31, Training Loss: 0.8040251684188843\n",
      "Epoch 32, Training Loss: 0.8035579825850094\n",
      "Epoch 33, Training Loss: 0.8031069503812229\n",
      "Epoch 34, Training Loss: 0.8027610989879159\n",
      "Epoch 35, Training Loss: 0.802402834261165\n",
      "Epoch 36, Training Loss: 0.8020495733092813\n",
      "Epoch 37, Training Loss: 0.8017855019429151\n",
      "Epoch 38, Training Loss: 0.8015381439994363\n",
      "Epoch 39, Training Loss: 0.801205585493761\n",
      "Epoch 40, Training Loss: 0.8011059101890116\n",
      "Epoch 41, Training Loss: 0.8009591477758744\n",
      "Epoch 42, Training Loss: 0.8008458493737614\n",
      "Epoch 43, Training Loss: 0.8006366191892063\n",
      "Epoch 44, Training Loss: 0.8004679957558127\n",
      "Epoch 45, Training Loss: 0.8003606164455414\n",
      "Epoch 46, Training Loss: 0.8002577865123749\n",
      "Epoch 47, Training Loss: 0.800053059423671\n",
      "Epoch 48, Training Loss: 0.7998134104644551\n",
      "Epoch 49, Training Loss: 0.7997824832271128\n",
      "Epoch 50, Training Loss: 0.7996558997210335\n",
      "Epoch 51, Training Loss: 0.7995305012955385\n",
      "Epoch 52, Training Loss: 0.7994917710388408\n",
      "Epoch 53, Training Loss: 0.7995326612276189\n",
      "Epoch 54, Training Loss: 0.7993371532945072\n",
      "Epoch 55, Training Loss: 0.7993792578052072\n",
      "Epoch 56, Training Loss: 0.7991977357163149\n",
      "Epoch 57, Training Loss: 0.7991237042230718\n",
      "Epoch 58, Training Loss: 0.7991487134905423\n",
      "Epoch 59, Training Loss: 0.7989927043634303\n",
      "Epoch 60, Training Loss: 0.7988371368716745\n",
      "Epoch 61, Training Loss: 0.7988132175277262\n",
      "Epoch 62, Training Loss: 0.7987143075466157\n",
      "Epoch 63, Training Loss: 0.7987427500416251\n",
      "Epoch 64, Training Loss: 0.7986945762353785\n",
      "Epoch 65, Training Loss: 0.7986000969830681\n",
      "Epoch 66, Training Loss: 0.7984807574748993\n",
      "Epoch 67, Training Loss: 0.7983838675302618\n",
      "Epoch 68, Training Loss: 0.7983358409825493\n",
      "Epoch 69, Training Loss: 0.7981867551803589\n",
      "Epoch 70, Training Loss: 0.7982530591067146\n",
      "Epoch 71, Training Loss: 0.7979839215559118\n",
      "Epoch 72, Training Loss: 0.7981137317068436\n",
      "Epoch 73, Training Loss: 0.7981106866107267\n",
      "Epoch 74, Training Loss: 0.7980452477230745\n",
      "Epoch 75, Training Loss: 0.797950113661149\n",
      "Epoch 76, Training Loss: 0.7979593236306134\n",
      "Epoch 77, Training Loss: 0.7977245428982903\n",
      "Epoch 78, Training Loss: 0.7978677496489357\n",
      "Epoch 79, Training Loss: 0.7977977856467752\n",
      "Epoch 80, Training Loss: 0.7976711301242604\n",
      "Epoch 81, Training Loss: 0.7976118976929608\n",
      "Epoch 82, Training Loss: 0.7976157019418829\n",
      "Epoch 83, Training Loss: 0.7976846485979416\n",
      "Epoch 84, Training Loss: 0.7975341431533589\n",
      "Epoch 85, Training Loss: 0.7975443977468154\n",
      "Epoch 86, Training Loss: 0.7975728494980756\n",
      "Epoch 87, Training Loss: 0.7974332943383385\n",
      "Epoch 88, Training Loss: 0.797425406259649\n",
      "Epoch 89, Training Loss: 0.7972640495440539\n",
      "Epoch 90, Training Loss: 0.797298262610155\n",
      "Epoch 91, Training Loss: 0.7972795642824734\n",
      "Epoch 92, Training Loss: 0.7971543883576112\n",
      "Epoch 93, Training Loss: 0.7971733587629655\n",
      "Epoch 94, Training Loss: 0.7971510733576382\n",
      "Epoch 95, Training Loss: 0.7970812345252317\n",
      "Epoch 96, Training Loss: 0.7971392386801103\n",
      "Epoch 97, Training Loss: 0.7971598547346451\n",
      "Epoch 98, Training Loss: 0.7970314928363351\n",
      "Epoch 99, Training Loss: 0.7970287508824292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:33:49,788] Trial 62 finished with value: 0.6348666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7969109302408555\n",
      "Epoch 1, Training Loss: 0.9268274404948815\n",
      "Epoch 2, Training Loss: 0.8347619991553458\n",
      "Epoch 3, Training Loss: 0.826586899990426\n",
      "Epoch 4, Training Loss: 0.8204436795155805\n",
      "Epoch 5, Training Loss: 0.8188213201393758\n",
      "Epoch 6, Training Loss: 0.8159264197923187\n",
      "Epoch 7, Training Loss: 0.8124878523045016\n",
      "Epoch 8, Training Loss: 0.812246010536538\n",
      "Epoch 9, Training Loss: 0.81121005420398\n",
      "Epoch 10, Training Loss: 0.8098741259790004\n",
      "Epoch 11, Training Loss: 0.8089396293898274\n",
      "Epoch 12, Training Loss: 0.8089681997335046\n",
      "Epoch 13, Training Loss: 0.8082675964312446\n",
      "Epoch 14, Training Loss: 0.8059645137392488\n",
      "Epoch 15, Training Loss: 0.8065282025731596\n",
      "Epoch 16, Training Loss: 0.8062511127694209\n",
      "Epoch 17, Training Loss: 0.8084743740863369\n",
      "Epoch 18, Training Loss: 0.8050700172445828\n",
      "Epoch 19, Training Loss: 0.805140363184133\n",
      "Epoch 20, Training Loss: 0.8058417085418128\n",
      "Epoch 21, Training Loss: 0.8056375783188898\n",
      "Epoch 22, Training Loss: 0.8040841042547298\n",
      "Epoch 23, Training Loss: 0.805010191569651\n",
      "Epoch 24, Training Loss: 0.8043198924315603\n",
      "Epoch 25, Training Loss: 0.8036199635132811\n",
      "Epoch 26, Training Loss: 0.8038894130771321\n",
      "Epoch 27, Training Loss: 0.8025289350882508\n",
      "Epoch 28, Training Loss: 0.8031037932051752\n",
      "Epoch 29, Training Loss: 0.8034722189257916\n",
      "Epoch 30, Training Loss: 0.8035534894556031\n",
      "Epoch 31, Training Loss: 0.8027614488637537\n",
      "Epoch 32, Training Loss: 0.8024934124229546\n",
      "Epoch 33, Training Loss: 0.8025073328412565\n",
      "Epoch 34, Training Loss: 0.80334583153402\n",
      "Epoch 35, Training Loss: 0.801941182290701\n",
      "Epoch 36, Training Loss: 0.8023317128195798\n",
      "Epoch 37, Training Loss: 0.8010635667277458\n",
      "Epoch 38, Training Loss: 0.8022409045606628\n",
      "Epoch 39, Training Loss: 0.8032021374630749\n",
      "Epoch 40, Training Loss: 0.8027342082862567\n",
      "Epoch 41, Training Loss: 0.8036423763834444\n",
      "Epoch 42, Training Loss: 0.8021553528039975\n",
      "Epoch 43, Training Loss: 0.8016875607626779\n",
      "Epoch 44, Training Loss: 0.8029244964284108\n",
      "Epoch 45, Training Loss: 0.8020035549214012\n",
      "Epoch 46, Training Loss: 0.8028883865005092\n",
      "Epoch 47, Training Loss: 0.8009951496482792\n",
      "Epoch 48, Training Loss: 0.8019837041546528\n",
      "Epoch 49, Training Loss: 0.8025250894682748\n",
      "Epoch 50, Training Loss: 0.8006719165278556\n",
      "Epoch 51, Training Loss: 0.8024172509523263\n",
      "Epoch 52, Training Loss: 0.8023865733827863\n",
      "Epoch 53, Training Loss: 0.8018987613513058\n",
      "Epoch 54, Training Loss: 0.8016073148053392\n",
      "Epoch 55, Training Loss: 0.8016303083950416\n",
      "Epoch 56, Training Loss: 0.8019060700459588\n",
      "Epoch 57, Training Loss: 0.8021681047919997\n",
      "Epoch 58, Training Loss: 0.8025042673698941\n",
      "Epoch 59, Training Loss: 0.8022269188013292\n",
      "Epoch 60, Training Loss: 0.8016297618249305\n",
      "Epoch 61, Training Loss: 0.8011621269964634\n",
      "Epoch 62, Training Loss: 0.8014960742534551\n",
      "Epoch 63, Training Loss: 0.8026667068775435\n",
      "Epoch 64, Training Loss: 0.8015669205134972\n",
      "Epoch 65, Training Loss: 0.8007621080355537\n",
      "Epoch 66, Training Loss: 0.8004332316549201\n",
      "Epoch 67, Training Loss: 0.8004084177483294\n",
      "Epoch 68, Training Loss: 0.8023342436417601\n",
      "Epoch 69, Training Loss: 0.8013182996807242\n",
      "Epoch 70, Training Loss: 0.8034886188973162\n",
      "Epoch 71, Training Loss: 0.8010435695038702\n",
      "Epoch 72, Training Loss: 0.8019405570245327\n",
      "Epoch 73, Training Loss: 0.8025132798610773\n",
      "Epoch 74, Training Loss: 0.801241814193869\n",
      "Epoch 75, Training Loss: 0.8009531001399334\n",
      "Epoch 76, Training Loss: 0.8014265763132196\n",
      "Epoch 77, Training Loss: 0.800859532768565\n",
      "Epoch 78, Training Loss: 0.8015077221662479\n",
      "Epoch 79, Training Loss: 0.8012668551358961\n",
      "Epoch 80, Training Loss: 0.8011235675417391\n",
      "Epoch 81, Training Loss: 0.8013060787566623\n",
      "Epoch 82, Training Loss: 0.7996182720463975\n",
      "Epoch 83, Training Loss: 0.8008713715058521\n",
      "Epoch 84, Training Loss: 0.7985761438097273\n",
      "Epoch 85, Training Loss: 0.8003678318253137\n",
      "Epoch 86, Training Loss: 0.800752608310011\n",
      "Epoch 87, Training Loss: 0.801810093840262\n",
      "Epoch 88, Training Loss: 0.801395815476439\n",
      "Epoch 89, Training Loss: 0.8000128350759808\n",
      "Epoch 90, Training Loss: 0.8011946106315555\n",
      "Epoch 91, Training Loss: 0.8003985124423092\n",
      "Epoch 92, Training Loss: 0.8003907529035009\n",
      "Epoch 93, Training Loss: 0.7996900172161877\n",
      "Epoch 94, Training Loss: 0.800412981223343\n",
      "Epoch 95, Training Loss: 0.8006840149262794\n",
      "Epoch 96, Training Loss: 0.7994106409244968\n",
      "Epoch 97, Training Loss: 0.8006641628150654\n",
      "Epoch 98, Training Loss: 0.8006975324530351\n",
      "Epoch 99, Training Loss: 0.7996609832111158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:35:53,551] Trial 63 finished with value: 0.5736 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7999134831858757\n",
      "Epoch 1, Training Loss: 0.8941092904175029\n",
      "Epoch 2, Training Loss: 0.8157955331662122\n",
      "Epoch 3, Training Loss: 0.8117005963185254\n",
      "Epoch 4, Training Loss: 0.8093595978792976\n",
      "Epoch 5, Training Loss: 0.8077323439541985\n",
      "Epoch 6, Training Loss: 0.8056471593940959\n",
      "Epoch 7, Training Loss: 0.8052118342764237\n",
      "Epoch 8, Training Loss: 0.8034681643457974\n",
      "Epoch 9, Training Loss: 0.8026546170431025\n",
      "Epoch 10, Training Loss: 0.8017518676729763\n",
      "Epoch 11, Training Loss: 0.801773755199769\n",
      "Epoch 12, Training Loss: 0.8006445729031282\n",
      "Epoch 13, Training Loss: 0.7994092783507178\n",
      "Epoch 14, Training Loss: 0.7999023271308225\n",
      "Epoch 15, Training Loss: 0.7989041545811821\n",
      "Epoch 16, Training Loss: 0.7987590443386751\n",
      "Epoch 17, Training Loss: 0.7982005615094129\n",
      "Epoch 18, Training Loss: 0.7980262334907756\n",
      "Epoch 19, Training Loss: 0.7973216206185958\n",
      "Epoch 20, Training Loss: 0.7969153764668633\n",
      "Epoch 21, Training Loss: 0.796989364343531\n",
      "Epoch 22, Training Loss: 0.795960412235821\n",
      "Epoch 23, Training Loss: 0.7949620884306291\n",
      "Epoch 24, Training Loss: 0.7952420715724721\n",
      "Epoch 25, Training Loss: 0.7946552829181447\n",
      "Epoch 26, Training Loss: 0.7939631408102372\n",
      "Epoch 27, Training Loss: 0.7928652060031891\n",
      "Epoch 28, Training Loss: 0.7927607570676243\n",
      "Epoch 29, Training Loss: 0.7923558271632475\n",
      "Epoch 30, Training Loss: 0.7914811181320863\n",
      "Epoch 31, Training Loss: 0.7909026430634891\n",
      "Epoch 32, Training Loss: 0.7903595113754273\n",
      "Epoch 33, Training Loss: 0.7896520638465881\n",
      "Epoch 34, Training Loss: 0.7889204654272864\n",
      "Epoch 35, Training Loss: 0.7888624102929059\n",
      "Epoch 36, Training Loss: 0.7886266841607935\n",
      "Epoch 37, Training Loss: 0.7881354240108939\n",
      "Epoch 38, Training Loss: 0.787942283223657\n",
      "Epoch 39, Training Loss: 0.7871384302307578\n",
      "Epoch 40, Training Loss: 0.7869539699834935\n",
      "Epoch 41, Training Loss: 0.7865381427372203\n",
      "Epoch 42, Training Loss: 0.7866079350078807\n",
      "Epoch 43, Training Loss: 0.7859443803394542\n",
      "Epoch 44, Training Loss: 0.7863816320194917\n",
      "Epoch 45, Training Loss: 0.7859962291577283\n",
      "Epoch 46, Training Loss: 0.7853651099345264\n",
      "Epoch 47, Training Loss: 0.785429578037823\n",
      "Epoch 48, Training Loss: 0.7853685390949249\n",
      "Epoch 49, Training Loss: 0.7847473523196052\n",
      "Epoch 50, Training Loss: 0.7850844261926763\n",
      "Epoch 51, Training Loss: 0.7852002197153428\n",
      "Epoch 52, Training Loss: 0.7841841121982126\n",
      "Epoch 53, Training Loss: 0.7841563190432156\n",
      "Epoch 54, Training Loss: 0.7846214145071366\n",
      "Epoch 55, Training Loss: 0.7840992601478801\n",
      "Epoch 56, Training Loss: 0.7843933639105628\n",
      "Epoch 57, Training Loss: 0.7837255935809192\n",
      "Epoch 58, Training Loss: 0.784015761122984\n",
      "Epoch 59, Training Loss: 0.7837117024730234\n",
      "Epoch 60, Training Loss: 0.7843029417711146\n",
      "Epoch 61, Training Loss: 0.7833264126497157\n",
      "Epoch 62, Training Loss: 0.7838663408335518\n",
      "Epoch 63, Training Loss: 0.7832547023015863\n",
      "Epoch 64, Training Loss: 0.7832576990127563\n",
      "Epoch 65, Training Loss: 0.7834341431365294\n",
      "Epoch 66, Training Loss: 0.7833746628200307\n",
      "Epoch 67, Training Loss: 0.7836733857323142\n",
      "Epoch 68, Training Loss: 0.7827990243715398\n",
      "Epoch 69, Training Loss: 0.7830319957873401\n",
      "Epoch 70, Training Loss: 0.7829824056344874\n",
      "Epoch 71, Training Loss: 0.7830360411896425\n",
      "Epoch 72, Training Loss: 0.7830365391338573\n",
      "Epoch 73, Training Loss: 0.7829553104148191\n",
      "Epoch 74, Training Loss: 0.7831877880236682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:37:33,090] Trial 64 finished with value: 0.6388666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7827628681940191\n",
      "Epoch 1, Training Loss: 0.8431157316179837\n",
      "Epoch 2, Training Loss: 0.8162910833779503\n",
      "Epoch 3, Training Loss: 0.8068939478257123\n",
      "Epoch 4, Training Loss: 0.8042947051805608\n",
      "Epoch 5, Training Loss: 0.8028951234677258\n",
      "Epoch 6, Training Loss: 0.8020222311160143\n",
      "Epoch 7, Training Loss: 0.8026696272457348\n",
      "Epoch 8, Training Loss: 0.8016957303355722\n",
      "Epoch 9, Training Loss: 0.798935863200356\n",
      "Epoch 10, Training Loss: 0.7987886247214149\n",
      "Epoch 11, Training Loss: 0.7999153933805578\n",
      "Epoch 12, Training Loss: 0.8000017574254205\n",
      "Epoch 13, Training Loss: 0.7986568910935345\n",
      "Epoch 14, Training Loss: 0.798836118964588\n",
      "Epoch 15, Training Loss: 0.7991949833140654\n",
      "Epoch 16, Training Loss: 0.7981111468287075\n",
      "Epoch 17, Training Loss: 0.7956569060157327\n",
      "Epoch 18, Training Loss: 0.7969477778322557\n",
      "Epoch 19, Training Loss: 0.7989195225519292\n",
      "Epoch 20, Training Loss: 0.7978526853112613\n",
      "Epoch 21, Training Loss: 0.796913905915092\n",
      "Epoch 22, Training Loss: 0.7960560161927167\n",
      "Epoch 23, Training Loss: 0.795922244015862\n",
      "Epoch 24, Training Loss: 0.7957633160843569\n",
      "Epoch 25, Training Loss: 0.796575465482824\n",
      "Epoch 26, Training Loss: 0.7957421265630161\n",
      "Epoch 27, Training Loss: 0.7961586481683395\n",
      "Epoch 28, Training Loss: 0.7950870838586022\n",
      "Epoch 29, Training Loss: 0.7954283363678876\n",
      "Epoch 30, Training Loss: 0.7958705263278064\n",
      "Epoch 31, Training Loss: 0.795939787275651\n",
      "Epoch 32, Training Loss: 0.7944036790202645\n",
      "Epoch 33, Training Loss: 0.7954710455151165\n",
      "Epoch 34, Training Loss: 0.7953567331678727\n",
      "Epoch 35, Training Loss: 0.7960301956709693\n",
      "Epoch 36, Training Loss: 0.7951539171443266\n",
      "Epoch 37, Training Loss: 0.7950896939810584\n",
      "Epoch 38, Training Loss: 0.7948828978398267\n",
      "Epoch 39, Training Loss: 0.7938951653592726\n",
      "Epoch 40, Training Loss: 0.7949841216732474\n",
      "Epoch 41, Training Loss: 0.7946103400342605\n",
      "Epoch 42, Training Loss: 0.7947281139738419\n",
      "Epoch 43, Training Loss: 0.7940149929944207\n",
      "Epoch 44, Training Loss: 0.7942982996211333\n",
      "Epoch 45, Training Loss: 0.7936927416044123\n",
      "Epoch 46, Training Loss: 0.7947085017316482\n",
      "Epoch 47, Training Loss: 0.7926661619719337\n",
      "Epoch 48, Training Loss: 0.7934296001406277\n",
      "Epoch 49, Training Loss: 0.7934795234483831\n",
      "Epoch 50, Training Loss: 0.7950958656563478\n",
      "Epoch 51, Training Loss: 0.7940552000438466\n",
      "Epoch 52, Training Loss: 0.7921557550570544\n",
      "Epoch 53, Training Loss: 0.7922659161511589\n",
      "Epoch 54, Training Loss: 0.793728691479739\n",
      "Epoch 55, Training Loss: 0.7928157491543714\n",
      "Epoch 56, Training Loss: 0.793595102113836\n",
      "Epoch 57, Training Loss: 0.7939290171511033\n",
      "Epoch 58, Training Loss: 0.7935973450716804\n",
      "Epoch 59, Training Loss: 0.793001573716893\n",
      "Epoch 60, Training Loss: 0.7932298857324264\n",
      "Epoch 61, Training Loss: 0.7941111817780663\n",
      "Epoch 62, Training Loss: 0.7926720383588005\n",
      "Epoch 63, Training Loss: 0.7929555575286641\n",
      "Epoch 64, Training Loss: 0.7935565523540272\n",
      "Epoch 65, Training Loss: 0.7933812785148621\n",
      "Epoch 66, Training Loss: 0.7929742564173305\n",
      "Epoch 67, Training Loss: 0.7938783324466032\n",
      "Epoch 68, Training Loss: 0.7930324522887959\n",
      "Epoch 69, Training Loss: 0.7935752205287709\n",
      "Epoch 70, Training Loss: 0.7929104699808008\n",
      "Epoch 71, Training Loss: 0.7928412919885972\n",
      "Epoch 72, Training Loss: 0.7930654676521526\n",
      "Epoch 73, Training Loss: 0.7935304565990673\n",
      "Epoch 74, Training Loss: 0.7928166601237129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:39:16,050] Trial 65 finished with value: 0.636 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7942381673700669\n",
      "Epoch 1, Training Loss: 1.0081431859417966\n",
      "Epoch 2, Training Loss: 0.9298286349253547\n",
      "Epoch 3, Training Loss: 0.9143111079259026\n",
      "Epoch 4, Training Loss: 0.9006180141205178\n",
      "Epoch 5, Training Loss: 0.8819332522557194\n",
      "Epoch 6, Training Loss: 0.8566331799765279\n",
      "Epoch 7, Training Loss: 0.8308358336749829\n",
      "Epoch 8, Training Loss: 0.8155656923476915\n",
      "Epoch 9, Training Loss: 0.8101106542393677\n",
      "Epoch 10, Training Loss: 0.8070447836603437\n",
      "Epoch 11, Training Loss: 0.8054634807701397\n",
      "Epoch 12, Training Loss: 0.8043744855357292\n",
      "Epoch 13, Training Loss: 0.8029332546363199\n",
      "Epoch 14, Training Loss: 0.8026099115386045\n",
      "Epoch 15, Training Loss: 0.80162129518681\n",
      "Epoch 16, Training Loss: 0.8007113395776964\n",
      "Epoch 17, Training Loss: 0.7988677848998765\n",
      "Epoch 18, Training Loss: 0.798940480382819\n",
      "Epoch 19, Training Loss: 0.7980909793896782\n",
      "Epoch 20, Training Loss: 0.7979960853892162\n",
      "Epoch 21, Training Loss: 0.7972734596496238\n",
      "Epoch 22, Training Loss: 0.7964477020995061\n",
      "Epoch 23, Training Loss: 0.7955835035869053\n",
      "Epoch 24, Training Loss: 0.7951217507957515\n",
      "Epoch 25, Training Loss: 0.7944379139663582\n",
      "Epoch 26, Training Loss: 0.7940974447960244\n",
      "Epoch 27, Training Loss: 0.7933932975718849\n",
      "Epoch 28, Training Loss: 0.7938113622199324\n",
      "Epoch 29, Training Loss: 0.792150285414287\n",
      "Epoch 30, Training Loss: 0.792209449836186\n",
      "Epoch 31, Training Loss: 0.7920688665002809\n",
      "Epoch 32, Training Loss: 0.7921010847378495\n",
      "Epoch 33, Training Loss: 0.7924679627095846\n",
      "Epoch 34, Training Loss: 0.7914583227688209\n",
      "Epoch 35, Training Loss: 0.790753608269799\n",
      "Epoch 36, Training Loss: 0.7906531611779579\n",
      "Epoch 37, Training Loss: 0.7903047042681759\n",
      "Epoch 38, Training Loss: 0.7898016187481414\n",
      "Epoch 39, Training Loss: 0.7894389348818843\n",
      "Epoch 40, Training Loss: 0.7893572686310101\n",
      "Epoch 41, Training Loss: 0.7887863791078553\n",
      "Epoch 42, Training Loss: 0.7886378846670452\n",
      "Epoch 43, Training Loss: 0.7887028368792139\n",
      "Epoch 44, Training Loss: 0.78796886958574\n",
      "Epoch 45, Training Loss: 0.788009392318869\n",
      "Epoch 46, Training Loss: 0.788039405453474\n",
      "Epoch 47, Training Loss: 0.7875331403617573\n",
      "Epoch 48, Training Loss: 0.786986732303648\n",
      "Epoch 49, Training Loss: 0.7874527899842513\n",
      "Epoch 50, Training Loss: 0.7873069612603438\n",
      "Epoch 51, Training Loss: 0.7866306762049969\n",
      "Epoch 52, Training Loss: 0.7866035364624253\n",
      "Epoch 53, Training Loss: 0.7866354054974434\n",
      "Epoch 54, Training Loss: 0.7869964256322474\n",
      "Epoch 55, Training Loss: 0.7862938705243563\n",
      "Epoch 56, Training Loss: 0.7864847258517617\n",
      "Epoch 57, Training Loss: 0.7861837093095134\n",
      "Epoch 58, Training Loss: 0.7864574189472916\n",
      "Epoch 59, Training Loss: 0.7853341297995775\n",
      "Epoch 60, Training Loss: 0.78574666743888\n",
      "Epoch 61, Training Loss: 0.7857574615263401\n",
      "Epoch 62, Training Loss: 0.78554086541771\n",
      "Epoch 63, Training Loss: 0.7854642307847962\n",
      "Epoch 64, Training Loss: 0.7848659966225014\n",
      "Epoch 65, Training Loss: 0.7843317995394082\n",
      "Epoch 66, Training Loss: 0.7847764383581348\n",
      "Epoch 67, Training Loss: 0.7843711991059152\n",
      "Epoch 68, Training Loss: 0.7848372909359466\n",
      "Epoch 69, Training Loss: 0.7847835314901251\n",
      "Epoch 70, Training Loss: 0.7851698533932966\n",
      "Epoch 71, Training Loss: 0.7850742747012834\n",
      "Epoch 72, Training Loss: 0.784686787773792\n",
      "Epoch 73, Training Loss: 0.7841176980420163\n",
      "Epoch 74, Training Loss: 0.7847913209657024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:40:25,875] Trial 66 finished with value: 0.6286 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7841393451941641\n",
      "Epoch 1, Training Loss: 0.9291108918891233\n",
      "Epoch 2, Training Loss: 0.8603846450412974\n",
      "Epoch 3, Training Loss: 0.8259960074985728\n",
      "Epoch 4, Training Loss: 0.8150098144306857\n",
      "Epoch 5, Training Loss: 0.8117334205964032\n",
      "Epoch 6, Training Loss: 0.8101677722790662\n",
      "Epoch 7, Training Loss: 0.8091417614151449\n",
      "Epoch 8, Training Loss: 0.8083043826327604\n",
      "Epoch 9, Training Loss: 0.8076561589802013\n",
      "Epoch 10, Training Loss: 0.8073545493799097\n",
      "Epoch 11, Training Loss: 0.8066276405839359\n",
      "Epoch 12, Training Loss: 0.8063409080224878\n",
      "Epoch 13, Training Loss: 0.8056498923722435\n",
      "Epoch 14, Training Loss: 0.8057120108604431\n",
      "Epoch 15, Training Loss: 0.8053612566695494\n",
      "Epoch 16, Training Loss: 0.8049515321675469\n",
      "Epoch 17, Training Loss: 0.8050107248390422\n",
      "Epoch 18, Training Loss: 0.8047971653938294\n",
      "Epoch 19, Training Loss: 0.8044797733250786\n",
      "Epoch 20, Training Loss: 0.8040262166892781\n",
      "Epoch 21, Training Loss: 0.8037359314806322\n",
      "Epoch 22, Training Loss: 0.8036116954859566\n",
      "Epoch 23, Training Loss: 0.8033432527149424\n",
      "Epoch 24, Training Loss: 0.803117764486986\n",
      "Epoch 25, Training Loss: 0.8027864746486439\n",
      "Epoch 26, Training Loss: 0.8027849268913269\n",
      "Epoch 27, Training Loss: 0.8025420077408061\n",
      "Epoch 28, Training Loss: 0.802258123439901\n",
      "Epoch 29, Training Loss: 0.8020923527549295\n",
      "Epoch 30, Training Loss: 0.8017933851129868\n",
      "Epoch 31, Training Loss: 0.8015680566956015\n",
      "Epoch 32, Training Loss: 0.8017954514307134\n",
      "Epoch 33, Training Loss: 0.8013966439050787\n",
      "Epoch 34, Training Loss: 0.8014412089656381\n",
      "Epoch 35, Training Loss: 0.8011913357061499\n",
      "Epoch 36, Training Loss: 0.8012130607576932\n",
      "Epoch 37, Training Loss: 0.8007629332121681\n",
      "Epoch 38, Training Loss: 0.8007634583641501\n",
      "Epoch 39, Training Loss: 0.8003887839878306\n",
      "Epoch 40, Training Loss: 0.8005025411353391\n",
      "Epoch 41, Training Loss: 0.8002490577978246\n",
      "Epoch 42, Training Loss: 0.8003589704457451\n",
      "Epoch 43, Training Loss: 0.800225989187465\n",
      "Epoch 44, Training Loss: 0.800213471440708\n",
      "Epoch 45, Training Loss: 0.7999231436673333\n",
      "Epoch 46, Training Loss: 0.7998754903148202\n",
      "Epoch 47, Training Loss: 0.7997380574310528\n",
      "Epoch 48, Training Loss: 0.7993687080635744\n",
      "Epoch 49, Training Loss: 0.7996183684292961\n",
      "Epoch 50, Training Loss: 0.7992559860734378\n",
      "Epoch 51, Training Loss: 0.7994336490070119\n",
      "Epoch 52, Training Loss: 0.7994681428460514\n",
      "Epoch 53, Training Loss: 0.7990643685705522\n",
      "Epoch 54, Training Loss: 0.799240178010043\n",
      "Epoch 55, Training Loss: 0.7990632884642657\n",
      "Epoch 56, Training Loss: 0.7988531537616954\n",
      "Epoch 57, Training Loss: 0.7987637402029598\n",
      "Epoch 58, Training Loss: 0.7988234107634601\n",
      "Epoch 59, Training Loss: 0.7986950430449318\n",
      "Epoch 60, Training Loss: 0.7987683485536015\n",
      "Epoch 61, Training Loss: 0.7989372244301964\n",
      "Epoch 62, Training Loss: 0.798518727386699\n",
      "Epoch 63, Training Loss: 0.7984029344250174\n",
      "Epoch 64, Training Loss: 0.79843482150751\n",
      "Epoch 65, Training Loss: 0.7985309169572943\n",
      "Epoch 66, Training Loss: 0.7982286890815286\n",
      "Epoch 67, Training Loss: 0.7985373049623826\n",
      "Epoch 68, Training Loss: 0.7982071980308084\n",
      "Epoch 69, Training Loss: 0.7979670133310206\n",
      "Epoch 70, Training Loss: 0.7983098037102643\n",
      "Epoch 71, Training Loss: 0.7979964749252095\n",
      "Epoch 72, Training Loss: 0.7978943900501027\n",
      "Epoch 73, Training Loss: 0.798055795992122\n",
      "Epoch 74, Training Loss: 0.7980117414979374\n",
      "Epoch 75, Training Loss: 0.7979908489479738\n",
      "Epoch 76, Training Loss: 0.7979338488859289\n",
      "Epoch 77, Training Loss: 0.7981151873925153\n",
      "Epoch 78, Training Loss: 0.7981479733831742\n",
      "Epoch 79, Training Loss: 0.7979337100421681\n",
      "Epoch 80, Training Loss: 0.79774498147123\n",
      "Epoch 81, Training Loss: 0.7978047616341535\n",
      "Epoch 82, Training Loss: 0.7975732804045957\n",
      "Epoch 83, Training Loss: 0.7977644665802226\n",
      "Epoch 84, Training Loss: 0.7975046433420743\n",
      "Epoch 85, Training Loss: 0.7977425524066476\n",
      "Epoch 86, Training Loss: 0.797811469260384\n",
      "Epoch 87, Training Loss: 0.7974720780288472\n",
      "Epoch 88, Training Loss: 0.7975712575631984\n",
      "Epoch 89, Training Loss: 0.7972504751121297\n",
      "Epoch 90, Training Loss: 0.7974295711517334\n",
      "Epoch 91, Training Loss: 0.7973407330933739\n",
      "Epoch 92, Training Loss: 0.797312426777447\n",
      "Epoch 93, Training Loss: 0.7973126546775593\n",
      "Epoch 94, Training Loss: 0.797444904201171\n",
      "Epoch 95, Training Loss: 0.7973692189244663\n",
      "Epoch 96, Training Loss: 0.7972753880304448\n",
      "Epoch 97, Training Loss: 0.7972167961737688\n",
      "Epoch 98, Training Loss: 0.7972090892931994\n",
      "Epoch 99, Training Loss: 0.797270810884588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:42:15,025] Trial 67 finished with value: 0.6344 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7971861764262704\n",
      "Epoch 1, Training Loss: 0.8850557373551762\n",
      "Epoch 2, Training Loss: 0.821954381536035\n",
      "Epoch 3, Training Loss: 0.8154962635040284\n",
      "Epoch 4, Training Loss: 0.8086114724243388\n",
      "Epoch 5, Training Loss: 0.8045169259520139\n",
      "Epoch 6, Training Loss: 0.8008301852029912\n",
      "Epoch 7, Training Loss: 0.7993667294698603\n",
      "Epoch 8, Training Loss: 0.7973966023501228\n",
      "Epoch 9, Training Loss: 0.7962929411495433\n",
      "Epoch 10, Training Loss: 0.7945137469908771\n",
      "Epoch 11, Training Loss: 0.7937358513299156\n",
      "Epoch 12, Training Loss: 0.7932333072494058\n",
      "Epoch 13, Training Loss: 0.792708408762427\n",
      "Epoch 14, Training Loss: 0.7926244586355546\n",
      "Epoch 15, Training Loss: 0.7918104534289416\n",
      "Epoch 16, Training Loss: 0.7914043294682223\n",
      "Epoch 17, Training Loss: 0.7917521638729993\n",
      "Epoch 18, Training Loss: 0.7906885487893048\n",
      "Epoch 19, Training Loss: 0.7906201575784122\n",
      "Epoch 20, Training Loss: 0.7904060274011948\n",
      "Epoch 21, Training Loss: 0.790157314118217\n",
      "Epoch 22, Training Loss: 0.7889172486697926\n",
      "Epoch 23, Training Loss: 0.7894802742845872\n",
      "Epoch 24, Training Loss: 0.7886415006833918\n",
      "Epoch 25, Training Loss: 0.788720169137506\n",
      "Epoch 26, Training Loss: 0.7886883293179905\n",
      "Epoch 27, Training Loss: 0.7885087607187383\n",
      "Epoch 28, Training Loss: 0.7885205773746267\n",
      "Epoch 29, Training Loss: 0.7882903627788319\n",
      "Epoch 30, Training Loss: 0.7874847607752856\n",
      "Epoch 31, Training Loss: 0.7877798341302311\n",
      "Epoch 32, Training Loss: 0.7872122918858248\n",
      "Epoch 33, Training Loss: 0.7871804533986484\n",
      "Epoch 34, Training Loss: 0.7870996126707862\n",
      "Epoch 35, Training Loss: 0.7871490537419039\n",
      "Epoch 36, Training Loss: 0.7867762001822977\n",
      "Epoch 37, Training Loss: 0.7864419347398421\n",
      "Epoch 38, Training Loss: 0.7862214052677154\n",
      "Epoch 39, Training Loss: 0.7859677984434016\n",
      "Epoch 40, Training Loss: 0.7857424621722278\n",
      "Epoch 41, Training Loss: 0.7861132008889142\n",
      "Epoch 42, Training Loss: 0.7859566420667312\n",
      "Epoch 43, Training Loss: 0.7855559828702141\n",
      "Epoch 44, Training Loss: 0.7858286641625797\n",
      "Epoch 45, Training Loss: 0.7851195797499488\n",
      "Epoch 46, Training Loss: 0.7858474158539491\n",
      "Epoch 47, Training Loss: 0.7854753334382001\n",
      "Epoch 48, Training Loss: 0.7852437208680546\n",
      "Epoch 49, Training Loss: 0.7852755547972287\n",
      "Epoch 50, Training Loss: 0.7850247599096859\n",
      "Epoch 51, Training Loss: 0.784522122705684\n",
      "Epoch 52, Training Loss: 0.7848133655155406\n",
      "Epoch 53, Training Loss: 0.7851383757591247\n",
      "Epoch 54, Training Loss: 0.784998844651615\n",
      "Epoch 55, Training Loss: 0.7846950646007762\n",
      "Epoch 56, Training Loss: 0.7844847552916583\n",
      "Epoch 57, Training Loss: 0.7841884302391725\n",
      "Epoch 58, Training Loss: 0.7843002694494584\n",
      "Epoch 59, Training Loss: 0.784232905401903\n",
      "Epoch 60, Training Loss: 0.78454370673965\n",
      "Epoch 61, Training Loss: 0.783870226986268\n",
      "Epoch 62, Training Loss: 0.7841607236862183\n",
      "Epoch 63, Training Loss: 0.7843677830696106\n",
      "Epoch 64, Training Loss: 0.784061405658722\n",
      "Epoch 65, Training Loss: 0.7842857369955848\n",
      "Epoch 66, Training Loss: 0.7841274296536165\n",
      "Epoch 67, Training Loss: 0.7835877790170558\n",
      "Epoch 68, Training Loss: 0.7841314760376425\n",
      "Epoch 69, Training Loss: 0.7841652638771954\n",
      "Epoch 70, Training Loss: 0.7836524274769952\n",
      "Epoch 71, Training Loss: 0.7835605807164137\n",
      "Epoch 72, Training Loss: 0.7839022348207586\n",
      "Epoch 73, Training Loss: 0.7837299846200382\n",
      "Epoch 74, Training Loss: 0.78379430251963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:43:50,276] Trial 68 finished with value: 0.6330666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7841910929539624\n",
      "Epoch 1, Training Loss: 0.8687829087551375\n",
      "Epoch 2, Training Loss: 0.819012889557315\n",
      "Epoch 3, Training Loss: 0.8117080597949208\n",
      "Epoch 4, Training Loss: 0.8044920079690173\n",
      "Epoch 5, Training Loss: 0.8001356782769798\n",
      "Epoch 6, Training Loss: 0.7956941276564634\n",
      "Epoch 7, Training Loss: 0.7942960373441079\n",
      "Epoch 8, Training Loss: 0.7934528911024108\n",
      "Epoch 9, Training Loss: 0.7912123028048895\n",
      "Epoch 10, Training Loss: 0.7900868239259361\n",
      "Epoch 11, Training Loss: 0.7906638656343733\n",
      "Epoch 12, Training Loss: 0.7886735571954483\n",
      "Epoch 13, Training Loss: 0.7896965360731111\n",
      "Epoch 14, Training Loss: 0.7886523734358021\n",
      "Epoch 15, Training Loss: 0.7897173164482404\n",
      "Epoch 16, Training Loss: 0.7881513540009807\n",
      "Epoch 17, Training Loss: 0.7887149515008568\n",
      "Epoch 18, Training Loss: 0.7885338667640113\n",
      "Epoch 19, Training Loss: 0.7885454018313186\n",
      "Epoch 20, Training Loss: 0.7869540850918992\n",
      "Epoch 21, Training Loss: 0.7863440683013514\n",
      "Epoch 22, Training Loss: 0.786776464146779\n",
      "Epoch 23, Training Loss: 0.7854344185133626\n",
      "Epoch 24, Training Loss: 0.786647239394654\n",
      "Epoch 25, Training Loss: 0.7856203960296803\n",
      "Epoch 26, Training Loss: 0.7862785717598477\n",
      "Epoch 27, Training Loss: 0.7854230178029914\n",
      "Epoch 28, Training Loss: 0.7859530737525539\n",
      "Epoch 29, Training Loss: 0.785413032097924\n",
      "Epoch 30, Training Loss: 0.7850581845842806\n",
      "Epoch 31, Training Loss: 0.7857326174140873\n",
      "Epoch 32, Training Loss: 0.7848273781905497\n",
      "Epoch 33, Training Loss: 0.7843578872824074\n",
      "Epoch 34, Training Loss: 0.7840651034860683\n",
      "Epoch 35, Training Loss: 0.7839682969831883\n",
      "Epoch 36, Training Loss: 0.7848205469604722\n",
      "Epoch 37, Training Loss: 0.784110743300359\n",
      "Epoch 38, Training Loss: 0.7839825446444346\n",
      "Epoch 39, Training Loss: 0.7836666651238177\n",
      "Epoch 40, Training Loss: 0.7824682396157343\n",
      "Epoch 41, Training Loss: 0.7830766733427693\n",
      "Epoch 42, Training Loss: 0.7828932253937972\n",
      "Epoch 43, Training Loss: 0.7832090951446303\n",
      "Epoch 44, Training Loss: 0.7829051387041135\n",
      "Epoch 45, Training Loss: 0.783157417738348\n",
      "Epoch 46, Training Loss: 0.7827684156876757\n",
      "Epoch 47, Training Loss: 0.7831010075440085\n",
      "Epoch 48, Training Loss: 0.7822660251667625\n",
      "Epoch 49, Training Loss: 0.7824800411561378\n",
      "Epoch 50, Training Loss: 0.7824238332590663\n",
      "Epoch 51, Training Loss: 0.7815063530341126\n",
      "Epoch 52, Training Loss: 0.7822262673449696\n",
      "Epoch 53, Training Loss: 0.7810159089870022\n",
      "Epoch 54, Training Loss: 0.782288562534447\n",
      "Epoch 55, Training Loss: 0.7812732038641335\n",
      "Epoch 56, Training Loss: 0.7821330833255796\n",
      "Epoch 57, Training Loss: 0.7814306569726843\n",
      "Epoch 58, Training Loss: 0.7821081351516839\n",
      "Epoch 59, Training Loss: 0.7814519613309014\n",
      "Epoch 60, Training Loss: 0.7805599546970281\n",
      "Epoch 61, Training Loss: 0.7810484593972228\n",
      "Epoch 62, Training Loss: 0.7809040731953499\n",
      "Epoch 63, Training Loss: 0.7807443849126199\n",
      "Epoch 64, Training Loss: 0.7811878593344438\n",
      "Epoch 65, Training Loss: 0.7798300321837117\n",
      "Epoch 66, Training Loss: 0.7805232367121188\n",
      "Epoch 67, Training Loss: 0.7813628845645073\n",
      "Epoch 68, Training Loss: 0.7810237159406332\n",
      "Epoch 69, Training Loss: 0.779876991232535\n",
      "Epoch 70, Training Loss: 0.7794763394764491\n",
      "Epoch 71, Training Loss: 0.7797855853138114\n",
      "Epoch 72, Training Loss: 0.7793872824288849\n",
      "Epoch 73, Training Loss: 0.7798042526818756\n",
      "Epoch 74, Training Loss: 0.7794710933713984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:45:29,943] Trial 69 finished with value: 0.6374666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7797753928299237\n",
      "Epoch 1, Training Loss: 0.9222095033000497\n",
      "Epoch 2, Training Loss: 0.8288726328401005\n",
      "Epoch 3, Training Loss: 0.8205049856270061\n",
      "Epoch 4, Training Loss: 0.8152310834912693\n",
      "Epoch 5, Training Loss: 0.8140854860754574\n",
      "Epoch 6, Training Loss: 0.8119112939694348\n",
      "Epoch 7, Training Loss: 0.8105930490353528\n",
      "Epoch 8, Training Loss: 0.8105405078915988\n",
      "Epoch 9, Training Loss: 0.8085707063534681\n",
      "Epoch 10, Training Loss: 0.8083502013543072\n",
      "Epoch 11, Training Loss: 0.8076239160229178\n",
      "Epoch 12, Training Loss: 0.8080700830852284\n",
      "Epoch 13, Training Loss: 0.806842242900063\n",
      "Epoch 14, Training Loss: 0.8072242325894973\n",
      "Epoch 15, Training Loss: 0.8054175052923315\n",
      "Epoch 16, Training Loss: 0.8064089402731727\n",
      "Epoch 17, Training Loss: 0.8055886164132287\n",
      "Epoch 18, Training Loss: 0.8051939598953023\n",
      "Epoch 19, Training Loss: 0.8048803377151489\n",
      "Epoch 20, Training Loss: 0.8050336278186125\n",
      "Epoch 21, Training Loss: 0.804662468363257\n",
      "Epoch 22, Training Loss: 0.8034046438161064\n",
      "Epoch 23, Training Loss: 0.8035989722083596\n",
      "Epoch 24, Training Loss: 0.8045381750078763\n",
      "Epoch 25, Training Loss: 0.8043211918718675\n",
      "Epoch 26, Training Loss: 0.8040497173982508\n",
      "Epoch 27, Training Loss: 0.8018891686551711\n",
      "Epoch 28, Training Loss: 0.80284967639867\n",
      "Epoch 29, Training Loss: 0.8030488768745871\n",
      "Epoch 30, Training Loss: 0.8033481076885672\n",
      "Epoch 31, Training Loss: 0.8026179155882667\n",
      "Epoch 32, Training Loss: 0.8022638689770418\n",
      "Epoch 33, Training Loss: 0.8017693272758932\n",
      "Epoch 34, Training Loss: 0.803368723041871\n",
      "Epoch 35, Training Loss: 0.8020689393492306\n",
      "Epoch 36, Training Loss: 0.8026021148176754\n",
      "Epoch 37, Training Loss: 0.802439456687254\n",
      "Epoch 38, Training Loss: 0.802589428915697\n",
      "Epoch 39, Training Loss: 0.8027772905546077\n",
      "Epoch 40, Training Loss: 0.8026833889063667\n",
      "Epoch 41, Training Loss: 0.8019093954563141\n",
      "Epoch 42, Training Loss: 0.8021365017750683\n",
      "Epoch 43, Training Loss: 0.8024039206084083\n",
      "Epoch 44, Training Loss: 0.8016375395129709\n",
      "Epoch 45, Training Loss: 0.8014657081575954\n",
      "Epoch 46, Training Loss: 0.8018551243753994\n",
      "Epoch 47, Training Loss: 0.8019118787961848\n",
      "Epoch 48, Training Loss: 0.8016493183725021\n",
      "Epoch 49, Training Loss: 0.8027614089320688\n",
      "Epoch 50, Training Loss: 0.8020290281492121\n",
      "Epoch 51, Training Loss: 0.8023143651906182\n",
      "Epoch 52, Training Loss: 0.8023000525726992\n",
      "Epoch 53, Training Loss: 0.8025750250676099\n",
      "Epoch 54, Training Loss: 0.8027556497910443\n",
      "Epoch 55, Training Loss: 0.8020846491701463\n",
      "Epoch 56, Training Loss: 0.8027934978288763\n",
      "Epoch 57, Training Loss: 0.8017508942239425\n",
      "Epoch 58, Training Loss: 0.8021124876947964\n",
      "Epoch 59, Training Loss: 0.8034922856443069\n",
      "Epoch 60, Training Loss: 0.8016669909393086\n",
      "Epoch 61, Training Loss: 0.802196787034764\n",
      "Epoch 62, Training Loss: 0.8017174621890573\n",
      "Epoch 63, Training Loss: 0.8006312776313108\n",
      "Epoch 64, Training Loss: 0.8019098355489619\n",
      "Epoch 65, Training Loss: 0.8017560768127442\n",
      "Epoch 66, Training Loss: 0.802458177243962\n",
      "Epoch 67, Training Loss: 0.8021354989444508\n",
      "Epoch 68, Training Loss: 0.8025023529108832\n",
      "Epoch 69, Training Loss: 0.8019837469213149\n",
      "Epoch 70, Training Loss: 0.801773183556164\n",
      "Epoch 71, Training Loss: 0.8013453534771414\n",
      "Epoch 72, Training Loss: 0.8018155926816604\n",
      "Epoch 73, Training Loss: 0.8016793004204246\n",
      "Epoch 74, Training Loss: 0.8018496536507326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:47:17,194] Trial 70 finished with value: 0.6303333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8016940569176393\n",
      "Epoch 1, Training Loss: 1.0792338654574225\n",
      "Epoch 2, Training Loss: 1.0429458919693442\n",
      "Epoch 3, Training Loss: 1.0200276083805981\n",
      "Epoch 4, Training Loss: 1.0028437670539407\n",
      "Epoch 5, Training Loss: 0.9899396081531749\n",
      "Epoch 6, Training Loss: 0.9806058073745054\n",
      "Epoch 7, Training Loss: 0.9740915128763984\n",
      "Epoch 8, Training Loss: 0.9696348346682155\n",
      "Epoch 9, Training Loss: 0.9665782178149503\n",
      "Epoch 10, Training Loss: 0.9644072192556717\n",
      "Epoch 11, Training Loss: 0.9627982052634744\n",
      "Epoch 12, Training Loss: 0.9614960119303535\n",
      "Epoch 13, Training Loss: 0.9604205966697019\n",
      "Epoch 14, Training Loss: 0.9594309387487524\n",
      "Epoch 15, Training Loss: 0.95852414727211\n",
      "Epoch 16, Training Loss: 0.957645335407818\n",
      "Epoch 17, Training Loss: 0.9567839075537289\n",
      "Epoch 18, Training Loss: 0.9559408267806558\n",
      "Epoch 19, Training Loss: 0.9551008242018083\n",
      "Epoch 20, Training Loss: 0.9542647985851064\n",
      "Epoch 21, Training Loss: 0.9534289866335252\n",
      "Epoch 22, Training Loss: 0.9525911693012014\n",
      "Epoch 23, Training Loss: 0.9517503680902369\n",
      "Epoch 24, Training Loss: 0.9509056912450229\n",
      "Epoch 25, Training Loss: 0.95005975912599\n",
      "Epoch 26, Training Loss: 0.9492081368670744\n",
      "Epoch 27, Training Loss: 0.9483431248805102\n",
      "Epoch 28, Training Loss: 0.9474810334514169\n",
      "Epoch 29, Training Loss: 0.9466082447416642\n",
      "Epoch 30, Training Loss: 0.9457200065079857\n",
      "Epoch 31, Training Loss: 0.9448305370527156\n",
      "Epoch 32, Training Loss: 0.9439233121451209\n",
      "Epoch 33, Training Loss: 0.943011400419123\n",
      "Epoch 34, Training Loss: 0.9420867893275092\n",
      "Epoch 35, Training Loss: 0.9411456580021802\n",
      "Epoch 36, Training Loss: 0.9401947397344252\n",
      "Epoch 37, Training Loss: 0.9392187436187969\n",
      "Epoch 38, Training Loss: 0.9382417146598592\n",
      "Epoch 39, Training Loss: 0.9372406416780809\n",
      "Epoch 40, Training Loss: 0.9362243118706871\n",
      "Epoch 41, Training Loss: 0.9351852294977974\n",
      "Epoch 42, Training Loss: 0.9341365626279046\n",
      "Epoch 43, Training Loss: 0.9330589390502256\n",
      "Epoch 44, Training Loss: 0.9319638785193948\n",
      "Epoch 45, Training Loss: 0.9308422846653882\n",
      "Epoch 46, Training Loss: 0.9296957544719472\n",
      "Epoch 47, Training Loss: 0.9285344167316661\n",
      "Epoch 48, Training Loss: 0.927342626136892\n",
      "Epoch 49, Training Loss: 0.9261130175169776\n",
      "Epoch 50, Training Loss: 0.9248738364612354\n",
      "Epoch 51, Training Loss: 0.9235997413887697\n",
      "Epoch 52, Training Loss: 0.9222776816171758\n",
      "Epoch 53, Training Loss: 0.9209430914766649\n",
      "Epoch 54, Training Loss: 0.9195794700875002\n",
      "Epoch 55, Training Loss: 0.9181734567530015\n",
      "Epoch 56, Training Loss: 0.9167355039540459\n",
      "Epoch 57, Training Loss: 0.9152487754821778\n",
      "Epoch 58, Training Loss: 0.9137328980950749\n",
      "Epoch 59, Training Loss: 0.9122031117888058\n",
      "Epoch 60, Training Loss: 0.9105995261669159\n",
      "Epoch 61, Training Loss: 0.9089871518752154\n",
      "Epoch 62, Training Loss: 0.9073094090293435\n",
      "Epoch 63, Training Loss: 0.9056153624899247\n",
      "Epoch 64, Training Loss: 0.903867360423593\n",
      "Epoch 65, Training Loss: 0.9020878248354968\n",
      "Epoch 66, Training Loss: 0.9002550359333262\n",
      "Epoch 67, Training Loss: 0.8983913775752572\n",
      "Epoch 68, Training Loss: 0.8964976850677939\n",
      "Epoch 69, Training Loss: 0.8945657882269691\n",
      "Epoch 70, Training Loss: 0.8925850746912115\n",
      "Epoch 71, Training Loss: 0.8905798378411461\n",
      "Epoch 72, Training Loss: 0.8885188425989712\n",
      "Epoch 73, Training Loss: 0.8864569450827205\n",
      "Epoch 74, Training Loss: 0.8843650732320898\n",
      "Epoch 75, Training Loss: 0.8822590088143069\n",
      "Epoch 76, Training Loss: 0.8801152239827549\n",
      "Epoch 77, Training Loss: 0.8779553842544555\n",
      "Epoch 78, Training Loss: 0.8757834202401779\n",
      "Epoch 79, Training Loss: 0.8736084727679982\n",
      "Epoch 80, Training Loss: 0.871428498099832\n",
      "Epoch 81, Training Loss: 0.8692269023025737\n",
      "Epoch 82, Training Loss: 0.8670585745222428\n",
      "Epoch 83, Training Loss: 0.8648899797832265\n",
      "Epoch 84, Training Loss: 0.862725974742104\n",
      "Epoch 85, Training Loss: 0.8605903686495389\n",
      "Epoch 86, Training Loss: 0.8584754190725439\n",
      "Epoch 87, Training Loss: 0.8564146449986626\n",
      "Epoch 88, Training Loss: 0.8543584552231956\n",
      "Epoch 89, Training Loss: 0.8523191418367274\n",
      "Epoch 90, Training Loss: 0.8503560399307925\n",
      "Epoch 91, Training Loss: 0.8484272014393526\n",
      "Epoch 92, Training Loss: 0.8465616085950066\n",
      "Epoch 93, Training Loss: 0.8447093530963449\n",
      "Epoch 94, Training Loss: 0.8429281544685364\n",
      "Epoch 95, Training Loss: 0.8412406232777764\n",
      "Epoch 96, Training Loss: 0.8395754817654105\n",
      "Epoch 97, Training Loss: 0.8379666663618649\n",
      "Epoch 98, Training Loss: 0.8364565059016733\n",
      "Epoch 99, Training Loss: 0.8349337837275337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:49:06,458] Trial 71 finished with value: 0.615 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8335582585895762\n",
      "Epoch 1, Training Loss: 0.9419795528390353\n",
      "Epoch 2, Training Loss: 0.8708819682436778\n",
      "Epoch 3, Training Loss: 0.829847093318638\n",
      "Epoch 4, Training Loss: 0.8173571334745651\n",
      "Epoch 5, Training Loss: 0.8122795534313173\n",
      "Epoch 6, Training Loss: 0.8104855860982623\n",
      "Epoch 7, Training Loss: 0.809347991566909\n",
      "Epoch 8, Training Loss: 0.8078379087878349\n",
      "Epoch 9, Training Loss: 0.8075381237761419\n",
      "Epoch 10, Training Loss: 0.8072263284733421\n",
      "Epoch 11, Training Loss: 0.8061835029967745\n",
      "Epoch 12, Training Loss: 0.8059380915828217\n",
      "Epoch 13, Training Loss: 0.8059060408656759\n",
      "Epoch 14, Training Loss: 0.8059391904594306\n",
      "Epoch 15, Training Loss: 0.8049645412237124\n",
      "Epoch 16, Training Loss: 0.8051839987138161\n",
      "Epoch 17, Training Loss: 0.8049854650533289\n",
      "Epoch 18, Training Loss: 0.803989052772522\n",
      "Epoch 19, Training Loss: 0.80393519634591\n",
      "Epoch 20, Training Loss: 0.8040347238232318\n",
      "Epoch 21, Training Loss: 0.8031399679363223\n",
      "Epoch 22, Training Loss: 0.8035232042011462\n",
      "Epoch 23, Training Loss: 0.803767490745487\n",
      "Epoch 24, Training Loss: 0.8033987203038725\n",
      "Epoch 25, Training Loss: 0.8023158362933568\n",
      "Epoch 26, Training Loss: 0.8026021132791848\n",
      "Epoch 27, Training Loss: 0.8024890601186824\n",
      "Epoch 28, Training Loss: 0.8023482224098721\n",
      "Epoch 29, Training Loss: 0.8019433232178366\n",
      "Epoch 30, Training Loss: 0.8013883022885574\n",
      "Epoch 31, Training Loss: 0.8018096977606752\n",
      "Epoch 32, Training Loss: 0.8013023570964211\n",
      "Epoch 33, Training Loss: 0.8020180045213915\n",
      "Epoch 34, Training Loss: 0.8016092985196221\n",
      "Epoch 35, Training Loss: 0.8014128135559254\n",
      "Epoch 36, Training Loss: 0.8010513870339645\n",
      "Epoch 37, Training Loss: 0.8003257978231387\n",
      "Epoch 38, Training Loss: 0.8010772903162734\n",
      "Epoch 39, Training Loss: 0.8004531812847109\n",
      "Epoch 40, Training Loss: 0.8002770046542461\n",
      "Epoch 41, Training Loss: 0.8004391805569928\n",
      "Epoch 42, Training Loss: 0.800120564779841\n",
      "Epoch 43, Training Loss: 0.8000909948707523\n",
      "Epoch 44, Training Loss: 0.8003738169383285\n",
      "Epoch 45, Training Loss: 0.7998091220855713\n",
      "Epoch 46, Training Loss: 0.7992314866610936\n",
      "Epoch 47, Training Loss: 0.7999857718783213\n",
      "Epoch 48, Training Loss: 0.7993348559938875\n",
      "Epoch 49, Training Loss: 0.7989450904659758\n",
      "Epoch 50, Training Loss: 0.8002700113712397\n",
      "Epoch 51, Training Loss: 0.7992434005988271\n",
      "Epoch 52, Training Loss: 0.7990566959058432\n",
      "Epoch 53, Training Loss: 0.800298415718222\n",
      "Epoch 54, Training Loss: 0.7998996126024347\n",
      "Epoch 55, Training Loss: 0.798542138508388\n",
      "Epoch 56, Training Loss: 0.7987684199684545\n",
      "Epoch 57, Training Loss: 0.7983088911027837\n",
      "Epoch 58, Training Loss: 0.7984447464010769\n",
      "Epoch 59, Training Loss: 0.7985426335406483\n",
      "Epoch 60, Training Loss: 0.7988222512087427\n",
      "Epoch 61, Training Loss: 0.7988375280136453\n",
      "Epoch 62, Training Loss: 0.7978610579232525\n",
      "Epoch 63, Training Loss: 0.7996272819382804\n",
      "Epoch 64, Training Loss: 0.7990283442619152\n",
      "Epoch 65, Training Loss: 0.7986877773937426\n",
      "Epoch 66, Training Loss: 0.7974903374686277\n",
      "Epoch 67, Training Loss: 0.7981377264610807\n",
      "Epoch 68, Training Loss: 0.7979479651702078\n",
      "Epoch 69, Training Loss: 0.7982839328902108\n",
      "Epoch 70, Training Loss: 0.7977777862907353\n",
      "Epoch 71, Training Loss: 0.7976929707186563\n",
      "Epoch 72, Training Loss: 0.7981845547382097\n",
      "Epoch 73, Training Loss: 0.7971225943332328\n",
      "Epoch 74, Training Loss: 0.7974198548417343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:50:25,224] Trial 72 finished with value: 0.6348 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.797418367056022\n",
      "Epoch 1, Training Loss: 0.8526901982111089\n",
      "Epoch 2, Training Loss: 0.817125256762785\n",
      "Epoch 3, Training Loss: 0.8101486840668847\n",
      "Epoch 4, Training Loss: 0.807231149743585\n",
      "Epoch 5, Training Loss: 0.8040972131140092\n",
      "Epoch 6, Training Loss: 0.8026779814327465\n",
      "Epoch 7, Training Loss: 0.8014445533471949\n",
      "Epoch 8, Training Loss: 0.8001840841770173\n",
      "Epoch 9, Training Loss: 0.7988997663469876\n",
      "Epoch 10, Training Loss: 0.7985176029626061\n",
      "Epoch 11, Training Loss: 0.7974891391922446\n",
      "Epoch 12, Training Loss: 0.7970791351093965\n",
      "Epoch 13, Training Loss: 0.7973279749645906\n",
      "Epoch 14, Training Loss: 0.7964259413410636\n",
      "Epoch 15, Training Loss: 0.795630719240974\n",
      "Epoch 16, Training Loss: 0.7949812708882724\n",
      "Epoch 17, Training Loss: 0.7951313744573032\n",
      "Epoch 18, Training Loss: 0.7951012898893918\n",
      "Epoch 19, Training Loss: 0.7944599758877474\n",
      "Epoch 20, Training Loss: 0.7942061724382289\n",
      "Epoch 21, Training Loss: 0.7937162707132451\n",
      "Epoch 22, Training Loss: 0.7940953085001777\n",
      "Epoch 23, Training Loss: 0.7946297137176289\n",
      "Epoch 24, Training Loss: 0.7940181253236883\n",
      "Epoch 25, Training Loss: 0.7947120313083424\n",
      "Epoch 26, Training Loss: 0.7947285170414868\n",
      "Epoch 27, Training Loss: 0.7943116591257208\n",
      "Epoch 28, Training Loss: 0.7938596966687371\n",
      "Epoch 29, Training Loss: 0.7935626979435192\n",
      "Epoch 30, Training Loss: 0.7935066218937145\n",
      "Epoch 31, Training Loss: 0.7937119898375343\n",
      "Epoch 32, Training Loss: 0.7931380556611454\n",
      "Epoch 33, Training Loss: 0.7930899177579319\n",
      "Epoch 34, Training Loss: 0.7926249795801499\n",
      "Epoch 35, Training Loss: 0.793138484393849\n",
      "Epoch 36, Training Loss: 0.7931227565512937\n",
      "Epoch 37, Training Loss: 0.79260432825369\n",
      "Epoch 38, Training Loss: 0.7924921025248135\n",
      "Epoch 39, Training Loss: 0.7922960806594176\n",
      "Epoch 40, Training Loss: 0.7927207484666039\n",
      "Epoch 41, Training Loss: 0.7916487965163063\n",
      "Epoch 42, Training Loss: 0.7917592142609989\n",
      "Epoch 43, Training Loss: 0.7925814656650318\n",
      "Epoch 44, Training Loss: 0.7923660556007834\n",
      "Epoch 45, Training Loss: 0.7925901707480936\n",
      "Epoch 46, Training Loss: 0.7918877917878768\n",
      "Epoch 47, Training Loss: 0.7920364106402678\n",
      "Epoch 48, Training Loss: 0.7919420161667992\n",
      "Epoch 49, Training Loss: 0.7916764872214374\n",
      "Epoch 50, Training Loss: 0.7925053799853605\n",
      "Epoch 51, Training Loss: 0.7920494597799638\n",
      "Epoch 52, Training Loss: 0.7919769369153415\n",
      "Epoch 53, Training Loss: 0.7920372172664194\n",
      "Epoch 54, Training Loss: 0.7911296771554386\n",
      "Epoch 55, Training Loss: 0.7918718701250413\n",
      "Epoch 56, Training Loss: 0.7915773358765771\n",
      "Epoch 57, Training Loss: 0.7919185391594382\n",
      "Epoch 58, Training Loss: 0.7920201281239004\n",
      "Epoch 59, Training Loss: 0.7924103933222154\n",
      "Epoch 60, Training Loss: 0.7917442704649532\n",
      "Epoch 61, Training Loss: 0.7925141680240632\n",
      "Epoch 62, Training Loss: 0.7914665810500874\n",
      "Epoch 63, Training Loss: 0.7919443941817564\n",
      "Epoch 64, Training Loss: 0.791943439245224\n",
      "Epoch 65, Training Loss: 0.7921404012511758\n",
      "Epoch 66, Training Loss: 0.7924224332500907\n",
      "Epoch 67, Training Loss: 0.7926546754556544\n",
      "Epoch 68, Training Loss: 0.7919270577851464\n",
      "Epoch 69, Training Loss: 0.7917666179993573\n",
      "Epoch 70, Training Loss: 0.792504253247205\n",
      "Epoch 71, Training Loss: 0.792336333358989\n",
      "Epoch 72, Training Loss: 0.7920817030878629\n",
      "Epoch 73, Training Loss: 0.7928989376741297\n",
      "Epoch 74, Training Loss: 0.7918158714911517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:52:12,837] Trial 73 finished with value: 0.6365333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.792047829207252\n",
      "Epoch 1, Training Loss: 0.9747619696224437\n",
      "Epoch 2, Training Loss: 0.9315305273673113\n",
      "Epoch 3, Training Loss: 0.8934339461607091\n",
      "Epoch 4, Training Loss: 0.8511096617053536\n",
      "Epoch 5, Training Loss: 0.8247794551007888\n",
      "Epoch 6, Training Loss: 0.8165320231634028\n",
      "Epoch 7, Training Loss: 0.814085786412744\n",
      "Epoch 8, Training Loss: 0.8130177212462706\n",
      "Epoch 9, Training Loss: 0.8120951801187852\n",
      "Epoch 10, Training Loss: 0.8112172220033758\n",
      "Epoch 11, Training Loss: 0.8106031816145953\n",
      "Epoch 12, Training Loss: 0.8097225270551793\n",
      "Epoch 13, Training Loss: 0.8084714206527261\n",
      "Epoch 14, Training Loss: 0.8079638758827659\n",
      "Epoch 15, Training Loss: 0.8072207282571232\n",
      "Epoch 16, Training Loss: 0.8065428513639114\n",
      "Epoch 17, Training Loss: 0.805700784851523\n",
      "Epoch 18, Training Loss: 0.8045243601238027\n",
      "Epoch 19, Training Loss: 0.8041395696471719\n",
      "Epoch 20, Training Loss: 0.8040088328193216\n",
      "Epoch 21, Training Loss: 0.8033513252174153\n",
      "Epoch 22, Training Loss: 0.8033349957185633\n",
      "Epoch 23, Training Loss: 0.8032461228791405\n",
      "Epoch 24, Training Loss: 0.8027535584393669\n",
      "Epoch 25, Training Loss: 0.802562346458435\n",
      "Epoch 26, Training Loss: 0.801957563863081\n",
      "Epoch 27, Training Loss: 0.8021297839809867\n",
      "Epoch 28, Training Loss: 0.8019259987157934\n",
      "Epoch 29, Training Loss: 0.8016319648658528\n",
      "Epoch 30, Training Loss: 0.8011128268522375\n",
      "Epoch 31, Training Loss: 0.8014724682359134\n",
      "Epoch 32, Training Loss: 0.801174017050687\n",
      "Epoch 33, Training Loss: 0.8010385626203873\n",
      "Epoch 34, Training Loss: 0.8009121906757355\n",
      "Epoch 35, Training Loss: 0.8005620239061467\n",
      "Epoch 36, Training Loss: 0.8003418162991018\n",
      "Epoch 37, Training Loss: 0.8002579108406516\n",
      "Epoch 38, Training Loss: 0.8000525525738211\n",
      "Epoch 39, Training Loss: 0.7998242958854227\n",
      "Epoch 40, Training Loss: 0.7998288943487055\n",
      "Epoch 41, Training Loss: 0.7999707461104674\n",
      "Epoch 42, Training Loss: 0.7993338989510256\n",
      "Epoch 43, Training Loss: 0.7991431776916279\n",
      "Epoch 44, Training Loss: 0.799549659911324\n",
      "Epoch 45, Training Loss: 0.7994671909949359\n",
      "Epoch 46, Training Loss: 0.7988231398778803\n",
      "Epoch 47, Training Loss: 0.7986859747241525\n",
      "Epoch 48, Training Loss: 0.798919434968163\n",
      "Epoch 49, Training Loss: 0.7985783466872047\n",
      "Epoch 50, Training Loss: 0.7985686053248012\n",
      "Epoch 51, Training Loss: 0.7979925922085257\n",
      "Epoch 52, Training Loss: 0.7982165359048282\n",
      "Epoch 53, Training Loss: 0.7980983347752515\n",
      "Epoch 54, Training Loss: 0.7985883418251486\n",
      "Epoch 55, Training Loss: 0.797859855820151\n",
      "Epoch 56, Training Loss: 0.7978578815740698\n",
      "Epoch 57, Training Loss: 0.7977241636725033\n",
      "Epoch 58, Training Loss: 0.7980603476131664\n",
      "Epoch 59, Training Loss: 0.7976000457651475\n",
      "Epoch 60, Training Loss: 0.7972908611157361\n",
      "Epoch 61, Training Loss: 0.7974510769283071\n",
      "Epoch 62, Training Loss: 0.7973885923273423\n",
      "Epoch 63, Training Loss: 0.7973866122610429\n",
      "Epoch 64, Training Loss: 0.7971883112542769\n",
      "Epoch 65, Training Loss: 0.7967648673057556\n",
      "Epoch 66, Training Loss: 0.7967556140703314\n",
      "Epoch 67, Training Loss: 0.7968097714115592\n",
      "Epoch 68, Training Loss: 0.7966011416210848\n",
      "Epoch 69, Training Loss: 0.7966014781418969\n",
      "Epoch 70, Training Loss: 0.7963148572865655\n",
      "Epoch 71, Training Loss: 0.7961539898199194\n",
      "Epoch 72, Training Loss: 0.7962133054172291\n",
      "Epoch 73, Training Loss: 0.7956441591767703\n",
      "Epoch 74, Training Loss: 0.7958024593661813\n",
      "Epoch 75, Training Loss: 0.7957744482685538\n",
      "Epoch 76, Training Loss: 0.7956708417219274\n",
      "Epoch 77, Training Loss: 0.7957428707795985\n",
      "Epoch 78, Training Loss: 0.7954441675719093\n",
      "Epoch 79, Training Loss: 0.7950575287903057\n",
      "Epoch 80, Training Loss: 0.7950815843834597\n",
      "Epoch 81, Training Loss: 0.7947577475800234\n",
      "Epoch 82, Training Loss: 0.7948474428232979\n",
      "Epoch 83, Training Loss: 0.7947971066306619\n",
      "Epoch 84, Training Loss: 0.7942979041267844\n",
      "Epoch 85, Training Loss: 0.7942181823534123\n",
      "Epoch 86, Training Loss: 0.7940267912079306\n",
      "Epoch 87, Training Loss: 0.7938388763455784\n",
      "Epoch 88, Training Loss: 0.7935265992669498\n",
      "Epoch 89, Training Loss: 0.7933153902081882\n",
      "Epoch 90, Training Loss: 0.7932435091804055\n",
      "Epoch 91, Training Loss: 0.793143063362907\n",
      "Epoch 92, Training Loss: 0.7928385024210985\n",
      "Epoch 93, Training Loss: 0.7926773384038139\n",
      "Epoch 94, Training Loss: 0.7926898079058703\n",
      "Epoch 95, Training Loss: 0.7923134812186746\n",
      "Epoch 96, Training Loss: 0.7921120769837323\n",
      "Epoch 97, Training Loss: 0.7920830618633944\n",
      "Epoch 98, Training Loss: 0.7919275291527019\n",
      "Epoch 99, Training Loss: 0.7916468784388374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:54:23,002] Trial 74 finished with value: 0.6364 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7916396162790411\n",
      "Epoch 1, Training Loss: 1.0500299986671\n",
      "Epoch 2, Training Loss: 0.9877625291487749\n",
      "Epoch 3, Training Loss: 0.9664463164525874\n",
      "Epoch 4, Training Loss: 0.9597684856022105\n",
      "Epoch 5, Training Loss: 0.9566259004088009\n",
      "Epoch 6, Training Loss: 0.954269630838843\n",
      "Epoch 7, Training Loss: 0.951895777127322\n",
      "Epoch 8, Training Loss: 0.9494445328151478\n",
      "Epoch 9, Training Loss: 0.9470487243287704\n",
      "Epoch 10, Training Loss: 0.9445477416936089\n",
      "Epoch 11, Training Loss: 0.9417795754881466\n",
      "Epoch 12, Training Loss: 0.9388326897340662\n",
      "Epoch 13, Training Loss: 0.9358873470390544\n",
      "Epoch 14, Training Loss: 0.9326286881811479\n",
      "Epoch 15, Training Loss: 0.9292023423840018\n",
      "Epoch 16, Training Loss: 0.9257062474419089\n",
      "Epoch 17, Training Loss: 0.9218625418578877\n",
      "Epoch 18, Training Loss: 0.9178249174707076\n",
      "Epoch 19, Training Loss: 0.9135543310642242\n",
      "Epoch 20, Training Loss: 0.9092393498560961\n",
      "Epoch 21, Training Loss: 0.9046878746677848\n",
      "Epoch 22, Training Loss: 0.9000851437624763\n",
      "Epoch 23, Training Loss: 0.8953643759559182\n",
      "Epoch 24, Training Loss: 0.8905093749831704\n",
      "Epoch 25, Training Loss: 0.8856698008144603\n",
      "Epoch 26, Training Loss: 0.8809670320679159\n",
      "Epoch 27, Training Loss: 0.8762606011418735\n",
      "Epoch 28, Training Loss: 0.8717167097680709\n",
      "Epoch 29, Training Loss: 0.8670297473318437\n",
      "Epoch 30, Training Loss: 0.8626720676702612\n",
      "Epoch 31, Training Loss: 0.85864735028323\n",
      "Epoch 32, Training Loss: 0.854611756030251\n",
      "Epoch 33, Training Loss: 0.8509302225533654\n",
      "Epoch 34, Training Loss: 0.8472095091903911\n",
      "Epoch 35, Training Loss: 0.8438959836258608\n",
      "Epoch 36, Training Loss: 0.8408063442566815\n",
      "Epoch 37, Training Loss: 0.8379643525095547\n",
      "Epoch 38, Training Loss: 0.8352730120630826\n",
      "Epoch 39, Training Loss: 0.8328216585692237\n",
      "Epoch 40, Training Loss: 0.8306143445127151\n",
      "Epoch 41, Training Loss: 0.8286443851975833\n",
      "Epoch 42, Training Loss: 0.8267530655860901\n",
      "Epoch 43, Training Loss: 0.8250034637310926\n",
      "Epoch 44, Training Loss: 0.8235532695405623\n",
      "Epoch 45, Training Loss: 0.8221156855891733\n",
      "Epoch 46, Training Loss: 0.8210561902382795\n",
      "Epoch 47, Training Loss: 0.8198621512160582\n",
      "Epoch 48, Training Loss: 0.8188476263775545\n",
      "Epoch 49, Training Loss: 0.818062346051721\n",
      "Epoch 50, Training Loss: 0.8171204981383156\n",
      "Epoch 51, Training Loss: 0.8164174898231731\n",
      "Epoch 52, Training Loss: 0.8158618155647727\n",
      "Epoch 53, Training Loss: 0.8153375786893508\n",
      "Epoch 54, Training Loss: 0.8148109767717474\n",
      "Epoch 55, Training Loss: 0.8143459137748269\n",
      "Epoch 56, Training Loss: 0.8138876152739806\n",
      "Epoch 57, Training Loss: 0.8134181869029998\n",
      "Epoch 58, Training Loss: 0.8131433730966905\n",
      "Epoch 59, Training Loss: 0.8127929402098936\n",
      "Epoch 60, Training Loss: 0.8124710469386157\n",
      "Epoch 61, Training Loss: 0.8122915888533873\n",
      "Epoch 62, Training Loss: 0.8120015700424419\n",
      "Epoch 63, Training Loss: 0.811662068086512\n",
      "Epoch 64, Training Loss: 0.8115017592906952\n",
      "Epoch 65, Training Loss: 0.8113201918321498\n",
      "Epoch 66, Training Loss: 0.8110325775426976\n",
      "Epoch 67, Training Loss: 0.8108962868942934\n",
      "Epoch 68, Training Loss: 0.8106493815954994\n",
      "Epoch 69, Training Loss: 0.8105829729753382\n",
      "Epoch 70, Training Loss: 0.810373673509149\n",
      "Epoch 71, Training Loss: 0.8101921387980966\n",
      "Epoch 72, Training Loss: 0.8099884031099431\n",
      "Epoch 73, Training Loss: 0.8098907702810624\n",
      "Epoch 74, Training Loss: 0.8098587783645181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:55:35,009] Trial 75 finished with value: 0.6292666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8096406206663918\n",
      "Epoch 1, Training Loss: 0.9961499518021605\n",
      "Epoch 2, Training Loss: 0.9436995490153033\n",
      "Epoch 3, Training Loss: 0.9207747450448517\n",
      "Epoch 4, Training Loss: 0.8782913056531347\n",
      "Epoch 5, Training Loss: 0.8414399276102396\n",
      "Epoch 6, Training Loss: 0.8298812790024549\n",
      "Epoch 7, Training Loss: 0.8228808453656677\n",
      "Epoch 8, Training Loss: 0.8182920877198527\n",
      "Epoch 9, Training Loss: 0.8145641656746542\n",
      "Epoch 10, Training Loss: 0.813163998073205\n",
      "Epoch 11, Training Loss: 0.8118249396632489\n",
      "Epoch 12, Training Loss: 0.8106268099376134\n",
      "Epoch 13, Training Loss: 0.8089984762937503\n",
      "Epoch 14, Training Loss: 0.8077677624566214\n",
      "Epoch 15, Training Loss: 0.8064322248437351\n",
      "Epoch 16, Training Loss: 0.8058936171961907\n",
      "Epoch 17, Training Loss: 0.8059700524000297\n",
      "Epoch 18, Training Loss: 0.8040040156895056\n",
      "Epoch 19, Training Loss: 0.8040310925110838\n",
      "Epoch 20, Training Loss: 0.8028178006186522\n",
      "Epoch 21, Training Loss: 0.8033665691103254\n",
      "Epoch 22, Training Loss: 0.8016065359115601\n",
      "Epoch 23, Training Loss: 0.8017384992506271\n",
      "Epoch 24, Training Loss: 0.8005081617742553\n",
      "Epoch 25, Training Loss: 0.8006567042573054\n",
      "Epoch 26, Training Loss: 0.801135029828638\n",
      "Epoch 27, Training Loss: 0.7998361032708247\n",
      "Epoch 28, Training Loss: 0.7993758234762608\n",
      "Epoch 29, Training Loss: 0.799238641997029\n",
      "Epoch 30, Training Loss: 0.7986496247743305\n",
      "Epoch 31, Training Loss: 0.7996095643007666\n",
      "Epoch 32, Training Loss: 0.7984312555843727\n",
      "Epoch 33, Training Loss: 0.7983167736153853\n",
      "Epoch 34, Training Loss: 0.7976228826028063\n",
      "Epoch 35, Training Loss: 0.7983697374960533\n",
      "Epoch 36, Training Loss: 0.7972213201056746\n",
      "Epoch 37, Training Loss: 0.7972209161385557\n",
      "Epoch 38, Training Loss: 0.7965474975736517\n",
      "Epoch 39, Training Loss: 0.7968248350279672\n",
      "Epoch 40, Training Loss: 0.7962159062686719\n",
      "Epoch 41, Training Loss: 0.7956039511171499\n",
      "Epoch 42, Training Loss: 0.795733984879085\n",
      "Epoch 43, Training Loss: 0.7947702111158156\n",
      "Epoch 44, Training Loss: 0.79481046235651\n",
      "Epoch 45, Training Loss: 0.7944434679540476\n",
      "Epoch 46, Training Loss: 0.7935742101274935\n",
      "Epoch 47, Training Loss: 0.7939027189312124\n",
      "Epoch 48, Training Loss: 0.7935117874826704\n",
      "Epoch 49, Training Loss: 0.7923341880167337\n",
      "Epoch 50, Training Loss: 0.7920933052113182\n",
      "Epoch 51, Training Loss: 0.7917183270131735\n",
      "Epoch 52, Training Loss: 0.7909886422910188\n",
      "Epoch 53, Training Loss: 0.7905220525605338\n",
      "Epoch 54, Training Loss: 0.7899566048070004\n",
      "Epoch 55, Training Loss: 0.7901287698207942\n",
      "Epoch 56, Training Loss: 0.7891031639916556\n",
      "Epoch 57, Training Loss: 0.7895554469044047\n",
      "Epoch 58, Training Loss: 0.7884692508475225\n",
      "Epoch 59, Training Loss: 0.788665383113058\n",
      "Epoch 60, Training Loss: 0.7881315419548436\n",
      "Epoch 61, Training Loss: 0.7872522350540735\n",
      "Epoch 62, Training Loss: 0.7874246461947162\n",
      "Epoch 63, Training Loss: 0.786858450201221\n",
      "Epoch 64, Training Loss: 0.7864610900556235\n",
      "Epoch 65, Training Loss: 0.786283743874471\n",
      "Epoch 66, Training Loss: 0.7865651222099935\n",
      "Epoch 67, Training Loss: 0.7865979154307143\n",
      "Epoch 68, Training Loss: 0.7854695835956057\n",
      "Epoch 69, Training Loss: 0.7858737781531828\n",
      "Epoch 70, Training Loss: 0.7860146111115477\n",
      "Epoch 71, Training Loss: 0.7857077511629664\n",
      "Epoch 72, Training Loss: 0.7854099568567778\n",
      "Epoch 73, Training Loss: 0.7857368131329242\n",
      "Epoch 74, Training Loss: 0.785143270349144\n",
      "Epoch 75, Training Loss: 0.7846558198892981\n",
      "Epoch 76, Training Loss: 0.7847657062057266\n",
      "Epoch 77, Training Loss: 0.7850924054482826\n",
      "Epoch 78, Training Loss: 0.7849074075096532\n",
      "Epoch 79, Training Loss: 0.7849662239390208\n",
      "Epoch 80, Training Loss: 0.7852271278101699\n",
      "Epoch 81, Training Loss: 0.7846293845571073\n",
      "Epoch 82, Training Loss: 0.7847275882735288\n",
      "Epoch 83, Training Loss: 0.7843320966663218\n",
      "Epoch 84, Training Loss: 0.7846106451256831\n",
      "Epoch 85, Training Loss: 0.7837486076623874\n",
      "Epoch 86, Training Loss: 0.7845381769022547\n",
      "Epoch 87, Training Loss: 0.7845209947206024\n",
      "Epoch 88, Training Loss: 0.7848693725758029\n",
      "Epoch 89, Training Loss: 0.7844749966062101\n",
      "Epoch 90, Training Loss: 0.7837220077227829\n",
      "Epoch 91, Training Loss: 0.7839180966068927\n",
      "Epoch 92, Training Loss: 0.7838718792101494\n",
      "Epoch 93, Training Loss: 0.783673434060319\n",
      "Epoch 94, Training Loss: 0.7836097041467078\n",
      "Epoch 95, Training Loss: 0.7837216119120892\n",
      "Epoch 96, Training Loss: 0.7835512500060232\n",
      "Epoch 97, Training Loss: 0.7838157901190277\n",
      "Epoch 98, Training Loss: 0.7841824066370053\n",
      "Epoch 99, Training Loss: 0.7834380641915745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:57:20,349] Trial 76 finished with value: 0.6376666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7837845715365015\n",
      "Epoch 1, Training Loss: 1.0528135913961074\n",
      "Epoch 2, Training Loss: 0.9953846662184771\n",
      "Epoch 3, Training Loss: 0.969751398563385\n",
      "Epoch 4, Training Loss: 0.9614571135885576\n",
      "Epoch 5, Training Loss: 0.9580277572659885\n",
      "Epoch 6, Training Loss: 0.955870864391327\n",
      "Epoch 7, Training Loss: 0.953715580701828\n",
      "Epoch 8, Training Loss: 0.9515380001068116\n",
      "Epoch 9, Training Loss: 0.9493267947785995\n",
      "Epoch 10, Training Loss: 0.9469467581720913\n",
      "Epoch 11, Training Loss: 0.9445286103557138\n",
      "Epoch 12, Training Loss: 0.941740296307732\n",
      "Epoch 13, Training Loss: 0.9389876950488371\n",
      "Epoch 14, Training Loss: 0.9358753232394947\n",
      "Epoch 15, Training Loss: 0.9326075156997232\n",
      "Epoch 16, Training Loss: 0.9290709542526918\n",
      "Epoch 17, Training Loss: 0.925351408439524\n",
      "Epoch 18, Training Loss: 0.9213773735130535\n",
      "Epoch 19, Training Loss: 0.917077863146277\n",
      "Epoch 20, Training Loss: 0.9128583246118882\n",
      "Epoch 21, Training Loss: 0.9082313815285178\n",
      "Epoch 22, Training Loss: 0.9035107982859892\n",
      "Epoch 23, Training Loss: 0.8986699031381046\n",
      "Epoch 24, Training Loss: 0.8936565484018887\n",
      "Epoch 25, Training Loss: 0.8885969002807842\n",
      "Epoch 26, Training Loss: 0.8834483392098371\n",
      "Epoch 27, Training Loss: 0.8785074426847346\n",
      "Epoch 28, Training Loss: 0.8734374660604141\n",
      "Epoch 29, Training Loss: 0.8685708470204297\n",
      "Epoch 30, Training Loss: 0.8638367514750537\n",
      "Epoch 31, Training Loss: 0.8592954946966732\n",
      "Epoch 32, Training Loss: 0.8548961423425113\n",
      "Epoch 33, Training Loss: 0.8506700299066656\n",
      "Epoch 34, Training Loss: 0.8468874349313624\n",
      "Epoch 35, Training Loss: 0.843189674685983\n",
      "Epoch 36, Training Loss: 0.8398976802124697\n",
      "Epoch 37, Training Loss: 0.8367334278190837\n",
      "Epoch 38, Training Loss: 0.8338102328777314\n",
      "Epoch 39, Training Loss: 0.8313345274504493\n",
      "Epoch 40, Training Loss: 0.8290418533717885\n",
      "Epoch 41, Training Loss: 0.8269481424023123\n",
      "Epoch 42, Training Loss: 0.8250269357597126\n",
      "Epoch 43, Training Loss: 0.8233644982646493\n",
      "Epoch 44, Training Loss: 0.8218139135136324\n",
      "Epoch 45, Training Loss: 0.8204983122208539\n",
      "Epoch 46, Training Loss: 0.8192936982126797\n",
      "Epoch 47, Training Loss: 0.8182597625255584\n",
      "Epoch 48, Training Loss: 0.8173285287969253\n",
      "Epoch 49, Training Loss: 0.816571556890712\n",
      "Epoch 50, Training Loss: 0.8157071542038637\n",
      "Epoch 51, Training Loss: 0.8150556880586287\n",
      "Epoch 52, Training Loss: 0.814551056132597\n",
      "Epoch 53, Training Loss: 0.814041343085906\n",
      "Epoch 54, Training Loss: 0.8134587750715367\n",
      "Epoch 55, Training Loss: 0.8131510031924528\n",
      "Epoch 56, Training Loss: 0.8126576702735003\n",
      "Epoch 57, Training Loss: 0.8124572959366967\n",
      "Epoch 58, Training Loss: 0.8120297240509706\n",
      "Epoch 59, Training Loss: 0.8117344181677875\n",
      "Epoch 60, Training Loss: 0.8115494453907013\n",
      "Epoch 61, Training Loss: 0.8112586121699389\n",
      "Epoch 62, Training Loss: 0.8110765632461099\n",
      "Epoch 63, Training Loss: 0.8108879899978638\n",
      "Epoch 64, Training Loss: 0.810716784140643\n",
      "Epoch 65, Training Loss: 0.810474854146733\n",
      "Epoch 66, Training Loss: 0.8103410381429336\n",
      "Epoch 67, Training Loss: 0.8102128397717195\n",
      "Epoch 68, Training Loss: 0.8100964354066288\n",
      "Epoch 69, Training Loss: 0.8099255481187035\n",
      "Epoch 70, Training Loss: 0.8097088780823876\n",
      "Epoch 71, Training Loss: 0.8095808436590083\n",
      "Epoch 72, Training Loss: 0.8094899612314561\n",
      "Epoch 73, Training Loss: 0.809433346005047\n",
      "Epoch 74, Training Loss: 0.8092804601613213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 21:58:30,975] Trial 77 finished with value: 0.6293333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8091926430954652\n",
      "Epoch 1, Training Loss: 0.8822341935774859\n",
      "Epoch 2, Training Loss: 0.8226302688963273\n",
      "Epoch 3, Training Loss: 0.8139853146496941\n",
      "Epoch 4, Training Loss: 0.8098417032466215\n",
      "Epoch 5, Training Loss: 0.80467077683\n",
      "Epoch 6, Training Loss: 0.802077380138285\n",
      "Epoch 7, Training Loss: 0.7995285348331227\n",
      "Epoch 8, Training Loss: 0.7973484117844526\n",
      "Epoch 9, Training Loss: 0.7956108525921317\n",
      "Epoch 10, Training Loss: 0.794837587370592\n",
      "Epoch 11, Training Loss: 0.7938626829315635\n",
      "Epoch 12, Training Loss: 0.7925919573447283\n",
      "Epoch 13, Training Loss: 0.7920888886732214\n",
      "Epoch 14, Training Loss: 0.7919134935210733\n",
      "Epoch 15, Training Loss: 0.7909923714048722\n",
      "Epoch 16, Training Loss: 0.7909423852668089\n",
      "Epoch 17, Training Loss: 0.7905022295082317\n",
      "Epoch 18, Training Loss: 0.7904454748069539\n",
      "Epoch 19, Training Loss: 0.7897710320528816\n",
      "Epoch 20, Training Loss: 0.7898928754469927\n",
      "Epoch 21, Training Loss: 0.7893007160635556\n",
      "Epoch 22, Training Loss: 0.7891389145570643\n",
      "Epoch 23, Training Loss: 0.7889812612533569\n",
      "Epoch 24, Training Loss: 0.7887327078510733\n",
      "Epoch 25, Training Loss: 0.7884339144650627\n",
      "Epoch 26, Training Loss: 0.788567793229047\n",
      "Epoch 27, Training Loss: 0.7875985434476067\n",
      "Epoch 28, Training Loss: 0.7877996581442216\n",
      "Epoch 29, Training Loss: 0.7876407731981838\n",
      "Epoch 30, Training Loss: 0.78737743973732\n",
      "Epoch 31, Training Loss: 0.7873222698183621\n",
      "Epoch 32, Training Loss: 0.7867458612778607\n",
      "Epoch 33, Training Loss: 0.7871487224102021\n",
      "Epoch 34, Training Loss: 0.7870035742310917\n",
      "Epoch 35, Training Loss: 0.7868877660526948\n",
      "Epoch 36, Training Loss: 0.786816439628601\n",
      "Epoch 37, Training Loss: 0.7860887802348417\n",
      "Epoch 38, Training Loss: 0.7862736766478594\n",
      "Epoch 39, Training Loss: 0.7865876877307891\n",
      "Epoch 40, Training Loss: 0.7861252549816581\n",
      "Epoch 41, Training Loss: 0.7862424867293414\n",
      "Epoch 42, Training Loss: 0.7859197883044973\n",
      "Epoch 43, Training Loss: 0.7860762882232666\n",
      "Epoch 44, Training Loss: 0.785313159718233\n",
      "Epoch 45, Training Loss: 0.7853801753240474\n",
      "Epoch 46, Training Loss: 0.7855518198013306\n",
      "Epoch 47, Training Loss: 0.7858063802298377\n",
      "Epoch 48, Training Loss: 0.7852913949770086\n",
      "Epoch 49, Training Loss: 0.7852506037319408\n",
      "Epoch 50, Training Loss: 0.785053385776632\n",
      "Epoch 51, Training Loss: 0.7858107822081623\n",
      "Epoch 52, Training Loss: 0.7848867341350106\n",
      "Epoch 53, Training Loss: 0.7853628941844492\n",
      "Epoch 54, Training Loss: 0.7851805820184595\n",
      "Epoch 55, Training Loss: 0.7847565815729254\n",
      "Epoch 56, Training Loss: 0.7849874467008254\n",
      "Epoch 57, Training Loss: 0.7847039224119747\n",
      "Epoch 58, Training Loss: 0.7847891543893253\n",
      "Epoch 59, Training Loss: 0.7844714659101822\n",
      "Epoch 60, Training Loss: 0.7848914778232574\n",
      "Epoch 61, Training Loss: 0.7845535432591157\n",
      "Epoch 62, Training Loss: 0.784649833020042\n",
      "Epoch 63, Training Loss: 0.7841203556341283\n",
      "Epoch 64, Training Loss: 0.7845418228121365\n",
      "Epoch 65, Training Loss: 0.7842137000841253\n",
      "Epoch 66, Training Loss: 0.7841218736592461\n",
      "Epoch 67, Training Loss: 0.7840369038020863\n",
      "Epoch 68, Training Loss: 0.7838346932214849\n",
      "Epoch 69, Training Loss: 0.7842487425663892\n",
      "Epoch 70, Training Loss: 0.7841139094969806\n",
      "Epoch 71, Training Loss: 0.7841364834589116\n",
      "Epoch 72, Training Loss: 0.7843589897716746\n",
      "Epoch 73, Training Loss: 0.7838717128248776\n",
      "Epoch 74, Training Loss: 0.7844574758585762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:00:06,925] Trial 78 finished with value: 0.6362666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7840345743824454\n",
      "Epoch 1, Training Loss: 0.9876600012358497\n",
      "Epoch 2, Training Loss: 0.9326138167521533\n",
      "Epoch 3, Training Loss: 0.891417779712116\n",
      "Epoch 4, Training Loss: 0.8364716592255761\n",
      "Epoch 5, Training Loss: 0.8204082044433145\n",
      "Epoch 6, Training Loss: 0.817036637208041\n",
      "Epoch 7, Training Loss: 0.8155154047292822\n",
      "Epoch 8, Training Loss: 0.8139154638963587\n",
      "Epoch 9, Training Loss: 0.8129209825571846\n",
      "Epoch 10, Training Loss: 0.8119302589051863\n",
      "Epoch 11, Training Loss: 0.810847104437211\n",
      "Epoch 12, Training Loss: 0.8098987289737253\n",
      "Epoch 13, Training Loss: 0.8087217436818516\n",
      "Epoch 14, Training Loss: 0.8076847879325643\n",
      "Epoch 15, Training Loss: 0.8068183768496794\n",
      "Epoch 16, Training Loss: 0.8062905233747819\n",
      "Epoch 17, Training Loss: 0.8057897875589483\n",
      "Epoch 18, Training Loss: 0.8051838595726911\n",
      "Epoch 19, Training Loss: 0.8044375838952906\n",
      "Epoch 20, Training Loss: 0.8038591458516963\n",
      "Epoch 21, Training Loss: 0.8033109399851631\n",
      "Epoch 22, Training Loss: 0.8031649784480824\n",
      "Epoch 23, Training Loss: 0.8023508701604956\n",
      "Epoch 24, Training Loss: 0.8022687981409186\n",
      "Epoch 25, Training Loss: 0.8017180677722482\n",
      "Epoch 26, Training Loss: 0.8012119373854469\n",
      "Epoch 27, Training Loss: 0.801063351280549\n",
      "Epoch 28, Training Loss: 0.8009160330716302\n",
      "Epoch 29, Training Loss: 0.8002165509672726\n",
      "Epoch 30, Training Loss: 0.8002168057946598\n",
      "Epoch 31, Training Loss: 0.7997023479377522\n",
      "Epoch 32, Training Loss: 0.7991831116115345\n",
      "Epoch 33, Training Loss: 0.7990040601702297\n",
      "Epoch 34, Training Loss: 0.7988160225223092\n",
      "Epoch 35, Training Loss: 0.7986325479255003\n",
      "Epoch 36, Training Loss: 0.7984417997388279\n",
      "Epoch 37, Training Loss: 0.7980653358207029\n",
      "Epoch 38, Training Loss: 0.797356423069449\n",
      "Epoch 39, Training Loss: 0.7974697062548469\n",
      "Epoch 40, Training Loss: 0.7969199300513548\n",
      "Epoch 41, Training Loss: 0.7969921652008506\n",
      "Epoch 42, Training Loss: 0.7964085075434516\n",
      "Epoch 43, Training Loss: 0.7958279698035297\n",
      "Epoch 44, Training Loss: 0.7952472568960751\n",
      "Epoch 45, Training Loss: 0.7948351146894342\n",
      "Epoch 46, Training Loss: 0.7943947426711812\n",
      "Epoch 47, Training Loss: 0.7941071527845719\n",
      "Epoch 48, Training Loss: 0.7936725406085744\n",
      "Epoch 49, Training Loss: 0.7931510106956258\n",
      "Epoch 50, Training Loss: 0.7927173107511857\n",
      "Epoch 51, Training Loss: 0.7922109748335445\n",
      "Epoch 52, Training Loss: 0.7918265804122476\n",
      "Epoch 53, Training Loss: 0.7918643995593576\n",
      "Epoch 54, Training Loss: 0.7912749122872073\n",
      "Epoch 55, Training Loss: 0.7908195084684035\n",
      "Epoch 56, Training Loss: 0.7906655642565559\n",
      "Epoch 57, Training Loss: 0.7907489457551171\n",
      "Epoch 58, Training Loss: 0.7901566240366767\n",
      "Epoch 59, Training Loss: 0.7899341689137851\n",
      "Epoch 60, Training Loss: 0.7896930228962618\n",
      "Epoch 61, Training Loss: 0.7892560611752902\n",
      "Epoch 62, Training Loss: 0.789030322116964\n",
      "Epoch 63, Training Loss: 0.7890133139666389\n",
      "Epoch 64, Training Loss: 0.7883538741223952\n",
      "Epoch 65, Training Loss: 0.788225212097168\n",
      "Epoch 66, Training Loss: 0.7883510050352882\n",
      "Epoch 67, Training Loss: 0.7875398593790391\n",
      "Epoch 68, Training Loss: 0.7876487300676458\n",
      "Epoch 69, Training Loss: 0.7876742435202879\n",
      "Epoch 70, Training Loss: 0.7872204580026514\n",
      "Epoch 71, Training Loss: 0.7871496575018939\n",
      "Epoch 72, Training Loss: 0.7871213322527268\n",
      "Epoch 73, Training Loss: 0.7868689119114596\n",
      "Epoch 74, Training Loss: 0.7869805068829481\n",
      "Epoch 75, Training Loss: 0.7866249153193305\n",
      "Epoch 76, Training Loss: 0.7864524595176472\n",
      "Epoch 77, Training Loss: 0.7862528617241803\n",
      "Epoch 78, Training Loss: 0.7861819183826446\n",
      "Epoch 79, Training Loss: 0.7859113348932827\n",
      "Epoch 80, Training Loss: 0.7859749771566952\n",
      "Epoch 81, Training Loss: 0.7859493180583506\n",
      "Epoch 82, Training Loss: 0.7856167258234585\n",
      "Epoch 83, Training Loss: 0.7853604900135713\n",
      "Epoch 84, Training Loss: 0.7856951991249533\n",
      "Epoch 85, Training Loss: 0.7853984907795402\n",
      "Epoch 86, Training Loss: 0.7849608692702125\n",
      "Epoch 87, Training Loss: 0.7852986163952771\n",
      "Epoch 88, Training Loss: 0.7852757373276879\n",
      "Epoch 89, Training Loss: 0.7849302238576552\n",
      "Epoch 90, Training Loss: 0.7847487232264351\n",
      "Epoch 91, Training Loss: 0.7845980246628032\n",
      "Epoch 92, Training Loss: 0.7845060534336987\n",
      "Epoch 93, Training Loss: 0.784602419278201\n",
      "Epoch 94, Training Loss: 0.7843934297561646\n",
      "Epoch 95, Training Loss: 0.784549370022381\n",
      "Epoch 96, Training Loss: 0.7844344250594868\n",
      "Epoch 97, Training Loss: 0.7842116945631363\n",
      "Epoch 98, Training Loss: 0.7843324262254379\n",
      "Epoch 99, Training Loss: 0.7840118326159085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:02:28,896] Trial 79 finished with value: 0.6396 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.783892370111802\n",
      "Epoch 1, Training Loss: 1.0775932359695435\n",
      "Epoch 2, Training Loss: 1.0221043427551495\n",
      "Epoch 3, Training Loss: 0.9509181416034699\n",
      "Epoch 4, Training Loss: 0.9283497017271378\n",
      "Epoch 5, Training Loss: 0.9146253841063555\n",
      "Epoch 6, Training Loss: 0.9001442597192877\n",
      "Epoch 7, Training Loss: 0.8806678563707016\n",
      "Epoch 8, Training Loss: 0.8531929063095766\n",
      "Epoch 9, Training Loss: 0.8278920935883242\n",
      "Epoch 10, Training Loss: 0.814871094156714\n",
      "Epoch 11, Training Loss: 0.8104104067297543\n",
      "Epoch 12, Training Loss: 0.8077878755681654\n",
      "Epoch 13, Training Loss: 0.8059452043561375\n",
      "Epoch 14, Training Loss: 0.8039385705835679\n",
      "Epoch 15, Training Loss: 0.8029480739200816\n",
      "Epoch 16, Training Loss: 0.8021623051166534\n",
      "Epoch 17, Training Loss: 0.8013481053885292\n",
      "Epoch 18, Training Loss: 0.8004742735974929\n",
      "Epoch 19, Training Loss: 0.8003249291111442\n",
      "Epoch 20, Training Loss: 0.7995696672972511\n",
      "Epoch 21, Training Loss: 0.7991717996316797\n",
      "Epoch 22, Training Loss: 0.7989275765419006\n",
      "Epoch 23, Training Loss: 0.7983704442136428\n",
      "Epoch 24, Training Loss: 0.7979455110606025\n",
      "Epoch 25, Training Loss: 0.7975486138988944\n",
      "Epoch 26, Training Loss: 0.7972179930350359\n",
      "Epoch 27, Training Loss: 0.7966579804700964\n",
      "Epoch 28, Training Loss: 0.796596278443056\n",
      "Epoch 29, Training Loss: 0.7959011218127082\n",
      "Epoch 30, Training Loss: 0.7958719981417937\n",
      "Epoch 31, Training Loss: 0.7956870035564199\n",
      "Epoch 32, Training Loss: 0.7950917699056513\n",
      "Epoch 33, Training Loss: 0.7950868182322558\n",
      "Epoch 34, Training Loss: 0.794649796556024\n",
      "Epoch 35, Training Loss: 0.794644325691111\n",
      "Epoch 36, Training Loss: 0.7943633116694058\n",
      "Epoch 37, Training Loss: 0.7940257002325619\n",
      "Epoch 38, Training Loss: 0.7936822243999032\n",
      "Epoch 39, Training Loss: 0.7931465517072117\n",
      "Epoch 40, Training Loss: 0.7932079579549677\n",
      "Epoch 41, Training Loss: 0.7928073776469511\n",
      "Epoch 42, Training Loss: 0.792689498312333\n",
      "Epoch 43, Training Loss: 0.7923350839053883\n",
      "Epoch 44, Training Loss: 0.792290957324645\n",
      "Epoch 45, Training Loss: 0.7921268880367279\n",
      "Epoch 46, Training Loss: 0.7916741536645329\n",
      "Epoch 47, Training Loss: 0.7914365909380071\n",
      "Epoch 48, Training Loss: 0.7912351482755997\n",
      "Epoch 49, Training Loss: 0.790690399127848\n",
      "Epoch 50, Training Loss: 0.7908652065080755\n",
      "Epoch 51, Training Loss: 0.7902367686524111\n",
      "Epoch 52, Training Loss: 0.7903808687714969\n",
      "Epoch 53, Training Loss: 0.7902582608251011\n",
      "Epoch 54, Training Loss: 0.7899463665485382\n",
      "Epoch 55, Training Loss: 0.7896967432078194\n",
      "Epoch 56, Training Loss: 0.7894025984932395\n",
      "Epoch 57, Training Loss: 0.7890651308788973\n",
      "Epoch 58, Training Loss: 0.7885919059725369\n",
      "Epoch 59, Training Loss: 0.7886013430006363\n",
      "Epoch 60, Training Loss: 0.7884009809353772\n",
      "Epoch 61, Training Loss: 0.7881165212743423\n",
      "Epoch 62, Training Loss: 0.7880287180227392\n",
      "Epoch 63, Training Loss: 0.7877098359781153\n",
      "Epoch 64, Training Loss: 0.7874245574193842\n",
      "Epoch 65, Training Loss: 0.7872863250620225\n",
      "Epoch 66, Training Loss: 0.787195412902271\n",
      "Epoch 67, Training Loss: 0.7871480154289919\n",
      "Epoch 68, Training Loss: 0.7866475666270537\n",
      "Epoch 69, Training Loss: 0.7865248495690963\n",
      "Epoch 70, Training Loss: 0.7864532482624054\n",
      "Epoch 71, Training Loss: 0.7861413396807277\n",
      "Epoch 72, Training Loss: 0.7859513853578006\n",
      "Epoch 73, Training Loss: 0.7857582245153539\n",
      "Epoch 74, Training Loss: 0.7857396150336546\n",
      "Epoch 75, Training Loss: 0.7854563378586489\n",
      "Epoch 76, Training Loss: 0.7852509816955118\n",
      "Epoch 77, Training Loss: 0.7852402379933525\n",
      "Epoch 78, Training Loss: 0.7849909044714535\n",
      "Epoch 79, Training Loss: 0.7849237681837643\n",
      "Epoch 80, Training Loss: 0.7846707132283379\n",
      "Epoch 81, Training Loss: 0.7846359177196727\n",
      "Epoch 82, Training Loss: 0.7843989436065449\n",
      "Epoch 83, Training Loss: 0.7842055804589215\n",
      "Epoch 84, Training Loss: 0.7841998821847579\n",
      "Epoch 85, Training Loss: 0.7841846245877883\n",
      "Epoch 86, Training Loss: 0.7839892024853651\n",
      "Epoch 87, Training Loss: 0.784086919742472\n",
      "Epoch 88, Training Loss: 0.7836272634478176\n",
      "Epoch 89, Training Loss: 0.783707033886629\n",
      "Epoch 90, Training Loss: 0.7835070546234355\n",
      "Epoch 91, Training Loss: 0.7834727653335123\n",
      "Epoch 92, Training Loss: 0.7831671653074377\n",
      "Epoch 93, Training Loss: 0.7833518988945904\n",
      "Epoch 94, Training Loss: 0.7831917388298932\n",
      "Epoch 95, Training Loss: 0.7832600897901199\n",
      "Epoch 96, Training Loss: 0.7829922995847814\n",
      "Epoch 97, Training Loss: 0.7830203660095439\n",
      "Epoch 98, Training Loss: 0.7828698311833774\n",
      "Epoch 99, Training Loss: 0.7827684500638177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:04:26,488] Trial 80 finished with value: 0.6378 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7827849651084227\n",
      "Epoch 1, Training Loss: 0.8479843557329106\n",
      "Epoch 2, Training Loss: 0.8169809885491106\n",
      "Epoch 3, Training Loss: 0.8147844353116545\n",
      "Epoch 4, Training Loss: 0.8120912259682677\n",
      "Epoch 5, Training Loss: 0.8117787595978356\n",
      "Epoch 6, Training Loss: 0.809598343981836\n",
      "Epoch 7, Training Loss: 0.8064908750971457\n",
      "Epoch 8, Training Loss: 0.8085729028945579\n",
      "Epoch 9, Training Loss: 0.8058640337528142\n",
      "Epoch 10, Training Loss: 0.803684153413414\n",
      "Epoch 11, Training Loss: 0.8027662014602719\n",
      "Epoch 12, Training Loss: 0.8050226441003326\n",
      "Epoch 13, Training Loss: 0.8020103709142011\n",
      "Epoch 14, Training Loss: 0.8046521513981927\n",
      "Epoch 15, Training Loss: 0.8032871200626058\n",
      "Epoch 16, Training Loss: 0.8030336202535414\n",
      "Epoch 17, Training Loss: 0.8019623013367331\n",
      "Epoch 18, Training Loss: 0.8005931514546387\n",
      "Epoch 19, Training Loss: 0.8013909560397156\n",
      "Epoch 20, Training Loss: 0.801019191024895\n",
      "Epoch 21, Training Loss: 0.8006135687792212\n",
      "Epoch 22, Training Loss: 0.8012897858942362\n",
      "Epoch 23, Training Loss: 0.8010289509493606\n",
      "Epoch 24, Training Loss: 0.8005398504716114\n",
      "Epoch 25, Training Loss: 0.7986546589915914\n",
      "Epoch 26, Training Loss: 0.8004125758221275\n",
      "Epoch 27, Training Loss: 0.8012921560975842\n",
      "Epoch 28, Training Loss: 0.800501407178721\n",
      "Epoch 29, Training Loss: 0.800554799406152\n",
      "Epoch 30, Training Loss: 0.7998088043435175\n",
      "Epoch 31, Training Loss: 0.798873556198034\n",
      "Epoch 32, Training Loss: 0.7979197725317532\n",
      "Epoch 33, Training Loss: 0.798427186514202\n",
      "Epoch 34, Training Loss: 0.7989190679743774\n",
      "Epoch 35, Training Loss: 0.7980275332479548\n",
      "Epoch 36, Training Loss: 0.7990356220338578\n",
      "Epoch 37, Training Loss: 0.7982674118271448\n",
      "Epoch 38, Training Loss: 0.7994504607709727\n",
      "Epoch 39, Training Loss: 0.7982959125275002\n",
      "Epoch 40, Training Loss: 0.7986847637291241\n",
      "Epoch 41, Training Loss: 0.798035410472325\n",
      "Epoch 42, Training Loss: 0.800155314467007\n",
      "Epoch 43, Training Loss: 0.796909510148199\n",
      "Epoch 44, Training Loss: 0.7976261812045162\n",
      "Epoch 45, Training Loss: 0.7976528432136192\n",
      "Epoch 46, Training Loss: 0.7982477880062018\n",
      "Epoch 47, Training Loss: 0.7970285375315443\n",
      "Epoch 48, Training Loss: 0.7982703893704521\n",
      "Epoch 49, Training Loss: 0.7972534434239668\n",
      "Epoch 50, Training Loss: 0.798017822620564\n",
      "Epoch 51, Training Loss: 0.7973218850623396\n",
      "Epoch 52, Training Loss: 0.7971073978825619\n",
      "Epoch 53, Training Loss: 0.7973376047342343\n",
      "Epoch 54, Training Loss: 0.7970777914936381\n",
      "Epoch 55, Training Loss: 0.7977379134274963\n",
      "Epoch 56, Training Loss: 0.7978279676652492\n",
      "Epoch 57, Training Loss: 0.7961643556006869\n",
      "Epoch 58, Training Loss: 0.7982668325417024\n",
      "Epoch 59, Training Loss: 0.7977009528561643\n",
      "Epoch 60, Training Loss: 0.7978170842156375\n",
      "Epoch 61, Training Loss: 0.7968614107684086\n",
      "Epoch 62, Training Loss: 0.7968431836680362\n",
      "Epoch 63, Training Loss: 0.7975228721934153\n",
      "Epoch 64, Training Loss: 0.796338639492379\n",
      "Epoch 65, Training Loss: 0.7974046557469475\n",
      "Epoch 66, Training Loss: 0.7969426909783729\n",
      "Epoch 67, Training Loss: 0.7960571933509712\n",
      "Epoch 68, Training Loss: 0.7975940923941763\n",
      "Epoch 69, Training Loss: 0.7981811116512557\n",
      "Epoch 70, Training Loss: 0.7977861402626324\n",
      "Epoch 71, Training Loss: 0.7975908431791722\n",
      "Epoch 72, Training Loss: 0.7972753069454566\n",
      "Epoch 73, Training Loss: 0.795900553330443\n",
      "Epoch 74, Training Loss: 0.7965416614274333\n",
      "Epoch 75, Training Loss: 0.7972277753335193\n",
      "Epoch 76, Training Loss: 0.7972801412854876\n",
      "Epoch 77, Training Loss: 0.7971794629007354\n",
      "Epoch 78, Training Loss: 0.7974443160501637\n",
      "Epoch 79, Training Loss: 0.7954292643339114\n",
      "Epoch 80, Training Loss: 0.7979902264767124\n",
      "Epoch 81, Training Loss: 0.796591255539342\n",
      "Epoch 82, Training Loss: 0.7963408621630275\n",
      "Epoch 83, Training Loss: 0.7965261603656568\n",
      "Epoch 84, Training Loss: 0.7978292988655262\n",
      "Epoch 85, Training Loss: 0.7960053304980572\n",
      "Epoch 86, Training Loss: 0.7979534314987354\n",
      "Epoch 87, Training Loss: 0.7967389182936876\n",
      "Epoch 88, Training Loss: 0.7967316265393021\n",
      "Epoch 89, Training Loss: 0.7969882428197932\n",
      "Epoch 90, Training Loss: 0.7974129346976603\n",
      "Epoch 91, Training Loss: 0.7966006045054672\n",
      "Epoch 92, Training Loss: 0.7969988113955447\n",
      "Epoch 93, Training Loss: 0.7958932292192502\n",
      "Epoch 94, Training Loss: 0.795868638583592\n",
      "Epoch 95, Training Loss: 0.7970558244482915\n",
      "Epoch 96, Training Loss: 0.7963209914533715\n",
      "Epoch 97, Training Loss: 0.7966823627177934\n",
      "Epoch 98, Training Loss: 0.7966488492219967\n",
      "Epoch 99, Training Loss: 0.7967235588489618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:06:08,623] Trial 81 finished with value: 0.6327333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7969996317884975\n",
      "Epoch 1, Training Loss: 1.0703776042264206\n",
      "Epoch 2, Training Loss: 1.0462160671563974\n",
      "Epoch 3, Training Loss: 1.0268615435836908\n",
      "Epoch 4, Training Loss: 1.010318150376915\n",
      "Epoch 5, Training Loss: 0.9971833902194087\n",
      "Epoch 6, Training Loss: 0.986491840943358\n",
      "Epoch 7, Training Loss: 0.9782230372715713\n",
      "Epoch 8, Training Loss: 0.9724539056756443\n",
      "Epoch 9, Training Loss: 0.9681392865969722\n",
      "Epoch 10, Training Loss: 0.9644820022403746\n",
      "Epoch 11, Training Loss: 0.9619853017025424\n",
      "Epoch 12, Training Loss: 0.9595849151898147\n",
      "Epoch 13, Training Loss: 0.9582560818894466\n",
      "Epoch 14, Training Loss: 0.957038728814376\n",
      "Epoch 15, Training Loss: 0.9560784900995125\n",
      "Epoch 16, Training Loss: 0.9552039814174623\n",
      "Epoch 17, Training Loss: 0.9540458205050992\n",
      "Epoch 18, Training Loss: 0.9532322434554422\n",
      "Epoch 19, Training Loss: 0.9520961184250681\n",
      "Epoch 20, Training Loss: 0.9510871375413765\n",
      "Epoch 21, Training Loss: 0.9507752528764252\n",
      "Epoch 22, Training Loss: 0.9493808334035084\n",
      "Epoch 23, Training Loss: 0.9489992386416385\n",
      "Epoch 24, Training Loss: 0.9477247275804218\n",
      "Epoch 25, Training Loss: 0.9479423474548454\n",
      "Epoch 26, Training Loss: 0.9469535763998677\n",
      "Epoch 27, Training Loss: 0.9456343225966719\n",
      "Epoch 28, Training Loss: 0.9450108631212909\n",
      "Epoch 29, Training Loss: 0.9442874820608842\n",
      "Epoch 30, Training Loss: 0.9430195186371194\n",
      "Epoch 31, Training Loss: 0.9423601706225173\n",
      "Epoch 32, Training Loss: 0.9412460339696784\n",
      "Epoch 33, Training Loss: 0.9407703415791792\n",
      "Epoch 34, Training Loss: 0.939990618533658\n",
      "Epoch 35, Training Loss: 0.9391693356341886\n",
      "Epoch 36, Training Loss: 0.9378112905903866\n",
      "Epoch 37, Training Loss: 0.9369759241441139\n",
      "Epoch 38, Training Loss: 0.9365962508029507\n",
      "Epoch 39, Training Loss: 0.935798454374299\n",
      "Epoch 40, Training Loss: 0.9345743044874721\n",
      "Epoch 41, Training Loss: 0.9335213243513178\n",
      "Epoch 42, Training Loss: 0.9324439163494828\n",
      "Epoch 43, Training Loss: 0.9313905739246454\n",
      "Epoch 44, Training Loss: 0.9303401307952135\n",
      "Epoch 45, Training Loss: 0.9291906577303893\n",
      "Epoch 46, Training Loss: 0.9282961258314606\n",
      "Epoch 47, Training Loss: 0.9274723059252689\n",
      "Epoch 48, Training Loss: 0.926398918323947\n",
      "Epoch 49, Training Loss: 0.9254397705085295\n",
      "Epoch 50, Training Loss: 0.9238811964379218\n",
      "Epoch 51, Training Loss: 0.9230516039339224\n",
      "Epoch 52, Training Loss: 0.9211473735651575\n",
      "Epoch 53, Training Loss: 0.9203442039346337\n",
      "Epoch 54, Training Loss: 0.9190427935213075\n",
      "Epoch 55, Training Loss: 0.9180232069546119\n",
      "Epoch 56, Training Loss: 0.9164074902247665\n",
      "Epoch 57, Training Loss: 0.9149968969194513\n",
      "Epoch 58, Training Loss: 0.9147122435999993\n",
      "Epoch 59, Training Loss: 0.9131313995311134\n",
      "Epoch 60, Training Loss: 0.9118722260446477\n",
      "Epoch 61, Training Loss: 0.9101498869128694\n",
      "Epoch 62, Training Loss: 0.9087399647648173\n",
      "Epoch 63, Training Loss: 0.9069042577779383\n",
      "Epoch 64, Training Loss: 0.9052161457843351\n",
      "Epoch 65, Training Loss: 0.9041111666457098\n",
      "Epoch 66, Training Loss: 0.9027490199956679\n",
      "Epoch 67, Training Loss: 0.9014951319622814\n",
      "Epoch 68, Training Loss: 0.8996095266557278\n",
      "Epoch 69, Training Loss: 0.897814306997715\n",
      "Epoch 70, Training Loss: 0.8962630681525495\n",
      "Epoch 71, Training Loss: 0.8941080062012924\n",
      "Epoch 72, Training Loss: 0.8929771822197993\n",
      "Epoch 73, Training Loss: 0.8913195151135438\n",
      "Epoch 74, Training Loss: 0.8897738750715901\n",
      "Epoch 75, Training Loss: 0.8874426404336342\n",
      "Epoch 76, Training Loss: 0.8861099090791287\n",
      "Epoch 77, Training Loss: 0.8841780189284705\n",
      "Epoch 78, Training Loss: 0.8826998035710557\n",
      "Epoch 79, Training Loss: 0.8812868130834479\n",
      "Epoch 80, Training Loss: 0.8791113244859796\n",
      "Epoch 81, Training Loss: 0.8772100473705091\n",
      "Epoch 82, Training Loss: 0.8751606195492851\n",
      "Epoch 83, Training Loss: 0.873969252485978\n",
      "Epoch 84, Training Loss: 0.8720259254140065\n",
      "Epoch 85, Training Loss: 0.870396343926738\n",
      "Epoch 86, Training Loss: 0.8683823494982899\n",
      "Epoch 87, Training Loss: 0.8666214371982374\n",
      "Epoch 88, Training Loss: 0.8646940439267266\n",
      "Epoch 89, Training Loss: 0.8631539641466356\n",
      "Epoch 90, Training Loss: 0.8612754599492353\n",
      "Epoch 91, Training Loss: 0.8598012501135804\n",
      "Epoch 92, Training Loss: 0.8583791739062259\n",
      "Epoch 93, Training Loss: 0.8558949619307554\n",
      "Epoch 94, Training Loss: 0.8548758961204299\n",
      "Epoch 95, Training Loss: 0.8534779555815503\n",
      "Epoch 96, Training Loss: 0.8512661645286962\n",
      "Epoch 97, Training Loss: 0.8499422089497846\n",
      "Epoch 98, Training Loss: 0.8483294911850664\n",
      "Epoch 99, Training Loss: 0.8470189463823361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:07:45,133] Trial 82 finished with value: 0.6088 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8453251530353288\n",
      "Epoch 1, Training Loss: 0.850746539550669\n",
      "Epoch 2, Training Loss: 0.8144807295939501\n",
      "Epoch 3, Training Loss: 0.8084626966364243\n",
      "Epoch 4, Training Loss: 0.8059999249261969\n",
      "Epoch 5, Training Loss: 0.8034508451293496\n",
      "Epoch 6, Training Loss: 0.80101715073866\n",
      "Epoch 7, Training Loss: 0.8002764680105097\n",
      "Epoch 8, Training Loss: 0.7998176045978771\n",
      "Epoch 9, Training Loss: 0.7986227378424476\n",
      "Epoch 10, Training Loss: 0.7978687386653003\n",
      "Epoch 11, Training Loss: 0.796985802860821\n",
      "Epoch 12, Training Loss: 0.7963333861266866\n",
      "Epoch 13, Training Loss: 0.7968520081043243\n",
      "Epoch 14, Training Loss: 0.796453017837861\n",
      "Epoch 15, Training Loss: 0.79549368353451\n",
      "Epoch 16, Training Loss: 0.7949125930141\n",
      "Epoch 17, Training Loss: 0.7945812299672295\n",
      "Epoch 18, Training Loss: 0.7955832184062285\n",
      "Epoch 19, Training Loss: 0.7937801256600548\n",
      "Epoch 20, Training Loss: 0.7944720896552591\n",
      "Epoch 21, Training Loss: 0.7944244826541228\n",
      "Epoch 22, Training Loss: 0.7939472567333895\n",
      "Epoch 23, Training Loss: 0.7938372065740473\n",
      "Epoch 24, Training Loss: 0.7933052235491136\n",
      "Epoch 25, Training Loss: 0.793030024135814\n",
      "Epoch 26, Training Loss: 0.7932263071396771\n",
      "Epoch 27, Training Loss: 0.7933356746505289\n",
      "Epoch 28, Training Loss: 0.7929564770530252\n",
      "Epoch 29, Training Loss: 0.7928369541729198\n",
      "Epoch 30, Training Loss: 0.7929904285599204\n",
      "Epoch 31, Training Loss: 0.7929106119801016\n",
      "Epoch 32, Training Loss: 0.7923660505519193\n",
      "Epoch 33, Training Loss: 0.792667877814349\n",
      "Epoch 34, Training Loss: 0.7920899177298827\n",
      "Epoch 35, Training Loss: 0.791597059544395\n",
      "Epoch 36, Training Loss: 0.7924722529158873\n",
      "Epoch 37, Training Loss: 0.7923842461670146\n",
      "Epoch 38, Training Loss: 0.7920373483966379\n",
      "Epoch 39, Training Loss: 0.7918322924305411\n",
      "Epoch 40, Training Loss: 0.7918608571501339\n",
      "Epoch 41, Training Loss: 0.7918144254123464\n",
      "Epoch 42, Training Loss: 0.7918308497176451\n",
      "Epoch 43, Training Loss: 0.791649692479302\n",
      "Epoch 44, Training Loss: 0.7916199617526111\n",
      "Epoch 45, Training Loss: 0.7916266687477336\n",
      "Epoch 46, Training Loss: 0.7918011285978205\n",
      "Epoch 47, Training Loss: 0.7917541017953087\n",
      "Epoch 48, Training Loss: 0.7910595530622145\n",
      "Epoch 49, Training Loss: 0.7914423662073472\n",
      "Epoch 50, Training Loss: 0.7914284574985504\n",
      "Epoch 51, Training Loss: 0.7914963036424973\n",
      "Epoch 52, Training Loss: 0.7917076640970566\n",
      "Epoch 53, Training Loss: 0.7908192117775188\n",
      "Epoch 54, Training Loss: 0.7912468138161828\n",
      "Epoch 55, Training Loss: 0.7912129043831545\n",
      "Epoch 56, Training Loss: 0.7917164172144497\n",
      "Epoch 57, Training Loss: 0.7917572085997637\n",
      "Epoch 58, Training Loss: 0.790990848050398\n",
      "Epoch 59, Training Loss: 0.7908396999976214\n",
      "Epoch 60, Training Loss: 0.7910216575510362\n",
      "Epoch 61, Training Loss: 0.79057246390511\n",
      "Epoch 62, Training Loss: 0.7917508313235114\n",
      "Epoch 63, Training Loss: 0.7908457582137164\n",
      "Epoch 64, Training Loss: 0.7903926243501551\n",
      "Epoch 65, Training Loss: 0.7909406758757198\n",
      "Epoch 66, Training Loss: 0.7908171955276938\n",
      "Epoch 67, Training Loss: 0.7908683527918423\n",
      "Epoch 68, Training Loss: 0.7910756000350503\n",
      "Epoch 69, Training Loss: 0.7904610980258269\n",
      "Epoch 70, Training Loss: 0.7912171053886413\n",
      "Epoch 71, Training Loss: 0.7904479007861194\n",
      "Epoch 72, Training Loss: 0.7905130359705757\n",
      "Epoch 73, Training Loss: 0.7904000169389388\n",
      "Epoch 74, Training Loss: 0.7899720374275656\n",
      "Epoch 75, Training Loss: 0.7906222622534808\n",
      "Epoch 76, Training Loss: 0.7907595065762015\n",
      "Epoch 77, Training Loss: 0.7902161679548376\n",
      "Epoch 78, Training Loss: 0.7907210655773387\n",
      "Epoch 79, Training Loss: 0.7905132621176103\n",
      "Epoch 80, Training Loss: 0.7902670652024886\n",
      "Epoch 81, Training Loss: 0.7898586165203768\n",
      "Epoch 82, Training Loss: 0.7907823099108303\n",
      "Epoch 83, Training Loss: 0.7900775342829087\n",
      "Epoch 84, Training Loss: 0.7905981538576238\n",
      "Epoch 85, Training Loss: 0.7901677722790662\n",
      "Epoch 86, Training Loss: 0.7902670492845423\n",
      "Epoch 87, Training Loss: 0.7900841286603142\n",
      "Epoch 88, Training Loss: 0.7898984531795278\n",
      "Epoch 89, Training Loss: 0.7899865639209748\n",
      "Epoch 90, Training Loss: 0.7901373931239632\n",
      "Epoch 91, Training Loss: 0.7910622541343465\n",
      "Epoch 92, Training Loss: 0.7902079552762649\n",
      "Epoch 93, Training Loss: 0.7902141581563389\n",
      "Epoch 94, Training Loss: 0.7907272177584032\n",
      "Epoch 95, Training Loss: 0.7899862844102523\n",
      "Epoch 96, Training Loss: 0.7899758171333986\n",
      "Epoch 97, Training Loss: 0.7904178309440613\n",
      "Epoch 98, Training Loss: 0.7902839032341452\n",
      "Epoch 99, Training Loss: 0.7900543760552126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:09:56,797] Trial 83 finished with value: 0.6380666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7902221781366011\n",
      "Epoch 1, Training Loss: 1.0055884469720655\n",
      "Epoch 2, Training Loss: 0.9546770002608909\n",
      "Epoch 3, Training Loss: 0.9404385120348823\n",
      "Epoch 4, Training Loss: 0.9329105701661647\n",
      "Epoch 5, Training Loss: 0.9258946949377992\n",
      "Epoch 6, Training Loss: 0.9205430138380007\n",
      "Epoch 7, Training Loss: 0.9152564994374612\n",
      "Epoch 8, Training Loss: 0.9098082863298574\n",
      "Epoch 9, Training Loss: 0.9049845460662268\n",
      "Epoch 10, Training Loss: 0.8999360305922371\n",
      "Epoch 11, Training Loss: 0.8946959497337055\n",
      "Epoch 12, Training Loss: 0.8890585160793218\n",
      "Epoch 13, Training Loss: 0.8828889097486223\n",
      "Epoch 14, Training Loss: 0.8764215960538477\n",
      "Epoch 15, Training Loss: 0.8698489983279006\n",
      "Epoch 16, Training Loss: 0.8638519809658366\n",
      "Epoch 17, Training Loss: 0.8572129600926449\n",
      "Epoch 18, Training Loss: 0.8516235776413652\n",
      "Epoch 19, Training Loss: 0.8462300629544078\n",
      "Epoch 20, Training Loss: 0.8405980248200265\n",
      "Epoch 21, Training Loss: 0.8353993604954024\n",
      "Epoch 22, Training Loss: 0.8318075081459562\n",
      "Epoch 23, Training Loss: 0.828059947221799\n",
      "Epoch 24, Training Loss: 0.8248430938648998\n",
      "Epoch 25, Training Loss: 0.8215789180052908\n",
      "Epoch 26, Training Loss: 0.8194002328062416\n",
      "Epoch 27, Training Loss: 0.8178508541637793\n",
      "Epoch 28, Training Loss: 0.8160075401901302\n",
      "Epoch 29, Training Loss: 0.813826744897025\n",
      "Epoch 30, Training Loss: 0.8130320785637188\n",
      "Epoch 31, Training Loss: 0.8115201275151475\n",
      "Epoch 32, Training Loss: 0.8107165093708756\n",
      "Epoch 33, Training Loss: 0.8095454950977985\n",
      "Epoch 34, Training Loss: 0.8090288914235911\n",
      "Epoch 35, Training Loss: 0.8078687802293247\n",
      "Epoch 36, Training Loss: 0.807338361542924\n",
      "Epoch 37, Training Loss: 0.8071451666659879\n",
      "Epoch 38, Training Loss: 0.8062224843448266\n",
      "Epoch 39, Training Loss: 0.8058076154020496\n",
      "Epoch 40, Training Loss: 0.8067084512316195\n",
      "Epoch 41, Training Loss: 0.8052016611386063\n",
      "Epoch 42, Training Loss: 0.8049451135154954\n",
      "Epoch 43, Training Loss: 0.804688697112234\n",
      "Epoch 44, Training Loss: 0.8042460937249033\n",
      "Epoch 45, Training Loss: 0.8041740042822701\n",
      "Epoch 46, Training Loss: 0.8040022480756717\n",
      "Epoch 47, Training Loss: 0.8038131296186519\n",
      "Epoch 48, Training Loss: 0.8036312659880273\n",
      "Epoch 49, Training Loss: 0.8032553861912032\n",
      "Epoch 50, Training Loss: 0.8030901216922846\n",
      "Epoch 51, Training Loss: 0.8036251224969563\n",
      "Epoch 52, Training Loss: 0.803161112616833\n",
      "Epoch 53, Training Loss: 0.8031827473102655\n",
      "Epoch 54, Training Loss: 0.8030210249406055\n",
      "Epoch 55, Training Loss: 0.802445536000388\n",
      "Epoch 56, Training Loss: 0.802629191893384\n",
      "Epoch 57, Training Loss: 0.8026448864685861\n",
      "Epoch 58, Training Loss: 0.8021630552478303\n",
      "Epoch 59, Training Loss: 0.8024394053265564\n",
      "Epoch 60, Training Loss: 0.8021945066918108\n",
      "Epoch 61, Training Loss: 0.8015552241103093\n",
      "Epoch 62, Training Loss: 0.8015431601302068\n",
      "Epoch 63, Training Loss: 0.8014811453066374\n",
      "Epoch 64, Training Loss: 0.8011910617799687\n",
      "Epoch 65, Training Loss: 0.8010454150967132\n",
      "Epoch 66, Training Loss: 0.800916195991344\n",
      "Epoch 67, Training Loss: 0.8005837328451917\n",
      "Epoch 68, Training Loss: 0.80082373654932\n",
      "Epoch 69, Training Loss: 0.8004093263382303\n",
      "Epoch 70, Training Loss: 0.8005649702889579\n",
      "Epoch 71, Training Loss: 0.8000678870014678\n",
      "Epoch 72, Training Loss: 0.7996818887559991\n",
      "Epoch 73, Training Loss: 0.7996553426398371\n",
      "Epoch 74, Training Loss: 0.799677959062103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:11:01,054] Trial 84 finished with value: 0.6332666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7989993028174666\n",
      "Epoch 1, Training Loss: 0.8992162190881887\n",
      "Epoch 2, Training Loss: 0.8181784598450912\n",
      "Epoch 3, Training Loss: 0.8108022221048972\n",
      "Epoch 4, Training Loss: 0.8069087083178356\n",
      "Epoch 5, Training Loss: 0.8038572011137367\n",
      "Epoch 6, Training Loss: 0.801633263799481\n",
      "Epoch 7, Training Loss: 0.7998285689748319\n",
      "Epoch 8, Training Loss: 0.798899957918583\n",
      "Epoch 9, Training Loss: 0.7963709514840205\n",
      "Epoch 10, Training Loss: 0.7944744825811314\n",
      "Epoch 11, Training Loss: 0.7930435622545113\n",
      "Epoch 12, Training Loss: 0.7908638569645415\n",
      "Epoch 13, Training Loss: 0.7920432842763743\n",
      "Epoch 14, Training Loss: 0.7916634389332362\n",
      "Epoch 15, Training Loss: 0.790278270728606\n",
      "Epoch 16, Training Loss: 0.7901539785521371\n",
      "Epoch 17, Training Loss: 0.7887652311109958\n",
      "Epoch 18, Training Loss: 0.7898091619176076\n",
      "Epoch 19, Training Loss: 0.7894287090552481\n",
      "Epoch 20, Training Loss: 0.7887818160809968\n",
      "Epoch 21, Training Loss: 0.7880527084035085\n",
      "Epoch 22, Training Loss: 0.7881319141925726\n",
      "Epoch 23, Training Loss: 0.78830676302874\n",
      "Epoch 24, Training Loss: 0.7878182940913322\n",
      "Epoch 25, Training Loss: 0.7874006625404931\n",
      "Epoch 26, Training Loss: 0.7881595248566534\n",
      "Epoch 27, Training Loss: 0.7878907324676226\n",
      "Epoch 28, Training Loss: 0.7874766754028493\n",
      "Epoch 29, Training Loss: 0.7872735843622595\n",
      "Epoch 30, Training Loss: 0.7858877044871337\n",
      "Epoch 31, Training Loss: 0.7867807321978691\n",
      "Epoch 32, Training Loss: 0.7864775302714871\n",
      "Epoch 33, Training Loss: 0.7856234736012336\n",
      "Epoch 34, Training Loss: 0.7872803852074128\n",
      "Epoch 35, Training Loss: 0.7868064089825278\n",
      "Epoch 36, Training Loss: 0.7856777353394301\n",
      "Epoch 37, Training Loss: 0.785446586286215\n",
      "Epoch 38, Training Loss: 0.784862134331151\n",
      "Epoch 39, Training Loss: 0.7858470186255032\n",
      "Epoch 40, Training Loss: 0.784845541294356\n",
      "Epoch 41, Training Loss: 0.7849971328462874\n",
      "Epoch 42, Training Loss: 0.7843355768605282\n",
      "Epoch 43, Training Loss: 0.7842997153002517\n",
      "Epoch 44, Training Loss: 0.7845081029081703\n",
      "Epoch 45, Training Loss: 0.7841199578199172\n",
      "Epoch 46, Training Loss: 0.7848620295524598\n",
      "Epoch 47, Training Loss: 0.783643310858791\n",
      "Epoch 48, Training Loss: 0.7842888248594184\n",
      "Epoch 49, Training Loss: 0.7840047969853967\n",
      "Epoch 50, Training Loss: 0.7839969076608356\n",
      "Epoch 51, Training Loss: 0.7844947909054003\n",
      "Epoch 52, Training Loss: 0.7837902169478567\n",
      "Epoch 53, Training Loss: 0.7837941647472239\n",
      "Epoch 54, Training Loss: 0.7827075593453601\n",
      "Epoch 55, Training Loss: 0.7835897176785577\n",
      "Epoch 56, Training Loss: 0.7840276779088758\n",
      "Epoch 57, Training Loss: 0.7834818993295942\n",
      "Epoch 58, Training Loss: 0.7832462043690502\n",
      "Epoch 59, Training Loss: 0.7838545159289712\n",
      "Epoch 60, Training Loss: 0.7827271891715831\n",
      "Epoch 61, Training Loss: 0.7822987593206248\n",
      "Epoch 62, Training Loss: 0.7823114128041088\n",
      "Epoch 63, Training Loss: 0.7824067537945912\n",
      "Epoch 64, Training Loss: 0.7831053238166006\n",
      "Epoch 65, Training Loss: 0.7814278198812241\n",
      "Epoch 66, Training Loss: 0.7829547116630956\n",
      "Epoch 67, Training Loss: 0.78284756572623\n",
      "Epoch 68, Training Loss: 0.7819940434362656\n",
      "Epoch 69, Training Loss: 0.7812176241910547\n",
      "Epoch 70, Training Loss: 0.7815406678314496\n",
      "Epoch 71, Training Loss: 0.7834458900573559\n",
      "Epoch 72, Training Loss: 0.7824871655693628\n",
      "Epoch 73, Training Loss: 0.7812682194817335\n",
      "Epoch 74, Training Loss: 0.7818582201362553\n",
      "Epoch 75, Training Loss: 0.7826131314263308\n",
      "Epoch 76, Training Loss: 0.7811016094415708\n",
      "Epoch 77, Training Loss: 0.7814739334852175\n",
      "Epoch 78, Training Loss: 0.7819792832647051\n",
      "Epoch 79, Training Loss: 0.7813765534780975\n",
      "Epoch 80, Training Loss: 0.7812437154296645\n",
      "Epoch 81, Training Loss: 0.7811032746967517\n",
      "Epoch 82, Training Loss: 0.7808516903031141\n",
      "Epoch 83, Training Loss: 0.7806213236392889\n",
      "Epoch 84, Training Loss: 0.7806578139613446\n",
      "Epoch 85, Training Loss: 0.7809572851747498\n",
      "Epoch 86, Training Loss: 0.7808949741205775\n",
      "Epoch 87, Training Loss: 0.7802905578362315\n",
      "Epoch 88, Training Loss: 0.7803834636408583\n",
      "Epoch 89, Training Loss: 0.7805614866708455\n",
      "Epoch 90, Training Loss: 0.7800358212980113\n",
      "Epoch 91, Training Loss: 0.7807527410356622\n",
      "Epoch 92, Training Loss: 0.780879407179983\n",
      "Epoch 93, Training Loss: 0.7795721074692289\n",
      "Epoch 94, Training Loss: 0.7796260836429165\n",
      "Epoch 95, Training Loss: 0.7802440302712577\n",
      "Epoch 96, Training Loss: 0.779354573102822\n",
      "Epoch 97, Training Loss: 0.7798018928757288\n",
      "Epoch 98, Training Loss: 0.7792320601025918\n",
      "Epoch 99, Training Loss: 0.7797649862174701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:12:59,482] Trial 85 finished with value: 0.6402 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7792135911776608\n",
      "Epoch 1, Training Loss: 0.8717470396266264\n",
      "Epoch 2, Training Loss: 0.8069219154470108\n",
      "Epoch 3, Training Loss: 0.8010373103618622\n",
      "Epoch 4, Training Loss: 0.7975983205963584\n",
      "Epoch 5, Training Loss: 0.794544488472097\n",
      "Epoch 6, Training Loss: 0.7919646777124966\n",
      "Epoch 7, Training Loss: 0.7901472628817838\n",
      "Epoch 8, Training Loss: 0.7891376278680914\n",
      "Epoch 9, Training Loss: 0.7877787991832285\n",
      "Epoch 10, Training Loss: 0.7865732827607324\n",
      "Epoch 11, Training Loss: 0.7862221473104813\n",
      "Epoch 12, Training Loss: 0.7863500196092269\n",
      "Epoch 13, Training Loss: 0.7852150438112371\n",
      "Epoch 14, Training Loss: 0.7850070101373335\n",
      "Epoch 15, Training Loss: 0.78422649250311\n",
      "Epoch 16, Training Loss: 0.78403128119076\n",
      "Epoch 17, Training Loss: 0.7839968122454251\n",
      "Epoch 18, Training Loss: 0.7828915970465716\n",
      "Epoch 19, Training Loss: 0.7828907387396868\n",
      "Epoch 20, Training Loss: 0.7819945694418514\n",
      "Epoch 21, Training Loss: 0.7821121582564186\n",
      "Epoch 22, Training Loss: 0.7820568415697883\n",
      "Epoch 23, Training Loss: 0.7809416320744683\n",
      "Epoch 24, Training Loss: 0.781441866089316\n",
      "Epoch 25, Training Loss: 0.7816016979077283\n",
      "Epoch 26, Training Loss: 0.7809850347042083\n",
      "Epoch 27, Training Loss: 0.7810445697167341\n",
      "Epoch 28, Training Loss: 0.7804882745181813\n",
      "Epoch 29, Training Loss: 0.7801525437130648\n",
      "Epoch 30, Training Loss: 0.7801735538594863\n",
      "Epoch 31, Training Loss: 0.7794069051041322\n",
      "Epoch 32, Training Loss: 0.7796519671468174\n",
      "Epoch 33, Training Loss: 0.7789416718482971\n",
      "Epoch 34, Training Loss: 0.7792704209860634\n",
      "Epoch 35, Training Loss: 0.7791356059382943\n",
      "Epoch 36, Training Loss: 0.77870658678167\n",
      "Epoch 37, Training Loss: 0.778907400229398\n",
      "Epoch 38, Training Loss: 0.778605601086336\n",
      "Epoch 39, Training Loss: 0.777846101101707\n",
      "Epoch 40, Training Loss: 0.7781106044965632\n",
      "Epoch 41, Training Loss: 0.7782643681413988\n",
      "Epoch 42, Training Loss: 0.7777942796314464\n",
      "Epoch 43, Training Loss: 0.7777050392767962\n",
      "Epoch 44, Training Loss: 0.7771881506022285\n",
      "Epoch 45, Training Loss: 0.777204308089088\n",
      "Epoch 46, Training Loss: 0.7772848574554219\n",
      "Epoch 47, Training Loss: 0.7771039636696087\n",
      "Epoch 48, Training Loss: 0.7770848007061902\n",
      "Epoch 49, Training Loss: 0.7769321322441101\n",
      "Epoch 50, Training Loss: 0.776216348549899\n",
      "Epoch 51, Training Loss: 0.7768300768207101\n",
      "Epoch 52, Training Loss: 0.7759320722608005\n",
      "Epoch 53, Training Loss: 0.7757636459434734\n",
      "Epoch 54, Training Loss: 0.7758433874214397\n",
      "Epoch 55, Training Loss: 0.775897811721353\n",
      "Epoch 56, Training Loss: 0.7755118313957663\n",
      "Epoch 57, Training Loss: 0.7758977979071\n",
      "Epoch 58, Training Loss: 0.7749078985522775\n",
      "Epoch 59, Training Loss: 0.7749890462791218\n",
      "Epoch 60, Training Loss: 0.7752323249508353\n",
      "Epoch 61, Training Loss: 0.7747674892229193\n",
      "Epoch 62, Training Loss: 0.7744421231045443\n",
      "Epoch 63, Training Loss: 0.77460636566667\n",
      "Epoch 64, Training Loss: 0.7746737086772919\n",
      "Epoch 65, Training Loss: 0.7743997908339781\n",
      "Epoch 66, Training Loss: 0.7740095265472636\n",
      "Epoch 67, Training Loss: 0.7744270797336803\n",
      "Epoch 68, Training Loss: 0.7737410313942853\n",
      "Epoch 69, Training Loss: 0.7738385971854714\n",
      "Epoch 70, Training Loss: 0.7739296115145964\n",
      "Epoch 71, Training Loss: 0.7735857374527875\n",
      "Epoch 72, Training Loss: 0.7735731842938591\n",
      "Epoch 73, Training Loss: 0.7733744228587431\n",
      "Epoch 74, Training Loss: 0.7728641939163208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:14:57,648] Trial 86 finished with value: 0.6392666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7730265533222872\n",
      "Epoch 1, Training Loss: 0.9145884686842897\n",
      "Epoch 2, Training Loss: 0.8213540080794715\n",
      "Epoch 3, Training Loss: 0.8127409311613643\n",
      "Epoch 4, Training Loss: 0.8106776676679912\n",
      "Epoch 5, Training Loss: 0.808520900217214\n",
      "Epoch 6, Training Loss: 0.8081155495536059\n",
      "Epoch 7, Training Loss: 0.8064399379536622\n",
      "Epoch 8, Training Loss: 0.8046209032374217\n",
      "Epoch 9, Training Loss: 0.8060325792857579\n",
      "Epoch 10, Training Loss: 0.8034984170942379\n",
      "Epoch 11, Training Loss: 0.8043784337832516\n",
      "Epoch 12, Training Loss: 0.8032018041252194\n",
      "Epoch 13, Training Loss: 0.8042439418627804\n",
      "Epoch 14, Training Loss: 0.8044519968498919\n",
      "Epoch 15, Training Loss: 0.8028742980239982\n",
      "Epoch 16, Training Loss: 0.8033239029403916\n",
      "Epoch 17, Training Loss: 0.8022896138348974\n",
      "Epoch 18, Training Loss: 0.8026301763111487\n",
      "Epoch 19, Training Loss: 0.8034729120426608\n",
      "Epoch 20, Training Loss: 0.8021281963004205\n",
      "Epoch 21, Training Loss: 0.8025921982033808\n",
      "Epoch 22, Training Loss: 0.8031021662224505\n",
      "Epoch 23, Training Loss: 0.8005433832792411\n",
      "Epoch 24, Training Loss: 0.8022255250385829\n",
      "Epoch 25, Training Loss: 0.8029057749231955\n",
      "Epoch 26, Training Loss: 0.8007520287556755\n",
      "Epoch 27, Training Loss: 0.8012769279623391\n",
      "Epoch 28, Training Loss: 0.8010320997775946\n",
      "Epoch 29, Training Loss: 0.8010061110768999\n",
      "Epoch 30, Training Loss: 0.8017499048010748\n",
      "Epoch 31, Training Loss: 0.8012955121527937\n",
      "Epoch 32, Training Loss: 0.8005211982511936\n",
      "Epoch 33, Training Loss: 0.8004788253540384\n",
      "Epoch 34, Training Loss: 0.8002710039454295\n",
      "Epoch 35, Training Loss: 0.8003965620707748\n",
      "Epoch 36, Training Loss: 0.7993842978674667\n",
      "Epoch 37, Training Loss: 0.8010142693842264\n",
      "Epoch 38, Training Loss: 0.7993342718683687\n",
      "Epoch 39, Training Loss: 0.8000760041681447\n",
      "Epoch 40, Training Loss: 0.7998098645891462\n",
      "Epoch 41, Training Loss: 0.8000470519962167\n",
      "Epoch 42, Training Loss: 0.7986659282132199\n",
      "Epoch 43, Training Loss: 0.7985701299251471\n",
      "Epoch 44, Training Loss: 0.7999071166031343\n",
      "Epoch 45, Training Loss: 0.8004109284931555\n",
      "Epoch 46, Training Loss: 0.800024237847866\n",
      "Epoch 47, Training Loss: 0.79904926880858\n",
      "Epoch 48, Training Loss: 0.7992245507419558\n",
      "Epoch 49, Training Loss: 0.7985370306144083\n",
      "Epoch 50, Training Loss: 0.7989605701955638\n",
      "Epoch 51, Training Loss: 0.7989612086374956\n",
      "Epoch 52, Training Loss: 0.7982608530754434\n",
      "Epoch 53, Training Loss: 0.7976646615150279\n",
      "Epoch 54, Training Loss: 0.7983027783551611\n",
      "Epoch 55, Training Loss: 0.7984663283914551\n",
      "Epoch 56, Training Loss: 0.7975063252269774\n",
      "Epoch 57, Training Loss: 0.7980969038224758\n",
      "Epoch 58, Training Loss: 0.7974156380596017\n",
      "Epoch 59, Training Loss: 0.7979470734309433\n",
      "Epoch 60, Training Loss: 0.7974124759659731\n",
      "Epoch 61, Training Loss: 0.7973122839640854\n",
      "Epoch 62, Training Loss: 0.7974073481739016\n",
      "Epoch 63, Training Loss: 0.7965006731506578\n",
      "Epoch 64, Training Loss: 0.7962000461897456\n",
      "Epoch 65, Training Loss: 0.7969432232971478\n",
      "Epoch 66, Training Loss: 0.7959728714218713\n",
      "Epoch 67, Training Loss: 0.7968318258909355\n",
      "Epoch 68, Training Loss: 0.7969494715669101\n",
      "Epoch 69, Training Loss: 0.7961627485160541\n",
      "Epoch 70, Training Loss: 0.7960002873176919\n",
      "Epoch 71, Training Loss: 0.7958679122136052\n",
      "Epoch 72, Training Loss: 0.7959704539829627\n",
      "Epoch 73, Training Loss: 0.7952448415576964\n",
      "Epoch 74, Training Loss: 0.7949093263848384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:16:14,309] Trial 87 finished with value: 0.6333333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7955615986558727\n",
      "Epoch 1, Training Loss: 0.871953157046262\n",
      "Epoch 2, Training Loss: 0.812916183050941\n",
      "Epoch 3, Training Loss: 0.8067936674987568\n",
      "Epoch 4, Training Loss: 0.8025917712379904\n",
      "Epoch 5, Training Loss: 0.8000949522326974\n",
      "Epoch 6, Training Loss: 0.7975136191003462\n",
      "Epoch 7, Training Loss: 0.7935500459110035\n",
      "Epoch 8, Training Loss: 0.7914959453835206\n",
      "Epoch 9, Training Loss: 0.7903933973873363\n",
      "Epoch 10, Training Loss: 0.789850563582252\n",
      "Epoch 11, Training Loss: 0.7881100661614362\n",
      "Epoch 12, Training Loss: 0.7899713474862716\n",
      "Epoch 13, Training Loss: 0.7884612701219671\n",
      "Epoch 14, Training Loss: 0.7877171773069045\n",
      "Epoch 15, Training Loss: 0.7884224036160637\n",
      "Epoch 16, Training Loss: 0.7870456159816068\n",
      "Epoch 17, Training Loss: 0.7868110537528992\n",
      "Epoch 18, Training Loss: 0.7879475467345294\n",
      "Epoch 19, Training Loss: 0.7870258334804984\n",
      "Epoch 20, Training Loss: 0.7868116117224974\n",
      "Epoch 21, Training Loss: 0.7861442789610694\n",
      "Epoch 22, Training Loss: 0.786334400878233\n",
      "Epoch 23, Training Loss: 0.7860122710817\n",
      "Epoch 24, Training Loss: 0.785423256930183\n",
      "Epoch 25, Training Loss: 0.7855949705488542\n",
      "Epoch 26, Training Loss: 0.7853171525282018\n",
      "Epoch 27, Training Loss: 0.7857561990794013\n",
      "Epoch 28, Training Loss: 0.7848889516381656\n",
      "Epoch 29, Training Loss: 0.784977554433486\n",
      "Epoch 30, Training Loss: 0.7848274677641252\n",
      "Epoch 31, Training Loss: 0.7846265223447014\n",
      "Epoch 32, Training Loss: 0.7836248467249028\n",
      "Epoch 33, Training Loss: 0.7842181073216831\n",
      "Epoch 34, Training Loss: 0.7839208387627321\n",
      "Epoch 35, Training Loss: 0.7841012461045209\n",
      "Epoch 36, Training Loss: 0.7842929344317492\n",
      "Epoch 37, Training Loss: 0.7835076992651996\n",
      "Epoch 38, Training Loss: 0.7836615132584291\n",
      "Epoch 39, Training Loss: 0.7834828479850994\n",
      "Epoch 40, Training Loss: 0.7836571903088514\n",
      "Epoch 41, Training Loss: 0.7832233792192795\n",
      "Epoch 42, Training Loss: 0.7830967316907995\n",
      "Epoch 43, Training Loss: 0.7826338617941913\n",
      "Epoch 44, Training Loss: 0.7825931901090285\n",
      "Epoch 45, Training Loss: 0.7827892045413747\n",
      "Epoch 46, Training Loss: 0.7825621320219601\n",
      "Epoch 47, Training Loss: 0.7824608202541575\n",
      "Epoch 48, Training Loss: 0.7820886655414806\n",
      "Epoch 49, Training Loss: 0.7822671453391804\n",
      "Epoch 50, Training Loss: 0.7819601994402269\n",
      "Epoch 51, Training Loss: 0.7816669185021344\n",
      "Epoch 52, Training Loss: 0.781920473575592\n",
      "Epoch 53, Training Loss: 0.7819040799842162\n",
      "Epoch 54, Training Loss: 0.7815380157442654\n",
      "Epoch 55, Training Loss: 0.7815717671198004\n",
      "Epoch 56, Training Loss: 0.7810573799469892\n",
      "Epoch 57, Training Loss: 0.7814601981639862\n",
      "Epoch 58, Training Loss: 0.7810667744804831\n",
      "Epoch 59, Training Loss: 0.7815420025236466\n",
      "Epoch 60, Training Loss: 0.7814237043436836\n",
      "Epoch 61, Training Loss: 0.7809325757447411\n",
      "Epoch 62, Training Loss: 0.7814158811288722\n",
      "Epoch 63, Training Loss: 0.7808872276193956\n",
      "Epoch 64, Training Loss: 0.7813015378923978\n",
      "Epoch 65, Training Loss: 0.7808066492220934\n",
      "Epoch 66, Training Loss: 0.7808631666267619\n",
      "Epoch 67, Training Loss: 0.7804254050114575\n",
      "Epoch 68, Training Loss: 0.7802400585483102\n",
      "Epoch 69, Training Loss: 0.780590883212931\n",
      "Epoch 70, Training Loss: 0.7802927558562335\n",
      "Epoch 71, Training Loss: 0.7802748668193817\n",
      "Epoch 72, Training Loss: 0.780066817928763\n",
      "Epoch 73, Training Loss: 0.7800087163027595\n",
      "Epoch 74, Training Loss: 0.7802753317356109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:18:07,859] Trial 88 finished with value: 0.6386666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7798414785020492\n",
      "Epoch 1, Training Loss: 0.8496125266964274\n",
      "Epoch 2, Training Loss: 0.8178528644984826\n",
      "Epoch 3, Training Loss: 0.8134590774550474\n",
      "Epoch 4, Training Loss: 0.8132394241211109\n",
      "Epoch 5, Training Loss: 0.806808638931217\n",
      "Epoch 6, Training Loss: 0.8076851556175634\n",
      "Epoch 7, Training Loss: 0.8065123254195192\n",
      "Epoch 8, Training Loss: 0.811392220966798\n",
      "Epoch 9, Training Loss: 0.8069924860072315\n",
      "Epoch 10, Training Loss: 0.8097686128508775\n",
      "Epoch 11, Training Loss: 0.8064344899994986\n",
      "Epoch 12, Training Loss: 0.8075268771415366\n",
      "Epoch 13, Training Loss: 0.8053137105210383\n",
      "Epoch 14, Training Loss: 0.8046422663487887\n",
      "Epoch 15, Training Loss: 0.8057105003442979\n",
      "Epoch 16, Training Loss: 0.804903565313583\n",
      "Epoch 17, Training Loss: 0.8051400389886441\n",
      "Epoch 18, Training Loss: 0.8042261743007746\n",
      "Epoch 19, Training Loss: 0.8038119878984036\n",
      "Epoch 20, Training Loss: 0.8045278747278944\n",
      "Epoch 21, Training Loss: 0.8046010309592225\n",
      "Epoch 22, Training Loss: 0.8041249960885012\n",
      "Epoch 23, Training Loss: 0.803391917038681\n",
      "Epoch 24, Training Loss: 0.8041839803968157\n",
      "Epoch 25, Training Loss: 0.8054321682542787\n",
      "Epoch 26, Training Loss: 0.8045639466522332\n",
      "Epoch 27, Training Loss: 0.8066764818994623\n",
      "Epoch 28, Training Loss: 0.8034604209258144\n",
      "Epoch 29, Training Loss: 0.8058304747244469\n",
      "Epoch 30, Training Loss: 0.8067370650463535\n",
      "Epoch 31, Training Loss: 0.8028540456205382\n",
      "Epoch 32, Training Loss: 0.8058412777750116\n",
      "Epoch 33, Training Loss: 0.8069187708367083\n",
      "Epoch 34, Training Loss: 0.8006077529792499\n",
      "Epoch 35, Training Loss: 0.8058302600580947\n",
      "Epoch 36, Training Loss: 0.8033297847088118\n",
      "Epoch 37, Training Loss: 0.8059749845275306\n",
      "Epoch 38, Training Loss: 0.8051497668251956\n",
      "Epoch 39, Training Loss: 0.8070444753295497\n",
      "Epoch 40, Training Loss: 0.8040595441832579\n",
      "Epoch 41, Training Loss: 0.8052466428369508\n",
      "Epoch 42, Training Loss: 0.8035682379751277\n",
      "Epoch 43, Training Loss: 0.8046423778498083\n",
      "Epoch 44, Training Loss: 0.807095536074244\n",
      "Epoch 45, Training Loss: 0.8031578561417142\n",
      "Epoch 46, Training Loss: 0.8042968416572513\n",
      "Epoch 47, Training Loss: 0.8076492627760521\n",
      "Epoch 48, Training Loss: 0.8040532942105056\n",
      "Epoch 49, Training Loss: 0.8037749268058547\n",
      "Epoch 50, Training Loss: 0.8068252844021733\n",
      "Epoch 51, Training Loss: 0.8048390120491946\n",
      "Epoch 52, Training Loss: 0.8033985699926104\n",
      "Epoch 53, Training Loss: 0.8031645783804413\n",
      "Epoch 54, Training Loss: 0.8112675925842802\n",
      "Epoch 55, Training Loss: 0.8050308397838047\n",
      "Epoch 56, Training Loss: 0.8034085576695608\n",
      "Epoch 57, Training Loss: 0.8072862830377163\n",
      "Epoch 58, Training Loss: 0.802867977780507\n",
      "Epoch 59, Training Loss: 0.8070094205383072\n",
      "Epoch 60, Training Loss: 0.8080804973616635\n",
      "Epoch 61, Training Loss: 0.8080026226832454\n",
      "Epoch 62, Training Loss: 0.8075054194694175\n",
      "Epoch 63, Training Loss: 0.8045722732866617\n",
      "Epoch 64, Training Loss: 0.8054282900982334\n",
      "Epoch 65, Training Loss: 0.8063230063682212\n",
      "Epoch 66, Training Loss: 0.8042000998231701\n",
      "Epoch 67, Training Loss: 0.8073638392570324\n",
      "Epoch 68, Training Loss: 0.8038381216221286\n",
      "Epoch 69, Training Loss: 0.8055106772516006\n",
      "Epoch 70, Training Loss: 0.8090397728116889\n",
      "Epoch 71, Training Loss: 0.804368589874497\n",
      "Epoch 72, Training Loss: 0.8070331721377552\n",
      "Epoch 73, Training Loss: 0.8049456109677938\n",
      "Epoch 74, Training Loss: 0.8046618371081532\n",
      "Epoch 75, Training Loss: 0.8089838876760096\n",
      "Epoch 76, Training Loss: 0.8058315470702666\n",
      "Epoch 77, Training Loss: 0.8016691937482446\n",
      "Epoch 78, Training Loss: 0.8065073033024494\n",
      "Epoch 79, Training Loss: 0.8057390122933495\n",
      "Epoch 80, Training Loss: 0.8042808512996014\n",
      "Epoch 81, Training Loss: 0.8053364561912709\n",
      "Epoch 82, Training Loss: 0.8061587784523354\n",
      "Epoch 83, Training Loss: 0.8023933022542107\n",
      "Epoch 84, Training Loss: 0.8051984585317454\n",
      "Epoch 85, Training Loss: 0.8057651207859354\n",
      "Epoch 86, Training Loss: 0.8060879051237178\n",
      "Epoch 87, Training Loss: 0.804692048148105\n",
      "Epoch 88, Training Loss: 0.805601253724636\n",
      "Epoch 89, Training Loss: 0.8064300663489148\n",
      "Epoch 90, Training Loss: 0.803118894602123\n",
      "Epoch 91, Training Loss: 0.8052931538201813\n",
      "Epoch 92, Training Loss: 0.805364133451218\n",
      "Epoch 93, Training Loss: 0.8073327904357049\n",
      "Epoch 94, Training Loss: 0.8062569329613134\n",
      "Epoch 95, Training Loss: 0.8041674484883933\n",
      "Epoch 96, Training Loss: 0.805451852665808\n",
      "Epoch 97, Training Loss: 0.8049165420066146\n",
      "Epoch 98, Training Loss: 0.8046025269461754\n",
      "Epoch 99, Training Loss: 0.8059066392425308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:20:07,535] Trial 89 finished with value: 0.6143333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8032651357184676\n",
      "Epoch 1, Training Loss: 0.8482961962097569\n",
      "Epoch 2, Training Loss: 0.814524928833309\n",
      "Epoch 3, Training Loss: 0.8125535669183372\n",
      "Epoch 4, Training Loss: 0.8088299718118251\n",
      "Epoch 5, Training Loss: 0.8074836299831706\n",
      "Epoch 6, Training Loss: 0.8087464890981976\n",
      "Epoch 7, Training Loss: 0.8066863605850622\n",
      "Epoch 8, Training Loss: 0.8063577335579951\n",
      "Epoch 9, Training Loss: 0.8058887660055232\n",
      "Epoch 10, Training Loss: 0.8060755136317777\n",
      "Epoch 11, Training Loss: 0.8024127205511681\n",
      "Epoch 12, Training Loss: 0.803093984073266\n",
      "Epoch 13, Training Loss: 0.8019527042718758\n",
      "Epoch 14, Training Loss: 0.8015930624832784\n",
      "Epoch 15, Training Loss: 0.7995562539961105\n",
      "Epoch 16, Training Loss: 0.8007744714729768\n",
      "Epoch 17, Training Loss: 0.7999183552605765\n",
      "Epoch 18, Training Loss: 0.799000985371439\n",
      "Epoch 19, Training Loss: 0.7999999099208001\n",
      "Epoch 20, Training Loss: 0.8000558425609331\n",
      "Epoch 21, Training Loss: 0.7991998875947823\n",
      "Epoch 22, Training Loss: 0.7985372823880131\n",
      "Epoch 23, Training Loss: 0.7989285322060262\n",
      "Epoch 24, Training Loss: 0.7995867178852397\n",
      "Epoch 25, Training Loss: 0.7994574881137761\n",
      "Epoch 26, Training Loss: 0.798519724204128\n",
      "Epoch 27, Training Loss: 0.798458066470641\n",
      "Epoch 28, Training Loss: 0.7984737934922813\n",
      "Epoch 29, Training Loss: 0.7989766562791695\n",
      "Epoch 30, Training Loss: 0.7988616222725775\n",
      "Epoch 31, Training Loss: 0.8002073518315652\n",
      "Epoch 32, Training Loss: 0.7992914925840564\n",
      "Epoch 33, Training Loss: 0.7986430027431115\n",
      "Epoch 34, Training Loss: 0.798210704954047\n",
      "Epoch 35, Training Loss: 0.7975681906355951\n",
      "Epoch 36, Training Loss: 0.8004572956185592\n",
      "Epoch 37, Training Loss: 0.7967215506654036\n",
      "Epoch 38, Training Loss: 0.7981710070057919\n",
      "Epoch 39, Training Loss: 0.7968784110438555\n",
      "Epoch 40, Training Loss: 0.7987129406821459\n",
      "Epoch 41, Training Loss: 0.7977564530265062\n",
      "Epoch 42, Training Loss: 0.7984608912826481\n",
      "Epoch 43, Training Loss: 0.7986998235372672\n",
      "Epoch 44, Training Loss: 0.799163470053135\n",
      "Epoch 45, Training Loss: 0.7995980520893756\n",
      "Epoch 46, Training Loss: 0.7968291801617557\n",
      "Epoch 47, Training Loss: 0.797346189864596\n",
      "Epoch 48, Training Loss: 0.7967363504538859\n",
      "Epoch 49, Training Loss: 0.7974771317682768\n",
      "Epoch 50, Training Loss: 0.7968809341129504\n",
      "Epoch 51, Training Loss: 0.7961377092770168\n",
      "Epoch 52, Training Loss: 0.7967507629466236\n",
      "Epoch 53, Training Loss: 0.7960285452075471\n",
      "Epoch 54, Training Loss: 0.7966485831970559\n",
      "Epoch 55, Training Loss: 0.795878277326885\n",
      "Epoch 56, Training Loss: 0.7950139449951344\n",
      "Epoch 57, Training Loss: 0.7960267472984199\n",
      "Epoch 58, Training Loss: 0.7958820098324826\n",
      "Epoch 59, Training Loss: 0.7976504162738197\n",
      "Epoch 60, Training Loss: 0.7958241452848105\n",
      "Epoch 61, Training Loss: 0.7952141513501791\n",
      "Epoch 62, Training Loss: 0.7965395757130214\n",
      "Epoch 63, Training Loss: 0.7962132019656045\n",
      "Epoch 64, Training Loss: 0.7965839111715332\n",
      "Epoch 65, Training Loss: 0.7965931659354303\n",
      "Epoch 66, Training Loss: 0.797849792824652\n",
      "Epoch 67, Training Loss: 0.7982693915080307\n",
      "Epoch 68, Training Loss: 0.7955949553869721\n",
      "Epoch 69, Training Loss: 0.7964265041781547\n",
      "Epoch 70, Training Loss: 0.7975246882080136\n",
      "Epoch 71, Training Loss: 0.7978918693119422\n",
      "Epoch 72, Training Loss: 0.7966776941951953\n",
      "Epoch 73, Training Loss: 0.7958009625736036\n",
      "Epoch 74, Training Loss: 0.7962942353764871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:21:49,583] Trial 90 finished with value: 0.6325333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.796620808328901\n",
      "Epoch 1, Training Loss: 0.8487308419451994\n",
      "Epoch 2, Training Loss: 0.8227440717641045\n",
      "Epoch 3, Training Loss: 0.8191272432663862\n",
      "Epoch 4, Training Loss: 0.8122186805220212\n",
      "Epoch 5, Training Loss: 0.8187253485707676\n",
      "Epoch 6, Training Loss: 0.8116093211314257\n",
      "Epoch 7, Training Loss: 0.8142752139007344\n",
      "Epoch 8, Training Loss: 0.8121138586717493\n",
      "Epoch 9, Training Loss: 0.8107349507247701\n",
      "Epoch 10, Training Loss: 0.8123165225281435\n",
      "Epoch 11, Training Loss: 0.8122966271989486\n",
      "Epoch 12, Training Loss: 0.8131031818950878\n",
      "Epoch 13, Training Loss: 0.8125408760940327\n",
      "Epoch 14, Training Loss: 0.8117572945005753\n",
      "Epoch 15, Training Loss: 0.8094546264760635\n",
      "Epoch 16, Training Loss: 0.8133805145936853\n",
      "Epoch 17, Training Loss: 0.8142759262113011\n",
      "Epoch 18, Training Loss: 0.8119625726868125\n",
      "Epoch 19, Training Loss: 0.812402844358893\n",
      "Epoch 20, Training Loss: 0.8127850967295029\n",
      "Epoch 21, Training Loss: 0.8130509680158952\n",
      "Epoch 22, Training Loss: 0.8104696960308972\n",
      "Epoch 23, Training Loss: 0.8080612911897547\n",
      "Epoch 24, Training Loss: 0.8135265142076156\n",
      "Epoch 25, Training Loss: 0.8106735247022966\n",
      "Epoch 26, Training Loss: 0.8098924810044905\n",
      "Epoch 27, Training Loss: 0.8134650492668152\n",
      "Epoch 28, Training Loss: 0.813140482341542\n",
      "Epoch 29, Training Loss: 0.809791530230466\n",
      "Epoch 30, Training Loss: 0.8084473957734949\n",
      "Epoch 31, Training Loss: 0.8129376872146831\n",
      "Epoch 32, Training Loss: 0.8155876163875355\n",
      "Epoch 33, Training Loss: 0.8144591691914727\n",
      "Epoch 34, Training Loss: 0.8134875113122604\n",
      "Epoch 35, Training Loss: 0.8110418734129737\n",
      "Epoch 36, Training Loss: 0.8131391301575829\n",
      "Epoch 37, Training Loss: 0.8130925212186926\n",
      "Epoch 38, Training Loss: 0.8149825753183926\n",
      "Epoch 39, Training Loss: 0.813038512468338\n",
      "Epoch 40, Training Loss: 0.815306039277245\n",
      "Epoch 41, Training Loss: 0.814065332552966\n",
      "Epoch 42, Training Loss: 0.8138425972181208\n",
      "Epoch 43, Training Loss: 0.815990900853101\n",
      "Epoch 44, Training Loss: 0.8178028907495386\n",
      "Epoch 45, Training Loss: 0.8236406430777381\n",
      "Epoch 46, Training Loss: 0.8219334849189309\n",
      "Epoch 47, Training Loss: 0.8194571044865776\n",
      "Epoch 48, Training Loss: 0.8223437484572915\n",
      "Epoch 49, Training Loss: 0.8261337532015408\n",
      "Epoch 50, Training Loss: 0.829818651676178\n",
      "Epoch 51, Training Loss: 0.8314101989830242\n",
      "Epoch 52, Training Loss: 0.8314268688594594\n",
      "Epoch 53, Training Loss: 0.8297274946465212\n",
      "Epoch 54, Training Loss: 0.8305623365149778\n",
      "Epoch 55, Training Loss: 0.8318884320820079\n",
      "Epoch 56, Training Loss: 0.8320135445454542\n",
      "Epoch 57, Training Loss: 0.8336840457074782\n",
      "Epoch 58, Training Loss: 0.8304237930213704\n",
      "Epoch 59, Training Loss: 0.8329152997802286\n",
      "Epoch 60, Training Loss: 0.8338247816001668\n",
      "Epoch 61, Training Loss: 0.8301373359035044\n",
      "Epoch 62, Training Loss: 0.8286929918036742\n",
      "Epoch 63, Training Loss: 0.8311327486178455\n",
      "Epoch 64, Training Loss: 0.8316606134526869\n",
      "Epoch 65, Training Loss: 0.8299482133108027\n",
      "Epoch 66, Training Loss: 0.8321871564668768\n",
      "Epoch 67, Training Loss: 0.8310968384321998\n",
      "Epoch 68, Training Loss: 0.8304987828871783\n",
      "Epoch 69, Training Loss: 0.8299400760145749\n",
      "Epoch 70, Training Loss: 0.8304886474328883\n",
      "Epoch 71, Training Loss: 0.8316625954123105\n",
      "Epoch 72, Training Loss: 0.82951836123186\n",
      "Epoch 73, Training Loss: 0.8303386360757491\n",
      "Epoch 74, Training Loss: 0.8294745911570156\n",
      "Epoch 75, Training Loss: 0.8307308092538048\n",
      "Epoch 76, Training Loss: 0.8277948294667636\n",
      "Epoch 77, Training Loss: 0.8301397902124068\n",
      "Epoch 78, Training Loss: 0.8319015224540934\n",
      "Epoch 79, Training Loss: 0.8320973539352416\n",
      "Epoch 80, Training Loss: 0.8324170121725868\n",
      "Epoch 81, Training Loss: 0.8311067290867076\n",
      "Epoch 82, Training Loss: 0.8334724835087272\n",
      "Epoch 83, Training Loss: 0.8286585835849538\n",
      "Epoch 84, Training Loss: 0.8301887360039879\n",
      "Epoch 85, Training Loss: 0.8292778602067162\n",
      "Epoch 86, Training Loss: 0.8267718579488642\n",
      "Epoch 87, Training Loss: 0.8308767696689157\n",
      "Epoch 88, Training Loss: 0.8292998602109797\n",
      "Epoch 89, Training Loss: 0.8325256004053003\n",
      "Epoch 90, Training Loss: 0.8316254673284643\n",
      "Epoch 91, Training Loss: 0.8308269655003268\n",
      "Epoch 92, Training Loss: 0.8275677577888264\n",
      "Epoch 93, Training Loss: 0.8325412947991315\n",
      "Epoch 94, Training Loss: 0.8290228882957907\n",
      "Epoch 95, Training Loss: 0.8313251600545996\n",
      "Epoch 96, Training Loss: 0.8319574932491078\n",
      "Epoch 97, Training Loss: 0.8251684512110318\n",
      "Epoch 98, Training Loss: 0.8246491064043606\n",
      "Epoch 99, Training Loss: 0.8314372594917522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:24:09,972] Trial 91 finished with value: 0.6280666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.822698398477891\n",
      "Epoch 1, Training Loss: 0.8480146189998178\n",
      "Epoch 2, Training Loss: 0.8206290617409875\n",
      "Epoch 3, Training Loss: 0.8171875876538893\n",
      "Epoch 4, Training Loss: 0.8171981459505417\n",
      "Epoch 5, Training Loss: 0.8122292443583994\n",
      "Epoch 6, Training Loss: 0.8134571704443764\n",
      "Epoch 7, Training Loss: 0.8119103883294498\n",
      "Epoch 8, Training Loss: 0.8130531482135548\n",
      "Epoch 9, Training Loss: 0.8108032457968768\n",
      "Epoch 10, Training Loss: 0.8114001555302564\n",
      "Epoch 11, Training Loss: 0.8106849351349998\n",
      "Epoch 12, Training Loss: 0.8121604908213896\n",
      "Epoch 13, Training Loss: 0.8096825558297774\n",
      "Epoch 14, Training Loss: 0.8122705880333395\n",
      "Epoch 15, Training Loss: 0.8089624523415285\n",
      "Epoch 16, Training Loss: 0.8094260358810424\n",
      "Epoch 17, Training Loss: 0.8097189887832192\n",
      "Epoch 18, Training Loss: 0.809214159600875\n",
      "Epoch 19, Training Loss: 0.8085518101383659\n",
      "Epoch 20, Training Loss: 0.8101097112543443\n",
      "Epoch 21, Training Loss: 0.8089992352794199\n",
      "Epoch 22, Training Loss: 0.8102661822122686\n",
      "Epoch 23, Training Loss: 0.8102090814534355\n",
      "Epoch 24, Training Loss: 0.8066554486050326\n",
      "Epoch 25, Training Loss: 0.808725047602373\n",
      "Epoch 26, Training Loss: 0.80692005907788\n",
      "Epoch 27, Training Loss: 0.8071226547746098\n",
      "Epoch 28, Training Loss: 0.8063356996283811\n",
      "Epoch 29, Training Loss: 0.8061611803138957\n",
      "Epoch 30, Training Loss: 0.8055246806845946\n",
      "Epoch 31, Training Loss: 0.8044697281893561\n",
      "Epoch 32, Training Loss: 0.8042173128268298\n",
      "Epoch 33, Training Loss: 0.8047442367497613\n",
      "Epoch 34, Training Loss: 0.8050881502908819\n",
      "Epoch 35, Training Loss: 0.8035112883764155\n",
      "Epoch 36, Training Loss: 0.8026905434271868\n",
      "Epoch 37, Training Loss: 0.8032499412228079\n",
      "Epoch 38, Training Loss: 0.8026842237220091\n",
      "Epoch 39, Training Loss: 0.8015432294677286\n",
      "Epoch 40, Training Loss: 0.801985429875991\n",
      "Epoch 41, Training Loss: 0.8033700819576488\n",
      "Epoch 42, Training Loss: 0.8012195810850928\n",
      "Epoch 43, Training Loss: 0.8021490019910475\n",
      "Epoch 44, Training Loss: 0.8013549801882576\n",
      "Epoch 45, Training Loss: 0.8015261532278621\n",
      "Epoch 46, Training Loss: 0.800603047469083\n",
      "Epoch 47, Training Loss: 0.801455953541924\n",
      "Epoch 48, Training Loss: 0.8012891149520874\n",
      "Epoch 49, Training Loss: 0.8002351438297944\n",
      "Epoch 50, Training Loss: 0.8009951883203843\n",
      "Epoch 51, Training Loss: 0.8011643861321842\n",
      "Epoch 52, Training Loss: 0.8009908239981708\n",
      "Epoch 53, Training Loss: 0.8003344744093278\n",
      "Epoch 54, Training Loss: 0.8009470730669358\n",
      "Epoch 55, Training Loss: 0.8001546568029068\n",
      "Epoch 56, Training Loss: 0.7988627419752233\n",
      "Epoch 57, Training Loss: 0.7994590522261227\n",
      "Epoch 58, Training Loss: 0.7997299594037673\n",
      "Epoch 59, Training Loss: 0.7993158753479228\n",
      "Epoch 60, Training Loss: 0.800782977623098\n",
      "Epoch 61, Training Loss: 0.7988228170310749\n",
      "Epoch 62, Training Loss: 0.7981210933012121\n",
      "Epoch 63, Training Loss: 0.7990507207197302\n",
      "Epoch 64, Training Loss: 0.7991742621449863\n",
      "Epoch 65, Training Loss: 0.7995257738057305\n",
      "Epoch 66, Training Loss: 0.8002311863618738\n",
      "Epoch 67, Training Loss: 0.7988754120293785\n",
      "Epoch 68, Training Loss: 0.7985967660651487\n",
      "Epoch 69, Training Loss: 0.8000994014038759\n",
      "Epoch 70, Training Loss: 0.7983318927007563\n",
      "Epoch 71, Training Loss: 0.8008426060396082\n",
      "Epoch 72, Training Loss: 0.7983220458030701\n",
      "Epoch 73, Training Loss: 0.7985897225492141\n",
      "Epoch 74, Training Loss: 0.7983958213469562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:25:38,887] Trial 92 finished with value: 0.633 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7986613561125363\n",
      "Epoch 1, Training Loss: 0.9805706723292071\n",
      "Epoch 2, Training Loss: 0.9470673893627367\n",
      "Epoch 3, Training Loss: 0.9330025211312717\n",
      "Epoch 4, Training Loss: 0.9146739663038038\n",
      "Epoch 5, Training Loss: 0.8937053018942811\n",
      "Epoch 6, Training Loss: 0.8733359785008251\n",
      "Epoch 7, Training Loss: 0.8552398578564924\n",
      "Epoch 8, Training Loss: 0.8408287395211987\n",
      "Epoch 9, Training Loss: 0.83082511398129\n",
      "Epoch 10, Training Loss: 0.823914138804701\n",
      "Epoch 11, Training Loss: 0.8195078418667155\n",
      "Epoch 12, Training Loss: 0.8162994651866139\n",
      "Epoch 13, Training Loss: 0.813799160136316\n",
      "Epoch 14, Training Loss: 0.8121928911460073\n",
      "Epoch 15, Training Loss: 0.8119942194537113\n",
      "Epoch 16, Training Loss: 0.8106176590560971\n",
      "Epoch 17, Training Loss: 0.809932135460072\n",
      "Epoch 18, Training Loss: 0.8098354996595167\n",
      "Epoch 19, Training Loss: 0.8096299417933127\n",
      "Epoch 20, Training Loss: 0.8086900003870627\n",
      "Epoch 21, Training Loss: 0.8085917689746484\n",
      "Epoch 22, Training Loss: 0.8082754766134391\n",
      "Epoch 23, Training Loss: 0.808091810323242\n",
      "Epoch 24, Training Loss: 0.8079370437708117\n",
      "Epoch 25, Training Loss: 0.8077568565992485\n",
      "Epoch 26, Training Loss: 0.8074967765270319\n",
      "Epoch 27, Training Loss: 0.8073390531360655\n",
      "Epoch 28, Training Loss: 0.8074655772151803\n",
      "Epoch 29, Training Loss: 0.8070807604861439\n",
      "Epoch 30, Training Loss: 0.8065170446732887\n",
      "Epoch 31, Training Loss: 0.8067312956752634\n",
      "Epoch 32, Training Loss: 0.806441286721624\n",
      "Epoch 33, Training Loss: 0.8059195911973939\n",
      "Epoch 34, Training Loss: 0.805722357097425\n",
      "Epoch 35, Training Loss: 0.8057887819476593\n",
      "Epoch 36, Training Loss: 0.8059989848531278\n",
      "Epoch 37, Training Loss: 0.8052121730675375\n",
      "Epoch 38, Training Loss: 0.8049282591145738\n",
      "Epoch 39, Training Loss: 0.8054298477065295\n",
      "Epoch 40, Training Loss: 0.8054633466820967\n",
      "Epoch 41, Training Loss: 0.804909410512537\n",
      "Epoch 42, Training Loss: 0.8048915521542829\n",
      "Epoch 43, Training Loss: 0.8045098021514434\n",
      "Epoch 44, Training Loss: 0.8048867810041385\n",
      "Epoch 45, Training Loss: 0.804193585707729\n",
      "Epoch 46, Training Loss: 0.8041553875557462\n",
      "Epoch 47, Training Loss: 0.8042096254520846\n",
      "Epoch 48, Training Loss: 0.804444191957775\n",
      "Epoch 49, Training Loss: 0.8038645472741665\n",
      "Epoch 50, Training Loss: 0.8046476970937916\n",
      "Epoch 51, Training Loss: 0.8037894533989125\n",
      "Epoch 52, Training Loss: 0.8036579297897511\n",
      "Epoch 53, Training Loss: 0.8031453530591234\n",
      "Epoch 54, Training Loss: 0.8034003012162402\n",
      "Epoch 55, Training Loss: 0.8029738476401881\n",
      "Epoch 56, Training Loss: 0.8032073366910891\n",
      "Epoch 57, Training Loss: 0.8027266977424908\n",
      "Epoch 58, Training Loss: 0.8024580764591246\n",
      "Epoch 59, Training Loss: 0.8021999564385952\n",
      "Epoch 60, Training Loss: 0.802699619576447\n",
      "Epoch 61, Training Loss: 0.8026236396086843\n",
      "Epoch 62, Training Loss: 0.802231654816104\n",
      "Epoch 63, Training Loss: 0.8029795060480448\n",
      "Epoch 64, Training Loss: 0.8021702276136642\n",
      "Epoch 65, Training Loss: 0.8015256340342357\n",
      "Epoch 66, Training Loss: 0.8014595993479392\n",
      "Epoch 67, Training Loss: 0.80225069379448\n",
      "Epoch 68, Training Loss: 0.8021787566349918\n",
      "Epoch 69, Training Loss: 0.8013326950539323\n",
      "Epoch 70, Training Loss: 0.8013282598409438\n",
      "Epoch 71, Training Loss: 0.8011434331872409\n",
      "Epoch 72, Training Loss: 0.8012948841080629\n",
      "Epoch 73, Training Loss: 0.8011727005915534\n",
      "Epoch 74, Training Loss: 0.8011162100877977\n",
      "Epoch 75, Training Loss: 0.8010463505759275\n",
      "Epoch 76, Training Loss: 0.8013132096233224\n",
      "Epoch 77, Training Loss: 0.8007167278375841\n",
      "Epoch 78, Training Loss: 0.8003600309665938\n",
      "Epoch 79, Training Loss: 0.8004096594968236\n",
      "Epoch 80, Training Loss: 0.8005936627997492\n",
      "Epoch 81, Training Loss: 0.8003090604803615\n",
      "Epoch 82, Training Loss: 0.7998998237283607\n",
      "Epoch 83, Training Loss: 0.7998480078869297\n",
      "Epoch 84, Training Loss: 0.7996445552298898\n",
      "Epoch 85, Training Loss: 0.800809061168728\n",
      "Epoch 86, Training Loss: 0.800306811099662\n",
      "Epoch 87, Training Loss: 0.7995123599704943\n",
      "Epoch 88, Training Loss: 0.8000308292252677\n",
      "Epoch 89, Training Loss: 0.8004926606228477\n",
      "Epoch 90, Training Loss: 0.7995323848007316\n",
      "Epoch 91, Training Loss: 0.7993060446323309\n",
      "Epoch 92, Training Loss: 0.7995654619726024\n",
      "Epoch 93, Training Loss: 0.7998771987463299\n",
      "Epoch 94, Training Loss: 0.7998162625427533\n",
      "Epoch 95, Training Loss: 0.7992648961848783\n",
      "Epoch 96, Training Loss: 0.7990510398283936\n",
      "Epoch 97, Training Loss: 0.7990810242810644\n",
      "Epoch 98, Training Loss: 0.7990753182791229\n",
      "Epoch 99, Training Loss: 0.7989932051278594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:27:17,239] Trial 93 finished with value: 0.6344666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7988590547016688\n",
      "Epoch 1, Training Loss: 1.0064777087448236\n",
      "Epoch 2, Training Loss: 0.9464196320763207\n",
      "Epoch 3, Training Loss: 0.9255302124453667\n",
      "Epoch 4, Training Loss: 0.894423204346707\n",
      "Epoch 5, Training Loss: 0.8541961009341075\n",
      "Epoch 6, Training Loss: 0.8272247740200588\n",
      "Epoch 7, Training Loss: 0.8160078876896908\n",
      "Epoch 8, Training Loss: 0.8122755284148051\n",
      "Epoch 9, Training Loss: 0.8106654994021681\n",
      "Epoch 10, Training Loss: 0.8105938749205798\n",
      "Epoch 11, Training Loss: 0.8104517507373838\n",
      "Epoch 12, Training Loss: 0.8091770565599428\n",
      "Epoch 13, Training Loss: 0.8083478671267517\n",
      "Epoch 14, Training Loss: 0.8072820096087635\n",
      "Epoch 15, Training Loss: 0.8068331901292155\n",
      "Epoch 16, Training Loss: 0.8065762335196474\n",
      "Epoch 17, Training Loss: 0.8051014547061203\n",
      "Epoch 18, Training Loss: 0.804802776010413\n",
      "Epoch 19, Training Loss: 0.8037931464220348\n",
      "Epoch 20, Training Loss: 0.803485139420158\n",
      "Epoch 21, Training Loss: 0.8035911169267238\n",
      "Epoch 22, Training Loss: 0.8025516589781395\n",
      "Epoch 23, Training Loss: 0.8025025657245091\n",
      "Epoch 24, Training Loss: 0.8025317123061733\n",
      "Epoch 25, Training Loss: 0.8021447436253827\n",
      "Epoch 26, Training Loss: 0.8017455307164587\n",
      "Epoch 27, Training Loss: 0.8012193738069749\n",
      "Epoch 28, Training Loss: 0.8012140036525582\n",
      "Epoch 29, Training Loss: 0.8015081079382645\n",
      "Epoch 30, Training Loss: 0.8007919494370769\n",
      "Epoch 31, Training Loss: 0.8009881537659724\n",
      "Epoch 32, Training Loss: 0.8002345921401691\n",
      "Epoch 33, Training Loss: 0.8000251778086326\n",
      "Epoch 34, Training Loss: 0.8002748433808635\n",
      "Epoch 35, Training Loss: 0.8006884192165575\n",
      "Epoch 36, Training Loss: 0.7995413825924235\n",
      "Epoch 37, Training Loss: 0.8002652144073543\n",
      "Epoch 38, Training Loss: 0.7994154269533946\n",
      "Epoch 39, Training Loss: 0.799610263871071\n",
      "Epoch 40, Training Loss: 0.7996906212397984\n",
      "Epoch 41, Training Loss: 0.7992800542286465\n",
      "Epoch 42, Training Loss: 0.7994932279550939\n",
      "Epoch 43, Training Loss: 0.7983803909524043\n",
      "Epoch 44, Training Loss: 0.7982184190499155\n",
      "Epoch 45, Training Loss: 0.7985467321890637\n",
      "Epoch 46, Training Loss: 0.79819660823148\n",
      "Epoch 47, Training Loss: 0.7977227205620673\n",
      "Epoch 48, Training Loss: 0.7980139495734881\n",
      "Epoch 49, Training Loss: 0.798170945160371\n",
      "Epoch 50, Training Loss: 0.7981686611820881\n",
      "Epoch 51, Training Loss: 0.797482384774918\n",
      "Epoch 52, Training Loss: 0.7976721642608929\n",
      "Epoch 53, Training Loss: 0.7977431731116503\n",
      "Epoch 54, Training Loss: 0.7976261295770344\n",
      "Epoch 55, Training Loss: 0.7977548068627379\n",
      "Epoch 56, Training Loss: 0.7973762932576631\n",
      "Epoch 57, Training Loss: 0.7977630576693026\n",
      "Epoch 58, Training Loss: 0.7966048603219197\n",
      "Epoch 59, Training Loss: 0.796825136994957\n",
      "Epoch 60, Training Loss: 0.7967774064021003\n",
      "Epoch 61, Training Loss: 0.7970967925580821\n",
      "Epoch 62, Training Loss: 0.7975763328989646\n",
      "Epoch 63, Training Loss: 0.7981341881859572\n",
      "Epoch 64, Training Loss: 0.7967567919788504\n",
      "Epoch 65, Training Loss: 0.7966848096453157\n",
      "Epoch 66, Training Loss: 0.7959585680997461\n",
      "Epoch 67, Training Loss: 0.7958438066611613\n",
      "Epoch 68, Training Loss: 0.7969462018263967\n",
      "Epoch 69, Training Loss: 0.7963393463227982\n",
      "Epoch 70, Training Loss: 0.7960833320940347\n",
      "Epoch 71, Training Loss: 0.7957018747365564\n",
      "Epoch 72, Training Loss: 0.7962764525772037\n",
      "Epoch 73, Training Loss: 0.7957585850156339\n",
      "Epoch 74, Training Loss: 0.796486034429163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:28:44,174] Trial 94 finished with value: 0.6357333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7958769039103859\n",
      "Epoch 1, Training Loss: 0.8864087591493937\n",
      "Epoch 2, Training Loss: 0.8252429548062776\n",
      "Epoch 3, Training Loss: 0.8202992426721674\n",
      "Epoch 4, Training Loss: 0.8157485102352343\n",
      "Epoch 5, Training Loss: 0.8143338340565675\n",
      "Epoch 6, Training Loss: 0.812774968685064\n",
      "Epoch 7, Training Loss: 0.8109146517918522\n",
      "Epoch 8, Training Loss: 0.8103610844540416\n",
      "Epoch 9, Training Loss: 0.8082477081090884\n",
      "Epoch 10, Training Loss: 0.8082963094675452\n",
      "Epoch 11, Training Loss: 0.8069222258894067\n",
      "Epoch 12, Training Loss: 0.807545172451134\n",
      "Epoch 13, Training Loss: 0.8064714644188271\n",
      "Epoch 14, Training Loss: 0.8057618233494293\n",
      "Epoch 15, Training Loss: 0.8055229051668841\n",
      "Epoch 16, Training Loss: 0.8050587332338319\n",
      "Epoch 17, Training Loss: 0.8048477821780327\n",
      "Epoch 18, Training Loss: 0.8051707834229433\n",
      "Epoch 19, Training Loss: 0.8040250851695699\n",
      "Epoch 20, Training Loss: 0.8033108168078544\n",
      "Epoch 21, Training Loss: 0.8036173436874734\n",
      "Epoch 22, Training Loss: 0.8032777342581211\n",
      "Epoch 23, Training Loss: 0.8030524989716092\n",
      "Epoch 24, Training Loss: 0.8025871366486513\n",
      "Epoch 25, Training Loss: 0.8017615935856238\n",
      "Epoch 26, Training Loss: 0.8019172068825342\n",
      "Epoch 27, Training Loss: 0.8018048847528328\n",
      "Epoch 28, Training Loss: 0.8024031369309677\n",
      "Epoch 29, Training Loss: 0.8010909606639603\n",
      "Epoch 30, Training Loss: 0.8015165587116901\n",
      "Epoch 31, Training Loss: 0.7999915954761936\n",
      "Epoch 32, Training Loss: 0.8005199925343793\n",
      "Epoch 33, Training Loss: 0.8001420314150646\n",
      "Epoch 34, Training Loss: 0.7998640536365652\n",
      "Epoch 35, Training Loss: 0.7996105382316991\n",
      "Epoch 36, Training Loss: 0.7993756381192602\n",
      "Epoch 37, Training Loss: 0.7992622143343875\n",
      "Epoch 38, Training Loss: 0.799475850288133\n",
      "Epoch 39, Training Loss: 0.7979512534643475\n",
      "Epoch 40, Training Loss: 0.7983308131533458\n",
      "Epoch 41, Training Loss: 0.7970283760612172\n",
      "Epoch 42, Training Loss: 0.7974196200980279\n",
      "Epoch 43, Training Loss: 0.7972997094455518\n",
      "Epoch 44, Training Loss: 0.796620572061467\n",
      "Epoch 45, Training Loss: 0.7963428880935325\n",
      "Epoch 46, Training Loss: 0.7974120580164114\n",
      "Epoch 47, Training Loss: 0.796815089802993\n",
      "Epoch 48, Training Loss: 0.796775065508104\n",
      "Epoch 49, Training Loss: 0.7961401041289021\n",
      "Epoch 50, Training Loss: 0.7964507777888076\n",
      "Epoch 51, Training Loss: 0.796836477025111\n",
      "Epoch 52, Training Loss: 0.7960321913088174\n",
      "Epoch 53, Training Loss: 0.7961410495571624\n",
      "Epoch 54, Training Loss: 0.7957665230994834\n",
      "Epoch 55, Training Loss: 0.7951320586347939\n",
      "Epoch 56, Training Loss: 0.7949995059716074\n",
      "Epoch 57, Training Loss: 0.7947280105791594\n",
      "Epoch 58, Training Loss: 0.7943670592809978\n",
      "Epoch 59, Training Loss: 0.7940168072854666\n",
      "Epoch 60, Training Loss: 0.7949834360215897\n",
      "Epoch 61, Training Loss: 0.7943425131023378\n",
      "Epoch 62, Training Loss: 0.7947889939286655\n",
      "Epoch 63, Training Loss: 0.7943968737035766\n",
      "Epoch 64, Training Loss: 0.7937138439121103\n",
      "Epoch 65, Training Loss: 0.79356858501757\n",
      "Epoch 66, Training Loss: 0.7935976754453845\n",
      "Epoch 67, Training Loss: 0.7936306127031943\n",
      "Epoch 68, Training Loss: 0.7939182535149998\n",
      "Epoch 69, Training Loss: 0.7936733457378875\n",
      "Epoch 70, Training Loss: 0.7937436397810628\n",
      "Epoch 71, Training Loss: 0.7938652216043688\n",
      "Epoch 72, Training Loss: 0.7928003152510277\n",
      "Epoch 73, Training Loss: 0.7929098248481751\n",
      "Epoch 74, Training Loss: 0.7933245440174762\n",
      "Epoch 75, Training Loss: 0.793484287781823\n",
      "Epoch 76, Training Loss: 0.7934820477227519\n",
      "Epoch 77, Training Loss: 0.7926159244730957\n",
      "Epoch 78, Training Loss: 0.7939466340201241\n",
      "Epoch 79, Training Loss: 0.7923094701498075\n",
      "Epoch 80, Training Loss: 0.7926256054326107\n",
      "Epoch 81, Training Loss: 0.792804115248802\n",
      "Epoch 82, Training Loss: 0.7921107341472368\n",
      "Epoch 83, Training Loss: 0.7926482853136565\n",
      "Epoch 84, Training Loss: 0.7928135680076771\n",
      "Epoch 85, Training Loss: 0.7918733928436623\n",
      "Epoch 86, Training Loss: 0.7921227058969942\n",
      "Epoch 87, Training Loss: 0.7925566468023716\n",
      "Epoch 88, Training Loss: 0.7922770090569231\n",
      "Epoch 89, Training Loss: 0.7929644804251822\n",
      "Epoch 90, Training Loss: 0.7923939810659653\n",
      "Epoch 91, Training Loss: 0.7915709571730821\n",
      "Epoch 92, Training Loss: 0.7922675610484934\n",
      "Epoch 93, Training Loss: 0.7921019961959437\n",
      "Epoch 94, Training Loss: 0.7921522761197914\n",
      "Epoch 95, Training Loss: 0.7921610625166642\n",
      "Epoch 96, Training Loss: 0.7910140666746556\n",
      "Epoch 97, Training Loss: 0.7922773721522854\n",
      "Epoch 98, Training Loss: 0.7919252576684593\n",
      "Epoch 99, Training Loss: 0.7919177971388164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:30:19,907] Trial 95 finished with value: 0.6198 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.791598493622658\n",
      "Epoch 1, Training Loss: 0.8642249413798837\n",
      "Epoch 2, Training Loss: 0.8226365566954893\n",
      "Epoch 3, Training Loss: 0.816446018078748\n",
      "Epoch 4, Training Loss: 0.8107783713060267\n",
      "Epoch 5, Training Loss: 0.810617015431909\n",
      "Epoch 6, Training Loss: 0.8073383062727311\n",
      "Epoch 7, Training Loss: 0.8061927993157331\n",
      "Epoch 8, Training Loss: 0.8041356788663303\n",
      "Epoch 9, Training Loss: 0.804503061841516\n",
      "Epoch 10, Training Loss: 0.8038056188471177\n",
      "Epoch 11, Training Loss: 0.8029276234262129\n",
      "Epoch 12, Training Loss: 0.8021606744738186\n",
      "Epoch 13, Training Loss: 0.8020942338073955\n",
      "Epoch 14, Training Loss: 0.8004207698737874\n",
      "Epoch 15, Training Loss: 0.8005197473834542\n",
      "Epoch 16, Training Loss: 0.8003615625465618\n",
      "Epoch 17, Training Loss: 0.799850036536946\n",
      "Epoch 18, Training Loss: 0.799988131312763\n",
      "Epoch 19, Training Loss: 0.7998182339528027\n",
      "Epoch 20, Training Loss: 0.7990574808681712\n",
      "Epoch 21, Training Loss: 0.7992606942092672\n",
      "Epoch 22, Training Loss: 0.7984347777506885\n",
      "Epoch 23, Training Loss: 0.7979141778805676\n",
      "Epoch 24, Training Loss: 0.7981692239817451\n",
      "Epoch 25, Training Loss: 0.7976636768789852\n",
      "Epoch 26, Training Loss: 0.7979803914182326\n",
      "Epoch 27, Training Loss: 0.7974461523925557\n",
      "Epoch 28, Training Loss: 0.7974113808659946\n",
      "Epoch 29, Training Loss: 0.7969912462374743\n",
      "Epoch 30, Training Loss: 0.7970371143256917\n",
      "Epoch 31, Training Loss: 0.7972863495349884\n",
      "Epoch 32, Training Loss: 0.7971247865171993\n",
      "Epoch 33, Training Loss: 0.7961547693785499\n",
      "Epoch 34, Training Loss: 0.7968641649274265\n",
      "Epoch 35, Training Loss: 0.7962262233565836\n",
      "Epoch 36, Training Loss: 0.7967905618863947\n",
      "Epoch 37, Training Loss: 0.7962841632786919\n",
      "Epoch 38, Training Loss: 0.7967011836697073\n",
      "Epoch 39, Training Loss: 0.796558112186544\n",
      "Epoch 40, Training Loss: 0.7965502840631148\n",
      "Epoch 41, Training Loss: 0.7963337413002463\n",
      "Epoch 42, Training Loss: 0.7961390881678637\n",
      "Epoch 43, Training Loss: 0.7952820324196535\n",
      "Epoch 44, Training Loss: 0.7955447006926817\n",
      "Epoch 45, Training Loss: 0.7958011589330786\n",
      "Epoch 46, Training Loss: 0.7960042539764853\n",
      "Epoch 47, Training Loss: 0.7958311775151421\n",
      "Epoch 48, Training Loss: 0.7958687325084911\n",
      "Epoch 49, Training Loss: 0.7954387128353119\n",
      "Epoch 50, Training Loss: 0.7956112769772025\n",
      "Epoch 51, Training Loss: 0.7953543390245998\n",
      "Epoch 52, Training Loss: 0.7952430049110861\n",
      "Epoch 53, Training Loss: 0.7952717251637402\n",
      "Epoch 54, Training Loss: 0.7952627579604878\n",
      "Epoch 55, Training Loss: 0.7950138951049132\n",
      "Epoch 56, Training Loss: 0.795374727529638\n",
      "Epoch 57, Training Loss: 0.7948553225573372\n",
      "Epoch 58, Training Loss: 0.7952979814305025\n",
      "Epoch 59, Training Loss: 0.7949992549419403\n",
      "Epoch 60, Training Loss: 0.794335217405768\n",
      "Epoch 61, Training Loss: 0.7946385238451116\n",
      "Epoch 62, Training Loss: 0.795338434121188\n",
      "Epoch 63, Training Loss: 0.794689552082735\n",
      "Epoch 64, Training Loss: 0.7957002664313597\n",
      "Epoch 65, Training Loss: 0.7949167113444384\n",
      "Epoch 66, Training Loss: 0.7949341802036061\n",
      "Epoch 67, Training Loss: 0.7946307361125946\n",
      "Epoch 68, Training Loss: 0.7951229366835426\n",
      "Epoch 69, Training Loss: 0.794720494186177\n",
      "Epoch 70, Training Loss: 0.7942837074924918\n",
      "Epoch 71, Training Loss: 0.7944993173374849\n",
      "Epoch 72, Training Loss: 0.7939311210548177\n",
      "Epoch 73, Training Loss: 0.7940646698194391\n",
      "Epoch 74, Training Loss: 0.7942346743976368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:31:55,602] Trial 96 finished with value: 0.6312 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7944413643023547\n",
      "Epoch 1, Training Loss: 0.8536384843377506\n",
      "Epoch 2, Training Loss: 0.8214206255884732\n",
      "Epoch 3, Training Loss: 0.816607459713431\n",
      "Epoch 4, Training Loss: 0.8145952437204473\n",
      "Epoch 5, Training Loss: 0.8135866628674899\n",
      "Epoch 6, Training Loss: 0.8110540037295397\n",
      "Epoch 7, Training Loss: 0.8105987649805405\n",
      "Epoch 8, Training Loss: 0.80965620952494\n",
      "Epoch 9, Training Loss: 0.8097011022006764\n",
      "Epoch 10, Training Loss: 0.813467140267877\n",
      "Epoch 11, Training Loss: 0.8079164570219377\n",
      "Epoch 12, Training Loss: 0.8099664784179015\n",
      "Epoch 13, Training Loss: 0.8067907081632053\n",
      "Epoch 14, Training Loss: 0.8092981269780327\n",
      "Epoch 15, Training Loss: 0.8080503139075111\n",
      "Epoch 16, Training Loss: 0.8112670676147237\n",
      "Epoch 17, Training Loss: 0.805955133788726\n",
      "Epoch 18, Training Loss: 0.8079825948967653\n",
      "Epoch 19, Training Loss: 0.8099295021505917\n",
      "Epoch 20, Training Loss: 0.8056172806375167\n",
      "Epoch 21, Training Loss: 0.8110860519549425\n",
      "Epoch 22, Training Loss: 0.8048492187612197\n",
      "Epoch 23, Training Loss: 0.8093710754198187\n",
      "Epoch 24, Training Loss: 0.8090132619352902\n",
      "Epoch 25, Training Loss: 0.8106555034132564\n",
      "Epoch 26, Training Loss: 0.8095705449581146\n",
      "Epoch 27, Training Loss: 0.8071507796820472\n",
      "Epoch 28, Training Loss: 0.8080919087634367\n",
      "Epoch 29, Training Loss: 0.8097264604007497\n",
      "Epoch 30, Training Loss: 0.8069708154482\n",
      "Epoch 31, Training Loss: 0.8102649752532735\n",
      "Epoch 32, Training Loss: 0.807324100522434\n",
      "Epoch 33, Training Loss: 0.807200007017921\n",
      "Epoch 34, Training Loss: 0.8077576001952677\n",
      "Epoch 35, Training Loss: 0.8074893386574352\n",
      "Epoch 36, Training Loss: 0.8099819098500645\n",
      "Epoch 37, Training Loss: 0.8062017656073851\n",
      "Epoch 38, Training Loss: 0.8088922205392052\n",
      "Epoch 39, Training Loss: 0.8109186044861288\n",
      "Epoch 40, Training Loss: 0.8050700766900006\n",
      "Epoch 41, Training Loss: 0.8108348298774046\n",
      "Epoch 42, Training Loss: 0.8061116823729346\n",
      "Epoch 43, Training Loss: 0.8087372504262363\n",
      "Epoch 44, Training Loss: 0.8087815204087425\n",
      "Epoch 45, Training Loss: 0.8083883311468012\n",
      "Epoch 46, Training Loss: 0.8088807462243472\n",
      "Epoch 47, Training Loss: 0.8081025007893058\n",
      "Epoch 48, Training Loss: 0.8098123499225167\n",
      "Epoch 49, Training Loss: 0.8064017425565159\n",
      "Epoch 50, Training Loss: 0.8084240693204543\n",
      "Epoch 51, Training Loss: 0.8098808539614958\n",
      "Epoch 52, Training Loss: 0.8086906547406141\n",
      "Epoch 53, Training Loss: 0.8069804983279284\n",
      "Epoch 54, Training Loss: 0.809925405698664\n",
      "Epoch 55, Training Loss: 0.8104243212587693\n",
      "Epoch 56, Training Loss: 0.8093609352672801\n",
      "Epoch 57, Training Loss: 0.8078002039825215\n",
      "Epoch 58, Training Loss: 0.8097999496319714\n",
      "Epoch 59, Training Loss: 0.8101857029690462\n",
      "Epoch 60, Training Loss: 0.8110668856256148\n",
      "Epoch 61, Training Loss: 0.8095331572785097\n",
      "Epoch 62, Training Loss: 0.809159358178868\n",
      "Epoch 63, Training Loss: 0.806633751672857\n",
      "Epoch 64, Training Loss: 0.8078319017326131\n",
      "Epoch 65, Training Loss: 0.8088788116679472\n",
      "Epoch 66, Training Loss: 0.8078398088146659\n",
      "Epoch 67, Training Loss: 0.8112200324675616\n",
      "Epoch 68, Training Loss: 0.8095872020020204\n",
      "Epoch 69, Training Loss: 0.8081338425243602\n",
      "Epoch 70, Training Loss: 0.8084637505867902\n",
      "Epoch 71, Training Loss: 0.809450785482631\n",
      "Epoch 72, Training Loss: 0.8066515038995182\n",
      "Epoch 73, Training Loss: 0.8095706110842088\n",
      "Epoch 74, Training Loss: 0.8079117319864385\n",
      "Epoch 75, Training Loss: 0.808882617669947\n",
      "Epoch 76, Training Loss: 0.8087096535458285\n",
      "Epoch 77, Training Loss: 0.8107018209204955\n",
      "Epoch 78, Training Loss: 0.8082344813907848\n",
      "Epoch 79, Training Loss: 0.8111664208945106\n",
      "Epoch 80, Training Loss: 0.806872813280891\n",
      "Epoch 81, Training Loss: 0.8112455976009368\n",
      "Epoch 82, Training Loss: 0.8077366298787734\n",
      "Epoch 83, Training Loss: 0.8082386217397802\n",
      "Epoch 84, Training Loss: 0.8096080596306745\n",
      "Epoch 85, Training Loss: 0.8091277732568629\n",
      "Epoch 86, Training Loss: 0.8100268678104177\n",
      "Epoch 87, Training Loss: 0.8110286806611454\n",
      "Epoch 88, Training Loss: 0.8074031061985913\n",
      "Epoch 89, Training Loss: 0.8083739585736218\n",
      "Epoch 90, Training Loss: 0.8104492114571964\n",
      "Epoch 91, Training Loss: 0.8090017147625194\n",
      "Epoch 92, Training Loss: 0.8106611117194681\n",
      "Epoch 93, Training Loss: 0.809026707200443\n",
      "Epoch 94, Training Loss: 0.8085015401419471\n",
      "Epoch 95, Training Loss: 0.8101925567318411\n",
      "Epoch 96, Training Loss: 0.8079729437126832\n",
      "Epoch 97, Training Loss: 0.8084516379412483\n",
      "Epoch 98, Training Loss: 0.8118529911602245\n",
      "Epoch 99, Training Loss: 0.8111587954268736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:34:14,559] Trial 97 finished with value: 0.6280666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8079973205398111\n",
      "Epoch 1, Training Loss: 1.0888918605962195\n",
      "Epoch 2, Training Loss: 1.0843242313628807\n",
      "Epoch 3, Training Loss: 1.0787024055208478\n",
      "Epoch 4, Training Loss: 1.0710173943885286\n",
      "Epoch 5, Training Loss: 1.0596978445698444\n",
      "Epoch 6, Training Loss: 1.0436427361983105\n",
      "Epoch 7, Training Loss: 1.022803228869474\n",
      "Epoch 8, Training Loss: 0.9994545425687518\n",
      "Epoch 9, Training Loss: 0.9798147201538085\n",
      "Epoch 10, Training Loss: 0.9675853263166614\n",
      "Epoch 11, Training Loss: 0.9623212901273168\n",
      "Epoch 12, Training Loss: 0.9584180593490601\n",
      "Epoch 13, Training Loss: 0.9561612430371736\n",
      "Epoch 14, Training Loss: 0.9550241249844544\n",
      "Epoch 15, Training Loss: 0.953504098447642\n",
      "Epoch 16, Training Loss: 0.9523069605791479\n",
      "Epoch 17, Training Loss: 0.9509950257781753\n",
      "Epoch 18, Training Loss: 0.9493901120092636\n",
      "Epoch 19, Training Loss: 0.9479974497529797\n",
      "Epoch 20, Training Loss: 0.946744439834939\n",
      "Epoch 21, Training Loss: 0.9445891567638942\n",
      "Epoch 22, Training Loss: 0.9431761327542757\n",
      "Epoch 23, Training Loss: 0.9416766894491095\n",
      "Epoch 24, Training Loss: 0.9394800406649597\n",
      "Epoch 25, Training Loss: 0.9370835702222092\n",
      "Epoch 26, Training Loss: 0.9346821866537395\n",
      "Epoch 27, Training Loss: 0.9329866027473507\n",
      "Epoch 28, Training Loss: 0.9300789759571391\n",
      "Epoch 29, Training Loss: 0.9271432793230042\n",
      "Epoch 30, Training Loss: 0.9241835049220494\n",
      "Epoch 31, Training Loss: 0.9209949068557051\n",
      "Epoch 32, Training Loss: 0.9172549877847944\n",
      "Epoch 33, Training Loss: 0.9129958409115784\n",
      "Epoch 34, Training Loss: 0.9092624959192778\n",
      "Epoch 35, Training Loss: 0.9049870949042471\n",
      "Epoch 36, Training Loss: 0.8998426020593572\n",
      "Epoch 37, Training Loss: 0.8952229635159772\n",
      "Epoch 38, Training Loss: 0.8902083377192791\n",
      "Epoch 39, Training Loss: 0.8846631401463558\n",
      "Epoch 40, Training Loss: 0.8796672593382068\n",
      "Epoch 41, Training Loss: 0.8743526922132736\n",
      "Epoch 42, Training Loss: 0.8695817149671397\n",
      "Epoch 43, Training Loss: 0.8645022939918633\n",
      "Epoch 44, Training Loss: 0.8604324249396647\n",
      "Epoch 45, Training Loss: 0.8555754380118578\n",
      "Epoch 46, Training Loss: 0.8519437891200072\n",
      "Epoch 47, Training Loss: 0.8480057469884256\n",
      "Epoch 48, Training Loss: 0.8447268925215069\n",
      "Epoch 49, Training Loss: 0.8417147761897037\n",
      "Epoch 50, Training Loss: 0.8390391099721866\n",
      "Epoch 51, Training Loss: 0.8367816605065999\n",
      "Epoch 52, Training Loss: 0.8344958654023651\n",
      "Epoch 53, Training Loss: 0.8321662615116378\n",
      "Epoch 54, Training Loss: 0.8314489180880381\n",
      "Epoch 55, Training Loss: 0.8287938277524217\n",
      "Epoch 56, Training Loss: 0.8275381646658245\n",
      "Epoch 57, Training Loss: 0.8258537933342439\n",
      "Epoch 58, Training Loss: 0.825194457778357\n",
      "Epoch 59, Training Loss: 0.8239792069098106\n",
      "Epoch 60, Training Loss: 0.8222144351866012\n",
      "Epoch 61, Training Loss: 0.821596188114998\n",
      "Epoch 62, Training Loss: 0.8207161709778291\n",
      "Epoch 63, Training Loss: 0.8205764417361496\n",
      "Epoch 64, Training Loss: 0.8190557871546064\n",
      "Epoch 65, Training Loss: 0.8186275983215274\n",
      "Epoch 66, Training Loss: 0.818029909295247\n",
      "Epoch 67, Training Loss: 0.8180242375323646\n",
      "Epoch 68, Training Loss: 0.8171181664431005\n",
      "Epoch 69, Training Loss: 0.8161008011129566\n",
      "Epoch 70, Training Loss: 0.8151434884035498\n",
      "Epoch 71, Training Loss: 0.814859714454278\n",
      "Epoch 72, Training Loss: 0.8147898529705249\n",
      "Epoch 73, Training Loss: 0.8139102429375613\n",
      "Epoch 74, Training Loss: 0.8135548227711727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:35:25,536] Trial 98 finished with value: 0.6250666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.813167959765384\n",
      "Epoch 1, Training Loss: 0.9297226477834515\n",
      "Epoch 2, Training Loss: 0.8521580932731915\n",
      "Epoch 3, Training Loss: 0.8414505937045678\n",
      "Epoch 4, Training Loss: 0.8365504476360809\n",
      "Epoch 5, Training Loss: 0.8312258834229376\n",
      "Epoch 6, Training Loss: 0.8287700020280996\n",
      "Epoch 7, Training Loss: 0.8302121072783506\n",
      "Epoch 8, Training Loss: 0.8254175197809263\n",
      "Epoch 9, Training Loss: 0.8226293682155753\n",
      "Epoch 10, Training Loss: 0.8247677533250106\n",
      "Epoch 11, Training Loss: 0.8229762344432057\n",
      "Epoch 12, Training Loss: 0.8227046948626525\n",
      "Epoch 13, Training Loss: 0.8209817076087894\n",
      "Epoch 14, Training Loss: 0.820065899271714\n",
      "Epoch 15, Training Loss: 0.8197254482964824\n",
      "Epoch 16, Training Loss: 0.8203504818722718\n",
      "Epoch 17, Training Loss: 0.8183788163769514\n",
      "Epoch 18, Training Loss: 0.8171650145286904\n",
      "Epoch 19, Training Loss: 0.8184852975651734\n",
      "Epoch 20, Training Loss: 0.8179133307664914\n",
      "Epoch 21, Training Loss: 0.8188001889035218\n",
      "Epoch 22, Training Loss: 0.818256064599618\n",
      "Epoch 23, Training Loss: 0.8174678850890998\n",
      "Epoch 24, Training Loss: 0.8167838268710258\n",
      "Epoch 25, Training Loss: 0.8187368818691798\n",
      "Epoch 26, Training Loss: 0.8170166522040403\n",
      "Epoch 27, Training Loss: 0.817101154022647\n",
      "Epoch 28, Training Loss: 0.8183390637089435\n",
      "Epoch 29, Training Loss: 0.8174432862522011\n",
      "Epoch 30, Training Loss: 0.8181260125977653\n",
      "Epoch 31, Training Loss: 0.8166102445215211\n",
      "Epoch 32, Training Loss: 0.8175482808199144\n",
      "Epoch 33, Training Loss: 0.8166905550132121\n",
      "Epoch 34, Training Loss: 0.8172160853120617\n",
      "Epoch 35, Training Loss: 0.8190717593171543\n",
      "Epoch 36, Training Loss: 0.8150589215128046\n",
      "Epoch 37, Training Loss: 0.8162424749897835\n",
      "Epoch 38, Training Loss: 0.8172219068484199\n",
      "Epoch 39, Training Loss: 0.8148165117529101\n",
      "Epoch 40, Training Loss: 0.8171364881042251\n",
      "Epoch 41, Training Loss: 0.8166946649551392\n",
      "Epoch 42, Training Loss: 0.817210826927558\n",
      "Epoch 43, Training Loss: 0.8157205368343152\n",
      "Epoch 44, Training Loss: 0.8141784661694577\n",
      "Epoch 45, Training Loss: 0.8166829314446987\n",
      "Epoch 46, Training Loss: 0.8148543827515796\n",
      "Epoch 47, Training Loss: 0.8174527710541747\n",
      "Epoch 48, Training Loss: 0.8161850492757066\n",
      "Epoch 49, Training Loss: 0.81635505579468\n",
      "Epoch 50, Training Loss: 0.816659354625788\n",
      "Epoch 51, Training Loss: 0.8168256047973059\n",
      "Epoch 52, Training Loss: 0.8162206382231605\n",
      "Epoch 53, Training Loss: 0.8150161386432504\n",
      "Epoch 54, Training Loss: 0.8159178619994256\n",
      "Epoch 55, Training Loss: 0.8166509257223373\n",
      "Epoch 56, Training Loss: 0.8169467615901975\n",
      "Epoch 57, Training Loss: 0.816854304926736\n",
      "Epoch 58, Training Loss: 0.8167388477719816\n",
      "Epoch 59, Training Loss: 0.816859505588847\n",
      "Epoch 60, Training Loss: 0.8161683920630836\n",
      "Epoch 61, Training Loss: 0.8148708741467698\n",
      "Epoch 62, Training Loss: 0.8160483499218647\n",
      "Epoch 63, Training Loss: 0.8173502719491944\n",
      "Epoch 64, Training Loss: 0.8158344931172249\n",
      "Epoch 65, Training Loss: 0.8176308852389342\n",
      "Epoch 66, Training Loss: 0.8164893772369041\n",
      "Epoch 67, Training Loss: 0.8186514411653791\n",
      "Epoch 68, Training Loss: 0.818292476628956\n",
      "Epoch 69, Training Loss: 0.8160389448018899\n",
      "Epoch 70, Training Loss: 0.8172144603012199\n",
      "Epoch 71, Training Loss: 0.8184858302424725\n",
      "Epoch 72, Training Loss: 0.8148031399214178\n",
      "Epoch 73, Training Loss: 0.8176867423200966\n",
      "Epoch 74, Training Loss: 0.8166481796960186\n",
      "Epoch 75, Training Loss: 0.8162166350766232\n",
      "Epoch 76, Training Loss: 0.8173289743581212\n",
      "Epoch 77, Training Loss: 0.8174830976285432\n",
      "Epoch 78, Training Loss: 0.8191012982139014\n",
      "Epoch 79, Training Loss: 0.8190248779784468\n",
      "Epoch 80, Training Loss: 0.8216229480012018\n",
      "Epoch 81, Training Loss: 0.8192744791955876\n",
      "Epoch 82, Training Loss: 0.8201596405273094\n",
      "Epoch 83, Training Loss: 0.8188981680045451\n",
      "Epoch 84, Training Loss: 0.8202035012101768\n",
      "Epoch 85, Training Loss: 0.822183414778315\n",
      "Epoch 86, Training Loss: 0.8233075263804959\n",
      "Epoch 87, Training Loss: 0.8226849356988319\n",
      "Epoch 88, Training Loss: 0.8211559208712184\n",
      "Epoch 89, Training Loss: 0.8240431509968034\n",
      "Epoch 90, Training Loss: 0.8240684543337141\n",
      "Epoch 91, Training Loss: 0.8236438236738506\n",
      "Epoch 92, Training Loss: 0.823988481930324\n",
      "Epoch 93, Training Loss: 0.8247250193043759\n",
      "Epoch 94, Training Loss: 0.8242653109973535\n",
      "Epoch 95, Training Loss: 0.8241466000564116\n",
      "Epoch 96, Training Loss: 0.826423158143696\n",
      "Epoch 97, Training Loss: 0.8254061477524893\n",
      "Epoch 98, Training Loss: 0.8246975628953231\n",
      "Epoch 99, Training Loss: 0.823779120839628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:37:13,602] Trial 99 finished with value: 0.6088666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8255881902866794\n",
      "Epoch 1, Training Loss: 1.0917481883127886\n",
      "Epoch 2, Training Loss: 1.0577581022018776\n",
      "Epoch 3, Training Loss: 1.034137166890883\n",
      "Epoch 4, Training Loss: 1.0159965581463692\n",
      "Epoch 5, Training Loss: 1.0014532878882902\n",
      "Epoch 6, Training Loss: 0.9895866887013715\n",
      "Epoch 7, Training Loss: 0.9805641477269338\n",
      "Epoch 8, Training Loss: 0.9721303711260172\n",
      "Epoch 9, Training Loss: 0.9657660138338132\n",
      "Epoch 10, Training Loss: 0.9604944666525475\n",
      "Epoch 11, Training Loss: 0.9561370073404527\n",
      "Epoch 12, Training Loss: 0.9524806829323446\n",
      "Epoch 13, Training Loss: 0.9492532713968951\n",
      "Epoch 14, Training Loss: 0.9464282885529941\n",
      "Epoch 15, Training Loss: 0.9439690153401598\n",
      "Epoch 16, Training Loss: 0.9421480851065843\n",
      "Epoch 17, Training Loss: 0.9405074976440659\n",
      "Epoch 18, Training Loss: 0.9388081071968365\n",
      "Epoch 19, Training Loss: 0.938321742617098\n",
      "Epoch 20, Training Loss: 0.9364482696791341\n",
      "Epoch 21, Training Loss: 0.9351204790567097\n",
      "Epoch 22, Training Loss: 0.9339688131683751\n",
      "Epoch 23, Training Loss: 0.9336848820958819\n",
      "Epoch 24, Training Loss: 0.9323748034642155\n",
      "Epoch 25, Training Loss: 0.9314247328535955\n",
      "Epoch 26, Training Loss: 0.9304446552929125\n",
      "Epoch 27, Training Loss: 0.929813775980383\n",
      "Epoch 28, Training Loss: 0.9288868148524062\n",
      "Epoch 29, Training Loss: 0.9280073209812767\n",
      "Epoch 30, Training Loss: 0.9272212995622391\n",
      "Epoch 31, Training Loss: 0.9263748951424333\n",
      "Epoch 32, Training Loss: 0.9256953348790793\n",
      "Epoch 33, Training Loss: 0.9257873278811461\n",
      "Epoch 34, Training Loss: 0.9243772797118452\n",
      "Epoch 35, Training Loss: 0.9239782762706727\n",
      "Epoch 36, Training Loss: 0.923430211561963\n",
      "Epoch 37, Training Loss: 0.9224847084597537\n",
      "Epoch 38, Training Loss: 0.9217280220268365\n",
      "Epoch 39, Training Loss: 0.9212812338556562\n",
      "Epoch 40, Training Loss: 0.9203950962625949\n",
      "Epoch 41, Training Loss: 0.9200978035317328\n",
      "Epoch 42, Training Loss: 0.9192824990229499\n",
      "Epoch 43, Training Loss: 0.9189636519977025\n",
      "Epoch 44, Training Loss: 0.9184525154587021\n",
      "Epoch 45, Training Loss: 0.9186143742468124\n",
      "Epoch 46, Training Loss: 0.9168163769227221\n",
      "Epoch 47, Training Loss: 0.9167186742438409\n",
      "Epoch 48, Training Loss: 0.9163010510286891\n",
      "Epoch 49, Training Loss: 0.9153412523126243\n",
      "Epoch 50, Training Loss: 0.9147196784951633\n",
      "Epoch 51, Training Loss: 0.9142078339605403\n",
      "Epoch 52, Training Loss: 0.9138193314236805\n",
      "Epoch 53, Training Loss: 0.9132841572725683\n",
      "Epoch 54, Training Loss: 0.9133442786403169\n",
      "Epoch 55, Training Loss: 0.9125247765304451\n",
      "Epoch 56, Training Loss: 0.9114238992669529\n",
      "Epoch 57, Training Loss: 0.9111477055047688\n",
      "Epoch 58, Training Loss: 0.9110480957461479\n",
      "Epoch 59, Training Loss: 0.910301149070711\n",
      "Epoch 60, Training Loss: 0.9093852937669682\n",
      "Epoch 61, Training Loss: 0.9092524019399084\n",
      "Epoch 62, Training Loss: 0.908936878075277\n",
      "Epoch 63, Training Loss: 0.9080717891678775\n",
      "Epoch 64, Training Loss: 0.9076574145403123\n",
      "Epoch 65, Training Loss: 0.9070614931278659\n",
      "Epoch 66, Training Loss: 0.9063394291956622\n",
      "Epoch 67, Training Loss: 0.9057631387746423\n",
      "Epoch 68, Training Loss: 0.9053673996961207\n",
      "Epoch 69, Training Loss: 0.9052234586020161\n",
      "Epoch 70, Training Loss: 0.904518283220162\n",
      "Epoch 71, Training Loss: 0.9035542825110873\n",
      "Epoch 72, Training Loss: 0.9035347415988607\n",
      "Epoch 73, Training Loss: 0.9031378346278255\n",
      "Epoch 74, Training Loss: 0.9017918902232235\n",
      "Epoch 75, Training Loss: 0.9013197499110286\n",
      "Epoch 76, Training Loss: 0.9015466415792479\n",
      "Epoch 77, Training Loss: 0.9001769527456814\n",
      "Epoch 78, Training Loss: 0.8999851103115799\n",
      "Epoch 79, Training Loss: 0.9000570457680781\n",
      "Epoch 80, Training Loss: 0.8989062350495417\n",
      "Epoch 81, Training Loss: 0.8983308893397338\n",
      "Epoch 82, Training Loss: 0.8978137757545127\n",
      "Epoch 83, Training Loss: 0.8976551394713552\n",
      "Epoch 84, Training Loss: 0.8969090562117727\n",
      "Epoch 85, Training Loss: 0.8959359691555339\n",
      "Epoch 86, Training Loss: 0.8951889754237985\n",
      "Epoch 87, Training Loss: 0.895173225994397\n",
      "Epoch 88, Training Loss: 0.8942092651711371\n",
      "Epoch 89, Training Loss: 0.8942649680869024\n",
      "Epoch 90, Training Loss: 0.8929900570919639\n",
      "Epoch 91, Training Loss: 0.8928997498705871\n",
      "Epoch 92, Training Loss: 0.8923295831321774\n",
      "Epoch 93, Training Loss: 0.8912759087139502\n",
      "Epoch 94, Training Loss: 0.8907658906807577\n",
      "Epoch 95, Training Loss: 0.8899193889216372\n",
      "Epoch 96, Training Loss: 0.8896053576827946\n",
      "Epoch 97, Training Loss: 0.8893453183030724\n",
      "Epoch 98, Training Loss: 0.8881344431325009\n",
      "Epoch 99, Training Loss: 0.8876809905346175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:38:37,281] Trial 100 finished with value: 0.5818 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8868685388027278\n",
      "Epoch 1, Training Loss: 0.8723776435852051\n",
      "Epoch 2, Training Loss: 0.818268924110076\n",
      "Epoch 3, Training Loss: 0.8129439827975105\n",
      "Epoch 4, Training Loss: 0.8123756129601423\n",
      "Epoch 5, Training Loss: 0.8087150683823754\n",
      "Epoch 6, Training Loss: 0.8076990018171423\n",
      "Epoch 7, Training Loss: 0.8072572508279015\n",
      "Epoch 8, Training Loss: 0.8062864875092226\n",
      "Epoch 9, Training Loss: 0.8071561875764062\n",
      "Epoch 10, Training Loss: 0.8052280296998865\n",
      "Epoch 11, Training Loss: 0.8053137362704558\n",
      "Epoch 12, Training Loss: 0.8059512682522044\n",
      "Epoch 13, Training Loss: 0.8047487660015331\n",
      "Epoch 14, Training Loss: 0.8041649304418003\n",
      "Epoch 15, Training Loss: 0.8034745081733254\n",
      "Epoch 16, Training Loss: 0.8038035878714394\n",
      "Epoch 17, Training Loss: 0.80341751778827\n",
      "Epoch 18, Training Loss: 0.8034565613550299\n",
      "Epoch 19, Training Loss: 0.8034804081916809\n",
      "Epoch 20, Training Loss: 0.8027085487281574\n",
      "Epoch 21, Training Loss: 0.8027185086642995\n",
      "Epoch 22, Training Loss: 0.801460832567776\n",
      "Epoch 23, Training Loss: 0.8013167705956628\n",
      "Epoch 24, Training Loss: 0.802066678089254\n",
      "Epoch 25, Training Loss: 0.8018193516310523\n",
      "Epoch 26, Training Loss: 0.8004345845474916\n",
      "Epoch 27, Training Loss: 0.7999467666710124\n",
      "Epoch 28, Training Loss: 0.7997409076550428\n",
      "Epoch 29, Training Loss: 0.7993078904292162\n",
      "Epoch 30, Training Loss: 0.7983605176561019\n",
      "Epoch 31, Training Loss: 0.7990391937424155\n",
      "Epoch 32, Training Loss: 0.7989209631611319\n",
      "Epoch 33, Training Loss: 0.7987473795694463\n",
      "Epoch 34, Training Loss: 0.7980966366739835\n",
      "Epoch 35, Training Loss: 0.7981983536131242\n",
      "Epoch 36, Training Loss: 0.7969025270377889\n",
      "Epoch 37, Training Loss: 0.7975378251776976\n",
      "Epoch 38, Training Loss: 0.7966071475253386\n",
      "Epoch 39, Training Loss: 0.7964392540034125\n",
      "Epoch 40, Training Loss: 0.7960440250705271\n",
      "Epoch 41, Training Loss: 0.796283548859989\n",
      "Epoch 42, Training Loss: 0.7958390632096459\n",
      "Epoch 43, Training Loss: 0.7955445306441363\n",
      "Epoch 44, Training Loss: 0.7959913152806899\n",
      "Epoch 45, Training Loss: 0.7958190312105067\n",
      "Epoch 46, Training Loss: 0.795078834365396\n",
      "Epoch 47, Training Loss: 0.7950496591539944\n",
      "Epoch 48, Training Loss: 0.7957979208581588\n",
      "Epoch 49, Training Loss: 0.7948210587922264\n",
      "Epoch 50, Training Loss: 0.7949036265822018\n",
      "Epoch 51, Training Loss: 0.7943230571466334\n",
      "Epoch 52, Training Loss: 0.7942328332452213\n",
      "Epoch 53, Training Loss: 0.7948263521054212\n",
      "Epoch 54, Training Loss: 0.794024597266141\n",
      "Epoch 55, Training Loss: 0.793775400133694\n",
      "Epoch 56, Training Loss: 0.7943199786017923\n",
      "Epoch 57, Training Loss: 0.7941169545229744\n",
      "Epoch 58, Training Loss: 0.7930552951728597\n",
      "Epoch 59, Training Loss: 0.7946148726519416\n",
      "Epoch 60, Training Loss: 0.7938217895171221\n",
      "Epoch 61, Training Loss: 0.7939247223910163\n",
      "Epoch 62, Training Loss: 0.7933288740410525\n",
      "Epoch 63, Training Loss: 0.7933368594506207\n",
      "Epoch 64, Training Loss: 0.7935304385073044\n",
      "Epoch 65, Training Loss: 0.7934181776467492\n",
      "Epoch 66, Training Loss: 0.7931200048502753\n",
      "Epoch 67, Training Loss: 0.7938776291117948\n",
      "Epoch 68, Training Loss: 0.7926056603824391\n",
      "Epoch 69, Training Loss: 0.7934108363179599\n",
      "Epoch 70, Training Loss: 0.7924465798630433\n",
      "Epoch 71, Training Loss: 0.7926334069756901\n",
      "Epoch 72, Training Loss: 0.7923531927781946\n",
      "Epoch 73, Training Loss: 0.7921711085123174\n",
      "Epoch 74, Training Loss: 0.7922890738178702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:40:05,950] Trial 101 finished with value: 0.6386666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7930424370485194\n",
      "Epoch 1, Training Loss: 1.0042002245895845\n",
      "Epoch 2, Training Loss: 0.9574921114104135\n",
      "Epoch 3, Training Loss: 0.9499694352759455\n",
      "Epoch 4, Training Loss: 0.9434615543910435\n",
      "Epoch 5, Training Loss: 0.9372496131667517\n",
      "Epoch 6, Training Loss: 0.9310496165340109\n",
      "Epoch 7, Training Loss: 0.925478318400849\n",
      "Epoch 8, Training Loss: 0.9184060478568974\n",
      "Epoch 9, Training Loss: 0.911869858530231\n",
      "Epoch 10, Training Loss: 0.9044232536079292\n",
      "Epoch 11, Training Loss: 0.8976495148544025\n",
      "Epoch 12, Training Loss: 0.8899728893337393\n",
      "Epoch 13, Training Loss: 0.8825032640220527\n",
      "Epoch 14, Training Loss: 0.875428595220236\n",
      "Epoch 15, Training Loss: 0.8682470355715071\n",
      "Epoch 16, Training Loss: 0.8613553187900916\n",
      "Epoch 17, Training Loss: 0.8550311189845092\n",
      "Epoch 18, Training Loss: 0.8487686863519195\n",
      "Epoch 19, Training Loss: 0.8436319395115501\n",
      "Epoch 20, Training Loss: 0.8394880823622969\n",
      "Epoch 21, Training Loss: 0.8349138813807552\n",
      "Epoch 22, Training Loss: 0.8313327035509555\n",
      "Epoch 23, Training Loss: 0.8284040305847512\n",
      "Epoch 24, Training Loss: 0.8249464071782908\n",
      "Epoch 25, Training Loss: 0.8225949360015697\n",
      "Epoch 26, Training Loss: 0.8205985209099332\n",
      "Epoch 27, Training Loss: 0.8189699625610409\n",
      "Epoch 28, Training Loss: 0.8174154954745357\n",
      "Epoch 29, Training Loss: 0.8164091465168429\n",
      "Epoch 30, Training Loss: 0.8151714158237429\n",
      "Epoch 31, Training Loss: 0.8145968214013523\n",
      "Epoch 32, Training Loss: 0.8136395645320864\n",
      "Epoch 33, Training Loss: 0.812981939495058\n",
      "Epoch 34, Training Loss: 0.8126211615433371\n",
      "Epoch 35, Training Loss: 0.8119937262140718\n",
      "Epoch 36, Training Loss: 0.8112664294422121\n",
      "Epoch 37, Training Loss: 0.8109644530411053\n",
      "Epoch 38, Training Loss: 0.810376268849337\n",
      "Epoch 39, Training Loss: 0.809994824129836\n",
      "Epoch 40, Training Loss: 0.8098714752304823\n",
      "Epoch 41, Training Loss: 0.8101009395785798\n",
      "Epoch 42, Training Loss: 0.8093083731213907\n",
      "Epoch 43, Training Loss: 0.8092489083906762\n",
      "Epoch 44, Training Loss: 0.809241126981893\n",
      "Epoch 45, Training Loss: 0.8089242789978371\n",
      "Epoch 46, Training Loss: 0.80873363627527\n",
      "Epoch 47, Training Loss: 0.8080459065007087\n",
      "Epoch 48, Training Loss: 0.8079632577143218\n",
      "Epoch 49, Training Loss: 0.8082685267118583\n",
      "Epoch 50, Training Loss: 0.8081238810281108\n",
      "Epoch 51, Training Loss: 0.8073977586918307\n",
      "Epoch 52, Training Loss: 0.8071210033015201\n",
      "Epoch 53, Training Loss: 0.8067541047146446\n",
      "Epoch 54, Training Loss: 0.8064957648291624\n",
      "Epoch 55, Training Loss: 0.8073029765509125\n",
      "Epoch 56, Training Loss: 0.8065952263380352\n",
      "Epoch 57, Training Loss: 0.8069386441904799\n",
      "Epoch 58, Training Loss: 0.8069323851649922\n",
      "Epoch 59, Training Loss: 0.8060385090067871\n",
      "Epoch 60, Training Loss: 0.8060854843684605\n",
      "Epoch 61, Training Loss: 0.8055001397778218\n",
      "Epoch 62, Training Loss: 0.8052546508778307\n",
      "Epoch 63, Training Loss: 0.8053098611365583\n",
      "Epoch 64, Training Loss: 0.8050517157504433\n",
      "Epoch 65, Training Loss: 0.8051989074040177\n",
      "Epoch 66, Training Loss: 0.804575733134621\n",
      "Epoch 67, Training Loss: 0.8044553532636255\n",
      "Epoch 68, Training Loss: 0.8043973642184322\n",
      "Epoch 69, Training Loss: 0.8041553042885056\n",
      "Epoch 70, Training Loss: 0.8054529770872647\n",
      "Epoch 71, Training Loss: 0.8042285041701525\n",
      "Epoch 72, Training Loss: 0.8040759406591717\n",
      "Epoch 73, Training Loss: 0.804667025580442\n",
      "Epoch 74, Training Loss: 0.8037568044841737\n",
      "Epoch 75, Training Loss: 0.8039848233524122\n",
      "Epoch 76, Training Loss: 0.8034086713217254\n",
      "Epoch 77, Training Loss: 0.8031913709819765\n",
      "Epoch 78, Training Loss: 0.8031770093100411\n",
      "Epoch 79, Training Loss: 0.8031509842191423\n",
      "Epoch 80, Training Loss: 0.8028389640320512\n",
      "Epoch 81, Training Loss: 0.8029630911977668\n",
      "Epoch 82, Training Loss: 0.8027902533237199\n",
      "Epoch 83, Training Loss: 0.8022867194691995\n",
      "Epoch 84, Training Loss: 0.8028344419665803\n",
      "Epoch 85, Training Loss: 0.8024432767602734\n",
      "Epoch 86, Training Loss: 0.8026120160755358\n",
      "Epoch 87, Training Loss: 0.8019473783055643\n",
      "Epoch 88, Training Loss: 0.8018613666520082\n",
      "Epoch 89, Training Loss: 0.8018178784757628\n",
      "Epoch 90, Training Loss: 0.8013478752813841\n",
      "Epoch 91, Training Loss: 0.801468529109668\n",
      "Epoch 92, Training Loss: 0.8017572594764537\n",
      "Epoch 93, Training Loss: 0.802136238535544\n",
      "Epoch 94, Training Loss: 0.8015807455643675\n",
      "Epoch 95, Training Loss: 0.8015803839927329\n",
      "Epoch 96, Training Loss: 0.8011015560393943\n",
      "Epoch 97, Training Loss: 0.8015634466830949\n",
      "Epoch 98, Training Loss: 0.8008001865300917\n",
      "Epoch 99, Training Loss: 0.8013581002565254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:41:28,904] Trial 102 finished with value: 0.6346 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8010007668258552\n",
      "Epoch 1, Training Loss: 0.8625323190408595\n",
      "Epoch 2, Training Loss: 0.827230935236987\n",
      "Epoch 3, Training Loss: 0.821609320710687\n",
      "Epoch 4, Training Loss: 0.8179437392599442\n",
      "Epoch 5, Training Loss: 0.8145379396045909\n",
      "Epoch 6, Training Loss: 0.8127398650085225\n",
      "Epoch 7, Training Loss: 0.8127830167377696\n",
      "Epoch 8, Training Loss: 0.8111016822562498\n",
      "Epoch 9, Training Loss: 0.8110675478682798\n",
      "Epoch 10, Training Loss: 0.810585231220021\n",
      "Epoch 11, Training Loss: 0.8093266004674575\n",
      "Epoch 12, Training Loss: 0.8087243030351751\n",
      "Epoch 13, Training Loss: 0.8090728231738595\n",
      "Epoch 14, Training Loss: 0.8084203544083763\n",
      "Epoch 15, Training Loss: 0.8080974021378685\n",
      "Epoch 16, Training Loss: 0.8078457907368155\n",
      "Epoch 17, Training Loss: 0.8078487169742584\n",
      "Epoch 18, Training Loss: 0.806863586622126\n",
      "Epoch 19, Training Loss: 0.8069386971698088\n",
      "Epoch 20, Training Loss: 0.8066363291880664\n",
      "Epoch 21, Training Loss: 0.8064504348530489\n",
      "Epoch 22, Training Loss: 0.8062989335200366\n",
      "Epoch 23, Training Loss: 0.8061669611930847\n",
      "Epoch 24, Training Loss: 0.8061936694033006\n",
      "Epoch 25, Training Loss: 0.8057025663992938\n",
      "Epoch 26, Training Loss: 0.8055734764828402\n",
      "Epoch 27, Training Loss: 0.8055209555345423\n",
      "Epoch 28, Training Loss: 0.8055293048830593\n",
      "Epoch 29, Training Loss: 0.8049335612269009\n",
      "Epoch 30, Training Loss: 0.8048977075604832\n",
      "Epoch 31, Training Loss: 0.8044099502002492\n",
      "Epoch 32, Training Loss: 0.804204349377576\n",
      "Epoch 33, Training Loss: 0.804482120626113\n",
      "Epoch 34, Training Loss: 0.8040956603078281\n",
      "Epoch 35, Training Loss: 0.8038158957397237\n",
      "Epoch 36, Training Loss: 0.8035637757357429\n",
      "Epoch 37, Training Loss: 0.8031748925237094\n",
      "Epoch 38, Training Loss: 0.8028592079527238\n",
      "Epoch 39, Training Loss: 0.8026378794978647\n",
      "Epoch 40, Training Loss: 0.8025791609988493\n",
      "Epoch 41, Training Loss: 0.8030318889898412\n",
      "Epoch 42, Training Loss: 0.8023266330186059\n",
      "Epoch 43, Training Loss: 0.8018735584090738\n",
      "Epoch 44, Training Loss: 0.801868085230098\n",
      "Epoch 45, Training Loss: 0.8019859092375812\n",
      "Epoch 46, Training Loss: 0.8018205657426049\n",
      "Epoch 47, Training Loss: 0.8015334323574514\n",
      "Epoch 48, Training Loss: 0.8014008747128879\n",
      "Epoch 49, Training Loss: 0.801277493799434\n",
      "Epoch 50, Training Loss: 0.8009900326588575\n",
      "Epoch 51, Training Loss: 0.8010656804196975\n",
      "Epoch 52, Training Loss: 0.8009207571955288\n",
      "Epoch 53, Training Loss: 0.8006625171969919\n",
      "Epoch 54, Training Loss: 0.8007481957183165\n",
      "Epoch 55, Training Loss: 0.8005306854668786\n",
      "Epoch 56, Training Loss: 0.7998973477587981\n",
      "Epoch 57, Training Loss: 0.7999944203040179\n",
      "Epoch 58, Training Loss: 0.7990983662184546\n",
      "Epoch 59, Training Loss: 0.7993293927697575\n",
      "Epoch 60, Training Loss: 0.7996616735177882\n",
      "Epoch 61, Training Loss: 0.7987116204289829\n",
      "Epoch 62, Training Loss: 0.798630962862688\n",
      "Epoch 63, Training Loss: 0.7983907351774328\n",
      "Epoch 64, Training Loss: 0.7983068435332354\n",
      "Epoch 65, Training Loss: 0.7979577362537384\n",
      "Epoch 66, Training Loss: 0.7980454479946809\n",
      "Epoch 67, Training Loss: 0.7974046381080852\n",
      "Epoch 68, Training Loss: 0.7974238675482133\n",
      "Epoch 69, Training Loss: 0.7975363027348238\n",
      "Epoch 70, Training Loss: 0.7972749393827775\n",
      "Epoch 71, Training Loss: 0.7971620012732114\n",
      "Epoch 72, Training Loss: 0.797404580116272\n",
      "Epoch 73, Training Loss: 0.7971826270047356\n",
      "Epoch 74, Training Loss: 0.7970246526073007\n",
      "Epoch 75, Training Loss: 0.7961668925425586\n",
      "Epoch 76, Training Loss: 0.796638516888899\n",
      "Epoch 77, Training Loss: 0.7966683504861943\n",
      "Epoch 78, Training Loss: 0.7965354556897107\n",
      "Epoch 79, Training Loss: 0.7963336063833798\n",
      "Epoch 80, Training Loss: 0.7961211089526906\n",
      "Epoch 81, Training Loss: 0.7964339253481697\n",
      "Epoch 82, Training Loss: 0.796230708360672\n",
      "Epoch 83, Training Loss: 0.7963903094039244\n",
      "Epoch 84, Training Loss: 0.7957209987500135\n",
      "Epoch 85, Training Loss: 0.795502321509754\n",
      "Epoch 86, Training Loss: 0.7958198473032783\n",
      "Epoch 87, Training Loss: 0.7958059685370501\n",
      "Epoch 88, Training Loss: 0.7960838045793421\n",
      "Epoch 89, Training Loss: 0.7957560386377223\n",
      "Epoch 90, Training Loss: 0.7959452026030597\n",
      "Epoch 91, Training Loss: 0.7957515668167787\n",
      "Epoch 92, Training Loss: 0.7954472333543441\n",
      "Epoch 93, Training Loss: 0.7955572118478663\n",
      "Epoch 94, Training Loss: 0.7956557962473702\n",
      "Epoch 95, Training Loss: 0.7956134094911463\n",
      "Epoch 96, Training Loss: 0.7954560359786539\n",
      "Epoch 97, Training Loss: 0.7954277358335607\n",
      "Epoch 98, Training Loss: 0.7952926996876212\n",
      "Epoch 99, Training Loss: 0.7952967850600972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:43:18,843] Trial 103 finished with value: 0.6364 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.795210897992639\n",
      "Epoch 1, Training Loss: 0.9155542272679946\n",
      "Epoch 2, Training Loss: 0.8590190828547758\n",
      "Epoch 3, Training Loss: 0.8250845259077408\n",
      "Epoch 4, Training Loss: 0.8110141535366283\n",
      "Epoch 5, Training Loss: 0.8053947668215807\n",
      "Epoch 6, Training Loss: 0.8031052680576549\n",
      "Epoch 7, Training Loss: 0.8022162483720219\n",
      "Epoch 8, Training Loss: 0.801506102996714\n",
      "Epoch 9, Training Loss: 0.8006745410666746\n",
      "Epoch 10, Training Loss: 0.8001826675499186\n",
      "Epoch 11, Training Loss: 0.7998284238927504\n",
      "Epoch 12, Training Loss: 0.7992465118099662\n",
      "Epoch 13, Training Loss: 0.799220558755538\n",
      "Epoch 14, Training Loss: 0.7988490979110493\n",
      "Epoch 15, Training Loss: 0.7987073767886442\n",
      "Epoch 16, Training Loss: 0.798335420033511\n",
      "Epoch 17, Training Loss: 0.7981186434801887\n",
      "Epoch 18, Training Loss: 0.7979273487539852\n",
      "Epoch 19, Training Loss: 0.7975901653486139\n",
      "Epoch 20, Training Loss: 0.7976543018397163\n",
      "Epoch 21, Training Loss: 0.7973862151538624\n",
      "Epoch 22, Training Loss: 0.7973946330126594\n",
      "Epoch 23, Training Loss: 0.7971159855057212\n",
      "Epoch 24, Training Loss: 0.7971153062231401\n",
      "Epoch 25, Training Loss: 0.7967536414370817\n",
      "Epoch 26, Training Loss: 0.7967116434433881\n",
      "Epoch 27, Training Loss: 0.7964366384814767\n",
      "Epoch 28, Training Loss: 0.7963556602421928\n",
      "Epoch 29, Training Loss: 0.7960264750789193\n",
      "Epoch 30, Training Loss: 0.7960394024147707\n",
      "Epoch 31, Training Loss: 0.7957709267560173\n",
      "Epoch 32, Training Loss: 0.7952738397261676\n",
      "Epoch 33, Training Loss: 0.7950927923707402\n",
      "Epoch 34, Training Loss: 0.794871699739905\n",
      "Epoch 35, Training Loss: 0.7945377164728501\n",
      "Epoch 36, Training Loss: 0.794059219009736\n",
      "Epoch 37, Training Loss: 0.7934917808280272\n",
      "Epoch 38, Training Loss: 0.7930912966587964\n",
      "Epoch 39, Training Loss: 0.7928231870426851\n",
      "Epoch 40, Training Loss: 0.7923970247016233\n",
      "Epoch 41, Training Loss: 0.7919799571177538\n",
      "Epoch 42, Training Loss: 0.7919135172928081\n",
      "Epoch 43, Training Loss: 0.7916778212435105\n",
      "Epoch 44, Training Loss: 0.7912534549657037\n",
      "Epoch 45, Training Loss: 0.7911693702725803\n",
      "Epoch 46, Training Loss: 0.7909718223179087\n",
      "Epoch 47, Training Loss: 0.7909832551900078\n",
      "Epoch 48, Training Loss: 0.7903307166520287\n",
      "Epoch 49, Training Loss: 0.7904847828780903\n",
      "Epoch 50, Training Loss: 0.790349034982569\n",
      "Epoch 51, Training Loss: 0.7902533659514259\n",
      "Epoch 52, Training Loss: 0.7900698152009179\n",
      "Epoch 53, Training Loss: 0.7899598826380337\n",
      "Epoch 54, Training Loss: 0.7897463055919198\n",
      "Epoch 55, Training Loss: 0.7898915726297042\n",
      "Epoch 56, Training Loss: 0.7895822130231296\n",
      "Epoch 57, Training Loss: 0.7896730341630823\n",
      "Epoch 58, Training Loss: 0.7893940272050746\n",
      "Epoch 59, Training Loss: 0.7892280078635496\n",
      "Epoch 60, Training Loss: 0.7893827300913193\n",
      "Epoch 61, Training Loss: 0.7890841278609108\n",
      "Epoch 62, Training Loss: 0.7889498931520126\n",
      "Epoch 63, Training Loss: 0.7888886246961706\n",
      "Epoch 64, Training Loss: 0.788704332744374\n",
      "Epoch 65, Training Loss: 0.7887602243703954\n",
      "Epoch 66, Training Loss: 0.7885348106131834\n",
      "Epoch 67, Training Loss: 0.7886185525445377\n",
      "Epoch 68, Training Loss: 0.7883016569474164\n",
      "Epoch 69, Training Loss: 0.7882831161162432\n",
      "Epoch 70, Training Loss: 0.7882671780445997\n",
      "Epoch 71, Training Loss: 0.7882239575947032\n",
      "Epoch 72, Training Loss: 0.7877668491531821\n",
      "Epoch 73, Training Loss: 0.7881117501679589\n",
      "Epoch 74, Training Loss: 0.7877931594147402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:44:40,557] Trial 104 finished with value: 0.6396666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7876648815239177\n",
      "Epoch 1, Training Loss: 0.9997740643365043\n",
      "Epoch 2, Training Loss: 0.942969306519157\n",
      "Epoch 3, Training Loss: 0.9154792537366537\n",
      "Epoch 4, Training Loss: 0.8710976791561098\n",
      "Epoch 5, Training Loss: 0.830708864846624\n",
      "Epoch 6, Training Loss: 0.8201655127052078\n",
      "Epoch 7, Training Loss: 0.8163741446079168\n",
      "Epoch 8, Training Loss: 0.8151618419733263\n",
      "Epoch 9, Training Loss: 0.8139579023633684\n",
      "Epoch 10, Training Loss: 0.8126484524934812\n",
      "Epoch 11, Training Loss: 0.8114598310083375\n",
      "Epoch 12, Training Loss: 0.811710738597956\n",
      "Epoch 13, Training Loss: 0.8100603876257302\n",
      "Epoch 14, Training Loss: 0.8097385160008768\n",
      "Epoch 15, Training Loss: 0.8091199520835303\n",
      "Epoch 16, Training Loss: 0.8080125375797874\n",
      "Epoch 17, Training Loss: 0.8070170355022401\n",
      "Epoch 18, Training Loss: 0.8062819220965967\n",
      "Epoch 19, Training Loss: 0.805757024503292\n",
      "Epoch 20, Training Loss: 0.8051674753203428\n",
      "Epoch 21, Training Loss: 0.8045173435282886\n",
      "Epoch 22, Training Loss: 0.8041755427095226\n",
      "Epoch 23, Training Loss: 0.8037830848442881\n",
      "Epoch 24, Training Loss: 0.8024909285674418\n",
      "Epoch 25, Training Loss: 0.8031628645452341\n",
      "Epoch 26, Training Loss: 0.8024086776532625\n",
      "Epoch 27, Training Loss: 0.8024355186555618\n",
      "Epoch 28, Training Loss: 0.8015674620642698\n",
      "Epoch 29, Training Loss: 0.801774411452444\n",
      "Epoch 30, Training Loss: 0.8009956295328929\n",
      "Epoch 31, Training Loss: 0.8009805848723963\n",
      "Epoch 32, Training Loss: 0.8002214334064857\n",
      "Epoch 33, Training Loss: 0.8006129565991853\n",
      "Epoch 34, Training Loss: 0.8002836101933529\n",
      "Epoch 35, Training Loss: 0.8000712657333316\n",
      "Epoch 36, Training Loss: 0.7990134025874891\n",
      "Epoch 37, Training Loss: 0.7992659083882668\n",
      "Epoch 38, Training Loss: 0.7991456686105943\n",
      "Epoch 39, Training Loss: 0.7995679534467539\n",
      "Epoch 40, Training Loss: 0.7985473523462625\n",
      "Epoch 41, Training Loss: 0.7987799721552913\n",
      "Epoch 42, Training Loss: 0.798596295557524\n",
      "Epoch 43, Training Loss: 0.798447198796093\n",
      "Epoch 44, Training Loss: 0.7977963753212663\n",
      "Epoch 45, Training Loss: 0.798325206641864\n",
      "Epoch 46, Training Loss: 0.7979457166858186\n",
      "Epoch 47, Training Loss: 0.7973291778026667\n",
      "Epoch 48, Training Loss: 0.7972611054441983\n",
      "Epoch 49, Training Loss: 0.7980193360407549\n",
      "Epoch 50, Training Loss: 0.7973332264369591\n",
      "Epoch 51, Training Loss: 0.7967474695435144\n",
      "Epoch 52, Training Loss: 0.7970287080994226\n",
      "Epoch 53, Training Loss: 0.7969819863039748\n",
      "Epoch 54, Training Loss: 0.7963280322856473\n",
      "Epoch 55, Training Loss: 0.7972602880090699\n",
      "Epoch 56, Training Loss: 0.7971058704799279\n",
      "Epoch 57, Training Loss: 0.7961042725950256\n",
      "Epoch 58, Training Loss: 0.795757642186674\n",
      "Epoch 59, Training Loss: 0.7951060221607523\n",
      "Epoch 60, Training Loss: 0.7959720977266929\n",
      "Epoch 61, Training Loss: 0.7955994691167559\n",
      "Epoch 62, Training Loss: 0.7960771134025172\n",
      "Epoch 63, Training Loss: 0.7951063332701088\n",
      "Epoch 64, Training Loss: 0.7949725816124364\n",
      "Epoch 65, Training Loss: 0.7946514895984105\n",
      "Epoch 66, Training Loss: 0.7945113978887859\n",
      "Epoch 67, Training Loss: 0.7944860270148829\n",
      "Epoch 68, Training Loss: 0.794281540508557\n",
      "Epoch 69, Training Loss: 0.7942378613285552\n",
      "Epoch 70, Training Loss: 0.7938445910475308\n",
      "Epoch 71, Training Loss: 0.7936749199279269\n",
      "Epoch 72, Training Loss: 0.7934840273140068\n",
      "Epoch 73, Training Loss: 0.7928386459673258\n",
      "Epoch 74, Training Loss: 0.7928284178102823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:46:09,478] Trial 105 finished with value: 0.6304 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7932363976213269\n",
      "Epoch 1, Training Loss: 0.8622489361202016\n",
      "Epoch 2, Training Loss: 0.8259447525529301\n",
      "Epoch 3, Training Loss: 0.8228479766845703\n",
      "Epoch 4, Training Loss: 0.8207009173140807\n",
      "Epoch 5, Training Loss: 0.8173164043707006\n",
      "Epoch 6, Training Loss: 0.8145987979804769\n",
      "Epoch 7, Training Loss: 0.8134786822515375\n",
      "Epoch 8, Training Loss: 0.8119032400495866\n",
      "Epoch 9, Training Loss: 0.8116145848526675\n",
      "Epoch 10, Training Loss: 0.8102200610497419\n",
      "Epoch 11, Training Loss: 0.8093432982528911\n",
      "Epoch 12, Training Loss: 0.808530837507809\n",
      "Epoch 13, Training Loss: 0.8076908119285808\n",
      "Epoch 14, Training Loss: 0.8067115362251506\n",
      "Epoch 15, Training Loss: 0.806995067876928\n",
      "Epoch 16, Training Loss: 0.8064028057631324\n",
      "Epoch 17, Training Loss: 0.8057548907925101\n",
      "Epoch 18, Training Loss: 0.8061430952128242\n",
      "Epoch 19, Training Loss: 0.8057572725941153\n",
      "Epoch 20, Training Loss: 0.8052737319469452\n",
      "Epoch 21, Training Loss: 0.8047902351267198\n",
      "Epoch 22, Training Loss: 0.8046401922141805\n",
      "Epoch 23, Training Loss: 0.8048272506629719\n",
      "Epoch 24, Training Loss: 0.8049165867356693\n",
      "Epoch 25, Training Loss: 0.8049451697573943\n",
      "Epoch 26, Training Loss: 0.8040920423058903\n",
      "Epoch 27, Training Loss: 0.8035969662666321\n",
      "Epoch 28, Training Loss: 0.8044933749647701\n",
      "Epoch 29, Training Loss: 0.8033700259292826\n",
      "Epoch 30, Training Loss: 0.8039039416874156\n",
      "Epoch 31, Training Loss: 0.8037920672753278\n",
      "Epoch 32, Training Loss: 0.8038108575344086\n",
      "Epoch 33, Training Loss: 0.8038334050599266\n",
      "Epoch 34, Training Loss: 0.8043235055839314\n",
      "Epoch 35, Training Loss: 0.8029390348406399\n",
      "Epoch 36, Training Loss: 0.8034775688367731\n",
      "Epoch 37, Training Loss: 0.8031662635943468\n",
      "Epoch 38, Training Loss: 0.8033253873095793\n",
      "Epoch 39, Training Loss: 0.8029881002622492\n",
      "Epoch 40, Training Loss: 0.8029845112912795\n",
      "Epoch 41, Training Loss: 0.8027770236660452\n",
      "Epoch 42, Training Loss: 0.8023553039747126\n",
      "Epoch 43, Training Loss: 0.8030587591143216\n",
      "Epoch 44, Training Loss: 0.803022729438894\n",
      "Epoch 45, Training Loss: 0.802967963288812\n",
      "Epoch 46, Training Loss: 0.80194611584439\n",
      "Epoch 47, Training Loss: 0.8031241328576032\n",
      "Epoch 48, Training Loss: 0.8019313226026648\n",
      "Epoch 49, Training Loss: 0.8026290685990277\n",
      "Epoch 50, Training Loss: 0.8025156511278714\n",
      "Epoch 51, Training Loss: 0.8021357325946583\n",
      "Epoch 52, Training Loss: 0.8019322555906633\n",
      "Epoch 53, Training Loss: 0.8024406583168927\n",
      "Epoch 54, Training Loss: 0.8023252922647139\n",
      "Epoch 55, Training Loss: 0.8022163452120388\n",
      "Epoch 56, Training Loss: 0.8016494215235991\n",
      "Epoch 57, Training Loss: 0.8014104672039256\n",
      "Epoch 58, Training Loss: 0.8019985988560845\n",
      "Epoch 59, Training Loss: 0.8019196177230162\n",
      "Epoch 60, Training Loss: 0.8020617541846107\n",
      "Epoch 61, Training Loss: 0.8017069573262159\n",
      "Epoch 62, Training Loss: 0.8018101491647608\n",
      "Epoch 63, Training Loss: 0.8018466771350188\n",
      "Epoch 64, Training Loss: 0.8021503186225891\n",
      "Epoch 65, Training Loss: 0.8011484052854426\n",
      "Epoch 66, Training Loss: 0.8017424922129687\n",
      "Epoch 67, Training Loss: 0.8018688740449793\n",
      "Epoch 68, Training Loss: 0.8017296726563398\n",
      "Epoch 69, Training Loss: 0.8014976336675532\n",
      "Epoch 70, Training Loss: 0.801711214500315\n",
      "Epoch 71, Training Loss: 0.8009371857082143\n",
      "Epoch 72, Training Loss: 0.80092365657582\n",
      "Epoch 73, Training Loss: 0.8012740614834953\n",
      "Epoch 74, Training Loss: 0.801308466686922\n",
      "Epoch 75, Training Loss: 0.8013339954264024\n",
      "Epoch 76, Training Loss: 0.8014097351887647\n",
      "Epoch 77, Training Loss: 0.8014253633863786\n",
      "Epoch 78, Training Loss: 0.8014426380045274\n",
      "Epoch 79, Training Loss: 0.8013151583250832\n",
      "Epoch 80, Training Loss: 0.8014332375105689\n",
      "Epoch 81, Training Loss: 0.8013437087395612\n",
      "Epoch 82, Training Loss: 0.8009304786429686\n",
      "Epoch 83, Training Loss: 0.801719890201793\n",
      "Epoch 84, Training Loss: 0.8015123029316172\n",
      "Epoch 85, Training Loss: 0.8017537150663488\n",
      "Epoch 86, Training Loss: 0.8020182838159449\n",
      "Epoch 87, Training Loss: 0.8010384033708011\n",
      "Epoch 88, Training Loss: 0.8013878192621119\n",
      "Epoch 89, Training Loss: 0.8013916972104241\n",
      "Epoch 90, Training Loss: 0.8015484884907218\n",
      "Epoch 91, Training Loss: 0.8016174420889686\n",
      "Epoch 92, Training Loss: 0.8013911414146423\n",
      "Epoch 93, Training Loss: 0.8005091871233547\n",
      "Epoch 94, Training Loss: 0.8014318032124463\n",
      "Epoch 95, Training Loss: 0.8008393599005307\n",
      "Epoch 96, Training Loss: 0.8006936287178713\n",
      "Epoch 97, Training Loss: 0.8009418248429018\n",
      "Epoch 98, Training Loss: 0.8014987331278184\n",
      "Epoch 99, Training Loss: 0.8013102353320403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:47:58,765] Trial 106 finished with value: 0.6332666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8006260965150945\n",
      "Epoch 1, Training Loss: 0.8841496550050894\n",
      "Epoch 2, Training Loss: 0.8172830257200657\n",
      "Epoch 3, Training Loss: 0.8088025141479378\n",
      "Epoch 4, Training Loss: 0.8074886517417162\n",
      "Epoch 5, Training Loss: 0.8066292175673004\n",
      "Epoch 6, Training Loss: 0.8043512043200041\n",
      "Epoch 7, Training Loss: 0.8028587356545871\n",
      "Epoch 8, Training Loss: 0.8001207196622863\n",
      "Epoch 9, Training Loss: 0.7976178594997951\n",
      "Epoch 10, Training Loss: 0.795726237978254\n",
      "Epoch 11, Training Loss: 0.7935132982587456\n",
      "Epoch 12, Training Loss: 0.7932688153776011\n",
      "Epoch 13, Training Loss: 0.7911873212434295\n",
      "Epoch 14, Training Loss: 0.7925247097373905\n",
      "Epoch 15, Training Loss: 0.7925696080788633\n",
      "Epoch 16, Training Loss: 0.7901708794715709\n",
      "Epoch 17, Training Loss: 0.7914222917162386\n",
      "Epoch 18, Training Loss: 0.7892638429663235\n",
      "Epoch 19, Training Loss: 0.7900398859404083\n",
      "Epoch 20, Training Loss: 0.789730946103433\n",
      "Epoch 21, Training Loss: 0.7886200536462598\n",
      "Epoch 22, Training Loss: 0.7882527637302428\n",
      "Epoch 23, Training Loss: 0.788240991768084\n",
      "Epoch 24, Training Loss: 0.7879628716554857\n",
      "Epoch 25, Training Loss: 0.7888846379473693\n",
      "Epoch 26, Training Loss: 0.7878060559581097\n",
      "Epoch 27, Training Loss: 0.7875245430415734\n",
      "Epoch 28, Training Loss: 0.7863937591699729\n",
      "Epoch 29, Training Loss: 0.7874224918229239\n",
      "Epoch 30, Training Loss: 0.7868699410804232\n",
      "Epoch 31, Training Loss: 0.7866236812190006\n",
      "Epoch 32, Training Loss: 0.7867130280437326\n",
      "Epoch 33, Training Loss: 0.7866643286289129\n",
      "Epoch 34, Training Loss: 0.7849346102404415\n",
      "Epoch 35, Training Loss: 0.7852889388127434\n",
      "Epoch 36, Training Loss: 0.7846085279059589\n",
      "Epoch 37, Training Loss: 0.7857255455246546\n",
      "Epoch 38, Training Loss: 0.7853643619028249\n",
      "Epoch 39, Training Loss: 0.7845221536500113\n",
      "Epoch 40, Training Loss: 0.7840142193593477\n",
      "Epoch 41, Training Loss: 0.7849066837389667\n",
      "Epoch 42, Training Loss: 0.7841984833093514\n",
      "Epoch 43, Training Loss: 0.7837263005120414\n",
      "Epoch 44, Training Loss: 0.7842798955458448\n",
      "Epoch 45, Training Loss: 0.7841379690887337\n",
      "Epoch 46, Training Loss: 0.784467895855581\n",
      "Epoch 47, Training Loss: 0.7839545275035658\n",
      "Epoch 48, Training Loss: 0.7833575857313055\n",
      "Epoch 49, Training Loss: 0.7842372416553641\n",
      "Epoch 50, Training Loss: 0.7828643031586382\n",
      "Epoch 51, Training Loss: 0.7835641130468899\n",
      "Epoch 52, Training Loss: 0.7832465714978096\n",
      "Epoch 53, Training Loss: 0.7828219570611653\n",
      "Epoch 54, Training Loss: 0.7834381247821607\n",
      "Epoch 55, Training Loss: 0.7817824957962323\n",
      "Epoch 56, Training Loss: 0.7820795703651313\n",
      "Epoch 57, Training Loss: 0.782526329406222\n",
      "Epoch 58, Training Loss: 0.7824309658287163\n",
      "Epoch 59, Training Loss: 0.7821136401111918\n",
      "Epoch 60, Training Loss: 0.7828130253275535\n",
      "Epoch 61, Training Loss: 0.7819831411641344\n",
      "Epoch 62, Training Loss: 0.7825345673955473\n",
      "Epoch 63, Training Loss: 0.7823334228723569\n",
      "Epoch 64, Training Loss: 0.7817127533425066\n",
      "Epoch 65, Training Loss: 0.7807605828557695\n",
      "Epoch 66, Training Loss: 0.7812336826682987\n",
      "Epoch 67, Training Loss: 0.7813672052290207\n",
      "Epoch 68, Training Loss: 0.7810855914775591\n",
      "Epoch 69, Training Loss: 0.7806965001095506\n",
      "Epoch 70, Training Loss: 0.78089029708303\n",
      "Epoch 71, Training Loss: 0.7813071289456877\n",
      "Epoch 72, Training Loss: 0.78161827675382\n",
      "Epoch 73, Training Loss: 0.7807856700474158\n",
      "Epoch 74, Training Loss: 0.7801674373167798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:49:27,613] Trial 107 finished with value: 0.6398 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7799901546392226\n",
      "Epoch 1, Training Loss: 0.9048946395852512\n",
      "Epoch 2, Training Loss: 0.8253809924412491\n",
      "Epoch 3, Training Loss: 0.8172663384810426\n",
      "Epoch 4, Training Loss: 0.8159936153799071\n",
      "Epoch 5, Training Loss: 0.8129274493769596\n",
      "Epoch 6, Training Loss: 0.8123133161014184\n",
      "Epoch 7, Training Loss: 0.8101524647913481\n",
      "Epoch 8, Training Loss: 0.8094968365547353\n",
      "Epoch 9, Training Loss: 0.8084267698732533\n",
      "Epoch 10, Training Loss: 0.8072716768522908\n",
      "Epoch 11, Training Loss: 0.8063762210365525\n",
      "Epoch 12, Training Loss: 0.8069246932976228\n",
      "Epoch 13, Training Loss: 0.8062181712093209\n",
      "Epoch 14, Training Loss: 0.8063432027522782\n",
      "Epoch 15, Training Loss: 0.805331249703142\n",
      "Epoch 16, Training Loss: 0.8049351682340292\n",
      "Epoch 17, Training Loss: 0.8041909490312849\n",
      "Epoch 18, Training Loss: 0.8042572144278907\n",
      "Epoch 19, Training Loss: 0.8039953747190031\n",
      "Epoch 20, Training Loss: 0.803463075573283\n",
      "Epoch 21, Training Loss: 0.8025675032371865\n",
      "Epoch 22, Training Loss: 0.8035663372591922\n",
      "Epoch 23, Training Loss: 0.8028847926541378\n",
      "Epoch 24, Training Loss: 0.8019672990741586\n",
      "Epoch 25, Training Loss: 0.802942352814782\n",
      "Epoch 26, Training Loss: 0.8022245198264157\n",
      "Epoch 27, Training Loss: 0.8015576049797517\n",
      "Epoch 28, Training Loss: 0.8018690587882709\n",
      "Epoch 29, Training Loss: 0.8012942219139042\n",
      "Epoch 30, Training Loss: 0.8016793173954899\n",
      "Epoch 31, Training Loss: 0.8014851195471627\n",
      "Epoch 32, Training Loss: 0.8012365197776852\n",
      "Epoch 33, Training Loss: 0.8006997131763545\n",
      "Epoch 34, Training Loss: 0.8004203283697142\n",
      "Epoch 35, Training Loss: 0.8005175366437525\n",
      "Epoch 36, Training Loss: 0.8010752525544704\n",
      "Epoch 37, Training Loss: 0.8004209381297118\n",
      "Epoch 38, Training Loss: 0.8007213452704867\n",
      "Epoch 39, Training Loss: 0.8007776910201051\n",
      "Epoch 40, Training Loss: 0.7998821042085948\n",
      "Epoch 41, Training Loss: 0.8002877108136514\n",
      "Epoch 42, Training Loss: 0.8000583753549964\n",
      "Epoch 43, Training Loss: 0.8001230487249847\n",
      "Epoch 44, Training Loss: 0.799575558461641\n",
      "Epoch 45, Training Loss: 0.7995463468078384\n",
      "Epoch 46, Training Loss: 0.799886909821876\n",
      "Epoch 47, Training Loss: 0.799167289411215\n",
      "Epoch 48, Training Loss: 0.7996212322012822\n",
      "Epoch 49, Training Loss: 0.7989404833406434\n",
      "Epoch 50, Training Loss: 0.7988422589194506\n",
      "Epoch 51, Training Loss: 0.7988331866443605\n",
      "Epoch 52, Training Loss: 0.7989395242884643\n",
      "Epoch 53, Training Loss: 0.7992475183386551\n",
      "Epoch 54, Training Loss: 0.798520098772264\n",
      "Epoch 55, Training Loss: 0.797989132977966\n",
      "Epoch 56, Training Loss: 0.798082761567338\n",
      "Epoch 57, Training Loss: 0.7978736888197132\n",
      "Epoch 58, Training Loss: 0.7974693123559307\n",
      "Epoch 59, Training Loss: 0.7969931269050541\n",
      "Epoch 60, Training Loss: 0.7965236105416951\n",
      "Epoch 61, Training Loss: 0.7973107935790729\n",
      "Epoch 62, Training Loss: 0.7965668908187321\n",
      "Epoch 63, Training Loss: 0.7969431627961926\n",
      "Epoch 64, Training Loss: 0.7963990796777539\n",
      "Epoch 65, Training Loss: 0.7960615012878762\n",
      "Epoch 66, Training Loss: 0.7966288606923325\n",
      "Epoch 67, Training Loss: 0.7963571988550344\n",
      "Epoch 68, Training Loss: 0.796807575046568\n",
      "Epoch 69, Training Loss: 0.7959506883657068\n",
      "Epoch 70, Training Loss: 0.7954458341562658\n",
      "Epoch 71, Training Loss: 0.795071936729259\n",
      "Epoch 72, Training Loss: 0.7956458921719315\n",
      "Epoch 73, Training Loss: 0.7961011674171103\n",
      "Epoch 74, Training Loss: 0.7951565294337452\n",
      "Epoch 75, Training Loss: 0.7945257318647284\n",
      "Epoch 76, Training Loss: 0.7945414860445754\n",
      "Epoch 77, Training Loss: 0.7946547246517095\n",
      "Epoch 78, Training Loss: 0.7941539949940559\n",
      "Epoch 79, Training Loss: 0.7938956732588603\n",
      "Epoch 80, Training Loss: 0.7943694282295113\n",
      "Epoch 81, Training Loss: 0.7941754198163972\n",
      "Epoch 82, Training Loss: 0.7944589265307089\n",
      "Epoch 83, Training Loss: 0.7942628309242707\n",
      "Epoch 84, Training Loss: 0.7944020468489568\n",
      "Epoch 85, Training Loss: 0.7939447602831331\n",
      "Epoch 86, Training Loss: 0.79330110827783\n",
      "Epoch 87, Training Loss: 0.7932393530705818\n",
      "Epoch 88, Training Loss: 0.7935675871103329\n",
      "Epoch 89, Training Loss: 0.7932825602983173\n",
      "Epoch 90, Training Loss: 0.7934367207656229\n",
      "Epoch 91, Training Loss: 0.7928789005243688\n",
      "Epoch 92, Training Loss: 0.7929535994852396\n",
      "Epoch 93, Training Loss: 0.7933910461296713\n",
      "Epoch 94, Training Loss: 0.7929430640729747\n",
      "Epoch 95, Training Loss: 0.793728530675845\n",
      "Epoch 96, Training Loss: 0.7930547165691404\n",
      "Epoch 97, Training Loss: 0.793256914436369\n",
      "Epoch 98, Training Loss: 0.7924488271089425\n",
      "Epoch 99, Training Loss: 0.7924442304704422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:51:01,232] Trial 108 finished with value: 0.6162666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7919850859426915\n",
      "Epoch 1, Training Loss: 0.8484807860582395\n",
      "Epoch 2, Training Loss: 0.8179687966977743\n",
      "Epoch 3, Training Loss: 0.8171640176522105\n",
      "Epoch 4, Training Loss: 0.8151383728013003\n",
      "Epoch 5, Training Loss: 0.813385352156216\n",
      "Epoch 6, Training Loss: 0.8100743146767294\n",
      "Epoch 7, Training Loss: 0.811676553855265\n",
      "Epoch 8, Training Loss: 0.8108227970456718\n",
      "Epoch 9, Training Loss: 0.8090294829884866\n",
      "Epoch 10, Training Loss: 0.810656802457078\n",
      "Epoch 11, Training Loss: 0.8081453614665154\n",
      "Epoch 12, Training Loss: 0.8142017956066848\n",
      "Epoch 13, Training Loss: 0.8074470439351591\n",
      "Epoch 14, Training Loss: 0.8065120439780386\n",
      "Epoch 15, Training Loss: 0.8086840128540096\n",
      "Epoch 16, Training Loss: 0.8106486752517241\n",
      "Epoch 17, Training Loss: 0.8054383052022833\n",
      "Epoch 18, Training Loss: 0.8078579679467625\n",
      "Epoch 19, Training Loss: 0.809381479786751\n",
      "Epoch 20, Training Loss: 0.8073356969015939\n",
      "Epoch 21, Training Loss: 0.8061415205324503\n",
      "Epoch 22, Training Loss: 0.8088301733920449\n",
      "Epoch 23, Training Loss: 0.8076570389414193\n",
      "Epoch 24, Training Loss: 0.8089348252554586\n",
      "Epoch 25, Training Loss: 0.8043512658069009\n",
      "Epoch 26, Training Loss: 0.8081699474413592\n",
      "Epoch 27, Training Loss: 0.8102080833643003\n",
      "Epoch 28, Training Loss: 0.8071931955509616\n",
      "Epoch 29, Training Loss: 0.8051375044019599\n",
      "Epoch 30, Training Loss: 0.8080178241980703\n",
      "Epoch 31, Training Loss: 0.8061821733202253\n",
      "Epoch 32, Training Loss: 0.8073402229108309\n",
      "Epoch 33, Training Loss: 0.8077071880039416\n",
      "Epoch 34, Training Loss: 0.8098182818047086\n",
      "Epoch 35, Training Loss: 0.8062161658043252\n",
      "Epoch 36, Training Loss: 0.8048328176476902\n",
      "Epoch 37, Training Loss: 0.8050978910205956\n",
      "Epoch 38, Training Loss: 0.8056130544583601\n",
      "Epoch 39, Training Loss: 0.8108522705565718\n",
      "Epoch 40, Training Loss: 0.8076948242976253\n",
      "Epoch 41, Training Loss: 0.8061513937505564\n",
      "Epoch 42, Training Loss: 0.8071774091935695\n",
      "Epoch 43, Training Loss: 0.803853270075375\n",
      "Epoch 44, Training Loss: 0.8104215646148625\n",
      "Epoch 45, Training Loss: 0.8067295657960992\n",
      "Epoch 46, Training Loss: 0.8047544846857401\n",
      "Epoch 47, Training Loss: 0.809306538105011\n",
      "Epoch 48, Training Loss: 0.8072559656953453\n",
      "Epoch 49, Training Loss: 0.8062780407138337\n",
      "Epoch 50, Training Loss: 0.808442461221738\n",
      "Epoch 51, Training Loss: 0.8066319460259345\n",
      "Epoch 52, Training Loss: 0.809042953907099\n",
      "Epoch 53, Training Loss: 0.8046739270812586\n",
      "Epoch 54, Training Loss: 0.8104188642107455\n",
      "Epoch 55, Training Loss: 0.8074378125649646\n",
      "Epoch 56, Training Loss: 0.8095124416781547\n",
      "Epoch 57, Training Loss: 0.8055417892179991\n",
      "Epoch 58, Training Loss: 0.8089931120549826\n",
      "Epoch 59, Training Loss: 0.814209947281314\n",
      "Epoch 60, Training Loss: 0.8076112168175834\n",
      "Epoch 61, Training Loss: 0.8097582946146341\n",
      "Epoch 62, Training Loss: 0.8077926864301351\n",
      "Epoch 63, Training Loss: 0.8138713215526782\n",
      "Epoch 64, Training Loss: 0.8112468437144631\n",
      "Epoch 65, Training Loss: 0.8107107391931061\n",
      "Epoch 66, Training Loss: 0.8054406510259872\n",
      "Epoch 67, Training Loss: 0.8078730931855682\n",
      "Epoch 68, Training Loss: 0.809743843222023\n",
      "Epoch 69, Training Loss: 0.8065791039538562\n",
      "Epoch 70, Training Loss: 0.8043853501628216\n",
      "Epoch 71, Training Loss: 0.8095235350436734\n",
      "Epoch 72, Training Loss: 0.814136975421045\n",
      "Epoch 73, Training Loss: 0.810366392987115\n",
      "Epoch 74, Training Loss: 0.8079884604403847\n",
      "Epoch 75, Training Loss: 0.8090837299375606\n",
      "Epoch 76, Training Loss: 0.8102328889352038\n",
      "Epoch 77, Training Loss: 0.8069905618975933\n",
      "Epoch 78, Training Loss: 0.8107529641094064\n",
      "Epoch 79, Training Loss: 0.8069261495332073\n",
      "Epoch 80, Training Loss: 0.8153832179263122\n",
      "Epoch 81, Training Loss: 0.8111018074186225\n",
      "Epoch 82, Training Loss: 0.8064333092897458\n",
      "Epoch 83, Training Loss: 0.8080431500323733\n",
      "Epoch 84, Training Loss: 0.8106950255264913\n",
      "Epoch 85, Training Loss: 0.807425057888031\n",
      "Epoch 86, Training Loss: 0.8140352206122606\n",
      "Epoch 87, Training Loss: 0.8117498799822385\n",
      "Epoch 88, Training Loss: 0.8132521272601938\n",
      "Epoch 89, Training Loss: 0.8157495535405955\n",
      "Epoch 90, Training Loss: 0.8155245368642018\n",
      "Epoch 91, Training Loss: 0.8167195623082326\n",
      "Epoch 92, Training Loss: 0.8221171597789104\n",
      "Epoch 93, Training Loss: 0.8249891688949184\n",
      "Epoch 94, Training Loss: 0.8236471846587676\n",
      "Epoch 95, Training Loss: 0.8283082717343381\n",
      "Epoch 96, Training Loss: 0.827015281171727\n",
      "Epoch 97, Training Loss: 0.8240601162265118\n",
      "Epoch 98, Training Loss: 0.8279158152135692\n",
      "Epoch 99, Training Loss: 0.8221359223351443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:52:58,942] Trial 109 finished with value: 0.5724666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8221071676204079\n",
      "Epoch 1, Training Loss: 0.8834381654683281\n",
      "Epoch 2, Training Loss: 0.8146192345899694\n",
      "Epoch 3, Training Loss: 0.81050956270274\n",
      "Epoch 4, Training Loss: 0.8094987508829902\n",
      "Epoch 5, Training Loss: 0.8083463971755084\n",
      "Epoch 6, Training Loss: 0.8055996378730325\n",
      "Epoch 7, Training Loss: 0.8025661005693323\n",
      "Epoch 8, Training Loss: 0.7982352131955763\n",
      "Epoch 9, Training Loss: 0.7969589764230391\n",
      "Epoch 10, Training Loss: 0.7951170269881978\n",
      "Epoch 11, Training Loss: 0.7936517458803514\n",
      "Epoch 12, Training Loss: 0.7941870415911955\n",
      "Epoch 13, Training Loss: 0.7927072527829339\n",
      "Epoch 14, Training Loss: 0.7915047430290896\n",
      "Epoch 15, Training Loss: 0.7924658840544083\n",
      "Epoch 16, Training Loss: 0.7907539022670073\n",
      "Epoch 17, Training Loss: 0.790765791570439\n",
      "Epoch 18, Training Loss: 0.7894098932602827\n",
      "Epoch 19, Training Loss: 0.7898670386566835\n",
      "Epoch 20, Training Loss: 0.7889928002918468\n",
      "Epoch 21, Training Loss: 0.7886031914458556\n",
      "Epoch 22, Training Loss: 0.7889644423653098\n",
      "Epoch 23, Training Loss: 0.7893298374204074\n",
      "Epoch 24, Training Loss: 0.7878693957889781\n",
      "Epoch 25, Training Loss: 0.7877053085495443\n",
      "Epoch 26, Training Loss: 0.7879036617279053\n",
      "Epoch 27, Training Loss: 0.7875869337951436\n",
      "Epoch 28, Training Loss: 0.7874123249334447\n",
      "Epoch 29, Training Loss: 0.7864410916496726\n",
      "Epoch 30, Training Loss: 0.7857262646450716\n",
      "Epoch 31, Training Loss: 0.7858938315335442\n",
      "Epoch 32, Training Loss: 0.7867538330134224\n",
      "Epoch 33, Training Loss: 0.7867705102527843\n",
      "Epoch 34, Training Loss: 0.786608816245023\n",
      "Epoch 35, Training Loss: 0.7856689774990082\n",
      "Epoch 36, Training Loss: 0.7860126740090987\n",
      "Epoch 37, Training Loss: 0.7854071132575764\n",
      "Epoch 38, Training Loss: 0.7856514033850501\n",
      "Epoch 39, Training Loss: 0.7851810876762165\n",
      "Epoch 40, Training Loss: 0.7856511953999015\n",
      "Epoch 41, Training Loss: 0.7846944581059848\n",
      "Epoch 42, Training Loss: 0.784942086093566\n",
      "Epoch 43, Training Loss: 0.7848848861105302\n",
      "Epoch 44, Training Loss: 0.7847735850250019\n",
      "Epoch 45, Training Loss: 0.7845307006555445\n",
      "Epoch 46, Training Loss: 0.7847961835300221\n",
      "Epoch 47, Training Loss: 0.7839610535958234\n",
      "Epoch 48, Training Loss: 0.784632573408239\n",
      "Epoch 49, Training Loss: 0.7843766890553867\n",
      "Epoch 50, Training Loss: 0.7841316570253933\n",
      "Epoch 51, Training Loss: 0.7845404028191286\n",
      "Epoch 52, Training Loss: 0.7834930760720197\n",
      "Epoch 53, Training Loss: 0.7844931978337905\n",
      "Epoch 54, Training Loss: 0.784554763471379\n",
      "Epoch 55, Training Loss: 0.7841639741729287\n",
      "Epoch 56, Training Loss: 0.7833351891181048\n",
      "Epoch 57, Training Loss: 0.7833220555501825\n",
      "Epoch 58, Training Loss: 0.78321552381796\n",
      "Epoch 59, Training Loss: 0.7837914484388688\n",
      "Epoch 60, Training Loss: 0.7827959187591778\n",
      "Epoch 61, Training Loss: 0.782361110238468\n",
      "Epoch 62, Training Loss: 0.7829735412317164\n",
      "Epoch 63, Training Loss: 0.7831177236753352\n",
      "Epoch 64, Training Loss: 0.7829317838304183\n",
      "Epoch 65, Training Loss: 0.7831631415731767\n",
      "Epoch 66, Training Loss: 0.7818806506605709\n",
      "Epoch 67, Training Loss: 0.7821434307098388\n",
      "Epoch 68, Training Loss: 0.7824624851170708\n",
      "Epoch 69, Training Loss: 0.7824321101693547\n",
      "Epoch 70, Training Loss: 0.7829346459052142\n",
      "Epoch 71, Training Loss: 0.7818827914490419\n",
      "Epoch 72, Training Loss: 0.7817993419310626\n",
      "Epoch 73, Training Loss: 0.781580237641054\n",
      "Epoch 74, Training Loss: 0.7820737551941591\n",
      "Epoch 75, Training Loss: 0.7822919610668632\n",
      "Epoch 76, Training Loss: 0.78147630053408\n",
      "Epoch 77, Training Loss: 0.7814206131065593\n",
      "Epoch 78, Training Loss: 0.7813724204371957\n",
      "Epoch 79, Training Loss: 0.781678908923093\n",
      "Epoch 80, Training Loss: 0.7815356805745293\n",
      "Epoch 81, Training Loss: 0.7815687981072594\n",
      "Epoch 82, Training Loss: 0.7812406044847825\n",
      "Epoch 83, Training Loss: 0.781024681189481\n",
      "Epoch 84, Training Loss: 0.7807821070446688\n",
      "Epoch 85, Training Loss: 0.781455116482342\n",
      "Epoch 86, Training Loss: 0.7813946527593276\n",
      "Epoch 87, Training Loss: 0.7813317305901472\n",
      "Epoch 88, Training Loss: 0.7809848116425907\n",
      "Epoch 89, Training Loss: 0.7809604831302868\n",
      "Epoch 90, Training Loss: 0.7809649632958805\n",
      "Epoch 91, Training Loss: 0.780457634084365\n",
      "Epoch 92, Training Loss: 0.7808336450071895\n",
      "Epoch 93, Training Loss: 0.780387889287051\n",
      "Epoch 94, Training Loss: 0.7801540280089659\n",
      "Epoch 95, Training Loss: 0.7803630671080422\n",
      "Epoch 96, Training Loss: 0.7808048599607804\n",
      "Epoch 97, Training Loss: 0.7804923197802376\n",
      "Epoch 98, Training Loss: 0.7803079358269187\n",
      "Epoch 99, Training Loss: 0.7800728282507728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:55:10,700] Trial 110 finished with value: 0.6367333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7802153373465819\n",
      "Epoch 1, Training Loss: 0.8699948067951919\n",
      "Epoch 2, Training Loss: 0.8139798155404572\n",
      "Epoch 3, Training Loss: 0.80875596650561\n",
      "Epoch 4, Training Loss: 0.8045335220215016\n",
      "Epoch 5, Training Loss: 0.8015997775515219\n",
      "Epoch 6, Training Loss: 0.797238464552657\n",
      "Epoch 7, Training Loss: 0.7941361491841481\n",
      "Epoch 8, Training Loss: 0.7937375895959095\n",
      "Epoch 9, Training Loss: 0.7916557524437294\n",
      "Epoch 10, Training Loss: 0.7896650584120499\n",
      "Epoch 11, Training Loss: 0.7883697435820013\n",
      "Epoch 12, Training Loss: 0.7893773929517072\n",
      "Epoch 13, Training Loss: 0.7880608279902236\n",
      "Epoch 14, Training Loss: 0.7884600723596443\n",
      "Epoch 15, Training Loss: 0.7871350079550778\n",
      "Epoch 16, Training Loss: 0.7873384381595411\n",
      "Epoch 17, Training Loss: 0.7871601154929713\n",
      "Epoch 18, Training Loss: 0.7877611820859121\n",
      "Epoch 19, Training Loss: 0.7863277563475128\n",
      "Epoch 20, Training Loss: 0.7870963046425268\n",
      "Epoch 21, Training Loss: 0.7867219002623307\n",
      "Epoch 22, Training Loss: 0.7861167741897411\n",
      "Epoch 23, Training Loss: 0.7858972909755276\n",
      "Epoch 24, Training Loss: 0.78583997586616\n",
      "Epoch 25, Training Loss: 0.7857434689550471\n",
      "Epoch 26, Training Loss: 0.7864430556619973\n",
      "Epoch 27, Training Loss: 0.7853176014763968\n",
      "Epoch 28, Training Loss: 0.7841989655691878\n",
      "Epoch 29, Training Loss: 0.7845283871306512\n",
      "Epoch 30, Training Loss: 0.7844979287986469\n",
      "Epoch 31, Training Loss: 0.7850658788716882\n",
      "Epoch 32, Training Loss: 0.7852381737608659\n",
      "Epoch 33, Training Loss: 0.7847331324018034\n",
      "Epoch 34, Training Loss: 0.7836826620245338\n",
      "Epoch 35, Training Loss: 0.7842193008365488\n",
      "Epoch 36, Training Loss: 0.7836659539911084\n",
      "Epoch 37, Training Loss: 0.7836432662225308\n",
      "Epoch 38, Training Loss: 0.7842348457279061\n",
      "Epoch 39, Training Loss: 0.7838275301725345\n",
      "Epoch 40, Training Loss: 0.784301609742014\n",
      "Epoch 41, Training Loss: 0.7831548963274274\n",
      "Epoch 42, Training Loss: 0.7833842107227871\n",
      "Epoch 43, Training Loss: 0.7826416006661896\n",
      "Epoch 44, Training Loss: 0.7824281152029683\n",
      "Epoch 45, Training Loss: 0.7831558024076591\n",
      "Epoch 46, Training Loss: 0.7829040443090568\n",
      "Epoch 47, Training Loss: 0.7821449108589861\n",
      "Epoch 48, Training Loss: 0.7823854370224744\n",
      "Epoch 49, Training Loss: 0.7822629568272067\n",
      "Epoch 50, Training Loss: 0.782687012593549\n",
      "Epoch 51, Training Loss: 0.7831038459799343\n",
      "Epoch 52, Training Loss: 0.781835575211317\n",
      "Epoch 53, Training Loss: 0.7820188685467369\n",
      "Epoch 54, Training Loss: 0.7812638186870661\n",
      "Epoch 55, Training Loss: 0.7816947968382585\n",
      "Epoch 56, Training Loss: 0.7821225940733028\n",
      "Epoch 57, Training Loss: 0.7811202425705759\n",
      "Epoch 58, Training Loss: 0.7817361508096967\n",
      "Epoch 59, Training Loss: 0.7819909063496984\n",
      "Epoch 60, Training Loss: 0.7810331589297245\n",
      "Epoch 61, Training Loss: 0.7812274239117042\n",
      "Epoch 62, Training Loss: 0.7812964635684078\n",
      "Epoch 63, Training Loss: 0.7816063569900685\n",
      "Epoch 64, Training Loss: 0.7813619748094028\n",
      "Epoch 65, Training Loss: 0.7808530984068276\n",
      "Epoch 66, Training Loss: 0.7804115278380258\n",
      "Epoch 67, Training Loss: 0.7802806331699056\n",
      "Epoch 68, Training Loss: 0.7808653422764369\n",
      "Epoch 69, Training Loss: 0.7809097621673928\n",
      "Epoch 70, Training Loss: 0.7807894249607746\n",
      "Epoch 71, Training Loss: 0.7800420121142738\n",
      "Epoch 72, Training Loss: 0.7803947647711388\n",
      "Epoch 73, Training Loss: 0.7802477176924397\n",
      "Epoch 74, Training Loss: 0.7799531652515096\n",
      "Epoch 75, Training Loss: 0.7802578904574975\n",
      "Epoch 76, Training Loss: 0.779703978936475\n",
      "Epoch 77, Training Loss: 0.7797375117925773\n",
      "Epoch 78, Training Loss: 0.7791806719805064\n",
      "Epoch 79, Training Loss: 0.779666779363962\n",
      "Epoch 80, Training Loss: 0.7798308458543362\n",
      "Epoch 81, Training Loss: 0.7796623136764183\n",
      "Epoch 82, Training Loss: 0.7800870877459534\n",
      "Epoch 83, Training Loss: 0.7786868288552851\n",
      "Epoch 84, Training Loss: 0.7800480819286261\n",
      "Epoch 85, Training Loss: 0.7794294840411136\n",
      "Epoch 86, Training Loss: 0.780461631681686\n",
      "Epoch 87, Training Loss: 0.7793084738846112\n",
      "Epoch 88, Training Loss: 0.7791595673202572\n",
      "Epoch 89, Training Loss: 0.7794596577945508\n",
      "Epoch 90, Training Loss: 0.7790701769348374\n",
      "Epoch 91, Training Loss: 0.7796525832405664\n",
      "Epoch 92, Training Loss: 0.7790092314992633\n",
      "Epoch 93, Training Loss: 0.7786675705945582\n",
      "Epoch 94, Training Loss: 0.7792235231041011\n",
      "Epoch 95, Training Loss: 0.7783998979661698\n",
      "Epoch 96, Training Loss: 0.7788684380681892\n",
      "Epoch 97, Training Loss: 0.779399922198819\n",
      "Epoch 98, Training Loss: 0.7790098592751008\n",
      "Epoch 99, Training Loss: 0.77856197294436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:57:20,578] Trial 111 finished with value: 0.6379333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7785364471880117\n",
      "Epoch 1, Training Loss: 0.8452699386372285\n",
      "Epoch 2, Training Loss: 0.8165113125829135\n",
      "Epoch 3, Training Loss: 0.8111273867242477\n",
      "Epoch 4, Training Loss: 0.8080539757363936\n",
      "Epoch 5, Training Loss: 0.8075120630685021\n",
      "Epoch 6, Training Loss: 0.8046267980687758\n",
      "Epoch 7, Training Loss: 0.8055429744720459\n",
      "Epoch 8, Training Loss: 0.8030554007081424\n",
      "Epoch 9, Training Loss: 0.8028537716585047\n",
      "Epoch 10, Training Loss: 0.8043340295202592\n",
      "Epoch 11, Training Loss: 0.8046531154828913\n",
      "Epoch 12, Training Loss: 0.802066915806602\n",
      "Epoch 13, Training Loss: 0.8025764267584857\n",
      "Epoch 14, Training Loss: 0.8036252897627213\n",
      "Epoch 15, Training Loss: 0.8016957161707037\n",
      "Epoch 16, Training Loss: 0.7998575495972353\n",
      "Epoch 17, Training Loss: 0.8018739663152133\n",
      "Epoch 18, Training Loss: 0.8002589830230264\n",
      "Epoch 19, Training Loss: 0.8010132014751434\n",
      "Epoch 20, Training Loss: 0.8023985814346987\n",
      "Epoch 21, Training Loss: 0.7999600988977096\n",
      "Epoch 22, Training Loss: 0.8008073241570417\n",
      "Epoch 23, Training Loss: 0.8032838578083936\n",
      "Epoch 24, Training Loss: 0.8004592992277706\n",
      "Epoch 25, Training Loss: 0.8007226792503805\n",
      "Epoch 26, Training Loss: 0.8004798433359932\n",
      "Epoch 27, Training Loss: 0.8011788768628064\n",
      "Epoch 28, Training Loss: 0.8013726170624004\n",
      "Epoch 29, Training Loss: 0.8002213027196772\n",
      "Epoch 30, Training Loss: 0.8032554892932667\n",
      "Epoch 31, Training Loss: 0.7995934350350323\n",
      "Epoch 32, Training Loss: 0.8008744049072266\n",
      "Epoch 33, Training Loss: 0.8005932104587555\n",
      "Epoch 34, Training Loss: 0.8002466083975399\n",
      "Epoch 35, Training Loss: 0.7996922499292037\n",
      "Epoch 36, Training Loss: 0.8017825076159308\n",
      "Epoch 37, Training Loss: 0.7975777224232169\n",
      "Epoch 38, Training Loss: 0.7987083463107838\n",
      "Epoch 39, Training Loss: 0.7997111384307637\n",
      "Epoch 40, Training Loss: 0.8004974719356088\n",
      "Epoch 41, Training Loss: 0.800226613703896\n",
      "Epoch 42, Training Loss: 0.7980564679818994\n",
      "Epoch 43, Training Loss: 0.79989546733744\n",
      "Epoch 44, Training Loss: 0.8003645887094385\n",
      "Epoch 45, Training Loss: 0.8000290360170252\n",
      "Epoch 46, Training Loss: 0.7999879155439489\n",
      "Epoch 47, Training Loss: 0.8000604304145364\n",
      "Epoch 48, Training Loss: 0.800407600402832\n",
      "Epoch 49, Training Loss: 0.799136492504793\n",
      "Epoch 50, Training Loss: 0.799521425892325\n",
      "Epoch 51, Training Loss: 0.8011940454735476\n",
      "Epoch 52, Training Loss: 0.7978134004508748\n",
      "Epoch 53, Training Loss: 0.800273704809301\n",
      "Epoch 54, Training Loss: 0.7988248158903682\n",
      "Epoch 55, Training Loss: 0.7994741446130416\n",
      "Epoch 56, Training Loss: 0.8018729263894698\n",
      "Epoch 57, Training Loss: 0.7994651243265938\n",
      "Epoch 58, Training Loss: 0.8008665106577032\n",
      "Epoch 59, Training Loss: 0.8004383925129386\n",
      "Epoch 60, Training Loss: 0.7988544194137349\n",
      "Epoch 61, Training Loss: 0.8024532077592962\n",
      "Epoch 62, Training Loss: 0.8017836765681996\n",
      "Epoch 63, Training Loss: 0.8009816027388853\n",
      "Epoch 64, Training Loss: 0.8011566967823927\n",
      "Epoch 65, Training Loss: 0.7995609913854038\n",
      "Epoch 66, Training Loss: 0.8009879415175494\n",
      "Epoch 67, Training Loss: 0.7994361700029934\n",
      "Epoch 68, Training Loss: 0.8018100869655609\n",
      "Epoch 69, Training Loss: 0.7991982992256389\n",
      "Epoch 70, Training Loss: 0.7995094250230228\n",
      "Epoch 71, Training Loss: 0.8003413822370417\n",
      "Epoch 72, Training Loss: 0.8016733235471389\n",
      "Epoch 73, Training Loss: 0.8005351272050072\n",
      "Epoch 74, Training Loss: 0.7994648833134596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 22:59:12,578] Trial 112 finished with value: 0.6315333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.799932791555629\n",
      "Epoch 1, Training Loss: 0.8953405656312642\n",
      "Epoch 2, Training Loss: 0.8237799870340448\n",
      "Epoch 3, Training Loss: 0.816500619121064\n",
      "Epoch 4, Training Loss: 0.8121979077059523\n",
      "Epoch 5, Training Loss: 0.808692149022468\n",
      "Epoch 6, Training Loss: 0.8049711832426545\n",
      "Epoch 7, Training Loss: 0.8024783032281059\n",
      "Epoch 8, Training Loss: 0.8007624078513984\n",
      "Epoch 9, Training Loss: 0.7991872878002941\n",
      "Epoch 10, Training Loss: 0.7978775957473239\n",
      "Epoch 11, Training Loss: 0.7960190608985442\n",
      "Epoch 12, Training Loss: 0.7949311179325993\n",
      "Epoch 13, Training Loss: 0.7945244453903427\n",
      "Epoch 14, Training Loss: 0.7926895540459712\n",
      "Epoch 15, Training Loss: 0.7923881335366041\n",
      "Epoch 16, Training Loss: 0.7913630706027038\n",
      "Epoch 17, Training Loss: 0.7907668158524018\n",
      "Epoch 18, Training Loss: 0.7908760705388578\n",
      "Epoch 19, Training Loss: 0.7906217996339152\n",
      "Epoch 20, Training Loss: 0.7900485515594482\n",
      "Epoch 21, Training Loss: 0.7896317155737625\n",
      "Epoch 22, Training Loss: 0.7897278368024898\n",
      "Epoch 23, Training Loss: 0.7899676517436379\n",
      "Epoch 24, Training Loss: 0.7894557470665838\n",
      "Epoch 25, Training Loss: 0.7888099179232031\n",
      "Epoch 26, Training Loss: 0.7890427546393602\n",
      "Epoch 27, Training Loss: 0.7888160486866657\n",
      "Epoch 28, Training Loss: 0.7887373327312613\n",
      "Epoch 29, Training Loss: 0.7874469083950932\n",
      "Epoch 30, Training Loss: 0.7879960898169898\n",
      "Epoch 31, Training Loss: 0.788036680938606\n",
      "Epoch 32, Training Loss: 0.78804989291313\n",
      "Epoch 33, Training Loss: 0.7881064089617335\n",
      "Epoch 34, Training Loss: 0.7873659811521831\n",
      "Epoch 35, Training Loss: 0.7875031304538698\n",
      "Epoch 36, Training Loss: 0.7869222646369074\n",
      "Epoch 37, Training Loss: 0.786676075315117\n",
      "Epoch 38, Training Loss: 0.7868367533038433\n",
      "Epoch 39, Training Loss: 0.7859290371263834\n",
      "Epoch 40, Training Loss: 0.786834226246167\n",
      "Epoch 41, Training Loss: 0.7865323791826578\n",
      "Epoch 42, Training Loss: 0.7864577945910002\n",
      "Epoch 43, Training Loss: 0.7858859998839242\n",
      "Epoch 44, Training Loss: 0.7854471103589338\n",
      "Epoch 45, Training Loss: 0.785385609300513\n",
      "Epoch 46, Training Loss: 0.7855873100739673\n",
      "Epoch 47, Training Loss: 0.7850051598441332\n",
      "Epoch 48, Training Loss: 0.785228112855352\n",
      "Epoch 49, Training Loss: 0.7857737774239447\n",
      "Epoch 50, Training Loss: 0.7852969739670144\n",
      "Epoch 51, Training Loss: 0.7851495853044037\n",
      "Epoch 52, Training Loss: 0.7855590428624835\n",
      "Epoch 53, Training Loss: 0.7849622241536477\n",
      "Epoch 54, Training Loss: 0.7847233664720579\n",
      "Epoch 55, Training Loss: 0.7856482727187021\n",
      "Epoch 56, Training Loss: 0.7843306487664244\n",
      "Epoch 57, Training Loss: 0.7843282694207099\n",
      "Epoch 58, Training Loss: 0.78483036541401\n",
      "Epoch 59, Training Loss: 0.7841402233991408\n",
      "Epoch 60, Training Loss: 0.7844605528322378\n",
      "Epoch 61, Training Loss: 0.7844776273670053\n",
      "Epoch 62, Training Loss: 0.7845417671633842\n",
      "Epoch 63, Training Loss: 0.7838339238238514\n",
      "Epoch 64, Training Loss: 0.7841420289269068\n",
      "Epoch 65, Training Loss: 0.783606573692838\n",
      "Epoch 66, Training Loss: 0.783589966583969\n",
      "Epoch 67, Training Loss: 0.7837550836398189\n",
      "Epoch 68, Training Loss: 0.7835080124381789\n",
      "Epoch 69, Training Loss: 0.7826204001007223\n",
      "Epoch 70, Training Loss: 0.783515268967564\n",
      "Epoch 71, Training Loss: 0.7838467084375539\n",
      "Epoch 72, Training Loss: 0.7835241729155519\n",
      "Epoch 73, Training Loss: 0.7843916116800523\n",
      "Epoch 74, Training Loss: 0.782894630181162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:00:32,258] Trial 113 finished with value: 0.6384 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.783996680087613\n",
      "Epoch 1, Training Loss: 0.8463555122123045\n",
      "Epoch 2, Training Loss: 0.8136104608984555\n",
      "Epoch 3, Training Loss: 0.805374189895742\n",
      "Epoch 4, Training Loss: 0.8031732432982501\n",
      "Epoch 5, Training Loss: 0.8016798637895023\n",
      "Epoch 6, Training Loss: 0.8010295857401455\n",
      "Epoch 7, Training Loss: 0.8005582870455349\n",
      "Epoch 8, Training Loss: 0.7994683635234833\n",
      "Epoch 9, Training Loss: 0.7999722513731788\n",
      "Epoch 10, Training Loss: 0.7982026572788463\n",
      "Epoch 11, Training Loss: 0.798888401073568\n",
      "Epoch 12, Training Loss: 0.7985719582613777\n",
      "Epoch 13, Training Loss: 0.7982475984797758\n",
      "Epoch 14, Training Loss: 0.7979722268441144\n",
      "Epoch 15, Training Loss: 0.7970220243930817\n",
      "Epoch 16, Training Loss: 0.7975484964426826\n",
      "Epoch 17, Training Loss: 0.7973422841464772\n",
      "Epoch 18, Training Loss: 0.7975530032550587\n",
      "Epoch 19, Training Loss: 0.7969827389015871\n",
      "Epoch 20, Training Loss: 0.7974604946024277\n",
      "Epoch 21, Training Loss: 0.7960007418604458\n",
      "Epoch 22, Training Loss: 0.7962154380714193\n",
      "Epoch 23, Training Loss: 0.7971162293939029\n",
      "Epoch 24, Training Loss: 0.7956050072698032\n",
      "Epoch 25, Training Loss: 0.7963319812101476\n",
      "Epoch 26, Training Loss: 0.7967437241357915\n",
      "Epoch 27, Training Loss: 0.7954021651604596\n",
      "Epoch 28, Training Loss: 0.7951569267581491\n",
      "Epoch 29, Training Loss: 0.7961706812241498\n",
      "Epoch 30, Training Loss: 0.7943128766733057\n",
      "Epoch 31, Training Loss: 0.7946004418064566\n",
      "Epoch 32, Training Loss: 0.7966010423267589\n",
      "Epoch 33, Training Loss: 0.7949599239405464\n",
      "Epoch 34, Training Loss: 0.7948502532173606\n",
      "Epoch 35, Training Loss: 0.794978215343812\n",
      "Epoch 36, Training Loss: 0.7942589447077583\n",
      "Epoch 37, Training Loss: 0.7939736543683444\n",
      "Epoch 38, Training Loss: 0.7958925289967481\n",
      "Epoch 39, Training Loss: 0.794703489892623\n",
      "Epoch 40, Training Loss: 0.7947338189097012\n",
      "Epoch 41, Training Loss: 0.7942042102533228\n",
      "Epoch 42, Training Loss: 0.7951794073160957\n",
      "Epoch 43, Training Loss: 0.7939592504501343\n",
      "Epoch 44, Training Loss: 0.7947471373922684\n",
      "Epoch 45, Training Loss: 0.7944038382698508\n",
      "Epoch 46, Training Loss: 0.7945621347427368\n",
      "Epoch 47, Training Loss: 0.7931160995539497\n",
      "Epoch 48, Training Loss: 0.7943507015705109\n",
      "Epoch 49, Training Loss: 0.7937169318339404\n",
      "Epoch 50, Training Loss: 0.7951402674703038\n",
      "Epoch 51, Training Loss: 0.793539428360322\n",
      "Epoch 52, Training Loss: 0.7932820820808411\n",
      "Epoch 53, Training Loss: 0.7935911543930279\n",
      "Epoch 54, Training Loss: 0.7945770632519441\n",
      "Epoch 55, Training Loss: 0.7938171013663797\n",
      "Epoch 56, Training Loss: 0.7928358733654022\n",
      "Epoch 57, Training Loss: 0.7934453127664678\n",
      "Epoch 58, Training Loss: 0.7944638868640451\n",
      "Epoch 59, Training Loss: 0.7921549062167897\n",
      "Epoch 60, Training Loss: 0.79238341310445\n",
      "Epoch 61, Training Loss: 0.7949276688519646\n",
      "Epoch 62, Training Loss: 0.7932896191232345\n",
      "Epoch 63, Training Loss: 0.79227106311742\n",
      "Epoch 64, Training Loss: 0.7938544466215022\n",
      "Epoch 65, Training Loss: 0.793080828891081\n",
      "Epoch 66, Training Loss: 0.7921273367545184\n",
      "Epoch 67, Training Loss: 0.7934327321894029\n",
      "Epoch 68, Training Loss: 0.7934292092744042\n",
      "Epoch 69, Training Loss: 0.7923070494567647\n",
      "Epoch 70, Training Loss: 0.7924901039460126\n",
      "Epoch 71, Training Loss: 0.7931851530776305\n",
      "Epoch 72, Training Loss: 0.7930450615462135\n",
      "Epoch 73, Training Loss: 0.7933803791158339\n",
      "Epoch 74, Training Loss: 0.7938334573717678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:02:12,321] Trial 114 finished with value: 0.6364666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7937282263531404\n",
      "Epoch 1, Training Loss: 0.9856951771062963\n",
      "Epoch 2, Training Loss: 0.9455099924171673\n",
      "Epoch 3, Training Loss: 0.9344935711692361\n",
      "Epoch 4, Training Loss: 0.9213781245315776\n",
      "Epoch 5, Training Loss: 0.904863276832244\n",
      "Epoch 6, Training Loss: 0.8847374408385333\n",
      "Epoch 7, Training Loss: 0.8636216113847844\n",
      "Epoch 8, Training Loss: 0.8451492329204784\n",
      "Epoch 9, Training Loss: 0.8321662540996776\n",
      "Epoch 10, Training Loss: 0.8237457048892974\n",
      "Epoch 11, Training Loss: 0.8186974551397211\n",
      "Epoch 12, Training Loss: 0.8156752584261052\n",
      "Epoch 13, Training Loss: 0.8137095412086038\n",
      "Epoch 14, Training Loss: 0.8119015726622413\n",
      "Epoch 15, Training Loss: 0.8113630586511948\n",
      "Epoch 16, Training Loss: 0.810284574101953\n",
      "Epoch 17, Training Loss: 0.8095475487849292\n",
      "Epoch 18, Training Loss: 0.8089887076265672\n",
      "Epoch 19, Training Loss: 0.8082998380240272\n",
      "Epoch 20, Training Loss: 0.8076561014792498\n",
      "Epoch 21, Training Loss: 0.8069898149546455\n",
      "Epoch 22, Training Loss: 0.8063911888879888\n",
      "Epoch 23, Training Loss: 0.8060304004304549\n",
      "Epoch 24, Training Loss: 0.8056610638253829\n",
      "Epoch 25, Training Loss: 0.8049845005484189\n",
      "Epoch 26, Training Loss: 0.8045211262562696\n",
      "Epoch 27, Training Loss: 0.8042290410574745\n",
      "Epoch 28, Training Loss: 0.803580980090534\n",
      "Epoch 29, Training Loss: 0.8031603952716379\n",
      "Epoch 30, Training Loss: 0.8029503020819496\n",
      "Epoch 31, Training Loss: 0.8026324722346138\n",
      "Epoch 32, Training Loss: 0.8022871218008154\n",
      "Epoch 33, Training Loss: 0.8018341780410093\n",
      "Epoch 34, Training Loss: 0.8017032807013568\n",
      "Epoch 35, Training Loss: 0.8013842964172363\n",
      "Epoch 36, Training Loss: 0.8011628379541285\n",
      "Epoch 37, Training Loss: 0.8007979469439562\n",
      "Epoch 38, Training Loss: 0.8005912308833179\n",
      "Epoch 39, Training Loss: 0.8005642435831182\n",
      "Epoch 40, Training Loss: 0.8001364888864405\n",
      "Epoch 41, Training Loss: 0.7999482383447535\n",
      "Epoch 42, Training Loss: 0.7997594449800604\n",
      "Epoch 43, Training Loss: 0.7995816741270177\n",
      "Epoch 44, Training Loss: 0.7994384378545425\n",
      "Epoch 45, Training Loss: 0.7993030311079586\n",
      "Epoch 46, Training Loss: 0.799187307918773\n",
      "Epoch 47, Training Loss: 0.7991083720852347\n",
      "Epoch 48, Training Loss: 0.7988929415450376\n",
      "Epoch 49, Training Loss: 0.7987994343392989\n",
      "Epoch 50, Training Loss: 0.7986283905365887\n",
      "Epoch 51, Training Loss: 0.7985949286993812\n",
      "Epoch 52, Training Loss: 0.7981987210582284\n",
      "Epoch 53, Training Loss: 0.7981481183977688\n",
      "Epoch 54, Training Loss: 0.7980525918567882\n",
      "Epoch 55, Training Loss: 0.7979712129340453\n",
      "Epoch 56, Training Loss: 0.7979601473668042\n",
      "Epoch 57, Training Loss: 0.7979195268715129\n",
      "Epoch 58, Training Loss: 0.7977647059104022\n",
      "Epoch 59, Training Loss: 0.7975838254479801\n",
      "Epoch 60, Training Loss: 0.7975101182741278\n",
      "Epoch 61, Training Loss: 0.7974306159159716\n",
      "Epoch 62, Training Loss: 0.7974377544487223\n",
      "Epoch 63, Training Loss: 0.7974416975414051\n",
      "Epoch 64, Training Loss: 0.7973126947879792\n",
      "Epoch 65, Training Loss: 0.7972355245842653\n",
      "Epoch 66, Training Loss: 0.7971945328572217\n",
      "Epoch 67, Training Loss: 0.7970241714225096\n",
      "Epoch 68, Training Loss: 0.7971277690635008\n",
      "Epoch 69, Training Loss: 0.7969450617537779\n",
      "Epoch 70, Training Loss: 0.7968173559974222\n",
      "Epoch 71, Training Loss: 0.796858586844276\n",
      "Epoch 72, Training Loss: 0.796925134939306\n",
      "Epoch 73, Training Loss: 0.7966968654885012\n",
      "Epoch 74, Training Loss: 0.7963981746224796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:03:33,033] Trial 115 finished with value: 0.6351333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7964795708656311\n",
      "Epoch 1, Training Loss: 1.0914549467258883\n",
      "Epoch 2, Training Loss: 1.0911286336138732\n",
      "Epoch 3, Training Loss: 1.0908597946166991\n",
      "Epoch 4, Training Loss: 1.0908040303036683\n",
      "Epoch 5, Training Loss: 1.0906195106362937\n",
      "Epoch 6, Training Loss: 1.090440213232112\n",
      "Epoch 7, Training Loss: 1.0902491800767138\n",
      "Epoch 8, Training Loss: 1.090155481754389\n",
      "Epoch 9, Training Loss: 1.089829685096454\n",
      "Epoch 10, Training Loss: 1.0896057012385891\n",
      "Epoch 11, Training Loss: 1.0892967005421343\n",
      "Epoch 12, Training Loss: 1.088984665655552\n",
      "Epoch 13, Training Loss: 1.0884195161045045\n",
      "Epoch 14, Training Loss: 1.0879415295177832\n",
      "Epoch 15, Training Loss: 1.0871679969300005\n",
      "Epoch 16, Training Loss: 1.0862132741096324\n",
      "Epoch 17, Training Loss: 1.0852397097680802\n",
      "Epoch 18, Training Loss: 1.083772819562066\n",
      "Epoch 19, Training Loss: 1.0816910998265545\n",
      "Epoch 20, Training Loss: 1.07919082336856\n",
      "Epoch 21, Training Loss: 1.0755221705687674\n",
      "Epoch 22, Training Loss: 1.070282168316662\n",
      "Epoch 23, Training Loss: 1.062857004574367\n",
      "Epoch 24, Training Loss: 1.0523953898508747\n",
      "Epoch 25, Training Loss: 1.0389853246229932\n",
      "Epoch 26, Training Loss: 1.023855229338309\n",
      "Epoch 27, Training Loss: 1.0109823566630372\n",
      "Epoch 28, Training Loss: 1.0021748968533106\n",
      "Epoch 29, Training Loss: 0.9974921125218384\n",
      "Epoch 30, Training Loss: 0.9952263811477144\n",
      "Epoch 31, Training Loss: 0.9936709142269049\n",
      "Epoch 32, Training Loss: 0.9919904461480621\n",
      "Epoch 33, Training Loss: 0.9908313652626554\n",
      "Epoch 34, Training Loss: 0.9889144848163863\n",
      "Epoch 35, Training Loss: 0.9871960164909076\n",
      "Epoch 36, Training Loss: 0.9853780755423065\n",
      "Epoch 37, Training Loss: 0.9827144948163427\n",
      "Epoch 38, Training Loss: 0.980362826272061\n",
      "Epoch 39, Training Loss: 0.9771598036127879\n",
      "Epoch 40, Training Loss: 0.9742482540302707\n",
      "Epoch 41, Training Loss: 0.9709966147752632\n",
      "Epoch 42, Training Loss: 0.9672162341892271\n",
      "Epoch 43, Training Loss: 0.963487006040444\n",
      "Epoch 44, Training Loss: 0.9597698684921838\n",
      "Epoch 45, Training Loss: 0.9557653924576321\n",
      "Epoch 46, Training Loss: 0.9527313544337911\n",
      "Epoch 47, Training Loss: 0.949718836017121\n",
      "Epoch 48, Training Loss: 0.9472886359781251\n",
      "Epoch 49, Training Loss: 0.9449044945544767\n",
      "Epoch 50, Training Loss: 0.9430079490618598\n",
      "Epoch 51, Training Loss: 0.9407320069191151\n",
      "Epoch 52, Training Loss: 0.938805725789608\n",
      "Epoch 53, Training Loss: 0.936707289595353\n",
      "Epoch 54, Training Loss: 0.9345438964384839\n",
      "Epoch 55, Training Loss: 0.9322215156447619\n",
      "Epoch 56, Training Loss: 0.9298012912721563\n",
      "Epoch 57, Training Loss: 0.9261812791788488\n",
      "Epoch 58, Training Loss: 0.9225086292826143\n",
      "Epoch 59, Training Loss: 0.9196765675580592\n",
      "Epoch 60, Training Loss: 0.9155085690039441\n",
      "Epoch 61, Training Loss: 0.9105110278703217\n",
      "Epoch 62, Training Loss: 0.9052238874865653\n",
      "Epoch 63, Training Loss: 0.8998005632171058\n",
      "Epoch 64, Training Loss: 0.8937896405843864\n",
      "Epoch 65, Training Loss: 0.8867461160609597\n",
      "Epoch 66, Training Loss: 0.8802178447407888\n",
      "Epoch 67, Training Loss: 0.8733398572842878\n",
      "Epoch 68, Training Loss: 0.8667374688879887\n",
      "Epoch 69, Training Loss: 0.8603275755294284\n",
      "Epoch 70, Training Loss: 0.8551422684712517\n",
      "Epoch 71, Training Loss: 0.8502032005697264\n",
      "Epoch 72, Training Loss: 0.8458277371592988\n",
      "Epoch 73, Training Loss: 0.8424704787426425\n",
      "Epoch 74, Training Loss: 0.8393961064797595\n",
      "Epoch 75, Training Loss: 0.8372882719326736\n",
      "Epoch 76, Training Loss: 0.8348891257791591\n",
      "Epoch 77, Training Loss: 0.833590277065908\n",
      "Epoch 78, Training Loss: 0.8322923208537855\n",
      "Epoch 79, Training Loss: 0.8308288683568624\n",
      "Epoch 80, Training Loss: 0.8296036612718625\n",
      "Epoch 81, Training Loss: 0.8285375889978911\n",
      "Epoch 82, Training Loss: 0.8273349733280956\n",
      "Epoch 83, Training Loss: 0.8269003658366383\n",
      "Epoch 84, Training Loss: 0.8256107239794911\n",
      "Epoch 85, Training Loss: 0.8244912311546785\n",
      "Epoch 86, Training Loss: 0.8238933384866642\n",
      "Epoch 87, Training Loss: 0.823529897506972\n",
      "Epoch 88, Training Loss: 0.8220729156544334\n",
      "Epoch 89, Training Loss: 0.8210275111341835\n",
      "Epoch 90, Training Loss: 0.8204988749403702\n",
      "Epoch 91, Training Loss: 0.8200960352008504\n",
      "Epoch 92, Training Loss: 0.8194929479656363\n",
      "Epoch 93, Training Loss: 0.8190446455675856\n",
      "Epoch 94, Training Loss: 0.8182213510785784\n",
      "Epoch 95, Training Loss: 0.8174423853257545\n",
      "Epoch 96, Training Loss: 0.8174718941064706\n",
      "Epoch 97, Training Loss: 0.8167363814841536\n",
      "Epoch 98, Training Loss: 0.8154434830174411\n",
      "Epoch 99, Training Loss: 0.8157521543646218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:05:12,500] Trial 116 finished with value: 0.6277333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.814621739279955\n",
      "Epoch 1, Training Loss: 1.0897894012226779\n",
      "Epoch 2, Training Loss: 1.0718769377820632\n",
      "Epoch 3, Training Loss: 1.0601047058666453\n",
      "Epoch 4, Training Loss: 1.0487777848804698\n",
      "Epoch 5, Training Loss: 1.037098908214008\n",
      "Epoch 6, Training Loss: 1.025032459076713\n",
      "Epoch 7, Training Loss: 1.0127664596894208\n",
      "Epoch 8, Training Loss: 1.0006700047324686\n",
      "Epoch 9, Training Loss: 0.9892231992413016\n",
      "Epoch 10, Training Loss: 0.9788903429227717\n",
      "Epoch 11, Training Loss: 0.9699245054581586\n",
      "Epoch 12, Training Loss: 0.9624027162439683\n",
      "Epoch 13, Training Loss: 0.9562836926123676\n",
      "Epoch 14, Training Loss: 0.9513786607630113\n",
      "Epoch 15, Training Loss: 0.9474690282344818\n",
      "Epoch 16, Training Loss: 0.9443277078516343\n",
      "Epoch 17, Training Loss: 0.9417509944298688\n",
      "Epoch 18, Training Loss: 0.9395849328181323\n",
      "Epoch 19, Training Loss: 0.9377077297603382\n",
      "Epoch 20, Training Loss: 0.9360422977279215\n",
      "Epoch 21, Training Loss: 0.9345094589626088\n",
      "Epoch 22, Training Loss: 0.9330770338984097\n",
      "Epoch 23, Training Loss: 0.9317189492197597\n",
      "Epoch 24, Training Loss: 0.9304307543530184\n",
      "Epoch 25, Training Loss: 0.9291540511215435\n",
      "Epoch 26, Training Loss: 0.9279607891335206\n",
      "Epoch 27, Training Loss: 0.9267598220180062\n",
      "Epoch 28, Training Loss: 0.9256051347536199\n",
      "Epoch 29, Training Loss: 0.9244609012323267\n",
      "Epoch 30, Training Loss: 0.923335671565112\n",
      "Epoch 31, Training Loss: 0.9222216638396769\n",
      "Epoch 32, Training Loss: 0.9211376842330484\n",
      "Epoch 33, Training Loss: 0.9200618680785684\n",
      "Epoch 34, Training Loss: 0.91898513471379\n",
      "Epoch 35, Training Loss: 0.9179335016362807\n",
      "Epoch 36, Training Loss: 0.9168762349381167\n",
      "Epoch 37, Training Loss: 0.9158260790740742\n",
      "Epoch 38, Training Loss: 0.9147904006172629\n",
      "Epoch 39, Training Loss: 0.9137441693334019\n",
      "Epoch 40, Training Loss: 0.9126871566211476\n",
      "Epoch 41, Training Loss: 0.9116483477985158\n",
      "Epoch 42, Training Loss: 0.9105723501654233\n",
      "Epoch 43, Training Loss: 0.9095450643230887\n",
      "Epoch 44, Training Loss: 0.9084766474892111\n",
      "Epoch 45, Training Loss: 0.9074197922033422\n",
      "Epoch 46, Training Loss: 0.9063220582990086\n",
      "Epoch 47, Training Loss: 0.9052425646781921\n",
      "Epoch 48, Training Loss: 0.9041334559636958\n",
      "Epoch 49, Training Loss: 0.9030133672321544\n",
      "Epoch 50, Training Loss: 0.9018661082492155\n",
      "Epoch 51, Training Loss: 0.9007093224805944\n",
      "Epoch 52, Training Loss: 0.8995234075013329\n",
      "Epoch 53, Training Loss: 0.898315580662559\n",
      "Epoch 54, Training Loss: 0.8970920347466188\n",
      "Epoch 55, Training Loss: 0.8958300218161415\n",
      "Epoch 56, Training Loss: 0.8945547436265384\n",
      "Epoch 57, Training Loss: 0.893223210923812\n",
      "Epoch 58, Training Loss: 0.8918923242653117\n",
      "Epoch 59, Training Loss: 0.8905258225693422\n",
      "Epoch 60, Training Loss: 0.8891277514485751\n",
      "Epoch 61, Training Loss: 0.8876732444763183\n",
      "Epoch 62, Training Loss: 0.8862164945462171\n",
      "Epoch 63, Training Loss: 0.8847081239083234\n",
      "Epoch 64, Training Loss: 0.8831691730723662\n",
      "Epoch 65, Training Loss: 0.881552493502112\n",
      "Epoch 66, Training Loss: 0.879978763776667\n",
      "Epoch 67, Training Loss: 0.8783310512234183\n",
      "Epoch 68, Training Loss: 0.8766552616568173\n",
      "Epoch 69, Training Loss: 0.8749270558357238\n",
      "Epoch 70, Training Loss: 0.8731814494553735\n",
      "Epoch 71, Training Loss: 0.8713967237051795\n",
      "Epoch 72, Training Loss: 0.8695798742771149\n",
      "Epoch 73, Training Loss: 0.8677453365045436\n",
      "Epoch 74, Training Loss: 0.865850451483446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:06:33,770] Trial 117 finished with value: 0.5975333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8639805766414194\n",
      "Epoch 1, Training Loss: 0.8777491124938516\n",
      "Epoch 2, Training Loss: 0.8527428719576667\n",
      "Epoch 3, Training Loss: 0.8559610175385195\n",
      "Epoch 4, Training Loss: 0.8574628302630256\n",
      "Epoch 5, Training Loss: 0.8614341484097874\n",
      "Epoch 6, Training Loss: 0.8571088759338155\n",
      "Epoch 7, Training Loss: 0.8573333003240473\n",
      "Epoch 8, Training Loss: 0.8508049423554365\n",
      "Epoch 9, Training Loss: 0.8634317298496471\n",
      "Epoch 10, Training Loss: 0.8576555022772621\n",
      "Epoch 11, Training Loss: 0.8669507091886857\n",
      "Epoch 12, Training Loss: 0.8659883812595817\n",
      "Epoch 13, Training Loss: 0.8653707103869495\n",
      "Epoch 14, Training Loss: 0.8608878953316632\n",
      "Epoch 15, Training Loss: 0.8589461174431969\n",
      "Epoch 16, Training Loss: 0.8574057125343996\n",
      "Epoch 17, Training Loss: 0.8768321682425106\n",
      "Epoch 18, Training Loss: 0.886552167710136\n",
      "Epoch 19, Training Loss: 0.8733623424698325\n",
      "Epoch 20, Training Loss: 0.8637672771425808\n",
      "Epoch 21, Training Loss: 0.8601697048720192\n",
      "Epoch 22, Training Loss: 0.8583530372731826\n",
      "Epoch 23, Training Loss: 0.8680510896093705\n",
      "Epoch 24, Training Loss: 0.8590017483514898\n",
      "Epoch 25, Training Loss: 0.8532465532246758\n",
      "Epoch 26, Training Loss: 0.8566303366773269\n",
      "Epoch 27, Training Loss: 0.8619882487549502\n",
      "Epoch 28, Training Loss: 0.8610007219454822\n",
      "Epoch 29, Training Loss: 0.8596225966425503\n",
      "Epoch 30, Training Loss: 0.8649150182219113\n",
      "Epoch 31, Training Loss: 0.8622302832322962\n",
      "Epoch 32, Training Loss: 0.8574825067379895\n",
      "Epoch 33, Training Loss: 0.8598321447652929\n",
      "Epoch 34, Training Loss: 0.8557080881034627\n",
      "Epoch 35, Training Loss: 0.8523536764874178\n",
      "Epoch 36, Training Loss: 0.8550496281595791\n",
      "Epoch 37, Training Loss: 0.8527615282816046\n",
      "Epoch 38, Training Loss: 0.8586536742659177\n",
      "Epoch 39, Training Loss: 0.8637516884242787\n",
      "Epoch 40, Training Loss: 0.859829833016676\n",
      "Epoch 41, Training Loss: 0.8530446689269122\n",
      "Epoch 42, Training Loss: 0.8548936423834632\n",
      "Epoch 43, Training Loss: 0.8521402160560384\n",
      "Epoch 44, Training Loss: 0.864536188770743\n",
      "Epoch 45, Training Loss: 0.8579856860637665\n",
      "Epoch 46, Training Loss: 0.8544461012587827\n",
      "Epoch 47, Training Loss: 0.8578231676185832\n",
      "Epoch 48, Training Loss: 0.8566884946121889\n",
      "Epoch 49, Training Loss: 0.8564467151725993\n",
      "Epoch 50, Training Loss: 0.863246863379198\n",
      "Epoch 51, Training Loss: 0.8546691634374506\n",
      "Epoch 52, Training Loss: 0.8554285786432378\n",
      "Epoch 53, Training Loss: 0.8633573369418873\n",
      "Epoch 54, Training Loss: 0.8590454438854667\n",
      "Epoch 55, Training Loss: 0.858414370172164\n",
      "Epoch 56, Training Loss: 0.8564812818695517\n",
      "Epoch 57, Training Loss: 0.8539925190280465\n",
      "Epoch 58, Training Loss: 0.8593442508753608\n",
      "Epoch 59, Training Loss: 0.8604116404056549\n",
      "Epoch 60, Training Loss: 0.8580719297072467\n",
      "Epoch 61, Training Loss: 0.8532575990873225\n",
      "Epoch 62, Training Loss: 0.8588224203446332\n",
      "Epoch 63, Training Loss: 0.8594694345137652\n",
      "Epoch 64, Training Loss: 0.8596344736043144\n",
      "Epoch 65, Training Loss: 0.86118104002055\n",
      "Epoch 66, Training Loss: 0.8588230287327486\n",
      "Epoch 67, Training Loss: 0.8670537105027367\n",
      "Epoch 68, Training Loss: 0.860920467376709\n",
      "Epoch 69, Training Loss: 0.8604373659105862\n",
      "Epoch 70, Training Loss: 0.8588662549327402\n",
      "Epoch 71, Training Loss: 0.8639489329562468\n",
      "Epoch 72, Training Loss: 0.86243124127388\n",
      "Epoch 73, Training Loss: 0.8602773892879486\n",
      "Epoch 74, Training Loss: 0.86035824397031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:08:27,721] Trial 118 finished with value: 0.6052 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.861720556231106\n",
      "Epoch 1, Training Loss: 0.8973941117875717\n",
      "Epoch 2, Training Loss: 0.8123541209276984\n",
      "Epoch 3, Training Loss: 0.8041777978223913\n",
      "Epoch 4, Training Loss: 0.8011320704572341\n",
      "Epoch 5, Training Loss: 0.7996866426748388\n",
      "Epoch 6, Training Loss: 0.7965186728449429\n",
      "Epoch 7, Training Loss: 0.7962291641796336\n",
      "Epoch 8, Training Loss: 0.7938465932537527\n",
      "Epoch 9, Training Loss: 0.7928224490670597\n",
      "Epoch 10, Training Loss: 0.7912583410739898\n",
      "Epoch 11, Training Loss: 0.7900538177349988\n",
      "Epoch 12, Training Loss: 0.7889825263444115\n",
      "Epoch 13, Training Loss: 0.7895877731547636\n",
      "Epoch 14, Training Loss: 0.7881553046142353\n",
      "Epoch 15, Training Loss: 0.7879419405320112\n",
      "Epoch 16, Training Loss: 0.7877585157927345\n",
      "Epoch 17, Training Loss: 0.7877185613267562\n",
      "Epoch 18, Training Loss: 0.7870023441314697\n",
      "Epoch 19, Training Loss: 0.7857406995576971\n",
      "Epoch 20, Training Loss: 0.7861811280250549\n",
      "Epoch 21, Training Loss: 0.7852577838476966\n",
      "Epoch 22, Training Loss: 0.7858221435546875\n",
      "Epoch 23, Training Loss: 0.7846796986635993\n",
      "Epoch 24, Training Loss: 0.7852673124565798\n",
      "Epoch 25, Training Loss: 0.7845547612975625\n",
      "Epoch 26, Training Loss: 0.7841806996569914\n",
      "Epoch 27, Training Loss: 0.7842385306077845\n",
      "Epoch 28, Training Loss: 0.7837156607122983\n",
      "Epoch 29, Training Loss: 0.7836800280739279\n",
      "Epoch 30, Training Loss: 0.7834127492063185\n",
      "Epoch 31, Training Loss: 0.7831932809072383\n",
      "Epoch 32, Training Loss: 0.7828444680746864\n",
      "Epoch 33, Training Loss: 0.7826358683670268\n",
      "Epoch 34, Training Loss: 0.7829451011910158\n",
      "Epoch 35, Training Loss: 0.7827534993255839\n",
      "Epoch 36, Training Loss: 0.7820520276181838\n",
      "Epoch 37, Training Loss: 0.7826377682124868\n",
      "Epoch 38, Training Loss: 0.7822728342869703\n",
      "Epoch 39, Training Loss: 0.7818692261331222\n",
      "Epoch 40, Training Loss: 0.782131929958568\n",
      "Epoch 41, Training Loss: 0.7818394484940697\n",
      "Epoch 42, Training Loss: 0.7815069568157196\n",
      "Epoch 43, Training Loss: 0.7812497585661271\n",
      "Epoch 44, Training Loss: 0.7810949164278367\n",
      "Epoch 45, Training Loss: 0.7811351987894843\n",
      "Epoch 46, Training Loss: 0.7811641889459947\n",
      "Epoch 47, Training Loss: 0.7808174974778119\n",
      "Epoch 48, Training Loss: 0.7804858904025134\n",
      "Epoch 49, Training Loss: 0.7810524622833027\n",
      "Epoch 50, Training Loss: 0.7808362538674298\n",
      "Epoch 51, Training Loss: 0.7810665897060843\n",
      "Epoch 52, Training Loss: 0.7802860799256494\n",
      "Epoch 53, Training Loss: 0.780375764650457\n",
      "Epoch 54, Training Loss: 0.7804096585862776\n",
      "Epoch 55, Training Loss: 0.7801872413298663\n",
      "Epoch 56, Training Loss: 0.7799373557287104\n",
      "Epoch 57, Training Loss: 0.7802238766586079\n",
      "Epoch 58, Training Loss: 0.7797762856763952\n",
      "Epoch 59, Training Loss: 0.7801308411009171\n",
      "Epoch 60, Training Loss: 0.7795676707520205\n",
      "Epoch 61, Training Loss: 0.7798510290594662\n",
      "Epoch 62, Training Loss: 0.7795179732406841\n",
      "Epoch 63, Training Loss: 0.7794524502052981\n",
      "Epoch 64, Training Loss: 0.7797374792659983\n",
      "Epoch 65, Training Loss: 0.7796432391334982\n",
      "Epoch 66, Training Loss: 0.7791947506455814\n",
      "Epoch 67, Training Loss: 0.7793779336003697\n",
      "Epoch 68, Training Loss: 0.7790964273845449\n",
      "Epoch 69, Training Loss: 0.7795327563145581\n",
      "Epoch 70, Training Loss: 0.779021827543483\n",
      "Epoch 71, Training Loss: 0.7791160831030677\n",
      "Epoch 72, Training Loss: 0.7788237870440764\n",
      "Epoch 73, Training Loss: 0.7785595890353708\n",
      "Epoch 74, Training Loss: 0.7785911506765029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:10:07,553] Trial 119 finished with value: 0.6397333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7789551751052632\n",
      "Epoch 1, Training Loss: 0.9759270860868342\n",
      "Epoch 2, Training Loss: 0.9233105660887325\n",
      "Epoch 3, Training Loss: 0.9064877245005439\n",
      "Epoch 4, Training Loss: 0.8917884109300726\n",
      "Epoch 5, Training Loss: 0.8760421561493593\n",
      "Epoch 6, Training Loss: 0.8596458278684055\n",
      "Epoch 7, Training Loss: 0.84448090244742\n",
      "Epoch 8, Training Loss: 0.8320340516987969\n",
      "Epoch 9, Training Loss: 0.8229583855236278\n",
      "Epoch 10, Training Loss: 0.8168250489234924\n",
      "Epoch 11, Training Loss: 0.8125894531081704\n",
      "Epoch 12, Training Loss: 0.8095369946255403\n",
      "Epoch 13, Training Loss: 0.8075708957980661\n",
      "Epoch 14, Training Loss: 0.8060467322433696\n",
      "Epoch 15, Training Loss: 0.8051155666744008\n",
      "Epoch 16, Training Loss: 0.804432023342918\n",
      "Epoch 17, Training Loss: 0.8039166040280286\n",
      "Epoch 18, Training Loss: 0.8033815458942862\n",
      "Epoch 19, Training Loss: 0.8027421973733341\n",
      "Epoch 20, Training Loss: 0.802458729393342\n",
      "Epoch 21, Training Loss: 0.8020847177505493\n",
      "Epoch 22, Training Loss: 0.8018446378146901\n",
      "Epoch 23, Training Loss: 0.801786824464798\n",
      "Epoch 24, Training Loss: 0.8013878337074729\n",
      "Epoch 25, Training Loss: 0.8011900092573727\n",
      "Epoch 26, Training Loss: 0.8010810643083909\n",
      "Epoch 27, Training Loss: 0.8006786617811988\n",
      "Epoch 28, Training Loss: 0.8006558989777285\n",
      "Epoch 29, Training Loss: 0.8006123065948486\n",
      "Epoch 30, Training Loss: 0.8004305024708018\n",
      "Epoch 31, Training Loss: 0.8004103818360497\n",
      "Epoch 32, Training Loss: 0.8001925216001623\n",
      "Epoch 33, Training Loss: 0.800258260193993\n",
      "Epoch 34, Training Loss: 0.80005944406285\n",
      "Epoch 35, Training Loss: 0.7998959650011623\n",
      "Epoch 36, Training Loss: 0.7997792947292328\n",
      "Epoch 37, Training Loss: 0.7995671089256511\n",
      "Epoch 38, Training Loss: 0.7997636785927941\n",
      "Epoch 39, Training Loss: 0.7994743894128239\n",
      "Epoch 40, Training Loss: 0.79941605610006\n",
      "Epoch 41, Training Loss: 0.7993151279758005\n",
      "Epoch 42, Training Loss: 0.7992117951898013\n",
      "Epoch 43, Training Loss: 0.7990880833653843\n",
      "Epoch 44, Training Loss: 0.7990761085818796\n",
      "Epoch 45, Training Loss: 0.7989432854512158\n",
      "Epoch 46, Training Loss: 0.7988972904401667\n",
      "Epoch 47, Training Loss: 0.7989150127943825\n",
      "Epoch 48, Training Loss: 0.7986148081807529\n",
      "Epoch 49, Training Loss: 0.7986830856519587\n",
      "Epoch 50, Training Loss: 0.7986227645593531\n",
      "Epoch 51, Training Loss: 0.7983768193862018\n",
      "Epoch 52, Training Loss: 0.7983419719864341\n",
      "Epoch 53, Training Loss: 0.7982911198279437\n",
      "Epoch 54, Training Loss: 0.7983315298837774\n",
      "Epoch 55, Training Loss: 0.7980276832861058\n",
      "Epoch 56, Training Loss: 0.7980363545698278\n",
      "Epoch 57, Training Loss: 0.797943153591717\n",
      "Epoch 58, Training Loss: 0.7980050978239844\n",
      "Epoch 59, Training Loss: 0.7978457373731277\n",
      "Epoch 60, Training Loss: 0.7978338198802051\n",
      "Epoch 61, Training Loss: 0.797836509101531\n",
      "Epoch 62, Training Loss: 0.7978192151294035\n",
      "Epoch 63, Training Loss: 0.7974225410994361\n",
      "Epoch 64, Training Loss: 0.7976078810411341\n",
      "Epoch 65, Training Loss: 0.797394598792581\n",
      "Epoch 66, Training Loss: 0.7973371559732101\n",
      "Epoch 67, Training Loss: 0.797341872734182\n",
      "Epoch 68, Training Loss: 0.7971972277585198\n",
      "Epoch 69, Training Loss: 0.797131157762864\n",
      "Epoch 70, Training Loss: 0.7969940470947939\n",
      "Epoch 71, Training Loss: 0.7970371801712933\n",
      "Epoch 72, Training Loss: 0.7966756397135117\n",
      "Epoch 73, Training Loss: 0.7968983158644508\n",
      "Epoch 74, Training Loss: 0.7966521080101238\n",
      "Epoch 75, Training Loss: 0.7964609875398524\n",
      "Epoch 76, Training Loss: 0.7965378923275891\n",
      "Epoch 77, Training Loss: 0.7963052129745484\n",
      "Epoch 78, Training Loss: 0.7961509373608757\n",
      "Epoch 79, Training Loss: 0.7960594460543464\n",
      "Epoch 80, Training Loss: 0.7960474772313062\n",
      "Epoch 81, Training Loss: 0.7958906647738289\n",
      "Epoch 82, Training Loss: 0.7955922217228834\n",
      "Epoch 83, Training Loss: 0.7955683169645421\n",
      "Epoch 84, Training Loss: 0.7953499618698568\n",
      "Epoch 85, Training Loss: 0.7953978431926054\n",
      "Epoch 86, Training Loss: 0.7949213651348562\n",
      "Epoch 87, Training Loss: 0.7950463380533106\n",
      "Epoch 88, Training Loss: 0.7949518916186165\n",
      "Epoch 89, Training Loss: 0.794701621462317\n",
      "Epoch 90, Training Loss: 0.7946941391159507\n",
      "Epoch 91, Training Loss: 0.7944740245622747\n",
      "Epoch 92, Training Loss: 0.7941274337908801\n",
      "Epoch 93, Training Loss: 0.7940940778395709\n",
      "Epoch 94, Training Loss: 0.7940979861511903\n",
      "Epoch 95, Training Loss: 0.7939411825993482\n",
      "Epoch 96, Training Loss: 0.7938730546306161\n",
      "Epoch 97, Training Loss: 0.793559613718706\n",
      "Epoch 98, Training Loss: 0.7934597865272971\n",
      "Epoch 99, Training Loss: 0.7933978072334739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:11:41,683] Trial 120 finished with value: 0.6351333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7932972905916326\n",
      "Epoch 1, Training Loss: 1.0610874312064227\n",
      "Epoch 2, Training Loss: 1.0016741306641523\n",
      "Epoch 3, Training Loss: 0.9732957175198723\n",
      "Epoch 4, Training Loss: 0.9632397905518026\n",
      "Epoch 5, Training Loss: 0.9594070630915025\n",
      "Epoch 6, Training Loss: 0.956922939454808\n",
      "Epoch 7, Training Loss: 0.9547967489326702\n",
      "Epoch 8, Training Loss: 0.952609703540802\n",
      "Epoch 9, Training Loss: 0.9503549019028159\n",
      "Epoch 10, Training Loss: 0.9480303724373088\n",
      "Epoch 11, Training Loss: 0.9455729739105\n",
      "Epoch 12, Training Loss: 0.9430209753092598\n",
      "Epoch 13, Training Loss: 0.9404086564568912\n",
      "Epoch 14, Training Loss: 0.9375366565059213\n",
      "Epoch 15, Training Loss: 0.9343906031636631\n",
      "Epoch 16, Training Loss: 0.9311503090577967\n",
      "Epoch 17, Training Loss: 0.9277077903467066\n",
      "Epoch 18, Training Loss: 0.9241632427187527\n",
      "Epoch 19, Training Loss: 0.9202977749880622\n",
      "Epoch 20, Training Loss: 0.9162306600458482\n",
      "Epoch 21, Training Loss: 0.9120675166915445\n",
      "Epoch 22, Training Loss: 0.9075778189827414\n",
      "Epoch 23, Training Loss: 0.9030718121107887\n",
      "Epoch 24, Training Loss: 0.8983020869423362\n",
      "Epoch 25, Training Loss: 0.8935306000008303\n",
      "Epoch 26, Training Loss: 0.8887360557387857\n",
      "Epoch 27, Training Loss: 0.8839433580987593\n",
      "Epoch 28, Training Loss: 0.8789707486068501\n",
      "Epoch 29, Training Loss: 0.8742663682208341\n",
      "Epoch 30, Training Loss: 0.8694806324033176\n",
      "Epoch 31, Training Loss: 0.8649149821085088\n",
      "Epoch 32, Training Loss: 0.8605138854419484\n",
      "Epoch 33, Training Loss: 0.8563279592289644\n",
      "Epoch 34, Training Loss: 0.8522398534241845\n",
      "Epoch 35, Training Loss: 0.8484069205031676\n",
      "Epoch 36, Training Loss: 0.8448803891855128\n",
      "Epoch 37, Training Loss: 0.8415668470719281\n",
      "Epoch 38, Training Loss: 0.8385133089037502\n",
      "Epoch 39, Training Loss: 0.835546249852461\n",
      "Epoch 40, Training Loss: 0.8330112648711485\n",
      "Epoch 41, Training Loss: 0.8306449766720042\n",
      "Epoch 42, Training Loss: 0.8284578800902647\n",
      "Epoch 43, Training Loss: 0.8266415473994086\n",
      "Epoch 44, Training Loss: 0.8248304971526651\n",
      "Epoch 45, Training Loss: 0.8233030875290142\n",
      "Epoch 46, Training Loss: 0.8219002296644099\n",
      "Epoch 47, Training Loss: 0.8205693450394799\n",
      "Epoch 48, Training Loss: 0.8195054935006534\n",
      "Epoch 49, Training Loss: 0.8184365933081683\n",
      "Epoch 50, Training Loss: 0.8175310643981485\n",
      "Epoch 51, Training Loss: 0.8167667719195871\n",
      "Epoch 52, Training Loss: 0.8161715376377106\n",
      "Epoch 53, Training Loss: 0.8153868072173175\n",
      "Epoch 54, Training Loss: 0.814807724531959\n",
      "Epoch 55, Training Loss: 0.8143046069145202\n",
      "Epoch 56, Training Loss: 0.8138767215083628\n",
      "Epoch 57, Training Loss: 0.8133265066848082\n",
      "Epoch 58, Training Loss: 0.8129414026176228\n",
      "Epoch 59, Training Loss: 0.8124572698508992\n",
      "Epoch 60, Training Loss: 0.8121744693026823\n",
      "Epoch 61, Training Loss: 0.8118115164953119\n",
      "Epoch 62, Training Loss: 0.8114634525775909\n",
      "Epoch 63, Training Loss: 0.81122927385218\n",
      "Epoch 64, Training Loss: 0.8109526395797729\n",
      "Epoch 65, Training Loss: 0.8107637103866129\n",
      "Epoch 66, Training Loss: 0.8104079639210421\n",
      "Epoch 67, Training Loss: 0.8102473308759577\n",
      "Epoch 68, Training Loss: 0.8100472930599661\n",
      "Epoch 69, Training Loss: 0.8095926998643315\n",
      "Epoch 70, Training Loss: 0.8095241498245912\n",
      "Epoch 71, Training Loss: 0.8092981414935169\n",
      "Epoch 72, Training Loss: 0.8091194572168238\n",
      "Epoch 73, Training Loss: 0.8089723479747772\n",
      "Epoch 74, Training Loss: 0.808579371396233\n",
      "Epoch 75, Training Loss: 0.8085720201800851\n",
      "Epoch 76, Training Loss: 0.8083939414865831\n",
      "Epoch 77, Training Loss: 0.8081455739105449\n",
      "Epoch 78, Training Loss: 0.807851182011997\n",
      "Epoch 79, Training Loss: 0.8078025480578928\n",
      "Epoch 80, Training Loss: 0.8075859348212971\n",
      "Epoch 81, Training Loss: 0.8074715098913978\n",
      "Epoch 82, Training Loss: 0.8072312002322253\n",
      "Epoch 83, Training Loss: 0.8071485147756688\n",
      "Epoch 84, Training Loss: 0.8070206933863022\n",
      "Epoch 85, Training Loss: 0.8067563479086932\n",
      "Epoch 86, Training Loss: 0.8066760800866519\n",
      "Epoch 87, Training Loss: 0.8065648043856901\n",
      "Epoch 88, Training Loss: 0.8064656216957989\n",
      "Epoch 89, Training Loss: 0.8062881785280565\n",
      "Epoch 90, Training Loss: 0.8062093849041883\n",
      "Epoch 91, Training Loss: 0.806069631506415\n",
      "Epoch 92, Training Loss: 0.8060002697916592\n",
      "Epoch 93, Training Loss: 0.8057671281169443\n",
      "Epoch 94, Training Loss: 0.805755092606825\n",
      "Epoch 95, Training Loss: 0.8055603654244367\n",
      "Epoch 96, Training Loss: 0.8054720452252556\n",
      "Epoch 97, Training Loss: 0.8054358037079081\n",
      "Epoch 98, Training Loss: 0.8052994080150828\n",
      "Epoch 99, Training Loss: 0.805083607785842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:13:12,323] Trial 121 finished with value: 0.6319333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8050854564414305\n",
      "Epoch 1, Training Loss: 0.8681188241879743\n",
      "Epoch 2, Training Loss: 0.8205245331714027\n",
      "Epoch 3, Training Loss: 0.8124323762449107\n",
      "Epoch 4, Training Loss: 0.8101729277381323\n",
      "Epoch 5, Training Loss: 0.8066585117712953\n",
      "Epoch 6, Training Loss: 0.8041991390680012\n",
      "Epoch 7, Training Loss: 0.8035608262047732\n",
      "Epoch 8, Training Loss: 0.8016361094058905\n",
      "Epoch 9, Training Loss: 0.8006509287913043\n",
      "Epoch 10, Training Loss: 0.7992889186493436\n",
      "Epoch 11, Training Loss: 0.8003577415208171\n",
      "Epoch 12, Training Loss: 0.7987612809453691\n",
      "Epoch 13, Training Loss: 0.7997766799496528\n",
      "Epoch 14, Training Loss: 0.7978626690412822\n",
      "Epoch 15, Training Loss: 0.799004828750639\n",
      "Epoch 16, Training Loss: 0.7978893635864545\n",
      "Epoch 17, Training Loss: 0.7978215480209293\n",
      "Epoch 18, Training Loss: 0.797165622567772\n",
      "Epoch 19, Training Loss: 0.7966214344913798\n",
      "Epoch 20, Training Loss: 0.7963353742334179\n",
      "Epoch 21, Training Loss: 0.7962366563933236\n",
      "Epoch 22, Training Loss: 0.7968156922132449\n",
      "Epoch 23, Training Loss: 0.7965721124993231\n",
      "Epoch 24, Training Loss: 0.7949016781229722\n",
      "Epoch 25, Training Loss: 0.7946774667367003\n",
      "Epoch 26, Training Loss: 0.7953845040242474\n",
      "Epoch 27, Training Loss: 0.7954366793309836\n",
      "Epoch 28, Training Loss: 0.7946107369616515\n",
      "Epoch 29, Training Loss: 0.7941638078904689\n",
      "Epoch 30, Training Loss: 0.7937660798990637\n",
      "Epoch 31, Training Loss: 0.7940535912836405\n",
      "Epoch 32, Training Loss: 0.7938950773468592\n",
      "Epoch 33, Training Loss: 0.7933973707650838\n",
      "Epoch 34, Training Loss: 0.7936253325383466\n",
      "Epoch 35, Training Loss: 0.7926946437448488\n",
      "Epoch 36, Training Loss: 0.7927088668920044\n",
      "Epoch 37, Training Loss: 0.7929839598505121\n",
      "Epoch 38, Training Loss: 0.793841094361212\n",
      "Epoch 39, Training Loss: 0.7927887754332751\n",
      "Epoch 40, Training Loss: 0.7929541270535692\n",
      "Epoch 41, Training Loss: 0.792787604224413\n",
      "Epoch 42, Training Loss: 0.7924773543400871\n",
      "Epoch 43, Training Loss: 0.792437700042151\n",
      "Epoch 44, Training Loss: 0.7930510618632898\n",
      "Epoch 45, Training Loss: 0.7917189457362755\n",
      "Epoch 46, Training Loss: 0.7913227442960094\n",
      "Epoch 47, Training Loss: 0.7917873730336813\n",
      "Epoch 48, Training Loss: 0.7918957458402878\n",
      "Epoch 49, Training Loss: 0.791784549835033\n",
      "Epoch 50, Training Loss: 0.7914599655266095\n",
      "Epoch 51, Training Loss: 0.7909546120722492\n",
      "Epoch 52, Training Loss: 0.7903466920207317\n",
      "Epoch 53, Training Loss: 0.7913123479463104\n",
      "Epoch 54, Training Loss: 0.7905913034327945\n",
      "Epoch 55, Training Loss: 0.7907610491702431\n",
      "Epoch 56, Training Loss: 0.7913084208517146\n",
      "Epoch 57, Training Loss: 0.7907019139232492\n",
      "Epoch 58, Training Loss: 0.7906501562971818\n",
      "Epoch 59, Training Loss: 0.7909679436145869\n",
      "Epoch 60, Training Loss: 0.7909685992656794\n",
      "Epoch 61, Training Loss: 0.7909208045866256\n",
      "Epoch 62, Training Loss: 0.7916284835428223\n",
      "Epoch 63, Training Loss: 0.7907336710090924\n",
      "Epoch 64, Training Loss: 0.7897411527042102\n",
      "Epoch 65, Training Loss: 0.790627516301951\n",
      "Epoch 66, Training Loss: 0.790732893370148\n",
      "Epoch 67, Training Loss: 0.7911173608070029\n",
      "Epoch 68, Training Loss: 0.7915896214040599\n",
      "Epoch 69, Training Loss: 0.79118008748033\n",
      "Epoch 70, Training Loss: 0.7907798783223432\n",
      "Epoch 71, Training Loss: 0.791152786939664\n",
      "Epoch 72, Training Loss: 0.790409539427076\n",
      "Epoch 73, Training Loss: 0.7903105076094319\n",
      "Epoch 74, Training Loss: 0.791346390086009\n",
      "Epoch 75, Training Loss: 0.7904113771323871\n",
      "Epoch 76, Training Loss: 0.7905357628836668\n",
      "Epoch 77, Training Loss: 0.7898519586799736\n",
      "Epoch 78, Training Loss: 0.790217932274467\n",
      "Epoch 79, Training Loss: 0.7896209030223073\n",
      "Epoch 80, Training Loss: 0.7900662694658552\n",
      "Epoch 81, Training Loss: 0.7909486441683948\n",
      "Epoch 82, Training Loss: 0.7906798842258023\n",
      "Epoch 83, Training Loss: 0.7896558033792596\n",
      "Epoch 84, Training Loss: 0.7899246926594498\n",
      "Epoch 85, Training Loss: 0.7889100760445559\n",
      "Epoch 86, Training Loss: 0.7888449856213161\n",
      "Epoch 87, Training Loss: 0.7904040979263478\n",
      "Epoch 88, Training Loss: 0.7896961671965462\n",
      "Epoch 89, Training Loss: 0.7902622651336785\n",
      "Epoch 90, Training Loss: 0.7891507269744586\n",
      "Epoch 91, Training Loss: 0.7892805994901442\n",
      "Epoch 92, Training Loss: 0.7901865206266704\n",
      "Epoch 93, Training Loss: 0.7896012665633868\n",
      "Epoch 94, Training Loss: 0.7899510606787259\n",
      "Epoch 95, Training Loss: 0.7889339192917473\n",
      "Epoch 96, Training Loss: 0.7898014602804543\n",
      "Epoch 97, Training Loss: 0.7888290385554607\n",
      "Epoch 98, Training Loss: 0.7891207683355288\n",
      "Epoch 99, Training Loss: 0.7886016137617872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:15:13,963] Trial 122 finished with value: 0.6124666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7889805458093945\n",
      "Epoch 1, Training Loss: 0.9916534352302552\n",
      "Epoch 2, Training Loss: 0.9481626864040599\n",
      "Epoch 3, Training Loss: 0.9297694232183344\n",
      "Epoch 4, Training Loss: 0.9050491225018221\n",
      "Epoch 5, Training Loss: 0.8779724267651053\n",
      "Epoch 6, Training Loss: 0.8543248964758481\n",
      "Epoch 7, Training Loss: 0.8371892196290633\n",
      "Epoch 8, Training Loss: 0.8264770052713506\n",
      "Epoch 9, Training Loss: 0.8201220680685605\n",
      "Epoch 10, Training Loss: 0.8163930714130402\n",
      "Epoch 11, Training Loss: 0.8140637488926158\n",
      "Epoch 12, Training Loss: 0.8123123349161709\n",
      "Epoch 13, Training Loss: 0.8110419673078201\n",
      "Epoch 14, Training Loss: 0.8103726321809432\n",
      "Epoch 15, Training Loss: 0.8095212531089783\n",
      "Epoch 16, Training Loss: 0.8092224520795486\n",
      "Epoch 17, Training Loss: 0.8087703804408802\n",
      "Epoch 18, Training Loss: 0.8082522510781007\n",
      "Epoch 19, Training Loss: 0.8082635502254262\n",
      "Epoch 20, Training Loss: 0.8077614284262937\n",
      "Epoch 21, Training Loss: 0.8074298134972068\n",
      "Epoch 22, Training Loss: 0.8073060839316424\n",
      "Epoch 23, Training Loss: 0.8071345086658702\n",
      "Epoch 24, Training Loss: 0.806825420996722\n",
      "Epoch 25, Training Loss: 0.8065598920513601\n",
      "Epoch 26, Training Loss: 0.8068640532213099\n",
      "Epoch 27, Training Loss: 0.8064285282527699\n",
      "Epoch 28, Training Loss: 0.8061714406574474\n",
      "Epoch 29, Training Loss: 0.8060067074439105\n",
      "Epoch 30, Training Loss: 0.805778823740342\n",
      "Epoch 31, Training Loss: 0.8058422764609842\n",
      "Epoch 32, Training Loss: 0.8056192842651816\n",
      "Epoch 33, Training Loss: 0.8054301312390496\n",
      "Epoch 34, Training Loss: 0.8054124051683089\n",
      "Epoch 35, Training Loss: 0.8051558012120864\n",
      "Epoch 36, Training Loss: 0.8049340227070977\n",
      "Epoch 37, Training Loss: 0.8050209024373223\n",
      "Epoch 38, Training Loss: 0.8046310447244083\n",
      "Epoch 39, Training Loss: 0.8046180503508624\n",
      "Epoch 40, Training Loss: 0.8043502803409801\n",
      "Epoch 41, Training Loss: 0.8043407275396235\n",
      "Epoch 42, Training Loss: 0.8040488393166486\n",
      "Epoch 43, Training Loss: 0.8038611822268542\n",
      "Epoch 44, Training Loss: 0.8036622597189511\n",
      "Epoch 45, Training Loss: 0.8036985678532544\n",
      "Epoch 46, Training Loss: 0.8036723966458265\n",
      "Epoch 47, Training Loss: 0.8033126702028163\n",
      "Epoch 48, Training Loss: 0.8035172393041499\n",
      "Epoch 49, Training Loss: 0.8032720224997577\n",
      "Epoch 50, Training Loss: 0.8031370852274053\n",
      "Epoch 51, Training Loss: 0.8025908494696897\n",
      "Epoch 52, Training Loss: 0.8025331725793726\n",
      "Epoch 53, Training Loss: 0.8025935335720287\n",
      "Epoch 54, Training Loss: 0.802562262170455\n",
      "Epoch 55, Training Loss: 0.8023285954138812\n",
      "Epoch 56, Training Loss: 0.8022010653159197\n",
      "Epoch 57, Training Loss: 0.8019964669732487\n",
      "Epoch 58, Training Loss: 0.8020423793792725\n",
      "Epoch 59, Training Loss: 0.8017679798603058\n",
      "Epoch 60, Training Loss: 0.8017210803312413\n",
      "Epoch 61, Training Loss: 0.8017796182632446\n",
      "Epoch 62, Training Loss: 0.8016869780596565\n",
      "Epoch 63, Training Loss: 0.8014874588040745\n",
      "Epoch 64, Training Loss: 0.8012533769186805\n",
      "Epoch 65, Training Loss: 0.8012670969261843\n",
      "Epoch 66, Training Loss: 0.801132924416486\n",
      "Epoch 67, Training Loss: 0.8010890241230235\n",
      "Epoch 68, Training Loss: 0.8009496817869298\n",
      "Epoch 69, Training Loss: 0.801015925617779\n",
      "Epoch 70, Training Loss: 0.8008618916483486\n",
      "Epoch 71, Training Loss: 0.8007854819999022\n",
      "Epoch 72, Training Loss: 0.8004656422138214\n",
      "Epoch 73, Training Loss: 0.8007881028511945\n",
      "Epoch 74, Training Loss: 0.8008195362371557\n",
      "Epoch 75, Training Loss: 0.800431863490273\n",
      "Epoch 76, Training Loss: 0.8003520497855018\n",
      "Epoch 77, Training Loss: 0.8001198896239785\n",
      "Epoch 78, Training Loss: 0.8002369590366588\n",
      "Epoch 79, Training Loss: 0.800080438782187\n",
      "Epoch 80, Training Loss: 0.8000284122719484\n",
      "Epoch 81, Training Loss: 0.7997826271898606\n",
      "Epoch 82, Training Loss: 0.7997933353396023\n",
      "Epoch 83, Training Loss: 0.799896235185511\n",
      "Epoch 84, Training Loss: 0.7994972493368037\n",
      "Epoch 85, Training Loss: 0.799805733316085\n",
      "Epoch 86, Training Loss: 0.7996021603135501\n",
      "Epoch 87, Training Loss: 0.7994689411976759\n",
      "Epoch 88, Training Loss: 0.7995755861086004\n",
      "Epoch 89, Training Loss: 0.7996012025019702\n",
      "Epoch 90, Training Loss: 0.799351142504636\n",
      "Epoch 91, Training Loss: 0.7995686148895936\n",
      "Epoch 92, Training Loss: 0.79927659034729\n",
      "Epoch 93, Training Loss: 0.7992479288578034\n",
      "Epoch 94, Training Loss: 0.7992278674770804\n",
      "Epoch 95, Training Loss: 0.7992607160876779\n",
      "Epoch 96, Training Loss: 0.7991106700897217\n",
      "Epoch 97, Training Loss: 0.798900387357263\n",
      "Epoch 98, Training Loss: 0.7989054715633392\n",
      "Epoch 99, Training Loss: 0.798952266328475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:17:09,364] Trial 123 finished with value: 0.6352666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7987954330444336\n",
      "Epoch 1, Training Loss: 1.0989299015437854\n",
      "Epoch 2, Training Loss: 1.0894175241975224\n",
      "Epoch 3, Training Loss: 1.0832546949386597\n",
      "Epoch 4, Training Loss: 1.0776629436717313\n",
      "Epoch 5, Training Loss: 1.072502485443564\n",
      "Epoch 6, Training Loss: 1.0677099972612718\n",
      "Epoch 7, Training Loss: 1.0631916551028981\n",
      "Epoch 8, Training Loss: 1.0588912874109604\n",
      "Epoch 9, Training Loss: 1.0547972886702595\n",
      "Epoch 10, Training Loss: 1.0508761253076442\n",
      "Epoch 11, Training Loss: 1.047087701769436\n",
      "Epoch 12, Training Loss: 1.0434520045448752\n",
      "Epoch 13, Training Loss: 1.039923529695062\n",
      "Epoch 14, Training Loss: 1.036509181541555\n",
      "Epoch 15, Training Loss: 1.0331981161762687\n",
      "Epoch 16, Training Loss: 1.0299844627520618\n",
      "Epoch 17, Training Loss: 1.026882476315779\n",
      "Epoch 18, Training Loss: 1.023855647129171\n",
      "Epoch 19, Training Loss: 1.020949791249107\n",
      "Epoch 20, Training Loss: 1.0181172626860002\n",
      "Epoch 21, Training Loss: 1.0153909087882322\n",
      "Epoch 22, Training Loss: 1.012750327797497\n",
      "Epoch 23, Training Loss: 1.0102055328032549\n",
      "Epoch 24, Training Loss: 1.0077470658807193\n",
      "Epoch 25, Training Loss: 1.0053999276722179\n",
      "Epoch 26, Training Loss: 1.0031238092394437\n",
      "Epoch 27, Training Loss: 1.0009309136867524\n",
      "Epoch 28, Training Loss: 0.9988560010405148\n",
      "Epoch 29, Training Loss: 0.9968442737354952\n",
      "Epoch 30, Training Loss: 0.9949290771344129\n",
      "Epoch 31, Training Loss: 0.9930865005184623\n",
      "Epoch 32, Training Loss: 0.9913482086097493\n",
      "Epoch 33, Training Loss: 0.9896714934881996\n",
      "Epoch 34, Training Loss: 0.9880876178601209\n",
      "Epoch 35, Training Loss: 0.9865682439243092\n",
      "Epoch 36, Training Loss: 0.9851345881293802\n",
      "Epoch 37, Training Loss: 0.9837685878136578\n",
      "Epoch 38, Training Loss: 0.9824704087481779\n",
      "Epoch 39, Training Loss: 0.9812288127927219\n",
      "Epoch 40, Training Loss: 0.9800736367001253\n",
      "Epoch 41, Training Loss: 0.978958305050345\n",
      "Epoch 42, Training Loss: 0.9779143533987158\n",
      "Epoch 43, Training Loss: 0.9769277278114767\n",
      "Epoch 44, Training Loss: 0.9759938354351941\n",
      "Epoch 45, Training Loss: 0.9751120915833642\n",
      "Epoch 46, Training Loss: 0.9742660739141352\n",
      "Epoch 47, Training Loss: 0.973478780634263\n",
      "Epoch 48, Training Loss: 0.9727330736552968\n",
      "Epoch 49, Training Loss: 0.9720231112311868\n",
      "Epoch 50, Training Loss: 0.9713551803897409\n",
      "Epoch 51, Training Loss: 0.9707288508555468\n",
      "Epoch 52, Training Loss: 0.9701338878098656\n",
      "Epoch 53, Training Loss: 0.9695821017377517\n",
      "Epoch 54, Training Loss: 0.9690459537506103\n",
      "Epoch 55, Training Loss: 0.9685460617963005\n",
      "Epoch 56, Training Loss: 0.9680700975305894\n",
      "Epoch 57, Training Loss: 0.9676223751376657\n",
      "Epoch 58, Training Loss: 0.9671826901155359\n",
      "Epoch 59, Training Loss: 0.9668029759210699\n",
      "Epoch 60, Training Loss: 0.9664177035584169\n",
      "Epoch 61, Training Loss: 0.9660395761798409\n",
      "Epoch 62, Training Loss: 0.9657103615648607\n",
      "Epoch 63, Training Loss: 0.9653821502713597\n",
      "Epoch 64, Training Loss: 0.9650765221259173\n",
      "Epoch 65, Training Loss: 0.96478164006682\n",
      "Epoch 66, Training Loss: 0.9644935001345242\n",
      "Epoch 67, Training Loss: 0.9642338428076576\n",
      "Epoch 68, Training Loss: 0.9639770744828617\n",
      "Epoch 69, Training Loss: 0.9637284534818986\n",
      "Epoch 70, Training Loss: 0.9634960225750419\n",
      "Epoch 71, Training Loss: 0.9632719792337978\n",
      "Epoch 72, Training Loss: 0.963054869034711\n",
      "Epoch 73, Training Loss: 0.9628506063012516\n",
      "Epoch 74, Training Loss: 0.962637863369549\n",
      "Epoch 75, Training Loss: 0.9624629616036134\n",
      "Epoch 76, Training Loss: 0.9622765661688412\n",
      "Epoch 77, Training Loss: 0.9620842369163738\n",
      "Epoch 78, Training Loss: 0.9619066167578978\n",
      "Epoch 79, Training Loss: 0.961745740035001\n",
      "Epoch 80, Training Loss: 0.9615923824731042\n",
      "Epoch 81, Training Loss: 0.9614315417233635\n",
      "Epoch 82, Training Loss: 0.9612803781733793\n",
      "Epoch 83, Training Loss: 0.9611298676098095\n",
      "Epoch 84, Training Loss: 0.9609858341076795\n",
      "Epoch 85, Training Loss: 0.9608373869867886\n",
      "Epoch 86, Training Loss: 0.9606916923382703\n",
      "Epoch 87, Training Loss: 0.960560431059669\n",
      "Epoch 88, Training Loss: 0.9604229608002831\n",
      "Epoch 89, Training Loss: 0.9602964058343102\n",
      "Epoch 90, Training Loss: 0.960153660143123\n",
      "Epoch 91, Training Loss: 0.9600345434862024\n",
      "Epoch 92, Training Loss: 0.9599155336969039\n",
      "Epoch 93, Training Loss: 0.9597908048770007\n",
      "Epoch 94, Training Loss: 0.9596638278400197\n",
      "Epoch 95, Training Loss: 0.9595456020972308\n",
      "Epoch 96, Training Loss: 0.9594274519471562\n",
      "Epoch 97, Training Loss: 0.9593080674900728\n",
      "Epoch 98, Training Loss: 0.9591801081685459\n",
      "Epoch 99, Training Loss: 0.9590656455124126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:18:41,264] Trial 124 finished with value: 0.5262 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.9589499981263104\n",
      "Epoch 1, Training Loss: 1.0964161924754872\n",
      "Epoch 2, Training Loss: 1.0910481420685263\n",
      "Epoch 3, Training Loss: 1.0910342107099644\n",
      "Epoch 4, Training Loss: 1.0910297005316791\n",
      "Epoch 5, Training Loss: 1.0910312736735623\n",
      "Epoch 6, Training Loss: 1.0910274436894585\n",
      "Epoch 7, Training Loss: 1.0910160505070405\n",
      "Epoch 8, Training Loss: 1.0910091318803674\n",
      "Epoch 9, Training Loss: 1.0910097722446217\n",
      "Epoch 10, Training Loss: 1.091003495244419\n",
      "Epoch 11, Training Loss: 1.0909989557546729\n",
      "Epoch 12, Training Loss: 1.0909676469073577\n",
      "Epoch 13, Training Loss: 1.0909901273951812\n",
      "Epoch 14, Training Loss: 1.0909779610353358\n",
      "Epoch 15, Training Loss: 1.090967934552361\n",
      "Epoch 16, Training Loss: 1.0909652764656965\n",
      "Epoch 17, Training Loss: 1.0909489588176502\n",
      "Epoch 18, Training Loss: 1.0909376065871295\n",
      "Epoch 19, Training Loss: 1.0909506142840666\n",
      "Epoch 20, Training Loss: 1.0909319571887746\n",
      "Epoch 21, Training Loss: 1.0909336491192088\n",
      "Epoch 22, Training Loss: 1.0909314086857964\n",
      "Epoch 23, Training Loss: 1.0909178550103131\n",
      "Epoch 24, Training Loss: 1.0909123105161331\n",
      "Epoch 25, Training Loss: 1.0909024182487936\n",
      "Epoch 26, Training Loss: 1.0909061854025897\n",
      "Epoch 27, Training Loss: 1.0908942613882178\n",
      "Epoch 28, Training Loss: 1.0908939261997448\n",
      "Epoch 29, Training Loss: 1.0908868237102733\n",
      "Epoch 30, Training Loss: 1.090876475502463\n",
      "Epoch 31, Training Loss: 1.0908680615705602\n",
      "Epoch 32, Training Loss: 1.0908561420440674\n",
      "Epoch 33, Training Loss: 1.0908582986102384\n",
      "Epoch 34, Training Loss: 1.0908502919533674\n",
      "Epoch 35, Training Loss: 1.0908503023315879\n",
      "Epoch 36, Training Loss: 1.090840978902929\n",
      "Epoch 37, Training Loss: 1.0908333497888902\n",
      "Epoch 38, Training Loss: 1.0908242796449101\n",
      "Epoch 39, Training Loss: 1.0908224310594445\n",
      "Epoch 40, Training Loss: 1.0908012256902806\n",
      "Epoch 41, Training Loss: 1.0908135972303503\n",
      "Epoch 42, Training Loss: 1.0907989875008077\n",
      "Epoch 43, Training Loss: 1.0907810875948738\n",
      "Epoch 44, Training Loss: 1.0907829056066625\n",
      "Epoch 45, Training Loss: 1.0907879274031695\n",
      "Epoch 46, Training Loss: 1.0907805988367867\n",
      "Epoch 47, Training Loss: 1.0907411990446203\n",
      "Epoch 48, Training Loss: 1.0907581602825838\n",
      "Epoch 49, Training Loss: 1.0907598139257992\n",
      "Epoch 50, Training Loss: 1.0907418758728924\n",
      "Epoch 51, Training Loss: 1.0907513492247638\n",
      "Epoch 52, Training Loss: 1.0907457705105053\n",
      "Epoch 53, Training Loss: 1.0907204044566434\n",
      "Epoch 54, Training Loss: 1.090733825599446\n",
      "Epoch 55, Training Loss: 1.0907204553660224\n",
      "Epoch 56, Training Loss: 1.0907189505240495\n",
      "Epoch 57, Training Loss: 1.09071100767921\n",
      "Epoch 58, Training Loss: 1.0907024540620691\n",
      "Epoch 59, Training Loss: 1.090688727462993\n",
      "Epoch 60, Training Loss: 1.0906953842499676\n",
      "Epoch 61, Training Loss: 1.0906915250946494\n",
      "Epoch 62, Training Loss: 1.0906605100631714\n",
      "Epoch 63, Training Loss: 1.0906682039709652\n",
      "Epoch 64, Training Loss: 1.0906657782722922\n",
      "Epoch 65, Training Loss: 1.0906599661883185\n",
      "Epoch 66, Training Loss: 1.0906494519289802\n",
      "Epoch 67, Training Loss: 1.090647909781512\n",
      "Epoch 68, Training Loss: 1.0906387891488916\n",
      "Epoch 69, Training Loss: 1.0906359936209287\n",
      "Epoch 70, Training Loss: 1.0906284339287702\n",
      "Epoch 71, Training Loss: 1.0906272116829367\n",
      "Epoch 72, Training Loss: 1.0906048383432276\n",
      "Epoch 73, Training Loss: 1.090612869683434\n",
      "Epoch 74, Training Loss: 1.0905980368221508\n",
      "Epoch 75, Training Loss: 1.0905938595883986\n",
      "Epoch 76, Training Loss: 1.090586316725787\n",
      "Epoch 77, Training Loss: 1.0905760331714855\n",
      "Epoch 78, Training Loss: 1.0905756054205054\n",
      "Epoch 79, Training Loss: 1.0905714077108046\n",
      "Epoch 80, Training Loss: 1.0905503826982834\n",
      "Epoch 81, Training Loss: 1.0905353310528922\n",
      "Epoch 82, Training Loss: 1.0905534094922682\n",
      "Epoch 83, Training Loss: 1.090534730238073\n",
      "Epoch 84, Training Loss: 1.0905340689771315\n",
      "Epoch 85, Training Loss: 1.0905214581770055\n",
      "Epoch 86, Training Loss: 1.0905255871660569\n",
      "Epoch 87, Training Loss: 1.0905171436422012\n",
      "Epoch 88, Training Loss: 1.0905138185445\n",
      "Epoch 89, Training Loss: 1.090486270680147\n",
      "Epoch 90, Training Loss: 1.090492259053623\n",
      "Epoch 91, Training Loss: 1.0904922189432031\n",
      "Epoch 92, Training Loss: 1.0904779316397275\n",
      "Epoch 93, Training Loss: 1.0904788641368641\n",
      "Epoch 94, Training Loss: 1.090467602926142\n",
      "Epoch 95, Training Loss: 1.0904641089719884\n",
      "Epoch 96, Training Loss: 1.0904532867319443\n",
      "Epoch 97, Training Loss: 1.090445767991683\n",
      "Epoch 98, Training Loss: 1.0904458057179172\n",
      "Epoch 99, Training Loss: 1.0904161831911872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:20:36,716] Trial 125 finished with value: 0.3558 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.0904298367219813\n",
      "Epoch 1, Training Loss: 0.9154466581523867\n",
      "Epoch 2, Training Loss: 0.8691201932448194\n",
      "Epoch 3, Training Loss: 0.8367837305355789\n",
      "Epoch 4, Training Loss: 0.8179751769044346\n",
      "Epoch 5, Training Loss: 0.8096948465010277\n",
      "Epoch 6, Training Loss: 0.8047337953309367\n",
      "Epoch 7, Training Loss: 0.8042051515184847\n",
      "Epoch 8, Training Loss: 0.8026272587310103\n",
      "Epoch 9, Training Loss: 0.8023405897886233\n",
      "Epoch 10, Training Loss: 0.8006824969349051\n",
      "Epoch 11, Training Loss: 0.799991532196676\n",
      "Epoch 12, Training Loss: 0.7995821889181782\n",
      "Epoch 13, Training Loss: 0.7998117283770912\n",
      "Epoch 14, Training Loss: 0.799264168918581\n",
      "Epoch 15, Training Loss: 0.7992579063078514\n",
      "Epoch 16, Training Loss: 0.7987303503473898\n",
      "Epoch 17, Training Loss: 0.7993259771425921\n",
      "Epoch 18, Training Loss: 0.7990462285235412\n",
      "Epoch 19, Training Loss: 0.7982639633623281\n",
      "Epoch 20, Training Loss: 0.7981661618204046\n",
      "Epoch 21, Training Loss: 0.7984844582421439\n",
      "Epoch 22, Training Loss: 0.7971807541703819\n",
      "Epoch 23, Training Loss: 0.7979026083659408\n",
      "Epoch 24, Training Loss: 0.7975066179619696\n",
      "Epoch 25, Training Loss: 0.797847388920031\n",
      "Epoch 26, Training Loss: 0.7978793364718444\n",
      "Epoch 27, Training Loss: 0.7976566375646376\n",
      "Epoch 28, Training Loss: 0.7972406809491323\n",
      "Epoch 29, Training Loss: 0.7968273053491922\n",
      "Epoch 30, Training Loss: 0.7967038901228654\n",
      "Epoch 31, Training Loss: 0.7968064139660139\n",
      "Epoch 32, Training Loss: 0.7964786591386437\n",
      "Epoch 33, Training Loss: 0.7959004779507343\n",
      "Epoch 34, Training Loss: 0.7966443862233843\n",
      "Epoch 35, Training Loss: 0.79602663750039\n",
      "Epoch 36, Training Loss: 0.7957805272331812\n",
      "Epoch 37, Training Loss: 0.7953570846328162\n",
      "Epoch 38, Training Loss: 0.795381906785463\n",
      "Epoch 39, Training Loss: 0.7947698490960258\n",
      "Epoch 40, Training Loss: 0.794526864084086\n",
      "Epoch 41, Training Loss: 0.7938297381078391\n",
      "Epoch 42, Training Loss: 0.7942774765473559\n",
      "Epoch 43, Training Loss: 0.7935663715341037\n",
      "Epoch 44, Training Loss: 0.7934014237016663\n",
      "Epoch 45, Training Loss: 0.7926227289034908\n",
      "Epoch 46, Training Loss: 0.7921333436679123\n",
      "Epoch 47, Training Loss: 0.7920695054799991\n",
      "Epoch 48, Training Loss: 0.7924172465066265\n",
      "Epoch 49, Training Loss: 0.7930000097231757\n",
      "Epoch 50, Training Loss: 0.7917422801928413\n",
      "Epoch 51, Training Loss: 0.7909420861337418\n",
      "Epoch 52, Training Loss: 0.7910163592575188\n",
      "Epoch 53, Training Loss: 0.7914570179200711\n",
      "Epoch 54, Training Loss: 0.7914793331820266\n",
      "Epoch 55, Training Loss: 0.790582427584139\n",
      "Epoch 56, Training Loss: 0.7906040084989447\n",
      "Epoch 57, Training Loss: 0.7900856120693952\n",
      "Epoch 58, Training Loss: 0.7903156129937423\n",
      "Epoch 59, Training Loss: 0.7901948680555014\n",
      "Epoch 60, Training Loss: 0.7901252694596025\n",
      "Epoch 61, Training Loss: 0.7903266435278986\n",
      "Epoch 62, Training Loss: 0.7900731149472688\n",
      "Epoch 63, Training Loss: 0.7900563244532822\n",
      "Epoch 64, Training Loss: 0.7897102424972936\n",
      "Epoch 65, Training Loss: 0.79003766326976\n",
      "Epoch 66, Training Loss: 0.7896153805847455\n",
      "Epoch 67, Training Loss: 0.7896877093422682\n",
      "Epoch 68, Training Loss: 0.7895472684300932\n",
      "Epoch 69, Training Loss: 0.7897363907412479\n",
      "Epoch 70, Training Loss: 0.7896014965566477\n",
      "Epoch 71, Training Loss: 0.7900332417703213\n",
      "Epoch 72, Training Loss: 0.7893867859266754\n",
      "Epoch 73, Training Loss: 0.7895797690950839\n",
      "Epoch 74, Training Loss: 0.789999666877259\n",
      "Epoch 75, Training Loss: 0.7893180836412244\n",
      "Epoch 76, Training Loss: 0.7892059277771111\n",
      "Epoch 77, Training Loss: 0.7892939921608545\n",
      "Epoch 78, Training Loss: 0.7888256587480243\n",
      "Epoch 79, Training Loss: 0.7889909967443997\n",
      "Epoch 80, Training Loss: 0.7888458552217125\n",
      "Epoch 81, Training Loss: 0.7890591561346125\n",
      "Epoch 82, Training Loss: 0.7893007258723553\n",
      "Epoch 83, Training Loss: 0.7896581210588154\n",
      "Epoch 84, Training Loss: 0.7886925304742683\n",
      "Epoch 85, Training Loss: 0.7887247164446608\n",
      "Epoch 86, Training Loss: 0.7888998213567232\n",
      "Epoch 87, Training Loss: 0.7891242476334249\n",
      "Epoch 88, Training Loss: 0.7890346098663216\n",
      "Epoch 89, Training Loss: 0.7884650902640551\n",
      "Epoch 90, Training Loss: 0.7892398509764134\n",
      "Epoch 91, Training Loss: 0.7890688356600309\n",
      "Epoch 92, Training Loss: 0.7893254407366416\n",
      "Epoch 93, Training Loss: 0.7891776666605383\n",
      "Epoch 94, Training Loss: 0.7881173411258181\n",
      "Epoch 95, Training Loss: 0.7882130628241633\n",
      "Epoch 96, Training Loss: 0.7884017361734147\n",
      "Epoch 97, Training Loss: 0.7880142929858731\n",
      "Epoch 98, Training Loss: 0.7878409286190693\n",
      "Epoch 99, Training Loss: 0.7876963891481098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:22:12,539] Trial 126 finished with value: 0.6353333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7877075346788965\n",
      "Epoch 1, Training Loss: 0.9230888232062845\n",
      "Epoch 2, Training Loss: 0.8321495762292077\n",
      "Epoch 3, Training Loss: 0.8067984475808985\n",
      "Epoch 4, Training Loss: 0.8023449791178984\n",
      "Epoch 5, Training Loss: 0.80097153635586\n",
      "Epoch 6, Training Loss: 0.7989175825259265\n",
      "Epoch 7, Training Loss: 0.7980444132580476\n",
      "Epoch 8, Training Loss: 0.7965606433503768\n",
      "Epoch 9, Training Loss: 0.7952000696518842\n",
      "Epoch 10, Training Loss: 0.7934366416931152\n",
      "Epoch 11, Training Loss: 0.7925477718605715\n",
      "Epoch 12, Training Loss: 0.7917653081697577\n",
      "Epoch 13, Training Loss: 0.7909754046271829\n",
      "Epoch 14, Training Loss: 0.7901117612333859\n",
      "Epoch 15, Training Loss: 0.7892375448872061\n",
      "Epoch 16, Training Loss: 0.7888481362426982\n",
      "Epoch 17, Training Loss: 0.7885819021393271\n",
      "Epoch 18, Training Loss: 0.7871364920279559\n",
      "Epoch 19, Training Loss: 0.7875170606725356\n",
      "Epoch 20, Training Loss: 0.7872944871117087\n",
      "Epoch 21, Training Loss: 0.786919357215657\n",
      "Epoch 22, Training Loss: 0.7862700088585124\n",
      "Epoch 23, Training Loss: 0.7863119512445786\n",
      "Epoch 24, Training Loss: 0.7862423219400294\n",
      "Epoch 25, Training Loss: 0.7855436832063338\n",
      "Epoch 26, Training Loss: 0.7857632127228905\n",
      "Epoch 27, Training Loss: 0.7849632883773131\n",
      "Epoch 28, Training Loss: 0.7848675570768469\n",
      "Epoch 29, Training Loss: 0.784584765364142\n",
      "Epoch 30, Training Loss: 0.7841962906893561\n",
      "Epoch 31, Training Loss: 0.7842837370143217\n",
      "Epoch 32, Training Loss: 0.7837765222437242\n",
      "Epoch 33, Training Loss: 0.7835793948874754\n",
      "Epoch 34, Training Loss: 0.7841182576207554\n",
      "Epoch 35, Training Loss: 0.7830221594782436\n",
      "Epoch 36, Training Loss: 0.7827385016749887\n",
      "Epoch 37, Training Loss: 0.7828084123134613\n",
      "Epoch 38, Training Loss: 0.7825482502404382\n",
      "Epoch 39, Training Loss: 0.7826317991228665\n",
      "Epoch 40, Training Loss: 0.7824389766945559\n",
      "Epoch 41, Training Loss: 0.782393405788085\n",
      "Epoch 42, Training Loss: 0.7824844709564658\n",
      "Epoch 43, Training Loss: 0.7822593142004574\n",
      "Epoch 44, Training Loss: 0.7821133645141826\n",
      "Epoch 45, Training Loss: 0.7821685221615959\n",
      "Epoch 46, Training Loss: 0.7819044727437636\n",
      "Epoch 47, Training Loss: 0.7813006185784059\n",
      "Epoch 48, Training Loss: 0.7817386668569901\n",
      "Epoch 49, Training Loss: 0.781361924199497\n",
      "Epoch 50, Training Loss: 0.7814271935995887\n",
      "Epoch 51, Training Loss: 0.7812517412269817\n",
      "Epoch 52, Training Loss: 0.7811641775159275\n",
      "Epoch 53, Training Loss: 0.7811885530808392\n",
      "Epoch 54, Training Loss: 0.7813457524075228\n",
      "Epoch 55, Training Loss: 0.7810250303324531\n",
      "Epoch 56, Training Loss: 0.7807701331727644\n",
      "Epoch 57, Training Loss: 0.7807515403803658\n",
      "Epoch 58, Training Loss: 0.7809090261599597\n",
      "Epoch 59, Training Loss: 0.7804336875326493\n",
      "Epoch 60, Training Loss: 0.7807348107590395\n",
      "Epoch 61, Training Loss: 0.78057468512479\n",
      "Epoch 62, Training Loss: 0.7804372091153089\n",
      "Epoch 63, Training Loss: 0.7803682611269109\n",
      "Epoch 64, Training Loss: 0.7804356816936941\n",
      "Epoch 65, Training Loss: 0.780199131895514\n",
      "Epoch 66, Training Loss: 0.7802669110718895\n",
      "Epoch 67, Training Loss: 0.7802502672812518\n",
      "Epoch 68, Training Loss: 0.7801104401139652\n",
      "Epoch 69, Training Loss: 0.780335948747747\n",
      "Epoch 70, Training Loss: 0.7797568916573244\n",
      "Epoch 71, Training Loss: 0.7800822629648096\n",
      "Epoch 72, Training Loss: 0.7799760778511272\n",
      "Epoch 73, Training Loss: 0.7800361948153552\n",
      "Epoch 74, Training Loss: 0.7799049477717456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:23:51,185] Trial 127 finished with value: 0.6396 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.779733045241412\n",
      "Epoch 1, Training Loss: 0.8464762987050795\n",
      "Epoch 2, Training Loss: 0.8161458210837572\n",
      "Epoch 3, Training Loss: 0.8120716613038141\n",
      "Epoch 4, Training Loss: 0.8080511094035959\n",
      "Epoch 5, Training Loss: 0.809033379160372\n",
      "Epoch 6, Training Loss: 0.8064647434349347\n",
      "Epoch 7, Training Loss: 0.8074373656645754\n",
      "Epoch 8, Training Loss: 0.8056801013928607\n",
      "Epoch 9, Training Loss: 0.8081737927028111\n",
      "Epoch 10, Training Loss: 0.8066714834449883\n",
      "Epoch 11, Training Loss: 0.8051917381752702\n",
      "Epoch 12, Training Loss: 0.8054385619055956\n",
      "Epoch 13, Training Loss: 0.8046582640561842\n",
      "Epoch 14, Training Loss: 0.8051420479788816\n",
      "Epoch 15, Training Loss: 0.8047484749241879\n",
      "Epoch 16, Training Loss: 0.8057847503432654\n",
      "Epoch 17, Training Loss: 0.8013931311162791\n",
      "Epoch 18, Training Loss: 0.8039337457570814\n",
      "Epoch 19, Training Loss: 0.8060112109757904\n",
      "Epoch 20, Training Loss: 0.802632585055846\n",
      "Epoch 21, Training Loss: 0.8036765033141114\n",
      "Epoch 22, Training Loss: 0.8032823650460494\n",
      "Epoch 23, Training Loss: 0.8035404201737024\n",
      "Epoch 24, Training Loss: 0.8060363742641936\n",
      "Epoch 25, Training Loss: 0.8035407951899938\n",
      "Epoch 26, Training Loss: 0.8038486266494693\n",
      "Epoch 27, Training Loss: 0.8042452369417463\n",
      "Epoch 28, Training Loss: 0.8025369914850794\n",
      "Epoch 29, Training Loss: 0.8026958331129604\n",
      "Epoch 30, Training Loss: 0.8037054673173374\n",
      "Epoch 31, Training Loss: 0.8019688523801646\n",
      "Epoch 32, Training Loss: 0.8022081322239754\n",
      "Epoch 33, Training Loss: 0.8026007078644029\n",
      "Epoch 34, Training Loss: 0.8021991790685439\n",
      "Epoch 35, Training Loss: 0.8028822759936627\n",
      "Epoch 36, Training Loss: 0.8030563298024629\n",
      "Epoch 37, Training Loss: 0.8003959861016812\n",
      "Epoch 38, Training Loss: 0.8033055764840061\n",
      "Epoch 39, Training Loss: 0.8035887245845078\n",
      "Epoch 40, Training Loss: 0.8023251996900802\n",
      "Epoch 41, Training Loss: 0.8020947988768269\n",
      "Epoch 42, Training Loss: 0.8023032117607002\n",
      "Epoch 43, Training Loss: 0.8028197398311213\n",
      "Epoch 44, Training Loss: 0.8041281730608832\n",
      "Epoch 45, Training Loss: 0.802301754091019\n",
      "Epoch 46, Training Loss: 0.8032267183289492\n",
      "Epoch 47, Training Loss: 0.8009257884850179\n",
      "Epoch 48, Training Loss: 0.8035421585678157\n",
      "Epoch 49, Training Loss: 0.8035095448780777\n",
      "Epoch 50, Training Loss: 0.8030743750414454\n",
      "Epoch 51, Training Loss: 0.8013127906878191\n",
      "Epoch 52, Training Loss: 0.8007012428197645\n",
      "Epoch 53, Training Loss: 0.8029431592252918\n",
      "Epoch 54, Training Loss: 0.8020531260877624\n",
      "Epoch 55, Training Loss: 0.8021987917727994\n",
      "Epoch 56, Training Loss: 0.802340364904332\n",
      "Epoch 57, Training Loss: 0.8059570417368322\n",
      "Epoch 58, Training Loss: 0.8013513314096551\n",
      "Epoch 59, Training Loss: 0.8013813104844631\n",
      "Epoch 60, Training Loss: 0.8022284145642044\n",
      "Epoch 61, Training Loss: 0.8023241784339561\n",
      "Epoch 62, Training Loss: 0.8022935289637487\n",
      "Epoch 63, Training Loss: 0.8025327841142067\n",
      "Epoch 64, Training Loss: 0.8010319372765103\n",
      "Epoch 65, Training Loss: 0.8021699126501729\n",
      "Epoch 66, Training Loss: 0.8014091383245655\n",
      "Epoch 67, Training Loss: 0.8015158655051898\n",
      "Epoch 68, Training Loss: 0.802813704659168\n",
      "Epoch 69, Training Loss: 0.8035547396294156\n",
      "Epoch 70, Training Loss: 0.8002500013749402\n",
      "Epoch 71, Training Loss: 0.7997938391857577\n",
      "Epoch 72, Training Loss: 0.8025342566626412\n",
      "Epoch 73, Training Loss: 0.8010793899234973\n",
      "Epoch 74, Training Loss: 0.8034554581893117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:25:18,078] Trial 128 finished with value: 0.6304666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8062205603248195\n",
      "Epoch 1, Training Loss: 1.091696035772338\n",
      "Epoch 2, Training Loss: 1.091131819818253\n",
      "Epoch 3, Training Loss: 1.0910230849918565\n",
      "Epoch 4, Training Loss: 1.091058539806452\n",
      "Epoch 5, Training Loss: 1.0909652066410036\n",
      "Epoch 6, Training Loss: 1.0908723483408305\n",
      "Epoch 7, Training Loss: 1.0907271700694148\n",
      "Epoch 8, Training Loss: 1.0906910582592613\n",
      "Epoch 9, Training Loss: 1.0905400337133193\n",
      "Epoch 10, Training Loss: 1.0904638681196628\n",
      "Epoch 11, Training Loss: 1.0904282356563368\n",
      "Epoch 12, Training Loss: 1.0901820051938969\n",
      "Epoch 13, Training Loss: 1.0901525395257132\n",
      "Epoch 14, Training Loss: 1.0898931211098692\n",
      "Epoch 15, Training Loss: 1.0895765704319889\n",
      "Epoch 16, Training Loss: 1.0895858036844355\n",
      "Epoch 17, Training Loss: 1.0892950253379077\n",
      "Epoch 18, Training Loss: 1.0890006068953895\n",
      "Epoch 19, Training Loss: 1.0886147719576842\n",
      "Epoch 20, Training Loss: 1.0883484250620792\n",
      "Epoch 21, Training Loss: 1.0878532762814286\n",
      "Epoch 22, Training Loss: 1.0871954609576922\n",
      "Epoch 23, Training Loss: 1.0863877895183134\n",
      "Epoch 24, Training Loss: 1.0854952774549786\n",
      "Epoch 25, Training Loss: 1.0843925246618744\n",
      "Epoch 26, Training Loss: 1.0829057317031057\n",
      "Epoch 27, Training Loss: 1.0806987228250144\n",
      "Epoch 28, Training Loss: 1.077746157717884\n",
      "Epoch 29, Training Loss: 1.0736287360800836\n",
      "Epoch 30, Training Loss: 1.0678255502442668\n",
      "Epoch 31, Training Loss: 1.0592581085692672\n",
      "Epoch 32, Training Loss: 1.0475221381151587\n",
      "Epoch 33, Training Loss: 1.0326911636761256\n",
      "Epoch 34, Training Loss: 1.0182669610905468\n",
      "Epoch 35, Training Loss: 1.0072173623214091\n",
      "Epoch 36, Training Loss: 1.0006401332697474\n",
      "Epoch 37, Training Loss: 0.9971749847992919\n",
      "Epoch 38, Training Loss: 0.9949032089764014\n",
      "Epoch 39, Training Loss: 0.9928180358463661\n",
      "Epoch 40, Training Loss: 0.9906123249154342\n",
      "Epoch 41, Training Loss: 0.9887895394984941\n",
      "Epoch 42, Training Loss: 0.986555368380439\n",
      "Epoch 43, Training Loss: 0.9839057914296487\n",
      "Epoch 44, Training Loss: 0.9814928087076746\n",
      "Epoch 45, Training Loss: 0.9784599760421236\n",
      "Epoch 46, Training Loss: 0.9753316386301715\n",
      "Epoch 47, Training Loss: 0.9720358442543144\n",
      "Epoch 48, Training Loss: 0.9679391566075777\n",
      "Epoch 49, Training Loss: 0.9649326251861744\n",
      "Epoch 50, Training Loss: 0.960229354155691\n",
      "Epoch 51, Training Loss: 0.9561806056732521\n",
      "Epoch 52, Training Loss: 0.9523808477516461\n",
      "Epoch 53, Training Loss: 0.9495433723119865\n",
      "Epoch 54, Training Loss: 0.947117026676809\n",
      "Epoch 55, Training Loss: 0.9443342437421469\n",
      "Epoch 56, Training Loss: 0.943066638484037\n",
      "Epoch 57, Training Loss: 0.9403234081160753\n",
      "Epoch 58, Training Loss: 0.9383595990955381\n",
      "Epoch 59, Training Loss: 0.9359072566032409\n",
      "Epoch 60, Training Loss: 0.9335130484480607\n",
      "Epoch 61, Training Loss: 0.9312045294539373\n",
      "Epoch 62, Training Loss: 0.9280077273684336\n",
      "Epoch 63, Training Loss: 0.9241446773808701\n",
      "Epoch 64, Training Loss: 0.9208257121251042\n",
      "Epoch 65, Training Loss: 0.9164703150440876\n",
      "Epoch 66, Training Loss: 0.9116076284781435\n",
      "Epoch 67, Training Loss: 0.9066373749783164\n",
      "Epoch 68, Training Loss: 0.9006132607173203\n",
      "Epoch 69, Training Loss: 0.894525402829163\n",
      "Epoch 70, Training Loss: 0.8889484504111728\n",
      "Epoch 71, Training Loss: 0.8817626855427161\n",
      "Epoch 72, Training Loss: 0.8748438121680927\n",
      "Epoch 73, Training Loss: 0.8682848305630505\n",
      "Epoch 74, Training Loss: 0.8627842157406914\n",
      "Epoch 75, Training Loss: 0.8579292175465061\n",
      "Epoch 76, Training Loss: 0.8538241234937108\n",
      "Epoch 77, Training Loss: 0.8498379329093417\n",
      "Epoch 78, Training Loss: 0.8472230007773951\n",
      "Epoch 79, Training Loss: 0.8443988667394882\n",
      "Epoch 80, Training Loss: 0.8436561726986017\n",
      "Epoch 81, Training Loss: 0.8415950513423833\n",
      "Epoch 82, Training Loss: 0.8402561552542492\n",
      "Epoch 83, Training Loss: 0.8390117393400436\n",
      "Epoch 84, Training Loss: 0.8391912057883757\n",
      "Epoch 85, Training Loss: 0.8380232551940402\n",
      "Epoch 86, Training Loss: 0.8369356807909514\n",
      "Epoch 87, Training Loss: 0.8357564756744786\n",
      "Epoch 88, Training Loss: 0.8351468030671428\n",
      "Epoch 89, Training Loss: 0.8348387984404887\n",
      "Epoch 90, Training Loss: 0.8343992530851436\n",
      "Epoch 91, Training Loss: 0.8336316264661631\n",
      "Epoch 92, Training Loss: 0.833207124695742\n",
      "Epoch 93, Training Loss: 0.8326746622422584\n",
      "Epoch 94, Training Loss: 0.8322874327351276\n",
      "Epoch 95, Training Loss: 0.831312164328152\n",
      "Epoch 96, Training Loss: 0.831277644992771\n",
      "Epoch 97, Training Loss: 0.8307631635128108\n",
      "Epoch 98, Training Loss: 0.8297937929182124\n",
      "Epoch 99, Training Loss: 0.8294809338741733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:26:59,963] Trial 129 finished with value: 0.6134666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8284035874488659\n",
      "Epoch 1, Training Loss: 0.8790924880737648\n",
      "Epoch 2, Training Loss: 0.8278626082535077\n",
      "Epoch 3, Training Loss: 0.8175444516024195\n",
      "Epoch 4, Training Loss: 0.8141274065003359\n",
      "Epoch 5, Training Loss: 0.8118910149524087\n",
      "Epoch 6, Training Loss: 0.809943972135845\n",
      "Epoch 7, Training Loss: 0.8080681924084971\n",
      "Epoch 8, Training Loss: 0.8065423599759439\n",
      "Epoch 9, Training Loss: 0.8068592331463234\n",
      "Epoch 10, Training Loss: 0.8053849020398649\n",
      "Epoch 11, Training Loss: 0.8051918802404763\n",
      "Epoch 12, Training Loss: 0.8036898854083585\n",
      "Epoch 13, Training Loss: 0.8037635543292626\n",
      "Epoch 14, Training Loss: 0.8027231765868968\n",
      "Epoch 15, Training Loss: 0.8038140141874328\n",
      "Epoch 16, Training Loss: 0.8011848929233121\n",
      "Epoch 17, Training Loss: 0.8008147861724509\n",
      "Epoch 18, Training Loss: 0.8003414579800197\n",
      "Epoch 19, Training Loss: 0.8004048294590828\n",
      "Epoch 20, Training Loss: 0.8006435385324004\n",
      "Epoch 21, Training Loss: 0.7996759082141676\n",
      "Epoch 22, Training Loss: 0.7995764083880231\n",
      "Epoch 23, Training Loss: 0.7994989702576085\n",
      "Epoch 24, Training Loss: 0.7988575211144928\n",
      "Epoch 25, Training Loss: 0.7983364790005791\n",
      "Epoch 26, Training Loss: 0.7985759182083876\n",
      "Epoch 27, Training Loss: 0.7995686818782548\n",
      "Epoch 28, Training Loss: 0.7986397479709826\n",
      "Epoch 29, Training Loss: 0.7988251349083463\n",
      "Epoch 30, Training Loss: 0.7974404024898558\n",
      "Epoch 31, Training Loss: 0.7972196307397427\n",
      "Epoch 32, Training Loss: 0.7982887452706359\n",
      "Epoch 33, Training Loss: 0.7978843988332533\n",
      "Epoch 34, Training Loss: 0.7977763651008892\n",
      "Epoch 35, Training Loss: 0.7985673133591961\n",
      "Epoch 36, Training Loss: 0.7977474195616586\n",
      "Epoch 37, Training Loss: 0.7969016182691531\n",
      "Epoch 38, Training Loss: 0.7979252805387167\n",
      "Epoch 39, Training Loss: 0.7978489489483654\n",
      "Epoch 40, Training Loss: 0.7970999668415327\n",
      "Epoch 41, Training Loss: 0.7972510105685184\n",
      "Epoch 42, Training Loss: 0.7977010097718776\n",
      "Epoch 43, Training Loss: 0.7971340733363216\n",
      "Epoch 44, Training Loss: 0.7957849266833829\n",
      "Epoch 45, Training Loss: 0.7964943456470518\n",
      "Epoch 46, Training Loss: 0.7965841787202018\n",
      "Epoch 47, Training Loss: 0.7956817087374235\n",
      "Epoch 48, Training Loss: 0.7960420801227254\n",
      "Epoch 49, Training Loss: 0.7973953349249704\n",
      "Epoch 50, Training Loss: 0.797172314690468\n",
      "Epoch 51, Training Loss: 0.7962650693448863\n",
      "Epoch 52, Training Loss: 0.796847853355838\n",
      "Epoch 53, Training Loss: 0.7966092333757788\n",
      "Epoch 54, Training Loss: 0.796277954345359\n",
      "Epoch 55, Training Loss: 0.7958767345973423\n",
      "Epoch 56, Training Loss: 0.7960517056006238\n",
      "Epoch 57, Training Loss: 0.7953912408728349\n",
      "Epoch 58, Training Loss: 0.7966485850793078\n",
      "Epoch 59, Training Loss: 0.7957229799794076\n",
      "Epoch 60, Training Loss: 0.7955611619734226\n",
      "Epoch 61, Training Loss: 0.7958985170923678\n",
      "Epoch 62, Training Loss: 0.7952320837436762\n",
      "Epoch 63, Training Loss: 0.7949956262918343\n",
      "Epoch 64, Training Loss: 0.7957988832229959\n",
      "Epoch 65, Training Loss: 0.795722222328186\n",
      "Epoch 66, Training Loss: 0.7953537016882932\n",
      "Epoch 67, Training Loss: 0.7953692066938357\n",
      "Epoch 68, Training Loss: 0.7951971462794712\n",
      "Epoch 69, Training Loss: 0.7950109621635953\n",
      "Epoch 70, Training Loss: 0.7950811021310046\n",
      "Epoch 71, Training Loss: 0.7947360806895378\n",
      "Epoch 72, Training Loss: 0.7949461617864164\n",
      "Epoch 73, Training Loss: 0.7950068111706498\n",
      "Epoch 74, Training Loss: 0.7957858843910963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:28:22,112] Trial 130 finished with value: 0.5082666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7952067979296348\n",
      "Epoch 1, Training Loss: 0.8649110319919155\n",
      "Epoch 2, Training Loss: 0.82754096016848\n",
      "Epoch 3, Training Loss: 0.8211582025190941\n",
      "Epoch 4, Training Loss: 0.8189597428293156\n",
      "Epoch 5, Training Loss: 0.8171546201060589\n",
      "Epoch 6, Training Loss: 0.8160955554560612\n",
      "Epoch 7, Training Loss: 0.8131445306584351\n",
      "Epoch 8, Training Loss: 0.8121941831775178\n",
      "Epoch 9, Training Loss: 0.8118056027512801\n",
      "Epoch 10, Training Loss: 0.810411887061327\n",
      "Epoch 11, Training Loss: 0.8096711723428024\n",
      "Epoch 12, Training Loss: 0.8084879271966174\n",
      "Epoch 13, Training Loss: 0.8083585140400363\n",
      "Epoch 14, Training Loss: 0.8074044141554295\n",
      "Epoch 15, Training Loss: 0.8077398501840749\n",
      "Epoch 16, Training Loss: 0.8071832389759838\n",
      "Epoch 17, Training Loss: 0.807463814979209\n",
      "Epoch 18, Training Loss: 0.8062056562954322\n",
      "Epoch 19, Training Loss: 0.8079954902032265\n",
      "Epoch 20, Training Loss: 0.8063065549484769\n",
      "Epoch 21, Training Loss: 0.8061183340567395\n",
      "Epoch 22, Training Loss: 0.8056845325276367\n",
      "Epoch 23, Training Loss: 0.8052769092688883\n",
      "Epoch 24, Training Loss: 0.8061396939413888\n",
      "Epoch 25, Training Loss: 0.8045715730889399\n",
      "Epoch 26, Training Loss: 0.8054417006951525\n",
      "Epoch 27, Training Loss: 0.8054352702054762\n",
      "Epoch 28, Training Loss: 0.8054404714950045\n",
      "Epoch 29, Training Loss: 0.8052031836115328\n",
      "Epoch 30, Training Loss: 0.8051814048810113\n",
      "Epoch 31, Training Loss: 0.8048441777551981\n",
      "Epoch 32, Training Loss: 0.8045416246679492\n",
      "Epoch 33, Training Loss: 0.80453358879663\n",
      "Epoch 34, Training Loss: 0.8045956401896656\n",
      "Epoch 35, Training Loss: 0.8037625446355432\n",
      "Epoch 36, Training Loss: 0.8039171655375258\n",
      "Epoch 37, Training Loss: 0.803330811880585\n",
      "Epoch 38, Training Loss: 0.8031512849761131\n",
      "Epoch 39, Training Loss: 0.8031134457516491\n",
      "Epoch 40, Training Loss: 0.8029384388959497\n",
      "Epoch 41, Training Loss: 0.8024105763973151\n",
      "Epoch 42, Training Loss: 0.8030877785575121\n",
      "Epoch 43, Training Loss: 0.8038926413184718\n",
      "Epoch 44, Training Loss: 0.8031083038875035\n",
      "Epoch 45, Training Loss: 0.8024563119823771\n",
      "Epoch 46, Training Loss: 0.8015463026842676\n",
      "Epoch 47, Training Loss: 0.8019228453026679\n",
      "Epoch 48, Training Loss: 0.8017738195290243\n",
      "Epoch 49, Training Loss: 0.8014277397241808\n",
      "Epoch 50, Training Loss: 0.8023484292783235\n",
      "Epoch 51, Training Loss: 0.8011612071130508\n",
      "Epoch 52, Training Loss: 0.8005094439463508\n",
      "Epoch 53, Training Loss: 0.8008389292803025\n",
      "Epoch 54, Training Loss: 0.8004272336350348\n",
      "Epoch 55, Training Loss: 0.8009491062702093\n",
      "Epoch 56, Training Loss: 0.7998410438236437\n",
      "Epoch 57, Training Loss: 0.8005727784974235\n",
      "Epoch 58, Training Loss: 0.8007099683123423\n",
      "Epoch 59, Training Loss: 0.8000957330366723\n",
      "Epoch 60, Training Loss: 0.7997633532473916\n",
      "Epoch 61, Training Loss: 0.7997062250187522\n",
      "Epoch 62, Training Loss: 0.7997897730734115\n",
      "Epoch 63, Training Loss: 0.8002735497359943\n",
      "Epoch 64, Training Loss: 0.8000726673836098\n",
      "Epoch 65, Training Loss: 0.7995572682610131\n",
      "Epoch 66, Training Loss: 0.7996625549811169\n",
      "Epoch 67, Training Loss: 0.79973373332418\n",
      "Epoch 68, Training Loss: 0.7985184570004169\n",
      "Epoch 69, Training Loss: 0.7993222708092597\n",
      "Epoch 70, Training Loss: 0.7990119974416001\n",
      "Epoch 71, Training Loss: 0.7987064746985758\n",
      "Epoch 72, Training Loss: 0.7986571488523841\n",
      "Epoch 73, Training Loss: 0.7986862420139457\n",
      "Epoch 74, Training Loss: 0.7985227184188097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:29:32,757] Trial 131 finished with value: 0.6225333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7985433795398339\n",
      "Epoch 1, Training Loss: 0.8993964561842438\n",
      "Epoch 2, Training Loss: 0.8412660038560853\n",
      "Epoch 3, Training Loss: 0.8334472564826334\n",
      "Epoch 4, Training Loss: 0.8276834041552437\n",
      "Epoch 5, Training Loss: 0.8250107792983378\n",
      "Epoch 6, Training Loss: 0.8249087776456561\n",
      "Epoch 7, Training Loss: 0.821506669109029\n",
      "Epoch 8, Training Loss: 0.8217512688242403\n",
      "Epoch 9, Training Loss: 0.8214274850106777\n",
      "Epoch 10, Training Loss: 0.8200483061317214\n",
      "Epoch 11, Training Loss: 0.8193170626360671\n",
      "Epoch 12, Training Loss: 0.8184547307796048\n",
      "Epoch 13, Training Loss: 0.817422926874089\n",
      "Epoch 14, Training Loss: 0.818306537319843\n",
      "Epoch 15, Training Loss: 0.8174102094836702\n",
      "Epoch 16, Training Loss: 0.8186205759980625\n",
      "Epoch 17, Training Loss: 0.8161439214433942\n",
      "Epoch 18, Training Loss: 0.8195833822838345\n",
      "Epoch 19, Training Loss: 0.8161560965659923\n",
      "Epoch 20, Training Loss: 0.8185723909758087\n",
      "Epoch 21, Training Loss: 0.8181597136017075\n",
      "Epoch 22, Training Loss: 0.8158053537060443\n",
      "Epoch 23, Training Loss: 0.8174922015433921\n",
      "Epoch 24, Training Loss: 0.8185538292827462\n",
      "Epoch 25, Training Loss: 0.8180438962197841\n",
      "Epoch 26, Training Loss: 0.8177801847457886\n",
      "Epoch 27, Training Loss: 0.816036045372038\n",
      "Epoch 28, Training Loss: 0.8172545635610595\n",
      "Epoch 29, Training Loss: 0.8173664995602199\n",
      "Epoch 30, Training Loss: 0.8155217731805672\n",
      "Epoch 31, Training Loss: 0.8158383048566661\n",
      "Epoch 32, Training Loss: 0.8175929269396273\n",
      "Epoch 33, Training Loss: 0.8187257182329221\n",
      "Epoch 34, Training Loss: 0.8163454719055864\n",
      "Epoch 35, Training Loss: 0.817249261884761\n",
      "Epoch 36, Training Loss: 0.8179729529789516\n",
      "Epoch 37, Training Loss: 0.8172015659791186\n",
      "Epoch 38, Training Loss: 0.8176345748112614\n",
      "Epoch 39, Training Loss: 0.8171140080107782\n",
      "Epoch 40, Training Loss: 0.8185430351952862\n",
      "Epoch 41, Training Loss: 0.8160659119598848\n",
      "Epoch 42, Training Loss: 0.8171051811903043\n",
      "Epoch 43, Training Loss: 0.8167315441862981\n",
      "Epoch 44, Training Loss: 0.8164299544535185\n",
      "Epoch 45, Training Loss: 0.816532524098131\n",
      "Epoch 46, Training Loss: 0.8156243235545051\n",
      "Epoch 47, Training Loss: 0.8159778166534309\n",
      "Epoch 48, Training Loss: 0.8163873513838402\n",
      "Epoch 49, Training Loss: 0.8172175644931937\n",
      "Epoch 50, Training Loss: 0.818133604974675\n",
      "Epoch 51, Training Loss: 0.8171826757882771\n",
      "Epoch 52, Training Loss: 0.8158779580790297\n",
      "Epoch 53, Training Loss: 0.8176763868869695\n",
      "Epoch 54, Training Loss: 0.817793869972229\n",
      "Epoch 55, Training Loss: 0.8158189384560836\n",
      "Epoch 56, Training Loss: 0.8171203107762157\n",
      "Epoch 57, Training Loss: 0.8175724502792932\n",
      "Epoch 58, Training Loss: 0.8140481974845543\n",
      "Epoch 59, Training Loss: 0.8150332304768096\n",
      "Epoch 60, Training Loss: 0.8175514886253759\n",
      "Epoch 61, Training Loss: 0.8142320631141949\n",
      "Epoch 62, Training Loss: 0.8154667672358061\n",
      "Epoch 63, Training Loss: 0.8162882620230654\n",
      "Epoch 64, Training Loss: 0.8162343815753335\n",
      "Epoch 65, Training Loss: 0.8174175821748891\n",
      "Epoch 66, Training Loss: 0.817032992302027\n",
      "Epoch 67, Training Loss: 0.8172736181352371\n",
      "Epoch 68, Training Loss: 0.8180730424429241\n",
      "Epoch 69, Training Loss: 0.8176238977819457\n",
      "Epoch 70, Training Loss: 0.8161332696900332\n",
      "Epoch 71, Training Loss: 0.8171053202528703\n",
      "Epoch 72, Training Loss: 0.8179473875160505\n",
      "Epoch 73, Training Loss: 0.8180669093490543\n",
      "Epoch 74, Training Loss: 0.8174451853099622\n",
      "Epoch 75, Training Loss: 0.8189074735892446\n",
      "Epoch 76, Training Loss: 0.8163094147703701\n",
      "Epoch 77, Training Loss: 0.815718352436123\n",
      "Epoch 78, Training Loss: 0.8180379570874953\n",
      "Epoch 79, Training Loss: 0.8164738283121497\n",
      "Epoch 80, Training Loss: 0.8178891452631556\n",
      "Epoch 81, Training Loss: 0.8164794935319657\n",
      "Epoch 82, Training Loss: 0.8162943494051023\n",
      "Epoch 83, Training Loss: 0.8210400124241535\n",
      "Epoch 84, Training Loss: 0.8155170319671917\n",
      "Epoch 85, Training Loss: 0.8203310061218148\n",
      "Epoch 86, Training Loss: 0.815635967882056\n",
      "Epoch 87, Training Loss: 0.8164918505159536\n",
      "Epoch 88, Training Loss: 0.8159983931627489\n",
      "Epoch 89, Training Loss: 0.817979813876905\n",
      "Epoch 90, Training Loss: 0.81671150384989\n",
      "Epoch 91, Training Loss: 0.8180688408084382\n",
      "Epoch 92, Training Loss: 0.8181213389661975\n",
      "Epoch 93, Training Loss: 0.8186641618721467\n",
      "Epoch 94, Training Loss: 0.8160675511324316\n",
      "Epoch 95, Training Loss: 0.8143842711484521\n",
      "Epoch 96, Training Loss: 0.8152649969982921\n",
      "Epoch 97, Training Loss: 0.8147433695040252\n",
      "Epoch 98, Training Loss: 0.8164825776465854\n",
      "Epoch 99, Training Loss: 0.8179060987063816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:31:35,259] Trial 132 finished with value: 0.49706666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8178656550278341\n",
      "Epoch 1, Training Loss: 1.0579714322090148\n",
      "Epoch 2, Training Loss: 1.0120044729288886\n",
      "Epoch 3, Training Loss: 0.9876360535621643\n",
      "Epoch 4, Training Loss: 0.9752758785556345\n",
      "Epoch 5, Training Loss: 0.968976308668361\n",
      "Epoch 6, Training Loss: 0.9656889625857858\n",
      "Epoch 7, Training Loss: 0.9636437033204471\n",
      "Epoch 8, Training Loss: 0.9621751381369198\n",
      "Epoch 9, Training Loss: 0.960875140989528\n",
      "Epoch 10, Training Loss: 0.9595547125620001\n",
      "Epoch 11, Training Loss: 0.9583649575710297\n",
      "Epoch 12, Training Loss: 0.957196286425871\n",
      "Epoch 13, Training Loss: 0.9560271622152889\n",
      "Epoch 14, Training Loss: 0.9547089536049786\n",
      "Epoch 15, Training Loss: 0.953569288183661\n",
      "Epoch 16, Training Loss: 0.9522463318179636\n",
      "Epoch 17, Training Loss: 0.9509557015054366\n",
      "Epoch 18, Training Loss: 0.9495305540982415\n",
      "Epoch 19, Training Loss: 0.9482233037668116\n",
      "Epoch 20, Training Loss: 0.9467438721656799\n",
      "Epoch 21, Training Loss: 0.9453363651387832\n",
      "Epoch 22, Training Loss: 0.9439005141398485\n",
      "Epoch 23, Training Loss: 0.9423103157912983\n",
      "Epoch 24, Training Loss: 0.9407408933779773\n",
      "Epoch 25, Training Loss: 0.9391591543309828\n",
      "Epoch 26, Training Loss: 0.9375216169217053\n",
      "Epoch 27, Training Loss: 0.9358069189857035\n",
      "Epoch 28, Training Loss: 0.934010153167388\n",
      "Epoch 29, Training Loss: 0.9322591015871833\n",
      "Epoch 30, Training Loss: 0.9303675871035632\n",
      "Epoch 31, Training Loss: 0.9283727840115042\n",
      "Epoch 32, Training Loss: 0.9265375926214106\n",
      "Epoch 33, Training Loss: 0.9244909111892475\n",
      "Epoch 34, Training Loss: 0.9224352281233844\n",
      "Epoch 35, Training Loss: 0.9203797766040354\n",
      "Epoch 36, Training Loss: 0.9182944743072285\n",
      "Epoch 37, Training Loss: 0.9160165284661685\n",
      "Epoch 38, Training Loss: 0.9138256528798272\n",
      "Epoch 39, Training Loss: 0.9115304005847258\n",
      "Epoch 40, Training Loss: 0.9092186932002797\n",
      "Epoch 41, Training Loss: 0.9068112972203423\n",
      "Epoch 42, Training Loss: 0.904394541347728\n",
      "Epoch 43, Training Loss: 0.9020687338183908\n",
      "Epoch 44, Training Loss: 0.8995935455490561\n",
      "Epoch 45, Training Loss: 0.8971360146298129\n",
      "Epoch 46, Training Loss: 0.8946305798081791\n",
      "Epoch 47, Training Loss: 0.8922395926363328\n",
      "Epoch 48, Training Loss: 0.8896254811567419\n",
      "Epoch 49, Training Loss: 0.8871043759233811\n",
      "Epoch 50, Training Loss: 0.8847029128495385\n",
      "Epoch 51, Training Loss: 0.8822112396885367\n",
      "Epoch 52, Training Loss: 0.8796383722389446\n",
      "Epoch 53, Training Loss: 0.8771390522928799\n",
      "Epoch 54, Training Loss: 0.874692118658739\n",
      "Epoch 55, Training Loss: 0.8722534302402946\n",
      "Epoch 56, Training Loss: 0.8698425006866455\n",
      "Epoch 57, Training Loss: 0.8675278656622942\n",
      "Epoch 58, Training Loss: 0.8651248417882358\n",
      "Epoch 59, Training Loss: 0.8629134358378018\n",
      "Epoch 60, Training Loss: 0.8606724516784443\n",
      "Epoch 61, Training Loss: 0.8584326590509975\n",
      "Epoch 62, Training Loss: 0.8563159576584312\n",
      "Epoch 63, Training Loss: 0.8542656025465797\n",
      "Epoch 64, Training Loss: 0.8522833420949824\n",
      "Epoch 65, Training Loss: 0.8502912633559283\n",
      "Epoch 66, Training Loss: 0.848471103556016\n",
      "Epoch 67, Training Loss: 0.8466382907418644\n",
      "Epoch 68, Training Loss: 0.8448567717215594\n",
      "Epoch 69, Training Loss: 0.8431562589196597\n",
      "Epoch 70, Training Loss: 0.8415593316975762\n",
      "Epoch 71, Training Loss: 0.8399660237396465\n",
      "Epoch 72, Training Loss: 0.8384736807907329\n",
      "Epoch 73, Training Loss: 0.8370721631190357\n",
      "Epoch 74, Training Loss: 0.8356479038210476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:32:45,544] Trial 133 finished with value: 0.6146666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8343880904422086\n",
      "Epoch 1, Training Loss: 0.9897787014344581\n",
      "Epoch 2, Training Loss: 0.9437426099203583\n",
      "Epoch 3, Training Loss: 0.9212045878395998\n",
      "Epoch 4, Training Loss: 0.8902391698127402\n",
      "Epoch 5, Training Loss: 0.8547245972138598\n",
      "Epoch 6, Training Loss: 0.8295878702536561\n",
      "Epoch 7, Training Loss: 0.8182781819114111\n",
      "Epoch 8, Training Loss: 0.8139454525216181\n",
      "Epoch 9, Training Loss: 0.811917485688862\n",
      "Epoch 10, Training Loss: 0.8107184864524611\n",
      "Epoch 11, Training Loss: 0.8101106456347874\n",
      "Epoch 12, Training Loss: 0.8096452344629101\n",
      "Epoch 13, Training Loss: 0.808188341613999\n",
      "Epoch 14, Training Loss: 0.8077409477162182\n",
      "Epoch 15, Training Loss: 0.80701447895595\n",
      "Epoch 16, Training Loss: 0.8065873067181809\n",
      "Epoch 17, Training Loss: 0.8064817221541154\n",
      "Epoch 18, Training Loss: 0.8056607780599953\n",
      "Epoch 19, Training Loss: 0.8050121028620497\n",
      "Epoch 20, Training Loss: 0.8044917941093445\n",
      "Epoch 21, Training Loss: 0.8046738334168169\n",
      "Epoch 22, Training Loss: 0.8034760203576625\n",
      "Epoch 23, Training Loss: 0.8040790097157758\n",
      "Epoch 24, Training Loss: 0.8026207311709125\n",
      "Epoch 25, Training Loss: 0.802851436819349\n",
      "Epoch 26, Training Loss: 0.802900671331506\n",
      "Epoch 27, Training Loss: 0.801881758700636\n",
      "Epoch 28, Training Loss: 0.8011533603184205\n",
      "Epoch 29, Training Loss: 0.8022759145363829\n",
      "Epoch 30, Training Loss: 0.8020761471045644\n",
      "Epoch 31, Training Loss: 0.8008692907211475\n",
      "Epoch 32, Training Loss: 0.801337816751093\n",
      "Epoch 33, Training Loss: 0.8019288264719168\n",
      "Epoch 34, Training Loss: 0.801488250897343\n",
      "Epoch 35, Training Loss: 0.8004639521577305\n",
      "Epoch 36, Training Loss: 0.800782331040031\n",
      "Epoch 37, Training Loss: 0.801227309022631\n",
      "Epoch 38, Training Loss: 0.801788061902039\n",
      "Epoch 39, Training Loss: 0.8000550916320399\n",
      "Epoch 40, Training Loss: 0.8003615001090487\n",
      "Epoch 41, Training Loss: 0.8001914519116394\n",
      "Epoch 42, Training Loss: 0.7996841815181245\n",
      "Epoch 43, Training Loss: 0.7996907162487059\n",
      "Epoch 44, Training Loss: 0.7998558834979409\n",
      "Epoch 45, Training Loss: 0.8000809892675931\n",
      "Epoch 46, Training Loss: 0.800304499066862\n",
      "Epoch 47, Training Loss: 0.7991893810437138\n",
      "Epoch 48, Training Loss: 0.7987860105987779\n",
      "Epoch 49, Training Loss: 0.7997450737128581\n",
      "Epoch 50, Training Loss: 0.8003130884995138\n",
      "Epoch 51, Training Loss: 0.7992783354637318\n",
      "Epoch 52, Training Loss: 0.7987108806022127\n",
      "Epoch 53, Training Loss: 0.7994380623774421\n",
      "Epoch 54, Training Loss: 0.7983523003140787\n",
      "Epoch 55, Training Loss: 0.798926471677938\n",
      "Epoch 56, Training Loss: 0.7984740514504282\n",
      "Epoch 57, Training Loss: 0.7990196660048979\n",
      "Epoch 58, Training Loss: 0.7990107108775835\n",
      "Epoch 59, Training Loss: 0.7985934206417629\n",
      "Epoch 60, Training Loss: 0.7978638251024978\n",
      "Epoch 61, Training Loss: 0.798009253086004\n",
      "Epoch 62, Training Loss: 0.7981377979866544\n",
      "Epoch 63, Training Loss: 0.7979831023323805\n",
      "Epoch 64, Training Loss: 0.7979238061976612\n",
      "Epoch 65, Training Loss: 0.7984841349429654\n",
      "Epoch 66, Training Loss: 0.7975063924502609\n",
      "Epoch 67, Training Loss: 0.7977245714431419\n",
      "Epoch 68, Training Loss: 0.7976292267777866\n",
      "Epoch 69, Training Loss: 0.7979072325211719\n",
      "Epoch 70, Training Loss: 0.7975320163526033\n",
      "Epoch 71, Training Loss: 0.7975851641561752\n",
      "Epoch 72, Training Loss: 0.7975040222469129\n",
      "Epoch 73, Training Loss: 0.7971863942935055\n",
      "Epoch 74, Training Loss: 0.7971620141115404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:34:07,403] Trial 134 finished with value: 0.6346 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7965282299016652\n",
      "Epoch 1, Training Loss: 1.0954022702048807\n",
      "Epoch 2, Training Loss: 1.0863727775742027\n",
      "Epoch 3, Training Loss: 1.0801195133433623\n",
      "Epoch 4, Training Loss: 1.0746916705019334\n",
      "Epoch 5, Training Loss: 1.0691752513717203\n",
      "Epoch 6, Training Loss: 1.0630372752862818\n",
      "Epoch 7, Training Loss: 1.0559902320188634\n",
      "Epoch 8, Training Loss: 1.047886938347536\n",
      "Epoch 9, Training Loss: 1.0386085370007683\n",
      "Epoch 10, Training Loss: 1.028081786141676\n",
      "Epoch 11, Training Loss: 1.0164404902738684\n",
      "Epoch 12, Training Loss: 1.0040843024674584\n",
      "Epoch 13, Training Loss: 0.9915710539677564\n",
      "Epoch 14, Training Loss: 0.9795357098298915\n",
      "Epoch 15, Training Loss: 0.96863933486097\n",
      "Epoch 16, Training Loss: 0.9593145312982447\n",
      "Epoch 17, Training Loss: 0.9517311021860908\n",
      "Epoch 18, Training Loss: 0.9458181452049929\n",
      "Epoch 19, Training Loss: 0.9412558075259714\n",
      "Epoch 20, Training Loss: 0.937733746696921\n",
      "Epoch 21, Training Loss: 0.934953066180734\n",
      "Epoch 22, Training Loss: 0.9326743078933043\n",
      "Epoch 23, Training Loss: 0.9307295110646416\n",
      "Epoch 24, Training Loss: 0.9289993038598229\n",
      "Epoch 25, Training Loss: 0.927425464111216\n",
      "Epoch 26, Training Loss: 0.9259324418095981\n",
      "Epoch 27, Training Loss: 0.9244837454487296\n",
      "Epoch 28, Training Loss: 0.92309987152324\n",
      "Epoch 29, Training Loss: 0.9217191391832689\n",
      "Epoch 30, Training Loss: 0.9203778374195098\n",
      "Epoch 31, Training Loss: 0.9190713496769176\n",
      "Epoch 32, Training Loss: 0.9177929603352266\n",
      "Epoch 33, Training Loss: 0.9165196988863104\n",
      "Epoch 34, Training Loss: 0.9152613054303562\n",
      "Epoch 35, Training Loss: 0.9139919745220858\n",
      "Epoch 36, Training Loss: 0.9127227180144366\n",
      "Epoch 37, Training Loss: 0.9114546661517199\n",
      "Epoch 38, Training Loss: 0.9101656985282898\n",
      "Epoch 39, Training Loss: 0.908831797768088\n",
      "Epoch 40, Training Loss: 0.9075170784136828\n",
      "Epoch 41, Training Loss: 0.9061480752159567\n",
      "Epoch 42, Training Loss: 0.9047443451600916\n",
      "Epoch 43, Training Loss: 0.9033186916042777\n",
      "Epoch 44, Training Loss: 0.9018465941793778\n",
      "Epoch 45, Training Loss: 0.9003252222257502\n",
      "Epoch 46, Training Loss: 0.8987351799712462\n",
      "Epoch 47, Training Loss: 0.8971044227656196\n",
      "Epoch 48, Training Loss: 0.8954025091143215\n",
      "Epoch 49, Training Loss: 0.8935965242105371\n",
      "Epoch 50, Training Loss: 0.8917427921295166\n",
      "Epoch 51, Training Loss: 0.8897727396908929\n",
      "Epoch 52, Training Loss: 0.8877489733695983\n",
      "Epoch 53, Training Loss: 0.8855716696907492\n",
      "Epoch 54, Training Loss: 0.8832925578425912\n",
      "Epoch 55, Training Loss: 0.8809542817227981\n",
      "Epoch 56, Training Loss: 0.8784548220914953\n",
      "Epoch 57, Training Loss: 0.8758525189932654\n",
      "Epoch 58, Training Loss: 0.873147217315786\n",
      "Epoch 59, Training Loss: 0.8703129830781151\n",
      "Epoch 60, Training Loss: 0.8673723698363585\n",
      "Epoch 61, Training Loss: 0.8643229741909925\n",
      "Epoch 62, Training Loss: 0.8611568826086381\n",
      "Epoch 63, Training Loss: 0.8578764896533069\n",
      "Epoch 64, Training Loss: 0.8545561664244707\n",
      "Epoch 65, Training Loss: 0.8511356716997484\n",
      "Epoch 66, Training Loss: 0.8478451275124269\n",
      "Epoch 67, Training Loss: 0.8444983455714058\n",
      "Epoch 68, Training Loss: 0.8411452458886539\n",
      "Epoch 69, Training Loss: 0.8379484128952026\n",
      "Epoch 70, Training Loss: 0.8347335186425378\n",
      "Epoch 71, Training Loss: 0.8318663844641517\n",
      "Epoch 72, Training Loss: 0.8290306108839371\n",
      "Epoch 73, Training Loss: 0.8264414797109716\n",
      "Epoch 74, Training Loss: 0.8240120915104361\n",
      "Epoch 75, Training Loss: 0.8217906214910395\n",
      "Epoch 76, Training Loss: 0.8197637229807236\n",
      "Epoch 77, Training Loss: 0.8179787824434392\n",
      "Epoch 78, Training Loss: 0.8163315067571753\n",
      "Epoch 79, Training Loss: 0.8149336596797494\n",
      "Epoch 80, Training Loss: 0.8136785079451169\n",
      "Epoch 81, Training Loss: 0.8125139152302462\n",
      "Epoch 82, Training Loss: 0.811481461945702\n",
      "Epoch 83, Training Loss: 0.8106823146343232\n",
      "Epoch 84, Training Loss: 0.8098667185446795\n",
      "Epoch 85, Training Loss: 0.809156317430384\n",
      "Epoch 86, Training Loss: 0.8085278203206904\n",
      "Epoch 87, Training Loss: 0.8079864751591402\n",
      "Epoch 88, Training Loss: 0.8075633186452529\n",
      "Epoch 89, Training Loss: 0.8071527606599471\n",
      "Epoch 90, Training Loss: 0.8067630596020643\n",
      "Epoch 91, Training Loss: 0.8064263972114114\n",
      "Epoch 92, Training Loss: 0.806070184918011\n",
      "Epoch 93, Training Loss: 0.8057021859112908\n",
      "Epoch 94, Training Loss: 0.8054225118020002\n",
      "Epoch 95, Training Loss: 0.8051992727728451\n",
      "Epoch 96, Training Loss: 0.8049727448295144\n",
      "Epoch 97, Training Loss: 0.8047930115811965\n",
      "Epoch 98, Training Loss: 0.8045606159462648\n",
      "Epoch 99, Training Loss: 0.8044025341202231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:36:07,059] Trial 135 finished with value: 0.6314666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8041356294996598\n",
      "Epoch 1, Training Loss: 1.064073636672076\n",
      "Epoch 2, Training Loss: 1.019068968225928\n",
      "Epoch 3, Training Loss: 0.9916019047007841\n",
      "Epoch 4, Training Loss: 0.9755599462985992\n",
      "Epoch 5, Training Loss: 0.9668123670886545\n",
      "Epoch 6, Training Loss: 0.9620618688358981\n",
      "Epoch 7, Training Loss: 0.9592115226913901\n",
      "Epoch 8, Training Loss: 0.9573971199288088\n",
      "Epoch 9, Training Loss: 0.9558839039942798\n",
      "Epoch 10, Training Loss: 0.9544839335189146\n",
      "Epoch 11, Training Loss: 0.9533558051025166\n",
      "Epoch 12, Training Loss: 0.9520943169734057\n",
      "Epoch 13, Training Loss: 0.950859366164488\n",
      "Epoch 14, Training Loss: 0.9495644994343029\n",
      "Epoch 15, Training Loss: 0.9481783214737387\n",
      "Epoch 16, Training Loss: 0.9469056285128874\n",
      "Epoch 17, Training Loss: 0.9453460959125968\n",
      "Epoch 18, Training Loss: 0.9440384213363423\n",
      "Epoch 19, Training Loss: 0.9425608090793385\n",
      "Epoch 20, Training Loss: 0.9410384471276227\n",
      "Epoch 21, Training Loss: 0.9393993548084708\n",
      "Epoch 22, Training Loss: 0.9378104435696322\n",
      "Epoch 23, Training Loss: 0.9360972418504603\n",
      "Epoch 24, Training Loss: 0.9343514495036181\n",
      "Epoch 25, Training Loss: 0.9324769747257232\n",
      "Epoch 26, Training Loss: 0.9306898673842935\n",
      "Epoch 27, Training Loss: 0.9287394354623907\n",
      "Epoch 28, Training Loss: 0.9268467745360206\n",
      "Epoch 29, Training Loss: 0.9248111879124361\n",
      "Epoch 30, Training Loss: 0.9226973290303174\n",
      "Epoch 31, Training Loss: 0.9206257195332471\n",
      "Epoch 32, Training Loss: 0.9184440574926489\n",
      "Epoch 33, Training Loss: 0.9162608087062836\n",
      "Epoch 34, Training Loss: 0.9139059150920195\n",
      "Epoch 35, Training Loss: 0.9116665281968959\n",
      "Epoch 36, Training Loss: 0.9093273756784551\n",
      "Epoch 37, Training Loss: 0.9069358831994674\n",
      "Epoch 38, Training Loss: 0.9045522236122805\n",
      "Epoch 39, Training Loss: 0.9020668489792768\n",
      "Epoch 40, Training Loss: 0.8995886761300704\n",
      "Epoch 41, Training Loss: 0.8970592345910914\n",
      "Epoch 42, Training Loss: 0.8946597202385174\n",
      "Epoch 43, Training Loss: 0.8920718014941496\n",
      "Epoch 44, Training Loss: 0.8895551319683299\n",
      "Epoch 45, Training Loss: 0.8870623193067663\n",
      "Epoch 46, Training Loss: 0.8844759374506334\n",
      "Epoch 47, Training Loss: 0.8819876689770643\n",
      "Epoch 48, Training Loss: 0.8795407732795266\n",
      "Epoch 49, Training Loss: 0.8770794338338516\n",
      "Epoch 50, Training Loss: 0.8745179766767165\n",
      "Epoch 51, Training Loss: 0.8721444077351515\n",
      "Epoch 52, Training Loss: 0.8696970004193924\n",
      "Epoch 53, Training Loss: 0.8673657375924727\n",
      "Epoch 54, Training Loss: 0.8649875807060915\n",
      "Epoch 55, Training Loss: 0.8627190484720118\n",
      "Epoch 56, Training Loss: 0.8605405709322761\n",
      "Epoch 57, Training Loss: 0.8582989839946522\n",
      "Epoch 58, Training Loss: 0.8561883738461663\n",
      "Epoch 59, Training Loss: 0.8541011382551754\n",
      "Epoch 60, Training Loss: 0.8520403420223909\n",
      "Epoch 61, Training Loss: 0.8501237165927887\n",
      "Epoch 62, Training Loss: 0.8481959857660182\n",
      "Epoch 63, Training Loss: 0.8463697425056906\n",
      "Epoch 64, Training Loss: 0.844591119149152\n",
      "Epoch 65, Training Loss: 0.8429082242180319\n",
      "Epoch 66, Training Loss: 0.8412490256393657\n",
      "Epoch 67, Training Loss: 0.8395666991261875\n",
      "Epoch 68, Training Loss: 0.8381137459418353\n",
      "Epoch 69, Training Loss: 0.8367208401595845\n",
      "Epoch 70, Training Loss: 0.8352953144382028\n",
      "Epoch 71, Training Loss: 0.8339009512172025\n",
      "Epoch 72, Training Loss: 0.8326713045204387\n",
      "Epoch 73, Training Loss: 0.831439792619032\n",
      "Epoch 74, Training Loss: 0.8302553293985478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:37:17,562] Trial 136 finished with value: 0.6180666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8291383518892176\n",
      "Epoch 1, Training Loss: 0.989186594002229\n",
      "Epoch 2, Training Loss: 0.8806427828351358\n",
      "Epoch 3, Training Loss: 0.8753638480838977\n",
      "Epoch 4, Training Loss: 0.8684259576456888\n",
      "Epoch 5, Training Loss: 0.8629842259830102\n",
      "Epoch 6, Training Loss: 0.863427746475191\n",
      "Epoch 7, Training Loss: 0.8576140211040812\n",
      "Epoch 8, Training Loss: 0.859672805151545\n",
      "Epoch 9, Training Loss: 0.8576834169993723\n",
      "Epoch 10, Training Loss: 0.8566161463135167\n",
      "Epoch 11, Training Loss: 0.8548757390868394\n",
      "Epoch 12, Training Loss: 0.8578942600945781\n",
      "Epoch 13, Training Loss: 0.8525864455932961\n",
      "Epoch 14, Training Loss: 0.8555324904900744\n",
      "Epoch 15, Training Loss: 0.8552425080672242\n",
      "Epoch 16, Training Loss: 0.8512626918634973\n",
      "Epoch 17, Training Loss: 0.8544327142543362\n",
      "Epoch 18, Training Loss: 0.8512466305180599\n",
      "Epoch 19, Training Loss: 0.8533788583332435\n",
      "Epoch 20, Training Loss: 0.8536561369895935\n",
      "Epoch 21, Training Loss: 0.8500561404945259\n",
      "Epoch 22, Training Loss: 0.8511812422508583\n",
      "Epoch 23, Training Loss: 0.8534644914748973\n",
      "Epoch 24, Training Loss: 0.8523612409606016\n",
      "Epoch 25, Training Loss: 0.8531116087633864\n",
      "Epoch 26, Training Loss: 0.8552800908124536\n",
      "Epoch 27, Training Loss: 0.8494757306306882\n",
      "Epoch 28, Training Loss: 0.8518226993711371\n",
      "Epoch 29, Training Loss: 0.8518466895684264\n",
      "Epoch 30, Training Loss: 0.8506500552471419\n",
      "Epoch 31, Training Loss: 0.8500991780058782\n",
      "Epoch 32, Training Loss: 0.8487103040953328\n",
      "Epoch 33, Training Loss: 0.8481000133026811\n",
      "Epoch 34, Training Loss: 0.8497687403420756\n",
      "Epoch 35, Training Loss: 0.8465908496003401\n",
      "Epoch 36, Training Loss: 0.8467339213629415\n",
      "Epoch 37, Training Loss: 0.8466769093857672\n",
      "Epoch 38, Training Loss: 0.846980413189508\n",
      "Epoch 39, Training Loss: 0.8468744372066699\n",
      "Epoch 40, Training Loss: 0.8485453456864321\n",
      "Epoch 41, Training Loss: 0.8466866443031713\n",
      "Epoch 42, Training Loss: 0.8464550281825819\n",
      "Epoch 43, Training Loss: 0.8465038310316272\n",
      "Epoch 44, Training Loss: 0.8467947140672153\n",
      "Epoch 45, Training Loss: 0.8488975960509222\n",
      "Epoch 46, Training Loss: 0.846650953400404\n",
      "Epoch 47, Training Loss: 0.848264693855343\n",
      "Epoch 48, Training Loss: 0.8450361718808798\n",
      "Epoch 49, Training Loss: 0.8456449661936078\n",
      "Epoch 50, Training Loss: 0.8472456751909471\n",
      "Epoch 51, Training Loss: 0.8462705544959334\n",
      "Epoch 52, Training Loss: 0.8471520713397435\n",
      "Epoch 53, Training Loss: 0.8465312311523839\n",
      "Epoch 54, Training Loss: 0.846985415856641\n",
      "Epoch 55, Training Loss: 0.8460157075322661\n",
      "Epoch 56, Training Loss: 0.847271241908683\n",
      "Epoch 57, Training Loss: 0.8463763538159822\n",
      "Epoch 58, Training Loss: 0.8465879658111056\n",
      "Epoch 59, Training Loss: 0.8482609646660941\n",
      "Epoch 60, Training Loss: 0.8422002122814494\n",
      "Epoch 61, Training Loss: 0.8466013669967651\n",
      "Epoch 62, Training Loss: 0.8440950425943934\n",
      "Epoch 63, Training Loss: 0.8451489370568355\n",
      "Epoch 64, Training Loss: 0.8474038114225058\n",
      "Epoch 65, Training Loss: 0.8482953968800997\n",
      "Epoch 66, Training Loss: 0.8444530125847436\n",
      "Epoch 67, Training Loss: 0.8441192220924492\n",
      "Epoch 68, Training Loss: 0.847938178804584\n",
      "Epoch 69, Training Loss: 0.8429344483784267\n",
      "Epoch 70, Training Loss: 0.8444383287788334\n",
      "Epoch 71, Training Loss: 0.8439912174877368\n",
      "Epoch 72, Training Loss: 0.8468306198155969\n",
      "Epoch 73, Training Loss: 0.8474841458456857\n",
      "Epoch 74, Training Loss: 0.8459802721676074\n",
      "Epoch 75, Training Loss: 0.8451338800272548\n",
      "Epoch 76, Training Loss: 0.8465167150461584\n",
      "Epoch 77, Training Loss: 0.8464232754886598\n",
      "Epoch 78, Training Loss: 0.847278286371016\n",
      "Epoch 79, Training Loss: 0.8439456696797134\n",
      "Epoch 80, Training Loss: 0.8453618564103779\n",
      "Epoch 81, Training Loss: 0.8426215488211553\n",
      "Epoch 82, Training Loss: 0.8453205619539533\n",
      "Epoch 83, Training Loss: 0.8433548747148729\n",
      "Epoch 84, Training Loss: 0.8427115719569357\n",
      "Epoch 85, Training Loss: 0.8445520063091938\n",
      "Epoch 86, Training Loss: 0.8415982887260895\n",
      "Epoch 87, Training Loss: 0.8433662451747664\n",
      "Epoch 88, Training Loss: 0.8405478927425872\n",
      "Epoch 89, Training Loss: 0.8449547609888521\n",
      "Epoch 90, Training Loss: 0.8435997919032449\n",
      "Epoch 91, Training Loss: 0.8423742124012539\n",
      "Epoch 92, Training Loss: 0.8448004027058308\n",
      "Epoch 93, Training Loss: 0.8434619490365337\n",
      "Epoch 94, Training Loss: 0.8432003200502324\n",
      "Epoch 95, Training Loss: 0.8417579416941879\n",
      "Epoch 96, Training Loss: 0.84357689762474\n",
      "Epoch 97, Training Loss: 0.8459374196547315\n",
      "Epoch 98, Training Loss: 0.8444574416131901\n",
      "Epoch 99, Training Loss: 0.8445567581886636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:39:10,889] Trial 137 finished with value: 0.4371333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 31 with value: 0.6411333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8432427287998056\n",
      "Epoch 1, Training Loss: 0.986862937001621\n",
      "Epoch 2, Training Loss: 0.9154107908641591\n",
      "Epoch 3, Training Loss: 0.8471483222877279\n",
      "Epoch 4, Training Loss: 0.8176561495837044\n",
      "Epoch 5, Training Loss: 0.8138893654065974\n",
      "Epoch 6, Training Loss: 0.8121338058920468\n",
      "Epoch 7, Training Loss: 0.8094958727500018\n",
      "Epoch 8, Training Loss: 0.8079810264531304\n",
      "Epoch 9, Training Loss: 0.8069289173098172\n",
      "Epoch 10, Training Loss: 0.8054415899164536\n",
      "Epoch 11, Training Loss: 0.804869908024283\n",
      "Epoch 12, Training Loss: 0.8040194125035229\n",
      "Epoch 13, Training Loss: 0.803579892761567\n",
      "Epoch 14, Training Loss: 0.8027325783757603\n",
      "Epoch 15, Training Loss: 0.8033956886039061\n",
      "Epoch 16, Training Loss: 0.8020253785217509\n",
      "Epoch 17, Training Loss: 0.8022053821647869\n",
      "Epoch 18, Training Loss: 0.8017766507232891\n",
      "Epoch 19, Training Loss: 0.801307541622835\n",
      "Epoch 20, Training Loss: 0.8009792634318856\n",
      "Epoch 21, Training Loss: 0.8005578025649576\n",
      "Epoch 22, Training Loss: 0.8001564330914441\n",
      "Epoch 23, Training Loss: 0.8003547073111814\n",
      "Epoch 24, Training Loss: 0.7991823209033293\n",
      "Epoch 25, Training Loss: 0.7991967995026532\n",
      "Epoch 26, Training Loss: 0.7988224636105931\n",
      "Epoch 27, Training Loss: 0.798394750286551\n",
      "Epoch 28, Training Loss: 0.7988776733594782\n",
      "Epoch 29, Training Loss: 0.7976207191803876\n",
      "Epoch 30, Training Loss: 0.7975696992874145\n",
      "Epoch 31, Training Loss: 0.7970306533224443\n",
      "Epoch 32, Training Loss: 0.7969270955113804\n",
      "Epoch 33, Training Loss: 0.796847604302799\n",
      "Epoch 34, Training Loss: 0.7959806905073278\n",
      "Epoch 35, Training Loss: 0.7955597282157225\n",
      "Epoch 36, Training Loss: 0.7951616971633013\n",
      "Epoch 37, Training Loss: 0.7942482345244464\n",
      "Epoch 38, Training Loss: 0.7933906407917247\n",
      "Epoch 39, Training Loss: 0.7934368346017949\n",
      "Epoch 40, Training Loss: 0.793030179458506\n",
      "Epoch 41, Training Loss: 0.7925435499583974\n",
      "Epoch 42, Training Loss: 0.7937502297233133\n",
      "Epoch 43, Training Loss: 0.7915983171322767\n",
      "Epoch 44, Training Loss: 0.7909290144022774\n",
      "Epoch 45, Training Loss: 0.7908291804790497\n",
      "Epoch 46, Training Loss: 0.7900768486892475\n",
      "Epoch 47, Training Loss: 0.7896101699857151\n",
      "Epoch 48, Training Loss: 0.7896513270630556\n",
      "Epoch 49, Training Loss: 0.7887127145598917\n",
      "Epoch 50, Training Loss: 0.7880744688651141\n",
      "Epoch 51, Training Loss: 0.7880944249910466\n",
      "Epoch 52, Training Loss: 0.788509909545674\n",
      "Epoch 53, Training Loss: 0.7873654620787677\n",
      "Epoch 54, Training Loss: 0.7878617708122029\n",
      "Epoch 55, Training Loss: 0.7870350790725035\n",
      "Epoch 56, Training Loss: 0.787264015674591\n",
      "Epoch 57, Training Loss: 0.786373775916941\n",
      "Epoch 58, Training Loss: 0.7865715243535883\n",
      "Epoch 59, Training Loss: 0.7864708631880143\n",
      "Epoch 60, Training Loss: 0.7862448537349701\n",
      "Epoch 61, Training Loss: 0.7860280367206125\n",
      "Epoch 62, Training Loss: 0.7862602875513189\n",
      "Epoch 63, Training Loss: 0.7855665212519028\n",
      "Epoch 64, Training Loss: 0.7860143212009879\n",
      "Epoch 65, Training Loss: 0.7854983889355379\n",
      "Epoch 66, Training Loss: 0.7849253953905666\n",
      "Epoch 67, Training Loss: 0.7846172593621646\n",
      "Epoch 68, Training Loss: 0.7848052021335153\n",
      "Epoch 69, Training Loss: 0.7846917860648212\n",
      "Epoch 70, Training Loss: 0.7846941560857437\n",
      "Epoch 71, Training Loss: 0.7843079132192275\n",
      "Epoch 72, Training Loss: 0.7845091436189764\n",
      "Epoch 73, Training Loss: 0.784438787207884\n",
      "Epoch 74, Training Loss: 0.7839938904958613\n",
      "Epoch 75, Training Loss: 0.7844506373124964\n",
      "Epoch 76, Training Loss: 0.7838605340088115\n",
      "Epoch 77, Training Loss: 0.7843175611075233\n",
      "Epoch 78, Training Loss: 0.7844087428205153\n",
      "Epoch 79, Training Loss: 0.7838950190123389\n",
      "Epoch 80, Training Loss: 0.7838008558750152\n",
      "Epoch 81, Training Loss: 0.7836168475712046\n",
      "Epoch 82, Training Loss: 0.7840096803973703\n",
      "Epoch 83, Training Loss: 0.7834627307162565\n",
      "Epoch 84, Training Loss: 0.7835996822048636\n",
      "Epoch 85, Training Loss: 0.7832454387580647\n",
      "Epoch 86, Training Loss: 0.7835386729941649\n",
      "Epoch 87, Training Loss: 0.7835807796786813\n",
      "Epoch 88, Training Loss: 0.7831144656854517\n",
      "Epoch 89, Training Loss: 0.7835162516201244\n",
      "Epoch 90, Training Loss: 0.7833395315619076\n",
      "Epoch 91, Training Loss: 0.7828726214521071\n",
      "Epoch 92, Training Loss: 0.7831264061086318\n",
      "Epoch 93, Training Loss: 0.7830236717532663\n",
      "Epoch 94, Training Loss: 0.7828401261918685\n",
      "Epoch 95, Training Loss: 0.7825299929871279\n",
      "Epoch 96, Training Loss: 0.7828823626742644\n",
      "Epoch 97, Training Loss: 0.783148741090999\n",
      "Epoch 98, Training Loss: 0.7827124415425694\n",
      "Epoch 99, Training Loss: 0.7826547748902265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:41:44,042] Trial 138 finished with value: 0.6412 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7822145945885602\n",
      "Epoch 1, Training Loss: 0.8676833955680623\n",
      "Epoch 2, Training Loss: 0.8363751499793108\n",
      "Epoch 3, Training Loss: 0.8447263299717622\n",
      "Epoch 4, Training Loss: 0.8460313138541053\n",
      "Epoch 5, Training Loss: 0.8417058890006122\n",
      "Epoch 6, Training Loss: 0.8442810377653908\n",
      "Epoch 7, Training Loss: 0.847964657054228\n",
      "Epoch 8, Training Loss: 0.8525887242485495\n",
      "Epoch 9, Training Loss: 0.8521385914437911\n",
      "Epoch 10, Training Loss: 0.8530381373096915\n",
      "Epoch 11, Training Loss: 0.855676226756152\n",
      "Epoch 12, Training Loss: 0.8522981133180506\n",
      "Epoch 13, Training Loss: 0.8495772433280945\n",
      "Epoch 14, Training Loss: 0.8466916464356815\n",
      "Epoch 15, Training Loss: 0.8480899370417876\n",
      "Epoch 16, Training Loss: 0.852384755821789\n",
      "Epoch 17, Training Loss: 0.8543052843739005\n",
      "Epoch 18, Training Loss: 0.850796270721099\n",
      "Epoch 19, Training Loss: 0.8515459672843708\n",
      "Epoch 20, Training Loss: 0.8575190917183371\n",
      "Epoch 21, Training Loss: 0.8517010823418112\n",
      "Epoch 22, Training Loss: 0.8450572531363544\n",
      "Epoch 23, Training Loss: 0.8561579417481142\n",
      "Epoch 24, Training Loss: 0.8552464876455419\n",
      "Epoch 25, Training Loss: 0.8547828484282775\n",
      "Epoch 26, Training Loss: 0.858961899210425\n",
      "Epoch 27, Training Loss: 0.8560391133673051\n",
      "Epoch 28, Training Loss: 0.8547315610857571\n",
      "Epoch 29, Training Loss: 0.8558479088895461\n",
      "Epoch 30, Training Loss: 0.8548841346011442\n",
      "Epoch 31, Training Loss: 0.8683131957054138\n",
      "Epoch 32, Training Loss: 0.8532426892308628\n",
      "Epoch 33, Training Loss: 0.8576093302754795\n",
      "Epoch 34, Training Loss: 0.8560441554293913\n",
      "Epoch 35, Training Loss: 0.8547078861208524\n",
      "Epoch 36, Training Loss: 0.8548627040666692\n",
      "Epoch 37, Training Loss: 0.8556541355918436\n",
      "Epoch 38, Training Loss: 0.8524236142635345\n",
      "Epoch 39, Training Loss: 0.8539868624771343\n",
      "Epoch 40, Training Loss: 0.8523142913509818\n",
      "Epoch 41, Training Loss: 0.8552961976387922\n",
      "Epoch 42, Training Loss: 0.8587983435742995\n",
      "Epoch 43, Training Loss: 0.853609504629584\n",
      "Epoch 44, Training Loss: 0.8468370608722462\n",
      "Epoch 45, Training Loss: 0.8476761123012094\n",
      "Epoch 46, Training Loss: 0.8509048355326932\n",
      "Epoch 47, Training Loss: 0.847856757290223\n",
      "Epoch 48, Training Loss: 0.8563021655643688\n",
      "Epoch 49, Training Loss: 0.8481215206314535\n",
      "Epoch 50, Training Loss: 0.8505719716408674\n",
      "Epoch 51, Training Loss: 0.8524708053644966\n",
      "Epoch 52, Training Loss: 0.8550678070853738\n",
      "Epoch 53, Training Loss: 0.85862634174964\n",
      "Epoch 54, Training Loss: 0.8587533793729895\n",
      "Epoch 55, Training Loss: 0.8564633384872885\n",
      "Epoch 56, Training Loss: 0.860579806916854\n",
      "Epoch 57, Training Loss: 0.8589094682300792\n",
      "Epoch 58, Training Loss: 0.8581866999934701\n",
      "Epoch 59, Training Loss: 0.8559079688436845\n",
      "Epoch 60, Training Loss: 0.854750175335828\n",
      "Epoch 61, Training Loss: 0.8523191765476675\n",
      "Epoch 62, Training Loss: 0.8534204374341404\n",
      "Epoch 63, Training Loss: 0.8584102739306058\n",
      "Epoch 64, Training Loss: 0.8567949043301974\n",
      "Epoch 65, Training Loss: 0.8542757294458502\n",
      "Epoch 66, Training Loss: 0.8630695516922895\n",
      "Epoch 67, Training Loss: 0.8591541210342856\n",
      "Epoch 68, Training Loss: 0.8561595633450676\n",
      "Epoch 69, Training Loss: 0.8624133882101844\n",
      "Epoch 70, Training Loss: 0.855062675966936\n",
      "Epoch 71, Training Loss: 0.8541784567692701\n",
      "Epoch 72, Training Loss: 0.8594696874478284\n",
      "Epoch 73, Training Loss: 0.8608932159928715\n",
      "Epoch 74, Training Loss: 0.8550337298477397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:43:38,901] Trial 139 finished with value: 0.6103333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8644144826776841\n",
      "Epoch 1, Training Loss: 0.8913880313845242\n",
      "Epoch 2, Training Loss: 0.815701292402604\n",
      "Epoch 3, Training Loss: 0.810400420216953\n",
      "Epoch 4, Training Loss: 0.8051055378072403\n",
      "Epoch 5, Training Loss: 0.7994332782661213\n",
      "Epoch 6, Training Loss: 0.7962823351691751\n",
      "Epoch 7, Training Loss: 0.7972205603823942\n",
      "Epoch 8, Training Loss: 0.794414930343628\n",
      "Epoch 9, Training Loss: 0.793688873683705\n",
      "Epoch 10, Training Loss: 0.7935608826665317\n",
      "Epoch 11, Training Loss: 0.7934615323824041\n",
      "Epoch 12, Training Loss: 0.79108226684963\n",
      "Epoch 13, Training Loss: 0.790959552526474\n",
      "Epoch 14, Training Loss: 0.79049872230081\n",
      "Epoch 15, Training Loss: 0.7905336797938627\n",
      "Epoch 16, Training Loss: 0.7901604613837074\n",
      "Epoch 17, Training Loss: 0.7893271747056175\n",
      "Epoch 18, Training Loss: 0.788073899605695\n",
      "Epoch 19, Training Loss: 0.7899346202962538\n",
      "Epoch 20, Training Loss: 0.7893456908534555\n",
      "Epoch 21, Training Loss: 0.7889949601538041\n",
      "Epoch 22, Training Loss: 0.786777883347343\n",
      "Epoch 23, Training Loss: 0.7884335400777704\n",
      "Epoch 24, Training Loss: 0.7864380570018993\n",
      "Epoch 25, Training Loss: 0.7866736593667198\n",
      "Epoch 26, Training Loss: 0.787157367257511\n",
      "Epoch 27, Training Loss: 0.7858567906828487\n",
      "Epoch 28, Training Loss: 0.7856384223348954\n",
      "Epoch 29, Training Loss: 0.7860394113204059\n",
      "Epoch 30, Training Loss: 0.7855472776469062\n",
      "Epoch 31, Training Loss: 0.7850522338642794\n",
      "Epoch 32, Training Loss: 0.7847461128936094\n",
      "Epoch 33, Training Loss: 0.7854855139115278\n",
      "Epoch 34, Training Loss: 0.7842481794076808\n",
      "Epoch 35, Training Loss: 0.7832354124854592\n",
      "Epoch 36, Training Loss: 0.7842707361894495\n",
      "Epoch 37, Training Loss: 0.7846944315293256\n",
      "Epoch 38, Training Loss: 0.7837232246819664\n",
      "Epoch 39, Training Loss: 0.7850079811320585\n",
      "Epoch 40, Training Loss: 0.783611745483735\n",
      "Epoch 41, Training Loss: 0.7832425186914557\n",
      "Epoch 42, Training Loss: 0.7830285976914798\n",
      "Epoch 43, Training Loss: 0.7833130844200359\n",
      "Epoch 44, Training Loss: 0.7826809524788576\n",
      "Epoch 45, Training Loss: 0.7825035905136781\n",
      "Epoch 46, Training Loss: 0.7822903136646047\n",
      "Epoch 47, Training Loss: 0.7823914271242478\n",
      "Epoch 48, Training Loss: 0.7819815562051885\n",
      "Epoch 49, Training Loss: 0.7813949582857244\n",
      "Epoch 50, Training Loss: 0.7823108321778914\n",
      "Epoch 51, Training Loss: 0.7814276259085712\n",
      "Epoch 52, Training Loss: 0.7821762957292444\n",
      "Epoch 53, Training Loss: 0.7813975324350245\n",
      "Epoch 54, Training Loss: 0.7811521630427417\n",
      "Epoch 55, Training Loss: 0.7809924650893492\n",
      "Epoch 56, Training Loss: 0.780074210727916\n",
      "Epoch 57, Training Loss: 0.7803069297005149\n",
      "Epoch 58, Training Loss: 0.780398437205483\n",
      "Epoch 59, Training Loss: 0.7800217687382418\n",
      "Epoch 60, Training Loss: 0.7801423994933858\n",
      "Epoch 61, Training Loss: 0.7800720467286951\n",
      "Epoch 62, Training Loss: 0.7793946644839118\n",
      "Epoch 63, Training Loss: 0.7795108525893267\n",
      "Epoch 64, Training Loss: 0.7792337130097782\n",
      "Epoch 65, Training Loss: 0.7785057563641492\n",
      "Epoch 66, Training Loss: 0.7792818613613353\n",
      "Epoch 67, Training Loss: 0.7785483086109162\n",
      "Epoch 68, Training Loss: 0.7792851513974807\n",
      "Epoch 69, Training Loss: 0.7785094763250912\n",
      "Epoch 70, Training Loss: 0.7778572555850534\n",
      "Epoch 71, Training Loss: 0.7776895862467149\n",
      "Epoch 72, Training Loss: 0.7778985564147725\n",
      "Epoch 73, Training Loss: 0.7781922731680029\n",
      "Epoch 74, Training Loss: 0.777398408020244\n",
      "Epoch 75, Training Loss: 0.7774270986108219\n",
      "Epoch 76, Training Loss: 0.7774838813613443\n",
      "Epoch 77, Training Loss: 0.777069480489282\n",
      "Epoch 78, Training Loss: 0.7764961190784678\n",
      "Epoch 79, Training Loss: 0.7765858588499182\n",
      "Epoch 80, Training Loss: 0.776687053231632\n",
      "Epoch 81, Training Loss: 0.7766288765037761\n",
      "Epoch 82, Training Loss: 0.7760146700634676\n",
      "Epoch 83, Training Loss: 0.7761186089936425\n",
      "Epoch 84, Training Loss: 0.77507601127905\n",
      "Epoch 85, Training Loss: 0.7756386894338271\n",
      "Epoch 86, Training Loss: 0.7751999888700597\n",
      "Epoch 87, Training Loss: 0.774673810706419\n",
      "Epoch 88, Training Loss: 0.7752929292005651\n",
      "Epoch 89, Training Loss: 0.7748680761281181\n",
      "Epoch 90, Training Loss: 0.7751766959358665\n",
      "Epoch 91, Training Loss: 0.7744562910584842\n",
      "Epoch 92, Training Loss: 0.7745353905593648\n",
      "Epoch 93, Training Loss: 0.774306000400992\n",
      "Epoch 94, Training Loss: 0.7740140739609214\n",
      "Epoch 95, Training Loss: 0.7739304046771106\n",
      "Epoch 96, Training Loss: 0.7735216062910416\n",
      "Epoch 97, Training Loss: 0.7735394701536964\n",
      "Epoch 98, Training Loss: 0.7728982250129476\n",
      "Epoch 99, Training Loss: 0.7731286113402422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:46:12,962] Trial 140 finished with value: 0.6372 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7732350589247311\n",
      "Epoch 1, Training Loss: 1.0080650427287683\n",
      "Epoch 2, Training Loss: 0.9592148216147172\n",
      "Epoch 3, Training Loss: 0.9495392039306182\n",
      "Epoch 4, Training Loss: 0.9423369983085116\n",
      "Epoch 5, Training Loss: 0.9360623042386277\n",
      "Epoch 6, Training Loss: 0.9296955882158494\n",
      "Epoch 7, Training Loss: 0.9233923709482179\n",
      "Epoch 8, Training Loss: 0.9169379257618037\n",
      "Epoch 9, Training Loss: 0.9102012362695278\n",
      "Epoch 10, Training Loss: 0.902975669032649\n",
      "Epoch 11, Training Loss: 0.895302048751286\n",
      "Epoch 12, Training Loss: 0.8872415726346181\n",
      "Epoch 13, Training Loss: 0.8800851967101707\n",
      "Epoch 14, Training Loss: 0.872882400329848\n",
      "Epoch 15, Training Loss: 0.865704263601088\n",
      "Epoch 16, Training Loss: 0.8586114594810887\n",
      "Epoch 17, Training Loss: 0.8524790274469476\n",
      "Epoch 18, Training Loss: 0.8466930590177837\n",
      "Epoch 19, Training Loss: 0.841402726782892\n",
      "Epoch 20, Training Loss: 0.8366998142765877\n",
      "Epoch 21, Training Loss: 0.8327940903211895\n",
      "Epoch 22, Training Loss: 0.8292477012576913\n",
      "Epoch 23, Training Loss: 0.8259374996773282\n",
      "Epoch 24, Training Loss: 0.8233322343431917\n",
      "Epoch 25, Training Loss: 0.8210156342140714\n",
      "Epoch 26, Training Loss: 0.8200749050405689\n",
      "Epoch 27, Training Loss: 0.8174130347438324\n",
      "Epoch 28, Training Loss: 0.816082261200238\n",
      "Epoch 29, Training Loss: 0.8152828295428054\n",
      "Epoch 30, Training Loss: 0.8145762503595281\n",
      "Epoch 31, Training Loss: 0.8130395150722418\n",
      "Epoch 32, Training Loss: 0.8122203455831771\n",
      "Epoch 33, Training Loss: 0.8118851739661138\n",
      "Epoch 34, Training Loss: 0.8117115150716968\n",
      "Epoch 35, Training Loss: 0.8104801169015411\n",
      "Epoch 36, Training Loss: 0.8105619189434482\n",
      "Epoch 37, Training Loss: 0.8099731228405371\n",
      "Epoch 38, Training Loss: 0.809200586322555\n",
      "Epoch 39, Training Loss: 0.8090704173970044\n",
      "Epoch 40, Training Loss: 0.8088703668207154\n",
      "Epoch 41, Training Loss: 0.8083998545668178\n",
      "Epoch 42, Training Loss: 0.8083149589094004\n",
      "Epoch 43, Training Loss: 0.8080257034839544\n",
      "Epoch 44, Training Loss: 0.8077154341496919\n",
      "Epoch 45, Training Loss: 0.8074213678675487\n",
      "Epoch 46, Training Loss: 0.8071841645957832\n",
      "Epoch 47, Training Loss: 0.8070661219439113\n",
      "Epoch 48, Training Loss: 0.806934308467951\n",
      "Epoch 49, Training Loss: 0.8066687274696236\n",
      "Epoch 50, Training Loss: 0.8062576052837802\n",
      "Epoch 51, Training Loss: 0.8060282926810415\n",
      "Epoch 52, Training Loss: 0.8057337761821604\n",
      "Epoch 53, Training Loss: 0.8058374831551\n",
      "Epoch 54, Training Loss: 0.8061869497586014\n",
      "Epoch 55, Training Loss: 0.8055448184336038\n",
      "Epoch 56, Training Loss: 0.8050332949573832\n",
      "Epoch 57, Training Loss: 0.8053536924204432\n",
      "Epoch 58, Training Loss: 0.8045078057543675\n",
      "Epoch 59, Training Loss: 0.8052848346251295\n",
      "Epoch 60, Training Loss: 0.8043279966017357\n",
      "Epoch 61, Training Loss: 0.8046739561217172\n",
      "Epoch 62, Training Loss: 0.8045537455637652\n",
      "Epoch 63, Training Loss: 0.8039677282921354\n",
      "Epoch 64, Training Loss: 0.8038406943916379\n",
      "Epoch 65, Training Loss: 0.8038037423800705\n",
      "Epoch 66, Training Loss: 0.8043356904409882\n",
      "Epoch 67, Training Loss: 0.8036461610543101\n",
      "Epoch 68, Training Loss: 0.8035815576861676\n",
      "Epoch 69, Training Loss: 0.8031829954089975\n",
      "Epoch 70, Training Loss: 0.8034684798771278\n",
      "Epoch 71, Training Loss: 0.8036197436483283\n",
      "Epoch 72, Training Loss: 0.803158922661516\n",
      "Epoch 73, Training Loss: 0.8029979468288279\n",
      "Epoch 74, Training Loss: 0.802590981372317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:47:15,794] Trial 141 finished with value: 0.6343333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8028891285559289\n",
      "Epoch 1, Training Loss: 0.9776467673760608\n",
      "Epoch 2, Training Loss: 0.9457698443778475\n",
      "Epoch 3, Training Loss: 0.9326588939903374\n",
      "Epoch 4, Training Loss: 0.9160540566408545\n",
      "Epoch 5, Training Loss: 0.8969621044352538\n",
      "Epoch 6, Training Loss: 0.8774389864806842\n",
      "Epoch 7, Training Loss: 0.8597612572791881\n",
      "Epoch 8, Training Loss: 0.845910720538376\n",
      "Epoch 9, Training Loss: 0.8351668567585766\n",
      "Epoch 10, Training Loss: 0.8276371481723355\n",
      "Epoch 11, Training Loss: 0.8220932302618386\n",
      "Epoch 12, Training Loss: 0.8176961642010767\n",
      "Epoch 13, Training Loss: 0.8154307640584788\n",
      "Epoch 14, Training Loss: 0.8131338013742203\n",
      "Epoch 15, Training Loss: 0.8121831512092648\n",
      "Epoch 16, Training Loss: 0.8118303505997909\n",
      "Epoch 17, Training Loss: 0.8106844450298109\n",
      "Epoch 18, Training Loss: 0.8104696998022553\n",
      "Epoch 19, Training Loss: 0.8097752478785981\n",
      "Epoch 20, Training Loss: 0.8090840476796143\n",
      "Epoch 21, Training Loss: 0.8089657936777387\n",
      "Epoch 22, Training Loss: 0.8085270904060593\n",
      "Epoch 23, Training Loss: 0.8081348662089585\n",
      "Epoch 24, Training Loss: 0.8080246286284655\n",
      "Epoch 25, Training Loss: 0.8079420044906157\n",
      "Epoch 26, Training Loss: 0.808104305607932\n",
      "Epoch 27, Training Loss: 0.807381518532459\n",
      "Epoch 28, Training Loss: 0.8075050155919298\n",
      "Epoch 29, Training Loss: 0.8067536064556666\n",
      "Epoch 30, Training Loss: 0.8069494664220882\n",
      "Epoch 31, Training Loss: 0.8065583157360106\n",
      "Epoch 32, Training Loss: 0.8060007212305428\n",
      "Epoch 33, Training Loss: 0.8065997555739898\n",
      "Epoch 34, Training Loss: 0.8059881517313476\n",
      "Epoch 35, Training Loss: 0.8062295120461542\n",
      "Epoch 36, Training Loss: 0.8061769231817776\n",
      "Epoch 37, Training Loss: 0.8056349569693544\n",
      "Epoch 38, Training Loss: 0.805845106185827\n",
      "Epoch 39, Training Loss: 0.8050217675087147\n",
      "Epoch 40, Training Loss: 0.8051998226266158\n",
      "Epoch 41, Training Loss: 0.8051460033072565\n",
      "Epoch 42, Training Loss: 0.8052298514466537\n",
      "Epoch 43, Training Loss: 0.8046055783006482\n",
      "Epoch 44, Training Loss: 0.8050207237551983\n",
      "Epoch 45, Training Loss: 0.8045872751035188\n",
      "Epoch 46, Training Loss: 0.8046720156095978\n",
      "Epoch 47, Training Loss: 0.8043031470219891\n",
      "Epoch 48, Training Loss: 0.8046458454956685\n",
      "Epoch 49, Training Loss: 0.8040917424331034\n",
      "Epoch 50, Training Loss: 0.8040204523201275\n",
      "Epoch 51, Training Loss: 0.8038854267364158\n",
      "Epoch 52, Training Loss: 0.8040518388712317\n",
      "Epoch 53, Training Loss: 0.8034570425972902\n",
      "Epoch 54, Training Loss: 0.8032024518887799\n",
      "Epoch 55, Training Loss: 0.8034982923278235\n",
      "Epoch 56, Training Loss: 0.8030537426919866\n",
      "Epoch 57, Training Loss: 0.8037143075376525\n",
      "Epoch 58, Training Loss: 0.802952107899171\n",
      "Epoch 59, Training Loss: 0.8034007249021888\n",
      "Epoch 60, Training Loss: 0.8032737781230669\n",
      "Epoch 61, Training Loss: 0.8032582538468497\n",
      "Epoch 62, Training Loss: 0.8027497209104381\n",
      "Epoch 63, Training Loss: 0.802684695917861\n",
      "Epoch 64, Training Loss: 0.8023162629371299\n",
      "Epoch 65, Training Loss: 0.8025287720493804\n",
      "Epoch 66, Training Loss: 0.8026715635357047\n",
      "Epoch 67, Training Loss: 0.8030671670024556\n",
      "Epoch 68, Training Loss: 0.8023407704848096\n",
      "Epoch 69, Training Loss: 0.8018254886892505\n",
      "Epoch 70, Training Loss: 0.8022878928292067\n",
      "Epoch 71, Training Loss: 0.802060926319065\n",
      "Epoch 72, Training Loss: 0.8017547401270472\n",
      "Epoch 73, Training Loss: 0.8015814558903974\n",
      "Epoch 74, Training Loss: 0.8018619834928584\n",
      "Epoch 75, Training Loss: 0.8015124511001701\n",
      "Epoch 76, Training Loss: 0.8022849886937249\n",
      "Epoch 77, Training Loss: 0.8010393832859241\n",
      "Epoch 78, Training Loss: 0.8013142008530466\n",
      "Epoch 79, Training Loss: 0.8008852491701456\n",
      "Epoch 80, Training Loss: 0.8011057934366671\n",
      "Epoch 81, Training Loss: 0.8005990199576644\n",
      "Epoch 82, Training Loss: 0.801370188168117\n",
      "Epoch 83, Training Loss: 0.8010116136163697\n",
      "Epoch 84, Training Loss: 0.8003218375650564\n",
      "Epoch 85, Training Loss: 0.8005804058304407\n",
      "Epoch 86, Training Loss: 0.8004504062179336\n",
      "Epoch 87, Training Loss: 0.8000580944513019\n",
      "Epoch 88, Training Loss: 0.8003114361512034\n",
      "Epoch 89, Training Loss: 0.8002500657748459\n",
      "Epoch 90, Training Loss: 0.8001751475764397\n",
      "Epoch 91, Training Loss: 0.8005598366708684\n",
      "Epoch 92, Training Loss: 0.8000518072816662\n",
      "Epoch 93, Training Loss: 0.7998213616528905\n",
      "Epoch 94, Training Loss: 0.7997426482071553\n",
      "Epoch 95, Training Loss: 0.7997741609587705\n",
      "Epoch 96, Training Loss: 0.799819781188678\n",
      "Epoch 97, Training Loss: 0.799919867694826\n",
      "Epoch 98, Training Loss: 0.7991431581346612\n",
      "Epoch 99, Training Loss: 0.8000834539420623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:48:49,951] Trial 142 finished with value: 0.6344666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7995319953538421\n",
      "Epoch 1, Training Loss: 0.8718124346625536\n",
      "Epoch 2, Training Loss: 0.8198072238524158\n",
      "Epoch 3, Training Loss: 0.8157077121555357\n",
      "Epoch 4, Training Loss: 0.8114284224976275\n",
      "Epoch 5, Training Loss: 0.8115697400014203\n",
      "Epoch 6, Training Loss: 0.8089618920383597\n",
      "Epoch 7, Training Loss: 0.8086785726081159\n",
      "Epoch 8, Training Loss: 0.8074045495879382\n",
      "Epoch 9, Training Loss: 0.8078082472758186\n",
      "Epoch 10, Training Loss: 0.8063026358310441\n",
      "Epoch 11, Training Loss: 0.8070787595627004\n",
      "Epoch 12, Training Loss: 0.8058556622132322\n",
      "Epoch 13, Training Loss: 0.8064207579856528\n",
      "Epoch 14, Training Loss: 0.8058335481729723\n",
      "Epoch 15, Training Loss: 0.8056423563706248\n",
      "Epoch 16, Training Loss: 0.8043351370589178\n",
      "Epoch 17, Training Loss: 0.8050729835840096\n",
      "Epoch 18, Training Loss: 0.8037653580644077\n",
      "Epoch 19, Training Loss: 0.8043873159508956\n",
      "Epoch 20, Training Loss: 0.8032506171922038\n",
      "Epoch 21, Training Loss: 0.8033939133013102\n",
      "Epoch 22, Training Loss: 0.8036883247526069\n",
      "Epoch 23, Training Loss: 0.8017544931935189\n",
      "Epoch 24, Training Loss: 0.8021352186238855\n",
      "Epoch 25, Training Loss: 0.8012359381618356\n",
      "Epoch 26, Training Loss: 0.8012879346546374\n",
      "Epoch 27, Training Loss: 0.8010214936464353\n",
      "Epoch 28, Training Loss: 0.8008663400671536\n",
      "Epoch 29, Training Loss: 0.7995267130826649\n",
      "Epoch 30, Training Loss: 0.7996338836232523\n",
      "Epoch 31, Training Loss: 0.7999929584954915\n",
      "Epoch 32, Training Loss: 0.800603407307675\n",
      "Epoch 33, Training Loss: 0.7999099949248751\n",
      "Epoch 34, Training Loss: 0.7996973888318342\n",
      "Epoch 35, Training Loss: 0.7986585582109322\n",
      "Epoch 36, Training Loss: 0.7975509792342221\n",
      "Epoch 37, Training Loss: 0.7976393964057579\n",
      "Epoch 38, Training Loss: 0.797829838892571\n",
      "Epoch 39, Training Loss: 0.7965683655631274\n",
      "Epoch 40, Training Loss: 0.7964305905471171\n",
      "Epoch 41, Training Loss: 0.7976125868639552\n",
      "Epoch 42, Training Loss: 0.7967202959204078\n",
      "Epoch 43, Training Loss: 0.7960795914320121\n",
      "Epoch 44, Training Loss: 0.7951870189573532\n",
      "Epoch 45, Training Loss: 0.7955433529122431\n",
      "Epoch 46, Training Loss: 0.7962198776410039\n",
      "Epoch 47, Training Loss: 0.7954606645089343\n",
      "Epoch 48, Training Loss: 0.7942179873473663\n",
      "Epoch 49, Training Loss: 0.7952409188550218\n",
      "Epoch 50, Training Loss: 0.7942327738704538\n",
      "Epoch 51, Training Loss: 0.7955248109380105\n",
      "Epoch 52, Training Loss: 0.7952438745283543\n",
      "Epoch 53, Training Loss: 0.7953519939479972\n",
      "Epoch 54, Training Loss: 0.7952524734618969\n",
      "Epoch 55, Training Loss: 0.7946072089044671\n",
      "Epoch 56, Training Loss: 0.7933501723565554\n",
      "Epoch 57, Training Loss: 0.7948961522346153\n",
      "Epoch 58, Training Loss: 0.793703175577006\n",
      "Epoch 59, Training Loss: 0.7935957559069297\n",
      "Epoch 60, Training Loss: 0.7938912999361081\n",
      "Epoch 61, Training Loss: 0.7926897889689395\n",
      "Epoch 62, Training Loss: 0.793662342720462\n",
      "Epoch 63, Training Loss: 0.7933147755780614\n",
      "Epoch 64, Training Loss: 0.7928254977204746\n",
      "Epoch 65, Training Loss: 0.7931508652249674\n",
      "Epoch 66, Training Loss: 0.793181063806204\n",
      "Epoch 67, Training Loss: 0.7931548929752263\n",
      "Epoch 68, Training Loss: 0.7918224194892367\n",
      "Epoch 69, Training Loss: 0.7925787871045278\n",
      "Epoch 70, Training Loss: 0.7931735922519426\n",
      "Epoch 71, Training Loss: 0.792442448515641\n",
      "Epoch 72, Training Loss: 0.7926722714775487\n",
      "Epoch 73, Training Loss: 0.7924630827473519\n",
      "Epoch 74, Training Loss: 0.793107431454766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:50:04,827] Trial 143 finished with value: 0.6384 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7921857626814591\n",
      "Epoch 1, Training Loss: 1.0376772589253305\n",
      "Epoch 2, Training Loss: 0.9610781155134502\n",
      "Epoch 3, Training Loss: 0.9379663985474666\n",
      "Epoch 4, Training Loss: 0.9275260658192456\n",
      "Epoch 5, Training Loss: 0.9205950396401542\n",
      "Epoch 6, Training Loss: 0.9142282707350594\n",
      "Epoch 7, Training Loss: 0.907550625245374\n",
      "Epoch 8, Training Loss: 0.8999799038234509\n",
      "Epoch 9, Training Loss: 0.8920258929854945\n",
      "Epoch 10, Training Loss: 0.8820063019157353\n",
      "Epoch 11, Training Loss: 0.8709228049543567\n",
      "Epoch 12, Training Loss: 0.8574737091709796\n",
      "Epoch 13, Training Loss: 0.8444585516040487\n",
      "Epoch 14, Training Loss: 0.8318275694560288\n",
      "Epoch 15, Training Loss: 0.8221581671470987\n",
      "Epoch 16, Training Loss: 0.815820640639255\n",
      "Epoch 17, Training Loss: 0.8116234795491498\n",
      "Epoch 18, Training Loss: 0.8093298980167933\n",
      "Epoch 19, Training Loss: 0.8069073034408397\n",
      "Epoch 20, Training Loss: 0.8055042290149774\n",
      "Epoch 21, Training Loss: 0.804447497102551\n",
      "Epoch 22, Training Loss: 0.8032297663222578\n",
      "Epoch 23, Training Loss: 0.8033586770072019\n",
      "Epoch 24, Training Loss: 0.8020001815674\n",
      "Epoch 25, Training Loss: 0.8009173787626108\n",
      "Epoch 26, Training Loss: 0.8011265302959242\n",
      "Epoch 27, Training Loss: 0.8001172453837287\n",
      "Epoch 28, Training Loss: 0.7992510543730026\n",
      "Epoch 29, Training Loss: 0.7992154616162293\n",
      "Epoch 30, Training Loss: 0.7989731754575456\n",
      "Epoch 31, Training Loss: 0.7989523281728415\n",
      "Epoch 32, Training Loss: 0.7984695231107841\n",
      "Epoch 33, Training Loss: 0.7975974370662431\n",
      "Epoch 34, Training Loss: 0.7978146819243753\n",
      "Epoch 35, Training Loss: 0.7974645492725803\n",
      "Epoch 36, Training Loss: 0.7978155243665652\n",
      "Epoch 37, Training Loss: 0.7971812791394112\n",
      "Epoch 38, Training Loss: 0.7967469383003121\n",
      "Epoch 39, Training Loss: 0.7963744169787357\n",
      "Epoch 40, Training Loss: 0.7958483097248508\n",
      "Epoch 41, Training Loss: 0.7960333112487219\n",
      "Epoch 42, Training Loss: 0.7960632713217485\n",
      "Epoch 43, Training Loss: 0.7956817980995752\n",
      "Epoch 44, Training Loss: 0.7953544565609523\n",
      "Epoch 45, Training Loss: 0.7957109280994961\n",
      "Epoch 46, Training Loss: 0.7960457174401534\n",
      "Epoch 47, Training Loss: 0.7951747264180865\n",
      "Epoch 48, Training Loss: 0.7948644865724377\n",
      "Epoch 49, Training Loss: 0.7946699736709881\n",
      "Epoch 50, Training Loss: 0.7945638143926634\n",
      "Epoch 51, Training Loss: 0.79517667329401\n",
      "Epoch 52, Training Loss: 0.7945625519393978\n",
      "Epoch 53, Training Loss: 0.7941211959473172\n",
      "Epoch 54, Training Loss: 0.7942886790834871\n",
      "Epoch 55, Training Loss: 0.7932831048069143\n",
      "Epoch 56, Training Loss: 0.7934645923456751\n",
      "Epoch 57, Training Loss: 0.7935719573408141\n",
      "Epoch 58, Training Loss: 0.7940144033360302\n",
      "Epoch 59, Training Loss: 0.7935379286457721\n",
      "Epoch 60, Training Loss: 0.7931244106221019\n",
      "Epoch 61, Training Loss: 0.7929011614699113\n",
      "Epoch 62, Training Loss: 0.7925606055367261\n",
      "Epoch 63, Training Loss: 0.7929605756487165\n",
      "Epoch 64, Training Loss: 0.7927853811952404\n",
      "Epoch 65, Training Loss: 0.7927270199123182\n",
      "Epoch 66, Training Loss: 0.7919475333134931\n",
      "Epoch 67, Training Loss: 0.7921206036904701\n",
      "Epoch 68, Training Loss: 0.7917729678010582\n",
      "Epoch 69, Training Loss: 0.7914349342647352\n",
      "Epoch 70, Training Loss: 0.7924090955490456\n",
      "Epoch 71, Training Loss: 0.7918886216959559\n",
      "Epoch 72, Training Loss: 0.7915555587388519\n",
      "Epoch 73, Training Loss: 0.7916961469148335\n",
      "Epoch 74, Training Loss: 0.7908786175842571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:51:14,843] Trial 144 finished with value: 0.6359333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7914227594110302\n",
      "Epoch 1, Training Loss: 1.0947070413962343\n",
      "Epoch 2, Training Loss: 1.0758471523012434\n",
      "Epoch 3, Training Loss: 1.0604944286489846\n",
      "Epoch 4, Training Loss: 1.0462630216340374\n",
      "Epoch 5, Training Loss: 1.0325452102754349\n",
      "Epoch 6, Training Loss: 1.0193152062874988\n",
      "Epoch 7, Training Loss: 1.0074547077480116\n",
      "Epoch 8, Training Loss: 0.9969095373512211\n",
      "Epoch 9, Training Loss: 0.9883408757977019\n",
      "Epoch 10, Training Loss: 0.9810617825142423\n",
      "Epoch 11, Training Loss: 0.9754381867279683\n",
      "Epoch 12, Training Loss: 0.9711166830887472\n",
      "Epoch 13, Training Loss: 0.9676390133405987\n",
      "Epoch 14, Training Loss: 0.9654166032497148\n",
      "Epoch 15, Training Loss: 0.9630959855882745\n",
      "Epoch 16, Training Loss: 0.961657842926513\n",
      "Epoch 17, Training Loss: 0.960473742700161\n",
      "Epoch 18, Training Loss: 0.959200568396346\n",
      "Epoch 19, Training Loss: 0.9579887926130366\n",
      "Epoch 20, Training Loss: 0.9569637787073179\n",
      "Epoch 21, Training Loss: 0.956119954586029\n",
      "Epoch 22, Training Loss: 0.9553212888258741\n",
      "Epoch 23, Training Loss: 0.9539713375550464\n",
      "Epoch 24, Training Loss: 0.953428472343244\n",
      "Epoch 25, Training Loss: 0.9527057799181544\n",
      "Epoch 26, Training Loss: 0.9518488862460717\n",
      "Epoch 27, Training Loss: 0.9508429053134488\n",
      "Epoch 28, Training Loss: 0.9501659325190953\n",
      "Epoch 29, Training Loss: 0.9491211901930042\n",
      "Epoch 30, Training Loss: 0.9485157565962999\n",
      "Epoch 31, Training Loss: 0.9474045749893762\n",
      "Epoch 32, Training Loss: 0.9466476374999024\n",
      "Epoch 33, Training Loss: 0.9456530412336938\n",
      "Epoch 34, Training Loss: 0.9446860731096196\n",
      "Epoch 35, Training Loss: 0.9436238932430296\n",
      "Epoch 36, Training Loss: 0.9430991003387853\n",
      "Epoch 37, Training Loss: 0.9418076591384142\n",
      "Epoch 38, Training Loss: 0.9402785535145523\n",
      "Epoch 39, Training Loss: 0.9395341770093244\n",
      "Epoch 40, Training Loss: 0.9386656948498318\n",
      "Epoch 41, Training Loss: 0.9375667674200875\n",
      "Epoch 42, Training Loss: 0.9367813550439993\n",
      "Epoch 43, Training Loss: 0.9352637860111724\n",
      "Epoch 44, Training Loss: 0.9343884010063974\n",
      "Epoch 45, Training Loss: 0.932594120592103\n",
      "Epoch 46, Training Loss: 0.9319327067611809\n",
      "Epoch 47, Training Loss: 0.9304220989234465\n",
      "Epoch 48, Training Loss: 0.9288592097454501\n",
      "Epoch 49, Training Loss: 0.9274941148614525\n",
      "Epoch 50, Training Loss: 0.9261845902392739\n",
      "Epoch 51, Training Loss: 0.9249675183367908\n",
      "Epoch 52, Training Loss: 0.9236185471814378\n",
      "Epoch 53, Training Loss: 0.9211658345129257\n",
      "Epoch 54, Training Loss: 0.9200157145808514\n",
      "Epoch 55, Training Loss: 0.918339567076891\n",
      "Epoch 56, Training Loss: 0.9165096861079223\n",
      "Epoch 57, Training Loss: 0.9143489559790245\n",
      "Epoch 58, Training Loss: 0.9127305953126205\n",
      "Epoch 59, Training Loss: 0.9105748409615424\n",
      "Epoch 60, Training Loss: 0.9085480327893021\n",
      "Epoch 61, Training Loss: 0.9062612727172392\n",
      "Epoch 62, Training Loss: 0.9041649376539359\n",
      "Epoch 63, Training Loss: 0.9020718780675329\n",
      "Epoch 64, Training Loss: 0.8998404140759232\n",
      "Epoch 65, Training Loss: 0.8972154813601558\n",
      "Epoch 66, Training Loss: 0.8946931233083395\n",
      "Epoch 67, Training Loss: 0.8927166629554634\n",
      "Epoch 68, Training Loss: 0.8895138464475932\n",
      "Epoch 69, Training Loss: 0.8875678782176254\n",
      "Epoch 70, Training Loss: 0.8844110992618073\n",
      "Epoch 71, Training Loss: 0.8814563887459891\n",
      "Epoch 72, Training Loss: 0.8783340956931723\n",
      "Epoch 73, Training Loss: 0.875320920101682\n",
      "Epoch 74, Training Loss: 0.8725852663355662\n",
      "Epoch 75, Training Loss: 0.8696474882893096\n",
      "Epoch 76, Training Loss: 0.8669740600693495\n",
      "Epoch 77, Training Loss: 0.8642077015754872\n",
      "Epoch 78, Training Loss: 0.8618983698070497\n",
      "Epoch 79, Training Loss: 0.8589314923250586\n",
      "Epoch 80, Training Loss: 0.8565022581502011\n",
      "Epoch 81, Training Loss: 0.8531485195446732\n",
      "Epoch 82, Training Loss: 0.8508344695980388\n",
      "Epoch 83, Training Loss: 0.8489473459415866\n",
      "Epoch 84, Training Loss: 0.8460788927580181\n",
      "Epoch 85, Training Loss: 0.8450142039392228\n",
      "Epoch 86, Training Loss: 0.8420739495664611\n",
      "Epoch 87, Training Loss: 0.8404603265281907\n",
      "Epoch 88, Training Loss: 0.8386233524272316\n",
      "Epoch 89, Training Loss: 0.8365384375242363\n",
      "Epoch 90, Training Loss: 0.8353420762191142\n",
      "Epoch 91, Training Loss: 0.8340773499997934\n",
      "Epoch 92, Training Loss: 0.8324239876037254\n",
      "Epoch 93, Training Loss: 0.8305543341134723\n",
      "Epoch 94, Training Loss: 0.8294459041796233\n",
      "Epoch 95, Training Loss: 0.8284954042362987\n",
      "Epoch 96, Training Loss: 0.8277145121330606\n",
      "Epoch 97, Training Loss: 0.8261554549063058\n",
      "Epoch 98, Training Loss: 0.825232052668593\n",
      "Epoch 99, Training Loss: 0.8242765415880017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:52:58,014] Trial 145 finished with value: 0.6200666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8241094900253124\n",
      "Epoch 1, Training Loss: 0.909601757666644\n",
      "Epoch 2, Training Loss: 0.8475695840050192\n",
      "Epoch 3, Training Loss: 0.8393498614255119\n",
      "Epoch 4, Training Loss: 0.8377863151185653\n",
      "Epoch 5, Training Loss: 0.8352601234351887\n",
      "Epoch 6, Training Loss: 0.8335153849685893\n",
      "Epoch 7, Training Loss: 0.831787570364335\n",
      "Epoch 8, Training Loss: 0.8324148265053244\n",
      "Epoch 9, Training Loss: 0.8313650782669292\n",
      "Epoch 10, Training Loss: 0.8293615447773653\n",
      "Epoch 11, Training Loss: 0.829108072308933\n",
      "Epoch 12, Training Loss: 0.8296068849984337\n",
      "Epoch 13, Training Loss: 0.8285188296261956\n",
      "Epoch 14, Training Loss: 0.8287910802224103\n",
      "Epoch 15, Training Loss: 0.8269950817613041\n",
      "Epoch 16, Training Loss: 0.83052329231711\n",
      "Epoch 17, Training Loss: 0.829179126515108\n",
      "Epoch 18, Training Loss: 0.8284538354593165\n",
      "Epoch 19, Training Loss: 0.8292686730272629\n",
      "Epoch 20, Training Loss: 0.8299180187898524\n",
      "Epoch 21, Training Loss: 0.8296671194889966\n",
      "Epoch 22, Training Loss: 0.8289940188211553\n",
      "Epoch 23, Training Loss: 0.8285721019436332\n",
      "Epoch 24, Training Loss: 0.828888052070842\n",
      "Epoch 25, Training Loss: 0.8291481593777151\n",
      "Epoch 26, Training Loss: 0.8290290536600001\n",
      "Epoch 27, Training Loss: 0.8296504993298475\n",
      "Epoch 28, Training Loss: 0.8289396896783043\n",
      "Epoch 29, Training Loss: 0.8288931182552787\n",
      "Epoch 30, Training Loss: 0.8302078129263485\n",
      "Epoch 31, Training Loss: 0.8285587896318997\n",
      "Epoch 32, Training Loss: 0.8267724448793075\n",
      "Epoch 33, Training Loss: 0.827653019989238\n",
      "Epoch 34, Training Loss: 0.8272023183457992\n",
      "Epoch 35, Training Loss: 0.82568134097492\n",
      "Epoch 36, Training Loss: 0.8265077440878924\n",
      "Epoch 37, Training Loss: 0.8277835014988394\n",
      "Epoch 38, Training Loss: 0.8274433027295506\n",
      "Epoch 39, Training Loss: 0.8299482325946583\n",
      "Epoch 40, Training Loss: 0.8264561299015494\n",
      "Epoch 41, Training Loss: 0.8277313608982984\n",
      "Epoch 42, Training Loss: 0.8265144099207485\n",
      "Epoch 43, Training Loss: 0.8285631250633912\n",
      "Epoch 44, Training Loss: 0.8265476364247939\n",
      "Epoch 45, Training Loss: 0.8260944623105666\n",
      "Epoch 46, Training Loss: 0.8258537389951593\n",
      "Epoch 47, Training Loss: 0.8286813407084521\n",
      "Epoch 48, Training Loss: 0.8279648242277258\n",
      "Epoch 49, Training Loss: 0.8293963496825274\n",
      "Epoch 50, Training Loss: 0.8274930848794825\n",
      "Epoch 51, Training Loss: 0.8262063313231749\n",
      "Epoch 52, Training Loss: 0.8251213273581337\n",
      "Epoch 53, Training Loss: 0.8283709467859829\n",
      "Epoch 54, Training Loss: 0.8288434231281281\n",
      "Epoch 55, Training Loss: 0.823904908544877\n",
      "Epoch 56, Training Loss: 0.8255815595037796\n",
      "Epoch 57, Training Loss: 0.8247781580335953\n",
      "Epoch 58, Training Loss: 0.8245792477271137\n",
      "Epoch 59, Training Loss: 0.8265138578414917\n",
      "Epoch 60, Training Loss: 0.8245122424995198\n",
      "Epoch 61, Training Loss: 0.8263145093356862\n",
      "Epoch 62, Training Loss: 0.8236280931444729\n",
      "Epoch 63, Training Loss: 0.8254989748141345\n",
      "Epoch 64, Training Loss: 0.8270647109256071\n",
      "Epoch 65, Training Loss: 0.825229499129688\n",
      "Epoch 66, Training Loss: 0.8248521231903749\n",
      "Epoch 67, Training Loss: 0.826838309063631\n",
      "Epoch 68, Training Loss: 0.8258813135062947\n",
      "Epoch 69, Training Loss: 0.8285001043712391\n",
      "Epoch 70, Training Loss: 0.8285621126259074\n",
      "Epoch 71, Training Loss: 0.8262986373200136\n",
      "Epoch 72, Training Loss: 0.8272230170053594\n",
      "Epoch 73, Training Loss: 0.825497909784317\n",
      "Epoch 74, Training Loss: 0.8253723709022298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:54:46,208] Trial 146 finished with value: 0.6123333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.826470395747353\n",
      "Epoch 1, Training Loss: 0.8496972210267011\n",
      "Epoch 2, Training Loss: 0.8108639707284815\n",
      "Epoch 3, Training Loss: 0.8062835148502798\n",
      "Epoch 4, Training Loss: 0.8020381486415863\n",
      "Epoch 5, Training Loss: 0.8020351946353912\n",
      "Epoch 6, Training Loss: 0.8010668004260344\n",
      "Epoch 7, Training Loss: 0.798583409575855\n",
      "Epoch 8, Training Loss: 0.7977005326747895\n",
      "Epoch 9, Training Loss: 0.7974913428110235\n",
      "Epoch 10, Training Loss: 0.7964633551064659\n",
      "Epoch 11, Training Loss: 0.796992072427974\n",
      "Epoch 12, Training Loss: 0.7951469084094552\n",
      "Epoch 13, Training Loss: 0.7965215688593248\n",
      "Epoch 14, Training Loss: 0.7944513666629791\n",
      "Epoch 15, Training Loss: 0.7944409597621245\n",
      "Epoch 16, Training Loss: 0.7935917701440699\n",
      "Epoch 17, Training Loss: 0.793840162894305\n",
      "Epoch 18, Training Loss: 0.7945875316507676\n",
      "Epoch 19, Training Loss: 0.7940121552523445\n",
      "Epoch 20, Training Loss: 0.793834861797445\n",
      "Epoch 21, Training Loss: 0.7932885787767523\n",
      "Epoch 22, Training Loss: 0.7933968601507299\n",
      "Epoch 23, Training Loss: 0.7927105032696443\n",
      "Epoch 24, Training Loss: 0.7938290736254524\n",
      "Epoch 25, Training Loss: 0.7932427047981936\n",
      "Epoch 26, Training Loss: 0.7926127292829401\n",
      "Epoch 27, Training Loss: 0.7933146983735702\n",
      "Epoch 28, Training Loss: 0.7932628525004667\n",
      "Epoch 29, Training Loss: 0.7925028540807612\n",
      "Epoch 30, Training Loss: 0.7927869571657742\n",
      "Epoch 31, Training Loss: 0.7920513092770296\n",
      "Epoch 32, Training Loss: 0.7931632431815653\n",
      "Epoch 33, Training Loss: 0.7918374775437748\n",
      "Epoch 34, Training Loss: 0.7911117814568912\n",
      "Epoch 35, Training Loss: 0.7913091597136329\n",
      "Epoch 36, Training Loss: 0.7911177398176754\n",
      "Epoch 37, Training Loss: 0.7912863290309906\n",
      "Epoch 38, Training Loss: 0.7911914108781254\n",
      "Epoch 39, Training Loss: 0.79114522394012\n",
      "Epoch 40, Training Loss: 0.7926453296577229\n",
      "Epoch 41, Training Loss: 0.7896179407484392\n",
      "Epoch 42, Training Loss: 0.7906301497711855\n",
      "Epoch 43, Training Loss: 0.791324084295946\n",
      "Epoch 44, Training Loss: 0.7909876648117514\n",
      "Epoch 45, Training Loss: 0.7902264604848974\n",
      "Epoch 46, Training Loss: 0.7903618820975808\n",
      "Epoch 47, Training Loss: 0.7897170535255881\n",
      "Epoch 48, Training Loss: 0.7916943810967838\n",
      "Epoch 49, Training Loss: 0.7906713117571438\n",
      "Epoch 50, Training Loss: 0.7898138464198393\n",
      "Epoch 51, Training Loss: 0.7900695187905256\n",
      "Epoch 52, Training Loss: 0.790446022678824\n",
      "Epoch 53, Training Loss: 0.7906022264676935\n",
      "Epoch 54, Training Loss: 0.7900950316821828\n",
      "Epoch 55, Training Loss: 0.7897828574040356\n",
      "Epoch 56, Training Loss: 0.7901963733925539\n",
      "Epoch 57, Training Loss: 0.7894693139020135\n",
      "Epoch 58, Training Loss: 0.790536950896768\n",
      "Epoch 59, Training Loss: 0.7901581755105187\n",
      "Epoch 60, Training Loss: 0.7890288877487183\n",
      "Epoch 61, Training Loss: 0.7896259705459371\n",
      "Epoch 62, Training Loss: 0.7901968424460467\n",
      "Epoch 63, Training Loss: 0.7907830878566293\n",
      "Epoch 64, Training Loss: 0.7898850602262161\n",
      "Epoch 65, Training Loss: 0.789211994339438\n",
      "Epoch 66, Training Loss: 0.7907781915804919\n",
      "Epoch 67, Training Loss: 0.7890429719756631\n",
      "Epoch 68, Training Loss: 0.7897414832956651\n",
      "Epoch 69, Training Loss: 0.7879770142190596\n",
      "Epoch 70, Training Loss: 0.7885393207914689\n",
      "Epoch 71, Training Loss: 0.789447401972378\n",
      "Epoch 72, Training Loss: 0.79006283065852\n",
      "Epoch 73, Training Loss: 0.7897379421486574\n",
      "Epoch 74, Training Loss: 0.7884586287947262\n",
      "Epoch 75, Training Loss: 0.7895646887667039\n",
      "Epoch 76, Training Loss: 0.7890657997131347\n",
      "Epoch 77, Training Loss: 0.7886745680780972\n",
      "Epoch 78, Training Loss: 0.7883521590513342\n",
      "Epoch 79, Training Loss: 0.78865170100156\n",
      "Epoch 80, Training Loss: 0.7888244562289294\n",
      "Epoch 81, Training Loss: 0.7889066743149477\n",
      "Epoch 82, Training Loss: 0.7887336399975945\n",
      "Epoch 83, Training Loss: 0.7889855032107409\n",
      "Epoch 84, Training Loss: 0.7881207487863653\n",
      "Epoch 85, Training Loss: 0.7888893257169163\n",
      "Epoch 86, Training Loss: 0.7882246286027572\n",
      "Epoch 87, Training Loss: 0.7893248145019307\n",
      "Epoch 88, Training Loss: 0.7884314504090477\n",
      "Epoch 89, Training Loss: 0.7882590379434473\n",
      "Epoch 90, Training Loss: 0.788105044855791\n",
      "Epoch 91, Training Loss: 0.7888221037387848\n",
      "Epoch 92, Training Loss: 0.7876625081370858\n",
      "Epoch 93, Training Loss: 0.7881450845914728\n",
      "Epoch 94, Training Loss: 0.788505694024703\n",
      "Epoch 95, Training Loss: 0.787422941642649\n",
      "Epoch 96, Training Loss: 0.7885267623733072\n",
      "Epoch 97, Training Loss: 0.7878994218040916\n",
      "Epoch 98, Training Loss: 0.7882706641449648\n",
      "Epoch 99, Training Loss: 0.7883118827202741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:57:14,795] Trial 147 finished with value: 0.634 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7873372824753032\n",
      "Epoch 1, Training Loss: 1.0590470225670758\n",
      "Epoch 2, Training Loss: 1.0156946761467878\n",
      "Epoch 3, Training Loss: 0.989027799578274\n",
      "Epoch 4, Training Loss: 0.9737997926683987\n",
      "Epoch 5, Training Loss: 0.9658827156179092\n",
      "Epoch 6, Training Loss: 0.9616564632163328\n",
      "Epoch 7, Training Loss: 0.9592659399088691\n",
      "Epoch 8, Training Loss: 0.9576610679486218\n",
      "Epoch 9, Training Loss: 0.9563324911454144\n",
      "Epoch 10, Training Loss: 0.9551832347056445\n",
      "Epoch 11, Training Loss: 0.9540589387977825\n",
      "Epoch 12, Training Loss: 0.9530822228684145\n",
      "Epoch 13, Training Loss: 0.9519646998713999\n",
      "Epoch 14, Training Loss: 0.950831940314349\n",
      "Epoch 15, Training Loss: 0.9495975686522091\n",
      "Epoch 16, Training Loss: 0.9485190726729\n",
      "Epoch 17, Training Loss: 0.9472273286651163\n",
      "Epoch 18, Training Loss: 0.9459494394414565\n",
      "Epoch 19, Training Loss: 0.9447467929475448\n",
      "Epoch 20, Training Loss: 0.9434445569795721\n",
      "Epoch 21, Training Loss: 0.9421455090186175\n",
      "Epoch 22, Training Loss: 0.9406853720721077\n",
      "Epoch 23, Training Loss: 0.939290355163462\n",
      "Epoch 24, Training Loss: 0.9378140826786265\n",
      "Epoch 25, Training Loss: 0.9362119591236114\n",
      "Epoch 26, Training Loss: 0.9346413806606741\n",
      "Epoch 27, Training Loss: 0.9330602155012243\n",
      "Epoch 28, Training Loss: 0.9314017463431639\n",
      "Epoch 29, Training Loss: 0.9296421130264506\n",
      "Epoch 30, Training Loss: 0.9278694891228395\n",
      "Epoch 31, Training Loss: 0.9261279714808744\n",
      "Epoch 32, Training Loss: 0.9242254499126883\n",
      "Epoch 33, Training Loss: 0.9222948136049158\n",
      "Epoch 34, Training Loss: 0.9202915118021123\n",
      "Epoch 35, Training Loss: 0.9183100941601922\n",
      "Epoch 36, Training Loss: 0.9161786591305452\n",
      "Epoch 37, Training Loss: 0.9140682200123282\n",
      "Epoch 38, Training Loss: 0.9118828128365909\n",
      "Epoch 39, Training Loss: 0.9097579549340641\n",
      "Epoch 40, Training Loss: 0.9073872654578266\n",
      "Epoch 41, Training Loss: 0.9051064243036158\n",
      "Epoch 42, Training Loss: 0.90281369805336\n",
      "Epoch 43, Training Loss: 0.900490062938017\n",
      "Epoch 44, Training Loss: 0.8980445967702304\n",
      "Epoch 45, Training Loss: 0.8956679555247812\n",
      "Epoch 46, Training Loss: 0.8932506989731508\n",
      "Epoch 47, Training Loss: 0.8907811767914716\n",
      "Epoch 48, Training Loss: 0.8883364528768203\n",
      "Epoch 49, Training Loss: 0.8858891805480508\n",
      "Epoch 50, Training Loss: 0.8834084153175354\n",
      "Epoch 51, Training Loss: 0.8809385304591235\n",
      "Epoch 52, Training Loss: 0.8785118651390076\n",
      "Epoch 53, Training Loss: 0.87603170542156\n",
      "Epoch 54, Training Loss: 0.8736153244270998\n",
      "Epoch 55, Training Loss: 0.8712492728233338\n",
      "Epoch 56, Training Loss: 0.8688937058168299\n",
      "Epoch 57, Training Loss: 0.8665285789265352\n",
      "Epoch 58, Training Loss: 0.8642648302106296\n",
      "Epoch 59, Training Loss: 0.8620813420940848\n",
      "Epoch 60, Training Loss: 0.8598481686676249\n",
      "Epoch 61, Training Loss: 0.8576700373256908\n",
      "Epoch 62, Training Loss: 0.8555947737132802\n",
      "Epoch 63, Training Loss: 0.8534891403422636\n",
      "Epoch 64, Training Loss: 0.8515998053550721\n",
      "Epoch 65, Training Loss: 0.8496296888239243\n",
      "Epoch 66, Training Loss: 0.8477097386472365\n",
      "Epoch 67, Training Loss: 0.8459111076944015\n",
      "Epoch 68, Training Loss: 0.8442180784309612\n",
      "Epoch 69, Training Loss: 0.8424063221146079\n",
      "Epoch 70, Training Loss: 0.8407653619261349\n",
      "Epoch 71, Training Loss: 0.8392604269700892\n",
      "Epoch 72, Training Loss: 0.837756431313122\n",
      "Epoch 73, Training Loss: 0.836318235467462\n",
      "Epoch 74, Training Loss: 0.8349165811258203\n",
      "Epoch 75, Training Loss: 0.8336290217147154\n",
      "Epoch 76, Training Loss: 0.8322959319282981\n",
      "Epoch 77, Training Loss: 0.8311295732329873\n",
      "Epoch 78, Training Loss: 0.8300224851860719\n",
      "Epoch 79, Training Loss: 0.828835805163664\n",
      "Epoch 80, Training Loss: 0.82778984560686\n",
      "Epoch 81, Training Loss: 0.8268313437349656\n",
      "Epoch 82, Training Loss: 0.8258792816190159\n",
      "Epoch 83, Training Loss: 0.8250150655998904\n",
      "Epoch 84, Training Loss: 0.8241821859163396\n",
      "Epoch 85, Training Loss: 0.8233603492905112\n",
      "Epoch 86, Training Loss: 0.8225046244088341\n",
      "Epoch 87, Training Loss: 0.8218551439397476\n",
      "Epoch 88, Training Loss: 0.8210952472686768\n",
      "Epoch 89, Training Loss: 0.8205400147157557\n",
      "Epoch 90, Training Loss: 0.8199168353220996\n",
      "Epoch 91, Training Loss: 0.8193420091797324\n",
      "Epoch 92, Training Loss: 0.8187949299111086\n",
      "Epoch 93, Training Loss: 0.8182370541376226\n",
      "Epoch 94, Training Loss: 0.8178067031327416\n",
      "Epoch 95, Training Loss: 0.8172915154344895\n",
      "Epoch 96, Training Loss: 0.8169515616052291\n",
      "Epoch 97, Training Loss: 0.8164465972255258\n",
      "Epoch 98, Training Loss: 0.8160353978942422\n",
      "Epoch 99, Training Loss: 0.8156756058861228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:58:47,605] Trial 148 finished with value: 0.6243333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8153602194786072\n",
      "Epoch 1, Training Loss: 1.0849987338360092\n",
      "Epoch 2, Training Loss: 1.041793728412542\n",
      "Epoch 3, Training Loss: 1.0176093367705668\n",
      "Epoch 4, Training Loss: 1.002019955609974\n",
      "Epoch 5, Training Loss: 0.9909212706680585\n",
      "Epoch 6, Training Loss: 0.9829785658900899\n",
      "Epoch 7, Training Loss: 0.9770160829214225\n",
      "Epoch 8, Training Loss: 0.9726640480801575\n",
      "Epoch 9, Training Loss: 0.9691179595495525\n",
      "Epoch 10, Training Loss: 0.9665584719270692\n",
      "Epoch 11, Training Loss: 0.9641407357122666\n",
      "Epoch 12, Training Loss: 0.9621983761177924\n",
      "Epoch 13, Training Loss: 0.960761180676912\n",
      "Epoch 14, Training Loss: 0.9594247514143922\n",
      "Epoch 15, Training Loss: 0.9590910059168822\n",
      "Epoch 16, Training Loss: 0.95818041043174\n",
      "Epoch 17, Training Loss: 0.9569543087392821\n",
      "Epoch 18, Training Loss: 0.9556248507105318\n",
      "Epoch 19, Training Loss: 0.9547609536271346\n",
      "Epoch 20, Training Loss: 0.9542064919507594\n",
      "Epoch 21, Training Loss: 0.9534471562034206\n",
      "Epoch 22, Training Loss: 0.9526511901303342\n",
      "Epoch 23, Training Loss: 0.9521352828893447\n",
      "Epoch 24, Training Loss: 0.9512404263467718\n",
      "Epoch 25, Training Loss: 0.9505687757542259\n",
      "Epoch 26, Training Loss: 0.9493407085425871\n",
      "Epoch 27, Training Loss: 0.9486012280435491\n",
      "Epoch 28, Training Loss: 0.9481866310413619\n",
      "Epoch 29, Training Loss: 0.9472831284193168\n",
      "Epoch 30, Training Loss: 0.9467153942674622\n",
      "Epoch 31, Training Loss: 0.945292299851439\n",
      "Epoch 32, Training Loss: 0.9449732040104113\n",
      "Epoch 33, Training Loss: 0.9438004724065164\n",
      "Epoch 34, Training Loss: 0.9429372621658153\n",
      "Epoch 35, Training Loss: 0.942201450534333\n",
      "Epoch 36, Training Loss: 0.9410555278448234\n",
      "Epoch 37, Training Loss: 0.9404720023162383\n",
      "Epoch 38, Training Loss: 0.93911992598297\n",
      "Epoch 39, Training Loss: 0.9380544719839454\n",
      "Epoch 40, Training Loss: 0.9370518635986442\n",
      "Epoch 41, Training Loss: 0.9365506845309322\n",
      "Epoch 42, Training Loss: 0.9353751547354504\n",
      "Epoch 43, Training Loss: 0.9340621118258713\n",
      "Epoch 44, Training Loss: 0.9330372684880307\n",
      "Epoch 45, Training Loss: 0.9319307619467714\n",
      "Epoch 46, Training Loss: 0.9307677813042375\n",
      "Epoch 47, Training Loss: 0.9299248162965129\n",
      "Epoch 48, Training Loss: 0.9282081664953017\n",
      "Epoch 49, Training Loss: 0.927452804540333\n",
      "Epoch 50, Training Loss: 0.9261769570802387\n",
      "Epoch 51, Training Loss: 0.9246467475604294\n",
      "Epoch 52, Training Loss: 0.9237383928514065\n",
      "Epoch 53, Training Loss: 0.9224422178770366\n",
      "Epoch 54, Training Loss: 0.9206749790593197\n",
      "Epoch 55, Training Loss: 0.9194839961546705\n",
      "Epoch 56, Training Loss: 0.918093516772851\n",
      "Epoch 57, Training Loss: 0.9168144851698912\n",
      "Epoch 58, Training Loss: 0.9151170424052647\n",
      "Epoch 59, Training Loss: 0.9135247267278513\n",
      "Epoch 60, Training Loss: 0.9119375881395841\n",
      "Epoch 61, Training Loss: 0.9110305958224418\n",
      "Epoch 62, Training Loss: 0.9089003484948237\n",
      "Epoch 63, Training Loss: 0.9075837165789497\n",
      "Epoch 64, Training Loss: 0.9057012269371434\n",
      "Epoch 65, Training Loss: 0.9038369732691829\n",
      "Epoch 66, Training Loss: 0.9026421421452573\n",
      "Epoch 67, Training Loss: 0.9004562089317724\n",
      "Epoch 68, Training Loss: 0.8988239198699033\n",
      "Epoch 69, Training Loss: 0.8971208213863516\n",
      "Epoch 70, Training Loss: 0.8951269166810172\n",
      "Epoch 71, Training Loss: 0.8934585099829767\n",
      "Epoch 72, Training Loss: 0.89190411397389\n",
      "Epoch 73, Training Loss: 0.8899406500328753\n",
      "Epoch 74, Training Loss: 0.8883055688743304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 23:59:57,642] Trial 149 finished with value: 0.5849333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8867983141339811\n",
      "Epoch 1, Training Loss: 0.8506541944952573\n",
      "Epoch 2, Training Loss: 0.820791692313026\n",
      "Epoch 3, Training Loss: 0.8170699253503014\n",
      "Epoch 4, Training Loss: 0.8150580146733453\n",
      "Epoch 5, Training Loss: 0.8143321793219622\n",
      "Epoch 6, Training Loss: 0.813740647540373\n",
      "Epoch 7, Training Loss: 0.811310859077117\n",
      "Epoch 8, Training Loss: 0.811555556269253\n",
      "Epoch 9, Training Loss: 0.8115649740836199\n",
      "Epoch 10, Training Loss: 0.8110615816537071\n",
      "Epoch 11, Training Loss: 0.81094217524809\n",
      "Epoch 12, Training Loss: 0.8101423966884613\n",
      "Epoch 13, Training Loss: 0.8124168539047241\n",
      "Epoch 14, Training Loss: 0.8101413699458627\n",
      "Epoch 15, Training Loss: 0.8101540928027209\n",
      "Epoch 16, Training Loss: 0.8096633984762079\n",
      "Epoch 17, Training Loss: 0.8079434645877165\n",
      "Epoch 18, Training Loss: 0.8092800232943367\n",
      "Epoch 19, Training Loss: 0.8073800939672133\n",
      "Epoch 20, Training Loss: 0.8083053922653198\n",
      "Epoch 21, Training Loss: 0.8082798905232373\n",
      "Epoch 22, Training Loss: 0.8077184506724863\n",
      "Epoch 23, Training Loss: 0.8071292054653167\n",
      "Epoch 24, Training Loss: 0.8066927664420184\n",
      "Epoch 25, Training Loss: 0.8056298852668089\n",
      "Epoch 26, Training Loss: 0.8055180811180788\n",
      "Epoch 27, Training Loss: 0.8069778636623831\n",
      "Epoch 28, Training Loss: 0.8054045456297257\n",
      "Epoch 29, Training Loss: 0.8055679190860076\n",
      "Epoch 30, Training Loss: 0.8048952100557439\n",
      "Epoch 31, Training Loss: 0.8047433752873364\n",
      "Epoch 32, Training Loss: 0.8065075091053457\n",
      "Epoch 33, Training Loss: 0.8036126537883983\n",
      "Epoch 34, Training Loss: 0.8019694164921256\n",
      "Epoch 35, Training Loss: 0.8037480148147135\n",
      "Epoch 36, Training Loss: 0.8032967441923478\n",
      "Epoch 37, Training Loss: 0.8027092613192166\n",
      "Epoch 38, Training Loss: 0.8042508198233211\n",
      "Epoch 39, Training Loss: 0.8019591810422785\n",
      "Epoch 40, Training Loss: 0.8016947977683123\n",
      "Epoch 41, Training Loss: 0.8008129977478701\n",
      "Epoch 42, Training Loss: 0.8021138203845305\n",
      "Epoch 43, Training Loss: 0.8024742051433115\n",
      "Epoch 44, Training Loss: 0.8014807551748613\n",
      "Epoch 45, Training Loss: 0.8024960544530083\n",
      "Epoch 46, Training Loss: 0.8002819496743819\n",
      "Epoch 47, Training Loss: 0.800397593904944\n",
      "Epoch 48, Training Loss: 0.7999723403594073\n",
      "Epoch 49, Training Loss: 0.8023812945450054\n",
      "Epoch 50, Training Loss: 0.8009839743726394\n",
      "Epoch 51, Training Loss: 0.799873658769271\n",
      "Epoch 52, Training Loss: 0.8015026191402884\n",
      "Epoch 53, Training Loss: 0.7993664023455451\n",
      "Epoch 54, Training Loss: 0.8012282316824969\n",
      "Epoch 55, Training Loss: 0.800453917910071\n",
      "Epoch 56, Training Loss: 0.8010638876522289\n",
      "Epoch 57, Training Loss: 0.8008139043695787\n",
      "Epoch 58, Training Loss: 0.8002962316485013\n",
      "Epoch 59, Training Loss: 0.8002825230710646\n",
      "Epoch 60, Training Loss: 0.8004997077408958\n",
      "Epoch 61, Training Loss: 0.7994952212361729\n",
      "Epoch 62, Training Loss: 0.8003298304361456\n",
      "Epoch 63, Training Loss: 0.7991545167390038\n",
      "Epoch 64, Training Loss: 0.7995432748514063\n",
      "Epoch 65, Training Loss: 0.7994518216217266\n",
      "Epoch 66, Training Loss: 0.8008513085982378\n",
      "Epoch 67, Training Loss: 0.7994548087260303\n",
      "Epoch 68, Training Loss: 0.7999349545731264\n",
      "Epoch 69, Training Loss: 0.7992433481356677\n",
      "Epoch 70, Training Loss: 0.8005458310772391\n",
      "Epoch 71, Training Loss: 0.7990245302985696\n",
      "Epoch 72, Training Loss: 0.7997160631768844\n",
      "Epoch 73, Training Loss: 0.7986235335293939\n",
      "Epoch 74, Training Loss: 0.7981858308876262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:01:26,223] Trial 150 finished with value: 0.6294666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7991892573412727\n",
      "Epoch 1, Training Loss: 1.0920542799725252\n",
      "Epoch 2, Training Loss: 1.0910161753261791\n",
      "Epoch 3, Training Loss: 1.090782863673042\n",
      "Epoch 4, Training Loss: 1.09053231085048\n",
      "Epoch 5, Training Loss: 1.0903053955470814\n",
      "Epoch 6, Training Loss: 1.0900673717610976\n",
      "Epoch 7, Training Loss: 1.089820850877201\n",
      "Epoch 8, Training Loss: 1.0895862217510448\n",
      "Epoch 9, Training Loss: 1.0893522662274977\n",
      "Epoch 10, Training Loss: 1.089115309154286\n",
      "Epoch 11, Training Loss: 1.088879951028263\n",
      "Epoch 12, Training Loss: 1.088632395547979\n",
      "Epoch 13, Training Loss: 1.0883966214516583\n",
      "Epoch 14, Training Loss: 1.0881353198780732\n",
      "Epoch 15, Training Loss: 1.0878994206821218\n",
      "Epoch 16, Training Loss: 1.087652914944817\n",
      "Epoch 17, Training Loss: 1.0874035138242384\n",
      "Epoch 18, Training Loss: 1.0871617247076595\n",
      "Epoch 19, Training Loss: 1.086897046285517\n",
      "Epoch 20, Training Loss: 1.0866526255888098\n",
      "Epoch 21, Training Loss: 1.086380085804883\n",
      "Epoch 22, Training Loss: 1.0861191227856803\n",
      "Epoch 23, Training Loss: 1.0858472254697014\n",
      "Epoch 24, Training Loss: 1.0855868896316079\n",
      "Epoch 25, Training Loss: 1.0853206677997813\n",
      "Epoch 26, Training Loss: 1.085042301346274\n",
      "Epoch 27, Training Loss: 1.0847643675523646\n",
      "Epoch 28, Training Loss: 1.0844720315933227\n",
      "Epoch 29, Training Loss: 1.0841819318603068\n",
      "Epoch 30, Training Loss: 1.0838982203427483\n",
      "Epoch 31, Training Loss: 1.0835892882066613\n",
      "Epoch 32, Training Loss: 1.0832992518649383\n",
      "Epoch 33, Training Loss: 1.0829765248298644\n",
      "Epoch 34, Training Loss: 1.0826738493582782\n",
      "Epoch 35, Training Loss: 1.0823417092772092\n",
      "Epoch 36, Training Loss: 1.0820132502387552\n",
      "Epoch 37, Training Loss: 1.081684594294604\n",
      "Epoch 38, Training Loss: 1.0813462816967683\n",
      "Epoch 39, Training Loss: 1.0810063708529754\n",
      "Epoch 40, Training Loss: 1.0806533908843994\n",
      "Epoch 41, Training Loss: 1.080296767879935\n",
      "Epoch 42, Training Loss: 1.0799126623658573\n",
      "Epoch 43, Training Loss: 1.0795452549878288\n",
      "Epoch 44, Training Loss: 1.0791592058013468\n",
      "Epoch 45, Training Loss: 1.0787655322691974\n",
      "Epoch 46, Training Loss: 1.0783743276315576\n",
      "Epoch 47, Training Loss: 1.0779579987245447\n",
      "Epoch 48, Training Loss: 1.0775439897705528\n",
      "Epoch 49, Training Loss: 1.0771075769031748\n",
      "Epoch 50, Training Loss: 1.0766690635681153\n",
      "Epoch 51, Training Loss: 1.0762127898721134\n",
      "Epoch 52, Training Loss: 1.0757663768880508\n",
      "Epoch 53, Training Loss: 1.0752909873513614\n",
      "Epoch 54, Training Loss: 1.0748176462510053\n",
      "Epoch 55, Training Loss: 1.0743217398138607\n",
      "Epoch 56, Training Loss: 1.0738211902450112\n",
      "Epoch 57, Training Loss: 1.0733021929684807\n",
      "Epoch 58, Training Loss: 1.0727778007002438\n",
      "Epoch 59, Training Loss: 1.0722212826504427\n",
      "Epoch 60, Training Loss: 1.0716824431980359\n",
      "Epoch 61, Training Loss: 1.071115567964666\n",
      "Epoch 62, Training Loss: 1.0705284601099352\n",
      "Epoch 63, Training Loss: 1.0699431391323313\n",
      "Epoch 64, Training Loss: 1.0693292258767522\n",
      "Epoch 65, Training Loss: 1.068707512546988\n",
      "Epoch 66, Training Loss: 1.0680562445696662\n",
      "Epoch 67, Training Loss: 1.0674027914159439\n",
      "Epoch 68, Training Loss: 1.0667489492191988\n",
      "Epoch 69, Training Loss: 1.0660628219211803\n",
      "Epoch 70, Training Loss: 1.0653594548561993\n",
      "Epoch 71, Training Loss: 1.0646376518642202\n",
      "Epoch 72, Training Loss: 1.0639104423803443\n",
      "Epoch 73, Training Loss: 1.0631557918997372\n",
      "Epoch 74, Training Loss: 1.0623724555969238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:02:45,197] Trial 151 finished with value: 0.4586 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 1.0616015918114605\n",
      "Epoch 1, Training Loss: 1.0229671472893622\n",
      "Epoch 2, Training Loss: 0.9569102469243501\n",
      "Epoch 3, Training Loss: 0.9386818185784763\n",
      "Epoch 4, Training Loss: 0.9289867013020623\n",
      "Epoch 5, Training Loss: 0.9224935796027793\n",
      "Epoch 6, Training Loss: 0.9172930092739879\n",
      "Epoch 7, Training Loss: 0.9124546373697152\n",
      "Epoch 8, Training Loss: 0.9076197660058961\n",
      "Epoch 9, Training Loss: 0.9029550520100988\n",
      "Epoch 10, Training Loss: 0.8983578929327485\n",
      "Epoch 11, Training Loss: 0.8937659170394553\n",
      "Epoch 12, Training Loss: 0.8885888753977037\n",
      "Epoch 13, Training Loss: 0.8825291495574148\n",
      "Epoch 14, Training Loss: 0.8773112733561294\n",
      "Epoch 15, Training Loss: 0.8710361691345846\n",
      "Epoch 16, Training Loss: 0.8648362588165398\n",
      "Epoch 17, Training Loss: 0.8589304036663887\n",
      "Epoch 18, Training Loss: 0.8526305169987499\n",
      "Epoch 19, Training Loss: 0.8470838773519473\n",
      "Epoch 20, Training Loss: 0.8418612628951109\n",
      "Epoch 21, Training Loss: 0.8364054744404957\n",
      "Epoch 22, Training Loss: 0.8319037412342273\n",
      "Epoch 23, Training Loss: 0.8279960949618117\n",
      "Epoch 24, Training Loss: 0.8245165617842424\n",
      "Epoch 25, Training Loss: 0.8217963260815556\n",
      "Epoch 26, Training Loss: 0.8189011903633748\n",
      "Epoch 27, Training Loss: 0.8163405535812664\n",
      "Epoch 28, Training Loss: 0.8147641874793777\n",
      "Epoch 29, Training Loss: 0.8126615266154583\n",
      "Epoch 30, Training Loss: 0.8115104203833673\n",
      "Epoch 31, Training Loss: 0.8099126408870955\n",
      "Epoch 32, Training Loss: 0.8088417031711206\n",
      "Epoch 33, Training Loss: 0.8082212407786147\n",
      "Epoch 34, Training Loss: 0.807520912374769\n",
      "Epoch 35, Training Loss: 0.8066638653439687\n",
      "Epoch 36, Training Loss: 0.8055742840121564\n",
      "Epoch 37, Training Loss: 0.80509030200485\n",
      "Epoch 38, Training Loss: 0.8048092984615411\n",
      "Epoch 39, Training Loss: 0.8042233901812618\n",
      "Epoch 40, Training Loss: 0.803658928548483\n",
      "Epoch 41, Training Loss: 0.8037774091376397\n",
      "Epoch 42, Training Loss: 0.8028279575190149\n",
      "Epoch 43, Training Loss: 0.8025364204456932\n",
      "Epoch 44, Training Loss: 0.8023862762558729\n",
      "Epoch 45, Training Loss: 0.8021290723542521\n",
      "Epoch 46, Training Loss: 0.802230599679445\n",
      "Epoch 47, Training Loss: 0.8014461686736659\n",
      "Epoch 48, Training Loss: 0.8019447810667798\n",
      "Epoch 49, Training Loss: 0.8011818068368094\n",
      "Epoch 50, Training Loss: 0.8009176668367888\n",
      "Epoch 51, Training Loss: 0.8005826455309875\n",
      "Epoch 52, Training Loss: 0.8015552757377912\n",
      "Epoch 53, Training Loss: 0.8006044876306577\n",
      "Epoch 54, Training Loss: 0.8003859862349088\n",
      "Epoch 55, Training Loss: 0.8004060666363938\n",
      "Epoch 56, Training Loss: 0.8000829042348646\n",
      "Epoch 57, Training Loss: 0.8001448665346418\n",
      "Epoch 58, Training Loss: 0.7999719574935454\n",
      "Epoch 59, Training Loss: 0.7999933114625457\n",
      "Epoch 60, Training Loss: 0.7993891610238785\n",
      "Epoch 61, Training Loss: 0.7996653037859981\n",
      "Epoch 62, Training Loss: 0.7992269525850626\n",
      "Epoch 63, Training Loss: 0.7987954137916852\n",
      "Epoch 64, Training Loss: 0.79933921690274\n",
      "Epoch 65, Training Loss: 0.7988532494781608\n",
      "Epoch 66, Training Loss: 0.7990712513600974\n",
      "Epoch 67, Training Loss: 0.7993103461158007\n",
      "Epoch 68, Training Loss: 0.7986018853976314\n",
      "Epoch 69, Training Loss: 0.7986202746405637\n",
      "Epoch 70, Training Loss: 0.7987330822120036\n",
      "Epoch 71, Training Loss: 0.7984488878931318\n",
      "Epoch 72, Training Loss: 0.7983946107383958\n",
      "Epoch 73, Training Loss: 0.797998915758348\n",
      "Epoch 74, Training Loss: 0.798988828264681\n",
      "Epoch 75, Training Loss: 0.7986868828759157\n",
      "Epoch 76, Training Loss: 0.7983499525184918\n",
      "Epoch 77, Training Loss: 0.7983377588422675\n",
      "Epoch 78, Training Loss: 0.7987742213378275\n",
      "Epoch 79, Training Loss: 0.7985056463937114\n",
      "Epoch 80, Training Loss: 0.7983445991250805\n",
      "Epoch 81, Training Loss: 0.798080847675639\n",
      "Epoch 82, Training Loss: 0.798310321076472\n",
      "Epoch 83, Training Loss: 0.7979117233054083\n",
      "Epoch 84, Training Loss: 0.7974866704385083\n",
      "Epoch 85, Training Loss: 0.7980358912532491\n",
      "Epoch 86, Training Loss: 0.797718003817967\n",
      "Epoch 87, Training Loss: 0.797805713083511\n",
      "Epoch 88, Training Loss: 0.798029745431771\n",
      "Epoch 89, Training Loss: 0.7978156145353963\n",
      "Epoch 90, Training Loss: 0.7975677241060071\n",
      "Epoch 91, Training Loss: 0.7977465441352443\n",
      "Epoch 92, Training Loss: 0.7978767866478826\n",
      "Epoch 93, Training Loss: 0.7974477879983142\n",
      "Epoch 94, Training Loss: 0.7979832718246862\n",
      "Epoch 95, Training Loss: 0.7974928566387721\n",
      "Epoch 96, Training Loss: 0.797433316528349\n",
      "Epoch 97, Training Loss: 0.7974793288044464\n",
      "Epoch 98, Training Loss: 0.7974126745883684\n",
      "Epoch 99, Training Loss: 0.7978210338076255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:04:08,075] Trial 152 finished with value: 0.6358 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7974553779551857\n",
      "Epoch 1, Training Loss: 0.8663721152614144\n",
      "Epoch 2, Training Loss: 0.8139295744895935\n",
      "Epoch 3, Training Loss: 0.8068403987323537\n",
      "Epoch 4, Training Loss: 0.8043722297864802\n",
      "Epoch 5, Training Loss: 0.8008345328359043\n",
      "Epoch 6, Training Loss: 0.7981859031845542\n",
      "Epoch 7, Training Loss: 0.7970122323316686\n",
      "Epoch 8, Training Loss: 0.7961952422646915\n",
      "Epoch 9, Training Loss: 0.7972690209220438\n",
      "Epoch 10, Training Loss: 0.7952735326570624\n",
      "Epoch 11, Training Loss: 0.794194257329492\n",
      "Epoch 12, Training Loss: 0.7927692587936626\n",
      "Epoch 13, Training Loss: 0.7937286011611714\n",
      "Epoch 14, Training Loss: 0.7935764811319463\n",
      "Epoch 15, Training Loss: 0.7927697173286887\n",
      "Epoch 16, Training Loss: 0.7917789516028236\n",
      "Epoch 17, Training Loss: 0.7915123550330891\n",
      "Epoch 18, Training Loss: 0.7927852629914003\n",
      "Epoch 19, Training Loss: 0.7914465986981112\n",
      "Epoch 20, Training Loss: 0.7909246214698342\n",
      "Epoch 21, Training Loss: 0.7908949593936696\n",
      "Epoch 22, Training Loss: 0.7897478334342732\n",
      "Epoch 23, Training Loss: 0.7901059586160323\n",
      "Epoch 24, Training Loss: 0.7915543465754565\n",
      "Epoch 25, Training Loss: 0.7901460018578698\n",
      "Epoch 26, Training Loss: 0.7902863097190856\n",
      "Epoch 27, Training Loss: 0.7894681480351616\n",
      "Epoch 28, Training Loss: 0.789383319125456\n",
      "Epoch 29, Training Loss: 0.7893925637357375\n",
      "Epoch 30, Training Loss: 0.788741768247941\n",
      "Epoch 31, Training Loss: 0.7883698788109947\n",
      "Epoch 32, Training Loss: 0.7886381945890539\n",
      "Epoch 33, Training Loss: 0.7873013142277213\n",
      "Epoch 34, Training Loss: 0.7890424581836252\n",
      "Epoch 35, Training Loss: 0.7886308029819937\n",
      "Epoch 36, Training Loss: 0.787651818710215\n",
      "Epoch 37, Training Loss: 0.7872852865387412\n",
      "Epoch 38, Training Loss: 0.7878811177085427\n",
      "Epoch 39, Training Loss: 0.7875628521863152\n",
      "Epoch 40, Training Loss: 0.7872372416187735\n",
      "Epoch 41, Training Loss: 0.7871267329945284\n",
      "Epoch 42, Training Loss: 0.7877413282674902\n",
      "Epoch 43, Training Loss: 0.786128359261681\n",
      "Epoch 44, Training Loss: 0.7876534743870006\n",
      "Epoch 45, Training Loss: 0.7870487117066103\n",
      "Epoch 46, Training Loss: 0.7868344785185422\n",
      "Epoch 47, Training Loss: 0.7873807767559501\n",
      "Epoch 48, Training Loss: 0.7877505953872905\n",
      "Epoch 49, Training Loss: 0.7868783597385182\n",
      "Epoch 50, Training Loss: 0.7868313762019662\n",
      "Epoch 51, Training Loss: 0.7860544485905591\n",
      "Epoch 52, Training Loss: 0.7857444444123436\n",
      "Epoch 53, Training Loss: 0.7864575430926155\n",
      "Epoch 54, Training Loss: 0.7863444592672236\n",
      "Epoch 55, Training Loss: 0.7860115584205178\n",
      "Epoch 56, Training Loss: 0.7859881567955017\n",
      "Epoch 57, Training Loss: 0.785563883080202\n",
      "Epoch 58, Training Loss: 0.7857585682588465\n",
      "Epoch 59, Training Loss: 0.7853640104742611\n",
      "Epoch 60, Training Loss: 0.7853959611584158\n",
      "Epoch 61, Training Loss: 0.7855779646424687\n",
      "Epoch 62, Training Loss: 0.7861057371251723\n",
      "Epoch 63, Training Loss: 0.7854065850903006\n",
      "Epoch 64, Training Loss: 0.7858952369409449\n",
      "Epoch 65, Training Loss: 0.7849421863696154\n",
      "Epoch 66, Training Loss: 0.7857592017510358\n",
      "Epoch 67, Training Loss: 0.7846678967335645\n",
      "Epoch 68, Training Loss: 0.7863153780909146\n",
      "Epoch 69, Training Loss: 0.7844561264795415\n",
      "Epoch 70, Training Loss: 0.7853177655444425\n",
      "Epoch 71, Training Loss: 0.7851032506017124\n",
      "Epoch 72, Training Loss: 0.7842258267542895\n",
      "Epoch 73, Training Loss: 0.7843733920069302\n",
      "Epoch 74, Training Loss: 0.7843125857325162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:05:47,570] Trial 153 finished with value: 0.6365333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7842382934514214\n",
      "Epoch 1, Training Loss: 1.0856341761701247\n",
      "Epoch 2, Training Loss: 1.068420833419351\n",
      "Epoch 3, Training Loss: 1.054228412473903\n",
      "Epoch 4, Training Loss: 1.040616731082692\n",
      "Epoch 5, Training Loss: 1.026992117026273\n",
      "Epoch 6, Training Loss: 1.0134506600744584\n",
      "Epoch 7, Training Loss: 1.0005082557481877\n",
      "Epoch 8, Training Loss: 0.9889207510387197\n",
      "Epoch 9, Training Loss: 0.9793490805345423\n",
      "Epoch 10, Training Loss: 0.9719983283912434\n",
      "Epoch 11, Training Loss: 0.9666606769141028\n",
      "Epoch 12, Training Loss: 0.9628745567097383\n",
      "Epoch 13, Training Loss: 0.9601420695641462\n",
      "Epoch 14, Training Loss: 0.958082320059047\n",
      "Epoch 15, Training Loss: 0.9564111404559191\n",
      "Epoch 16, Training Loss: 0.9549668647962458\n",
      "Epoch 17, Training Loss: 0.9536322195389692\n",
      "Epoch 18, Training Loss: 0.952348659880021\n",
      "Epoch 19, Training Loss: 0.9510931081631604\n",
      "Epoch 20, Training Loss: 0.9498436528794906\n",
      "Epoch 21, Training Loss: 0.9485841443959404\n",
      "Epoch 22, Training Loss: 0.9472960889339447\n",
      "Epoch 23, Training Loss: 0.946008081295911\n",
      "Epoch 24, Training Loss: 0.9446799207434935\n",
      "Epoch 25, Training Loss: 0.9433356069817262\n",
      "Epoch 26, Training Loss: 0.9419391766716452\n",
      "Epoch 27, Training Loss: 0.9405165203879862\n",
      "Epoch 28, Training Loss: 0.939052966482499\n",
      "Epoch 29, Training Loss: 0.9375424304429223\n",
      "Epoch 30, Training Loss: 0.9359702562584596\n",
      "Epoch 31, Training Loss: 0.9343528804358314\n",
      "Epoch 32, Training Loss: 0.9326703027416678\n",
      "Epoch 33, Training Loss: 0.9309217445990619\n",
      "Epoch 34, Training Loss: 0.9291071102198433\n",
      "Epoch 35, Training Loss: 0.9271890563123366\n",
      "Epoch 36, Training Loss: 0.9252256497916054\n",
      "Epoch 37, Training Loss: 0.9231400724719553\n",
      "Epoch 38, Training Loss: 0.9209557956807753\n",
      "Epoch 39, Training Loss: 0.9186979285408469\n",
      "Epoch 40, Training Loss: 0.9162997024900773\n",
      "Epoch 41, Training Loss: 0.9138020251778995\n",
      "Epoch 42, Training Loss: 0.9111628744882696\n",
      "Epoch 43, Training Loss: 0.9084478162316715\n",
      "Epoch 44, Training Loss: 0.9055762023084304\n",
      "Epoch 45, Training Loss: 0.9025671055737664\n",
      "Epoch 46, Training Loss: 0.8994614229482762\n",
      "Epoch 47, Training Loss: 0.896216697622748\n",
      "Epoch 48, Training Loss: 0.8928680622577667\n",
      "Epoch 49, Training Loss: 0.8894457576555365\n",
      "Epoch 50, Training Loss: 0.8859104587751276\n",
      "Epoch 51, Training Loss: 0.8823351737330941\n",
      "Epoch 52, Training Loss: 0.8787126804800595\n",
      "Epoch 53, Training Loss: 0.8750709918667289\n",
      "Epoch 54, Training Loss: 0.8714475325977101\n",
      "Epoch 55, Training Loss: 0.8678488055397483\n",
      "Epoch 56, Training Loss: 0.8643274395606098\n",
      "Epoch 57, Training Loss: 0.8609422160597409\n",
      "Epoch 58, Training Loss: 0.8576031985703637\n",
      "Epoch 59, Training Loss: 0.8544276914175819\n",
      "Epoch 60, Training Loss: 0.8514213485577528\n",
      "Epoch 61, Training Loss: 0.8485763030192431\n",
      "Epoch 62, Training Loss: 0.8458670076201944\n",
      "Epoch 63, Training Loss: 0.8434089806500603\n",
      "Epoch 64, Training Loss: 0.8410560900323532\n",
      "Epoch 65, Training Loss: 0.8389078925637637\n",
      "Epoch 66, Training Loss: 0.836959227884517\n",
      "Epoch 67, Training Loss: 0.8351336548608892\n",
      "Epoch 68, Training Loss: 0.8334812953191645\n",
      "Epoch 69, Training Loss: 0.8319404266862308\n",
      "Epoch 70, Training Loss: 0.8305649476892808\n",
      "Epoch 71, Training Loss: 0.8292698372812832\n",
      "Epoch 72, Training Loss: 0.8280595088005066\n",
      "Epoch 73, Training Loss: 0.8270588456181919\n",
      "Epoch 74, Training Loss: 0.825976980153252\n",
      "Epoch 75, Training Loss: 0.8250869286761564\n",
      "Epoch 76, Training Loss: 0.8241827660448411\n",
      "Epoch 77, Training Loss: 0.8233944951085483\n",
      "Epoch 78, Training Loss: 0.8225590010951547\n",
      "Epoch 79, Training Loss: 0.8219174860505497\n",
      "Epoch 80, Training Loss: 0.8212230831034043\n",
      "Epoch 81, Training Loss: 0.8206098348954145\n",
      "Epoch 82, Training Loss: 0.8200078808560091\n",
      "Epoch 83, Training Loss: 0.8194474241312812\n",
      "Epoch 84, Training Loss: 0.8188933012765996\n",
      "Epoch 85, Training Loss: 0.8183829877657048\n",
      "Epoch 86, Training Loss: 0.8178973690201254\n",
      "Epoch 87, Training Loss: 0.8173801754502689\n",
      "Epoch 88, Training Loss: 0.8169470072493834\n",
      "Epoch 89, Training Loss: 0.8165041853399838\n",
      "Epoch 90, Training Loss: 0.8160221181196325\n",
      "Epoch 91, Training Loss: 0.8156360084870282\n",
      "Epoch 92, Training Loss: 0.8153216887922848\n",
      "Epoch 93, Training Loss: 0.814942145207349\n",
      "Epoch 94, Training Loss: 0.8145660475422354\n",
      "Epoch 95, Training Loss: 0.814204369432786\n",
      "Epoch 96, Training Loss: 0.8138109396485721\n",
      "Epoch 97, Training Loss: 0.8135025191307068\n",
      "Epoch 98, Training Loss: 0.8131794536113739\n",
      "Epoch 99, Training Loss: 0.8128522516699398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:07:45,999] Trial 154 finished with value: 0.6279333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8125592861456029\n",
      "Epoch 1, Training Loss: 0.8803726128269644\n",
      "Epoch 2, Training Loss: 0.8175093341574949\n",
      "Epoch 3, Training Loss: 0.8107386905305526\n",
      "Epoch 4, Training Loss: 0.8058817077384276\n",
      "Epoch 5, Training Loss: 0.8026153722230126\n",
      "Epoch 6, Training Loss: 0.8006711759987999\n",
      "Epoch 7, Training Loss: 0.7987659984476426\n",
      "Epoch 8, Training Loss: 0.7977214241027832\n",
      "Epoch 9, Training Loss: 0.7963982393461115\n",
      "Epoch 10, Training Loss: 0.7970627311398001\n",
      "Epoch 11, Training Loss: 0.7957881569862366\n",
      "Epoch 12, Training Loss: 0.7953861600511214\n",
      "Epoch 13, Training Loss: 0.7937712656750399\n",
      "Epoch 14, Training Loss: 0.794087941716699\n",
      "Epoch 15, Training Loss: 0.7942599263611961\n",
      "Epoch 16, Training Loss: 0.7927662659392637\n",
      "Epoch 17, Training Loss: 0.7934901686976937\n",
      "Epoch 18, Training Loss: 0.7925251490929548\n",
      "Epoch 19, Training Loss: 0.7923491110521205\n",
      "Epoch 20, Training Loss: 0.7923881452925065\n",
      "Epoch 21, Training Loss: 0.7922164745190564\n",
      "Epoch 22, Training Loss: 0.7910611302011153\n",
      "Epoch 23, Training Loss: 0.7905740972126232\n",
      "Epoch 24, Training Loss: 0.7905767727599424\n",
      "Epoch 25, Training Loss: 0.7901375128942377\n",
      "Epoch 26, Training Loss: 0.7900612617240232\n",
      "Epoch 27, Training Loss: 0.7898962413563447\n",
      "Epoch 28, Training Loss: 0.7898282088251675\n",
      "Epoch 29, Training Loss: 0.7904145533898298\n",
      "Epoch 30, Training Loss: 0.7884276751910939\n",
      "Epoch 31, Training Loss: 0.7886566941878375\n",
      "Epoch 32, Training Loss: 0.789068965771619\n",
      "Epoch 33, Training Loss: 0.7889889704479891\n",
      "Epoch 34, Training Loss: 0.7874697566032409\n",
      "Epoch 35, Training Loss: 0.7888450410085566\n",
      "Epoch 36, Training Loss: 0.7879746504390941\n",
      "Epoch 37, Training Loss: 0.7880383688562057\n",
      "Epoch 38, Training Loss: 0.7887461721897125\n",
      "Epoch 39, Training Loss: 0.788111888661104\n",
      "Epoch 40, Training Loss: 0.7883309014404521\n",
      "Epoch 41, Training Loss: 0.7876222808220807\n",
      "Epoch 42, Training Loss: 0.7880514468866237\n",
      "Epoch 43, Training Loss: 0.7875761354670805\n",
      "Epoch 44, Training Loss: 0.7867881942496581\n",
      "Epoch 45, Training Loss: 0.7872477187829859\n",
      "Epoch 46, Training Loss: 0.7867778184133417\n",
      "Epoch 47, Training Loss: 0.7876604528286878\n",
      "Epoch 48, Training Loss: 0.7879499735551722\n",
      "Epoch 49, Training Loss: 0.7865417118633494\n",
      "Epoch 50, Training Loss: 0.7859241404954125\n",
      "Epoch 51, Training Loss: 0.7871863637251012\n",
      "Epoch 52, Training Loss: 0.7864410898264717\n",
      "Epoch 53, Training Loss: 0.7855220952454736\n",
      "Epoch 54, Training Loss: 0.7862983732363757\n",
      "Epoch 55, Training Loss: 0.7861915833809796\n",
      "Epoch 56, Training Loss: 0.785646275211783\n",
      "Epoch 57, Training Loss: 0.7844655623856713\n",
      "Epoch 58, Training Loss: 0.7860448093975292\n",
      "Epoch 59, Training Loss: 0.7860916298978469\n",
      "Epoch 60, Training Loss: 0.7849345843932208\n",
      "Epoch 61, Training Loss: 0.7862569874174454\n",
      "Epoch 62, Training Loss: 0.7859359897585476\n",
      "Epoch 63, Training Loss: 0.7858119031260995\n",
      "Epoch 64, Training Loss: 0.7851007306575775\n",
      "Epoch 65, Training Loss: 0.7844354304846596\n",
      "Epoch 66, Training Loss: 0.7842389817097608\n",
      "Epoch 67, Training Loss: 0.7848303102044498\n",
      "Epoch 68, Training Loss: 0.7852067110818975\n",
      "Epoch 69, Training Loss: 0.7849605517527637\n",
      "Epoch 70, Training Loss: 0.7847311618047602\n",
      "Epoch 71, Training Loss: 0.7849345830608817\n",
      "Epoch 72, Training Loss: 0.785092139524572\n",
      "Epoch 73, Training Loss: 0.7849390303387361\n",
      "Epoch 74, Training Loss: 0.7845920383930206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:09:37,047] Trial 155 finished with value: 0.6362666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7843802384067984\n",
      "Epoch 1, Training Loss: 0.8527812177077272\n",
      "Epoch 2, Training Loss: 0.8209294030540868\n",
      "Epoch 3, Training Loss: 0.8165706297508756\n",
      "Epoch 4, Training Loss: 0.814146505979667\n",
      "Epoch 5, Training Loss: 0.8129269976364939\n",
      "Epoch 6, Training Loss: 0.8109188523507656\n",
      "Epoch 7, Training Loss: 0.8086696628341101\n",
      "Epoch 8, Training Loss: 0.8089172561365859\n",
      "Epoch 9, Training Loss: 0.8098827620197956\n",
      "Epoch 10, Training Loss: 0.8067309936186424\n",
      "Epoch 11, Training Loss: 0.807021791415107\n",
      "Epoch 12, Training Loss: 0.807008385389371\n",
      "Epoch 13, Training Loss: 0.8054679225261946\n",
      "Epoch 14, Training Loss: 0.8066844367443171\n",
      "Epoch 15, Training Loss: 0.8041555506842477\n",
      "Epoch 16, Training Loss: 0.8052211716659087\n",
      "Epoch 17, Training Loss: 0.8056479789260634\n",
      "Epoch 18, Training Loss: 0.8052029840031961\n",
      "Epoch 19, Training Loss: 0.803822522324727\n",
      "Epoch 20, Training Loss: 0.8060041975257988\n",
      "Epoch 21, Training Loss: 0.8039034357644561\n",
      "Epoch 22, Training Loss: 0.8046492026264506\n",
      "Epoch 23, Training Loss: 0.8038584473437833\n",
      "Epoch 24, Training Loss: 0.8050235072472938\n",
      "Epoch 25, Training Loss: 0.8033093344896359\n",
      "Epoch 26, Training Loss: 0.804423659636562\n",
      "Epoch 27, Training Loss: 0.8040661768805711\n",
      "Epoch 28, Training Loss: 0.8037238801332345\n",
      "Epoch 29, Training Loss: 0.8034881296014427\n",
      "Epoch 30, Training Loss: 0.8030994225265389\n",
      "Epoch 31, Training Loss: 0.8047710456346211\n",
      "Epoch 32, Training Loss: 0.8032523825652618\n",
      "Epoch 33, Training Loss: 0.8026401861269671\n",
      "Epoch 34, Training Loss: 0.8035119878618341\n",
      "Epoch 35, Training Loss: 0.8022452817823654\n",
      "Epoch 36, Training Loss: 0.8024699717536008\n",
      "Epoch 37, Training Loss: 0.8026226567146474\n",
      "Epoch 38, Training Loss: 0.80213127611275\n",
      "Epoch 39, Training Loss: 0.802254965699705\n",
      "Epoch 40, Training Loss: 0.8022360072996383\n",
      "Epoch 41, Training Loss: 0.8028386655606722\n",
      "Epoch 42, Training Loss: 0.8015002913941118\n",
      "Epoch 43, Training Loss: 0.802102554202976\n",
      "Epoch 44, Training Loss: 0.801289643291244\n",
      "Epoch 45, Training Loss: 0.8020929661908545\n",
      "Epoch 46, Training Loss: 0.8007125432329967\n",
      "Epoch 47, Training Loss: 0.8020512220554782\n",
      "Epoch 48, Training Loss: 0.8002661600148767\n",
      "Epoch 49, Training Loss: 0.7991156484847678\n",
      "Epoch 50, Training Loss: 0.7993530392646789\n",
      "Epoch 51, Training Loss: 0.8003332259959745\n",
      "Epoch 52, Training Loss: 0.7995916720619776\n",
      "Epoch 53, Training Loss: 0.7990144096819082\n",
      "Epoch 54, Training Loss: 0.7989058492775251\n",
      "Epoch 55, Training Loss: 0.7998967484423989\n",
      "Epoch 56, Training Loss: 0.7997770355160075\n",
      "Epoch 57, Training Loss: 0.7985126986539454\n",
      "Epoch 58, Training Loss: 0.7988799696578119\n",
      "Epoch 59, Training Loss: 0.7987012743949891\n",
      "Epoch 60, Training Loss: 0.7989857417300231\n",
      "Epoch 61, Training Loss: 0.7978307340826307\n",
      "Epoch 62, Training Loss: 0.7975200679069175\n",
      "Epoch 63, Training Loss: 0.7995195248073205\n",
      "Epoch 64, Training Loss: 0.7974057219978562\n",
      "Epoch 65, Training Loss: 0.7969346012836113\n",
      "Epoch 66, Training Loss: 0.7982162111683896\n",
      "Epoch 67, Training Loss: 0.7979672977798864\n",
      "Epoch 68, Training Loss: 0.7977561714057636\n",
      "Epoch 69, Training Loss: 0.7992741144689403\n",
      "Epoch 70, Training Loss: 0.7973881789616176\n",
      "Epoch 71, Training Loss: 0.7972299213696243\n",
      "Epoch 72, Training Loss: 0.7979334475402545\n",
      "Epoch 73, Training Loss: 0.7974129955571397\n",
      "Epoch 74, Training Loss: 0.7976962729504234\n",
      "Epoch 75, Training Loss: 0.7971998085652975\n",
      "Epoch 76, Training Loss: 0.7963953418839247\n",
      "Epoch 77, Training Loss: 0.7966028897385848\n",
      "Epoch 78, Training Loss: 0.7971798520339163\n",
      "Epoch 79, Training Loss: 0.7972004943324211\n",
      "Epoch 80, Training Loss: 0.7975244450389891\n",
      "Epoch 81, Training Loss: 0.7960651309866654\n",
      "Epoch 82, Training Loss: 0.7961308522331983\n",
      "Epoch 83, Training Loss: 0.7951040892672718\n",
      "Epoch 84, Training Loss: 0.7964805457825052\n",
      "Epoch 85, Training Loss: 0.7974126688519815\n",
      "Epoch 86, Training Loss: 0.7956540198254406\n",
      "Epoch 87, Training Loss: 0.7960799051406688\n",
      "Epoch 88, Training Loss: 0.7953430351458097\n",
      "Epoch 89, Training Loss: 0.795616296717995\n",
      "Epoch 90, Training Loss: 0.7956186621708977\n",
      "Epoch 91, Training Loss: 0.7953991387123452\n",
      "Epoch 92, Training Loss: 0.7945284448171916\n",
      "Epoch 93, Training Loss: 0.7954458933127554\n",
      "Epoch 94, Training Loss: 0.795329321147804\n",
      "Epoch 95, Training Loss: 0.7955471296059458\n",
      "Epoch 96, Training Loss: 0.7942879630210704\n",
      "Epoch 97, Training Loss: 0.7942880758665558\n",
      "Epoch 98, Training Loss: 0.7951765354414632\n",
      "Epoch 99, Training Loss: 0.7948363984437813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:11:16,334] Trial 156 finished with value: 0.6355333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7940529134040488\n",
      "Epoch 1, Training Loss: 0.9838226025946\n",
      "Epoch 2, Training Loss: 0.946891571633956\n",
      "Epoch 3, Training Loss: 0.93557573984651\n",
      "Epoch 4, Training Loss: 0.922271711545832\n",
      "Epoch 5, Training Loss: 0.9058152606908013\n",
      "Epoch 6, Training Loss: 0.886116343386033\n",
      "Epoch 7, Training Loss: 0.8647849837471457\n",
      "Epoch 8, Training Loss: 0.8456851933984195\n",
      "Epoch 9, Training Loss: 0.8314908750618205\n",
      "Epoch 10, Training Loss: 0.8224192657891441\n",
      "Epoch 11, Training Loss: 0.8170600698975956\n",
      "Epoch 12, Training Loss: 0.8138099519645466\n",
      "Epoch 13, Training Loss: 0.8117831427910749\n",
      "Epoch 14, Training Loss: 0.8102448752347161\n",
      "Epoch 15, Training Loss: 0.8094403467458837\n",
      "Epoch 16, Training Loss: 0.8085181007665746\n",
      "Epoch 17, Training Loss: 0.8078215390093186\n",
      "Epoch 18, Training Loss: 0.8073540241578046\n",
      "Epoch 19, Training Loss: 0.8067061875848209\n",
      "Epoch 20, Training Loss: 0.8059495376839357\n",
      "Epoch 21, Training Loss: 0.8056054760428036\n",
      "Epoch 22, Training Loss: 0.8049939951475928\n",
      "Epoch 23, Training Loss: 0.8045344208268558\n",
      "Epoch 24, Training Loss: 0.80423885233262\n",
      "Epoch 25, Training Loss: 0.8037626272089341\n",
      "Epoch 26, Training Loss: 0.8029769306323108\n",
      "Epoch 27, Training Loss: 0.8029177040212294\n",
      "Epoch 28, Training Loss: 0.802706807010314\n",
      "Epoch 29, Training Loss: 0.8020379256500917\n",
      "Epoch 30, Training Loss: 0.8016898830497966\n",
      "Epoch 31, Training Loss: 0.8017084023531745\n",
      "Epoch 32, Training Loss: 0.8013854298170875\n",
      "Epoch 33, Training Loss: 0.8011888513144325\n",
      "Epoch 34, Training Loss: 0.8008209973924301\n",
      "Epoch 35, Training Loss: 0.8006297476852642\n",
      "Epoch 36, Training Loss: 0.8005210975338432\n",
      "Epoch 37, Training Loss: 0.8001641523838043\n",
      "Epoch 38, Training Loss: 0.8000264937036178\n",
      "Epoch 39, Training Loss: 0.7998961144335129\n",
      "Epoch 40, Training Loss: 0.7997579899254967\n",
      "Epoch 41, Training Loss: 0.799392957687378\n",
      "Epoch 42, Training Loss: 0.7994118719942429\n",
      "Epoch 43, Training Loss: 0.7995464520594653\n",
      "Epoch 44, Training Loss: 0.7989954756989198\n",
      "Epoch 45, Training Loss: 0.7990331670115975\n",
      "Epoch 46, Training Loss: 0.7991631319242365\n",
      "Epoch 47, Training Loss: 0.798959030193441\n",
      "Epoch 48, Training Loss: 0.7989543538935044\n",
      "Epoch 49, Training Loss: 0.7988068356934716\n",
      "Epoch 50, Training Loss: 0.7986153884495006\n",
      "Epoch 51, Training Loss: 0.7986318726399365\n",
      "Epoch 52, Training Loss: 0.7984946821016424\n",
      "Epoch 53, Training Loss: 0.7984126161126529\n",
      "Epoch 54, Training Loss: 0.7984738971205319\n",
      "Epoch 55, Training Loss: 0.7982707443658044\n",
      "Epoch 56, Training Loss: 0.7983062459440793\n",
      "Epoch 57, Training Loss: 0.7979904312947217\n",
      "Epoch 58, Training Loss: 0.7979756353883183\n",
      "Epoch 59, Training Loss: 0.7981821348386652\n",
      "Epoch 60, Training Loss: 0.7979937857038835\n",
      "Epoch 61, Training Loss: 0.7978756553986494\n",
      "Epoch 62, Training Loss: 0.7978970489782445\n",
      "Epoch 63, Training Loss: 0.7977221047177034\n",
      "Epoch 64, Training Loss: 0.7978794109120089\n",
      "Epoch 65, Training Loss: 0.7977018261657042\n",
      "Epoch 66, Training Loss: 0.7976530896916109\n",
      "Epoch 67, Training Loss: 0.797482371821123\n",
      "Epoch 68, Training Loss: 0.7978258506690754\n",
      "Epoch 69, Training Loss: 0.797596111017115\n",
      "Epoch 70, Training Loss: 0.7975995408787447\n",
      "Epoch 71, Training Loss: 0.7973930921975304\n",
      "Epoch 72, Training Loss: 0.7973614227771759\n",
      "Epoch 73, Training Loss: 0.7974469906442305\n",
      "Epoch 74, Training Loss: 0.7975731806194081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:12:38,779] Trial 157 finished with value: 0.6351333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7974294402318842\n",
      "Epoch 1, Training Loss: 1.0803328399097218\n",
      "Epoch 2, Training Loss: 1.0530191866089316\n",
      "Epoch 3, Training Loss: 1.0309180591386908\n",
      "Epoch 4, Training Loss: 1.0110946255571702\n",
      "Epoch 5, Training Loss: 0.9942312594021068\n",
      "Epoch 6, Training Loss: 0.9814294005141538\n",
      "Epoch 7, Training Loss: 0.9726945604296292\n",
      "Epoch 8, Training Loss: 0.9670788104393903\n",
      "Epoch 9, Training Loss: 0.9634872000357684\n",
      "Epoch 10, Training Loss: 0.9610842988771551\n",
      "Epoch 11, Training Loss: 0.9593274687318241\n",
      "Epoch 12, Training Loss: 0.957927560525782\n",
      "Epoch 13, Training Loss: 0.9567194325783673\n",
      "Epoch 14, Training Loss: 0.9556104994521422\n",
      "Epoch 15, Training Loss: 0.9545490875664879\n",
      "Epoch 16, Training Loss: 0.9535284258337582\n",
      "Epoch 17, Training Loss: 0.9525232369759503\n",
      "Epoch 18, Training Loss: 0.951517647154191\n",
      "Epoch 19, Training Loss: 0.9505156179736642\n",
      "Epoch 20, Training Loss: 0.9495021654577817\n",
      "Epoch 21, Training Loss: 0.9484710625339957\n",
      "Epoch 22, Training Loss: 0.9474643614011652\n",
      "Epoch 23, Training Loss: 0.9464135725357953\n",
      "Epoch 24, Training Loss: 0.9453512988371008\n",
      "Epoch 25, Training Loss: 0.9442702325652628\n",
      "Epoch 26, Training Loss: 0.9431703573815963\n",
      "Epoch 27, Training Loss: 0.9420374512672425\n",
      "Epoch 28, Training Loss: 0.9408944177627564\n",
      "Epoch 29, Training Loss: 0.9397082638740539\n",
      "Epoch 30, Training Loss: 0.9384950056496788\n",
      "Epoch 31, Training Loss: 0.9372541690573973\n",
      "Epoch 32, Training Loss: 0.935967486115063\n",
      "Epoch 33, Training Loss: 0.9346353228653178\n",
      "Epoch 34, Training Loss: 0.9332694370606366\n",
      "Epoch 35, Training Loss: 0.9318369398397558\n",
      "Epoch 36, Training Loss: 0.9303728182876811\n",
      "Epoch 37, Training Loss: 0.9288594365821166\n",
      "Epoch 38, Training Loss: 0.9272753010076635\n",
      "Epoch 39, Training Loss: 0.9256220937476438\n",
      "Epoch 40, Training Loss: 0.9238958358063417\n",
      "Epoch 41, Training Loss: 0.9221240653711207\n",
      "Epoch 42, Training Loss: 0.9202491494487314\n",
      "Epoch 43, Training Loss: 0.9182987869487089\n",
      "Epoch 44, Training Loss: 0.9162503399568446\n",
      "Epoch 45, Training Loss: 0.9141293844755958\n",
      "Epoch 46, Training Loss: 0.9118750255949357\n",
      "Epoch 47, Training Loss: 0.9095213669889114\n",
      "Epoch 48, Training Loss: 0.9070715705086203\n",
      "Epoch 49, Training Loss: 0.9044987986368291\n",
      "Epoch 50, Training Loss: 0.9018124215743121\n",
      "Epoch 51, Training Loss: 0.8989911014893476\n",
      "Epoch 52, Training Loss: 0.896071790526895\n",
      "Epoch 53, Training Loss: 0.893029280760709\n",
      "Epoch 54, Training Loss: 0.8898801662641413\n",
      "Epoch 55, Training Loss: 0.8865942095307743\n",
      "Epoch 56, Training Loss: 0.8832268984878764\n",
      "Epoch 57, Training Loss: 0.8797633316236384\n",
      "Epoch 58, Training Loss: 0.8762865013935986\n",
      "Epoch 59, Training Loss: 0.8727144830367144\n",
      "Epoch 60, Training Loss: 0.8691357354556813\n",
      "Epoch 61, Training Loss: 0.8655267221787396\n",
      "Epoch 62, Training Loss: 0.8619854312784532\n",
      "Epoch 63, Training Loss: 0.8584903379748849\n",
      "Epoch 64, Training Loss: 0.8550248514203465\n",
      "Epoch 65, Training Loss: 0.8517288163830252\n",
      "Epoch 66, Training Loss: 0.8485270716162289\n",
      "Epoch 67, Training Loss: 0.8454656226494733\n",
      "Epoch 68, Training Loss: 0.8426114388073191\n",
      "Epoch 69, Training Loss: 0.8399120399531196\n",
      "Epoch 70, Training Loss: 0.8373455084772671\n",
      "Epoch 71, Training Loss: 0.8350097362434162\n",
      "Epoch 72, Training Loss: 0.8328861852954416\n",
      "Epoch 73, Training Loss: 0.8308500984837027\n",
      "Epoch 74, Training Loss: 0.8290845826092889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:14:10,661] Trial 158 finished with value: 0.6174666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.827406002072727\n",
      "Epoch 1, Training Loss: 0.9285826851339901\n",
      "Epoch 2, Training Loss: 0.8886944239279803\n",
      "Epoch 3, Training Loss: 0.8523344675933614\n",
      "Epoch 4, Training Loss: 0.8258548742182115\n",
      "Epoch 5, Training Loss: 0.8130187293361215\n",
      "Epoch 6, Training Loss: 0.8071720579792472\n",
      "Epoch 7, Training Loss: 0.8048687206997591\n",
      "Epoch 8, Training Loss: 0.8031748106900384\n",
      "Epoch 9, Training Loss: 0.802780371203142\n",
      "Epoch 10, Training Loss: 0.801825520361171\n",
      "Epoch 11, Training Loss: 0.8015027549687553\n",
      "Epoch 12, Training Loss: 0.8011541849024155\n",
      "Epoch 13, Training Loss: 0.8009293932774487\n",
      "Epoch 14, Training Loss: 0.8004424864404341\n",
      "Epoch 15, Training Loss: 0.8002761041416842\n",
      "Epoch 16, Training Loss: 0.7999290181608761\n",
      "Epoch 17, Training Loss: 0.7997055049503551\n",
      "Epoch 18, Training Loss: 0.7997294480660383\n",
      "Epoch 19, Training Loss: 0.7993674910769744\n",
      "Epoch 20, Training Loss: 0.7990172881238601\n",
      "Epoch 21, Training Loss: 0.7991347964370952\n",
      "Epoch 22, Training Loss: 0.7987472078379463\n",
      "Epoch 23, Training Loss: 0.7986549091339111\n",
      "Epoch 24, Training Loss: 0.7983870755924898\n",
      "Epoch 25, Training Loss: 0.7982289765862858\n",
      "Epoch 26, Training Loss: 0.7982146481205435\n",
      "Epoch 27, Training Loss: 0.7980410223147448\n",
      "Epoch 28, Training Loss: 0.7981642235727872\n",
      "Epoch 29, Training Loss: 0.7977488223244162\n",
      "Epoch 30, Training Loss: 0.7979375592400046\n",
      "Epoch 31, Training Loss: 0.7978614340108984\n",
      "Epoch 32, Training Loss: 0.7977430807141697\n",
      "Epoch 33, Training Loss: 0.7975948980275323\n",
      "Epoch 34, Training Loss: 0.7975771683103898\n",
      "Epoch 35, Training Loss: 0.7973564924913294\n",
      "Epoch 36, Training Loss: 0.7973879333804635\n",
      "Epoch 37, Training Loss: 0.7973259856420405\n",
      "Epoch 38, Training Loss: 0.7971608380009146\n",
      "Epoch 39, Training Loss: 0.7972091708463781\n",
      "Epoch 40, Training Loss: 0.7969690802518059\n",
      "Epoch 41, Training Loss: 0.7967967424673192\n",
      "Epoch 42, Training Loss: 0.7969343859307906\n",
      "Epoch 43, Training Loss: 0.7970812535285949\n",
      "Epoch 44, Training Loss: 0.7968608441773583\n",
      "Epoch 45, Training Loss: 0.7966494983785293\n",
      "Epoch 46, Training Loss: 0.7966466917009914\n",
      "Epoch 47, Training Loss: 0.7965915497611551\n",
      "Epoch 48, Training Loss: 0.7964295479129343\n",
      "Epoch 49, Training Loss: 0.7963930186103372\n",
      "Epoch 50, Training Loss: 0.7960827466319589\n",
      "Epoch 51, Training Loss: 0.7960553341052111\n",
      "Epoch 52, Training Loss: 0.7962402117953581\n",
      "Epoch 53, Training Loss: 0.7961531353698057\n",
      "Epoch 54, Training Loss: 0.7960064231648165\n",
      "Epoch 55, Training Loss: 0.7958565051415387\n",
      "Epoch 56, Training Loss: 0.7959403622150422\n",
      "Epoch 57, Training Loss: 0.7952187129329232\n",
      "Epoch 58, Training Loss: 0.7954403363255893\n",
      "Epoch 59, Training Loss: 0.7949355004114264\n",
      "Epoch 60, Training Loss: 0.7945921343214372\n",
      "Epoch 61, Training Loss: 0.7945362070728751\n",
      "Epoch 62, Training Loss: 0.7940846631106209\n",
      "Epoch 63, Training Loss: 0.7939135936428519\n",
      "Epoch 64, Training Loss: 0.7935537904851577\n",
      "Epoch 65, Training Loss: 0.7937593459381777\n",
      "Epoch 66, Training Loss: 0.793305953951443\n",
      "Epoch 67, Training Loss: 0.7930546471651863\n",
      "Epoch 68, Training Loss: 0.7930643957502702\n",
      "Epoch 69, Training Loss: 0.7926947003953597\n",
      "Epoch 70, Training Loss: 0.7924615364916184\n",
      "Epoch 71, Training Loss: 0.7921983266578001\n",
      "Epoch 72, Training Loss: 0.7920924194420085\n",
      "Epoch 73, Training Loss: 0.7919939832827624\n",
      "Epoch 74, Training Loss: 0.7918894518122953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:15:34,154] Trial 159 finished with value: 0.6362666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7914180259844836\n",
      "Epoch 1, Training Loss: 1.0597703949849409\n",
      "Epoch 2, Training Loss: 0.9446666401131709\n",
      "Epoch 3, Training Loss: 0.9147871925418538\n",
      "Epoch 4, Training Loss: 0.8878392676661785\n",
      "Epoch 5, Training Loss: 0.8424652660699715\n",
      "Epoch 6, Training Loss: 0.8167133684445145\n",
      "Epoch 7, Training Loss: 0.81078329453791\n",
      "Epoch 8, Training Loss: 0.8080303773844153\n",
      "Epoch 9, Training Loss: 0.8072041527669233\n",
      "Epoch 10, Training Loss: 0.8059941530227661\n",
      "Epoch 11, Training Loss: 0.8050381856753414\n",
      "Epoch 12, Training Loss: 0.8032862045711144\n",
      "Epoch 13, Training Loss: 0.8029877465470393\n",
      "Epoch 14, Training Loss: 0.8016524942297685\n",
      "Epoch 15, Training Loss: 0.8006537191849902\n",
      "Epoch 16, Training Loss: 0.7999909845509924\n",
      "Epoch 17, Training Loss: 0.7995519878272723\n",
      "Epoch 18, Training Loss: 0.7986959973224124\n",
      "Epoch 19, Training Loss: 0.7968147822788784\n",
      "Epoch 20, Training Loss: 0.7964291266928938\n",
      "Epoch 21, Training Loss: 0.7959718926060468\n",
      "Epoch 22, Training Loss: 0.7963199193316295\n",
      "Epoch 23, Training Loss: 0.7958545667784555\n",
      "Epoch 24, Training Loss: 0.7937288341217471\n",
      "Epoch 25, Training Loss: 0.7946506176676069\n",
      "Epoch 26, Training Loss: 0.7938551927867689\n",
      "Epoch 27, Training Loss: 0.7932380765900576\n",
      "Epoch 28, Training Loss: 0.7931337903316756\n",
      "Epoch 29, Training Loss: 0.7921298664315303\n",
      "Epoch 30, Training Loss: 0.7914733941393687\n",
      "Epoch 31, Training Loss: 0.7908321534332476\n",
      "Epoch 32, Training Loss: 0.7906814783139336\n",
      "Epoch 33, Training Loss: 0.790860817038027\n",
      "Epoch 34, Training Loss: 0.7903642841747829\n",
      "Epoch 35, Training Loss: 0.7895644628015676\n",
      "Epoch 36, Training Loss: 0.7895673825328512\n",
      "Epoch 37, Training Loss: 0.7893786017159771\n",
      "Epoch 38, Training Loss: 0.7888516519302712\n",
      "Epoch 39, Training Loss: 0.7880610896232433\n",
      "Epoch 40, Training Loss: 0.7884638215366163\n",
      "Epoch 41, Training Loss: 0.7879790359869936\n",
      "Epoch 42, Training Loss: 0.7880079600147735\n",
      "Epoch 43, Training Loss: 0.7867348234456285\n",
      "Epoch 44, Training Loss: 0.7868514884683423\n",
      "Epoch 45, Training Loss: 0.7863909368228196\n",
      "Epoch 46, Training Loss: 0.7858352273926699\n",
      "Epoch 47, Training Loss: 0.7862651507657273\n",
      "Epoch 48, Training Loss: 0.7857264493641101\n",
      "Epoch 49, Training Loss: 0.78530407355244\n",
      "Epoch 50, Training Loss: 0.7854617786586733\n",
      "Epoch 51, Training Loss: 0.7848512800116288\n",
      "Epoch 52, Training Loss: 0.7843483419794786\n",
      "Epoch 53, Training Loss: 0.7846641681248084\n",
      "Epoch 54, Training Loss: 0.7851184551877187\n",
      "Epoch 55, Training Loss: 0.7844306189314764\n",
      "Epoch 56, Training Loss: 0.7843785933085851\n",
      "Epoch 57, Training Loss: 0.7838121708174397\n",
      "Epoch 58, Training Loss: 0.7838470225495503\n",
      "Epoch 59, Training Loss: 0.7838089832685944\n",
      "Epoch 60, Training Loss: 0.784008238727885\n",
      "Epoch 61, Training Loss: 0.7841318259561868\n",
      "Epoch 62, Training Loss: 0.7834757766329257\n",
      "Epoch 63, Training Loss: 0.783567410124872\n",
      "Epoch 64, Training Loss: 0.7833726188293973\n",
      "Epoch 65, Training Loss: 0.7832961805781028\n",
      "Epoch 66, Training Loss: 0.782629872802505\n",
      "Epoch 67, Training Loss: 0.7830325319354695\n",
      "Epoch 68, Training Loss: 0.7832590557578811\n",
      "Epoch 69, Training Loss: 0.7827534438972187\n",
      "Epoch 70, Training Loss: 0.7827646986882489\n",
      "Epoch 71, Training Loss: 0.7830095610224215\n",
      "Epoch 72, Training Loss: 0.7820053280744338\n",
      "Epoch 73, Training Loss: 0.7821998156999287\n",
      "Epoch 74, Training Loss: 0.7827044225276861\n",
      "Epoch 75, Training Loss: 0.7818866416027671\n",
      "Epoch 76, Training Loss: 0.7818629802617811\n",
      "Epoch 77, Training Loss: 0.7815943226778418\n",
      "Epoch 78, Training Loss: 0.7809927934094479\n",
      "Epoch 79, Training Loss: 0.7808837181643435\n",
      "Epoch 80, Training Loss: 0.7815556326306852\n",
      "Epoch 81, Training Loss: 0.7818915013083838\n",
      "Epoch 82, Training Loss: 0.7816117482077807\n",
      "Epoch 83, Training Loss: 0.7816936460652746\n",
      "Epoch 84, Training Loss: 0.7808919071254874\n",
      "Epoch 85, Training Loss: 0.7808099352327504\n",
      "Epoch 86, Training Loss: 0.7817937449405068\n",
      "Epoch 87, Training Loss: 0.7813933308859516\n",
      "Epoch 88, Training Loss: 0.7812730559729095\n",
      "Epoch 89, Training Loss: 0.78033961617857\n",
      "Epoch 90, Training Loss: 0.7815462351741648\n",
      "Epoch 91, Training Loss: 0.7813124559875718\n",
      "Epoch 92, Training Loss: 0.7809750559634732\n",
      "Epoch 93, Training Loss: 0.7808492970645876\n",
      "Epoch 94, Training Loss: 0.7806187127765857\n",
      "Epoch 95, Training Loss: 0.7806717089244297\n",
      "Epoch 96, Training Loss: 0.7804567926808408\n",
      "Epoch 97, Training Loss: 0.7810155879285999\n",
      "Epoch 98, Training Loss: 0.7805313101388458\n",
      "Epoch 99, Training Loss: 0.7808818044519066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:17:18,083] Trial 160 finished with value: 0.6352 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.780218505949006\n",
      "Epoch 1, Training Loss: 0.8918544898313635\n",
      "Epoch 2, Training Loss: 0.8282820463180542\n",
      "Epoch 3, Training Loss: 0.8199033356414122\n",
      "Epoch 4, Training Loss: 0.8157450158455792\n",
      "Epoch 5, Training Loss: 0.8137854836267584\n",
      "Epoch 6, Training Loss: 0.8127163647202884\n",
      "Epoch 7, Training Loss: 0.8107100213976467\n",
      "Epoch 8, Training Loss: 0.8096539925126468\n",
      "Epoch 9, Training Loss: 0.8080066859020907\n",
      "Epoch 10, Training Loss: 0.8079289096944472\n",
      "Epoch 11, Training Loss: 0.8070599409411935\n",
      "Epoch 12, Training Loss: 0.8069006568544052\n",
      "Epoch 13, Training Loss: 0.8064008675603306\n",
      "Epoch 14, Training Loss: 0.8054250283101025\n",
      "Epoch 15, Training Loss: 0.8051810856426463\n",
      "Epoch 16, Training Loss: 0.8043643588879529\n",
      "Epoch 17, Training Loss: 0.804284693703932\n",
      "Epoch 18, Training Loss: 0.803977875288795\n",
      "Epoch 19, Training Loss: 0.8034489437411814\n",
      "Epoch 20, Training Loss: 0.8030730906654807\n",
      "Epoch 21, Training Loss: 0.8023521601452547\n",
      "Epoch 22, Training Loss: 0.8020607143289903\n",
      "Epoch 23, Training Loss: 0.8017788642294267\n",
      "Epoch 24, Training Loss: 0.8010982493793263\n",
      "Epoch 25, Training Loss: 0.800906081830754\n",
      "Epoch 26, Training Loss: 0.8008771162874558\n",
      "Epoch 27, Training Loss: 0.8006517864676083\n",
      "Epoch 28, Training Loss: 0.7999596197464887\n",
      "Epoch 29, Training Loss: 0.7997483480677885\n",
      "Epoch 30, Training Loss: 0.7997026463817147\n",
      "Epoch 31, Training Loss: 0.7995719000171213\n",
      "Epoch 32, Training Loss: 0.7989028469955219\n",
      "Epoch 33, Training Loss: 0.798335676613976\n",
      "Epoch 34, Training Loss: 0.7984729650441338\n",
      "Epoch 35, Training Loss: 0.7979477381005007\n",
      "Epoch 36, Training Loss: 0.797421780684415\n",
      "Epoch 37, Training Loss: 0.7979261862530428\n",
      "Epoch 38, Training Loss: 0.797107020406162\n",
      "Epoch 39, Training Loss: 0.7969925624482772\n",
      "Epoch 40, Training Loss: 0.7966890032852397\n",
      "Epoch 41, Training Loss: 0.7968282808275784\n",
      "Epoch 42, Training Loss: 0.7965017242291395\n",
      "Epoch 43, Training Loss: 0.796834374666214\n",
      "Epoch 44, Training Loss: 0.7962398785002092\n",
      "Epoch 45, Training Loss: 0.7962533781107735\n",
      "Epoch 46, Training Loss: 0.7960006601670209\n",
      "Epoch 47, Training Loss: 0.7963171793432797\n",
      "Epoch 48, Training Loss: 0.7962244682452257\n",
      "Epoch 49, Training Loss: 0.7951779344502617\n",
      "Epoch 50, Training Loss: 0.7951177332681768\n",
      "Epoch 51, Training Loss: 0.7954187421237722\n",
      "Epoch 52, Training Loss: 0.7955796436702504\n",
      "Epoch 53, Training Loss: 0.7949943729007946\n",
      "Epoch 54, Training Loss: 0.7954741726903355\n",
      "Epoch 55, Training Loss: 0.7952653394025915\n",
      "Epoch 56, Training Loss: 0.7947825253009796\n",
      "Epoch 57, Training Loss: 0.7944368659047519\n",
      "Epoch 58, Training Loss: 0.7947330346528222\n",
      "Epoch 59, Training Loss: 0.7945422220931334\n",
      "Epoch 60, Training Loss: 0.7940668024035061\n",
      "Epoch 61, Training Loss: 0.7943393872064702\n",
      "Epoch 62, Training Loss: 0.7941115553940044\n",
      "Epoch 63, Training Loss: 0.7940065496809342\n",
      "Epoch 64, Training Loss: 0.794126413709977\n",
      "Epoch 65, Training Loss: 0.7942198794729569\n",
      "Epoch 66, Training Loss: 0.7937212430729585\n",
      "Epoch 67, Training Loss: 0.7936381123346441\n",
      "Epoch 68, Training Loss: 0.7930289020959069\n",
      "Epoch 69, Training Loss: 0.7934611990171321\n",
      "Epoch 70, Training Loss: 0.79331058270791\n",
      "Epoch 71, Training Loss: 0.7936346605244805\n",
      "Epoch 72, Training Loss: 0.7932278217988856\n",
      "Epoch 73, Training Loss: 0.7931851046926836\n",
      "Epoch 74, Training Loss: 0.7931198466525359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:18:41,426] Trial 161 finished with value: 0.6328666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7931631982326508\n",
      "Epoch 1, Training Loss: 0.8799740879675921\n",
      "Epoch 2, Training Loss: 0.820851942651412\n",
      "Epoch 3, Training Loss: 0.81224556481137\n",
      "Epoch 4, Training Loss: 0.8065666622274063\n",
      "Epoch 5, Training Loss: 0.8033155380277073\n",
      "Epoch 6, Training Loss: 0.8009178884590373\n",
      "Epoch 7, Training Loss: 0.7993572061903337\n",
      "Epoch 8, Training Loss: 0.7986298395605649\n",
      "Epoch 9, Training Loss: 0.7972437850166769\n",
      "Epoch 10, Training Loss: 0.7963894880519193\n",
      "Epoch 11, Training Loss: 0.7961081773393295\n",
      "Epoch 12, Training Loss: 0.7960131020405713\n",
      "Epoch 13, Training Loss: 0.7947902134586783\n",
      "Epoch 14, Training Loss: 0.7939130883357104\n",
      "Epoch 15, Training Loss: 0.7937168047708624\n",
      "Epoch 16, Training Loss: 0.7930428749673507\n",
      "Epoch 17, Training Loss: 0.7930806765836828\n",
      "Epoch 18, Training Loss: 0.7928202842964845\n",
      "Epoch 19, Training Loss: 0.7928394518179052\n",
      "Epoch 20, Training Loss: 0.7919485306739807\n",
      "Epoch 21, Training Loss: 0.7916779161902034\n",
      "Epoch 22, Training Loss: 0.7919663004314198\n",
      "Epoch 23, Training Loss: 0.7917584788098054\n",
      "Epoch 24, Training Loss: 0.7911708543581121\n",
      "Epoch 25, Training Loss: 0.7909660531492795\n",
      "Epoch 26, Training Loss: 0.7915077429659226\n",
      "Epoch 27, Training Loss: 0.7907458722591401\n",
      "Epoch 28, Training Loss: 0.7915501278288224\n",
      "Epoch 29, Training Loss: 0.7907202184901518\n",
      "Epoch 30, Training Loss: 0.7899426152425654\n",
      "Epoch 31, Training Loss: 0.7912247056119582\n",
      "Epoch 32, Training Loss: 0.7901353702825659\n",
      "Epoch 33, Training Loss: 0.7906604801206027\n",
      "Epoch 34, Training Loss: 0.7908691397835227\n",
      "Epoch 35, Training Loss: 0.7907942295775694\n",
      "Epoch 36, Training Loss: 0.7906147390253404\n",
      "Epoch 37, Training Loss: 0.7904943398167106\n",
      "Epoch 38, Training Loss: 0.7899440421076382\n",
      "Epoch 39, Training Loss: 0.789926155945834\n",
      "Epoch 40, Training Loss: 0.7899731260888717\n",
      "Epoch 41, Training Loss: 0.7901444768204409\n",
      "Epoch 42, Training Loss: 0.7897348171823165\n",
      "Epoch 43, Training Loss: 0.7897054107048932\n",
      "Epoch 44, Training Loss: 0.7896987429787131\n",
      "Epoch 45, Training Loss: 0.7903125153569615\n",
      "Epoch 46, Training Loss: 0.7901859180366292\n",
      "Epoch 47, Training Loss: 0.7901525358592763\n",
      "Epoch 48, Training Loss: 0.789856711696176\n",
      "Epoch 49, Training Loss: 0.7891638124690337\n",
      "Epoch 50, Training Loss: 0.7898482304460862\n",
      "Epoch 51, Training Loss: 0.7900292425997117\n",
      "Epoch 52, Training Loss: 0.7893283487768734\n",
      "Epoch 53, Training Loss: 0.7894109107466305\n",
      "Epoch 54, Training Loss: 0.7892684768929201\n",
      "Epoch 55, Training Loss: 0.7889425080664018\n",
      "Epoch 56, Training Loss: 0.7893293773426729\n",
      "Epoch 57, Training Loss: 0.7895301646344802\n",
      "Epoch 58, Training Loss: 0.789597366697648\n",
      "Epoch 59, Training Loss: 0.7886037050275242\n",
      "Epoch 60, Training Loss: 0.7896585161545697\n",
      "Epoch 61, Training Loss: 0.789462832773433\n",
      "Epoch 62, Training Loss: 0.7897975201466504\n",
      "Epoch 63, Training Loss: 0.7887416087178623\n",
      "Epoch 64, Training Loss: 0.7895489817507126\n",
      "Epoch 65, Training Loss: 0.7895134775778827\n",
      "Epoch 66, Training Loss: 0.7893465201293721\n",
      "Epoch 67, Training Loss: 0.7893891483194688\n",
      "Epoch 68, Training Loss: 0.7891396391391754\n",
      "Epoch 69, Training Loss: 0.789482416826136\n",
      "Epoch 70, Training Loss: 0.7892761377727284\n",
      "Epoch 71, Training Loss: 0.7890024278444402\n",
      "Epoch 72, Training Loss: 0.7892649839906132\n",
      "Epoch 73, Training Loss: 0.7886704802513123\n",
      "Epoch 74, Training Loss: 0.7891700362457948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:20:16,380] Trial 162 finished with value: 0.6331333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7889840236130883\n",
      "Epoch 1, Training Loss: 1.0467342514740794\n",
      "Epoch 2, Training Loss: 0.9740714372548842\n",
      "Epoch 3, Training Loss: 0.941105580329895\n",
      "Epoch 4, Training Loss: 0.9276051685326081\n",
      "Epoch 5, Training Loss: 0.9175885596669706\n",
      "Epoch 6, Training Loss: 0.9074383317079759\n",
      "Epoch 7, Training Loss: 0.8970072536540211\n",
      "Epoch 8, Training Loss: 0.8846646890604406\n",
      "Epoch 9, Training Loss: 0.8713576067659191\n",
      "Epoch 10, Training Loss: 0.8563695083883472\n",
      "Epoch 11, Training Loss: 0.8421119071487198\n",
      "Epoch 12, Training Loss: 0.8289246413044463\n",
      "Epoch 13, Training Loss: 0.8195042824386654\n",
      "Epoch 14, Training Loss: 0.8141262559962452\n",
      "Epoch 15, Training Loss: 0.8104362355139022\n",
      "Epoch 16, Training Loss: 0.807918443715662\n",
      "Epoch 17, Training Loss: 0.8066854470654538\n",
      "Epoch 18, Training Loss: 0.8058657555651844\n",
      "Epoch 19, Training Loss: 0.8046850948405445\n",
      "Epoch 20, Training Loss: 0.8040454952340377\n",
      "Epoch 21, Training Loss: 0.8029380506142638\n",
      "Epoch 22, Training Loss: 0.8034082229872396\n",
      "Epoch 23, Training Loss: 0.8039474776812963\n",
      "Epoch 24, Training Loss: 0.8021260923909065\n",
      "Epoch 25, Training Loss: 0.8015615007931128\n",
      "Epoch 26, Training Loss: 0.8014093566657905\n",
      "Epoch 27, Training Loss: 0.8017458880754341\n",
      "Epoch 28, Training Loss: 0.800986729618302\n",
      "Epoch 29, Training Loss: 0.8000266499537274\n",
      "Epoch 30, Training Loss: 0.7995605145630084\n",
      "Epoch 31, Training Loss: 0.7991988372981996\n",
      "Epoch 32, Training Loss: 0.7994307651555628\n",
      "Epoch 33, Training Loss: 0.799043105240155\n",
      "Epoch 34, Training Loss: 0.799124757150062\n",
      "Epoch 35, Training Loss: 0.7982118338122404\n",
      "Epoch 36, Training Loss: 0.7984690382068319\n",
      "Epoch 37, Training Loss: 0.7982397316990042\n",
      "Epoch 38, Training Loss: 0.7983966497550333\n",
      "Epoch 39, Training Loss: 0.7983178773320707\n",
      "Epoch 40, Training Loss: 0.7980019555056006\n",
      "Epoch 41, Training Loss: 0.7968777842987749\n",
      "Epoch 42, Training Loss: 0.797810451428693\n",
      "Epoch 43, Training Loss: 0.7972369017457603\n",
      "Epoch 44, Training Loss: 0.7969412604668983\n",
      "Epoch 45, Training Loss: 0.7969104554420127\n",
      "Epoch 46, Training Loss: 0.797098558020771\n",
      "Epoch 47, Training Loss: 0.7969207675833451\n",
      "Epoch 48, Training Loss: 0.7966977995141108\n",
      "Epoch 49, Training Loss: 0.7961736312486175\n",
      "Epoch 50, Training Loss: 0.7968790669190257\n",
      "Epoch 51, Training Loss: 0.7962881411824908\n",
      "Epoch 52, Training Loss: 0.7952797332652529\n",
      "Epoch 53, Training Loss: 0.7958891642721075\n",
      "Epoch 54, Training Loss: 0.7959005948296167\n",
      "Epoch 55, Training Loss: 0.7955864692092838\n",
      "Epoch 56, Training Loss: 0.7954475964818682\n",
      "Epoch 57, Training Loss: 0.7953754971798201\n",
      "Epoch 58, Training Loss: 0.7953098802638233\n",
      "Epoch 59, Training Loss: 0.795630219287442\n",
      "Epoch 60, Training Loss: 0.7947415259099544\n",
      "Epoch 61, Training Loss: 0.7948912182248624\n",
      "Epoch 62, Training Loss: 0.7949927204533627\n",
      "Epoch 63, Training Loss: 0.7944554337881562\n",
      "Epoch 64, Training Loss: 0.7946244419069218\n",
      "Epoch 65, Training Loss: 0.7951985587751059\n",
      "Epoch 66, Training Loss: 0.7940065624122333\n",
      "Epoch 67, Training Loss: 0.7944081961660456\n",
      "Epoch 68, Training Loss: 0.7939383026352502\n",
      "Epoch 69, Training Loss: 0.7944239377975464\n",
      "Epoch 70, Training Loss: 0.7938421265523237\n",
      "Epoch 71, Training Loss: 0.79320622646719\n",
      "Epoch 72, Training Loss: 0.7936678380894482\n",
      "Epoch 73, Training Loss: 0.7936297715158391\n",
      "Epoch 74, Training Loss: 0.7938856132944724\n",
      "Epoch 75, Training Loss: 0.7936011173671349\n",
      "Epoch 76, Training Loss: 0.7932540084186354\n",
      "Epoch 77, Training Loss: 0.7932102853194215\n",
      "Epoch 78, Training Loss: 0.7930445040078987\n",
      "Epoch 79, Training Loss: 0.7927062826945369\n",
      "Epoch 80, Training Loss: 0.7927211246096102\n",
      "Epoch 81, Training Loss: 0.7926586977521279\n",
      "Epoch 82, Training Loss: 0.7925196869929034\n",
      "Epoch 83, Training Loss: 0.7919625353096122\n",
      "Epoch 84, Training Loss: 0.792505739595657\n",
      "Epoch 85, Training Loss: 0.7925135961152557\n",
      "Epoch 86, Training Loss: 0.7922102748003221\n",
      "Epoch 87, Training Loss: 0.7918516439602787\n",
      "Epoch 88, Training Loss: 0.7920046544612799\n",
      "Epoch 89, Training Loss: 0.7913292611003818\n",
      "Epoch 90, Training Loss: 0.791345400828168\n",
      "Epoch 91, Training Loss: 0.7912475033810265\n",
      "Epoch 92, Training Loss: 0.7913777959974189\n",
      "Epoch 93, Training Loss: 0.7910189661764561\n",
      "Epoch 94, Training Loss: 0.7913255117889634\n",
      "Epoch 95, Training Loss: 0.7908449618439926\n",
      "Epoch 96, Training Loss: 0.7910421988121549\n",
      "Epoch 97, Training Loss: 0.7905991684225269\n",
      "Epoch 98, Training Loss: 0.7905856449801223\n",
      "Epoch 99, Training Loss: 0.7908209145517278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:21:49,569] Trial 163 finished with value: 0.6329333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7909125560208371\n",
      "Epoch 1, Training Loss: 0.9621487464624293\n",
      "Epoch 2, Training Loss: 0.9321903960845049\n",
      "Epoch 3, Training Loss: 0.9135447591192583\n",
      "Epoch 4, Training Loss: 0.894745981202406\n",
      "Epoch 5, Training Loss: 0.8763195667547338\n",
      "Epoch 6, Training Loss: 0.8597961825482986\n",
      "Epoch 7, Training Loss: 0.8459869915597579\n",
      "Epoch 8, Training Loss: 0.8353028497976416\n",
      "Epoch 9, Training Loss: 0.8278458725704866\n",
      "Epoch 10, Training Loss: 0.8224786305427552\n",
      "Epoch 11, Training Loss: 0.8187946229121265\n",
      "Epoch 12, Training Loss: 0.8161976262401132\n",
      "Epoch 13, Training Loss: 0.8142070448398591\n",
      "Epoch 14, Training Loss: 0.8126332442199483\n",
      "Epoch 15, Training Loss: 0.8115887094946469\n",
      "Epoch 16, Training Loss: 0.8106411928990308\n",
      "Epoch 17, Training Loss: 0.8098029854017146\n",
      "Epoch 18, Training Loss: 0.8090821771060719\n",
      "Epoch 19, Training Loss: 0.8084634107701919\n",
      "Epoch 20, Training Loss: 0.8078244740121505\n",
      "Epoch 21, Training Loss: 0.8073556143396041\n",
      "Epoch 22, Training Loss: 0.806893917462405\n",
      "Epoch 23, Training Loss: 0.8065537709348342\n",
      "Epoch 24, Training Loss: 0.8060818271777209\n",
      "Epoch 25, Training Loss: 0.8055668408730451\n",
      "Epoch 26, Training Loss: 0.8053309657994439\n",
      "Epoch 27, Training Loss: 0.8050428084064932\n",
      "Epoch 28, Training Loss: 0.8048040478369769\n",
      "Epoch 29, Training Loss: 0.8043899888852063\n",
      "Epoch 30, Training Loss: 0.8040330882633434\n",
      "Epoch 31, Training Loss: 0.8038714395551121\n",
      "Epoch 32, Training Loss: 0.8032628689092748\n",
      "Epoch 33, Training Loss: 0.8033325634283178\n",
      "Epoch 34, Training Loss: 0.8030269918722265\n",
      "Epoch 35, Training Loss: 0.8027337461359361\n",
      "Epoch 36, Training Loss: 0.8025741119945751\n",
      "Epoch 37, Training Loss: 0.8022916404639974\n",
      "Epoch 38, Training Loss: 0.8023617185564602\n",
      "Epoch 39, Training Loss: 0.8019681215286255\n",
      "Epoch 40, Training Loss: 0.8018191410513485\n",
      "Epoch 41, Training Loss: 0.8016389411337236\n",
      "Epoch 42, Training Loss: 0.8014860306066626\n",
      "Epoch 43, Training Loss: 0.8013769629422356\n",
      "Epoch 44, Training Loss: 0.8012494501646827\n",
      "Epoch 45, Training Loss: 0.8011296756828532\n",
      "Epoch 46, Training Loss: 0.8007729886559879\n",
      "Epoch 47, Training Loss: 0.8007890173968146\n",
      "Epoch 48, Training Loss: 0.800563640945098\n",
      "Epoch 49, Training Loss: 0.8004735075726228\n",
      "Epoch 50, Training Loss: 0.8002522317100974\n",
      "Epoch 51, Training Loss: 0.8003179778071011\n",
      "Epoch 52, Training Loss: 0.8000832192336812\n",
      "Epoch 53, Training Loss: 0.7999022941028371\n",
      "Epoch 54, Training Loss: 0.7999556192229775\n",
      "Epoch 55, Training Loss: 0.799797808563008\n",
      "Epoch 56, Training Loss: 0.7996968500754412\n",
      "Epoch 57, Training Loss: 0.799586100508185\n",
      "Epoch 58, Training Loss: 0.7995420803042019\n",
      "Epoch 59, Training Loss: 0.7994335772009457\n",
      "Epoch 60, Training Loss: 0.7993651604652405\n",
      "Epoch 61, Training Loss: 0.7991895620261922\n",
      "Epoch 62, Training Loss: 0.7993226916649763\n",
      "Epoch 63, Training Loss: 0.7990503450702219\n",
      "Epoch 64, Training Loss: 0.7992122747617609\n",
      "Epoch 65, Training Loss: 0.7989576356551227\n",
      "Epoch 66, Training Loss: 0.7989642848688013\n",
      "Epoch 67, Training Loss: 0.7988542885640089\n",
      "Epoch 68, Training Loss: 0.7986647950200474\n",
      "Epoch 69, Training Loss: 0.7987001095098608\n",
      "Epoch 70, Training Loss: 0.7986222507673152\n",
      "Epoch 71, Training Loss: 0.7986428602302775\n",
      "Epoch 72, Training Loss: 0.7985449506254757\n",
      "Epoch 73, Training Loss: 0.7986096031525556\n",
      "Epoch 74, Training Loss: 0.7983921658291536\n",
      "Epoch 75, Training Loss: 0.7983474992303288\n",
      "Epoch 76, Training Loss: 0.7984830899799571\n",
      "Epoch 77, Training Loss: 0.798145205413594\n",
      "Epoch 78, Training Loss: 0.7981553384836982\n",
      "Epoch 79, Training Loss: 0.7981041501550113\n",
      "Epoch 80, Training Loss: 0.7981247508525848\n",
      "Epoch 81, Training Loss: 0.798102450020173\n",
      "Epoch 82, Training Loss: 0.7980224583429448\n",
      "Epoch 83, Training Loss: 0.7980259375712451\n",
      "Epoch 84, Training Loss: 0.7980078342381646\n",
      "Epoch 85, Training Loss: 0.7978450902770547\n",
      "Epoch 86, Training Loss: 0.7979545441795798\n",
      "Epoch 87, Training Loss: 0.7978545001675101\n",
      "Epoch 88, Training Loss: 0.7978721851461074\n",
      "Epoch 89, Training Loss: 0.797793684426476\n",
      "Epoch 90, Training Loss: 0.797829923138899\n",
      "Epoch 91, Training Loss: 0.7977308637254379\n",
      "Epoch 92, Training Loss: 0.7977440020617317\n",
      "Epoch 93, Training Loss: 0.7977286988847396\n",
      "Epoch 94, Training Loss: 0.7976155846960404\n",
      "Epoch 95, Training Loss: 0.7974985564456266\n",
      "Epoch 96, Training Loss: 0.7976828972732319\n",
      "Epoch 97, Training Loss: 0.7975433768945582\n",
      "Epoch 98, Training Loss: 0.7975495777410619\n",
      "Epoch 99, Training Loss: 0.7974837655880872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:23:23,499] Trial 164 finished with value: 0.6332 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 138 with value: 0.6412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7974749387011808\n",
      "Epoch 1, Training Loss: 0.83949103474617\n",
      "Epoch 2, Training Loss: 0.8108147849756129\n",
      "Epoch 3, Training Loss: 0.8070350139982561\n",
      "Epoch 4, Training Loss: 0.80402817066978\n",
      "Epoch 5, Training Loss: 0.8020312321186066\n",
      "Epoch 6, Training Loss: 0.8009072448225583\n",
      "Epoch 7, Training Loss: 0.8001722629631267\n",
      "Epoch 8, Training Loss: 0.7982316783596488\n",
      "Epoch 9, Training Loss: 0.798420135974884\n",
      "Epoch 10, Training Loss: 0.7969037739669576\n",
      "Epoch 11, Training Loss: 0.7970943447421579\n",
      "Epoch 12, Training Loss: 0.7971764108012704\n",
      "Epoch 13, Training Loss: 0.7951667055662941\n",
      "Epoch 14, Training Loss: 0.7953562133452472\n",
      "Epoch 15, Training Loss: 0.7956203424930572\n",
      "Epoch 16, Training Loss: 0.7942254259305842\n",
      "Epoch 17, Training Loss: 0.793080184880425\n",
      "Epoch 18, Training Loss: 0.7952641490627738\n",
      "Epoch 19, Training Loss: 0.7929863342818092\n",
      "Epoch 20, Training Loss: 0.7933913540138918\n",
      "Epoch 21, Training Loss: 0.792810956660439\n",
      "Epoch 22, Training Loss: 0.7930942991200616\n",
      "Epoch 23, Training Loss: 0.7924846239650951\n",
      "Epoch 24, Training Loss: 0.7921980759676766\n",
      "Epoch 25, Training Loss: 0.7926175620976617\n",
      "Epoch 26, Training Loss: 0.7920294970624587\n",
      "Epoch 27, Training Loss: 0.791949079247082\n",
      "Epoch 28, Training Loss: 0.7904743301167207\n",
      "Epoch 29, Training Loss: 0.7914430473832523\n",
      "Epoch 30, Training Loss: 0.7905135046734529\n",
      "Epoch 31, Training Loss: 0.7913604539983413\n",
      "Epoch 32, Training Loss: 0.7906590747132021\n",
      "Epoch 33, Training Loss: 0.7904983024737414\n",
      "Epoch 34, Training Loss: 0.7914232535221998\n",
      "Epoch 35, Training Loss: 0.7902466129555422\n",
      "Epoch 36, Training Loss: 0.7901288082319148\n",
      "Epoch 37, Training Loss: 0.7898620167900534\n",
      "Epoch 38, Training Loss: 0.7910486652570612\n",
      "Epoch 39, Training Loss: 0.7893479732906117\n",
      "Epoch 40, Training Loss: 0.7904332488424638\n",
      "Epoch 41, Training Loss: 0.791270123720169\n",
      "Epoch 42, Training Loss: 0.7900616418614107\n",
      "Epoch 43, Training Loss: 0.7896388753021465\n",
      "Epoch 44, Training Loss: 0.7889679844239179\n",
      "Epoch 45, Training Loss: 0.7892553930422839\n",
      "Epoch 46, Training Loss: 0.7905450943638297\n",
      "Epoch 47, Training Loss: 0.7898630119772518\n",
      "Epoch 48, Training Loss: 0.7899195672484005\n",
      "Epoch 49, Training Loss: 0.7886461139426512\n",
      "Epoch 50, Training Loss: 0.7895653712749481\n",
      "Epoch 51, Training Loss: 0.7895189962667577\n",
      "Epoch 52, Training Loss: 0.7901321068932028\n",
      "Epoch 53, Training Loss: 0.790060891754487\n",
      "Epoch 54, Training Loss: 0.7893041831605575\n",
      "Epoch 55, Training Loss: 0.7901543225260342\n",
      "Epoch 56, Training Loss: 0.7893248381334192\n",
      "Epoch 57, Training Loss: 0.7896893312650568\n",
      "Epoch 58, Training Loss: 0.7893802691908444\n",
      "Epoch 59, Training Loss: 0.7893541213344125\n",
      "Epoch 60, Training Loss: 0.7890911612791174\n",
      "Epoch 61, Training Loss: 0.7887051910512588\n",
      "Epoch 62, Training Loss: 0.7883951461315155\n",
      "Epoch 63, Training Loss: 0.7885206749158747\n",
      "Epoch 64, Training Loss: 0.789096805488362\n",
      "Epoch 65, Training Loss: 0.7891551835396711\n",
      "Epoch 66, Training Loss: 0.7886102747917175\n",
      "Epoch 67, Training Loss: 0.7887577469208661\n",
      "Epoch 68, Training Loss: 0.7894907527110155\n",
      "Epoch 69, Training Loss: 0.7884015671645894\n",
      "Epoch 70, Training Loss: 0.7882772848886602\n",
      "Epoch 71, Training Loss: 0.7885241690803977\n",
      "Epoch 72, Training Loss: 0.7877112739226397\n",
      "Epoch 73, Training Loss: 0.7886828439375934\n",
      "Epoch 74, Training Loss: 0.789011747416328\n",
      "Epoch 75, Training Loss: 0.7873792189009049\n",
      "Epoch 76, Training Loss: 0.7886057274481829\n",
      "Epoch 77, Training Loss: 0.7887969623593724\n",
      "Epoch 78, Training Loss: 0.7884881984486299\n",
      "Epoch 79, Training Loss: 0.7881475234031677\n",
      "Epoch 80, Training Loss: 0.7878058353592368\n",
      "Epoch 81, Training Loss: 0.7877626745139852\n",
      "Epoch 82, Training Loss: 0.7875898235685685\n",
      "Epoch 83, Training Loss: 0.7884771497109357\n",
      "Epoch 84, Training Loss: 0.7880649584181169\n",
      "Epoch 85, Training Loss: 0.7883144464212305\n",
      "Epoch 86, Training Loss: 0.7882153232658611\n",
      "Epoch 87, Training Loss: 0.7879063416228574\n",
      "Epoch 88, Training Loss: 0.7878593436409446\n",
      "Epoch 89, Training Loss: 0.7877667666182798\n",
      "Epoch 90, Training Loss: 0.7874413480478174\n",
      "Epoch 91, Training Loss: 0.7873800310667823\n",
      "Epoch 92, Training Loss: 0.7879049081662122\n",
      "Epoch 93, Training Loss: 0.787836288283853\n",
      "Epoch 94, Training Loss: 0.7872633903166827\n",
      "Epoch 95, Training Loss: 0.7867142828071818\n",
      "Epoch 96, Training Loss: 0.7873200722301708\n",
      "Epoch 97, Training Loss: 0.7876873899908626\n",
      "Epoch 98, Training Loss: 0.7880325984253603\n",
      "Epoch 99, Training Loss: 0.7877136456265169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:25:59,335] Trial 165 finished with value: 0.6417333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7872726011276245\n",
      "Epoch 1, Training Loss: 1.0913182468342602\n",
      "Epoch 2, Training Loss: 1.090580021349111\n",
      "Epoch 3, Training Loss: 1.0904907755385664\n",
      "Epoch 4, Training Loss: 1.090374127187227\n",
      "Epoch 5, Training Loss: 1.0904337723452346\n",
      "Epoch 6, Training Loss: 1.0902917815330333\n",
      "Epoch 7, Training Loss: 1.0901367295057254\n",
      "Epoch 8, Training Loss: 1.090043140174751\n",
      "Epoch 9, Training Loss: 1.0899494400598053\n",
      "Epoch 10, Training Loss: 1.0897671098995927\n",
      "Epoch 11, Training Loss: 1.089569846669534\n",
      "Epoch 12, Training Loss: 1.089271486970715\n",
      "Epoch 13, Training Loss: 1.0891614869124908\n",
      "Epoch 14, Training Loss: 1.0890709197610842\n",
      "Epoch 15, Training Loss: 1.0889224900338883\n",
      "Epoch 16, Training Loss: 1.0884522809121842\n",
      "Epoch 17, Training Loss: 1.0884293337513629\n",
      "Epoch 18, Training Loss: 1.088078022182436\n",
      "Epoch 19, Training Loss: 1.0875503332095038\n",
      "Epoch 20, Training Loss: 1.0873303843620128\n",
      "Epoch 21, Training Loss: 1.0868467225167984\n",
      "Epoch 22, Training Loss: 1.0864571730893358\n",
      "Epoch 23, Training Loss: 1.0858996106269665\n",
      "Epoch 24, Training Loss: 1.0851778914157608\n",
      "Epoch 25, Training Loss: 1.0845502580915178\n",
      "Epoch 26, Training Loss: 1.0836159103795102\n",
      "Epoch 27, Training Loss: 1.082738663558673\n",
      "Epoch 28, Training Loss: 1.081312702831469\n",
      "Epoch 29, Training Loss: 1.079973912956123\n",
      "Epoch 30, Training Loss: 1.078356871569067\n",
      "Epoch 31, Training Loss: 1.076185175350734\n",
      "Epoch 32, Training Loss: 1.073663394074691\n",
      "Epoch 33, Training Loss: 1.070702672542486\n",
      "Epoch 34, Training Loss: 1.0669897427236228\n",
      "Epoch 35, Training Loss: 1.0624039024338687\n",
      "Epoch 36, Training Loss: 1.0568714588208306\n",
      "Epoch 37, Training Loss: 1.0502714798862773\n",
      "Epoch 38, Training Loss: 1.0424261808395385\n",
      "Epoch 39, Training Loss: 1.0341446774346488\n",
      "Epoch 40, Training Loss: 1.0253250930542337\n",
      "Epoch 41, Training Loss: 1.0169242443894981\n",
      "Epoch 42, Training Loss: 1.0098727252250328\n",
      "Epoch 43, Training Loss: 1.0040265664122159\n",
      "Epoch 44, Training Loss: 1.0002366104520353\n",
      "Epoch 45, Training Loss: 0.997438243009094\n",
      "Epoch 46, Training Loss: 0.9953549208497643\n",
      "Epoch 47, Training Loss: 0.9944344058072656\n",
      "Epoch 48, Training Loss: 0.9937360531405399\n",
      "Epoch 49, Training Loss: 0.9927634846895261\n",
      "Epoch 50, Training Loss: 0.9922345328151732\n",
      "Epoch 51, Training Loss: 0.9917336255087889\n",
      "Epoch 52, Training Loss: 0.9909626238328174\n",
      "Epoch 53, Training Loss: 0.9907355762065801\n",
      "Epoch 54, Training Loss: 0.9898281231858677\n",
      "Epoch 55, Training Loss: 0.989196709493049\n",
      "Epoch 56, Training Loss: 0.9882339743743266\n",
      "Epoch 57, Training Loss: 0.9879114522969812\n",
      "Epoch 58, Training Loss: 0.987057027870551\n",
      "Epoch 59, Training Loss: 0.9859217258324301\n",
      "Epoch 60, Training Loss: 0.9852199624355574\n",
      "Epoch 61, Training Loss: 0.9843891357120714\n",
      "Epoch 62, Training Loss: 0.9835403989132185\n",
      "Epoch 63, Training Loss: 0.9820416993664619\n",
      "Epoch 64, Training Loss: 0.9815775184703053\n",
      "Epoch 65, Training Loss: 0.980311005366476\n",
      "Epoch 66, Training Loss: 0.9788772883271812\n",
      "Epoch 67, Training Loss: 0.9777218817768241\n",
      "Epoch 68, Training Loss: 0.9764544089933983\n",
      "Epoch 69, Training Loss: 0.9748702880135156\n",
      "Epoch 70, Training Loss: 0.9732405659847689\n",
      "Epoch 71, Training Loss: 0.9717385098450166\n",
      "Epoch 72, Training Loss: 0.9703141428474197\n",
      "Epoch 73, Training Loss: 0.9681552583113648\n",
      "Epoch 74, Training Loss: 0.9662125782859057\n",
      "Epoch 75, Training Loss: 0.9648773694396915\n",
      "Epoch 76, Training Loss: 0.9622150820897037\n",
      "Epoch 77, Training Loss: 0.960466398242721\n",
      "Epoch 78, Training Loss: 0.9577606833070741\n",
      "Epoch 79, Training Loss: 0.9562433438193529\n",
      "Epoch 80, Training Loss: 0.9538292909027042\n",
      "Epoch 81, Training Loss: 0.952150077031071\n",
      "Epoch 82, Training Loss: 0.9496162661036155\n",
      "Epoch 83, Training Loss: 0.9478568519864764\n",
      "Epoch 84, Training Loss: 0.9458008378968203\n",
      "Epoch 85, Training Loss: 0.9440399132276837\n",
      "Epoch 86, Training Loss: 0.9423794967787607\n",
      "Epoch 87, Training Loss: 0.9414571492295516\n",
      "Epoch 88, Training Loss: 0.9398321998746771\n",
      "Epoch 89, Training Loss: 0.9381209522261655\n",
      "Epoch 90, Training Loss: 0.9367156138993744\n",
      "Epoch 91, Training Loss: 0.935530394181273\n",
      "Epoch 92, Training Loss: 0.9343660023875703\n",
      "Epoch 93, Training Loss: 0.9324736274274669\n",
      "Epoch 94, Training Loss: 0.9316714601409166\n",
      "Epoch 95, Training Loss: 0.9295100508775926\n",
      "Epoch 96, Training Loss: 0.9285070201508084\n",
      "Epoch 97, Training Loss: 0.9268948681372449\n",
      "Epoch 98, Training Loss: 0.9248837419022294\n",
      "Epoch 99, Training Loss: 0.9228077244041557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:27:46,891] Trial 166 finished with value: 0.5538 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.921227322574845\n",
      "Epoch 1, Training Loss: 0.8624821848729077\n",
      "Epoch 2, Training Loss: 0.8251957279794356\n",
      "Epoch 3, Training Loss: 0.8158059997418348\n",
      "Epoch 4, Training Loss: 0.8124501277418698\n",
      "Epoch 5, Training Loss: 0.8100104181906757\n",
      "Epoch 6, Training Loss: 0.8079337895617765\n",
      "Epoch 7, Training Loss: 0.8064354498947368\n",
      "Epoch 8, Training Loss: 0.8058498780867632\n",
      "Epoch 9, Training Loss: 0.8050397428344278\n",
      "Epoch 10, Training Loss: 0.8042738195026622\n",
      "Epoch 11, Training Loss: 0.8034383289253011\n",
      "Epoch 12, Training Loss: 0.8026195967898649\n",
      "Epoch 13, Training Loss: 0.802301639809328\n",
      "Epoch 14, Training Loss: 0.801903271114125\n",
      "Epoch 15, Training Loss: 0.801005148046157\n",
      "Epoch 16, Training Loss: 0.8002225336607764\n",
      "Epoch 17, Training Loss: 0.80044285234283\n",
      "Epoch 18, Training Loss: 0.7997578423163471\n",
      "Epoch 19, Training Loss: 0.799431132218417\n",
      "Epoch 20, Training Loss: 0.7986110767196206\n",
      "Epoch 21, Training Loss: 0.7980256462097168\n",
      "Epoch 22, Training Loss: 0.7989319773982553\n",
      "Epoch 23, Training Loss: 0.7987088436940137\n",
      "Epoch 24, Training Loss: 0.7982461089246413\n",
      "Epoch 25, Training Loss: 0.7989954275243423\n",
      "Epoch 26, Training Loss: 0.797607531267054\n",
      "Epoch 27, Training Loss: 0.7977395695798537\n",
      "Epoch 28, Training Loss: 0.7984188523012049\n",
      "Epoch 29, Training Loss: 0.797123757390415\n",
      "Epoch 30, Training Loss: 0.7967304834197549\n",
      "Epoch 31, Training Loss: 0.7974960911273956\n",
      "Epoch 32, Training Loss: 0.7971122721363516\n",
      "Epoch 33, Training Loss: 0.7967604593669667\n",
      "Epoch 34, Training Loss: 0.7968904762408313\n",
      "Epoch 35, Training Loss: 0.7967858643391553\n",
      "Epoch 36, Training Loss: 0.7966468142060672\n",
      "Epoch 37, Training Loss: 0.7963966237096225\n",
      "Epoch 38, Training Loss: 0.7963240263041328\n",
      "Epoch 39, Training Loss: 0.7963481377152836\n",
      "Epoch 40, Training Loss: 0.796347510393928\n",
      "Epoch 41, Training Loss: 0.7960095983393052\n",
      "Epoch 42, Training Loss: 0.7966261245923884\n",
      "Epoch 43, Training Loss: 0.7956030515362235\n",
      "Epoch 44, Training Loss: 0.7958543787283056\n",
      "Epoch 45, Training Loss: 0.7957025023067699\n",
      "Epoch 46, Training Loss: 0.7958147513165194\n",
      "Epoch 47, Training Loss: 0.7952053783921634\n",
      "Epoch 48, Training Loss: 0.7954664838314056\n",
      "Epoch 49, Training Loss: 0.7954958757232217\n",
      "Epoch 50, Training Loss: 0.7957020694368026\n",
      "Epoch 51, Training Loss: 0.7952959367808173\n",
      "Epoch 52, Training Loss: 0.7954292677430546\n",
      "Epoch 53, Training Loss: 0.795406762010911\n",
      "Epoch 54, Training Loss: 0.7950345003604888\n",
      "Epoch 55, Training Loss: 0.7951592972699334\n",
      "Epoch 56, Training Loss: 0.7948765224568984\n",
      "Epoch 57, Training Loss: 0.7953886663913727\n",
      "Epoch 58, Training Loss: 0.7950206763604108\n",
      "Epoch 59, Training Loss: 0.7947147648474749\n",
      "Epoch 60, Training Loss: 0.7944237200652852\n",
      "Epoch 61, Training Loss: 0.7948493850932402\n",
      "Epoch 62, Training Loss: 0.7950584374455845\n",
      "Epoch 63, Training Loss: 0.7945646516715779\n",
      "Epoch 64, Training Loss: 0.7945466628495385\n",
      "Epoch 65, Training Loss: 0.7947449947104734\n",
      "Epoch 66, Training Loss: 0.7941814399466796\n",
      "Epoch 67, Training Loss: 0.7949666890677284\n",
      "Epoch 68, Training Loss: 0.7946316177003524\n",
      "Epoch 69, Training Loss: 0.7942549530898824\n",
      "Epoch 70, Training Loss: 0.794943809298908\n",
      "Epoch 71, Training Loss: 0.794312480617972\n",
      "Epoch 72, Training Loss: 0.7953580573727103\n",
      "Epoch 73, Training Loss: 0.7944996482484481\n",
      "Epoch 74, Training Loss: 0.7940113204367021\n",
      "Epoch 75, Training Loss: 0.7946162706262925\n",
      "Epoch 76, Training Loss: 0.7946620933448567\n",
      "Epoch 77, Training Loss: 0.7938904681626489\n",
      "Epoch 78, Training Loss: 0.7951027687156902\n",
      "Epoch 79, Training Loss: 0.7944883110242732\n",
      "Epoch 80, Training Loss: 0.7941199770394494\n",
      "Epoch 81, Training Loss: 0.7947577296285069\n",
      "Epoch 82, Training Loss: 0.7941104956234203\n",
      "Epoch 83, Training Loss: 0.7939376651539523\n",
      "Epoch 84, Training Loss: 0.7937374875825994\n",
      "Epoch 85, Training Loss: 0.7940689187190112\n",
      "Epoch 86, Training Loss: 0.7932310574194964\n",
      "Epoch 87, Training Loss: 0.7939960768643548\n",
      "Epoch 88, Training Loss: 0.7942326451750362\n",
      "Epoch 89, Training Loss: 0.7940224514288061\n",
      "Epoch 90, Training Loss: 0.7941881268164691\n",
      "Epoch 91, Training Loss: 0.7943054912370794\n",
      "Epoch 92, Training Loss: 0.7940006255402284\n",
      "Epoch 93, Training Loss: 0.7940385203501757\n",
      "Epoch 94, Training Loss: 0.7940675217263838\n",
      "Epoch 95, Training Loss: 0.7940732941206764\n",
      "Epoch 96, Training Loss: 0.7933342116720536\n",
      "Epoch 97, Training Loss: 0.7935327968877904\n",
      "Epoch 98, Training Loss: 0.7942024445533753\n",
      "Epoch 99, Training Loss: 0.7932295360284694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:29:52,592] Trial 167 finished with value: 0.6304 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7935437972405378\n",
      "Epoch 1, Training Loss: 0.8409220873861385\n",
      "Epoch 2, Training Loss: 0.8093637335569339\n",
      "Epoch 3, Training Loss: 0.8025609441269609\n",
      "Epoch 4, Training Loss: 0.8015804353513216\n",
      "Epoch 5, Training Loss: 0.8012148904621152\n",
      "Epoch 6, Training Loss: 0.7991147173974747\n",
      "Epoch 7, Training Loss: 0.7962182873173764\n",
      "Epoch 8, Training Loss: 0.7987660243995207\n",
      "Epoch 9, Training Loss: 0.7967571438703321\n",
      "Epoch 10, Training Loss: 0.795311879573908\n",
      "Epoch 11, Training Loss: 0.7952936997090964\n",
      "Epoch 12, Training Loss: 0.794510042936282\n",
      "Epoch 13, Training Loss: 0.7939891488032234\n",
      "Epoch 14, Training Loss: 0.7942317478638843\n",
      "Epoch 15, Training Loss: 0.7922951676791772\n",
      "Epoch 16, Training Loss: 0.7936064524758131\n",
      "Epoch 17, Training Loss: 0.7938369214982914\n",
      "Epoch 18, Training Loss: 0.7925513342807168\n",
      "Epoch 19, Training Loss: 0.7918505181943564\n",
      "Epoch 20, Training Loss: 0.7923267914836568\n",
      "Epoch 21, Training Loss: 0.7911369455488104\n",
      "Epoch 22, Training Loss: 0.7940936258860997\n",
      "Epoch 23, Training Loss: 0.7901360653844991\n",
      "Epoch 24, Training Loss: 0.7903812815372209\n",
      "Epoch 25, Training Loss: 0.7897529452815092\n",
      "Epoch 26, Training Loss: 0.7905947877948446\n",
      "Epoch 27, Training Loss: 0.7914274595733872\n",
      "Epoch 28, Training Loss: 0.7901550589647508\n",
      "Epoch 29, Training Loss: 0.789554843911551\n",
      "Epoch 30, Training Loss: 0.7900642877234552\n",
      "Epoch 31, Training Loss: 0.7894253218084349\n",
      "Epoch 32, Training Loss: 0.789470948939933\n",
      "Epoch 33, Training Loss: 0.7896806249044892\n",
      "Epoch 34, Training Loss: 0.7896467887369314\n",
      "Epoch 35, Training Loss: 0.7901909994003468\n",
      "Epoch 36, Training Loss: 0.7890452941557519\n",
      "Epoch 37, Training Loss: 0.7900903604084388\n",
      "Epoch 38, Training Loss: 0.7906521271942253\n",
      "Epoch 39, Training Loss: 0.7905490488934338\n",
      "Epoch 40, Training Loss: 0.7900248081164253\n",
      "Epoch 41, Training Loss: 0.7898539274258721\n",
      "Epoch 42, Training Loss: 0.7886633689242198\n",
      "Epoch 43, Training Loss: 0.7886306222220113\n",
      "Epoch 44, Training Loss: 0.7886045304456152\n",
      "Epoch 45, Training Loss: 0.7892473357064383\n",
      "Epoch 46, Training Loss: 0.7893298145523645\n",
      "Epoch 47, Training Loss: 0.7890323458757615\n",
      "Epoch 48, Training Loss: 0.7885113552100677\n",
      "Epoch 49, Training Loss: 0.7877173948108702\n",
      "Epoch 50, Training Loss: 0.7889393093890713\n",
      "Epoch 51, Training Loss: 0.7881764749835308\n",
      "Epoch 52, Training Loss: 0.7884157413826849\n",
      "Epoch 53, Training Loss: 0.7898358955419154\n",
      "Epoch 54, Training Loss: 0.7903192538964121\n",
      "Epoch 55, Training Loss: 0.7880788968918019\n",
      "Epoch 56, Training Loss: 0.7881289151378144\n",
      "Epoch 57, Training Loss: 0.788698515139128\n",
      "Epoch 58, Training Loss: 0.7882335891400961\n",
      "Epoch 59, Training Loss: 0.7875872715523369\n",
      "Epoch 60, Training Loss: 0.7877707921472707\n",
      "Epoch 61, Training Loss: 0.7890768822870756\n",
      "Epoch 62, Training Loss: 0.7872220653340333\n",
      "Epoch 63, Training Loss: 0.7886917193133132\n",
      "Epoch 64, Training Loss: 0.7878867740917923\n",
      "Epoch 65, Training Loss: 0.787485941191365\n",
      "Epoch 66, Training Loss: 0.7873481944987648\n",
      "Epoch 67, Training Loss: 0.7871194476471808\n",
      "Epoch 68, Training Loss: 0.7877770548476313\n",
      "Epoch 69, Training Loss: 0.7873419578810383\n",
      "Epoch 70, Training Loss: 0.7882355499984627\n",
      "Epoch 71, Training Loss: 0.787926563463713\n",
      "Epoch 72, Training Loss: 0.7872118448852596\n",
      "Epoch 73, Training Loss: 0.7881151432381537\n",
      "Epoch 74, Training Loss: 0.786779699110447\n",
      "Epoch 75, Training Loss: 0.7880416584194154\n",
      "Epoch 76, Training Loss: 0.7878460473584054\n",
      "Epoch 77, Training Loss: 0.7873452183895542\n",
      "Epoch 78, Training Loss: 0.78677096510292\n",
      "Epoch 79, Training Loss: 0.7873969074478723\n",
      "Epoch 80, Training Loss: 0.7870816331160696\n",
      "Epoch 81, Training Loss: 0.7868480193883853\n",
      "Epoch 82, Training Loss: 0.7866336351946781\n",
      "Epoch 83, Training Loss: 0.7861228579865362\n",
      "Epoch 84, Training Loss: 0.7870608135273582\n",
      "Epoch 85, Training Loss: 0.7863853712727252\n",
      "Epoch 86, Training Loss: 0.786138399561545\n",
      "Epoch 87, Training Loss: 0.7866561256853262\n",
      "Epoch 88, Training Loss: 0.7873924801224157\n",
      "Epoch 89, Training Loss: 0.7873511540262322\n",
      "Epoch 90, Training Loss: 0.7875831823599966\n",
      "Epoch 91, Training Loss: 0.7864467502536631\n",
      "Epoch 92, Training Loss: 0.7859415472001958\n",
      "Epoch 93, Training Loss: 0.7861602656823352\n",
      "Epoch 94, Training Loss: 0.7871877487440755\n",
      "Epoch 95, Training Loss: 0.7879159853870707\n",
      "Epoch 96, Training Loss: 0.7873598730653748\n",
      "Epoch 97, Training Loss: 0.7858321372727702\n",
      "Epoch 98, Training Loss: 0.7872163813813289\n",
      "Epoch 99, Training Loss: 0.78629172610161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:32:01,043] Trial 168 finished with value: 0.636 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7874696240389257\n",
      "Epoch 1, Training Loss: 1.1091932703466976\n",
      "Epoch 2, Training Loss: 1.0926040007086362\n",
      "Epoch 3, Training Loss: 1.0852575850486756\n",
      "Epoch 4, Training Loss: 1.0786043095588684\n",
      "Epoch 5, Training Loss: 1.0724447926353007\n",
      "Epoch 6, Training Loss: 1.0667154385061826\n",
      "Epoch 7, Training Loss: 1.0612979050243603\n",
      "Epoch 8, Training Loss: 1.056233989350936\n",
      "Epoch 9, Training Loss: 1.0513786655313828\n",
      "Epoch 10, Training Loss: 1.046744531883913\n",
      "Epoch 11, Training Loss: 1.0423402370424832\n",
      "Epoch 12, Training Loss: 1.03809157687075\n",
      "Epoch 13, Training Loss: 1.0340117997281693\n",
      "Epoch 14, Training Loss: 1.0301036196596483\n",
      "Epoch 15, Training Loss: 1.0263347256884856\n",
      "Epoch 16, Training Loss: 1.0227383977525375\n",
      "Epoch 17, Training Loss: 1.0192670445582446\n",
      "Epoch 18, Training Loss: 1.0159298271291397\n",
      "Epoch 19, Training Loss: 1.012721005117192\n",
      "Epoch 20, Training Loss: 1.0096637033714968\n",
      "Epoch 21, Training Loss: 1.0067238503343918\n",
      "Epoch 22, Training Loss: 1.0038895564920762\n",
      "Epoch 23, Training Loss: 1.0012126269060022\n",
      "Epoch 24, Training Loss: 0.9986477534911212\n",
      "Epoch 25, Training Loss: 0.9962019634948057\n",
      "Epoch 26, Training Loss: 0.9938761796670802\n",
      "Epoch 27, Training Loss: 0.9916649420822368\n",
      "Epoch 28, Training Loss: 0.9895547040771036\n",
      "Epoch 29, Training Loss: 0.9875608388816609\n",
      "Epoch 30, Training Loss: 0.9856606362146489\n",
      "Epoch 31, Training Loss: 0.9838740329882678\n",
      "Epoch 32, Training Loss: 0.9821728859228246\n",
      "Epoch 33, Training Loss: 0.9805806626993067\n",
      "Epoch 34, Training Loss: 0.9790671856964336\n",
      "Epoch 35, Training Loss: 0.9776450279880973\n",
      "Epoch 36, Training Loss: 0.976300829298356\n",
      "Epoch 37, Training Loss: 0.9750474452271181\n",
      "Epoch 38, Training Loss: 0.97384953919579\n",
      "Epoch 39, Training Loss: 0.9727355021588943\n",
      "Epoch 40, Training Loss: 0.9716829881948583\n",
      "Epoch 41, Training Loss: 0.9706955629937789\n",
      "Epoch 42, Training Loss: 0.9697469812280991\n",
      "Epoch 43, Training Loss: 0.9688898270971635\n",
      "Epoch 44, Training Loss: 0.9680811843451331\n",
      "Epoch 45, Training Loss: 0.9673153366060818\n",
      "Epoch 46, Training Loss: 0.9665951906933504\n",
      "Epoch 47, Training Loss: 0.9659158811148475\n",
      "Epoch 48, Training Loss: 0.9652810389154097\n",
      "Epoch 49, Training Loss: 0.9646971915048711\n",
      "Epoch 50, Training Loss: 0.9641246000458212\n",
      "Epoch 51, Training Loss: 0.9636066223593319\n",
      "Epoch 52, Training Loss: 0.9631083785786348\n",
      "Epoch 53, Training Loss: 0.9626460294863757\n",
      "Epoch 54, Training Loss: 0.9622024822235108\n",
      "Epoch 55, Training Loss: 0.9617930486622979\n",
      "Epoch 56, Training Loss: 0.9613959599242491\n",
      "Epoch 57, Training Loss: 0.9610201803375693\n",
      "Epoch 58, Training Loss: 0.9606872174319099\n",
      "Epoch 59, Training Loss: 0.9603482399267309\n",
      "Epoch 60, Training Loss: 0.9600338248645558\n",
      "Epoch 61, Training Loss: 0.9597423526820015\n",
      "Epoch 62, Training Loss: 0.9594633562424604\n",
      "Epoch 63, Training Loss: 0.9591866906250225\n",
      "Epoch 64, Training Loss: 0.9589315042074988\n",
      "Epoch 65, Training Loss: 0.9586920309066772\n",
      "Epoch 66, Training Loss: 0.9584564473348506\n",
      "Epoch 67, Training Loss: 0.9582384994450738\n",
      "Epoch 68, Training Loss: 0.9579951993858113\n",
      "Epoch 69, Training Loss: 0.9578093130448285\n",
      "Epoch 70, Training Loss: 0.9576127217797672\n",
      "Epoch 71, Training Loss: 0.957404204046025\n",
      "Epoch 72, Training Loss: 0.9572288184306201\n",
      "Epoch 73, Training Loss: 0.9570584231965682\n",
      "Epoch 74, Training Loss: 0.9568659536978777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:33:11,299] Trial 169 finished with value: 0.5277333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.956715574685265\n",
      "Epoch 1, Training Loss: 0.98702743880889\n",
      "Epoch 2, Training Loss: 0.9341258708168478\n",
      "Epoch 3, Training Loss: 0.9024379933581633\n",
      "Epoch 4, Training Loss: 0.8611600506305694\n",
      "Epoch 5, Training Loss: 0.8321580279574675\n",
      "Epoch 6, Training Loss: 0.8208892472351299\n",
      "Epoch 7, Training Loss: 0.8166068438221427\n",
      "Epoch 8, Training Loss: 0.8140451151483199\n",
      "Epoch 9, Training Loss: 0.8122403932318968\n",
      "Epoch 10, Training Loss: 0.8104755556583405\n",
      "Epoch 11, Training Loss: 0.8092463832743028\n",
      "Epoch 12, Training Loss: 0.8081640384477727\n",
      "Epoch 13, Training Loss: 0.806968031911289\n",
      "Epoch 14, Training Loss: 0.8061861893008737\n",
      "Epoch 15, Training Loss: 0.8053506426951464\n",
      "Epoch 16, Training Loss: 0.8044606320998248\n",
      "Epoch 17, Training Loss: 0.8038622622630175\n",
      "Epoch 18, Training Loss: 0.8033973307469312\n",
      "Epoch 19, Training Loss: 0.8028405432140125\n",
      "Epoch 20, Training Loss: 0.8021938438275281\n",
      "Epoch 21, Training Loss: 0.8018002248511595\n",
      "Epoch 22, Training Loss: 0.8015866912813747\n",
      "Epoch 23, Training Loss: 0.8011052466140074\n",
      "Epoch 24, Training Loss: 0.8010063982009887\n",
      "Epoch 25, Training Loss: 0.8010297205868889\n",
      "Epoch 26, Training Loss: 0.800527826126884\n",
      "Epoch 27, Training Loss: 0.799970389955184\n",
      "Epoch 28, Training Loss: 0.8002821021921495\n",
      "Epoch 29, Training Loss: 0.7995746619561139\n",
      "Epoch 30, Training Loss: 0.7994096225850722\n",
      "Epoch 31, Training Loss: 0.7995944413718056\n",
      "Epoch 32, Training Loss: 0.7994163689893835\n",
      "Epoch 33, Training Loss: 0.7993255056353177\n",
      "Epoch 34, Training Loss: 0.7989587596584768\n",
      "Epoch 35, Training Loss: 0.7989270487252403\n",
      "Epoch 36, Training Loss: 0.7987437207558575\n",
      "Epoch 37, Training Loss: 0.7987550361717448\n",
      "Epoch 38, Training Loss: 0.7986309122338014\n",
      "Epoch 39, Training Loss: 0.7983976038764505\n",
      "Epoch 40, Training Loss: 0.7984851100164301\n",
      "Epoch 41, Training Loss: 0.7980874357504003\n",
      "Epoch 42, Training Loss: 0.7981253767714781\n",
      "Epoch 43, Training Loss: 0.7978837036385256\n",
      "Epoch 44, Training Loss: 0.7979765353483312\n",
      "Epoch 45, Training Loss: 0.7978541130879346\n",
      "Epoch 46, Training Loss: 0.7974945961727815\n",
      "Epoch 47, Training Loss: 0.797562556477154\n",
      "Epoch 48, Training Loss: 0.7974742010060478\n",
      "Epoch 49, Training Loss: 0.7973587284368627\n",
      "Epoch 50, Training Loss: 0.7971855494555306\n",
      "Epoch 51, Training Loss: 0.7973592107436236\n",
      "Epoch 52, Training Loss: 0.797157661634333\n",
      "Epoch 53, Training Loss: 0.7971209151604596\n",
      "Epoch 54, Training Loss: 0.7968337743422564\n",
      "Epoch 55, Training Loss: 0.7968507187506731\n",
      "Epoch 56, Training Loss: 0.7966517108328203\n",
      "Epoch 57, Training Loss: 0.7963228763552272\n",
      "Epoch 58, Training Loss: 0.7964222530056448\n",
      "Epoch 59, Training Loss: 0.7961472485345953\n",
      "Epoch 60, Training Loss: 0.7959492059314952\n",
      "Epoch 61, Training Loss: 0.7958680213198942\n",
      "Epoch 62, Training Loss: 0.7959433637647068\n",
      "Epoch 63, Training Loss: 0.7957407449974733\n",
      "Epoch 64, Training Loss: 0.7957515263557434\n",
      "Epoch 65, Training Loss: 0.7954722816102645\n",
      "Epoch 66, Training Loss: 0.7951704627626083\n",
      "Epoch 67, Training Loss: 0.7952886961488163\n",
      "Epoch 68, Training Loss: 0.79470711553798\n",
      "Epoch 69, Training Loss: 0.7947442136792575\n",
      "Epoch 70, Training Loss: 0.7948210956769831\n",
      "Epoch 71, Training Loss: 0.7945082947085885\n",
      "Epoch 72, Training Loss: 0.7944180315382341\n",
      "Epoch 73, Training Loss: 0.7940254964547999\n",
      "Epoch 74, Training Loss: 0.794059329103021\n",
      "Epoch 75, Training Loss: 0.7939273739562315\n",
      "Epoch 76, Training Loss: 0.7936931017567129\n",
      "Epoch 77, Training Loss: 0.7935693998897777\n",
      "Epoch 78, Training Loss: 0.7934113198168138\n",
      "Epoch 79, Training Loss: 0.7932123810403487\n",
      "Epoch 80, Training Loss: 0.7929164500096265\n",
      "Epoch 81, Training Loss: 0.7929145009377423\n",
      "Epoch 82, Training Loss: 0.7926495491055882\n",
      "Epoch 83, Training Loss: 0.792590350894367\n",
      "Epoch 84, Training Loss: 0.792281048297882\n",
      "Epoch 85, Training Loss: 0.7921075669456931\n",
      "Epoch 86, Training Loss: 0.791996042728424\n",
      "Epoch 87, Training Loss: 0.7919339559358709\n",
      "Epoch 88, Training Loss: 0.7914638585202834\n",
      "Epoch 89, Training Loss: 0.791535397347282\n",
      "Epoch 90, Training Loss: 0.7912963320928461\n",
      "Epoch 91, Training Loss: 0.7911203379490797\n",
      "Epoch 92, Training Loss: 0.7907741800476523\n",
      "Epoch 93, Training Loss: 0.7909274541630464\n",
      "Epoch 94, Training Loss: 0.7905747653456295\n",
      "Epoch 95, Training Loss: 0.7904622054801268\n",
      "Epoch 96, Training Loss: 0.7899051495860605\n",
      "Epoch 97, Training Loss: 0.7898023514887866\n",
      "Epoch 98, Training Loss: 0.7896562940934125\n",
      "Epoch 99, Training Loss: 0.7895454303657308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:34:58,789] Trial 170 finished with value: 0.6357333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7894169708560494\n",
      "Epoch 1, Training Loss: 0.8620880772296647\n",
      "Epoch 2, Training Loss: 0.8113612110453441\n",
      "Epoch 3, Training Loss: 0.8087707185207453\n",
      "Epoch 4, Training Loss: 0.8052595574156682\n",
      "Epoch 5, Training Loss: 0.8050851121880954\n",
      "Epoch 6, Training Loss: 0.8045263466082121\n",
      "Epoch 7, Training Loss: 0.8027768372593069\n",
      "Epoch 8, Training Loss: 0.8028985167804518\n",
      "Epoch 9, Training Loss: 0.800338478016674\n",
      "Epoch 10, Training Loss: 0.8009041476070433\n",
      "Epoch 11, Training Loss: 0.8002347884321571\n",
      "Epoch 12, Training Loss: 0.7987737935288508\n",
      "Epoch 13, Training Loss: 0.7987234515803201\n",
      "Epoch 14, Training Loss: 0.7994993139926653\n",
      "Epoch 15, Training Loss: 0.7971885862207054\n",
      "Epoch 16, Training Loss: 0.798704759429272\n",
      "Epoch 17, Training Loss: 0.7970041635789369\n",
      "Epoch 18, Training Loss: 0.7969109346543936\n",
      "Epoch 19, Training Loss: 0.7972207953159074\n",
      "Epoch 20, Training Loss: 0.7971614461196096\n",
      "Epoch 21, Training Loss: 0.7972858839465263\n",
      "Epoch 22, Training Loss: 0.7968974951514625\n",
      "Epoch 23, Training Loss: 0.7972306835920291\n",
      "Epoch 24, Training Loss: 0.7964510064376028\n",
      "Epoch 25, Training Loss: 0.7966587877811346\n",
      "Epoch 26, Training Loss: 0.7953470683635626\n",
      "Epoch 27, Training Loss: 0.7972260975299921\n",
      "Epoch 28, Training Loss: 0.796570040498461\n",
      "Epoch 29, Training Loss: 0.7958678456177389\n",
      "Epoch 30, Training Loss: 0.796524032972809\n",
      "Epoch 31, Training Loss: 0.7952858536763299\n",
      "Epoch 32, Training Loss: 0.7948934425985007\n",
      "Epoch 33, Training Loss: 0.7958165873262218\n",
      "Epoch 34, Training Loss: 0.7949877176069675\n",
      "Epoch 35, Training Loss: 0.7946888497897557\n",
      "Epoch 36, Training Loss: 0.7940389666790353\n",
      "Epoch 37, Training Loss: 0.7942718140164712\n",
      "Epoch 38, Training Loss: 0.7957451323817547\n",
      "Epoch 39, Training Loss: 0.7943652289254325\n",
      "Epoch 40, Training Loss: 0.7951363799267246\n",
      "Epoch 41, Training Loss: 0.7932638690883952\n",
      "Epoch 42, Training Loss: 0.7938741758353728\n",
      "Epoch 43, Training Loss: 0.793249123257802\n",
      "Epoch 44, Training Loss: 0.7925912281624357\n",
      "Epoch 45, Training Loss: 0.7929266602473152\n",
      "Epoch 46, Training Loss: 0.7938725563816558\n",
      "Epoch 47, Training Loss: 0.7932863954314612\n",
      "Epoch 48, Training Loss: 0.7933423310294188\n",
      "Epoch 49, Training Loss: 0.7933155422820184\n",
      "Epoch 50, Training Loss: 0.7925827625102566\n",
      "Epoch 51, Training Loss: 0.7924266230791135\n",
      "Epoch 52, Training Loss: 0.792651267159254\n",
      "Epoch 53, Training Loss: 0.794597655938084\n",
      "Epoch 54, Training Loss: 0.7929884726839854\n",
      "Epoch 55, Training Loss: 0.7926494537439561\n",
      "Epoch 56, Training Loss: 0.7929151300200843\n",
      "Epoch 57, Training Loss: 0.7922528946310058\n",
      "Epoch 58, Training Loss: 0.7930454730987548\n",
      "Epoch 59, Training Loss: 0.7919340521769416\n",
      "Epoch 60, Training Loss: 0.7912811221933006\n",
      "Epoch 61, Training Loss: 0.793359394987723\n",
      "Epoch 62, Training Loss: 0.7935557248897123\n",
      "Epoch 63, Training Loss: 0.7922315275758729\n",
      "Epoch 64, Training Loss: 0.792473057965587\n",
      "Epoch 65, Training Loss: 0.7933373515767262\n",
      "Epoch 66, Training Loss: 0.7929043244598504\n",
      "Epoch 67, Training Loss: 0.7922452885405462\n",
      "Epoch 68, Training Loss: 0.7931152911114513\n",
      "Epoch 69, Training Loss: 0.7926177422803148\n",
      "Epoch 70, Training Loss: 0.7932698532154686\n",
      "Epoch 71, Training Loss: 0.7924123690540629\n",
      "Epoch 72, Training Loss: 0.7907684407736125\n",
      "Epoch 73, Training Loss: 0.7913754966025962\n",
      "Epoch 74, Training Loss: 0.7914961607832658\n",
      "Epoch 75, Training Loss: 0.7921919607578364\n",
      "Epoch 76, Training Loss: 0.7922289600945953\n",
      "Epoch 77, Training Loss: 0.7908572962409571\n",
      "Epoch 78, Training Loss: 0.7917246762971233\n",
      "Epoch 79, Training Loss: 0.7911901252610343\n",
      "Epoch 80, Training Loss: 0.7910221755056452\n",
      "Epoch 81, Training Loss: 0.7911088694307141\n",
      "Epoch 82, Training Loss: 0.7922329780750705\n",
      "Epoch 83, Training Loss: 0.790984601096103\n",
      "Epoch 84, Training Loss: 0.7917390480077356\n",
      "Epoch 85, Training Loss: 0.7916770099697257\n",
      "Epoch 86, Training Loss: 0.7917229172878696\n",
      "Epoch 87, Training Loss: 0.7918774193390867\n",
      "Epoch 88, Training Loss: 0.7909026849538761\n",
      "Epoch 89, Training Loss: 0.7904418355540226\n",
      "Epoch 90, Training Loss: 0.7906702432417332\n",
      "Epoch 91, Training Loss: 0.7914595204188412\n",
      "Epoch 92, Training Loss: 0.7909849014497341\n",
      "Epoch 93, Training Loss: 0.7918300374109942\n",
      "Epoch 94, Training Loss: 0.7911119778353469\n",
      "Epoch 95, Training Loss: 0.7913752350592076\n",
      "Epoch 96, Training Loss: 0.7905767910462573\n",
      "Epoch 97, Training Loss: 0.7915150126120202\n",
      "Epoch 98, Training Loss: 0.7914298550526898\n",
      "Epoch 99, Training Loss: 0.7915611033152817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:36:37,278] Trial 171 finished with value: 0.6392666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7914427397842694\n",
      "Epoch 1, Training Loss: 0.9159723147223977\n",
      "Epoch 2, Training Loss: 0.8474475856388316\n",
      "Epoch 3, Training Loss: 0.8218029801985797\n",
      "Epoch 4, Training Loss: 0.8147199055727791\n",
      "Epoch 5, Training Loss: 0.8121175203603856\n",
      "Epoch 6, Training Loss: 0.8108757206271676\n",
      "Epoch 7, Training Loss: 0.809769476371653\n",
      "Epoch 8, Training Loss: 0.8087716794715208\n",
      "Epoch 9, Training Loss: 0.8082017943438362\n",
      "Epoch 10, Training Loss: 0.807890613710179\n",
      "Epoch 11, Training Loss: 0.8074884885900161\n",
      "Epoch 12, Training Loss: 0.8069973375516779\n",
      "Epoch 13, Training Loss: 0.8067637582386241\n",
      "Epoch 14, Training Loss: 0.8062286485643948\n",
      "Epoch 15, Training Loss: 0.8058666950814865\n",
      "Epoch 16, Training Loss: 0.8059680865091436\n",
      "Epoch 17, Training Loss: 0.8053332335808698\n",
      "Epoch 18, Training Loss: 0.805130780023687\n",
      "Epoch 19, Training Loss: 0.8049539174753076\n",
      "Epoch 20, Training Loss: 0.804268469319624\n",
      "Epoch 21, Training Loss: 0.8044108150285832\n",
      "Epoch 22, Training Loss: 0.8042612795970019\n",
      "Epoch 23, Training Loss: 0.8038186458279105\n",
      "Epoch 24, Training Loss: 0.803493292612188\n",
      "Epoch 25, Training Loss: 0.8033125032396877\n",
      "Epoch 26, Training Loss: 0.8031487276974847\n",
      "Epoch 27, Training Loss: 0.8029748386495253\n",
      "Epoch 28, Training Loss: 0.8026476099911858\n",
      "Epoch 29, Training Loss: 0.8024532530588262\n",
      "Epoch 30, Training Loss: 0.8022323487786686\n",
      "Epoch 31, Training Loss: 0.8018312600780936\n",
      "Epoch 32, Training Loss: 0.8018204399417428\n",
      "Epoch 33, Training Loss: 0.8016947702099295\n",
      "Epoch 34, Training Loss: 0.8014580981170429\n",
      "Epoch 35, Training Loss: 0.8015027870150173\n",
      "Epoch 36, Training Loss: 0.8011390280723572\n",
      "Epoch 37, Training Loss: 0.8010372466900769\n",
      "Epoch 38, Training Loss: 0.8010392258447759\n",
      "Epoch 39, Training Loss: 0.8006775397412917\n",
      "Epoch 40, Training Loss: 0.800526458936579\n",
      "Epoch 41, Training Loss: 0.8005257809863371\n",
      "Epoch 42, Training Loss: 0.8002629908393412\n",
      "Epoch 43, Training Loss: 0.800068759988336\n",
      "Epoch 44, Training Loss: 0.7998160614686853\n",
      "Epoch 45, Training Loss: 0.8001546271408305\n",
      "Epoch 46, Training Loss: 0.7995949201724109\n",
      "Epoch 47, Training Loss: 0.7994917537184323\n",
      "Epoch 48, Training Loss: 0.7995726210229537\n",
      "Epoch 49, Training Loss: 0.7992988618682413\n",
      "Epoch 50, Training Loss: 0.7993436040597803\n",
      "Epoch 51, Training Loss: 0.7991063947537366\n",
      "Epoch 52, Training Loss: 0.799138943938648\n",
      "Epoch 53, Training Loss: 0.7990945931041942\n",
      "Epoch 54, Training Loss: 0.7991299279297099\n",
      "Epoch 55, Training Loss: 0.7990924488095676\n",
      "Epoch 56, Training Loss: 0.7986950048979591\n",
      "Epoch 57, Training Loss: 0.7988117358965032\n",
      "Epoch 58, Training Loss: 0.7987628191358903\n",
      "Epoch 59, Training Loss: 0.798763810115702\n",
      "Epoch 60, Training Loss: 0.7986158793814042\n",
      "Epoch 61, Training Loss: 0.7983248267454259\n",
      "Epoch 62, Training Loss: 0.7985394566199359\n",
      "Epoch 63, Training Loss: 0.7983113631080179\n",
      "Epoch 64, Training Loss: 0.7982043382700752\n",
      "Epoch 65, Training Loss: 0.7983543417033028\n",
      "Epoch 66, Training Loss: 0.7983500268880058\n",
      "Epoch 67, Training Loss: 0.7979773762646843\n",
      "Epoch 68, Training Loss: 0.798301665502436\n",
      "Epoch 69, Training Loss: 0.7980713068737704\n",
      "Epoch 70, Training Loss: 0.7981180367049049\n",
      "Epoch 71, Training Loss: 0.7979537400077371\n",
      "Epoch 72, Training Loss: 0.7979295687114492\n",
      "Epoch 73, Training Loss: 0.7979979272449718\n",
      "Epoch 74, Training Loss: 0.7978784157949336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:38:01,257] Trial 172 finished with value: 0.6336 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7979008838709662\n",
      "Epoch 1, Training Loss: 1.0933538837993846\n",
      "Epoch 2, Training Loss: 1.088226090739755\n",
      "Epoch 3, Training Loss: 1.0835572457313538\n",
      "Epoch 4, Training Loss: 1.0791192377314849\n",
      "Epoch 5, Training Loss: 1.0748949387494255\n",
      "Epoch 6, Training Loss: 1.070835275369532\n",
      "Epoch 7, Training Loss: 1.066918586843154\n",
      "Epoch 8, Training Loss: 1.0631514090650223\n",
      "Epoch 9, Training Loss: 1.0594883962238537\n",
      "Epoch 10, Training Loss: 1.0559193507362814\n",
      "Epoch 11, Training Loss: 1.0524586883713218\n",
      "Epoch 12, Training Loss: 1.049055634947384\n",
      "Epoch 13, Training Loss: 1.0457734286785125\n",
      "Epoch 14, Training Loss: 1.0425381093165453\n",
      "Epoch 15, Training Loss: 1.0393587505817414\n",
      "Epoch 16, Training Loss: 1.0362898161130794\n",
      "Epoch 17, Training Loss: 1.0332702237718245\n",
      "Epoch 18, Training Loss: 1.0303189901043388\n",
      "Epoch 19, Training Loss: 1.0274199449314791\n",
      "Epoch 20, Training Loss: 1.0246128645363977\n",
      "Epoch 21, Training Loss: 1.0218586131404428\n",
      "Epoch 22, Training Loss: 1.0191617138946758\n",
      "Epoch 23, Training Loss: 1.0165446413965786\n",
      "Epoch 24, Training Loss: 1.0139936227658215\n",
      "Epoch 25, Training Loss: 1.011519127032336\n",
      "Epoch 26, Training Loss: 1.0091170389512005\n",
      "Epoch 27, Training Loss: 1.0067773504818187\n",
      "Epoch 28, Training Loss: 1.0045160993407753\n",
      "Epoch 29, Training Loss: 1.002323399291319\n",
      "Epoch 30, Training Loss: 1.0002068298003253\n",
      "Epoch 31, Training Loss: 0.9981717235200546\n",
      "Epoch 32, Training Loss: 0.9961945918728323\n",
      "Epoch 33, Training Loss: 0.9942995456387015\n",
      "Epoch 34, Training Loss: 0.9924800604932449\n",
      "Epoch 35, Training Loss: 0.9907256050670848\n",
      "Epoch 36, Training Loss: 0.9890494432168848\n",
      "Epoch 37, Training Loss: 0.9874421973088209\n",
      "Epoch 38, Training Loss: 0.9858971304052017\n",
      "Epoch 39, Training Loss: 0.9844378336738138\n",
      "Epoch 40, Training Loss: 0.9830325976540061\n",
      "Epoch 41, Training Loss: 0.9816866702893201\n",
      "Epoch 42, Training Loss: 0.9804023605935713\n",
      "Epoch 43, Training Loss: 0.9792025080147911\n",
      "Epoch 44, Training Loss: 0.9780495997737436\n",
      "Epoch 45, Training Loss: 0.9769544327960294\n",
      "Epoch 46, Training Loss: 0.9759043092587415\n",
      "Epoch 47, Training Loss: 0.9749267334797803\n",
      "Epoch 48, Training Loss: 0.9739783942699433\n",
      "Epoch 49, Training Loss: 0.9731037856550777\n",
      "Epoch 50, Training Loss: 0.9722559233974007\n",
      "Epoch 51, Training Loss: 0.9714482210664188\n",
      "Epoch 52, Training Loss: 0.9707037889957428\n",
      "Epoch 53, Training Loss: 0.9699726121565875\n",
      "Epoch 54, Training Loss: 0.969321466403849\n",
      "Epoch 55, Training Loss: 0.9686743404584772\n",
      "Epoch 56, Training Loss: 0.9680722976432127\n",
      "Epoch 57, Training Loss: 0.9674919903278351\n",
      "Epoch 58, Training Loss: 0.966954090384876\n",
      "Epoch 59, Training Loss: 0.9664384698166567\n",
      "Epoch 60, Training Loss: 0.9659528381684247\n",
      "Epoch 61, Training Loss: 0.9654974781765657\n",
      "Epoch 62, Training Loss: 0.9650526163858526\n",
      "Epoch 63, Training Loss: 0.9646509912434746\n",
      "Epoch 64, Training Loss: 0.9642527518552892\n",
      "Epoch 65, Training Loss: 0.9638804425211513\n",
      "Epoch 66, Training Loss: 0.9635352124887354\n",
      "Epoch 67, Training Loss: 0.963192920404322\n",
      "Epoch 68, Training Loss: 0.9628706127054552\n",
      "Epoch 69, Training Loss: 0.9625723396329319\n",
      "Epoch 70, Training Loss: 0.9622739543634302\n",
      "Epoch 71, Training Loss: 0.9620012551195481\n",
      "Epoch 72, Training Loss: 0.9617360462160671\n",
      "Epoch 73, Training Loss: 0.9614837238367866\n",
      "Epoch 74, Training Loss: 0.9612368601911209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:39:09,586] Trial 173 finished with value: 0.5266666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9610099445371066\n",
      "Epoch 1, Training Loss: 0.9748418537308188\n",
      "Epoch 2, Training Loss: 0.9296841940459083\n",
      "Epoch 3, Training Loss: 0.8803420799620011\n",
      "Epoch 4, Training Loss: 0.835339925289154\n",
      "Epoch 5, Training Loss: 0.821017156418632\n",
      "Epoch 6, Training Loss: 0.8154721408731798\n",
      "Epoch 7, Training Loss: 0.8126092918480143\n",
      "Epoch 8, Training Loss: 0.8107261877901414\n",
      "Epoch 9, Training Loss: 0.8092443971774157\n",
      "Epoch 10, Training Loss: 0.8075938253542956\n",
      "Epoch 11, Training Loss: 0.8064895212650299\n",
      "Epoch 12, Training Loss: 0.8052465532106512\n",
      "Epoch 13, Training Loss: 0.805149274854099\n",
      "Epoch 14, Training Loss: 0.8035795633231892\n",
      "Epoch 15, Training Loss: 0.8032309946593116\n",
      "Epoch 16, Training Loss: 0.8024476692255805\n",
      "Epoch 17, Training Loss: 0.8014155891362359\n",
      "Epoch 18, Training Loss: 0.8016035747528076\n",
      "Epoch 19, Training Loss: 0.8010460309421314\n",
      "Epoch 20, Training Loss: 0.800572379547007\n",
      "Epoch 21, Training Loss: 0.8004518416348626\n",
      "Epoch 22, Training Loss: 0.8002087282433229\n",
      "Epoch 23, Training Loss: 0.7997488786192501\n",
      "Epoch 24, Training Loss: 0.7991986500515658\n",
      "Epoch 25, Training Loss: 0.798464714569204\n",
      "Epoch 26, Training Loss: 0.7984581669639138\n",
      "Epoch 27, Training Loss: 0.798077724611058\n",
      "Epoch 28, Training Loss: 0.7972878332699046\n",
      "Epoch 29, Training Loss: 0.7972959993166082\n",
      "Epoch 30, Training Loss: 0.7962839118873372\n",
      "Epoch 31, Training Loss: 0.7960630772394293\n",
      "Epoch 32, Training Loss: 0.7957538232382606\n",
      "Epoch 33, Training Loss: 0.7947213094374713\n",
      "Epoch 34, Training Loss: 0.7944695563176098\n",
      "Epoch 35, Training Loss: 0.7934153553317574\n",
      "Epoch 36, Training Loss: 0.7928636037602144\n",
      "Epoch 37, Training Loss: 0.7922395903923932\n",
      "Epoch 38, Training Loss: 0.7911926392947927\n",
      "Epoch 39, Training Loss: 0.7906357814283932\n",
      "Epoch 40, Training Loss: 0.790150910615921\n",
      "Epoch 41, Training Loss: 0.7894588093196645\n",
      "Epoch 42, Training Loss: 0.7887599569208482\n",
      "Epoch 43, Training Loss: 0.7885790876080008\n",
      "Epoch 44, Training Loss: 0.7878265824037439\n",
      "Epoch 45, Training Loss: 0.7872648651459638\n",
      "Epoch 46, Training Loss: 0.7873231182378881\n",
      "Epoch 47, Training Loss: 0.787200476141537\n",
      "Epoch 48, Training Loss: 0.786661307110506\n",
      "Epoch 49, Training Loss: 0.7864101926719441\n",
      "Epoch 50, Training Loss: 0.7863056443018072\n",
      "Epoch 51, Training Loss: 0.7864090736473308\n",
      "Epoch 52, Training Loss: 0.7860579225596259\n",
      "Epoch 53, Training Loss: 0.7857539601185742\n",
      "Epoch 54, Training Loss: 0.7858515447728774\n",
      "Epoch 55, Training Loss: 0.7853095819669611\n",
      "Epoch 56, Training Loss: 0.7854217424813439\n",
      "Epoch 57, Training Loss: 0.7855263306814082\n",
      "Epoch 58, Training Loss: 0.7852579403624815\n",
      "Epoch 59, Training Loss: 0.7852813926163842\n",
      "Epoch 60, Training Loss: 0.7851265588928672\n",
      "Epoch 61, Training Loss: 0.78501562518232\n",
      "Epoch 62, Training Loss: 0.7849198003376231\n",
      "Epoch 63, Training Loss: 0.7849017888658187\n",
      "Epoch 64, Training Loss: 0.7848034503179439\n",
      "Epoch 65, Training Loss: 0.7853005692538093\n",
      "Epoch 66, Training Loss: 0.7847475957169252\n",
      "Epoch 67, Training Loss: 0.784397178888321\n",
      "Epoch 68, Training Loss: 0.7843169077003703\n",
      "Epoch 69, Training Loss: 0.7843784581212436\n",
      "Epoch 70, Training Loss: 0.7843517686338985\n",
      "Epoch 71, Training Loss: 0.7846881081076229\n",
      "Epoch 72, Training Loss: 0.7842689130586736\n",
      "Epoch 73, Training Loss: 0.7841970141494975\n",
      "Epoch 74, Training Loss: 0.7840110125261195\n",
      "Epoch 75, Training Loss: 0.7841393557716818\n",
      "Epoch 76, Training Loss: 0.7839838190639721\n",
      "Epoch 77, Training Loss: 0.7841312597078436\n",
      "Epoch 78, Training Loss: 0.7838940188463996\n",
      "Epoch 79, Training Loss: 0.7839404015681323\n",
      "Epoch 80, Training Loss: 0.7837992618364447\n",
      "Epoch 81, Training Loss: 0.7838687531387105\n",
      "Epoch 82, Training Loss: 0.783639048127567\n",
      "Epoch 83, Training Loss: 0.7836976252583896\n",
      "Epoch 84, Training Loss: 0.7835076834874994\n",
      "Epoch 85, Training Loss: 0.7836083538392011\n",
      "Epoch 86, Training Loss: 0.7835058133041157\n",
      "Epoch 87, Training Loss: 0.7834956427181469\n",
      "Epoch 88, Training Loss: 0.7836406311568092\n",
      "Epoch 89, Training Loss: 0.783456687296138\n",
      "Epoch 90, Training Loss: 0.7832154781678143\n",
      "Epoch 91, Training Loss: 0.783025819974787\n",
      "Epoch 92, Training Loss: 0.7831961663330302\n",
      "Epoch 93, Training Loss: 0.7833118968150194\n",
      "Epoch 94, Training Loss: 0.783277758289786\n",
      "Epoch 95, Training Loss: 0.783268678819432\n",
      "Epoch 96, Training Loss: 0.7833778178691864\n",
      "Epoch 97, Training Loss: 0.783088202196009\n",
      "Epoch 98, Training Loss: 0.7832145651649026\n",
      "Epoch 99, Training Loss: 0.7830587290315068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:41:10,921] Trial 174 finished with value: 0.6398 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.783121123384027\n",
      "Epoch 1, Training Loss: 1.0896689273360978\n",
      "Epoch 2, Training Loss: 1.0864134365454652\n",
      "Epoch 3, Training Loss: 1.0824848085417784\n",
      "Epoch 4, Training Loss: 1.0771523993714411\n",
      "Epoch 5, Training Loss: 1.0693197216306414\n",
      "Epoch 6, Training Loss: 1.0578201503681957\n",
      "Epoch 7, Training Loss: 1.042244339527044\n",
      "Epoch 8, Training Loss: 1.023674102026717\n",
      "Epoch 9, Training Loss: 1.0061402310106091\n",
      "Epoch 10, Training Loss: 0.992288953558843\n",
      "Epoch 11, Training Loss: 0.9826713987759181\n",
      "Epoch 12, Training Loss: 0.9751895422325995\n",
      "Epoch 13, Training Loss: 0.9693157156607262\n",
      "Epoch 14, Training Loss: 0.9645248720520421\n",
      "Epoch 15, Training Loss: 0.9608764660089536\n",
      "Epoch 16, Training Loss: 0.9590194021848808\n",
      "Epoch 17, Training Loss: 0.9565950387402584\n",
      "Epoch 18, Training Loss: 0.9550885650448333\n",
      "Epoch 19, Training Loss: 0.9537652833121163\n",
      "Epoch 20, Training Loss: 0.9518019807966132\n",
      "Epoch 21, Training Loss: 0.9506549730336755\n",
      "Epoch 22, Training Loss: 0.9492275272993217\n",
      "Epoch 23, Training Loss: 0.9475550818264036\n",
      "Epoch 24, Training Loss: 0.9462460806495265\n",
      "Epoch 25, Training Loss: 0.9441437139546961\n",
      "Epoch 26, Training Loss: 0.9423779747539893\n",
      "Epoch 27, Training Loss: 0.940933586690659\n",
      "Epoch 28, Training Loss: 0.9389316386746285\n",
      "Epoch 29, Training Loss: 0.9363686661971243\n",
      "Epoch 30, Training Loss: 0.934589742000838\n",
      "Epoch 31, Training Loss: 0.931998584503518\n",
      "Epoch 32, Training Loss: 0.9293667442816541\n",
      "Epoch 33, Training Loss: 0.9263334986858798\n",
      "Epoch 34, Training Loss: 0.9231958298754871\n",
      "Epoch 35, Training Loss: 0.9195282331982949\n",
      "Epoch 36, Training Loss: 0.9160785935875169\n",
      "Epoch 37, Training Loss: 0.9125166093496452\n",
      "Epoch 38, Training Loss: 0.9082002641563128\n",
      "Epoch 39, Training Loss: 0.903942692907233\n",
      "Epoch 40, Training Loss: 0.8988688476103589\n",
      "Epoch 41, Training Loss: 0.8941692009904331\n",
      "Epoch 42, Training Loss: 0.8887447133100123\n",
      "Epoch 43, Training Loss: 0.8830506883169476\n",
      "Epoch 44, Training Loss: 0.8780416911705993\n",
      "Epoch 45, Training Loss: 0.8722401294493137\n",
      "Epoch 46, Training Loss: 0.8673071109262624\n",
      "Epoch 47, Training Loss: 0.862108280067157\n",
      "Epoch 48, Training Loss: 0.8570926960249593\n",
      "Epoch 49, Training Loss: 0.8526255017832706\n",
      "Epoch 50, Training Loss: 0.8480047139906346\n",
      "Epoch 51, Training Loss: 0.8448440755220284\n",
      "Epoch 52, Training Loss: 0.8404831825342394\n",
      "Epoch 53, Training Loss: 0.8376394428704914\n",
      "Epoch 54, Training Loss: 0.8350559406710747\n",
      "Epoch 55, Training Loss: 0.8324869898028839\n",
      "Epoch 56, Training Loss: 0.8299421007471873\n",
      "Epoch 57, Training Loss: 0.8278012352778499\n",
      "Epoch 58, Training Loss: 0.8258681239042067\n",
      "Epoch 59, Training Loss: 0.8245933159849698\n",
      "Epoch 60, Training Loss: 0.8235281826858234\n",
      "Epoch 61, Training Loss: 0.8224834334581418\n",
      "Epoch 62, Training Loss: 0.8204455863264271\n",
      "Epoch 63, Training Loss: 0.8197453891424308\n",
      "Epoch 64, Training Loss: 0.8189717517759567\n",
      "Epoch 65, Training Loss: 0.817993412430125\n",
      "Epoch 66, Training Loss: 0.8171923251080334\n",
      "Epoch 67, Training Loss: 0.8169170473751269\n",
      "Epoch 68, Training Loss: 0.8156546374908964\n",
      "Epoch 69, Training Loss: 0.8147021710424495\n",
      "Epoch 70, Training Loss: 0.8140627567929433\n",
      "Epoch 71, Training Loss: 0.8138177234427373\n",
      "Epoch 72, Training Loss: 0.8133001215475842\n",
      "Epoch 73, Training Loss: 0.812753585137819\n",
      "Epoch 74, Training Loss: 0.8122302415675686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:42:18,881] Trial 175 finished with value: 0.6268666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.812039505897608\n",
      "Epoch 1, Training Loss: 0.8738432752384859\n",
      "Epoch 2, Training Loss: 0.8119462847709655\n",
      "Epoch 3, Training Loss: 0.8063271303737864\n",
      "Epoch 4, Training Loss: 0.803768487327239\n",
      "Epoch 5, Training Loss: 0.8016744754594916\n",
      "Epoch 6, Training Loss: 0.7990376974554623\n",
      "Epoch 7, Training Loss: 0.796627061437158\n",
      "Epoch 8, Training Loss: 0.7951683401360231\n",
      "Epoch 9, Training Loss: 0.793744608444326\n",
      "Epoch 10, Training Loss: 0.7920988489599788\n",
      "Epoch 11, Training Loss: 0.7913002599688137\n",
      "Epoch 12, Training Loss: 0.7898828257532681\n",
      "Epoch 13, Training Loss: 0.7893142864984625\n",
      "Epoch 14, Training Loss: 0.7892267159854665\n",
      "Epoch 15, Training Loss: 0.7887388351384331\n",
      "Epoch 16, Training Loss: 0.7875858774605919\n",
      "Epoch 17, Training Loss: 0.7874712455272674\n",
      "Epoch 18, Training Loss: 0.7871305316336015\n",
      "Epoch 19, Training Loss: 0.786362960619085\n",
      "Epoch 20, Training Loss: 0.7862096182037802\n",
      "Epoch 21, Training Loss: 0.7858326741527109\n",
      "Epoch 22, Training Loss: 0.7853077185153962\n",
      "Epoch 23, Training Loss: 0.7851195176208721\n",
      "Epoch 24, Training Loss: 0.7849230736143449\n",
      "Epoch 25, Training Loss: 0.7848079566394581\n",
      "Epoch 26, Training Loss: 0.7845558646145989\n",
      "Epoch 27, Training Loss: 0.7843299175711239\n",
      "Epoch 28, Training Loss: 0.7834494830580319\n",
      "Epoch 29, Training Loss: 0.7839935762741986\n",
      "Epoch 30, Training Loss: 0.7836390222521389\n",
      "Epoch 31, Training Loss: 0.7833388506664949\n",
      "Epoch 32, Training Loss: 0.7831139608691721\n",
      "Epoch 33, Training Loss: 0.7834596500677221\n",
      "Epoch 34, Training Loss: 0.78318079513662\n",
      "Epoch 35, Training Loss: 0.7827715019618764\n",
      "Epoch 36, Training Loss: 0.7826744552219616\n",
      "Epoch 37, Training Loss: 0.7823683721878949\n",
      "Epoch 38, Training Loss: 0.7821590288947611\n",
      "Epoch 39, Training Loss: 0.7819828594432158\n",
      "Epoch 40, Training Loss: 0.7816103570601519\n",
      "Epoch 41, Training Loss: 0.7821690703840817\n",
      "Epoch 42, Training Loss: 0.7817070376873017\n",
      "Epoch 43, Training Loss: 0.7817182377506705\n",
      "Epoch 44, Training Loss: 0.7813968524512123\n",
      "Epoch 45, Training Loss: 0.7812587085892172\n",
      "Epoch 46, Training Loss: 0.7809921376144184\n",
      "Epoch 47, Training Loss: 0.7808819890022278\n",
      "Epoch 48, Training Loss: 0.7811695159182829\n",
      "Epoch 49, Training Loss: 0.7806928239149206\n",
      "Epoch 50, Training Loss: 0.7807978113959817\n",
      "Epoch 51, Training Loss: 0.780829995099236\n",
      "Epoch 52, Training Loss: 0.7804579085462233\n",
      "Epoch 53, Training Loss: 0.7804690998442033\n",
      "Epoch 54, Training Loss: 0.7802280083123375\n",
      "Epoch 55, Training Loss: 0.7803789467671338\n",
      "Epoch 56, Training Loss: 0.7797367028629079\n",
      "Epoch 57, Training Loss: 0.7803392629763659\n",
      "Epoch 58, Training Loss: 0.7797646722372841\n",
      "Epoch 59, Training Loss: 0.7797563175594106\n",
      "Epoch 60, Training Loss: 0.7796450363187228\n",
      "Epoch 61, Training Loss: 0.7795535586160772\n",
      "Epoch 62, Training Loss: 0.7797466688997605\n",
      "Epoch 63, Training Loss: 0.7793617587931015\n",
      "Epoch 64, Training Loss: 0.779201428118874\n",
      "Epoch 65, Training Loss: 0.7792040700070998\n",
      "Epoch 66, Training Loss: 0.7789579243519726\n",
      "Epoch 67, Training Loss: 0.7790144354455611\n",
      "Epoch 68, Training Loss: 0.7787315212277806\n",
      "Epoch 69, Training Loss: 0.779322300307891\n",
      "Epoch 70, Training Loss: 0.7789078978229972\n",
      "Epoch 71, Training Loss: 0.7788277285239276\n",
      "Epoch 72, Training Loss: 0.7787035010141485\n",
      "Epoch 73, Training Loss: 0.7784217933346244\n",
      "Epoch 74, Training Loss: 0.7784994812572703\n",
      "Epoch 75, Training Loss: 0.7782867463897256\n",
      "Epoch 76, Training Loss: 0.7785637033686919\n",
      "Epoch 77, Training Loss: 0.7783860724813798\n",
      "Epoch 78, Training Loss: 0.7782637046365177\n",
      "Epoch 79, Training Loss: 0.7784440058119156\n",
      "Epoch 80, Training Loss: 0.7783996338704053\n",
      "Epoch 81, Training Loss: 0.7778192907221178\n",
      "Epoch 82, Training Loss: 0.7783075597005732\n",
      "Epoch 83, Training Loss: 0.7780172124329735\n",
      "Epoch 84, Training Loss: 0.7782989292986253\n",
      "Epoch 85, Training Loss: 0.7779705743929919\n",
      "Epoch 86, Training Loss: 0.7779713827020982\n",
      "Epoch 87, Training Loss: 0.7780649474789114\n",
      "Epoch 88, Training Loss: 0.7777515192592845\n",
      "Epoch 89, Training Loss: 0.7776907667692969\n",
      "Epoch 90, Training Loss: 0.7777493140977971\n",
      "Epoch 91, Training Loss: 0.777518289159326\n",
      "Epoch 92, Training Loss: 0.7778533401208766\n",
      "Epoch 93, Training Loss: 0.7775796555070316\n",
      "Epoch 94, Training Loss: 0.7773052901380202\n",
      "Epoch 95, Training Loss: 0.7774224828972536\n",
      "Epoch 96, Training Loss: 0.7776550557332881\n",
      "Epoch 97, Training Loss: 0.7772625805349911\n",
      "Epoch 98, Training Loss: 0.7771508746287402\n",
      "Epoch 99, Training Loss: 0.7773061900980333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:44:25,750] Trial 176 finished with value: 0.6406 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7772736886669608\n",
      "Epoch 1, Training Loss: 0.8777849797019385\n",
      "Epoch 2, Training Loss: 0.8189464667685946\n",
      "Epoch 3, Training Loss: 0.8137162559014514\n",
      "Epoch 4, Training Loss: 0.8097501800472575\n",
      "Epoch 5, Training Loss: 0.8101422276712001\n",
      "Epoch 6, Training Loss: 0.8083244675980474\n",
      "Epoch 7, Training Loss: 0.8078855777145328\n",
      "Epoch 8, Training Loss: 0.8070555714736307\n",
      "Epoch 9, Training Loss: 0.8064916811491314\n",
      "Epoch 10, Training Loss: 0.8055796425145372\n",
      "Epoch 11, Training Loss: 0.8063811752132903\n",
      "Epoch 12, Training Loss: 0.8060533327267583\n",
      "Epoch 13, Training Loss: 0.8053731849319057\n",
      "Epoch 14, Training Loss: 0.8044863706244562\n",
      "Epoch 15, Training Loss: 0.8053683335619761\n",
      "Epoch 16, Training Loss: 0.8049099706169358\n",
      "Epoch 17, Training Loss: 0.8042827833864026\n",
      "Epoch 18, Training Loss: 0.8033289697833528\n",
      "Epoch 19, Training Loss: 0.803781805450755\n",
      "Epoch 20, Training Loss: 0.8034794797574667\n",
      "Epoch 21, Training Loss: 0.8029951490853963\n",
      "Epoch 22, Training Loss: 0.802909555291771\n",
      "Epoch 23, Training Loss: 0.8017443713388945\n",
      "Epoch 24, Training Loss: 0.8007705761077709\n",
      "Epoch 25, Training Loss: 0.8008430640052135\n",
      "Epoch 26, Training Loss: 0.8010709890745636\n",
      "Epoch 27, Training Loss: 0.8008780453438149\n",
      "Epoch 28, Training Loss: 0.8022010520884865\n",
      "Epoch 29, Training Loss: 0.8004021519108823\n",
      "Epoch 30, Training Loss: 0.7992829518210619\n",
      "Epoch 31, Training Loss: 0.7993056489112682\n",
      "Epoch 32, Training Loss: 0.799906595577871\n",
      "Epoch 33, Training Loss: 0.7986668262266575\n",
      "Epoch 34, Training Loss: 0.7989703331674848\n",
      "Epoch 35, Training Loss: 0.7984823194661534\n",
      "Epoch 36, Training Loss: 0.7986664521066766\n",
      "Epoch 37, Training Loss: 0.798660805619749\n",
      "Epoch 38, Training Loss: 0.7969384842349174\n",
      "Epoch 39, Training Loss: 0.796111838351515\n",
      "Epoch 40, Training Loss: 0.7979336807602331\n",
      "Epoch 41, Training Loss: 0.797048697077242\n",
      "Epoch 42, Training Loss: 0.7970204560380233\n",
      "Epoch 43, Training Loss: 0.7955691165493843\n",
      "Epoch 44, Training Loss: 0.7959978842197504\n",
      "Epoch 45, Training Loss: 0.7956349431123949\n",
      "Epoch 46, Training Loss: 0.7953227069144858\n",
      "Epoch 47, Training Loss: 0.7949641005437177\n",
      "Epoch 48, Training Loss: 0.7955611116904066\n",
      "Epoch 49, Training Loss: 0.7952041366942844\n",
      "Epoch 50, Training Loss: 0.7938421513353076\n",
      "Epoch 51, Training Loss: 0.7940009561696447\n",
      "Epoch 52, Training Loss: 0.7953259618658769\n",
      "Epoch 53, Training Loss: 0.7950217890560178\n",
      "Epoch 54, Training Loss: 0.795289525143186\n",
      "Epoch 55, Training Loss: 0.7948901846892852\n",
      "Epoch 56, Training Loss: 0.7948040788335011\n",
      "Epoch 57, Training Loss: 0.7935864516666957\n",
      "Epoch 58, Training Loss: 0.7936068448805271\n",
      "Epoch 59, Training Loss: 0.7934886375764258\n",
      "Epoch 60, Training Loss: 0.7936287653177304\n",
      "Epoch 61, Training Loss: 0.7935985156467983\n",
      "Epoch 62, Training Loss: 0.7928695607006102\n",
      "Epoch 63, Training Loss: 0.7931738767408787\n",
      "Epoch 64, Training Loss: 0.7926683231403954\n",
      "Epoch 65, Training Loss: 0.7941890943319277\n",
      "Epoch 66, Training Loss: 0.7920137364613382\n",
      "Epoch 67, Training Loss: 0.792680460573139\n",
      "Epoch 68, Training Loss: 0.7927748209551762\n",
      "Epoch 69, Training Loss: 0.7932281128445963\n",
      "Epoch 70, Training Loss: 0.7921029417138351\n",
      "Epoch 71, Training Loss: 0.7917819741973303\n",
      "Epoch 72, Training Loss: 0.792009236938075\n",
      "Epoch 73, Training Loss: 0.7924165403932557\n",
      "Epoch 74, Training Loss: 0.7921015878369038\n",
      "Epoch 75, Training Loss: 0.7920939974318769\n",
      "Epoch 76, Training Loss: 0.7918313098133059\n",
      "Epoch 77, Training Loss: 0.7920166617945621\n",
      "Epoch 78, Training Loss: 0.7921353405579589\n",
      "Epoch 79, Training Loss: 0.7918152698896881\n",
      "Epoch 80, Training Loss: 0.7924690658884838\n",
      "Epoch 81, Training Loss: 0.7913879540181697\n",
      "Epoch 82, Training Loss: 0.7908372644643138\n",
      "Epoch 83, Training Loss: 0.7916985968001803\n",
      "Epoch 84, Training Loss: 0.7922052219398039\n",
      "Epoch 85, Training Loss: 0.7914208924859987\n",
      "Epoch 86, Training Loss: 0.792375920320812\n",
      "Epoch 87, Training Loss: 0.7910582602472234\n",
      "Epoch 88, Training Loss: 0.7914298827486826\n",
      "Epoch 89, Training Loss: 0.7907861160156422\n",
      "Epoch 90, Training Loss: 0.7910182311122579\n",
      "Epoch 91, Training Loss: 0.7913723875705461\n",
      "Epoch 92, Training Loss: 0.7909435126118194\n",
      "Epoch 93, Training Loss: 0.7917948688779558\n",
      "Epoch 94, Training Loss: 0.7909562827949237\n",
      "Epoch 95, Training Loss: 0.7916370315659315\n",
      "Epoch 96, Training Loss: 0.7902940766255658\n",
      "Epoch 97, Training Loss: 0.7919443710406023\n",
      "Epoch 98, Training Loss: 0.7914173689103664\n",
      "Epoch 99, Training Loss: 0.7904336312659701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:46:05,252] Trial 177 finished with value: 0.6377333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7908789648149247\n",
      "Epoch 1, Training Loss: 1.0335682375290816\n",
      "Epoch 2, Training Loss: 0.9560587388627669\n",
      "Epoch 3, Training Loss: 0.9324964777862325\n",
      "Epoch 4, Training Loss: 0.922254629836363\n",
      "Epoch 5, Training Loss: 0.9138704166692846\n",
      "Epoch 6, Training Loss: 0.9056440476109\n",
      "Epoch 7, Training Loss: 0.8959952657362994\n",
      "Epoch 8, Training Loss: 0.8837339064654182\n",
      "Epoch 9, Training Loss: 0.868100675204221\n",
      "Epoch 10, Training Loss: 0.8501388486693887\n",
      "Epoch 11, Training Loss: 0.8332443215566523\n",
      "Epoch 12, Training Loss: 0.8208163022994995\n",
      "Epoch 13, Training Loss: 0.8133862792043125\n",
      "Epoch 14, Training Loss: 0.8091068500630996\n",
      "Epoch 15, Training Loss: 0.8063313191077288\n",
      "Epoch 16, Training Loss: 0.8052903223738951\n",
      "Epoch 17, Training Loss: 0.8039743149280548\n",
      "Epoch 18, Training Loss: 0.8033384791542502\n",
      "Epoch 19, Training Loss: 0.8027703477354611\n",
      "Epoch 20, Training Loss: 0.8022153972878175\n",
      "Epoch 21, Training Loss: 0.8018667340979857\n",
      "Epoch 22, Training Loss: 0.8010802454808179\n",
      "Epoch 23, Training Loss: 0.8007879562938914\n",
      "Epoch 24, Training Loss: 0.8003700553669649\n",
      "Epoch 25, Training Loss: 0.7998777294158935\n",
      "Epoch 26, Training Loss: 0.7996509765877443\n",
      "Epoch 27, Training Loss: 0.7990215265750885\n",
      "Epoch 28, Training Loss: 0.798845956325531\n",
      "Epoch 29, Training Loss: 0.7983281738617841\n",
      "Epoch 30, Training Loss: 0.7981525583828197\n",
      "Epoch 31, Training Loss: 0.7977788576659034\n",
      "Epoch 32, Training Loss: 0.7974935280575471\n",
      "Epoch 33, Training Loss: 0.7970177179224351\n",
      "Epoch 34, Training Loss: 0.7969449823042926\n",
      "Epoch 35, Training Loss: 0.7965664867793812\n",
      "Epoch 36, Training Loss: 0.7966341169441448\n",
      "Epoch 37, Training Loss: 0.7961134759117575\n",
      "Epoch 38, Training Loss: 0.7960419492160573\n",
      "Epoch 39, Training Loss: 0.7957322521770702\n",
      "Epoch 40, Training Loss: 0.7951956364687751\n",
      "Epoch 41, Training Loss: 0.7951616037593169\n",
      "Epoch 42, Training Loss: 0.7948154428425958\n",
      "Epoch 43, Training Loss: 0.794519371285158\n",
      "Epoch 44, Training Loss: 0.794563597651089\n",
      "Epoch 45, Training Loss: 0.7943599996847265\n",
      "Epoch 46, Training Loss: 0.7940510472129373\n",
      "Epoch 47, Training Loss: 0.7939891828508938\n",
      "Epoch 48, Training Loss: 0.7937993776798248\n",
      "Epoch 49, Training Loss: 0.7934666556470534\n",
      "Epoch 50, Training Loss: 0.7934308245602776\n",
      "Epoch 51, Training Loss: 0.79302962604691\n",
      "Epoch 52, Training Loss: 0.7929062032699585\n",
      "Epoch 53, Training Loss: 0.7927280101355384\n",
      "Epoch 54, Training Loss: 0.7924551587946275\n",
      "Epoch 55, Training Loss: 0.7924135180080638\n",
      "Epoch 56, Training Loss: 0.7923488424806033\n",
      "Epoch 57, Training Loss: 0.7921161939116085\n",
      "Epoch 58, Training Loss: 0.7919129454388338\n",
      "Epoch 59, Training Loss: 0.791635947858586\n",
      "Epoch 60, Training Loss: 0.7915674666096183\n",
      "Epoch 61, Training Loss: 0.7913664042949676\n",
      "Epoch 62, Training Loss: 0.7911041885965011\n",
      "Epoch 63, Training Loss: 0.7907636391415316\n",
      "Epoch 64, Training Loss: 0.7904813718795777\n",
      "Epoch 65, Training Loss: 0.7904612797849319\n",
      "Epoch 66, Training Loss: 0.7902600728764253\n",
      "Epoch 67, Training Loss: 0.7902790926484501\n",
      "Epoch 68, Training Loss: 0.7899893966141869\n",
      "Epoch 69, Training Loss: 0.7895526426679947\n",
      "Epoch 70, Training Loss: 0.7894057388866649\n",
      "Epoch 71, Training Loss: 0.7893145496705\n",
      "Epoch 72, Training Loss: 0.7891025284458609\n",
      "Epoch 73, Training Loss: 0.7891615728069754\n",
      "Epoch 74, Training Loss: 0.7886983419867123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:47:25,132] Trial 178 finished with value: 0.6377333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7886637411398046\n",
      "Epoch 1, Training Loss: 0.9926619043027548\n",
      "Epoch 2, Training Loss: 0.9400085297742284\n",
      "Epoch 3, Training Loss: 0.9081955905247452\n",
      "Epoch 4, Training Loss: 0.8549952676421717\n",
      "Epoch 5, Training Loss: 0.8241999933594152\n",
      "Epoch 6, Training Loss: 0.8170789389681995\n",
      "Epoch 7, Training Loss: 0.8147965882057534\n",
      "Epoch 8, Training Loss: 0.8141756566843592\n",
      "Epoch 9, Training Loss: 0.8125249531932344\n",
      "Epoch 10, Training Loss: 0.8119014240745315\n",
      "Epoch 11, Training Loss: 0.8119779548250643\n",
      "Epoch 12, Training Loss: 0.8096498939327728\n",
      "Epoch 13, Training Loss: 0.8093215847373905\n",
      "Epoch 14, Training Loss: 0.8082465950707743\n",
      "Epoch 15, Training Loss: 0.8077189242929445\n",
      "Epoch 16, Training Loss: 0.8075711906404424\n",
      "Epoch 17, Training Loss: 0.8064269323994343\n",
      "Epoch 18, Training Loss: 0.8053176461305833\n",
      "Epoch 19, Training Loss: 0.8056901257737239\n",
      "Epoch 20, Training Loss: 0.8048363155888435\n",
      "Epoch 21, Training Loss: 0.8051770839476048\n",
      "Epoch 22, Training Loss: 0.8045855748922305\n",
      "Epoch 23, Training Loss: 0.8037939417631106\n",
      "Epoch 24, Training Loss: 0.8033150647815905\n",
      "Epoch 25, Training Loss: 0.8032149533580121\n",
      "Epoch 26, Training Loss: 0.8032567433844832\n",
      "Epoch 27, Training Loss: 0.802425778001771\n",
      "Epoch 28, Training Loss: 0.8024048738909844\n",
      "Epoch 29, Training Loss: 0.8022307317059739\n",
      "Epoch 30, Training Loss: 0.8013837367968452\n",
      "Epoch 31, Training Loss: 0.8008294416549511\n",
      "Epoch 32, Training Loss: 0.8000915329259141\n",
      "Epoch 33, Training Loss: 0.8003258028424772\n",
      "Epoch 34, Training Loss: 0.8005709548641865\n",
      "Epoch 35, Training Loss: 0.7997730389573521\n",
      "Epoch 36, Training Loss: 0.8000348501635673\n",
      "Epoch 37, Training Loss: 0.7995096555329804\n",
      "Epoch 38, Training Loss: 0.7996270603703377\n",
      "Epoch 39, Training Loss: 0.7985791124795613\n",
      "Epoch 40, Training Loss: 0.7986871267619886\n",
      "Epoch 41, Training Loss: 0.798240956507231\n",
      "Epoch 42, Training Loss: 0.7983240669831297\n",
      "Epoch 43, Training Loss: 0.7979484148491595\n",
      "Epoch 44, Training Loss: 0.7976273646032004\n",
      "Epoch 45, Training Loss: 0.7968643772870975\n",
      "Epoch 46, Training Loss: 0.796591231518222\n",
      "Epoch 47, Training Loss: 0.7971078718515267\n",
      "Epoch 48, Training Loss: 0.796008416853453\n",
      "Epoch 49, Training Loss: 0.7959616356326226\n",
      "Epoch 50, Training Loss: 0.7955002506872765\n",
      "Epoch 51, Training Loss: 0.7953510485197368\n",
      "Epoch 52, Training Loss: 0.7952619626109761\n",
      "Epoch 53, Training Loss: 0.7943835772966084\n",
      "Epoch 54, Training Loss: 0.7940853516858323\n",
      "Epoch 55, Training Loss: 0.7935542224045087\n",
      "Epoch 56, Training Loss: 0.7937368611644086\n",
      "Epoch 57, Training Loss: 0.7932504724739189\n",
      "Epoch 58, Training Loss: 0.792762451422842\n",
      "Epoch 59, Training Loss: 0.7919067862338589\n",
      "Epoch 60, Training Loss: 0.7914819740263143\n",
      "Epoch 61, Training Loss: 0.7919488533098895\n",
      "Epoch 62, Training Loss: 0.7922212598915387\n",
      "Epoch 63, Training Loss: 0.790816296849932\n",
      "Epoch 64, Training Loss: 0.7905571736787494\n",
      "Epoch 65, Training Loss: 0.7908089625208001\n",
      "Epoch 66, Training Loss: 0.7903072188671371\n",
      "Epoch 67, Training Loss: 0.7893568955866018\n",
      "Epoch 68, Training Loss: 0.7898019692951576\n",
      "Epoch 69, Training Loss: 0.7892550521327141\n",
      "Epoch 70, Training Loss: 0.7892528958786699\n",
      "Epoch 71, Training Loss: 0.7894859397321715\n",
      "Epoch 72, Training Loss: 0.7888512177574903\n",
      "Epoch 73, Training Loss: 0.7888566367608264\n",
      "Epoch 74, Training Loss: 0.7877853608669195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:48:57,059] Trial 179 finished with value: 0.6378 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7878242552728582\n",
      "Epoch 1, Training Loss: 0.9127658872676075\n",
      "Epoch 2, Training Loss: 0.8281949270936779\n",
      "Epoch 3, Training Loss: 0.8179261149320387\n",
      "Epoch 4, Training Loss: 0.8103412943675106\n",
      "Epoch 5, Training Loss: 0.8054136605191051\n",
      "Epoch 6, Training Loss: 0.801997002802397\n",
      "Epoch 7, Training Loss: 0.8002900358429529\n",
      "Epoch 8, Training Loss: 0.7981621655306421\n",
      "Epoch 9, Training Loss: 0.7963277918951852\n",
      "Epoch 10, Training Loss: 0.7959201155748582\n",
      "Epoch 11, Training Loss: 0.7940552977691019\n",
      "Epoch 12, Training Loss: 0.7929008473130993\n",
      "Epoch 13, Training Loss: 0.7930389228620027\n",
      "Epoch 14, Training Loss: 0.791371674645216\n",
      "Epoch 15, Training Loss: 0.791368439771179\n",
      "Epoch 16, Training Loss: 0.7909674067246286\n",
      "Epoch 17, Training Loss: 0.7897710269555114\n",
      "Epoch 18, Training Loss: 0.7909382286824678\n",
      "Epoch 19, Training Loss: 0.7893718507953156\n",
      "Epoch 20, Training Loss: 0.7884881735744332\n",
      "Epoch 21, Training Loss: 0.7892380954627704\n",
      "Epoch 22, Training Loss: 0.7889089957215732\n",
      "Epoch 23, Training Loss: 0.7883811930964764\n",
      "Epoch 24, Training Loss: 0.7881353811213845\n",
      "Epoch 25, Training Loss: 0.7876243086685811\n",
      "Epoch 26, Training Loss: 0.7874456002299947\n",
      "Epoch 27, Training Loss: 0.7872996088257409\n",
      "Epoch 28, Training Loss: 0.788502531840389\n",
      "Epoch 29, Training Loss: 0.7872620081543026\n",
      "Epoch 30, Training Loss: 0.7868074936078007\n",
      "Epoch 31, Training Loss: 0.786409979027913\n",
      "Epoch 32, Training Loss: 0.7862822404481414\n",
      "Epoch 33, Training Loss: 0.787000618841415\n",
      "Epoch 34, Training Loss: 0.7857933046226214\n",
      "Epoch 35, Training Loss: 0.7859117424577698\n",
      "Epoch 36, Training Loss: 0.7856635722002588\n",
      "Epoch 37, Training Loss: 0.7849885135216821\n",
      "Epoch 38, Training Loss: 0.7849621705990985\n",
      "Epoch 39, Training Loss: 0.7852022825327135\n",
      "Epoch 40, Training Loss: 0.7844259795389678\n",
      "Epoch 41, Training Loss: 0.784321451635289\n",
      "Epoch 42, Training Loss: 0.7848206497672805\n",
      "Epoch 43, Training Loss: 0.784197591570087\n",
      "Epoch 44, Training Loss: 0.7842751053939189\n",
      "Epoch 45, Training Loss: 0.7845221876201773\n",
      "Epoch 46, Training Loss: 0.7844069807152999\n",
      "Epoch 47, Training Loss: 0.7836494076520877\n",
      "Epoch 48, Training Loss: 0.7837084530887747\n",
      "Epoch 49, Training Loss: 0.7838911669594901\n",
      "Epoch 50, Training Loss: 0.7831815458778152\n",
      "Epoch 51, Training Loss: 0.7832147639496883\n",
      "Epoch 52, Training Loss: 0.7833769132320146\n",
      "Epoch 53, Training Loss: 0.7828132564860178\n",
      "Epoch 54, Training Loss: 0.783453877617542\n",
      "Epoch 55, Training Loss: 0.7829897604490581\n",
      "Epoch 56, Training Loss: 0.7831720637199574\n",
      "Epoch 57, Training Loss: 0.7818571399925347\n",
      "Epoch 58, Training Loss: 0.7823129380555978\n",
      "Epoch 59, Training Loss: 0.7825089431346808\n",
      "Epoch 60, Training Loss: 0.7819808363018179\n",
      "Epoch 61, Training Loss: 0.7823865994474941\n",
      "Epoch 62, Training Loss: 0.7813095663723193\n",
      "Epoch 63, Training Loss: 0.7820229974904455\n",
      "Epoch 64, Training Loss: 0.781956642462795\n",
      "Epoch 65, Training Loss: 0.780992764593067\n",
      "Epoch 66, Training Loss: 0.7814126595518642\n",
      "Epoch 67, Training Loss: 0.7818626890505167\n",
      "Epoch 68, Training Loss: 0.7806726238781349\n",
      "Epoch 69, Training Loss: 0.7803029861665309\n",
      "Epoch 70, Training Loss: 0.7806252784298775\n",
      "Epoch 71, Training Loss: 0.7808086666845737\n",
      "Epoch 72, Training Loss: 0.7806070159252425\n",
      "Epoch 73, Training Loss: 0.7795162848512033\n",
      "Epoch 74, Training Loss: 0.7803185218258908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:50:28,924] Trial 180 finished with value: 0.6358 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7797442627132387\n",
      "Epoch 1, Training Loss: 0.9140949998182409\n",
      "Epoch 2, Training Loss: 0.8309514115838443\n",
      "Epoch 3, Training Loss: 0.8229939922164469\n",
      "Epoch 4, Training Loss: 0.8186678892023423\n",
      "Epoch 5, Training Loss: 0.8160028861550723\n",
      "Epoch 6, Training Loss: 0.8151498331041898\n",
      "Epoch 7, Training Loss: 0.8122391759648042\n",
      "Epoch 8, Training Loss: 0.8121207104710971\n",
      "Epoch 9, Training Loss: 0.8099208007139318\n",
      "Epoch 10, Training Loss: 0.8083734575439901\n",
      "Epoch 11, Training Loss: 0.8075806720817791\n",
      "Epoch 12, Training Loss: 0.8068931900753694\n",
      "Epoch 13, Training Loss: 0.8067222377833199\n",
      "Epoch 14, Training Loss: 0.8057949243573581\n",
      "Epoch 15, Training Loss: 0.8063344529095818\n",
      "Epoch 16, Training Loss: 0.8053657298228319\n",
      "Epoch 17, Training Loss: 0.805654456615448\n",
      "Epoch 18, Training Loss: 0.8045082018655889\n",
      "Epoch 19, Training Loss: 0.805588208436966\n",
      "Epoch 20, Training Loss: 0.8050131839163163\n",
      "Epoch 21, Training Loss: 0.8053813095653758\n",
      "Epoch 22, Training Loss: 0.8047717208020827\n",
      "Epoch 23, Training Loss: 0.8047883153662962\n",
      "Epoch 24, Training Loss: 0.8043723327272079\n",
      "Epoch 25, Training Loss: 0.8040187996976516\n",
      "Epoch 26, Training Loss: 0.8037163460254669\n",
      "Epoch 27, Training Loss: 0.803178180806777\n",
      "Epoch 28, Training Loss: 0.8045735266629387\n",
      "Epoch 29, Training Loss: 0.8040603053569794\n",
      "Epoch 30, Training Loss: 0.8028568351969999\n",
      "Epoch 31, Training Loss: 0.8034488494255964\n",
      "Epoch 32, Training Loss: 0.8042912168362562\n",
      "Epoch 33, Training Loss: 0.8045786140946781\n",
      "Epoch 34, Training Loss: 0.8046264153368333\n",
      "Epoch 35, Training Loss: 0.8046948601919062\n",
      "Epoch 36, Training Loss: 0.8038253045082092\n",
      "Epoch 37, Training Loss: 0.8034722262270311\n",
      "Epoch 38, Training Loss: 0.803613937812693\n",
      "Epoch 39, Training Loss: 0.8035344397320466\n",
      "Epoch 40, Training Loss: 0.8045188797221464\n",
      "Epoch 41, Training Loss: 0.8030815328569973\n",
      "Epoch 42, Training Loss: 0.803792743472492\n",
      "Epoch 43, Training Loss: 0.8035003289054422\n",
      "Epoch 44, Training Loss: 0.8038603390665615\n",
      "Epoch 45, Training Loss: 0.8031186523157008\n",
      "Epoch 46, Training Loss: 0.80527610968141\n",
      "Epoch 47, Training Loss: 0.8037550635197583\n",
      "Epoch 48, Training Loss: 0.8037853422585656\n",
      "Epoch 49, Training Loss: 0.803780055326574\n",
      "Epoch 50, Training Loss: 0.8039588732579175\n",
      "Epoch 51, Training Loss: 0.8031171921421499\n",
      "Epoch 52, Training Loss: 0.8039727462740506\n",
      "Epoch 53, Training Loss: 0.8034264810646281\n",
      "Epoch 54, Training Loss: 0.8034973732162924\n",
      "Epoch 55, Training Loss: 0.8036862406309914\n",
      "Epoch 56, Training Loss: 0.8034465460216298\n",
      "Epoch 57, Training Loss: 0.8039219169756946\n",
      "Epoch 58, Training Loss: 0.8035395141208873\n",
      "Epoch 59, Training Loss: 0.8030864630025976\n",
      "Epoch 60, Training Loss: 0.8033005686367259\n",
      "Epoch 61, Training Loss: 0.8029550229801851\n",
      "Epoch 62, Training Loss: 0.8042480588660521\n",
      "Epoch 63, Training Loss: 0.8030768828532275\n",
      "Epoch 64, Training Loss: 0.803524729644551\n",
      "Epoch 65, Training Loss: 0.8026656805767732\n",
      "Epoch 66, Training Loss: 0.8034506454888513\n",
      "Epoch 67, Training Loss: 0.8027484564921435\n",
      "Epoch 68, Training Loss: 0.8026337716859929\n",
      "Epoch 69, Training Loss: 0.8024025357470793\n",
      "Epoch 70, Training Loss: 0.8015865848344915\n",
      "Epoch 71, Training Loss: 0.802133501697989\n",
      "Epoch 72, Training Loss: 0.801812444153954\n",
      "Epoch 73, Training Loss: 0.8026063126676223\n",
      "Epoch 74, Training Loss: 0.8027351532262914\n",
      "Epoch 75, Training Loss: 0.8021239515613107\n",
      "Epoch 76, Training Loss: 0.8026153606526992\n",
      "Epoch 77, Training Loss: 0.8031926809338963\n",
      "Epoch 78, Training Loss: 0.8026740363064935\n",
      "Epoch 79, Training Loss: 0.8030398204046137\n",
      "Epoch 80, Training Loss: 0.8023778546557707\n",
      "Epoch 81, Training Loss: 0.8030429152881398\n",
      "Epoch 82, Training Loss: 0.8020898194874034\n",
      "Epoch 83, Training Loss: 0.8015054725198185\n",
      "Epoch 84, Training Loss: 0.804147495171603\n",
      "Epoch 85, Training Loss: 0.8027441617320565\n",
      "Epoch 86, Training Loss: 0.8032418413021986\n",
      "Epoch 87, Training Loss: 0.8026180800970862\n",
      "Epoch 88, Training Loss: 0.8019737428076127\n",
      "Epoch 89, Training Loss: 0.8021024715900421\n",
      "Epoch 90, Training Loss: 0.8016961449034073\n",
      "Epoch 91, Training Loss: 0.8019090222611147\n",
      "Epoch 92, Training Loss: 0.8019447764929604\n",
      "Epoch 93, Training Loss: 0.802960316363503\n",
      "Epoch 94, Training Loss: 0.8025817333249485\n",
      "Epoch 95, Training Loss: 0.8017490435347837\n",
      "Epoch 96, Training Loss: 0.8025316968384911\n",
      "Epoch 97, Training Loss: 0.8027738431622\n",
      "Epoch 98, Training Loss: 0.802419071337756\n",
      "Epoch 99, Training Loss: 0.8017713113392101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:52:53,200] Trial 181 finished with value: 0.6301333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.801873432748458\n",
      "Epoch 1, Training Loss: 1.0469023035554326\n",
      "Epoch 2, Training Loss: 0.9543386289652656\n",
      "Epoch 3, Training Loss: 0.9287184739112854\n",
      "Epoch 4, Training Loss: 0.9136651302786435\n",
      "Epoch 5, Training Loss: 0.8922834819204667\n",
      "Epoch 6, Training Loss: 0.8491274814745959\n",
      "Epoch 7, Training Loss: 0.8186265244203456\n",
      "Epoch 8, Training Loss: 0.8115696506640491\n",
      "Epoch 9, Training Loss: 0.8091940541127148\n",
      "Epoch 10, Training Loss: 0.8072134249350604\n",
      "Epoch 11, Training Loss: 0.8053009817179512\n",
      "Epoch 12, Training Loss: 0.8041250845263986\n",
      "Epoch 13, Training Loss: 0.802524289523854\n",
      "Epoch 14, Training Loss: 0.8021860045545242\n",
      "Epoch 15, Training Loss: 0.8004771740296308\n",
      "Epoch 16, Training Loss: 0.7995948839187622\n",
      "Epoch 17, Training Loss: 0.7988140146872577\n",
      "Epoch 18, Training Loss: 0.7979257903379553\n",
      "Epoch 19, Training Loss: 0.7970444062176872\n",
      "Epoch 20, Training Loss: 0.7966189216866213\n",
      "Epoch 21, Training Loss: 0.795344376003041\n",
      "Epoch 22, Training Loss: 0.7946016472227433\n",
      "Epoch 23, Training Loss: 0.7941251477774451\n",
      "Epoch 24, Training Loss: 0.793372049962773\n",
      "Epoch 25, Training Loss: 0.7926669776439667\n",
      "Epoch 26, Training Loss: 0.7920263936940362\n",
      "Epoch 27, Training Loss: 0.7915022380211774\n",
      "Epoch 28, Training Loss: 0.79103176649879\n",
      "Epoch 29, Training Loss: 0.7903026556968689\n",
      "Epoch 30, Training Loss: 0.78996917107526\n",
      "Epoch 31, Training Loss: 0.7889581160685596\n",
      "Epoch 32, Training Loss: 0.788906245582244\n",
      "Epoch 33, Training Loss: 0.7883712786786696\n",
      "Epoch 34, Training Loss: 0.7878786964977489\n",
      "Epoch 35, Training Loss: 0.7878715451324687\n",
      "Epoch 36, Training Loss: 0.7876251589550691\n",
      "Epoch 37, Training Loss: 0.7870878445400912\n",
      "Epoch 38, Training Loss: 0.7868970507032731\n",
      "Epoch 39, Training Loss: 0.7864478056571063\n",
      "Epoch 40, Training Loss: 0.7859715395114001\n",
      "Epoch 41, Training Loss: 0.7860802145565258\n",
      "Epoch 42, Training Loss: 0.7854157710075378\n",
      "Epoch 43, Training Loss: 0.7857398728062125\n",
      "Epoch 44, Training Loss: 0.7851578126935398\n",
      "Epoch 45, Training Loss: 0.7850998776099262\n",
      "Epoch 46, Training Loss: 0.7849025058045107\n",
      "Epoch 47, Training Loss: 0.7848616496254416\n",
      "Epoch 48, Training Loss: 0.7847891917649438\n",
      "Epoch 49, Training Loss: 0.7847287603686838\n",
      "Epoch 50, Training Loss: 0.7840607934839585\n",
      "Epoch 51, Training Loss: 0.7838273246148053\n",
      "Epoch 52, Training Loss: 0.7842253714449265\n",
      "Epoch 53, Training Loss: 0.7834697801225325\n",
      "Epoch 54, Training Loss: 0.7835300735165092\n",
      "Epoch 55, Training Loss: 0.7833405829177184\n",
      "Epoch 56, Training Loss: 0.7835636196416967\n",
      "Epoch 57, Training Loss: 0.783045081391054\n",
      "Epoch 58, Training Loss: 0.782821774482727\n",
      "Epoch 59, Training Loss: 0.783091610810336\n",
      "Epoch 60, Training Loss: 0.7832787138574263\n",
      "Epoch 61, Training Loss: 0.7829101497285507\n",
      "Epoch 62, Training Loss: 0.7829389673120836\n",
      "Epoch 63, Training Loss: 0.7829143571853637\n",
      "Epoch 64, Training Loss: 0.7826123959877912\n",
      "Epoch 65, Training Loss: 0.7827261475955739\n",
      "Epoch 66, Training Loss: 0.7824158082288855\n",
      "Epoch 67, Training Loss: 0.7823462756241069\n",
      "Epoch 68, Training Loss: 0.7822151932996863\n",
      "Epoch 69, Training Loss: 0.7820326654350056\n",
      "Epoch 70, Training Loss: 0.7818948906309464\n",
      "Epoch 71, Training Loss: 0.7817544450479396\n",
      "Epoch 72, Training Loss: 0.7818799779695623\n",
      "Epoch 73, Training Loss: 0.7819002305058872\n",
      "Epoch 74, Training Loss: 0.78208308114725\n",
      "Epoch 75, Training Loss: 0.7818625918556662\n",
      "Epoch 76, Training Loss: 0.7819476475435144\n",
      "Epoch 77, Training Loss: 0.781683281028972\n",
      "Epoch 78, Training Loss: 0.7814670475090251\n",
      "Epoch 79, Training Loss: 0.7814843923204086\n",
      "Epoch 80, Training Loss: 0.7817135406241698\n",
      "Epoch 81, Training Loss: 0.7816696927827947\n",
      "Epoch 82, Training Loss: 0.7812517142295837\n",
      "Epoch 83, Training Loss: 0.7813625860214234\n",
      "Epoch 84, Training Loss: 0.7813410012161031\n",
      "Epoch 85, Training Loss: 0.7812071205587948\n",
      "Epoch 86, Training Loss: 0.781168663221247\n",
      "Epoch 87, Training Loss: 0.7811540177289178\n",
      "Epoch 88, Training Loss: 0.781285293523003\n",
      "Epoch 89, Training Loss: 0.7809877873168272\n",
      "Epoch 90, Training Loss: 0.7809085374719956\n",
      "Epoch 91, Training Loss: 0.7808985682796029\n",
      "Epoch 92, Training Loss: 0.781157698280671\n",
      "Epoch 93, Training Loss: 0.7807727747103748\n",
      "Epoch 94, Training Loss: 0.7806718654492322\n",
      "Epoch 95, Training Loss: 0.7806577958780176\n",
      "Epoch 96, Training Loss: 0.781009561103933\n",
      "Epoch 97, Training Loss: 0.7805595873383915\n",
      "Epoch 98, Training Loss: 0.7807007235639235\n",
      "Epoch 99, Training Loss: 0.7805871103090398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:54:47,740] Trial 182 finished with value: 0.639 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7807684276384466\n",
      "Epoch 1, Training Loss: 1.0336396140210768\n",
      "Epoch 2, Training Loss: 0.9488690356647267\n",
      "Epoch 3, Training Loss: 0.928713968781864\n",
      "Epoch 4, Training Loss: 0.9197396453689126\n",
      "Epoch 5, Training Loss: 0.9118347363612231\n",
      "Epoch 6, Training Loss: 0.9039671727489023\n",
      "Epoch 7, Training Loss: 0.894737767682356\n",
      "Epoch 8, Training Loss: 0.8828411575625924\n",
      "Epoch 9, Training Loss: 0.8665114095631767\n",
      "Epoch 10, Training Loss: 0.8475692617191988\n",
      "Epoch 11, Training Loss: 0.8309340475587284\n",
      "Epoch 12, Training Loss: 0.8190434905360727\n",
      "Epoch 13, Training Loss: 0.8121330797672272\n",
      "Epoch 14, Training Loss: 0.8085752770479988\n",
      "Epoch 15, Training Loss: 0.8064214737275067\n",
      "Epoch 16, Training Loss: 0.8051107522319345\n",
      "Epoch 17, Training Loss: 0.8042564080041997\n",
      "Epoch 18, Training Loss: 0.803392862291897\n",
      "Epoch 19, Training Loss: 0.8029711926684661\n",
      "Epoch 20, Training Loss: 0.8022455546435188\n",
      "Epoch 21, Training Loss: 0.8017246553477119\n",
      "Epoch 22, Training Loss: 0.8011859527756187\n",
      "Epoch 23, Training Loss: 0.8007697670599994\n",
      "Epoch 24, Training Loss: 0.8002855727251839\n",
      "Epoch 25, Training Loss: 0.7997693042895373\n",
      "Epoch 26, Training Loss: 0.7992118752002716\n",
      "Epoch 27, Training Loss: 0.7990141535506529\n",
      "Epoch 28, Training Loss: 0.79838879199589\n",
      "Epoch 29, Training Loss: 0.7978260479253881\n",
      "Epoch 30, Training Loss: 0.7976023995175081\n",
      "Epoch 31, Training Loss: 0.7974066840199863\n",
      "Epoch 32, Training Loss: 0.7970878372472875\n",
      "Epoch 33, Training Loss: 0.7968438334324781\n",
      "Epoch 34, Training Loss: 0.7964108494450064\n",
      "Epoch 35, Training Loss: 0.7961247695193571\n",
      "Epoch 36, Training Loss: 0.796030380375245\n",
      "Epoch 37, Training Loss: 0.7957211422920227\n",
      "Epoch 38, Training Loss: 0.7956724303610184\n",
      "Epoch 39, Training Loss: 0.7953885898169349\n",
      "Epoch 40, Training Loss: 0.795050702936509\n",
      "Epoch 41, Training Loss: 0.7947905609186958\n",
      "Epoch 42, Training Loss: 0.7945815166305094\n",
      "Epoch 43, Training Loss: 0.7943737194117377\n",
      "Epoch 44, Training Loss: 0.7940340713192435\n",
      "Epoch 45, Training Loss: 0.7938808777051813\n",
      "Epoch 46, Training Loss: 0.793486553851296\n",
      "Epoch 47, Training Loss: 0.7935187319447012\n",
      "Epoch 48, Training Loss: 0.7929953869651346\n",
      "Epoch 49, Training Loss: 0.792884326542125\n",
      "Epoch 50, Training Loss: 0.7925367161105661\n",
      "Epoch 51, Training Loss: 0.792388752558652\n",
      "Epoch 52, Training Loss: 0.7922903493572684\n",
      "Epoch 53, Training Loss: 0.7918983065380769\n",
      "Epoch 54, Training Loss: 0.7919131602960474\n",
      "Epoch 55, Training Loss: 0.7916682802929598\n",
      "Epoch 56, Training Loss: 0.7913677301827599\n",
      "Epoch 57, Training Loss: 0.7911689245700836\n",
      "Epoch 58, Training Loss: 0.7909263102447286\n",
      "Epoch 59, Training Loss: 0.7907341227110695\n",
      "Epoch 60, Training Loss: 0.7908534028249629\n",
      "Epoch 61, Training Loss: 0.7903804767131806\n",
      "Epoch 62, Training Loss: 0.7902920903177822\n",
      "Epoch 63, Training Loss: 0.7902012403572307\n",
      "Epoch 64, Training Loss: 0.7902472387341892\n",
      "Epoch 65, Training Loss: 0.789688264762654\n",
      "Epoch 66, Training Loss: 0.7897593455454882\n",
      "Epoch 67, Training Loss: 0.7895444973076091\n",
      "Epoch 68, Training Loss: 0.7892927604563096\n",
      "Epoch 69, Training Loss: 0.7893858400513144\n",
      "Epoch 70, Training Loss: 0.789033259714351\n",
      "Epoch 71, Training Loss: 0.7890061192652759\n",
      "Epoch 72, Training Loss: 0.7888851911180159\n",
      "Epoch 73, Training Loss: 0.7885458376127131\n",
      "Epoch 74, Training Loss: 0.7886864002311931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:56:08,216] Trial 183 finished with value: 0.6364 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7883191934753867\n",
      "Epoch 1, Training Loss: 1.0909930940235362\n",
      "Epoch 2, Training Loss: 1.0907753853236928\n",
      "Epoch 3, Training Loss: 1.090697100162506\n",
      "Epoch 4, Training Loss: 1.0906008717593025\n",
      "Epoch 5, Training Loss: 1.0905267073126401\n",
      "Epoch 6, Training Loss: 1.0904986799464507\n",
      "Epoch 7, Training Loss: 1.0904009488049675\n",
      "Epoch 8, Training Loss: 1.090324653036454\n",
      "Epoch 9, Training Loss: 1.0902129169071422\n",
      "Epoch 10, Training Loss: 1.0900492332963383\n",
      "Epoch 11, Training Loss: 1.0899916323493508\n",
      "Epoch 12, Training Loss: 1.0899041805547827\n",
      "Epoch 13, Training Loss: 1.0897983172360588\n",
      "Epoch 14, Training Loss: 1.089596227197086\n",
      "Epoch 15, Training Loss: 1.0894648881519542\n",
      "Epoch 16, Training Loss: 1.089326639175415\n",
      "Epoch 17, Training Loss: 1.0890754810501548\n",
      "Epoch 18, Training Loss: 1.088835574458627\n",
      "Epoch 19, Training Loss: 1.0886318954299479\n",
      "Epoch 20, Training Loss: 1.0883299523241379\n",
      "Epoch 21, Training Loss: 1.088195580033695\n",
      "Epoch 22, Training Loss: 1.0877977200115427\n",
      "Epoch 23, Training Loss: 1.0874464761509615\n",
      "Epoch 24, Training Loss: 1.087008867824779\n",
      "Epoch 25, Training Loss: 1.0864568784657647\n",
      "Epoch 26, Training Loss: 1.0857796429185307\n",
      "Epoch 27, Training Loss: 1.0852056823057288\n",
      "Epoch 28, Training Loss: 1.0843565877746133\n",
      "Epoch 29, Training Loss: 1.083308409242069\n",
      "Epoch 30, Training Loss: 1.0821104778962978\n",
      "Epoch 31, Training Loss: 1.0806145485709695\n",
      "Epoch 32, Training Loss: 1.0787372388559229\n",
      "Epoch 33, Training Loss: 1.0764930875161114\n",
      "Epoch 34, Training Loss: 1.073646222002366\n",
      "Epoch 35, Training Loss: 1.0699863969578463\n",
      "Epoch 36, Training Loss: 1.0653953538221472\n",
      "Epoch 37, Training Loss: 1.0596991170153898\n",
      "Epoch 38, Training Loss: 1.0525191057429595\n",
      "Epoch 39, Training Loss: 1.0438850468046525\n",
      "Epoch 40, Training Loss: 1.0342166397852057\n",
      "Epoch 41, Training Loss: 1.0241703232596902\n",
      "Epoch 42, Training Loss: 1.014755985175862\n",
      "Epoch 43, Training Loss: 1.0072395063849056\n",
      "Epoch 44, Training Loss: 1.0018823960949392\n",
      "Epoch 45, Training Loss: 0.9983129742566277\n",
      "Epoch 46, Training Loss: 0.9961539905211505\n",
      "Epoch 47, Training Loss: 0.9947568420101615\n",
      "Epoch 48, Training Loss: 0.9936110365390778\n",
      "Epoch 49, Training Loss: 0.9926389481740839\n",
      "Epoch 50, Training Loss: 0.9917923342480379\n",
      "Epoch 51, Training Loss: 0.9909168743385988\n",
      "Epoch 52, Training Loss: 0.9899393206484177\n",
      "Epoch 53, Training Loss: 0.9890286562723272\n",
      "Epoch 54, Training Loss: 0.9879824729526744\n",
      "Epoch 55, Training Loss: 0.9869984345576343\n",
      "Epoch 56, Training Loss: 0.9858870545555564\n",
      "Epoch 57, Training Loss: 0.9847242315376507\n",
      "Epoch 58, Training Loss: 0.9833651352629942\n",
      "Epoch 59, Training Loss: 0.9820836869408103\n",
      "Epoch 60, Training Loss: 0.9805246368576499\n",
      "Epoch 61, Training Loss: 0.9789804035074571\n",
      "Epoch 62, Training Loss: 0.977272013425827\n",
      "Epoch 63, Training Loss: 0.9755104795624228\n",
      "Epoch 64, Training Loss: 0.9735136579765993\n",
      "Epoch 65, Training Loss: 0.9715547286762911\n",
      "Epoch 66, Training Loss: 0.9693660378456116\n",
      "Epoch 67, Training Loss: 0.9671200269811293\n",
      "Epoch 68, Training Loss: 0.9648095875627855\n",
      "Epoch 69, Training Loss: 0.9624199775387259\n",
      "Epoch 70, Training Loss: 0.9599873356258168\n",
      "Epoch 71, Training Loss: 0.9575278806686401\n",
      "Epoch 72, Training Loss: 0.9551594215982101\n",
      "Epoch 73, Training Loss: 0.952847926827038\n",
      "Epoch 74, Training Loss: 0.9506616175875944\n",
      "Epoch 75, Training Loss: 0.9485628053020029\n",
      "Epoch 76, Training Loss: 0.9466707058513866\n",
      "Epoch 77, Training Loss: 0.9448805147760054\n",
      "Epoch 78, Training Loss: 0.9431219731358921\n",
      "Epoch 79, Training Loss: 0.9415740388982436\n",
      "Epoch 80, Training Loss: 0.9399436039784376\n",
      "Epoch 81, Training Loss: 0.9382603002997005\n",
      "Epoch 82, Training Loss: 0.9367243090096642\n",
      "Epoch 83, Training Loss: 0.9349236331967746\n",
      "Epoch 84, Training Loss: 0.9331301508230322\n",
      "Epoch 85, Training Loss: 0.9311843808959512\n",
      "Epoch 86, Training Loss: 0.9291697561039645\n",
      "Epoch 87, Training Loss: 0.9269888793019687\n",
      "Epoch 88, Training Loss: 0.924685352269341\n",
      "Epoch 89, Training Loss: 0.9221238330532523\n",
      "Epoch 90, Training Loss: 0.9195278090589186\n",
      "Epoch 91, Training Loss: 0.9165181953065535\n",
      "Epoch 92, Training Loss: 0.9135007165460025\n",
      "Epoch 93, Training Loss: 0.9101520513786989\n",
      "Epoch 94, Training Loss: 0.9066251600489897\n",
      "Epoch 95, Training Loss: 0.9027632456667283\n",
      "Epoch 96, Training Loss: 0.8989050114154815\n",
      "Epoch 97, Training Loss: 0.8947711551890654\n",
      "Epoch 98, Training Loss: 0.8905094039440155\n",
      "Epoch 99, Training Loss: 0.8861533911087933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:58:03,343] Trial 184 finished with value: 0.5836666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8819614286983715\n",
      "Epoch 1, Training Loss: 0.8752233251593167\n",
      "Epoch 2, Training Loss: 0.8121708886963981\n",
      "Epoch 3, Training Loss: 0.8046542624781903\n",
      "Epoch 4, Training Loss: 0.8013699464331893\n",
      "Epoch 5, Training Loss: 0.799990875649273\n",
      "Epoch 6, Training Loss: 0.7980228208957758\n",
      "Epoch 7, Training Loss: 0.795809869300154\n",
      "Epoch 8, Training Loss: 0.793628504222497\n",
      "Epoch 9, Training Loss: 0.7925281300580591\n",
      "Epoch 10, Training Loss: 0.7907329032743784\n",
      "Epoch 11, Training Loss: 0.7900419705792477\n",
      "Epoch 12, Training Loss: 0.7898508541566088\n",
      "Epoch 13, Training Loss: 0.7886871306519759\n",
      "Epoch 14, Training Loss: 0.7878326879408126\n",
      "Epoch 15, Training Loss: 0.7880652882999047\n",
      "Epoch 16, Training Loss: 0.7876716757179203\n",
      "Epoch 17, Training Loss: 0.7867864228729019\n",
      "Epoch 18, Training Loss: 0.7867911304746356\n",
      "Epoch 19, Training Loss: 0.7860634516056318\n",
      "Epoch 20, Training Loss: 0.7862791159995517\n",
      "Epoch 21, Training Loss: 0.7855042755155635\n",
      "Epoch 22, Training Loss: 0.7849048394009583\n",
      "Epoch 23, Training Loss: 0.7847844422311712\n",
      "Epoch 24, Training Loss: 0.7843177551613715\n",
      "Epoch 25, Training Loss: 0.7840508911842691\n",
      "Epoch 26, Training Loss: 0.7840335001622823\n",
      "Epoch 27, Training Loss: 0.7842794291955187\n",
      "Epoch 28, Training Loss: 0.7831853257534199\n",
      "Epoch 29, Training Loss: 0.7829828701521221\n",
      "Epoch 30, Training Loss: 0.7833257614221788\n",
      "Epoch 31, Training Loss: 0.7825351969640058\n",
      "Epoch 32, Training Loss: 0.7824682641298251\n",
      "Epoch 33, Training Loss: 0.7829931836379201\n",
      "Epoch 34, Training Loss: 0.7817559003381801\n",
      "Epoch 35, Training Loss: 0.7825244418660501\n",
      "Epoch 36, Training Loss: 0.7814004319502895\n",
      "Epoch 37, Training Loss: 0.782477337166779\n",
      "Epoch 38, Training Loss: 0.7822488955985335\n",
      "Epoch 39, Training Loss: 0.7816985184984996\n",
      "Epoch 40, Training Loss: 0.7815143922217807\n",
      "Epoch 41, Training Loss: 0.7815329190483666\n",
      "Epoch 42, Training Loss: 0.7816657781600952\n",
      "Epoch 43, Training Loss: 0.7812651203987294\n",
      "Epoch 44, Training Loss: 0.7813901603670048\n",
      "Epoch 45, Training Loss: 0.7806121880846812\n",
      "Epoch 46, Training Loss: 0.7806818124046899\n",
      "Epoch 47, Training Loss: 0.780469064336074\n",
      "Epoch 48, Training Loss: 0.7805922614004379\n",
      "Epoch 49, Training Loss: 0.7803263864122836\n",
      "Epoch 50, Training Loss: 0.7796246986191971\n",
      "Epoch 51, Training Loss: 0.7801778372965361\n",
      "Epoch 52, Training Loss: 0.779756928655438\n",
      "Epoch 53, Training Loss: 0.7800634508742426\n",
      "Epoch 54, Training Loss: 0.7801914446335986\n",
      "Epoch 55, Training Loss: 0.7794519714843061\n",
      "Epoch 56, Training Loss: 0.7790857646250187\n",
      "Epoch 57, Training Loss: 0.7795156829339221\n",
      "Epoch 58, Training Loss: 0.7801860523403139\n",
      "Epoch 59, Training Loss: 0.7796811138776908\n",
      "Epoch 60, Training Loss: 0.7787467818063004\n",
      "Epoch 61, Training Loss: 0.7791100732365945\n",
      "Epoch 62, Training Loss: 0.7786025969605697\n",
      "Epoch 63, Training Loss: 0.7791387999864449\n",
      "Epoch 64, Training Loss: 0.7794069058913037\n",
      "Epoch 65, Training Loss: 0.7788144846607868\n",
      "Epoch 66, Training Loss: 0.7790967534359237\n",
      "Epoch 67, Training Loss: 0.7789212077183831\n",
      "Epoch 68, Training Loss: 0.7788507876539589\n",
      "Epoch 69, Training Loss: 0.7781238355134663\n",
      "Epoch 70, Training Loss: 0.7783555135691076\n",
      "Epoch 71, Training Loss: 0.7784470687235209\n",
      "Epoch 72, Training Loss: 0.7778634962282682\n",
      "Epoch 73, Training Loss: 0.77829293182918\n",
      "Epoch 74, Training Loss: 0.7783580312155243\n",
      "Epoch 75, Training Loss: 0.7780524823002349\n",
      "Epoch 76, Training Loss: 0.7779055998737651\n",
      "Epoch 77, Training Loss: 0.7784925613188206\n",
      "Epoch 78, Training Loss: 0.7777939722950297\n",
      "Epoch 79, Training Loss: 0.7776676162741238\n",
      "Epoch 80, Training Loss: 0.7774071015809711\n",
      "Epoch 81, Training Loss: 0.7775834314805224\n",
      "Epoch 82, Training Loss: 0.777344736927434\n",
      "Epoch 83, Training Loss: 0.7774972658408316\n",
      "Epoch 84, Training Loss: 0.7774003670627909\n",
      "Epoch 85, Training Loss: 0.7769784296365608\n",
      "Epoch 86, Training Loss: 0.7770194214089472\n",
      "Epoch 87, Training Loss: 0.7771478142057147\n",
      "Epoch 88, Training Loss: 0.7767733438570696\n",
      "Epoch 89, Training Loss: 0.7767894459846324\n",
      "Epoch 90, Training Loss: 0.7769786572097835\n",
      "Epoch 91, Training Loss: 0.7775350203191428\n",
      "Epoch 92, Training Loss: 0.7763597445828574\n",
      "Epoch 93, Training Loss: 0.7768633061781862\n",
      "Epoch 94, Training Loss: 0.7758192335752616\n",
      "Epoch 95, Training Loss: 0.7760296735548435\n",
      "Epoch 96, Training Loss: 0.7777386296064334\n",
      "Epoch 97, Training Loss: 0.7777058216862213\n",
      "Epoch 98, Training Loss: 0.7765249548998094\n",
      "Epoch 99, Training Loss: 0.7761466176886308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 00:59:55,915] Trial 185 finished with value: 0.6366 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7759278939182597\n",
      "Epoch 1, Training Loss: 0.9312133279061855\n",
      "Epoch 2, Training Loss: 0.8763089825336198\n",
      "Epoch 3, Training Loss: 0.8354181354207204\n",
      "Epoch 4, Training Loss: 0.8136073570502432\n",
      "Epoch 5, Training Loss: 0.8063559507068835\n",
      "Epoch 6, Training Loss: 0.8044854435705601\n",
      "Epoch 7, Training Loss: 0.8013103331838335\n",
      "Epoch 8, Training Loss: 0.8007975838238135\n",
      "Epoch 9, Training Loss: 0.8007448096920673\n",
      "Epoch 10, Training Loss: 0.8000656021268744\n",
      "Epoch 11, Training Loss: 0.7993283836465133\n",
      "Epoch 12, Training Loss: 0.7991012581308982\n",
      "Epoch 13, Training Loss: 0.7987065764298117\n",
      "Epoch 14, Training Loss: 0.7986265995448694\n",
      "Epoch 15, Training Loss: 0.7981643751151579\n",
      "Epoch 16, Training Loss: 0.7979115289853032\n",
      "Epoch 17, Training Loss: 0.79838669613788\n",
      "Epoch 18, Training Loss: 0.797296921292642\n",
      "Epoch 19, Training Loss: 0.797539584708393\n",
      "Epoch 20, Training Loss: 0.7970948437999066\n",
      "Epoch 21, Training Loss: 0.7980336586335548\n",
      "Epoch 22, Training Loss: 0.79676589938931\n",
      "Epoch 23, Training Loss: 0.7970578755651202\n",
      "Epoch 24, Training Loss: 0.7973589297524072\n",
      "Epoch 25, Training Loss: 0.797348521885119\n",
      "Epoch 26, Training Loss: 0.7968876285660536\n",
      "Epoch 27, Training Loss: 0.7965653493888396\n",
      "Epoch 28, Training Loss: 0.7971224523128424\n",
      "Epoch 29, Training Loss: 0.7965964784299521\n",
      "Epoch 30, Training Loss: 0.7971509205667596\n",
      "Epoch 31, Training Loss: 0.7971276789679563\n",
      "Epoch 32, Training Loss: 0.7955280484113478\n",
      "Epoch 33, Training Loss: 0.7959373890009142\n",
      "Epoch 34, Training Loss: 0.796128012990593\n",
      "Epoch 35, Training Loss: 0.7955941143340635\n",
      "Epoch 36, Training Loss: 0.7960916457319618\n",
      "Epoch 37, Training Loss: 0.7961880055585302\n",
      "Epoch 38, Training Loss: 0.7960599786356876\n",
      "Epoch 39, Training Loss: 0.7965092500349633\n",
      "Epoch 40, Training Loss: 0.7960054059673969\n",
      "Epoch 41, Training Loss: 0.7961603437151228\n",
      "Epoch 42, Training Loss: 0.7957205845001049\n",
      "Epoch 43, Training Loss: 0.7964077405463484\n",
      "Epoch 44, Training Loss: 0.7967180811372915\n",
      "Epoch 45, Training Loss: 0.7961724577093483\n",
      "Epoch 46, Training Loss: 0.7960439056382144\n",
      "Epoch 47, Training Loss: 0.7960729023567716\n",
      "Epoch 48, Training Loss: 0.796180346406492\n",
      "Epoch 49, Training Loss: 0.7953379271621991\n",
      "Epoch 50, Training Loss: 0.7951615798742251\n",
      "Epoch 51, Training Loss: 0.795659019265856\n",
      "Epoch 52, Training Loss: 0.7951429629684391\n",
      "Epoch 53, Training Loss: 0.795803365402652\n",
      "Epoch 54, Training Loss: 0.7950392825262887\n",
      "Epoch 55, Training Loss: 0.7948511115590432\n",
      "Epoch 56, Training Loss: 0.7950122093795834\n",
      "Epoch 57, Training Loss: 0.7940550545104464\n",
      "Epoch 58, Training Loss: 0.7935768497617621\n",
      "Epoch 59, Training Loss: 0.7937972922970478\n",
      "Epoch 60, Training Loss: 0.7936319715994641\n",
      "Epoch 61, Training Loss: 0.7929932418622468\n",
      "Epoch 62, Training Loss: 0.7932536085745445\n",
      "Epoch 63, Training Loss: 0.7924278394620221\n",
      "Epoch 64, Training Loss: 0.7928413833890643\n",
      "Epoch 65, Training Loss: 0.7926945734741097\n",
      "Epoch 66, Training Loss: 0.791911304445195\n",
      "Epoch 67, Training Loss: 0.7923313877636329\n",
      "Epoch 68, Training Loss: 0.7924708852194305\n",
      "Epoch 69, Training Loss: 0.7914721851062058\n",
      "Epoch 70, Training Loss: 0.7918229251875913\n",
      "Epoch 71, Training Loss: 0.7913976585954652\n",
      "Epoch 72, Training Loss: 0.7925089276822886\n",
      "Epoch 73, Training Loss: 0.7914781572227191\n",
      "Epoch 74, Training Loss: 0.7915692538247072\n",
      "Epoch 75, Training Loss: 0.7911377449232833\n",
      "Epoch 76, Training Loss: 0.7912568093242501\n",
      "Epoch 77, Training Loss: 0.790849931347639\n",
      "Epoch 78, Training Loss: 0.7904514766277227\n",
      "Epoch 79, Training Loss: 0.7906998473002498\n",
      "Epoch 80, Training Loss: 0.7908572191582587\n",
      "Epoch 81, Training Loss: 0.790478807911837\n",
      "Epoch 82, Training Loss: 0.7898316202755261\n",
      "Epoch 83, Training Loss: 0.790120325859328\n",
      "Epoch 84, Training Loss: 0.7904699656300078\n",
      "Epoch 85, Training Loss: 0.7910784575275909\n",
      "Epoch 86, Training Loss: 0.7899238375792826\n",
      "Epoch 87, Training Loss: 0.7895308454233901\n",
      "Epoch 88, Training Loss: 0.7895848976938348\n",
      "Epoch 89, Training Loss: 0.7894956046477296\n",
      "Epoch 90, Training Loss: 0.7896587653267653\n",
      "Epoch 91, Training Loss: 0.7899648027312487\n",
      "Epoch 92, Training Loss: 0.7897438145221625\n",
      "Epoch 93, Training Loss: 0.7895292404899024\n",
      "Epoch 94, Training Loss: 0.7896630022758828\n",
      "Epoch 95, Training Loss: 0.7894545467276323\n",
      "Epoch 96, Training Loss: 0.7896286814732659\n",
      "Epoch 97, Training Loss: 0.7892900158588151\n",
      "Epoch 98, Training Loss: 0.7895151239588745\n",
      "Epoch 99, Training Loss: 0.7890303871685401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:01:35,986] Trial 186 finished with value: 0.6368 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7893250180366344\n",
      "Epoch 1, Training Loss: 0.9745490275827565\n",
      "Epoch 2, Training Loss: 0.9305209296986573\n",
      "Epoch 3, Training Loss: 0.9185678508048667\n",
      "Epoch 4, Training Loss: 0.9086699121876767\n",
      "Epoch 5, Training Loss: 0.8982946756190824\n",
      "Epoch 6, Training Loss: 0.8873456467363171\n",
      "Epoch 7, Training Loss: 0.8762110300530168\n",
      "Epoch 8, Training Loss: 0.8644740283937382\n",
      "Epoch 9, Training Loss: 0.8533661483822013\n",
      "Epoch 10, Training Loss: 0.8421144597512439\n",
      "Epoch 11, Training Loss: 0.8328420458879686\n",
      "Epoch 12, Training Loss: 0.825442227833253\n",
      "Epoch 13, Training Loss: 0.820166774441425\n",
      "Epoch 14, Training Loss: 0.8159176492153254\n",
      "Epoch 15, Training Loss: 0.8129525905264947\n",
      "Epoch 16, Training Loss: 0.8107119579960529\n",
      "Epoch 17, Training Loss: 0.8084021052919832\n",
      "Epoch 18, Training Loss: 0.8072890115859813\n",
      "Epoch 19, Training Loss: 0.805929298329174\n",
      "Epoch 20, Training Loss: 0.8050141544270336\n",
      "Epoch 21, Training Loss: 0.804960802802466\n",
      "Epoch 22, Training Loss: 0.8034349709525144\n",
      "Epoch 23, Training Loss: 0.8034616975856007\n",
      "Epoch 24, Training Loss: 0.8028204875781124\n",
      "Epoch 25, Training Loss: 0.8025588086673192\n",
      "Epoch 26, Training Loss: 0.802081054493897\n",
      "Epoch 27, Training Loss: 0.8011862898679604\n",
      "Epoch 28, Training Loss: 0.801699433739024\n",
      "Epoch 29, Training Loss: 0.801850858577212\n",
      "Epoch 30, Training Loss: 0.8013823765561097\n",
      "Epoch 31, Training Loss: 0.8011956870107723\n",
      "Epoch 32, Training Loss: 0.8003886334878162\n",
      "Epoch 33, Training Loss: 0.8003311114203661\n",
      "Epoch 34, Training Loss: 0.8000483685866334\n",
      "Epoch 35, Training Loss: 0.7999939682788418\n",
      "Epoch 36, Training Loss: 0.8002135006108678\n",
      "Epoch 37, Training Loss: 0.8003369628038621\n",
      "Epoch 38, Training Loss: 0.7997590346443922\n",
      "Epoch 39, Training Loss: 0.8001424421045117\n",
      "Epoch 40, Training Loss: 0.7992666411668734\n",
      "Epoch 41, Training Loss: 0.7993819421395323\n",
      "Epoch 42, Training Loss: 0.7989104309476408\n",
      "Epoch 43, Training Loss: 0.7992835207989342\n",
      "Epoch 44, Training Loss: 0.7988374899204512\n",
      "Epoch 45, Training Loss: 0.7996544422063613\n",
      "Epoch 46, Training Loss: 0.7988709430049237\n",
      "Epoch 47, Training Loss: 0.7987786831712365\n",
      "Epoch 48, Training Loss: 0.7988137266689673\n",
      "Epoch 49, Training Loss: 0.7981676600929489\n",
      "Epoch 50, Training Loss: 0.7987760356494359\n",
      "Epoch 51, Training Loss: 0.7985940485072315\n",
      "Epoch 52, Training Loss: 0.7988365218155367\n",
      "Epoch 53, Training Loss: 0.7980954101211146\n",
      "Epoch 54, Training Loss: 0.7982432142236179\n",
      "Epoch 55, Training Loss: 0.7983608028046171\n",
      "Epoch 56, Training Loss: 0.7982065683916996\n",
      "Epoch 57, Training Loss: 0.7978318548740301\n",
      "Epoch 58, Training Loss: 0.7974985202452294\n",
      "Epoch 59, Training Loss: 0.7978149467841127\n",
      "Epoch 60, Training Loss: 0.7975147254484937\n",
      "Epoch 61, Training Loss: 0.7974581421766066\n",
      "Epoch 62, Training Loss: 0.7969199768582681\n",
      "Epoch 63, Training Loss: 0.7975407893496348\n",
      "Epoch 64, Training Loss: 0.7972160580462979\n",
      "Epoch 65, Training Loss: 0.797025392019659\n",
      "Epoch 66, Training Loss: 0.7967330131315647\n",
      "Epoch 67, Training Loss: 0.79671729946495\n",
      "Epoch 68, Training Loss: 0.7962174976678719\n",
      "Epoch 69, Training Loss: 0.7964567895222427\n",
      "Epoch 70, Training Loss: 0.7970593515195344\n",
      "Epoch 71, Training Loss: 0.7955635643095003\n",
      "Epoch 72, Training Loss: 0.795216727615299\n",
      "Epoch 73, Training Loss: 0.7955218530239019\n",
      "Epoch 74, Training Loss: 0.7949872374534607\n",
      "Epoch 75, Training Loss: 0.7952475615910122\n",
      "Epoch 76, Training Loss: 0.795337328337189\n",
      "Epoch 77, Training Loss: 0.7953742768531455\n",
      "Epoch 78, Training Loss: 0.7947827624199085\n",
      "Epoch 79, Training Loss: 0.7941948167811659\n",
      "Epoch 80, Training Loss: 0.7941889678177081\n",
      "Epoch 81, Training Loss: 0.7952688822172638\n",
      "Epoch 82, Training Loss: 0.794324016660676\n",
      "Epoch 83, Training Loss: 0.794071520540051\n",
      "Epoch 84, Training Loss: 0.794380798196434\n",
      "Epoch 85, Training Loss: 0.7951492473595124\n",
      "Epoch 86, Training Loss: 0.7943688368438778\n",
      "Epoch 87, Training Loss: 0.7936304160078665\n",
      "Epoch 88, Training Loss: 0.794412660957279\n",
      "Epoch 89, Training Loss: 0.7937427030470138\n",
      "Epoch 90, Training Loss: 0.7933362393450917\n",
      "Epoch 91, Training Loss: 0.7937758008340248\n",
      "Epoch 92, Training Loss: 0.7937119174720649\n",
      "Epoch 93, Training Loss: 0.7932712775424011\n",
      "Epoch 94, Training Loss: 0.7943283411793243\n",
      "Epoch 95, Training Loss: 0.7932391100360039\n",
      "Epoch 96, Training Loss: 0.793203394036544\n",
      "Epoch 97, Training Loss: 0.7933166347948232\n",
      "Epoch 98, Training Loss: 0.7936854392066037\n",
      "Epoch 99, Training Loss: 0.792917521824514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:02:58,460] Trial 187 finished with value: 0.6332 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7929610793751881\n",
      "Epoch 1, Training Loss: 1.091795932264889\n",
      "Epoch 2, Training Loss: 1.0878262216904584\n",
      "Epoch 3, Training Loss: 1.0844480973131516\n",
      "Epoch 4, Training Loss: 1.0802858006252962\n",
      "Epoch 5, Training Loss: 1.0752516017240636\n",
      "Epoch 6, Training Loss: 1.0686998454262229\n",
      "Epoch 7, Training Loss: 1.0601846801533419\n",
      "Epoch 8, Training Loss: 1.0495737273552839\n",
      "Epoch 9, Training Loss: 1.0367784314997055\n",
      "Epoch 10, Training Loss: 1.022877605452257\n",
      "Epoch 11, Training Loss: 1.0093039642362034\n",
      "Epoch 12, Training Loss: 0.9976409238226274\n",
      "Epoch 13, Training Loss: 0.9884398644811967\n",
      "Epoch 14, Training Loss: 0.9813985669612885\n",
      "Epoch 15, Training Loss: 0.975992166294771\n",
      "Epoch 16, Training Loss: 0.971625845712774\n",
      "Epoch 17, Training Loss: 0.968040645683513\n",
      "Epoch 18, Training Loss: 0.9649970034991994\n",
      "Epoch 19, Training Loss: 0.962433421050801\n",
      "Epoch 20, Training Loss: 0.9604619602596058\n",
      "Epoch 21, Training Loss: 0.9587265269896563\n",
      "Epoch 22, Training Loss: 0.9573414169339572\n",
      "Epoch 23, Training Loss: 0.9560861999147079\n",
      "Epoch 24, Training Loss: 0.9550961870305679\n",
      "Epoch 25, Training Loss: 0.9542638849510866\n",
      "Epoch 26, Training Loss: 0.9532696144721088\n",
      "Epoch 27, Training Loss: 0.9524685085521025\n",
      "Epoch 28, Training Loss: 0.9516110876027275\n",
      "Epoch 29, Training Loss: 0.9507114571683547\n",
      "Epoch 30, Training Loss: 0.9497493626790888\n",
      "Epoch 31, Training Loss: 0.9488470435142518\n",
      "Epoch 32, Training Loss: 0.9479574319895576\n",
      "Epoch 33, Training Loss: 0.946994110696456\n",
      "Epoch 34, Training Loss: 0.9459893713277929\n",
      "Epoch 35, Training Loss: 0.944973836996976\n",
      "Epoch 36, Training Loss: 0.9439755030940561\n",
      "Epoch 37, Training Loss: 0.942748456211651\n",
      "Epoch 38, Training Loss: 0.9415994986365823\n",
      "Epoch 39, Training Loss: 0.9405215606969946\n",
      "Epoch 40, Training Loss: 0.9392672808731304\n",
      "Epoch 41, Training Loss: 0.9379758743678822\n",
      "Epoch 42, Training Loss: 0.9366033225900987\n",
      "Epoch 43, Training Loss: 0.9352283659402062\n",
      "Epoch 44, Training Loss: 0.9337526548610015\n",
      "Epoch 45, Training Loss: 0.9321862595221576\n",
      "Epoch 46, Training Loss: 0.9304867763379041\n",
      "Epoch 47, Training Loss: 0.9288584046504077\n",
      "Epoch 48, Training Loss: 0.9270259896446676\n",
      "Epoch 49, Training Loss: 0.925133806957918\n",
      "Epoch 50, Training Loss: 0.9231982574042152\n",
      "Epoch 51, Training Loss: 0.921082532896715\n",
      "Epoch 52, Training Loss: 0.9189623587271747\n",
      "Epoch 53, Training Loss: 0.9165361063620624\n",
      "Epoch 54, Training Loss: 0.9141757779962876\n",
      "Epoch 55, Training Loss: 0.9115688034366158\n",
      "Epoch 56, Training Loss: 0.908905372689752\n",
      "Epoch 57, Training Loss: 0.9062041062467239\n",
      "Epoch 58, Training Loss: 0.9032550845426671\n",
      "Epoch 59, Training Loss: 0.9002936312030343\n",
      "Epoch 60, Training Loss: 0.8971925638703739\n",
      "Epoch 61, Training Loss: 0.8939754285531886\n",
      "Epoch 62, Training Loss: 0.8907141692498151\n",
      "Epoch 63, Training Loss: 0.8874390344058766\n",
      "Epoch 64, Training Loss: 0.8840895766370437\n",
      "Epoch 65, Training Loss: 0.8806384543110343\n",
      "Epoch 66, Training Loss: 0.8773216279113993\n",
      "Epoch 67, Training Loss: 0.8739651884752161\n",
      "Epoch 68, Training Loss: 0.8705917335959041\n",
      "Epoch 69, Training Loss: 0.8672719600621391\n",
      "Epoch 70, Training Loss: 0.8641417307713453\n",
      "Epoch 71, Training Loss: 0.8610571131986731\n",
      "Epoch 72, Training Loss: 0.8581068908467012\n",
      "Epoch 73, Training Loss: 0.8553481404220357\n",
      "Epoch 74, Training Loss: 0.8526293365394368\n",
      "Epoch 75, Training Loss: 0.8500198660177343\n",
      "Epoch 76, Training Loss: 0.8475734782920165\n",
      "Epoch 77, Training Loss: 0.8452649899090038\n",
      "Epoch 78, Training Loss: 0.8431938313035404\n",
      "Epoch 79, Training Loss: 0.8411702271770028\n",
      "Epoch 80, Training Loss: 0.8392731981417713\n",
      "Epoch 81, Training Loss: 0.8376879481007071\n",
      "Epoch 82, Training Loss: 0.8359452450976652\n",
      "Epoch 83, Training Loss: 0.8344779693379122\n",
      "Epoch 84, Training Loss: 0.8330308877720553\n",
      "Epoch 85, Training Loss: 0.8317671355079203\n",
      "Epoch 86, Training Loss: 0.8306411574868595\n",
      "Epoch 87, Training Loss: 0.8293406501237084\n",
      "Epoch 88, Training Loss: 0.8283203428633072\n",
      "Epoch 89, Training Loss: 0.8273561773580663\n",
      "Epoch 90, Training Loss: 0.8263103330836576\n",
      "Epoch 91, Training Loss: 0.8255209684371948\n",
      "Epoch 92, Training Loss: 0.8247000929888557\n",
      "Epoch 93, Training Loss: 0.8238534769591164\n",
      "Epoch 94, Training Loss: 0.8230464771214654\n",
      "Epoch 95, Training Loss: 0.8224796936091254\n",
      "Epoch 96, Training Loss: 0.8218500313338112\n",
      "Epoch 97, Training Loss: 0.8212592477658216\n",
      "Epoch 98, Training Loss: 0.8205326524201562\n",
      "Epoch 99, Training Loss: 0.8198597070048838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:04:43,598] Trial 188 finished with value: 0.6207333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8195398483556859\n",
      "Epoch 1, Training Loss: 0.8967321076787504\n",
      "Epoch 2, Training Loss: 0.8244925668365077\n",
      "Epoch 3, Training Loss: 0.8143958878696413\n",
      "Epoch 4, Training Loss: 0.8092528159457042\n",
      "Epoch 5, Training Loss: 0.8051096260995794\n",
      "Epoch 6, Training Loss: 0.8021638702629204\n",
      "Epoch 7, Training Loss: 0.8006416128990346\n",
      "Epoch 8, Training Loss: 0.799742146900722\n",
      "Epoch 9, Training Loss: 0.7983387133232633\n",
      "Epoch 10, Training Loss: 0.7961167449341681\n",
      "Epoch 11, Training Loss: 0.7966904740584524\n",
      "Epoch 12, Training Loss: 0.7958319967850707\n",
      "Epoch 13, Training Loss: 0.7943621381781155\n",
      "Epoch 14, Training Loss: 0.7939256331974403\n",
      "Epoch 15, Training Loss: 0.7941638774441597\n",
      "Epoch 16, Training Loss: 0.7922706061736086\n",
      "Epoch 17, Training Loss: 0.7930733719266447\n",
      "Epoch 18, Training Loss: 0.7925535723679048\n",
      "Epoch 19, Training Loss: 0.7919709581181519\n",
      "Epoch 20, Training Loss: 0.7914136485497755\n",
      "Epoch 21, Training Loss: 0.7910560209948317\n",
      "Epoch 22, Training Loss: 0.7907624860455219\n",
      "Epoch 23, Training Loss: 0.7902788698225093\n",
      "Epoch 24, Training Loss: 0.7905268142097874\n",
      "Epoch 25, Training Loss: 0.790615937853218\n",
      "Epoch 26, Training Loss: 0.7907368907354828\n",
      "Epoch 27, Training Loss: 0.7899920662542931\n",
      "Epoch 28, Training Loss: 0.7895611242244118\n",
      "Epoch 29, Training Loss: 0.7891744237197073\n",
      "Epoch 30, Training Loss: 0.7890249826854333\n",
      "Epoch 31, Training Loss: 0.7899440404167749\n",
      "Epoch 32, Training Loss: 0.7878982915914148\n",
      "Epoch 33, Training Loss: 0.788468266967544\n",
      "Epoch 34, Training Loss: 0.7883481252014188\n",
      "Epoch 35, Training Loss: 0.7887029008757799\n",
      "Epoch 36, Training Loss: 0.7886386744061807\n",
      "Epoch 37, Training Loss: 0.7884131099048414\n",
      "Epoch 38, Training Loss: 0.788758265075827\n",
      "Epoch 39, Training Loss: 0.7877460032477415\n",
      "Epoch 40, Training Loss: 0.7877822968296538\n",
      "Epoch 41, Training Loss: 0.7868042361467404\n",
      "Epoch 42, Training Loss: 0.7876950229020944\n",
      "Epoch 43, Training Loss: 0.7866844046384769\n",
      "Epoch 44, Training Loss: 0.7870924974742689\n",
      "Epoch 45, Training Loss: 0.7866021248631011\n",
      "Epoch 46, Training Loss: 0.7869436749838349\n",
      "Epoch 47, Training Loss: 0.7865213544745194\n",
      "Epoch 48, Training Loss: 0.7879968281079056\n",
      "Epoch 49, Training Loss: 0.7876292618593775\n",
      "Epoch 50, Training Loss: 0.7865563085204677\n",
      "Epoch 51, Training Loss: 0.7855619470427807\n",
      "Epoch 52, Training Loss: 0.7864848225636589\n",
      "Epoch 53, Training Loss: 0.7869068010409076\n",
      "Epoch 54, Training Loss: 0.7868151424522687\n",
      "Epoch 55, Training Loss: 0.7861843620027814\n",
      "Epoch 56, Training Loss: 0.7862117369372146\n",
      "Epoch 57, Training Loss: 0.7861786381642621\n",
      "Epoch 58, Training Loss: 0.7856158039175478\n",
      "Epoch 59, Training Loss: 0.7860487472742124\n",
      "Epoch 60, Training Loss: 0.7861772819569236\n",
      "Epoch 61, Training Loss: 0.7861759810519398\n",
      "Epoch 62, Training Loss: 0.7862549796140283\n",
      "Epoch 63, Training Loss: 0.786192008068687\n",
      "Epoch 64, Training Loss: 0.785881901773295\n",
      "Epoch 65, Training Loss: 0.7859622925744021\n",
      "Epoch 66, Training Loss: 0.7864516014443305\n",
      "Epoch 67, Training Loss: 0.7852527649779069\n",
      "Epoch 68, Training Loss: 0.7848335379496553\n",
      "Epoch 69, Training Loss: 0.7866505010683734\n",
      "Epoch 70, Training Loss: 0.7849130586573952\n",
      "Epoch 71, Training Loss: 0.7856641293468332\n",
      "Epoch 72, Training Loss: 0.7857195385416648\n",
      "Epoch 73, Training Loss: 0.785655563218253\n",
      "Epoch 74, Training Loss: 0.7854512969354042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:06:06,876] Trial 189 finished with value: 0.6216666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7854886769352103\n",
      "Epoch 1, Training Loss: 0.9526230324479871\n",
      "Epoch 2, Training Loss: 0.881659904458469\n",
      "Epoch 3, Training Loss: 0.836507288853925\n",
      "Epoch 4, Training Loss: 0.819404173255863\n",
      "Epoch 5, Training Loss: 0.8146110648499396\n",
      "Epoch 6, Training Loss: 0.8113317045054041\n",
      "Epoch 7, Training Loss: 0.8103427146610461\n",
      "Epoch 8, Training Loss: 0.8098554090449684\n",
      "Epoch 9, Training Loss: 0.8086835037496753\n",
      "Epoch 10, Training Loss: 0.8085698575005495\n",
      "Epoch 11, Training Loss: 0.8082807680717985\n",
      "Epoch 12, Training Loss: 0.8066172606066654\n",
      "Epoch 13, Training Loss: 0.8067093109726009\n",
      "Epoch 14, Training Loss: 0.8063898540977249\n",
      "Epoch 15, Training Loss: 0.8060737432393813\n",
      "Epoch 16, Training Loss: 0.8057149439826048\n",
      "Epoch 17, Training Loss: 0.8049508162010881\n",
      "Epoch 18, Training Loss: 0.8052183229224126\n",
      "Epoch 19, Training Loss: 0.8050574900512408\n",
      "Epoch 20, Training Loss: 0.8043365046493989\n",
      "Epoch 21, Training Loss: 0.8037855486224469\n",
      "Epoch 22, Training Loss: 0.803912767969576\n",
      "Epoch 23, Training Loss: 0.8031718506848902\n",
      "Epoch 24, Training Loss: 0.8032140934377685\n",
      "Epoch 25, Training Loss: 0.8029331630333922\n",
      "Epoch 26, Training Loss: 0.8027172885443035\n",
      "Epoch 27, Training Loss: 0.8031523534229823\n",
      "Epoch 28, Training Loss: 0.8023506765975091\n",
      "Epoch 29, Training Loss: 0.8035178271451391\n",
      "Epoch 30, Training Loss: 0.801884565048648\n",
      "Epoch 31, Training Loss: 0.8017799867723221\n",
      "Epoch 32, Training Loss: 0.8016579068693003\n",
      "Epoch 33, Training Loss: 0.8019044202073176\n",
      "Epoch 34, Training Loss: 0.8023262463117901\n",
      "Epoch 35, Training Loss: 0.8010397018346571\n",
      "Epoch 36, Training Loss: 0.8015735393179987\n",
      "Epoch 37, Training Loss: 0.8010464297201401\n",
      "Epoch 38, Training Loss: 0.8005390514108471\n",
      "Epoch 39, Training Loss: 0.8003032754238387\n",
      "Epoch 40, Training Loss: 0.8003287327916999\n",
      "Epoch 41, Training Loss: 0.7999733629979585\n",
      "Epoch 42, Training Loss: 0.800110135490733\n",
      "Epoch 43, Training Loss: 0.8002115282797275\n",
      "Epoch 44, Training Loss: 0.7998903059421625\n",
      "Epoch 45, Training Loss: 0.7995281783261694\n",
      "Epoch 46, Training Loss: 0.7998073165578053\n",
      "Epoch 47, Training Loss: 0.799142969461312\n",
      "Epoch 48, Training Loss: 0.7991022039176826\n",
      "Epoch 49, Training Loss: 0.7992079408545243\n",
      "Epoch 50, Training Loss: 0.7994196798568382\n",
      "Epoch 51, Training Loss: 0.7989719421343696\n",
      "Epoch 52, Training Loss: 0.7988035952238212\n",
      "Epoch 53, Training Loss: 0.7994648013796125\n",
      "Epoch 54, Training Loss: 0.7987443348518888\n",
      "Epoch 55, Training Loss: 0.798902087910731\n",
      "Epoch 56, Training Loss: 0.7989164149850831\n",
      "Epoch 57, Training Loss: 0.7982624740528881\n",
      "Epoch 58, Training Loss: 0.798541792174031\n",
      "Epoch 59, Training Loss: 0.7986814554472615\n",
      "Epoch 60, Training Loss: 0.7982406131306985\n",
      "Epoch 61, Training Loss: 0.7983851338687696\n",
      "Epoch 62, Training Loss: 0.7983269716563978\n",
      "Epoch 63, Training Loss: 0.7981692329385227\n",
      "Epoch 64, Training Loss: 0.7983091209167824\n",
      "Epoch 65, Training Loss: 0.798270430959257\n",
      "Epoch 66, Training Loss: 0.7982555945116775\n",
      "Epoch 67, Training Loss: 0.7980602844317156\n",
      "Epoch 68, Training Loss: 0.7981891651798908\n",
      "Epoch 69, Training Loss: 0.7981478262664681\n",
      "Epoch 70, Training Loss: 0.7979900163815434\n",
      "Epoch 71, Training Loss: 0.7972994518459291\n",
      "Epoch 72, Training Loss: 0.7979585609041658\n",
      "Epoch 73, Training Loss: 0.7976829424836582\n",
      "Epoch 74, Training Loss: 0.7978084393013689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:07:22,285] Trial 190 finished with value: 0.6354 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7976265842753245\n",
      "Epoch 1, Training Loss: 0.8417541201675639\n",
      "Epoch 2, Training Loss: 0.815585032561246\n",
      "Epoch 3, Training Loss: 0.8147009174964007\n",
      "Epoch 4, Training Loss: 0.8098627253840951\n",
      "Epoch 5, Training Loss: 0.8110302911085241\n",
      "Epoch 6, Training Loss: 0.8099102638048284\n",
      "Epoch 7, Training Loss: 0.8080142707684461\n",
      "Epoch 8, Training Loss: 0.8058567522553837\n",
      "Epoch 9, Training Loss: 0.8057155467482174\n",
      "Epoch 10, Training Loss: 0.8060141611099243\n",
      "Epoch 11, Training Loss: 0.805614554531434\n",
      "Epoch 12, Training Loss: 0.8055352033587063\n",
      "Epoch 13, Training Loss: 0.8041972223450156\n",
      "Epoch 14, Training Loss: 0.8038060247898102\n",
      "Epoch 15, Training Loss: 0.8037255866387311\n",
      "Epoch 16, Training Loss: 0.8036580274385564\n",
      "Epoch 17, Training Loss: 0.8040776899982901\n",
      "Epoch 18, Training Loss: 0.8036422323479372\n",
      "Epoch 19, Training Loss: 0.804186204180998\n",
      "Epoch 20, Training Loss: 0.8030123822829303\n",
      "Epoch 21, Training Loss: 0.8039595660742591\n",
      "Epoch 22, Training Loss: 0.8043695613215951\n",
      "Epoch 23, Training Loss: 0.8037615380567663\n",
      "Epoch 24, Training Loss: 0.8026295952937182\n",
      "Epoch 25, Training Loss: 0.804091341565637\n",
      "Epoch 26, Training Loss: 0.802037915131625\n",
      "Epoch 27, Training Loss: 0.803005417655496\n",
      "Epoch 28, Training Loss: 0.8034668875441832\n",
      "Epoch 29, Training Loss: 0.8022827536218307\n",
      "Epoch 30, Training Loss: 0.8032863418494954\n",
      "Epoch 31, Training Loss: 0.8025768976351794\n",
      "Epoch 32, Training Loss: 0.8026886591490577\n",
      "Epoch 33, Training Loss: 0.802620224461836\n",
      "Epoch 34, Training Loss: 0.8014813315167146\n",
      "Epoch 35, Training Loss: 0.8017662686460159\n",
      "Epoch 36, Training Loss: 0.8018291345063377\n",
      "Epoch 37, Training Loss: 0.8014606461805456\n",
      "Epoch 38, Training Loss: 0.8015677036958583\n",
      "Epoch 39, Training Loss: 0.8014398860931397\n",
      "Epoch 40, Training Loss: 0.8009342577176936\n",
      "Epoch 41, Training Loss: 0.800744135239545\n",
      "Epoch 42, Training Loss: 0.8006239872119006\n",
      "Epoch 43, Training Loss: 0.8000719085160424\n",
      "Epoch 44, Training Loss: 0.800875860522775\n",
      "Epoch 45, Training Loss: 0.8012420009865481\n",
      "Epoch 46, Training Loss: 0.8009856435831856\n",
      "Epoch 47, Training Loss: 0.8006986903443056\n",
      "Epoch 48, Training Loss: 0.8008324048799627\n",
      "Epoch 49, Training Loss: 0.7997303274799795\n",
      "Epoch 50, Training Loss: 0.8001352064048542\n",
      "Epoch 51, Training Loss: 0.800750743711696\n",
      "Epoch 52, Training Loss: 0.8001228393526638\n",
      "Epoch 53, Training Loss: 0.8004248948658214\n",
      "Epoch 54, Training Loss: 0.7997307080381056\n",
      "Epoch 55, Training Loss: 0.8003358358495376\n",
      "Epoch 56, Training Loss: 0.7999057931759778\n",
      "Epoch 57, Training Loss: 0.7995433493922739\n",
      "Epoch 58, Training Loss: 0.7990150872398826\n",
      "Epoch 59, Training Loss: 0.7987470607196584\n",
      "Epoch 60, Training Loss: 0.7991673435183132\n",
      "Epoch 61, Training Loss: 0.7991601498687968\n",
      "Epoch 62, Training Loss: 0.7997206530851476\n",
      "Epoch 63, Training Loss: 0.7995075568732093\n",
      "Epoch 64, Training Loss: 0.7988710109626546\n",
      "Epoch 65, Training Loss: 0.7981767083616818\n",
      "Epoch 66, Training Loss: 0.7992987317197463\n",
      "Epoch 67, Training Loss: 0.8000873653327718\n",
      "Epoch 68, Training Loss: 0.799218511651544\n",
      "Epoch 69, Training Loss: 0.7987184280507704\n",
      "Epoch 70, Training Loss: 0.7985363137722016\n",
      "Epoch 71, Training Loss: 0.7997063732848448\n",
      "Epoch 72, Training Loss: 0.7990735617104698\n",
      "Epoch 73, Training Loss: 0.7986335371522343\n",
      "Epoch 74, Training Loss: 0.7989340520606322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:08:50,122] Trial 191 finished with value: 0.633 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7982288850055022\n",
      "Epoch 1, Training Loss: 0.8507116356316735\n",
      "Epoch 2, Training Loss: 0.8191008293628692\n",
      "Epoch 3, Training Loss: 0.8158556497097016\n",
      "Epoch 4, Training Loss: 0.8148120394875021\n",
      "Epoch 5, Training Loss: 0.8125424794589772\n",
      "Epoch 6, Training Loss: 0.8102412336012896\n",
      "Epoch 7, Training Loss: 0.8101086025378283\n",
      "Epoch 8, Training Loss: 0.808369376449024\n",
      "Epoch 9, Training Loss: 0.8077181142919204\n",
      "Epoch 10, Training Loss: 0.8073561940473669\n",
      "Epoch 11, Training Loss: 0.8070650911331176\n",
      "Epoch 12, Training Loss: 0.80638058942907\n",
      "Epoch 13, Training Loss: 0.8062272168608273\n",
      "Epoch 14, Training Loss: 0.8054206832717447\n",
      "Epoch 15, Training Loss: 0.8058536891376271\n",
      "Epoch 16, Training Loss: 0.8048843786996953\n",
      "Epoch 17, Training Loss: 0.8050572262090795\n",
      "Epoch 18, Training Loss: 0.8055482763402603\n",
      "Epoch 19, Training Loss: 0.8048456007592818\n",
      "Epoch 20, Training Loss: 0.8056685728185317\n",
      "Epoch 21, Training Loss: 0.8054090932537528\n",
      "Epoch 22, Training Loss: 0.8048635922459995\n",
      "Epoch 23, Training Loss: 0.8034793207224677\n",
      "Epoch 24, Training Loss: 0.8045655895681942\n",
      "Epoch 25, Training Loss: 0.8043760472185472\n",
      "Epoch 26, Training Loss: 0.8039065532824572\n",
      "Epoch 27, Training Loss: 0.8036329026783214\n",
      "Epoch 28, Training Loss: 0.8027732811955844\n",
      "Epoch 29, Training Loss: 0.8040122629614437\n",
      "Epoch 30, Training Loss: 0.8027514885453617\n",
      "Epoch 31, Training Loss: 0.8040165020437802\n",
      "Epoch 32, Training Loss: 0.8029868344699636\n",
      "Epoch 33, Training Loss: 0.8049250523483052\n",
      "Epoch 34, Training Loss: 0.8029700446830076\n",
      "Epoch 35, Training Loss: 0.8032379153195549\n",
      "Epoch 36, Training Loss: 0.8020323213408975\n",
      "Epoch 37, Training Loss: 0.8026755222853492\n",
      "Epoch 38, Training Loss: 0.8019140077338499\n",
      "Epoch 39, Training Loss: 0.8026224813040566\n",
      "Epoch 40, Training Loss: 0.8020296932669246\n",
      "Epoch 41, Training Loss: 0.800783678223105\n",
      "Epoch 42, Training Loss: 0.802064073787016\n",
      "Epoch 43, Training Loss: 0.8011125354907092\n",
      "Epoch 44, Training Loss: 0.8005643916831297\n",
      "Epoch 45, Training Loss: 0.7999874957168803\n",
      "Epoch 46, Training Loss: 0.8005647136183346\n",
      "Epoch 47, Training Loss: 0.7999156238051022\n",
      "Epoch 48, Training Loss: 0.8004631997557248\n",
      "Epoch 49, Training Loss: 0.8003607843202704\n",
      "Epoch 50, Training Loss: 0.800412861178903\n",
      "Epoch 51, Training Loss: 0.7992141754486981\n",
      "Epoch 52, Training Loss: 0.7999975792099447\n",
      "Epoch 53, Training Loss: 0.800523258377524\n",
      "Epoch 54, Training Loss: 0.7993626687807195\n",
      "Epoch 55, Training Loss: 0.7992344608727624\n",
      "Epoch 56, Training Loss: 0.798743046241648\n",
      "Epoch 57, Training Loss: 0.7984658739847296\n",
      "Epoch 58, Training Loss: 0.7995322229581721\n",
      "Epoch 59, Training Loss: 0.7991717886223513\n",
      "Epoch 60, Training Loss: 0.7985969271379358\n",
      "Epoch 61, Training Loss: 0.7983677755383884\n",
      "Epoch 62, Training Loss: 0.7986488422926734\n",
      "Epoch 63, Training Loss: 0.7980445969104767\n",
      "Epoch 64, Training Loss: 0.7985772490501404\n",
      "Epoch 65, Training Loss: 0.7971671902432161\n",
      "Epoch 66, Training Loss: 0.797430100440979\n",
      "Epoch 67, Training Loss: 0.7973113792082843\n",
      "Epoch 68, Training Loss: 0.7975666480204638\n",
      "Epoch 69, Training Loss: 0.7977581214904785\n",
      "Epoch 70, Training Loss: 0.7970393727106206\n",
      "Epoch 71, Training Loss: 0.7968482967685251\n",
      "Epoch 72, Training Loss: 0.7972736622305477\n",
      "Epoch 73, Training Loss: 0.7966867713367238\n",
      "Epoch 74, Training Loss: 0.7959929512528812\n",
      "Epoch 75, Training Loss: 0.7962714339003844\n",
      "Epoch 76, Training Loss: 0.7964925251287572\n",
      "Epoch 77, Training Loss: 0.7969046172674964\n",
      "Epoch 78, Training Loss: 0.7966332238561967\n",
      "Epoch 79, Training Loss: 0.7968039688643287\n",
      "Epoch 80, Training Loss: 0.7952535864184884\n",
      "Epoch 81, Training Loss: 0.7954725267606623\n",
      "Epoch 82, Training Loss: 0.7955026543140411\n",
      "Epoch 83, Training Loss: 0.795710512750289\n",
      "Epoch 84, Training Loss: 0.7957055633208331\n",
      "Epoch 85, Training Loss: 0.7952482409336987\n",
      "Epoch 86, Training Loss: 0.7961598589841057\n",
      "Epoch 87, Training Loss: 0.7949386754456689\n",
      "Epoch 88, Training Loss: 0.7955101733347949\n",
      "Epoch 89, Training Loss: 0.7942393042760737\n",
      "Epoch 90, Training Loss: 0.7951963151903714\n",
      "Epoch 91, Training Loss: 0.7959632733288933\n",
      "Epoch 92, Training Loss: 0.7944476665468777\n",
      "Epoch 93, Training Loss: 0.7955248352359323\n",
      "Epoch 94, Training Loss: 0.7954533471079434\n",
      "Epoch 95, Training Loss: 0.7954350164357353\n",
      "Epoch 96, Training Loss: 0.794244148941601\n",
      "Epoch 97, Training Loss: 0.7946494650139528\n",
      "Epoch 98, Training Loss: 0.7946989412167493\n",
      "Epoch 99, Training Loss: 0.7949371717957889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:10:48,165] Trial 192 finished with value: 0.6362666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7948463633481194\n",
      "Epoch 1, Training Loss: 0.9994934284597411\n",
      "Epoch 2, Training Loss: 0.9587395901966812\n",
      "Epoch 3, Training Loss: 0.9502403041473905\n",
      "Epoch 4, Training Loss: 0.9436381884087297\n",
      "Epoch 5, Training Loss: 0.936422872632966\n",
      "Epoch 6, Training Loss: 0.92904407045895\n",
      "Epoch 7, Training Loss: 0.921624369639203\n",
      "Epoch 8, Training Loss: 0.9136509750122415\n",
      "Epoch 9, Training Loss: 0.9061158790624231\n",
      "Epoch 10, Training Loss: 0.8982050337289509\n",
      "Epoch 11, Training Loss: 0.8905990728758332\n",
      "Epoch 12, Training Loss: 0.8831921264641267\n",
      "Epoch 13, Training Loss: 0.8752196190052463\n",
      "Epoch 14, Training Loss: 0.8680224118376136\n",
      "Epoch 15, Training Loss: 0.8615386561343544\n",
      "Epoch 16, Training Loss: 0.8547084279526446\n",
      "Epoch 17, Training Loss: 0.8498762398734129\n",
      "Epoch 18, Training Loss: 0.8444802951095696\n",
      "Epoch 19, Training Loss: 0.8394062029687982\n",
      "Epoch 20, Training Loss: 0.8350855449088534\n",
      "Epoch 21, Training Loss: 0.8315071416080446\n",
      "Epoch 22, Training Loss: 0.8288178733416967\n",
      "Epoch 23, Training Loss: 0.8256334011716054\n",
      "Epoch 24, Training Loss: 0.8229339956340933\n",
      "Epoch 25, Training Loss: 0.8215569963132529\n",
      "Epoch 26, Training Loss: 0.8190637271207079\n",
      "Epoch 27, Training Loss: 0.8179316855014716\n",
      "Epoch 28, Training Loss: 0.8166384493497978\n",
      "Epoch 29, Training Loss: 0.8147523830707808\n",
      "Epoch 30, Training Loss: 0.8140447816454378\n",
      "Epoch 31, Training Loss: 0.8137116892893511\n",
      "Epoch 32, Training Loss: 0.8127141559034362\n",
      "Epoch 33, Training Loss: 0.8121716043106595\n",
      "Epoch 34, Training Loss: 0.8116450608224797\n",
      "Epoch 35, Training Loss: 0.8109950422344351\n",
      "Epoch 36, Training Loss: 0.8103882389857356\n",
      "Epoch 37, Training Loss: 0.8102181189042285\n",
      "Epoch 38, Training Loss: 0.8100120191287278\n",
      "Epoch 39, Training Loss: 0.809716384661825\n",
      "Epoch 40, Training Loss: 0.8093307725468972\n",
      "Epoch 41, Training Loss: 0.8090738609321135\n",
      "Epoch 42, Training Loss: 0.808752043085887\n",
      "Epoch 43, Training Loss: 0.8086726611718199\n",
      "Epoch 44, Training Loss: 0.8083577479634966\n",
      "Epoch 45, Training Loss: 0.8073990020088684\n",
      "Epoch 46, Training Loss: 0.8074055219951429\n",
      "Epoch 47, Training Loss: 0.8072993546500242\n",
      "Epoch 48, Training Loss: 0.8073444880937275\n",
      "Epoch 49, Training Loss: 0.8071189610581649\n",
      "Epoch 50, Training Loss: 0.8063806485412712\n",
      "Epoch 51, Training Loss: 0.806375283226931\n",
      "Epoch 52, Training Loss: 0.8061580819294865\n",
      "Epoch 53, Training Loss: 0.8064411620448406\n",
      "Epoch 54, Training Loss: 0.8063029917559229\n",
      "Epoch 55, Training Loss: 0.8055548911704157\n",
      "Epoch 56, Training Loss: 0.8065279363689566\n",
      "Epoch 57, Training Loss: 0.8056026445295578\n",
      "Epoch 58, Training Loss: 0.8055959163751817\n",
      "Epoch 59, Training Loss: 0.8050348810683515\n",
      "Epoch 60, Training Loss: 0.8057480432933435\n",
      "Epoch 61, Training Loss: 0.8049827700270746\n",
      "Epoch 62, Training Loss: 0.8051665886004168\n",
      "Epoch 63, Training Loss: 0.8048275597113416\n",
      "Epoch 64, Training Loss: 0.8049161410869512\n",
      "Epoch 65, Training Loss: 0.8049028027326541\n",
      "Epoch 66, Training Loss: 0.8051464319229126\n",
      "Epoch 67, Training Loss: 0.8041223062608475\n",
      "Epoch 68, Training Loss: 0.8045820283710509\n",
      "Epoch 69, Training Loss: 0.8039861279322689\n",
      "Epoch 70, Training Loss: 0.8037143148873982\n",
      "Epoch 71, Training Loss: 0.8037735333120016\n",
      "Epoch 72, Training Loss: 0.8033666227993213\n",
      "Epoch 73, Training Loss: 0.8037456981221536\n",
      "Epoch 74, Training Loss: 0.8031370398693515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:11:50,280] Trial 193 finished with value: 0.6335333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8032235860824585\n",
      "Epoch 1, Training Loss: 1.109108012780211\n",
      "Epoch 2, Training Loss: 1.0838919731011067\n",
      "Epoch 3, Training Loss: 1.069675348217326\n",
      "Epoch 4, Training Loss: 1.0584511457529282\n",
      "Epoch 5, Training Loss: 1.0482453715532345\n",
      "Epoch 6, Training Loss: 1.0381988380188332\n",
      "Epoch 7, Training Loss: 1.0279114762643227\n",
      "Epoch 8, Training Loss: 1.0176982077440822\n",
      "Epoch 9, Training Loss: 1.0076086253151857\n",
      "Epoch 10, Training Loss: 0.9980080069455886\n",
      "Epoch 11, Training Loss: 0.9886446914278475\n",
      "Epoch 12, Training Loss: 0.9799497813210452\n",
      "Epoch 13, Training Loss: 0.9718684191990615\n",
      "Epoch 14, Training Loss: 0.9651361374030436\n",
      "Epoch 15, Training Loss: 0.9590486915487992\n",
      "Epoch 16, Training Loss: 0.9543942948033038\n",
      "Epoch 17, Training Loss: 0.9497359042777155\n",
      "Epoch 18, Training Loss: 0.946148815639037\n",
      "Epoch 19, Training Loss: 0.9432797648852929\n",
      "Epoch 20, Training Loss: 0.9407409442994827\n",
      "Epoch 21, Training Loss: 0.9386475200939896\n",
      "Epoch 22, Training Loss: 0.9369186310839832\n",
      "Epoch 23, Training Loss: 0.9355806451991089\n",
      "Epoch 24, Training Loss: 0.9344201839059816\n",
      "Epoch 25, Training Loss: 0.9323405195895891\n",
      "Epoch 26, Training Loss: 0.9313617781588905\n",
      "Epoch 27, Training Loss: 0.9302262382399767\n",
      "Epoch 28, Training Loss: 0.9290595613027873\n",
      "Epoch 29, Training Loss: 0.9283355186756392\n",
      "Epoch 30, Training Loss: 0.9268974052335983\n",
      "Epoch 31, Training Loss: 0.9263442850650702\n",
      "Epoch 32, Training Loss: 0.9252885536143655\n",
      "Epoch 33, Training Loss: 0.9244089621350281\n",
      "Epoch 34, Training Loss: 0.9237281912251523\n",
      "Epoch 35, Training Loss: 0.9226733110004798\n",
      "Epoch 36, Training Loss: 0.921499002338352\n",
      "Epoch 37, Training Loss: 0.9212577015833747\n",
      "Epoch 38, Training Loss: 0.9206078951520131\n",
      "Epoch 39, Training Loss: 0.919632381335237\n",
      "Epoch 40, Training Loss: 0.9188010216655588\n",
      "Epoch 41, Training Loss: 0.9178626485336993\n",
      "Epoch 42, Training Loss: 0.9169199308058373\n",
      "Epoch 43, Training Loss: 0.9159283426471223\n",
      "Epoch 44, Training Loss: 0.9158654503356245\n",
      "Epoch 45, Training Loss: 0.9147645251195233\n",
      "Epoch 46, Training Loss: 0.9136059872189859\n",
      "Epoch 47, Training Loss: 0.9127476907314215\n",
      "Epoch 48, Training Loss: 0.9123750391759371\n",
      "Epoch 49, Training Loss: 0.9115073001474366\n",
      "Epoch 50, Training Loss: 0.9106953534864841\n",
      "Epoch 51, Training Loss: 0.9097732250851797\n",
      "Epoch 52, Training Loss: 0.9088687010277483\n",
      "Epoch 53, Training Loss: 0.908461312243813\n",
      "Epoch 54, Training Loss: 0.9072859007613103\n",
      "Epoch 55, Training Loss: 0.9068332820906675\n",
      "Epoch 56, Training Loss: 0.9057054259723291\n",
      "Epoch 57, Training Loss: 0.9056142711997929\n",
      "Epoch 58, Training Loss: 0.9041104945025049\n",
      "Epoch 59, Training Loss: 0.9034269925346948\n",
      "Epoch 60, Training Loss: 0.9020921010720102\n",
      "Epoch 61, Training Loss: 0.9010164192744664\n",
      "Epoch 62, Training Loss: 0.9008976745426207\n",
      "Epoch 63, Training Loss: 0.9003974982670375\n",
      "Epoch 64, Training Loss: 0.8987557297362421\n",
      "Epoch 65, Training Loss: 0.8978997364080041\n",
      "Epoch 66, Training Loss: 0.8965682019864706\n",
      "Epoch 67, Training Loss: 0.8962950315690579\n",
      "Epoch 68, Training Loss: 0.8951071913977314\n",
      "Epoch 69, Training Loss: 0.8939147686599789\n",
      "Epoch 70, Training Loss: 0.8928395680915144\n",
      "Epoch 71, Training Loss: 0.8917459223503457\n",
      "Epoch 72, Training Loss: 0.8909983542628754\n",
      "Epoch 73, Training Loss: 0.8899465671159271\n",
      "Epoch 74, Training Loss: 0.8888515262675465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:13:00,298] Trial 194 finished with value: 0.5814 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8880802450323464\n",
      "Epoch 1, Training Loss: 0.9168104219436646\n",
      "Epoch 2, Training Loss: 0.8496177485409905\n",
      "Epoch 3, Training Loss: 0.8417240413497477\n",
      "Epoch 4, Training Loss: 0.8365104365348816\n",
      "Epoch 5, Training Loss: 0.8344941518587224\n",
      "Epoch 6, Training Loss: 0.8324059117541593\n",
      "Epoch 7, Training Loss: 0.8329160622288199\n",
      "Epoch 8, Training Loss: 0.8285679418199202\n",
      "Epoch 9, Training Loss: 0.8290334991146536\n",
      "Epoch 10, Training Loss: 0.8276530486695907\n",
      "Epoch 11, Training Loss: 0.8259138213185703\n",
      "Epoch 12, Training Loss: 0.828371276995715\n",
      "Epoch 13, Training Loss: 0.8268954769302816\n",
      "Epoch 14, Training Loss: 0.8255047682453605\n",
      "Epoch 15, Training Loss: 0.8264978274177103\n",
      "Epoch 16, Training Loss: 0.8256889037524953\n",
      "Epoch 17, Training Loss: 0.8252761460051817\n",
      "Epoch 18, Training Loss: 0.826877553953844\n",
      "Epoch 19, Training Loss: 0.8259593418766471\n",
      "Epoch 20, Training Loss: 0.8254074949376723\n",
      "Epoch 21, Training Loss: 0.8267275745728436\n",
      "Epoch 22, Training Loss: 0.8239417823623209\n",
      "Epoch 23, Training Loss: 0.8261626841040218\n",
      "Epoch 24, Training Loss: 0.8264388435027179\n",
      "Epoch 25, Training Loss: 0.8268350869767807\n",
      "Epoch 26, Training Loss: 0.8277290779001573\n",
      "Epoch 27, Training Loss: 0.8296224624269148\n",
      "Epoch 28, Training Loss: 0.8280182600021362\n",
      "Epoch 29, Training Loss: 0.8296002601174747\n",
      "Epoch 30, Training Loss: 0.8307582223415375\n",
      "Epoch 31, Training Loss: 0.8309494316577911\n",
      "Epoch 32, Training Loss: 0.8325828196020687\n",
      "Epoch 33, Training Loss: 0.8302487315149868\n",
      "Epoch 34, Training Loss: 0.8304630182771122\n",
      "Epoch 35, Training Loss: 0.8290812624202055\n",
      "Epoch 36, Training Loss: 0.8310997260318083\n",
      "Epoch 37, Training Loss: 0.8333944593457615\n",
      "Epoch 38, Training Loss: 0.8332198718716116\n",
      "Epoch 39, Training Loss: 0.8330151717101827\n",
      "Epoch 40, Training Loss: 0.8320775978705462\n",
      "Epoch 41, Training Loss: 0.832617806336459\n",
      "Epoch 42, Training Loss: 0.8308866284174078\n",
      "Epoch 43, Training Loss: 0.8308358221194323\n",
      "Epoch 44, Training Loss: 0.833855420070536\n",
      "Epoch 45, Training Loss: 0.8339546710603377\n",
      "Epoch 46, Training Loss: 0.8314276751349954\n",
      "Epoch 47, Training Loss: 0.8319759321914\n",
      "Epoch 48, Training Loss: 0.8338903893442715\n",
      "Epoch 49, Training Loss: 0.8320155328862807\n",
      "Epoch 50, Training Loss: 0.8298408265674816\n",
      "Epoch 51, Training Loss: 0.8315614224181456\n",
      "Epoch 52, Training Loss: 0.8319412755966187\n",
      "Epoch 53, Training Loss: 0.8318651770142947\n",
      "Epoch 54, Training Loss: 0.831603311510647\n",
      "Epoch 55, Training Loss: 0.8319818535271812\n",
      "Epoch 56, Training Loss: 0.831960721576915\n",
      "Epoch 57, Training Loss: 0.8320800614356995\n",
      "Epoch 58, Training Loss: 0.8311015952334685\n",
      "Epoch 59, Training Loss: 0.8305531792079701\n",
      "Epoch 60, Training Loss: 0.8326511260341195\n",
      "Epoch 61, Training Loss: 0.831716416723588\n",
      "Epoch 62, Training Loss: 0.8313225658500896\n",
      "Epoch 63, Training Loss: 0.8309872068377102\n",
      "Epoch 64, Training Loss: 0.8308145934693953\n",
      "Epoch 65, Training Loss: 0.8321026676542619\n",
      "Epoch 66, Training Loss: 0.8301195002303404\n",
      "Epoch 67, Training Loss: 0.8310773271672866\n",
      "Epoch 68, Training Loss: 0.8302429183791665\n",
      "Epoch 69, Training Loss: 0.8312416842404534\n",
      "Epoch 70, Training Loss: 0.8323628742554608\n",
      "Epoch 71, Training Loss: 0.8305544097984539\n",
      "Epoch 72, Training Loss: 0.829571518757764\n",
      "Epoch 73, Training Loss: 0.828905936760061\n",
      "Epoch 74, Training Loss: 0.8305662820619696\n",
      "Epoch 75, Training Loss: 0.8295703700710746\n",
      "Epoch 76, Training Loss: 0.8305225227860843\n",
      "Epoch 77, Training Loss: 0.8283893820818733\n",
      "Epoch 78, Training Loss: 0.8301088192182429\n",
      "Epoch 79, Training Loss: 0.8300700639977174\n",
      "Epoch 80, Training Loss: 0.8314078406025381\n",
      "Epoch 81, Training Loss: 0.8307793805178474\n",
      "Epoch 82, Training Loss: 0.830130091414732\n",
      "Epoch 83, Training Loss: 0.8288198455642252\n",
      "Epoch 84, Training Loss: 0.8295421257439781\n",
      "Epoch 85, Training Loss: 0.8322462508958929\n",
      "Epoch 86, Training Loss: 0.830836598592646\n",
      "Epoch 87, Training Loss: 0.8324272661349352\n",
      "Epoch 88, Training Loss: 0.8305248501020319\n",
      "Epoch 89, Training Loss: 0.8307570261815015\n",
      "Epoch 90, Training Loss: 0.8296914346779094\n",
      "Epoch 91, Training Loss: 0.8314771440449883\n",
      "Epoch 92, Training Loss: 0.8287091391928055\n",
      "Epoch 93, Training Loss: 0.8288703231250538\n",
      "Epoch 94, Training Loss: 0.8275710785388947\n",
      "Epoch 95, Training Loss: 0.8279520408546224\n",
      "Epoch 96, Training Loss: 0.8308259844779968\n",
      "Epoch 97, Training Loss: 0.830562984382405\n",
      "Epoch 98, Training Loss: 0.8297022161764257\n",
      "Epoch 99, Training Loss: 0.8280452320856206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:15:06,369] Trial 195 finished with value: 0.6164 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8289405321373658\n",
      "Epoch 1, Training Loss: 0.8777821681779974\n",
      "Epoch 2, Training Loss: 0.8169459216734942\n",
      "Epoch 3, Training Loss: 0.8085303479783675\n",
      "Epoch 4, Training Loss: 0.8037433237889233\n",
      "Epoch 5, Training Loss: 0.8009044921398163\n",
      "Epoch 6, Training Loss: 0.7992052884662852\n",
      "Epoch 7, Training Loss: 0.7977961746384116\n",
      "Epoch 8, Training Loss: 0.7967137842318591\n",
      "Epoch 9, Training Loss: 0.7957096792669858\n",
      "Epoch 10, Training Loss: 0.7957752223575817\n",
      "Epoch 11, Training Loss: 0.7948633915536544\n",
      "Epoch 12, Training Loss: 0.7930663852831896\n",
      "Epoch 13, Training Loss: 0.7921476100472843\n",
      "Epoch 14, Training Loss: 0.7924855373186224\n",
      "Epoch 15, Training Loss: 0.7918419488037334\n",
      "Epoch 16, Training Loss: 0.7914886048260857\n",
      "Epoch 17, Training Loss: 0.7909279523877537\n",
      "Epoch 18, Training Loss: 0.7915342351969551\n",
      "Epoch 19, Training Loss: 0.7916486062021817\n",
      "Epoch 20, Training Loss: 0.7905040721332326\n",
      "Epoch 21, Training Loss: 0.7898530418031355\n",
      "Epoch 22, Training Loss: 0.7900854912926168\n",
      "Epoch 23, Training Loss: 0.7889759172411526\n",
      "Epoch 24, Training Loss: 0.7900838276217965\n",
      "Epoch 25, Training Loss: 0.7888841793817633\n",
      "Epoch 26, Training Loss: 0.7895227939942304\n",
      "Epoch 27, Training Loss: 0.788932592237697\n",
      "Epoch 28, Training Loss: 0.7895656623559839\n",
      "Epoch 29, Training Loss: 0.7886749691121718\n",
      "Epoch 30, Training Loss: 0.7883698272003847\n",
      "Epoch 31, Training Loss: 0.7882326691290912\n",
      "Epoch 32, Training Loss: 0.7873757719993592\n",
      "Epoch 33, Training Loss: 0.7874900192372939\n",
      "Epoch 34, Training Loss: 0.7865334934346816\n",
      "Epoch 35, Training Loss: 0.7877358334905961\n",
      "Epoch 36, Training Loss: 0.7872071020042195\n",
      "Epoch 37, Training Loss: 0.7871739254979526\n",
      "Epoch 38, Training Loss: 0.7875370874124414\n",
      "Epoch 39, Training Loss: 0.7868704944498399\n",
      "Epoch 40, Training Loss: 0.7856242864973405\n",
      "Epoch 41, Training Loss: 0.7871189075357774\n",
      "Epoch 42, Training Loss: 0.7874244067949407\n",
      "Epoch 43, Training Loss: 0.7863113075845382\n",
      "Epoch 44, Training Loss: 0.7855886981767767\n",
      "Epoch 45, Training Loss: 0.786037758027806\n",
      "Epoch 46, Training Loss: 0.786153386480668\n",
      "Epoch 47, Training Loss: 0.7858686787240645\n",
      "Epoch 48, Training Loss: 0.7851992935292861\n",
      "Epoch 49, Training Loss: 0.7853114866509157\n",
      "Epoch 50, Training Loss: 0.7859205055938048\n",
      "Epoch 51, Training Loss: 0.786317745517282\n",
      "Epoch 52, Training Loss: 0.7843218389679404\n",
      "Epoch 53, Training Loss: 0.7851252897346721\n",
      "Epoch 54, Training Loss: 0.785925053358078\n",
      "Epoch 55, Training Loss: 0.7850295419552747\n",
      "Epoch 56, Training Loss: 0.7843622834542219\n",
      "Epoch 57, Training Loss: 0.7845283976021935\n",
      "Epoch 58, Training Loss: 0.7847159906695871\n",
      "Epoch 59, Training Loss: 0.7842339099855984\n",
      "Epoch 60, Training Loss: 0.7839239633083344\n",
      "Epoch 61, Training Loss: 0.7839316639479469\n",
      "Epoch 62, Training Loss: 0.7848156984413371\n",
      "Epoch 63, Training Loss: 0.7841012855137096\n",
      "Epoch 64, Training Loss: 0.7836649591081283\n",
      "Epoch 65, Training Loss: 0.7836328311527476\n",
      "Epoch 66, Training Loss: 0.7834721906044904\n",
      "Epoch 67, Training Loss: 0.7834455431208891\n",
      "Epoch 68, Training Loss: 0.7837391860344831\n",
      "Epoch 69, Training Loss: 0.7846616327061373\n",
      "Epoch 70, Training Loss: 0.7841008644244251\n",
      "Epoch 71, Training Loss: 0.7835309021613177\n",
      "Epoch 72, Training Loss: 0.784411089069703\n",
      "Epoch 73, Training Loss: 0.7839466687510995\n",
      "Epoch 74, Training Loss: 0.7829396729609546\n",
      "Epoch 75, Training Loss: 0.7840085499426898\n",
      "Epoch 76, Training Loss: 0.782133160198436\n",
      "Epoch 77, Training Loss: 0.7837650360079372\n",
      "Epoch 78, Training Loss: 0.7825379793784197\n",
      "Epoch 79, Training Loss: 0.7832126351665049\n",
      "Epoch 80, Training Loss: 0.783315259218216\n",
      "Epoch 81, Training Loss: 0.7825345436264487\n",
      "Epoch 82, Training Loss: 0.7829122437449063\n",
      "Epoch 83, Training Loss: 0.783425589799881\n",
      "Epoch 84, Training Loss: 0.7818435916479896\n",
      "Epoch 85, Training Loss: 0.7824953359716079\n",
      "Epoch 86, Training Loss: 0.7815729775148279\n",
      "Epoch 87, Training Loss: 0.7820959521742428\n",
      "Epoch 88, Training Loss: 0.7818704262200524\n",
      "Epoch 89, Training Loss: 0.7816064543583814\n",
      "Epoch 90, Training Loss: 0.7820470237731934\n",
      "Epoch 91, Training Loss: 0.7835889019685633\n",
      "Epoch 92, Training Loss: 0.7814175815442029\n",
      "Epoch 93, Training Loss: 0.7822705531821531\n",
      "Epoch 94, Training Loss: 0.7833538204782149\n",
      "Epoch 95, Training Loss: 0.7824964681793661\n",
      "Epoch 96, Training Loss: 0.7806470220930436\n",
      "Epoch 97, Training Loss: 0.7816341987778159\n",
      "Epoch 98, Training Loss: 0.782088231830036\n",
      "Epoch 99, Training Loss: 0.7810851542388692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:17:34,715] Trial 196 finished with value: 0.6392 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7820263080035939\n",
      "Epoch 1, Training Loss: 0.8702701008319855\n",
      "Epoch 2, Training Loss: 0.8129946185560788\n",
      "Epoch 3, Training Loss: 0.8052436851753908\n",
      "Epoch 4, Training Loss: 0.8000934621165781\n",
      "Epoch 5, Training Loss: 0.7963636829572566\n",
      "Epoch 6, Training Loss: 0.7939481404949637\n",
      "Epoch 7, Training Loss: 0.7925452369802138\n",
      "Epoch 8, Training Loss: 0.7909574047256919\n",
      "Epoch 9, Training Loss: 0.7899094744289623\n",
      "Epoch 10, Training Loss: 0.7883723073847153\n",
      "Epoch 11, Training Loss: 0.7880935349183924\n",
      "Epoch 12, Training Loss: 0.7873120153651518\n",
      "Epoch 13, Training Loss: 0.7872450556474574\n",
      "Epoch 14, Training Loss: 0.7860218110505273\n",
      "Epoch 15, Training Loss: 0.7856932114152347\n",
      "Epoch 16, Training Loss: 0.7856637986968545\n",
      "Epoch 17, Training Loss: 0.7850864150243647\n",
      "Epoch 18, Training Loss: 0.7849108203018413\n",
      "Epoch 19, Training Loss: 0.7840629410042482\n",
      "Epoch 20, Training Loss: 0.7841750395298004\n",
      "Epoch 21, Training Loss: 0.783812890403411\n",
      "Epoch 22, Training Loss: 0.7832389039151809\n",
      "Epoch 23, Training Loss: 0.782853787506328\n",
      "Epoch 24, Training Loss: 0.7828715675718644\n",
      "Epoch 25, Training Loss: 0.7824685125491199\n",
      "Epoch 26, Training Loss: 0.7824221106837778\n",
      "Epoch 27, Training Loss: 0.782171358571333\n",
      "Epoch 28, Training Loss: 0.7813190711947049\n",
      "Epoch 29, Training Loss: 0.7814727720793556\n",
      "Epoch 30, Training Loss: 0.7811164610526141\n",
      "Epoch 31, Training Loss: 0.7806333154790541\n",
      "Epoch 32, Training Loss: 0.7808926775174982\n",
      "Epoch 33, Training Loss: 0.7807763556873097\n",
      "Epoch 34, Training Loss: 0.7805806544247795\n",
      "Epoch 35, Training Loss: 0.7797829186215121\n",
      "Epoch 36, Training Loss: 0.7800866159972023\n",
      "Epoch 37, Training Loss: 0.7800510170179255\n",
      "Epoch 38, Training Loss: 0.7799437893839444\n",
      "Epoch 39, Training Loss: 0.7793968817065744\n",
      "Epoch 40, Training Loss: 0.779231287240982\n",
      "Epoch 41, Training Loss: 0.7787638275062336\n",
      "Epoch 42, Training Loss: 0.778436029658598\n",
      "Epoch 43, Training Loss: 0.7785666932077969\n",
      "Epoch 44, Training Loss: 0.7784660356185016\n",
      "Epoch 45, Training Loss: 0.7783982398229486\n",
      "Epoch 46, Training Loss: 0.7781383745810565\n",
      "Epoch 47, Training Loss: 0.7781810918275047\n",
      "Epoch 48, Training Loss: 0.7772975582235\n",
      "Epoch 49, Training Loss: 0.7772901890558355\n",
      "Epoch 50, Training Loss: 0.7773992170305813\n",
      "Epoch 51, Training Loss: 0.7772831729580374\n",
      "Epoch 52, Training Loss: 0.7766769623055177\n",
      "Epoch 53, Training Loss: 0.7770397166644826\n",
      "Epoch 54, Training Loss: 0.7770541778732749\n",
      "Epoch 55, Training Loss: 0.77702331255464\n",
      "Epoch 56, Training Loss: 0.7765364810999702\n",
      "Epoch 57, Training Loss: 0.7764380519530353\n",
      "Epoch 58, Training Loss: 0.7760986400351805\n",
      "Epoch 59, Training Loss: 0.7763347053527831\n",
      "Epoch 60, Training Loss: 0.7760189314449535\n",
      "Epoch 61, Training Loss: 0.7761332631111145\n",
      "Epoch 62, Training Loss: 0.775437343190698\n",
      "Epoch 63, Training Loss: 0.7757168190619524\n",
      "Epoch 64, Training Loss: 0.7755751299858094\n",
      "Epoch 65, Training Loss: 0.7751121932618759\n",
      "Epoch 66, Training Loss: 0.7749011841241051\n",
      "Epoch 67, Training Loss: 0.7753486203446108\n",
      "Epoch 68, Training Loss: 0.7745993100895601\n",
      "Epoch 69, Training Loss: 0.7746842081406538\n",
      "Epoch 70, Training Loss: 0.7745376182303709\n",
      "Epoch 71, Training Loss: 0.774545413536184\n",
      "Epoch 72, Training Loss: 0.7745595933409298\n",
      "Epoch 73, Training Loss: 0.7740153565827538\n",
      "Epoch 74, Training Loss: 0.7744342407058267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:19:23,463] Trial 197 finished with value: 0.6383333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7739207655542037\n",
      "Epoch 1, Training Loss: 1.005480019415126\n",
      "Epoch 2, Training Loss: 0.9222818516282474\n",
      "Epoch 3, Training Loss: 0.8928361295251286\n",
      "Epoch 4, Training Loss: 0.8425678602386923\n",
      "Epoch 5, Training Loss: 0.8140678049536312\n",
      "Epoch 6, Training Loss: 0.8088962048642776\n",
      "Epoch 7, Training Loss: 0.8057096569678363\n",
      "Epoch 8, Training Loss: 0.8048090226509992\n",
      "Epoch 9, Training Loss: 0.8030981680926155\n",
      "Epoch 10, Training Loss: 0.8018674026517307\n",
      "Epoch 11, Training Loss: 0.8009262896285337\n",
      "Epoch 12, Training Loss: 0.8001917765421026\n",
      "Epoch 13, Training Loss: 0.7991969208857592\n",
      "Epoch 14, Training Loss: 0.7985668298777412\n",
      "Epoch 15, Training Loss: 0.7973915663186242\n",
      "Epoch 16, Training Loss: 0.7969720902863671\n",
      "Epoch 17, Training Loss: 0.7959941255345064\n",
      "Epoch 18, Training Loss: 0.7957528849209056\n",
      "Epoch 19, Training Loss: 0.7949596320881563\n",
      "Epoch 20, Training Loss: 0.7941649178897633\n",
      "Epoch 21, Training Loss: 0.7933305853955885\n",
      "Epoch 22, Training Loss: 0.7927821770135094\n",
      "Epoch 23, Training Loss: 0.7919882406907923\n",
      "Epoch 24, Training Loss: 0.791198945536333\n",
      "Epoch 25, Training Loss: 0.7906218387098873\n",
      "Epoch 26, Training Loss: 0.7900625038146972\n",
      "Epoch 27, Training Loss: 0.7890518831505495\n",
      "Epoch 28, Training Loss: 0.7885302893554463\n",
      "Epoch 29, Training Loss: 0.7880277526378632\n",
      "Epoch 30, Training Loss: 0.7870172044108895\n",
      "Epoch 31, Training Loss: 0.7868018709210789\n",
      "Epoch 32, Training Loss: 0.7866639985056485\n",
      "Epoch 33, Training Loss: 0.7860356150655186\n",
      "Epoch 34, Training Loss: 0.7856148418258219\n",
      "Epoch 35, Training Loss: 0.7854994811029995\n",
      "Epoch 36, Training Loss: 0.7850430282424478\n",
      "Epoch 37, Training Loss: 0.7850629957984475\n",
      "Epoch 38, Training Loss: 0.784671834637137\n",
      "Epoch 39, Training Loss: 0.7844918539944817\n",
      "Epoch 40, Training Loss: 0.784215831896838\n",
      "Epoch 41, Training Loss: 0.7841831888872034\n",
      "Epoch 42, Training Loss: 0.7836112093925476\n",
      "Epoch 43, Training Loss: 0.7835537349476533\n",
      "Epoch 44, Training Loss: 0.7835509799508488\n",
      "Epoch 45, Training Loss: 0.7831917081860935\n",
      "Epoch 46, Training Loss: 0.7830700584018931\n",
      "Epoch 47, Training Loss: 0.7825069261999691\n",
      "Epoch 48, Training Loss: 0.7826091706051546\n",
      "Epoch 49, Training Loss: 0.7826770416428062\n",
      "Epoch 50, Training Loss: 0.7823595741215874\n",
      "Epoch 51, Training Loss: 0.7822858879145453\n",
      "Epoch 52, Training Loss: 0.7820671897775987\n",
      "Epoch 53, Training Loss: 0.7820230360592113\n",
      "Epoch 54, Training Loss: 0.7819066199835609\n",
      "Epoch 55, Training Loss: 0.7820348868650548\n",
      "Epoch 56, Training Loss: 0.7820173031442306\n",
      "Epoch 57, Training Loss: 0.7818234291497399\n",
      "Epoch 58, Training Loss: 0.7815152913682601\n",
      "Epoch 59, Training Loss: 0.7814941766682794\n",
      "Epoch 60, Training Loss: 0.7813632570996004\n",
      "Epoch 61, Training Loss: 0.7815275451015024\n",
      "Epoch 62, Training Loss: 0.7813091194629669\n",
      "Epoch 63, Training Loss: 0.7811959223186269\n",
      "Epoch 64, Training Loss: 0.7809999968725092\n",
      "Epoch 65, Training Loss: 0.780997067900265\n",
      "Epoch 66, Training Loss: 0.7807649026197546\n",
      "Epoch 67, Training Loss: 0.780873254958321\n",
      "Epoch 68, Training Loss: 0.7806971984751084\n",
      "Epoch 69, Training Loss: 0.7807980448358199\n",
      "Epoch 70, Training Loss: 0.7804698859242832\n",
      "Epoch 71, Training Loss: 0.7805569392092088\n",
      "Epoch 72, Training Loss: 0.780247973133536\n",
      "Epoch 73, Training Loss: 0.7804640443886027\n",
      "Epoch 74, Training Loss: 0.7801569029864143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:20:53,163] Trial 198 finished with value: 0.6390666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.780154173374176\n",
      "Epoch 1, Training Loss: 1.0911430887709883\n",
      "Epoch 2, Training Loss: 1.08939893586295\n",
      "Epoch 3, Training Loss: 1.08803433493564\n",
      "Epoch 4, Training Loss: 1.0866095668391178\n",
      "Epoch 5, Training Loss: 1.084869587152524\n",
      "Epoch 6, Training Loss: 1.0830428686357083\n",
      "Epoch 7, Training Loss: 1.0806388690059345\n",
      "Epoch 8, Training Loss: 1.0780501932129825\n",
      "Epoch 9, Training Loss: 1.0747752060567526\n",
      "Epoch 10, Training Loss: 1.070833202591516\n",
      "Epoch 11, Training Loss: 1.0661061034166723\n",
      "Epoch 12, Training Loss: 1.0603014868901188\n",
      "Epoch 13, Training Loss: 1.0532062709779668\n",
      "Epoch 14, Training Loss: 1.0452249279595855\n",
      "Epoch 15, Training Loss: 1.0358464621063461\n",
      "Epoch 16, Training Loss: 1.0261562643194557\n",
      "Epoch 17, Training Loss: 1.0163156452931856\n",
      "Epoch 18, Training Loss: 1.0072259804359953\n",
      "Epoch 19, Training Loss: 0.9993805937300947\n",
      "Epoch 20, Training Loss: 0.992703080087676\n",
      "Epoch 21, Training Loss: 0.9879278216146885\n",
      "Epoch 22, Training Loss: 0.9836190552639782\n",
      "Epoch 23, Training Loss: 0.9795944784816942\n",
      "Epoch 24, Training Loss: 0.9767192188958477\n",
      "Epoch 25, Training Loss: 0.974072245458015\n",
      "Epoch 26, Training Loss: 0.9712976533667486\n",
      "Epoch 27, Training Loss: 0.9700721012918573\n",
      "Epoch 28, Training Loss: 0.9673096990226803\n",
      "Epoch 29, Training Loss: 0.9662709865354954\n",
      "Epoch 30, Training Loss: 0.9642330492349496\n",
      "Epoch 31, Training Loss: 0.9628631425083132\n",
      "Epoch 32, Training Loss: 0.9615421415271616\n",
      "Epoch 33, Training Loss: 0.9607231808784312\n",
      "Epoch 34, Training Loss: 0.960342468265304\n",
      "Epoch 35, Training Loss: 0.9592758625073541\n",
      "Epoch 36, Training Loss: 0.9582706668323144\n",
      "Epoch 37, Training Loss: 0.9572966386501054\n",
      "Epoch 38, Training Loss: 0.956574549262685\n",
      "Epoch 39, Training Loss: 0.9563124297256757\n",
      "Epoch 40, Training Loss: 0.9555993835728868\n",
      "Epoch 41, Training Loss: 0.9554168604370347\n",
      "Epoch 42, Training Loss: 0.9542773098873912\n",
      "Epoch 43, Training Loss: 0.9540610970411085\n",
      "Epoch 44, Training Loss: 0.9536163844560323\n",
      "Epoch 45, Training Loss: 0.9523785049753978\n",
      "Epoch 46, Training Loss: 0.9520147821957008\n",
      "Epoch 47, Training Loss: 0.951437959455906\n",
      "Epoch 48, Training Loss: 0.9499063928772632\n",
      "Epoch 49, Training Loss: 0.9497827104159764\n",
      "Epoch 50, Training Loss: 0.9488721085670299\n",
      "Epoch 51, Training Loss: 0.9482760012597966\n",
      "Epoch 52, Training Loss: 0.9476356163060755\n",
      "Epoch 53, Training Loss: 0.9468918089579819\n",
      "Epoch 54, Training Loss: 0.9460813672022712\n",
      "Epoch 55, Training Loss: 0.9450152896400681\n",
      "Epoch 56, Training Loss: 0.9444477474779115\n",
      "Epoch 57, Training Loss: 0.9436081114568209\n",
      "Epoch 58, Training Loss: 0.9427852886063712\n",
      "Epoch 59, Training Loss: 0.9415369955220617\n",
      "Epoch 60, Training Loss: 0.9404037018467609\n",
      "Epoch 61, Training Loss: 0.9394300125595322\n",
      "Epoch 62, Training Loss: 0.938236509678059\n",
      "Epoch 63, Training Loss: 0.9373521056390346\n",
      "Epoch 64, Training Loss: 0.9362884179990094\n",
      "Epoch 65, Training Loss: 0.9345043526556259\n",
      "Epoch 66, Training Loss: 0.9335349313298562\n",
      "Epoch 67, Training Loss: 0.9323301758981288\n",
      "Epoch 68, Training Loss: 0.930933652157174\n",
      "Epoch 69, Training Loss: 0.9293284437710181\n",
      "Epoch 70, Training Loss: 0.9280346265412811\n",
      "Epoch 71, Training Loss: 0.9264659571468382\n",
      "Epoch 72, Training Loss: 0.925428701253762\n",
      "Epoch 73, Training Loss: 0.9234223111231524\n",
      "Epoch 74, Training Loss: 0.9216831330069922\n",
      "Epoch 75, Training Loss: 0.9192056984829723\n",
      "Epoch 76, Training Loss: 0.9180980829367961\n",
      "Epoch 77, Training Loss: 0.9155763497926239\n",
      "Epoch 78, Training Loss: 0.9132605910301208\n",
      "Epoch 79, Training Loss: 0.9111738177170431\n",
      "Epoch 80, Training Loss: 0.9094579254774223\n",
      "Epoch 81, Training Loss: 0.9066706547163483\n",
      "Epoch 82, Training Loss: 0.9042794826335476\n",
      "Epoch 83, Training Loss: 0.9017861783952641\n",
      "Epoch 84, Training Loss: 0.8994267345371103\n",
      "Epoch 85, Training Loss: 0.8966581073022426\n",
      "Epoch 86, Training Loss: 0.8941098515252421\n",
      "Epoch 87, Training Loss: 0.8919015363643044\n",
      "Epoch 88, Training Loss: 0.8887721627278435\n",
      "Epoch 89, Training Loss: 0.8867163016383809\n",
      "Epoch 90, Training Loss: 0.8843972565536212\n",
      "Epoch 91, Training Loss: 0.881194439866489\n",
      "Epoch 92, Training Loss: 0.8777115651539393\n",
      "Epoch 93, Training Loss: 0.8757928695893825\n",
      "Epoch 94, Training Loss: 0.8725941743169512\n",
      "Epoch 95, Training Loss: 0.8697975229499931\n",
      "Epoch 96, Training Loss: 0.8674788482207104\n",
      "Epoch 97, Training Loss: 0.8648110798427037\n",
      "Epoch 98, Training Loss: 0.8620211010588739\n",
      "Epoch 99, Training Loss: 0.8600419718520086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:22:25,034] Trial 199 finished with value: 0.5975333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8578219035514315\n",
      "Epoch 1, Training Loss: 1.113621970047628\n",
      "Epoch 2, Training Loss: 1.091423966293048\n",
      "Epoch 3, Training Loss: 1.0785157477945313\n",
      "Epoch 4, Training Loss: 1.0685258010276277\n",
      "Epoch 5, Training Loss: 1.059692644983306\n",
      "Epoch 6, Training Loss: 1.0513815273915914\n",
      "Epoch 7, Training Loss: 1.0430185305444817\n",
      "Epoch 8, Training Loss: 1.0345600094114031\n",
      "Epoch 9, Training Loss: 1.0260923790752439\n",
      "Epoch 10, Training Loss: 1.017844816078817\n",
      "Epoch 11, Training Loss: 1.009735147397321\n",
      "Epoch 12, Training Loss: 1.0014452975495418\n",
      "Epoch 13, Training Loss: 0.9937502784836562\n",
      "Epoch 14, Training Loss: 0.9870389124504606\n",
      "Epoch 15, Training Loss: 0.9803487180767203\n",
      "Epoch 16, Training Loss: 0.9746469372197202\n",
      "Epoch 17, Training Loss: 0.9693960495461199\n",
      "Epoch 18, Training Loss: 0.9652719391019721\n",
      "Epoch 19, Training Loss: 0.9613345243877038\n",
      "Epoch 20, Training Loss: 0.958482999998824\n",
      "Epoch 21, Training Loss: 0.9560440522387512\n",
      "Epoch 22, Training Loss: 0.9537952310160587\n",
      "Epoch 23, Training Loss: 0.9518829017653501\n",
      "Epoch 24, Training Loss: 0.9508994250369251\n",
      "Epoch 25, Training Loss: 0.9491509139089656\n",
      "Epoch 26, Training Loss: 0.947602545408378\n",
      "Epoch 27, Training Loss: 0.9467758690504203\n",
      "Epoch 28, Training Loss: 0.9454763936817198\n",
      "Epoch 29, Training Loss: 0.9447641428251912\n",
      "Epoch 30, Training Loss: 0.9442627621772594\n",
      "Epoch 31, Training Loss: 0.9429338378117497\n",
      "Epoch 32, Training Loss: 0.9422279977260676\n",
      "Epoch 33, Training Loss: 0.9418531303118942\n",
      "Epoch 34, Training Loss: 0.9402740705282169\n",
      "Epoch 35, Training Loss: 0.9400132155059872\n",
      "Epoch 36, Training Loss: 0.9387871806782887\n",
      "Epoch 37, Training Loss: 0.9384817754415641\n",
      "Epoch 38, Training Loss: 0.9382981345169527\n",
      "Epoch 39, Training Loss: 0.9367868059559872\n",
      "Epoch 40, Training Loss: 0.9364636363839745\n",
      "Epoch 41, Training Loss: 0.9357587452221634\n",
      "Epoch 42, Training Loss: 0.9347757296454637\n",
      "Epoch 43, Training Loss: 0.9342686356458448\n",
      "Epoch 44, Training Loss: 0.9334719228565245\n",
      "Epoch 45, Training Loss: 0.9329268949372428\n",
      "Epoch 46, Training Loss: 0.9320770311176328\n",
      "Epoch 47, Training Loss: 0.9318239881580037\n",
      "Epoch 48, Training Loss: 0.9306745180510041\n",
      "Epoch 49, Training Loss: 0.9302946435777765\n",
      "Epoch 50, Training Loss: 0.930227275808951\n",
      "Epoch 51, Training Loss: 0.9290669888482058\n",
      "Epoch 52, Training Loss: 0.9282804355585486\n",
      "Epoch 53, Training Loss: 0.9276611230427161\n",
      "Epoch 54, Training Loss: 0.9271203647878833\n",
      "Epoch 55, Training Loss: 0.9270487157025732\n",
      "Epoch 56, Training Loss: 0.9261536352616504\n",
      "Epoch 57, Training Loss: 0.9250759850767322\n",
      "Epoch 58, Training Loss: 0.9249494376935457\n",
      "Epoch 59, Training Loss: 0.9244556892187076\n",
      "Epoch 60, Training Loss: 0.9233397086760156\n",
      "Epoch 61, Training Loss: 0.9232976192818548\n",
      "Epoch 62, Training Loss: 0.9226164645718452\n",
      "Epoch 63, Training Loss: 0.9218170454627589\n",
      "Epoch 64, Training Loss: 0.9210509445434226\n",
      "Epoch 65, Training Loss: 0.9201953954266426\n",
      "Epoch 66, Training Loss: 0.9201765986313497\n",
      "Epoch 67, Training Loss: 0.9194990799839335\n",
      "Epoch 68, Training Loss: 0.917984770652943\n",
      "Epoch 69, Training Loss: 0.9179207000517308\n",
      "Epoch 70, Training Loss: 0.9173545685029567\n",
      "Epoch 71, Training Loss: 0.916578003248774\n",
      "Epoch 72, Training Loss: 0.9158720997043122\n",
      "Epoch 73, Training Loss: 0.9155345077801468\n",
      "Epoch 74, Training Loss: 0.9145250475496278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:23:33,464] Trial 200 finished with value: 0.5632666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9140590203435798\n",
      "Epoch 1, Training Loss: 0.9773596868479162\n",
      "Epoch 2, Training Loss: 0.9232491593611868\n",
      "Epoch 3, Training Loss: 0.9103525730900298\n",
      "Epoch 4, Training Loss: 0.8993575071033678\n",
      "Epoch 5, Training Loss: 0.8880613586060087\n",
      "Epoch 6, Training Loss: 0.8755082194070171\n",
      "Epoch 7, Training Loss: 0.8631733578846867\n",
      "Epoch 8, Training Loss: 0.851565624896745\n",
      "Epoch 9, Training Loss: 0.8406083997927214\n",
      "Epoch 10, Training Loss: 0.8312012033354967\n",
      "Epoch 11, Training Loss: 0.8238324081091056\n",
      "Epoch 12, Training Loss: 0.8187804359242432\n",
      "Epoch 13, Training Loss: 0.8147021582252101\n",
      "Epoch 14, Training Loss: 0.8118274745188261\n",
      "Epoch 15, Training Loss: 0.8089013433546052\n",
      "Epoch 16, Training Loss: 0.8073838951892423\n",
      "Epoch 17, Training Loss: 0.806699069729425\n",
      "Epoch 18, Training Loss: 0.8056124211253977\n",
      "Epoch 19, Training Loss: 0.8048325415840722\n",
      "Epoch 20, Training Loss: 0.804919131357867\n",
      "Epoch 21, Training Loss: 0.8034965490040026\n",
      "Epoch 22, Training Loss: 0.8030281924663629\n",
      "Epoch 23, Training Loss: 0.8033341901642935\n",
      "Epoch 24, Training Loss: 0.8025026748951216\n",
      "Epoch 25, Training Loss: 0.8025372313377552\n",
      "Epoch 26, Training Loss: 0.8020681594547473\n",
      "Epoch 27, Training Loss: 0.8016409458074355\n",
      "Epoch 28, Training Loss: 0.8013630539851081\n",
      "Epoch 29, Training Loss: 0.8006581237441615\n",
      "Epoch 30, Training Loss: 0.800299402466394\n",
      "Epoch 31, Training Loss: 0.8007842601690077\n",
      "Epoch 32, Training Loss: 0.8002888882070556\n",
      "Epoch 33, Training Loss: 0.8008036174272236\n",
      "Epoch 34, Training Loss: 0.8009857771091892\n",
      "Epoch 35, Training Loss: 0.8002528362704399\n",
      "Epoch 36, Training Loss: 0.8000222081528571\n",
      "Epoch 37, Training Loss: 0.80002811932026\n",
      "Epoch 38, Training Loss: 0.7994972604557984\n",
      "Epoch 39, Training Loss: 0.7994639939831611\n",
      "Epoch 40, Training Loss: 0.7995214333211569\n",
      "Epoch 41, Training Loss: 0.7999648752069115\n",
      "Epoch 42, Training Loss: 0.7984986488980458\n",
      "Epoch 43, Training Loss: 0.7992754753370931\n",
      "Epoch 44, Training Loss: 0.7987296049756215\n",
      "Epoch 45, Training Loss: 0.7983362888035022\n",
      "Epoch 46, Training Loss: 0.7977110165402406\n",
      "Epoch 47, Training Loss: 0.7984752011478395\n",
      "Epoch 48, Training Loss: 0.7976454010135249\n",
      "Epoch 49, Training Loss: 0.7977973324015625\n",
      "Epoch 50, Training Loss: 0.7979124281639444\n",
      "Epoch 51, Training Loss: 0.7972184698832663\n",
      "Epoch 52, Training Loss: 0.798622306934873\n",
      "Epoch 53, Training Loss: 0.7985303222684932\n",
      "Epoch 54, Training Loss: 0.7978527465261015\n",
      "Epoch 55, Training Loss: 0.7971649117039559\n",
      "Epoch 56, Training Loss: 0.7975491053179691\n",
      "Epoch 57, Training Loss: 0.7972729403273504\n",
      "Epoch 58, Training Loss: 0.7967319216047014\n",
      "Epoch 59, Training Loss: 0.7968116892907853\n",
      "Epoch 60, Training Loss: 0.797014409975898\n",
      "Epoch 61, Training Loss: 0.7971379153710559\n",
      "Epoch 62, Training Loss: 0.7968925284263783\n",
      "Epoch 63, Training Loss: 0.7966684999322533\n",
      "Epoch 64, Training Loss: 0.7971500630665542\n",
      "Epoch 65, Training Loss: 0.7956728473193664\n",
      "Epoch 66, Training Loss: 0.7955115536101779\n",
      "Epoch 67, Training Loss: 0.7959699251597985\n",
      "Epoch 68, Training Loss: 0.7964449987375647\n",
      "Epoch 69, Training Loss: 0.7957104938370841\n",
      "Epoch 70, Training Loss: 0.7950555387296174\n",
      "Epoch 71, Training Loss: 0.7950942372020923\n",
      "Epoch 72, Training Loss: 0.7953843080907836\n",
      "Epoch 73, Training Loss: 0.7945765545045523\n",
      "Epoch 74, Training Loss: 0.7946684652701357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:24:34,256] Trial 201 finished with value: 0.6348666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7948816808542811\n",
      "Epoch 1, Training Loss: 0.8914664429776809\n",
      "Epoch 2, Training Loss: 0.8139912147381726\n",
      "Epoch 3, Training Loss: 0.8130400175908032\n",
      "Epoch 4, Training Loss: 0.8060186085981481\n",
      "Epoch 5, Training Loss: 0.803189835618524\n",
      "Epoch 6, Training Loss: 0.7975633162610671\n",
      "Epoch 7, Training Loss: 0.796916498857386\n",
      "Epoch 8, Training Loss: 0.7946246658353244\n",
      "Epoch 9, Training Loss: 0.7931820242545184\n",
      "Epoch 10, Training Loss: 0.7923106261561899\n",
      "Epoch 11, Training Loss: 0.7931655544393202\n",
      "Epoch 12, Training Loss: 0.7908138357190525\n",
      "Epoch 13, Training Loss: 0.7909994424791897\n",
      "Epoch 14, Training Loss: 0.7908502293334287\n",
      "Epoch 15, Training Loss: 0.7904503607048707\n",
      "Epoch 16, Training Loss: 0.7899807876699111\n",
      "Epoch 17, Training Loss: 0.7886326993914211\n",
      "Epoch 18, Training Loss: 0.7885205588621251\n",
      "Epoch 19, Training Loss: 0.7889067575510811\n",
      "Epoch 20, Training Loss: 0.7884813079413245\n",
      "Epoch 21, Training Loss: 0.7875381819640889\n",
      "Epoch 22, Training Loss: 0.787882800803465\n",
      "Epoch 23, Training Loss: 0.7868637084259706\n",
      "Epoch 24, Training Loss: 0.7871487486362457\n",
      "Epoch 25, Training Loss: 0.7876960966166328\n",
      "Epoch 26, Training Loss: 0.7866695245574502\n",
      "Epoch 27, Training Loss: 0.7866487481313593\n",
      "Epoch 28, Training Loss: 0.7864754692245932\n",
      "Epoch 29, Training Loss: 0.7861217044381534\n",
      "Epoch 30, Training Loss: 0.7859886837005615\n",
      "Epoch 31, Training Loss: 0.7865420727870044\n",
      "Epoch 32, Training Loss: 0.7862839059268727\n",
      "Epoch 33, Training Loss: 0.7851740912128897\n",
      "Epoch 34, Training Loss: 0.7855690332721261\n",
      "Epoch 35, Training Loss: 0.7845247280597687\n",
      "Epoch 36, Training Loss: 0.7851683964448817\n",
      "Epoch 37, Training Loss: 0.7851229864008287\n",
      "Epoch 38, Training Loss: 0.7844299914556391\n",
      "Epoch 39, Training Loss: 0.7847004424824434\n",
      "Epoch 40, Training Loss: 0.7840583937308367\n",
      "Epoch 41, Training Loss: 0.7840619538812076\n",
      "Epoch 42, Training Loss: 0.783686843549504\n",
      "Epoch 43, Training Loss: 0.7836083736139186\n",
      "Epoch 44, Training Loss: 0.7834913408055025\n",
      "Epoch 45, Training Loss: 0.7840353901947246\n",
      "Epoch 46, Training Loss: 0.7836940914041856\n",
      "Epoch 47, Training Loss: 0.7835058333593257\n",
      "Epoch 48, Training Loss: 0.7825672555671018\n",
      "Epoch 49, Training Loss: 0.7836892866387086\n",
      "Epoch 50, Training Loss: 0.7830387827228097\n",
      "Epoch 51, Training Loss: 0.7828450181904961\n",
      "Epoch 52, Training Loss: 0.7828305071942947\n",
      "Epoch 53, Training Loss: 0.7831636264744927\n",
      "Epoch 54, Training Loss: 0.7823810267448426\n",
      "Epoch 55, Training Loss: 0.7823114053642048\n",
      "Epoch 56, Training Loss: 0.7824119223566617\n",
      "Epoch 57, Training Loss: 0.782604906278498\n",
      "Epoch 58, Training Loss: 0.7816324022236992\n",
      "Epoch 59, Training Loss: 0.7812440858167761\n",
      "Epoch 60, Training Loss: 0.781615753103705\n",
      "Epoch 61, Training Loss: 0.7807105780349058\n",
      "Epoch 62, Training Loss: 0.7829499483108521\n",
      "Epoch 63, Training Loss: 0.7814269903126885\n",
      "Epoch 64, Training Loss: 0.7808613315750571\n",
      "Epoch 65, Training Loss: 0.7806495999588686\n",
      "Epoch 66, Training Loss: 0.780584626267938\n",
      "Epoch 67, Training Loss: 0.7812372287581949\n",
      "Epoch 68, Training Loss: 0.7801516114964204\n",
      "Epoch 69, Training Loss: 0.7799658123885884\n",
      "Epoch 70, Training Loss: 0.7800143941009746\n",
      "Epoch 71, Training Loss: 0.7803149154606988\n",
      "Epoch 72, Training Loss: 0.7796889176789452\n",
      "Epoch 73, Training Loss: 0.7800735627202426\n",
      "Epoch 74, Training Loss: 0.7797408533797544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:26:25,627] Trial 202 finished with value: 0.6392666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7790801081937903\n",
      "Epoch 1, Training Loss: 1.0930104470790778\n",
      "Epoch 2, Training Loss: 1.0920560040868315\n",
      "Epoch 3, Training Loss: 1.0919015871851068\n",
      "Epoch 4, Training Loss: 1.091588977225741\n",
      "Epoch 5, Training Loss: 1.0915458845912962\n",
      "Epoch 6, Training Loss: 1.0913021354746997\n",
      "Epoch 7, Training Loss: 1.091253680214846\n",
      "Epoch 8, Training Loss: 1.0909189864208824\n",
      "Epoch 9, Training Loss: 1.0906738795732196\n",
      "Epoch 10, Training Loss: 1.0905725878880437\n",
      "Epoch 11, Training Loss: 1.0902741113103422\n",
      "Epoch 12, Training Loss: 1.0900910067379026\n",
      "Epoch 13, Training Loss: 1.0898871556260532\n",
      "Epoch 14, Training Loss: 1.0897426838265325\n",
      "Epoch 15, Training Loss: 1.0895324918560516\n",
      "Epoch 16, Training Loss: 1.0893080888834215\n",
      "Epoch 17, Training Loss: 1.0892345218730153\n",
      "Epoch 18, Training Loss: 1.088975191116333\n",
      "Epoch 19, Training Loss: 1.088837970826859\n",
      "Epoch 20, Training Loss: 1.0884826722897982\n",
      "Epoch 21, Training Loss: 1.088360397797778\n",
      "Epoch 22, Training Loss: 1.0881439252007277\n",
      "Epoch 23, Training Loss: 1.0880228255924425\n",
      "Epoch 24, Training Loss: 1.087770732721888\n",
      "Epoch 25, Training Loss: 1.087575462169217\n",
      "Epoch 26, Training Loss: 1.0873136502459533\n",
      "Epoch 27, Training Loss: 1.0871435095493058\n",
      "Epoch 28, Training Loss: 1.0869245473603557\n",
      "Epoch 29, Training Loss: 1.0866414688583603\n",
      "Epoch 30, Training Loss: 1.0864497862364118\n",
      "Epoch 31, Training Loss: 1.0861430962282912\n",
      "Epoch 32, Training Loss: 1.0860459953322446\n",
      "Epoch 33, Training Loss: 1.0858221878682761\n",
      "Epoch 34, Training Loss: 1.0855346555996659\n",
      "Epoch 35, Training Loss: 1.0853237204085615\n",
      "Epoch 36, Training Loss: 1.085127023825968\n",
      "Epoch 37, Training Loss: 1.0848086414480569\n",
      "Epoch 38, Training Loss: 1.084648121030707\n",
      "Epoch 39, Training Loss: 1.08428139561101\n",
      "Epoch 40, Training Loss: 1.084137006809837\n",
      "Epoch 41, Training Loss: 1.0839226620537894\n",
      "Epoch 42, Training Loss: 1.0836408392827315\n",
      "Epoch 43, Training Loss: 1.0832563828704949\n",
      "Epoch 44, Training Loss: 1.0831576790128434\n",
      "Epoch 45, Training Loss: 1.0828685937967515\n",
      "Epoch 46, Training Loss: 1.0826578509538693\n",
      "Epoch 47, Training Loss: 1.0823315688541957\n",
      "Epoch 48, Training Loss: 1.0820073764127\n",
      "Epoch 49, Training Loss: 1.0817210953934748\n",
      "Epoch 50, Training Loss: 1.081453451715914\n",
      "Epoch 51, Training Loss: 1.0811201821592518\n",
      "Epoch 52, Training Loss: 1.0808184704386201\n",
      "Epoch 53, Training Loss: 1.080541896461544\n",
      "Epoch 54, Training Loss: 1.0802511856968242\n",
      "Epoch 55, Training Loss: 1.0799916685075688\n",
      "Epoch 56, Training Loss: 1.0794684148372564\n",
      "Epoch 57, Training Loss: 1.079263030496755\n",
      "Epoch 58, Training Loss: 1.0789933080960037\n",
      "Epoch 59, Training Loss: 1.0785339507841527\n",
      "Epoch 60, Training Loss: 1.0781920881199658\n",
      "Epoch 61, Training Loss: 1.0777413525975736\n",
      "Epoch 62, Training Loss: 1.0775661963269227\n",
      "Epoch 63, Training Loss: 1.0771416610344908\n",
      "Epoch 64, Training Loss: 1.0767081389749857\n",
      "Epoch 65, Training Loss: 1.0763415619842989\n",
      "Epoch 66, Training Loss: 1.0759055713065584\n",
      "Epoch 67, Training Loss: 1.075551261937708\n",
      "Epoch 68, Training Loss: 1.075097011802788\n",
      "Epoch 69, Training Loss: 1.0748010931158425\n",
      "Epoch 70, Training Loss: 1.0743845713765998\n",
      "Epoch 71, Training Loss: 1.0738371200131294\n",
      "Epoch 72, Training Loss: 1.073498849223431\n",
      "Epoch 73, Training Loss: 1.0729777637280915\n",
      "Epoch 74, Training Loss: 1.0725019779420437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:27:34,387] Trial 203 finished with value: 0.4311333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 1.0720626698400741\n",
      "Epoch 1, Training Loss: 0.8497617652541712\n",
      "Epoch 2, Training Loss: 0.813361072181759\n",
      "Epoch 3, Training Loss: 0.808304616652037\n",
      "Epoch 4, Training Loss: 0.8042894899396968\n",
      "Epoch 5, Training Loss: 0.8027049512791454\n",
      "Epoch 6, Training Loss: 0.8021450708683272\n",
      "Epoch 7, Training Loss: 0.7998054086713863\n",
      "Epoch 8, Training Loss: 0.7990390929064356\n",
      "Epoch 9, Training Loss: 0.7985890795413713\n",
      "Epoch 10, Training Loss: 0.7970871626882625\n",
      "Epoch 11, Training Loss: 0.7971953660921943\n",
      "Epoch 12, Training Loss: 0.7976709286073097\n",
      "Epoch 13, Training Loss: 0.7968585286821638\n",
      "Epoch 14, Training Loss: 0.7953031947738246\n",
      "Epoch 15, Training Loss: 0.7951195893431069\n",
      "Epoch 16, Training Loss: 0.7944398793062769\n",
      "Epoch 17, Training Loss: 0.7945323614249552\n",
      "Epoch 18, Training Loss: 0.7941679409572057\n",
      "Epoch 19, Training Loss: 0.7934367591277101\n",
      "Epoch 20, Training Loss: 0.7932175193065988\n",
      "Epoch 21, Training Loss: 0.7941385944086806\n",
      "Epoch 22, Training Loss: 0.7938884623068616\n",
      "Epoch 23, Training Loss: 0.7928396582155299\n",
      "Epoch 24, Training Loss: 0.7936910135405404\n",
      "Epoch 25, Training Loss: 0.7925370057722679\n",
      "Epoch 26, Training Loss: 0.7929073094425345\n",
      "Epoch 27, Training Loss: 0.7926266282124627\n",
      "Epoch 28, Training Loss: 0.7922581810700265\n",
      "Epoch 29, Training Loss: 0.7924625580472158\n",
      "Epoch 30, Training Loss: 0.7919223745962731\n",
      "Epoch 31, Training Loss: 0.7921689028130439\n",
      "Epoch 32, Training Loss: 0.7922993434102912\n",
      "Epoch 33, Training Loss: 0.7925722331032717\n",
      "Epoch 34, Training Loss: 0.7913884136909829\n",
      "Epoch 35, Training Loss: 0.791390125285414\n",
      "Epoch 36, Training Loss: 0.7919388686803946\n",
      "Epoch 37, Training Loss: 0.7905482933933573\n",
      "Epoch 38, Training Loss: 0.7912460755584831\n",
      "Epoch 39, Training Loss: 0.790973519113727\n",
      "Epoch 40, Training Loss: 0.7910065088953291\n",
      "Epoch 41, Training Loss: 0.7902756427463732\n",
      "Epoch 42, Training Loss: 0.7915373305629071\n",
      "Epoch 43, Training Loss: 0.7904829367659145\n",
      "Epoch 44, Training Loss: 0.7910567708481523\n",
      "Epoch 45, Training Loss: 0.7912455945982969\n",
      "Epoch 46, Training Loss: 0.7898173756617353\n",
      "Epoch 47, Training Loss: 0.7900526556753574\n",
      "Epoch 48, Training Loss: 0.7902901038191372\n",
      "Epoch 49, Training Loss: 0.7910467946439758\n",
      "Epoch 50, Training Loss: 0.7900800713919159\n",
      "Epoch 51, Training Loss: 0.790167784780488\n",
      "Epoch 52, Training Loss: 0.7896526819781253\n",
      "Epoch 53, Training Loss: 0.7895730743730874\n",
      "Epoch 54, Training Loss: 0.7896551812501779\n",
      "Epoch 55, Training Loss: 0.7896211937854164\n",
      "Epoch 56, Training Loss: 0.7900133621423764\n",
      "Epoch 57, Training Loss: 0.7898930967302251\n",
      "Epoch 58, Training Loss: 0.7896317296458366\n",
      "Epoch 59, Training Loss: 0.7898285787804682\n",
      "Epoch 60, Training Loss: 0.7890814650775795\n",
      "Epoch 61, Training Loss: 0.7902040282586463\n",
      "Epoch 62, Training Loss: 0.790406082178417\n",
      "Epoch 63, Training Loss: 0.7902184568849722\n",
      "Epoch 64, Training Loss: 0.7898898912551707\n",
      "Epoch 65, Training Loss: 0.7895780532879937\n",
      "Epoch 66, Training Loss: 0.7891248737062727\n",
      "Epoch 67, Training Loss: 0.7899808350362276\n",
      "Epoch 68, Training Loss: 0.7891939442856868\n",
      "Epoch 69, Training Loss: 0.7899286301512467\n",
      "Epoch 70, Training Loss: 0.7893587780178042\n",
      "Epoch 71, Training Loss: 0.7896330644313554\n",
      "Epoch 72, Training Loss: 0.7896781169382253\n",
      "Epoch 73, Training Loss: 0.7891037541224544\n",
      "Epoch 74, Training Loss: 0.7896639502138123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:28:58,896] Trial 204 finished with value: 0.5718666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7900871958051409\n",
      "Epoch 1, Training Loss: 0.8410230794347319\n",
      "Epoch 2, Training Loss: 0.8104743118572952\n",
      "Epoch 3, Training Loss: 0.8044021421805361\n",
      "Epoch 4, Training Loss: 0.8023415954489457\n",
      "Epoch 5, Training Loss: 0.8001461969282394\n",
      "Epoch 6, Training Loss: 0.7956363193074564\n",
      "Epoch 7, Training Loss: 0.7965684366405459\n",
      "Epoch 8, Training Loss: 0.7980951501014537\n",
      "Epoch 9, Training Loss: 0.7959906436446914\n",
      "Epoch 10, Training Loss: 0.7971528652019071\n",
      "Epoch 11, Training Loss: 0.7943704148880522\n",
      "Epoch 12, Training Loss: 0.7938905896100783\n",
      "Epoch 13, Training Loss: 0.7946086751787286\n",
      "Epoch 14, Training Loss: 0.7943529587042959\n",
      "Epoch 15, Training Loss: 0.7944705996298252\n",
      "Epoch 16, Training Loss: 0.7940301716775823\n",
      "Epoch 17, Training Loss: 0.7933063010972246\n",
      "Epoch 18, Training Loss: 0.7925450612727861\n",
      "Epoch 19, Training Loss: 0.7915162018367222\n",
      "Epoch 20, Training Loss: 0.7914233806438016\n",
      "Epoch 21, Training Loss: 0.7910973881420337\n",
      "Epoch 22, Training Loss: 0.7907096619892837\n",
      "Epoch 23, Training Loss: 0.789610883526336\n",
      "Epoch 24, Training Loss: 0.7900000869779659\n",
      "Epoch 25, Training Loss: 0.790926852799896\n",
      "Epoch 26, Training Loss: 0.7907755688617104\n",
      "Epoch 27, Training Loss: 0.7909816933753795\n",
      "Epoch 28, Training Loss: 0.791609473604905\n",
      "Epoch 29, Training Loss: 0.7906436825157108\n",
      "Epoch 30, Training Loss: 0.79082564518864\n",
      "Epoch 31, Training Loss: 0.7891213979936184\n",
      "Epoch 32, Training Loss: 0.790322672782984\n",
      "Epoch 33, Training Loss: 0.7898873547862347\n",
      "Epoch 34, Training Loss: 0.7889635012562114\n",
      "Epoch 35, Training Loss: 0.7892067620628759\n",
      "Epoch 36, Training Loss: 0.7888369348712434\n",
      "Epoch 37, Training Loss: 0.7884971734276391\n",
      "Epoch 38, Training Loss: 0.7897442264664442\n",
      "Epoch 39, Training Loss: 0.7890541047978222\n",
      "Epoch 40, Training Loss: 0.7886155793989511\n",
      "Epoch 41, Training Loss: 0.7884027883522493\n",
      "Epoch 42, Training Loss: 0.7881525105999825\n",
      "Epoch 43, Training Loss: 0.7887422844879609\n",
      "Epoch 44, Training Loss: 0.788615602210052\n",
      "Epoch 45, Training Loss: 0.7891867231605645\n",
      "Epoch 46, Training Loss: 0.7891898543314826\n",
      "Epoch 47, Training Loss: 0.788850356223888\n",
      "Epoch 48, Training Loss: 0.7898474241557875\n",
      "Epoch 49, Training Loss: 0.7890844649838326\n",
      "Epoch 50, Training Loss: 0.7895986001294358\n",
      "Epoch 51, Training Loss: 0.7912757558033878\n",
      "Epoch 52, Training Loss: 0.7887175948099983\n",
      "Epoch 53, Training Loss: 0.7894002876783672\n",
      "Epoch 54, Training Loss: 0.7875608852931432\n",
      "Epoch 55, Training Loss: 0.7883206580814562\n",
      "Epoch 56, Training Loss: 0.7887006921875745\n",
      "Epoch 57, Training Loss: 0.7877466418689355\n",
      "Epoch 58, Training Loss: 0.7883838834619163\n",
      "Epoch 59, Training Loss: 0.7877445901246896\n",
      "Epoch 60, Training Loss: 0.7872466722825416\n",
      "Epoch 61, Training Loss: 0.7888500374062617\n",
      "Epoch 62, Training Loss: 0.7883691958018711\n",
      "Epoch 63, Training Loss: 0.7887059755791399\n",
      "Epoch 64, Training Loss: 0.7876376267662621\n",
      "Epoch 65, Training Loss: 0.7885795068023796\n",
      "Epoch 66, Training Loss: 0.7875661935125079\n",
      "Epoch 67, Training Loss: 0.7871481613108986\n",
      "Epoch 68, Training Loss: 0.787228870929632\n",
      "Epoch 69, Training Loss: 0.7870225854386065\n",
      "Epoch 70, Training Loss: 0.786895527785882\n",
      "Epoch 71, Training Loss: 0.7868927057524373\n",
      "Epoch 72, Training Loss: 0.7878131155680893\n",
      "Epoch 73, Training Loss: 0.7869493786553691\n",
      "Epoch 74, Training Loss: 0.7883140521838252\n",
      "Epoch 75, Training Loss: 0.7879706276090521\n",
      "Epoch 76, Training Loss: 0.786592739103432\n",
      "Epoch 77, Training Loss: 0.7872136610791199\n",
      "Epoch 78, Training Loss: 0.7865034041548133\n",
      "Epoch 79, Training Loss: 0.7866834870854714\n",
      "Epoch 80, Training Loss: 0.7877372686127971\n",
      "Epoch 81, Training Loss: 0.7863687928457905\n",
      "Epoch 82, Training Loss: 0.7862639744478958\n",
      "Epoch 83, Training Loss: 0.7866579247596569\n",
      "Epoch 84, Training Loss: 0.7858308563555093\n",
      "Epoch 85, Training Loss: 0.7884218145133858\n",
      "Epoch 86, Training Loss: 0.7872487294942813\n",
      "Epoch 87, Training Loss: 0.786866179265474\n",
      "Epoch 88, Training Loss: 0.7862994528354559\n",
      "Epoch 89, Training Loss: 0.7860691660328916\n",
      "Epoch 90, Training Loss: 0.786348507278844\n",
      "Epoch 91, Training Loss: 0.7865153185407022\n",
      "Epoch 92, Training Loss: 0.7871947695886282\n",
      "Epoch 93, Training Loss: 0.7865462298680069\n",
      "Epoch 94, Training Loss: 0.7867456198634958\n",
      "Epoch 95, Training Loss: 0.7866436355096057\n",
      "Epoch 96, Training Loss: 0.7871303372813346\n",
      "Epoch 97, Training Loss: 0.7868600829203326\n",
      "Epoch 98, Training Loss: 0.7863017320632935\n",
      "Epoch 99, Training Loss: 0.7861052319519501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:31:11,569] Trial 205 finished with value: 0.6388666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7872690216042941\n",
      "Epoch 1, Training Loss: 0.8789851428870868\n",
      "Epoch 2, Training Loss: 0.8171228433013858\n",
      "Epoch 3, Training Loss: 0.8089810602647022\n",
      "Epoch 4, Training Loss: 0.8049373590856567\n",
      "Epoch 5, Training Loss: 0.8017719801207234\n",
      "Epoch 6, Training Loss: 0.7997043859689755\n",
      "Epoch 7, Training Loss: 0.7976165935509187\n",
      "Epoch 8, Training Loss: 0.7964914779017742\n",
      "Epoch 9, Training Loss: 0.7966268214964329\n",
      "Epoch 10, Training Loss: 0.7941415891611486\n",
      "Epoch 11, Training Loss: 0.7935656859462422\n",
      "Epoch 12, Training Loss: 0.7936456453531309\n",
      "Epoch 13, Training Loss: 0.793508894102914\n",
      "Epoch 14, Training Loss: 0.7917164852744655\n",
      "Epoch 15, Training Loss: 0.7913788730040529\n",
      "Epoch 16, Training Loss: 0.7917030694789456\n",
      "Epoch 17, Training Loss: 0.79069312568894\n",
      "Epoch 18, Training Loss: 0.7906058441427417\n",
      "Epoch 19, Training Loss: 0.7903817619596208\n",
      "Epoch 20, Training Loss: 0.790353936898081\n",
      "Epoch 21, Training Loss: 0.7904121320946772\n",
      "Epoch 22, Training Loss: 0.7902094147259131\n",
      "Epoch 23, Training Loss: 0.7906126844255548\n",
      "Epoch 24, Training Loss: 0.78961407439153\n",
      "Epoch 25, Training Loss: 0.7889467423123525\n",
      "Epoch 26, Training Loss: 0.7881375631891695\n",
      "Epoch 27, Training Loss: 0.7881129839814696\n",
      "Epoch 28, Training Loss: 0.7873133003263545\n",
      "Epoch 29, Training Loss: 0.7877922627262604\n",
      "Epoch 30, Training Loss: 0.7890232813985725\n",
      "Epoch 31, Training Loss: 0.7879156709613656\n",
      "Epoch 32, Training Loss: 0.7881299909792449\n",
      "Epoch 33, Training Loss: 0.788420702102489\n",
      "Epoch 34, Training Loss: 0.7881348304282454\n",
      "Epoch 35, Training Loss: 0.7883341371564937\n",
      "Epoch 36, Training Loss: 0.7869835442170164\n",
      "Epoch 37, Training Loss: 0.7858280236559703\n",
      "Epoch 38, Training Loss: 0.7868662843130585\n",
      "Epoch 39, Training Loss: 0.78513599759654\n",
      "Epoch 40, Training Loss: 0.7865107065752933\n",
      "Epoch 41, Training Loss: 0.7852271735220027\n",
      "Epoch 42, Training Loss: 0.7849288658988207\n",
      "Epoch 43, Training Loss: 0.7851048787733665\n",
      "Epoch 44, Training Loss: 0.7852922940612735\n",
      "Epoch 45, Training Loss: 0.7847628486783881\n",
      "Epoch 46, Training Loss: 0.7850394134234665\n",
      "Epoch 47, Training Loss: 0.784687764662549\n",
      "Epoch 48, Training Loss: 0.7847105208196138\n",
      "Epoch 49, Training Loss: 0.7842040346977406\n",
      "Epoch 50, Training Loss: 0.7847331911101377\n",
      "Epoch 51, Training Loss: 0.7840791931725982\n",
      "Epoch 52, Training Loss: 0.7831560350002202\n",
      "Epoch 53, Training Loss: 0.7828625097310633\n",
      "Epoch 54, Training Loss: 0.7843189573825751\n",
      "Epoch 55, Training Loss: 0.7846956319378731\n",
      "Epoch 56, Training Loss: 0.7840975821466374\n",
      "Epoch 57, Training Loss: 0.7838914484009707\n",
      "Epoch 58, Training Loss: 0.7834147053553646\n",
      "Epoch 59, Training Loss: 0.7835892605602293\n",
      "Epoch 60, Training Loss: 0.7833103421039151\n",
      "Epoch 61, Training Loss: 0.7828895334910629\n",
      "Epoch 62, Training Loss: 0.7833012173946639\n",
      "Epoch 63, Training Loss: 0.7822635431038706\n",
      "Epoch 64, Training Loss: 0.7823879712506344\n",
      "Epoch 65, Training Loss: 0.7832636539201091\n",
      "Epoch 66, Training Loss: 0.7814749099258194\n",
      "Epoch 67, Training Loss: 0.7833178703049968\n",
      "Epoch 68, Training Loss: 0.7819541901574099\n",
      "Epoch 69, Training Loss: 0.7816786231851219\n",
      "Epoch 70, Training Loss: 0.7817058883663407\n",
      "Epoch 71, Training Loss: 0.7813269662677793\n",
      "Epoch 72, Training Loss: 0.7812528270527832\n",
      "Epoch 73, Training Loss: 0.7812822991295865\n",
      "Epoch 74, Training Loss: 0.7816332856515297\n",
      "Epoch 75, Training Loss: 0.7808553352391809\n",
      "Epoch 76, Training Loss: 0.7822953049401592\n",
      "Epoch 77, Training Loss: 0.7807230283443193\n",
      "Epoch 78, Training Loss: 0.7798817410504908\n",
      "Epoch 79, Training Loss: 0.7818408320720931\n",
      "Epoch 80, Training Loss: 0.7820668265335542\n",
      "Epoch 81, Training Loss: 0.7827199357792847\n",
      "Epoch 82, Training Loss: 0.7799957584617729\n",
      "Epoch 83, Training Loss: 0.7812061808163062\n",
      "Epoch 84, Training Loss: 0.7805966251774838\n",
      "Epoch 85, Training Loss: 0.77993177874644\n",
      "Epoch 86, Training Loss: 0.7817783355712891\n",
      "Epoch 87, Training Loss: 0.7805739498676214\n",
      "Epoch 88, Training Loss: 0.7802211911158454\n",
      "Epoch 89, Training Loss: 0.7804890968745812\n",
      "Epoch 90, Training Loss: 0.7805358538950297\n",
      "Epoch 91, Training Loss: 0.7804813636873001\n",
      "Epoch 92, Training Loss: 0.7811348048367894\n",
      "Epoch 93, Training Loss: 0.7811124472689808\n",
      "Epoch 94, Training Loss: 0.7798165452211423\n",
      "Epoch 95, Training Loss: 0.780353291769673\n",
      "Epoch 96, Training Loss: 0.7799234817798872\n",
      "Epoch 97, Training Loss: 0.7806057563401703\n",
      "Epoch 98, Training Loss: 0.7802556881330963\n",
      "Epoch 99, Training Loss: 0.780233673493665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:33:19,904] Trial 206 finished with value: 0.6340666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7788628242965928\n",
      "Epoch 1, Training Loss: 1.0824321576527187\n",
      "Epoch 2, Training Loss: 1.0507791493172036\n",
      "Epoch 3, Training Loss: 1.0251349311125906\n",
      "Epoch 4, Training Loss: 1.0044998367029923\n",
      "Epoch 5, Training Loss: 0.9890921977229584\n",
      "Epoch 6, Training Loss: 0.9781093391260707\n",
      "Epoch 7, Training Loss: 0.9713827013969422\n",
      "Epoch 8, Training Loss: 0.9666122903501181\n",
      "Epoch 9, Training Loss: 0.9633600782631035\n",
      "Epoch 10, Training Loss: 0.9613785308106502\n",
      "Epoch 11, Training Loss: 0.9597213599914894\n",
      "Epoch 12, Training Loss: 0.9584184925358995\n",
      "Epoch 13, Training Loss: 0.9579505177368796\n",
      "Epoch 14, Training Loss: 0.9567302316651308\n",
      "Epoch 15, Training Loss: 0.955476521190844\n",
      "Epoch 16, Training Loss: 0.9552219594331612\n",
      "Epoch 17, Training Loss: 0.9540318316983101\n",
      "Epoch 18, Training Loss: 0.953302392296325\n",
      "Epoch 19, Training Loss: 0.9524451092669838\n",
      "Epoch 20, Training Loss: 0.9511433274226081\n",
      "Epoch 21, Training Loss: 0.9503726377523035\n",
      "Epoch 22, Training Loss: 0.9491671730701189\n",
      "Epoch 23, Training Loss: 0.9481346310529494\n",
      "Epoch 24, Training Loss: 0.946835460250539\n",
      "Epoch 25, Training Loss: 0.9463159330805442\n",
      "Epoch 26, Training Loss: 0.9454510900311004\n",
      "Epoch 27, Training Loss: 0.9442834829029284\n",
      "Epoch 28, Training Loss: 0.9433402885171703\n",
      "Epoch 29, Training Loss: 0.9421666359542904\n",
      "Epoch 30, Training Loss: 0.9408668203461439\n",
      "Epoch 31, Training Loss: 0.9400299378803798\n",
      "Epoch 32, Training Loss: 0.9389230732630967\n",
      "Epoch 33, Training Loss: 0.9372775597679884\n",
      "Epoch 34, Training Loss: 0.9360851097823982\n",
      "Epoch 35, Training Loss: 0.9347149027021308\n",
      "Epoch 36, Training Loss: 0.9338116871683221\n",
      "Epoch 37, Training Loss: 0.9321310305953923\n",
      "Epoch 38, Training Loss: 0.9310088678410179\n",
      "Epoch 39, Training Loss: 0.9296314388289487\n",
      "Epoch 40, Training Loss: 0.9280492658005621\n",
      "Epoch 41, Training Loss: 0.9268240649897354\n",
      "Epoch 42, Training Loss: 0.9250296130216211\n",
      "Epoch 43, Training Loss: 0.9240010277669233\n",
      "Epoch 44, Training Loss: 0.9222883105278015\n",
      "Epoch 45, Training Loss: 0.9211687673303418\n",
      "Epoch 46, Training Loss: 0.9192509605472249\n",
      "Epoch 47, Training Loss: 0.9176513727446248\n",
      "Epoch 48, Training Loss: 0.9160256749705264\n",
      "Epoch 49, Training Loss: 0.9142631693890221\n",
      "Epoch 50, Training Loss: 0.9127083779277658\n",
      "Epoch 51, Training Loss: 0.9107014469634321\n",
      "Epoch 52, Training Loss: 0.9091603392048886\n",
      "Epoch 53, Training Loss: 0.9071650253202682\n",
      "Epoch 54, Training Loss: 0.9055390663613054\n",
      "Epoch 55, Training Loss: 0.9039558099624806\n",
      "Epoch 56, Training Loss: 0.901769907313182\n",
      "Epoch 57, Training Loss: 0.8999849250442103\n",
      "Epoch 58, Training Loss: 0.898131893810473\n",
      "Epoch 59, Training Loss: 0.8960969511727641\n",
      "Epoch 60, Training Loss: 0.8947985500321353\n",
      "Epoch 61, Training Loss: 0.8925072035395113\n",
      "Epoch 62, Training Loss: 0.8903128524471943\n",
      "Epoch 63, Training Loss: 0.8882802424574257\n",
      "Epoch 64, Training Loss: 0.8867400578090123\n",
      "Epoch 65, Training Loss: 0.884469495231944\n",
      "Epoch 66, Training Loss: 0.8826954950963645\n",
      "Epoch 67, Training Loss: 0.8809339213192015\n",
      "Epoch 68, Training Loss: 0.8787466612077297\n",
      "Epoch 69, Training Loss: 0.8764585249406055\n",
      "Epoch 70, Training Loss: 0.8750078066847378\n",
      "Epoch 71, Training Loss: 0.8734075773031191\n",
      "Epoch 72, Training Loss: 0.8705578363927683\n",
      "Epoch 73, Training Loss: 0.8690591030551079\n",
      "Epoch 74, Training Loss: 0.8668382241313619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:34:21,882] Trial 207 finished with value: 0.5985333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8653927196237378\n",
      "Epoch 1, Training Loss: 0.8972243831569987\n",
      "Epoch 2, Training Loss: 0.8268226626224088\n",
      "Epoch 3, Training Loss: 0.8129640642861674\n",
      "Epoch 4, Training Loss: 0.8081349638171662\n",
      "Epoch 5, Training Loss: 0.8045336197193403\n",
      "Epoch 6, Training Loss: 0.8004550014223372\n",
      "Epoch 7, Training Loss: 0.7977448131805076\n",
      "Epoch 8, Training Loss: 0.7953971058802497\n",
      "Epoch 9, Training Loss: 0.7939140425588852\n",
      "Epoch 10, Training Loss: 0.7926046154552833\n",
      "Epoch 11, Training Loss: 0.7915333279989716\n",
      "Epoch 12, Training Loss: 0.7898783588767948\n",
      "Epoch 13, Training Loss: 0.7893883731132163\n",
      "Epoch 14, Training Loss: 0.7881600337817256\n",
      "Epoch 15, Training Loss: 0.7879847805302842\n",
      "Epoch 16, Training Loss: 0.7869038787103237\n",
      "Epoch 17, Training Loss: 0.7868848414349376\n",
      "Epoch 18, Training Loss: 0.7869012272447572\n",
      "Epoch 19, Training Loss: 0.785918174650436\n",
      "Epoch 20, Training Loss: 0.7856881113876973\n",
      "Epoch 21, Training Loss: 0.7857569121776666\n",
      "Epoch 22, Training Loss: 0.7848416873386928\n",
      "Epoch 23, Training Loss: 0.7849830524365705\n",
      "Epoch 24, Training Loss: 0.7848114391914884\n",
      "Epoch 25, Training Loss: 0.784235445987013\n",
      "Epoch 26, Training Loss: 0.7840951012489491\n",
      "Epoch 27, Training Loss: 0.7837197424773883\n",
      "Epoch 28, Training Loss: 0.7837348801749093\n",
      "Epoch 29, Training Loss: 0.7823342714094578\n",
      "Epoch 30, Training Loss: 0.7823537651309393\n",
      "Epoch 31, Training Loss: 0.7832942279657923\n",
      "Epoch 32, Training Loss: 0.7849131277629308\n",
      "Epoch 33, Training Loss: 0.7829092983912704\n",
      "Epoch 34, Training Loss: 0.7827880602133902\n",
      "Epoch 35, Training Loss: 0.782780503061481\n",
      "Epoch 36, Training Loss: 0.7821312401527749\n",
      "Epoch 37, Training Loss: 0.7814209090587788\n",
      "Epoch 38, Training Loss: 0.7814574676348751\n",
      "Epoch 39, Training Loss: 0.7821864008007193\n",
      "Epoch 40, Training Loss: 0.7816220736145076\n",
      "Epoch 41, Training Loss: 0.7811719232035759\n",
      "Epoch 42, Training Loss: 0.7815550326404715\n",
      "Epoch 43, Training Loss: 0.7814807959068987\n",
      "Epoch 44, Training Loss: 0.7822039415961818\n",
      "Epoch 45, Training Loss: 0.7812438966636371\n",
      "Epoch 46, Training Loss: 0.7807216460543468\n",
      "Epoch 47, Training Loss: 0.7805641293525696\n",
      "Epoch 48, Training Loss: 0.7800223249241822\n",
      "Epoch 49, Training Loss: 0.7800698688155726\n",
      "Epoch 50, Training Loss: 0.7797661713639596\n",
      "Epoch 51, Training Loss: 0.7798473367117401\n",
      "Epoch 52, Training Loss: 0.7802497155684277\n",
      "Epoch 53, Training Loss: 0.7794078796877897\n",
      "Epoch 54, Training Loss: 0.7799689027599822\n",
      "Epoch 55, Training Loss: 0.7796100571639556\n",
      "Epoch 56, Training Loss: 0.780132780397745\n",
      "Epoch 57, Training Loss: 0.7794375770074085\n",
      "Epoch 58, Training Loss: 0.7788616279462226\n",
      "Epoch 59, Training Loss: 0.7787746587642154\n",
      "Epoch 60, Training Loss: 0.7786738166235443\n",
      "Epoch 61, Training Loss: 0.7792678576663025\n",
      "Epoch 62, Training Loss: 0.778573841618416\n",
      "Epoch 63, Training Loss: 0.7795859029418544\n",
      "Epoch 64, Training Loss: 0.7793458869582728\n",
      "Epoch 65, Training Loss: 0.7783362367099389\n",
      "Epoch 66, Training Loss: 0.7784472866165907\n",
      "Epoch 67, Training Loss: 0.7793832506452288\n",
      "Epoch 68, Training Loss: 0.7787240528522578\n",
      "Epoch 69, Training Loss: 0.7791397683602527\n",
      "Epoch 70, Training Loss: 0.7780396226653479\n",
      "Epoch 71, Training Loss: 0.7778376416156166\n",
      "Epoch 72, Training Loss: 0.778010148123691\n",
      "Epoch 73, Training Loss: 0.777806856399192\n",
      "Epoch 74, Training Loss: 0.7784681062949331\n",
      "Epoch 75, Training Loss: 0.7781391340987127\n",
      "Epoch 76, Training Loss: 0.7773818370991183\n",
      "Epoch 77, Training Loss: 0.7783728929390584\n",
      "Epoch 78, Training Loss: 0.7781557901461321\n",
      "Epoch 79, Training Loss: 0.7770505151354281\n",
      "Epoch 80, Training Loss: 0.7781568150771292\n",
      "Epoch 81, Training Loss: 0.7779581968049357\n",
      "Epoch 82, Training Loss: 0.778056339930771\n",
      "Epoch 83, Training Loss: 0.7768194658415658\n",
      "Epoch 84, Training Loss: 0.7772046990860674\n",
      "Epoch 85, Training Loss: 0.7772089376485437\n",
      "Epoch 86, Training Loss: 0.7770996404769726\n",
      "Epoch 87, Training Loss: 0.7773142002578964\n",
      "Epoch 88, Training Loss: 0.7773886262026048\n",
      "Epoch 89, Training Loss: 0.7766261753283049\n",
      "Epoch 90, Training Loss: 0.7762979082146981\n",
      "Epoch 91, Training Loss: 0.7767074466647959\n",
      "Epoch 92, Training Loss: 0.7765726110092679\n",
      "Epoch 93, Training Loss: 0.7763062754071745\n",
      "Epoch 94, Training Loss: 0.7759932602706708\n",
      "Epoch 95, Training Loss: 0.7762202119468746\n",
      "Epoch 96, Training Loss: 0.7760370205219527\n",
      "Epoch 97, Training Loss: 0.7767486210156204\n",
      "Epoch 98, Training Loss: 0.7761249130829833\n",
      "Epoch 99, Training Loss: 0.7753749500539966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:36:23,011] Trial 208 finished with value: 0.6353333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.775597994309619\n",
      "Epoch 1, Training Loss: 0.9264986824288087\n",
      "Epoch 2, Training Loss: 0.8805626813103171\n",
      "Epoch 3, Training Loss: 0.8451323563211104\n",
      "Epoch 4, Training Loss: 0.8222646455203786\n",
      "Epoch 5, Training Loss: 0.8113555810732\n",
      "Epoch 6, Training Loss: 0.8065923981806811\n",
      "Epoch 7, Training Loss: 0.8041720329312717\n",
      "Epoch 8, Training Loss: 0.8030273383505204\n",
      "Epoch 9, Training Loss: 0.8019469162295846\n",
      "Epoch 10, Training Loss: 0.8012315860215355\n",
      "Epoch 11, Training Loss: 0.8005736749312456\n",
      "Epoch 12, Training Loss: 0.8003375563200782\n",
      "Epoch 13, Training Loss: 0.7998066233887392\n",
      "Epoch 14, Training Loss: 0.7993470327994403\n",
      "Epoch 15, Training Loss: 0.799425554485882\n",
      "Epoch 16, Training Loss: 0.7990995113989886\n",
      "Epoch 17, Training Loss: 0.7989098892492407\n",
      "Epoch 18, Training Loss: 0.798594143741271\n",
      "Epoch 19, Training Loss: 0.7982877415769241\n",
      "Epoch 20, Training Loss: 0.798189611715429\n",
      "Epoch 21, Training Loss: 0.7979031749332652\n",
      "Epoch 22, Training Loss: 0.7980223893418031\n",
      "Epoch 23, Training Loss: 0.7978263942634358\n",
      "Epoch 24, Training Loss: 0.7976368777190938\n",
      "Epoch 25, Training Loss: 0.7976155454972211\n",
      "Epoch 26, Training Loss: 0.7974627722010893\n",
      "Epoch 27, Training Loss: 0.7973313546881956\n",
      "Epoch 28, Training Loss: 0.797113018246258\n",
      "Epoch 29, Training Loss: 0.7968209643924937\n",
      "Epoch 30, Training Loss: 0.7970501093303456\n",
      "Epoch 31, Training Loss: 0.7967960009154151\n",
      "Epoch 32, Training Loss: 0.7965352035971249\n",
      "Epoch 33, Training Loss: 0.7968110311031341\n",
      "Epoch 34, Training Loss: 0.7964940775141996\n",
      "Epoch 35, Training Loss: 0.7964276045210221\n",
      "Epoch 36, Training Loss: 0.796353777997634\n",
      "Epoch 37, Training Loss: 0.7962426237499013\n",
      "Epoch 38, Training Loss: 0.7963155572554644\n",
      "Epoch 39, Training Loss: 0.7960063672766966\n",
      "Epoch 40, Training Loss: 0.795633570096072\n",
      "Epoch 41, Training Loss: 0.7957748227960924\n",
      "Epoch 42, Training Loss: 0.7954828579986797\n",
      "Epoch 43, Training Loss: 0.7953744799249313\n",
      "Epoch 44, Training Loss: 0.7950367321687586\n",
      "Epoch 45, Training Loss: 0.7945565227200003\n",
      "Epoch 46, Training Loss: 0.79442654560594\n",
      "Epoch 47, Training Loss: 0.7937401504376356\n",
      "Epoch 48, Training Loss: 0.7933161821786096\n",
      "Epoch 49, Training Loss: 0.7930299822723165\n",
      "Epoch 50, Training Loss: 0.7924848669416764\n",
      "Epoch 51, Training Loss: 0.791987242418177\n",
      "Epoch 52, Training Loss: 0.7920303622414084\n",
      "Epoch 53, Training Loss: 0.7918077517257017\n",
      "Epoch 54, Training Loss: 0.7914443664691028\n",
      "Epoch 55, Training Loss: 0.7912862300171571\n",
      "Epoch 56, Training Loss: 0.7912077579778783\n",
      "Epoch 57, Training Loss: 0.7910159552798551\n",
      "Epoch 58, Training Loss: 0.7908383804910323\n",
      "Epoch 59, Training Loss: 0.7907024786051582\n",
      "Epoch 60, Training Loss: 0.7907310858894797\n",
      "Epoch 61, Training Loss: 0.790671618125018\n",
      "Epoch 62, Training Loss: 0.7905375862822813\n",
      "Epoch 63, Training Loss: 0.7903063212422764\n",
      "Epoch 64, Training Loss: 0.7903090817086836\n",
      "Epoch 65, Training Loss: 0.7901826407628901\n",
      "Epoch 66, Training Loss: 0.79008409836713\n",
      "Epoch 67, Training Loss: 0.7901437459973728\n",
      "Epoch 68, Training Loss: 0.790109119275037\n",
      "Epoch 69, Training Loss: 0.7900555743189419\n",
      "Epoch 70, Training Loss: 0.7900661715339212\n",
      "Epoch 71, Training Loss: 0.7898875797496122\n",
      "Epoch 72, Training Loss: 0.7898821848981521\n",
      "Epoch 73, Training Loss: 0.7899573077875025\n",
      "Epoch 74, Training Loss: 0.7896725166545194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:37:45,866] Trial 209 finished with value: 0.6362 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7898017255698934\n",
      "Epoch 1, Training Loss: 0.8537046041909386\n",
      "Epoch 2, Training Loss: 0.8154625455772175\n",
      "Epoch 3, Training Loss: 0.8135530710220337\n",
      "Epoch 4, Training Loss: 0.8112102554124945\n",
      "Epoch 5, Training Loss: 0.8091402776802288\n",
      "Epoch 6, Training Loss: 0.8076915369314306\n",
      "Epoch 7, Training Loss: 0.8056552368051866\n",
      "Epoch 8, Training Loss: 0.8048748121542089\n",
      "Epoch 9, Training Loss: 0.8034377079150256\n",
      "Epoch 10, Training Loss: 0.8027361179099364\n",
      "Epoch 11, Training Loss: 0.8021336837375865\n",
      "Epoch 12, Training Loss: 0.8014702534675598\n",
      "Epoch 13, Training Loss: 0.7999193786172306\n",
      "Epoch 14, Training Loss: 0.7995508407143985\n",
      "Epoch 15, Training Loss: 0.7996278004786548\n",
      "Epoch 16, Training Loss: 0.7985685400401845\n",
      "Epoch 17, Training Loss: 0.7976042714539696\n",
      "Epoch 18, Training Loss: 0.7972433710098267\n",
      "Epoch 19, Training Loss: 0.7964366621830884\n",
      "Epoch 20, Training Loss: 0.7956303543203017\n",
      "Epoch 21, Training Loss: 0.7943661109138938\n",
      "Epoch 22, Training Loss: 0.7937119165588827\n",
      "Epoch 23, Training Loss: 0.792840957290986\n",
      "Epoch 24, Training Loss: 0.7921448559620801\n",
      "Epoch 25, Training Loss: 0.791194029415355\n",
      "Epoch 26, Training Loss: 0.7900736634871539\n",
      "Epoch 27, Training Loss: 0.7892021609755123\n",
      "Epoch 28, Training Loss: 0.7891107207887313\n",
      "Epoch 29, Training Loss: 0.7880197452096378\n",
      "Epoch 30, Training Loss: 0.7879527763058157\n",
      "Epoch 31, Training Loss: 0.7871873278477612\n",
      "Epoch 32, Training Loss: 0.7867055622269126\n",
      "Epoch 33, Training Loss: 0.786642720559064\n",
      "Epoch 34, Training Loss: 0.7862633810323827\n",
      "Epoch 35, Training Loss: 0.7858484601273256\n",
      "Epoch 36, Training Loss: 0.7859015771220712\n",
      "Epoch 37, Training Loss: 0.7855576414220473\n",
      "Epoch 38, Training Loss: 0.7850714630239151\n",
      "Epoch 39, Training Loss: 0.7850487477639142\n",
      "Epoch 40, Training Loss: 0.7849809784748975\n",
      "Epoch 41, Training Loss: 0.7847773263033698\n",
      "Epoch 42, Training Loss: 0.7849472381788142\n",
      "Epoch 43, Training Loss: 0.7847376927207498\n",
      "Epoch 44, Training Loss: 0.7843377419079052\n",
      "Epoch 45, Training Loss: 0.7844965499990126\n",
      "Epoch 46, Training Loss: 0.7841927289261538\n",
      "Epoch 47, Training Loss: 0.7841184583131005\n",
      "Epoch 48, Training Loss: 0.783727353811264\n",
      "Epoch 49, Training Loss: 0.7839679157733918\n",
      "Epoch 50, Training Loss: 0.7840659930425532\n",
      "Epoch 51, Training Loss: 0.7834701538085938\n",
      "Epoch 52, Training Loss: 0.7836930601035847\n",
      "Epoch 53, Training Loss: 0.7835774335440467\n",
      "Epoch 54, Training Loss: 0.7833800161586089\n",
      "Epoch 55, Training Loss: 0.7836159550442415\n",
      "Epoch 56, Training Loss: 0.7833077286972719\n",
      "Epoch 57, Training Loss: 0.7835945502449484\n",
      "Epoch 58, Training Loss: 0.7831406885034897\n",
      "Epoch 59, Training Loss: 0.7831427253695096\n",
      "Epoch 60, Training Loss: 0.7832269771660075\n",
      "Epoch 61, Training Loss: 0.7831651729695938\n",
      "Epoch 62, Training Loss: 0.7829429890828974\n",
      "Epoch 63, Training Loss: 0.7828290165873135\n",
      "Epoch 64, Training Loss: 0.7828805175248315\n",
      "Epoch 65, Training Loss: 0.7824152285912458\n",
      "Epoch 66, Training Loss: 0.7828968401516185\n",
      "Epoch 67, Training Loss: 0.7824027485707227\n",
      "Epoch 68, Training Loss: 0.7823917961821837\n",
      "Epoch 69, Training Loss: 0.7828079508332645\n",
      "Epoch 70, Training Loss: 0.7824405084638034\n",
      "Epoch 71, Training Loss: 0.7824915587200838\n",
      "Epoch 72, Training Loss: 0.7826316198180704\n",
      "Epoch 73, Training Loss: 0.7822487832518185\n",
      "Epoch 74, Training Loss: 0.7823607256833245\n",
      "Epoch 75, Training Loss: 0.7824140010160558\n",
      "Epoch 76, Training Loss: 0.7824237276526058\n",
      "Epoch 77, Training Loss: 0.7819960147493026\n",
      "Epoch 78, Training Loss: 0.7817611564608181\n",
      "Epoch 79, Training Loss: 0.7820707955781151\n",
      "Epoch 80, Training Loss: 0.7817839003310484\n",
      "Epoch 81, Training Loss: 0.7821680092110354\n",
      "Epoch 82, Training Loss: 0.7819004563023062\n",
      "Epoch 83, Training Loss: 0.7819387676435359\n",
      "Epoch 84, Training Loss: 0.7818102960726794\n",
      "Epoch 85, Training Loss: 0.7818603162204518\n",
      "Epoch 86, Training Loss: 0.7815544188723844\n",
      "Epoch 87, Training Loss: 0.7818178855671603\n",
      "Epoch 88, Training Loss: 0.7818488292834338\n",
      "Epoch 89, Training Loss: 0.7815264382081873\n",
      "Epoch 90, Training Loss: 0.7815595663996304\n",
      "Epoch 91, Training Loss: 0.7816215300559998\n",
      "Epoch 92, Training Loss: 0.78158063509885\n",
      "Epoch 93, Training Loss: 0.7815262000701007\n",
      "Epoch 94, Training Loss: 0.781522070239572\n",
      "Epoch 95, Training Loss: 0.7815649687542635\n",
      "Epoch 96, Training Loss: 0.781671214875053\n",
      "Epoch 97, Training Loss: 0.7814312002238105\n",
      "Epoch 98, Training Loss: 0.7813076940003564\n",
      "Epoch 99, Training Loss: 0.7812340529525981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:39:55,775] Trial 210 finished with value: 0.6388 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7812593264439527\n",
      "Epoch 1, Training Loss: 0.9076938236566414\n",
      "Epoch 2, Training Loss: 0.8275401885348155\n",
      "Epoch 3, Training Loss: 0.8207228383623568\n",
      "Epoch 4, Training Loss: 0.8169622760966309\n",
      "Epoch 5, Training Loss: 0.8135176223919804\n",
      "Epoch 6, Training Loss: 0.8125766007523788\n",
      "Epoch 7, Training Loss: 0.8094542278383011\n",
      "Epoch 8, Training Loss: 0.8098500248184778\n",
      "Epoch 9, Training Loss: 0.8095102678564258\n",
      "Epoch 10, Training Loss: 0.8077165035376871\n",
      "Epoch 11, Training Loss: 0.8067144609035406\n",
      "Epoch 12, Training Loss: 0.8061493587673159\n",
      "Epoch 13, Training Loss: 0.8057610254538686\n",
      "Epoch 14, Training Loss: 0.8053524917229674\n",
      "Epoch 15, Training Loss: 0.8048425103488721\n",
      "Epoch 16, Training Loss: 0.804670330456325\n",
      "Epoch 17, Training Loss: 0.8042694958529077\n",
      "Epoch 18, Training Loss: 0.8053814544713587\n",
      "Epoch 19, Training Loss: 0.8053191743398967\n",
      "Epoch 20, Training Loss: 0.8035243666261659\n",
      "Epoch 21, Training Loss: 0.8038305346230815\n",
      "Epoch 22, Training Loss: 0.8030143087071584\n",
      "Epoch 23, Training Loss: 0.8030508734229812\n",
      "Epoch 24, Training Loss: 0.802011946090182\n",
      "Epoch 25, Training Loss: 0.8031229978217218\n",
      "Epoch 26, Training Loss: 0.8022441046578543\n",
      "Epoch 27, Training Loss: 0.8021374755335929\n",
      "Epoch 28, Training Loss: 0.8015329110891299\n",
      "Epoch 29, Training Loss: 0.801457070766535\n",
      "Epoch 30, Training Loss: 0.8007286735047076\n",
      "Epoch 31, Training Loss: 0.8019605269109397\n",
      "Epoch 32, Training Loss: 0.8012206434307242\n",
      "Epoch 33, Training Loss: 0.8005262333647649\n",
      "Epoch 34, Training Loss: 0.8006269119735947\n",
      "Epoch 35, Training Loss: 0.8004602356960899\n",
      "Epoch 36, Training Loss: 0.8007892869468919\n",
      "Epoch 37, Training Loss: 0.8005667949081363\n",
      "Epoch 38, Training Loss: 0.8002168693040547\n",
      "Epoch 39, Training Loss: 0.8008464560472875\n",
      "Epoch 40, Training Loss: 0.8006064250056905\n",
      "Epoch 41, Training Loss: 0.7997400833251781\n",
      "Epoch 42, Training Loss: 0.799866541435844\n",
      "Epoch 43, Training Loss: 0.7997849431253017\n",
      "Epoch 44, Training Loss: 0.7993305967266399\n",
      "Epoch 45, Training Loss: 0.7985166158443107\n",
      "Epoch 46, Training Loss: 0.7993359740515401\n",
      "Epoch 47, Training Loss: 0.7981293047281136\n",
      "Epoch 48, Training Loss: 0.7982271382683201\n",
      "Epoch 49, Training Loss: 0.798455818613669\n",
      "Epoch 50, Training Loss: 0.7979058627795456\n",
      "Epoch 51, Training Loss: 0.7976679489128572\n",
      "Epoch 52, Training Loss: 0.7978847435542515\n",
      "Epoch 53, Training Loss: 0.7991491285481848\n",
      "Epoch 54, Training Loss: 0.7974173337893379\n",
      "Epoch 55, Training Loss: 0.7975614208027832\n",
      "Epoch 56, Training Loss: 0.7973198150333606\n",
      "Epoch 57, Training Loss: 0.7976174468384649\n",
      "Epoch 58, Training Loss: 0.7963095356647233\n",
      "Epoch 59, Training Loss: 0.7966104727042349\n",
      "Epoch 60, Training Loss: 0.7966669262800001\n",
      "Epoch 61, Training Loss: 0.7970146173821356\n",
      "Epoch 62, Training Loss: 0.7974472700205064\n",
      "Epoch 63, Training Loss: 0.796743489208078\n",
      "Epoch 64, Training Loss: 0.796553663741377\n",
      "Epoch 65, Training Loss: 0.7967259041348794\n",
      "Epoch 66, Training Loss: 0.7963171801172702\n",
      "Epoch 67, Training Loss: 0.7962698856690773\n",
      "Epoch 68, Training Loss: 0.796075893792891\n",
      "Epoch 69, Training Loss: 0.7961950366658376\n",
      "Epoch 70, Training Loss: 0.796100574059594\n",
      "Epoch 71, Training Loss: 0.7958132929371712\n",
      "Epoch 72, Training Loss: 0.7962649691373782\n",
      "Epoch 73, Training Loss: 0.7953746707815873\n",
      "Epoch 74, Training Loss: 0.7954142801743701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:41:07,434] Trial 211 finished with value: 0.5942 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7961375398743421\n",
      "Epoch 1, Training Loss: 0.9031541653145525\n",
      "Epoch 2, Training Loss: 0.8284713651900901\n",
      "Epoch 3, Training Loss: 0.8187683574239114\n",
      "Epoch 4, Training Loss: 0.8145583918220118\n",
      "Epoch 5, Training Loss: 0.8115529746937572\n",
      "Epoch 6, Training Loss: 0.8097630038297265\n",
      "Epoch 7, Training Loss: 0.8082839858263059\n",
      "Epoch 8, Training Loss: 0.8063512853213719\n",
      "Epoch 9, Training Loss: 0.8054612401732825\n",
      "Epoch 10, Training Loss: 0.8059023879524461\n",
      "Epoch 11, Training Loss: 0.8045002179934566\n",
      "Epoch 12, Training Loss: 0.8050872707725467\n",
      "Epoch 13, Training Loss: 0.8031186871062544\n",
      "Epoch 14, Training Loss: 0.8040590988065963\n",
      "Epoch 15, Training Loss: 0.8039011850392908\n",
      "Epoch 16, Training Loss: 0.8028591441032582\n",
      "Epoch 17, Training Loss: 0.8030848505801724\n",
      "Epoch 18, Training Loss: 0.8023575622336309\n",
      "Epoch 19, Training Loss: 0.8017767880195962\n",
      "Epoch 20, Training Loss: 0.8012789010105277\n",
      "Epoch 21, Training Loss: 0.801746684536898\n",
      "Epoch 22, Training Loss: 0.8013128298565857\n",
      "Epoch 23, Training Loss: 0.8010047303106552\n",
      "Epoch 24, Training Loss: 0.8002768912709746\n",
      "Epoch 25, Training Loss: 0.8009993546887448\n",
      "Epoch 26, Training Loss: 0.8010985204151698\n",
      "Epoch 27, Training Loss: 0.8007190760813261\n",
      "Epoch 28, Training Loss: 0.7999256164507759\n",
      "Epoch 29, Training Loss: 0.8005550577228231\n",
      "Epoch 30, Training Loss: 0.801046235489666\n",
      "Epoch 31, Training Loss: 0.8003996439446184\n",
      "Epoch 32, Training Loss: 0.7988402925039593\n",
      "Epoch 33, Training Loss: 0.798921489357052\n",
      "Epoch 34, Training Loss: 0.799178177252748\n",
      "Epoch 35, Training Loss: 0.8011187912826251\n",
      "Epoch 36, Training Loss: 0.7999934855260347\n",
      "Epoch 37, Training Loss: 0.7997669702185723\n",
      "Epoch 38, Training Loss: 0.7994770314908566\n",
      "Epoch 39, Training Loss: 0.7987685682182025\n",
      "Epoch 40, Training Loss: 0.7993170169063081\n",
      "Epoch 41, Training Loss: 0.7992249808813396\n",
      "Epoch 42, Training Loss: 0.7984580728344451\n",
      "Epoch 43, Training Loss: 0.7983543457841514\n",
      "Epoch 44, Training Loss: 0.7994694916825545\n",
      "Epoch 45, Training Loss: 0.7983986622408816\n",
      "Epoch 46, Training Loss: 0.7987212642691189\n",
      "Epoch 47, Training Loss: 0.8007438217786919\n",
      "Epoch 48, Training Loss: 0.7995002877443357\n",
      "Epoch 49, Training Loss: 0.7983599939292535\n",
      "Epoch 50, Training Loss: 0.79962659644005\n",
      "Epoch 51, Training Loss: 0.8001173096491878\n",
      "Epoch 52, Training Loss: 0.79864587828629\n",
      "Epoch 53, Training Loss: 0.7994765667987049\n",
      "Epoch 54, Training Loss: 0.7991842020723157\n",
      "Epoch 55, Training Loss: 0.7992456053432665\n",
      "Epoch 56, Training Loss: 0.7990918785109555\n",
      "Epoch 57, Training Loss: 0.7987680537360055\n",
      "Epoch 58, Training Loss: 0.798481345535221\n",
      "Epoch 59, Training Loss: 0.7985722910192676\n",
      "Epoch 60, Training Loss: 0.7982203325830904\n",
      "Epoch 61, Training Loss: 0.7980597781507592\n",
      "Epoch 62, Training Loss: 0.798556319215244\n",
      "Epoch 63, Training Loss: 0.7992782269205366\n",
      "Epoch 64, Training Loss: 0.7984425931944883\n",
      "Epoch 65, Training Loss: 0.7988594230852629\n",
      "Epoch 66, Training Loss: 0.797944814728615\n",
      "Epoch 67, Training Loss: 0.7978789604247961\n",
      "Epoch 68, Training Loss: 0.7993903272134021\n",
      "Epoch 69, Training Loss: 0.7979642264825061\n",
      "Epoch 70, Training Loss: 0.7989646337982407\n",
      "Epoch 71, Training Loss: 0.7979333488116587\n",
      "Epoch 72, Training Loss: 0.797659006513151\n",
      "Epoch 73, Training Loss: 0.7991705398810537\n",
      "Epoch 74, Training Loss: 0.7982277993869065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:42:33,694] Trial 212 finished with value: 0.6061333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7985538646690827\n",
      "Epoch 1, Training Loss: 0.8651981024882373\n",
      "Epoch 2, Training Loss: 0.8220558845996857\n",
      "Epoch 3, Training Loss: 0.8132746330429526\n",
      "Epoch 4, Training Loss: 0.8109440005526823\n",
      "Epoch 5, Training Loss: 0.8099142256905051\n",
      "Epoch 6, Training Loss: 0.8097705064100378\n",
      "Epoch 7, Training Loss: 0.8084471851937911\n",
      "Epoch 8, Training Loss: 0.80883144308539\n",
      "Epoch 9, Training Loss: 0.8065952035258798\n",
      "Epoch 10, Training Loss: 0.8067782941986533\n",
      "Epoch 11, Training Loss: 0.8052127871793859\n",
      "Epoch 12, Training Loss: 0.8060500653351055\n",
      "Epoch 13, Training Loss: 0.8051272940635681\n",
      "Epoch 14, Training Loss: 0.8034643407428966\n",
      "Epoch 15, Training Loss: 0.8050319509646472\n",
      "Epoch 16, Training Loss: 0.8046453062927021\n",
      "Epoch 17, Training Loss: 0.8037647659638348\n",
      "Epoch 18, Training Loss: 0.8029211594777949\n",
      "Epoch 19, Training Loss: 0.8025583262303296\n",
      "Epoch 20, Training Loss: 0.802221851419\n",
      "Epoch 21, Training Loss: 0.802037589970757\n",
      "Epoch 22, Training Loss: 0.8020477262665243\n",
      "Epoch 23, Training Loss: 0.8015443961059346\n",
      "Epoch 24, Training Loss: 0.8003190703953014\n",
      "Epoch 25, Training Loss: 0.8018214657026179\n",
      "Epoch 26, Training Loss: 0.8008521716033711\n",
      "Epoch 27, Training Loss: 0.8008222717397353\n",
      "Epoch 28, Training Loss: 0.7994577391708598\n",
      "Epoch 29, Training Loss: 0.8001327021682964\n",
      "Epoch 30, Training Loss: 0.7987879985921523\n",
      "Epoch 31, Training Loss: 0.798807888171252\n",
      "Epoch 32, Training Loss: 0.7989826991277582\n",
      "Epoch 33, Training Loss: 0.7989284370226019\n",
      "Epoch 34, Training Loss: 0.7982621871022617\n",
      "Epoch 35, Training Loss: 0.798642586960512\n",
      "Epoch 36, Training Loss: 0.7988694953217226\n",
      "Epoch 37, Training Loss: 0.7975910768088172\n",
      "Epoch 38, Training Loss: 0.7985903743435355\n",
      "Epoch 39, Training Loss: 0.7982440678512349\n",
      "Epoch 40, Training Loss: 0.7959645260081571\n",
      "Epoch 41, Training Loss: 0.7967705472777872\n",
      "Epoch 42, Training Loss: 0.7967269548949073\n",
      "Epoch 43, Training Loss: 0.7958896606809953\n",
      "Epoch 44, Training Loss: 0.7960648007953868\n",
      "Epoch 45, Training Loss: 0.7971985214598039\n",
      "Epoch 46, Training Loss: 0.7957326516684364\n",
      "Epoch 47, Training Loss: 0.7956412576226627\n",
      "Epoch 48, Training Loss: 0.7961196436601526\n",
      "Epoch 49, Training Loss: 0.7953484196522657\n",
      "Epoch 50, Training Loss: 0.795341093750561\n",
      "Epoch 51, Training Loss: 0.7948212953174816\n",
      "Epoch 52, Training Loss: 0.7946375579693739\n",
      "Epoch 53, Training Loss: 0.7939459489373599\n",
      "Epoch 54, Training Loss: 0.7939540898799896\n",
      "Epoch 55, Training Loss: 0.7941634270724128\n",
      "Epoch 56, Training Loss: 0.7942454807898578\n",
      "Epoch 57, Training Loss: 0.7946072940966662\n",
      "Epoch 58, Training Loss: 0.794830407815821\n",
      "Epoch 59, Training Loss: 0.7935797766376944\n",
      "Epoch 60, Training Loss: 0.7941002152246588\n",
      "Epoch 61, Training Loss: 0.7939074823435615\n",
      "Epoch 62, Training Loss: 0.7941037622620077\n",
      "Epoch 63, Training Loss: 0.7945951509475708\n",
      "Epoch 64, Training Loss: 0.7937345386953915\n",
      "Epoch 65, Training Loss: 0.7931274970138774\n",
      "Epoch 66, Training Loss: 0.7932887255444246\n",
      "Epoch 67, Training Loss: 0.7942959849974688\n",
      "Epoch 68, Training Loss: 0.7936189469870399\n",
      "Epoch 69, Training Loss: 0.7935751842049992\n",
      "Epoch 70, Training Loss: 0.7934552323116976\n",
      "Epoch 71, Training Loss: 0.7929917458225699\n",
      "Epoch 72, Training Loss: 0.793980368235532\n",
      "Epoch 73, Training Loss: 0.7936206885646371\n",
      "Epoch 74, Training Loss: 0.7930156005831326\n",
      "Epoch 75, Training Loss: 0.7933199988393222\n",
      "Epoch 76, Training Loss: 0.7932421817499049\n",
      "Epoch 77, Training Loss: 0.7931372000189388\n",
      "Epoch 78, Training Loss: 0.792806749904857\n",
      "Epoch 79, Training Loss: 0.7930417812571806\n",
      "Epoch 80, Training Loss: 0.7924283070424024\n",
      "Epoch 81, Training Loss: 0.7928821333015666\n",
      "Epoch 82, Training Loss: 0.7931974581409903\n",
      "Epoch 83, Training Loss: 0.7923048276760999\n",
      "Epoch 84, Training Loss: 0.792307546489379\n",
      "Epoch 85, Training Loss: 0.7926063588086296\n",
      "Epoch 86, Training Loss: 0.7925841680695029\n",
      "Epoch 87, Training Loss: 0.7922008190435522\n",
      "Epoch 88, Training Loss: 0.7924784414908466\n",
      "Epoch 89, Training Loss: 0.7920461279504439\n",
      "Epoch 90, Training Loss: 0.7919534622921663\n",
      "Epoch 91, Training Loss: 0.7928459731971517\n",
      "Epoch 92, Training Loss: 0.792041611811694\n",
      "Epoch 93, Training Loss: 0.7916764447969549\n",
      "Epoch 94, Training Loss: 0.7918966892887565\n",
      "Epoch 95, Training Loss: 0.7918328598667593\n",
      "Epoch 96, Training Loss: 0.7921647433673634\n",
      "Epoch 97, Training Loss: 0.792356323073892\n",
      "Epoch 98, Training Loss: 0.7918676084630629\n",
      "Epoch 99, Training Loss: 0.7920309850047617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:44:31,957] Trial 213 finished with value: 0.6387333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7918564687756932\n",
      "Epoch 1, Training Loss: 1.1006519826720742\n",
      "Epoch 2, Training Loss: 1.04919022546095\n",
      "Epoch 3, Training Loss: 1.022766027660931\n",
      "Epoch 4, Training Loss: 1.0041579855189604\n",
      "Epoch 5, Training Loss: 0.9903779686198515\n",
      "Epoch 6, Training Loss: 0.9799220545852886\n",
      "Epoch 7, Training Loss: 0.9718360106384053\n",
      "Epoch 8, Training Loss: 0.965521813210319\n",
      "Epoch 9, Training Loss: 0.960493767752367\n",
      "Epoch 10, Training Loss: 0.9564237126182107\n",
      "Epoch 11, Training Loss: 0.9530974756970125\n",
      "Epoch 12, Training Loss: 0.9503147240947275\n",
      "Epoch 13, Training Loss: 0.9479410177819869\n",
      "Epoch 14, Training Loss: 0.9458739657261792\n",
      "Epoch 15, Training Loss: 0.9440484460662393\n",
      "Epoch 16, Training Loss: 0.9423812664957607\n",
      "Epoch 17, Training Loss: 0.9408694952375749\n",
      "Epoch 18, Training Loss: 0.9394783102764803\n",
      "Epoch 19, Training Loss: 0.9381850300115697\n",
      "Epoch 20, Training Loss: 0.936980513123905\n",
      "Epoch 21, Training Loss: 0.9358669261371388\n",
      "Epoch 22, Training Loss: 0.9348185733486625\n",
      "Epoch 23, Training Loss: 0.9338229274749756\n",
      "Epoch 24, Training Loss: 0.9328685226861169\n",
      "Epoch 25, Training Loss: 0.9319495872189016\n",
      "Epoch 26, Training Loss: 0.931063263276044\n",
      "Epoch 27, Training Loss: 0.9302094461637385\n",
      "Epoch 28, Training Loss: 0.9293788673597224\n",
      "Epoch 29, Training Loss: 0.9285732502095839\n",
      "Epoch 30, Training Loss: 0.9277797178661122\n",
      "Epoch 31, Training Loss: 0.9270124154932359\n",
      "Epoch 32, Training Loss: 0.9262601552991306\n",
      "Epoch 33, Training Loss: 0.9255214992691488\n",
      "Epoch 34, Training Loss: 0.9247938440827762\n",
      "Epoch 35, Training Loss: 0.9240771034184624\n",
      "Epoch 36, Training Loss: 0.9233831257679883\n",
      "Epoch 37, Training Loss: 0.9226861507051132\n",
      "Epoch 38, Training Loss: 0.9220139088350184\n",
      "Epoch 39, Training Loss: 0.9213425965168897\n",
      "Epoch 40, Training Loss: 0.9206872292125926\n",
      "Epoch 41, Training Loss: 0.9200343343089609\n",
      "Epoch 42, Training Loss: 0.9193962073326111\n",
      "Epoch 43, Training Loss: 0.9187629789464614\n",
      "Epoch 44, Training Loss: 0.9181292706377366\n",
      "Epoch 45, Training Loss: 0.9175185094861423\n",
      "Epoch 46, Training Loss: 0.9169025602761437\n",
      "Epoch 47, Training Loss: 0.9162898789433872\n",
      "Epoch 48, Training Loss: 0.9156905358679155\n",
      "Epoch 49, Training Loss: 0.9150888332198648\n",
      "Epoch 50, Training Loss: 0.9144893986337325\n",
      "Epoch 51, Training Loss: 0.9139012486794416\n",
      "Epoch 52, Training Loss: 0.9133005130290985\n",
      "Epoch 53, Training Loss: 0.9127166670911452\n",
      "Epoch 54, Training Loss: 0.9121278179393095\n",
      "Epoch 55, Training Loss: 0.9115391794373008\n",
      "Epoch 56, Training Loss: 0.9109556981395273\n",
      "Epoch 57, Training Loss: 0.9103625031078563\n",
      "Epoch 58, Training Loss: 0.9097830092205721\n",
      "Epoch 59, Training Loss: 0.9091840844294604\n",
      "Epoch 60, Training Loss: 0.908617321322946\n",
      "Epoch 61, Training Loss: 0.9080273881379296\n",
      "Epoch 62, Training Loss: 0.9074420160405776\n",
      "Epoch 63, Training Loss: 0.9068526934876161\n",
      "Epoch 64, Training Loss: 0.9062637470049016\n",
      "Epoch 65, Training Loss: 0.9056658296024098\n",
      "Epoch 66, Training Loss: 0.9050738451761358\n",
      "Epoch 67, Training Loss: 0.904481011208366\n",
      "Epoch 68, Training Loss: 0.9038805726696463\n",
      "Epoch 69, Training Loss: 0.9032789005251491\n",
      "Epoch 70, Training Loss: 0.9026652122946347\n",
      "Epoch 71, Training Loss: 0.902061333656311\n",
      "Epoch 72, Training Loss: 0.9014505405285779\n",
      "Epoch 73, Training Loss: 0.9008361878114588\n",
      "Epoch 74, Training Loss: 0.9002173199373134\n",
      "Epoch 75, Training Loss: 0.8995864727216608\n",
      "Epoch 76, Training Loss: 0.8989621360161725\n",
      "Epoch 77, Training Loss: 0.8983360396413242\n",
      "Epoch 78, Training Loss: 0.897697939452003\n",
      "Epoch 79, Training Loss: 0.897065203961204\n",
      "Epoch 80, Training Loss: 0.8964139986739439\n",
      "Epoch 81, Training Loss: 0.8957837617397308\n",
      "Epoch 82, Training Loss: 0.8951310307138106\n",
      "Epoch 83, Training Loss: 0.894485144404804\n",
      "Epoch 84, Training Loss: 0.8938250308878282\n",
      "Epoch 85, Training Loss: 0.893161582736408\n",
      "Epoch 86, Training Loss: 0.8925020419148838\n",
      "Epoch 87, Training Loss: 0.891833002427045\n",
      "Epoch 88, Training Loss: 0.8911610852269566\n",
      "Epoch 89, Training Loss: 0.8904810101845685\n",
      "Epoch 90, Training Loss: 0.8897957272389356\n",
      "Epoch 91, Training Loss: 0.8891074881132911\n",
      "Epoch 92, Training Loss: 0.8884220403783462\n",
      "Epoch 93, Training Loss: 0.8877278137207031\n",
      "Epoch 94, Training Loss: 0.8870210719809812\n",
      "Epoch 95, Training Loss: 0.8863212962010327\n",
      "Epoch 96, Training Loss: 0.8856165980591494\n",
      "Epoch 97, Training Loss: 0.8849075733212863\n",
      "Epoch 98, Training Loss: 0.884195950311773\n",
      "Epoch 99, Training Loss: 0.8834764677636764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:46:04,749] Trial 214 finished with value: 0.5846 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8827578330039978\n",
      "Epoch 1, Training Loss: 0.8765345000682917\n",
      "Epoch 2, Training Loss: 0.8282309245346184\n",
      "Epoch 3, Training Loss: 0.8207350142916342\n",
      "Epoch 4, Training Loss: 0.8168844714200586\n",
      "Epoch 5, Training Loss: 0.8122703773634774\n",
      "Epoch 6, Training Loss: 0.8101465486046067\n",
      "Epoch 7, Training Loss: 0.8094076604771435\n",
      "Epoch 8, Training Loss: 0.8079691901242823\n",
      "Epoch 9, Training Loss: 0.8077051019310055\n",
      "Epoch 10, Training Loss: 0.8049465022589031\n",
      "Epoch 11, Training Loss: 0.8041413969563362\n",
      "Epoch 12, Training Loss: 0.803279779817825\n",
      "Epoch 13, Training Loss: 0.8042253371468164\n",
      "Epoch 14, Training Loss: 0.8030960369827156\n",
      "Epoch 15, Training Loss: 0.8025760283147482\n",
      "Epoch 16, Training Loss: 0.8026331907824467\n",
      "Epoch 17, Training Loss: 0.8019908476592903\n",
      "Epoch 18, Training Loss: 0.7997473201805487\n",
      "Epoch 19, Training Loss: 0.801375556978068\n",
      "Epoch 20, Training Loss: 0.8006332153664496\n",
      "Epoch 21, Training Loss: 0.8000546213827635\n",
      "Epoch 22, Training Loss: 0.7982902304122322\n",
      "Epoch 23, Training Loss: 0.7994536749402383\n",
      "Epoch 24, Training Loss: 0.8005099444461048\n",
      "Epoch 25, Training Loss: 0.7988137870802915\n",
      "Epoch 26, Training Loss: 0.7994825798766058\n",
      "Epoch 27, Training Loss: 0.7993373848441848\n",
      "Epoch 28, Training Loss: 0.7986056116290559\n",
      "Epoch 29, Training Loss: 0.79822966218891\n",
      "Epoch 30, Training Loss: 0.7982467950734877\n",
      "Epoch 31, Training Loss: 0.7979506947940453\n",
      "Epoch 32, Training Loss: 0.7977572910767748\n",
      "Epoch 33, Training Loss: 0.798418402851076\n",
      "Epoch 34, Training Loss: 0.7981719716150958\n",
      "Epoch 35, Training Loss: 0.7978183156565616\n",
      "Epoch 36, Training Loss: 0.7968192386447935\n",
      "Epoch 37, Training Loss: 0.7975762654964189\n",
      "Epoch 38, Training Loss: 0.7972472551173734\n",
      "Epoch 39, Training Loss: 0.7979340178625924\n",
      "Epoch 40, Training Loss: 0.7971726132514781\n",
      "Epoch 41, Training Loss: 0.7969605124086365\n",
      "Epoch 42, Training Loss: 0.7963454991354978\n",
      "Epoch 43, Training Loss: 0.796934009315376\n",
      "Epoch 44, Training Loss: 0.7983731055618228\n",
      "Epoch 45, Training Loss: 0.7970435990426773\n",
      "Epoch 46, Training Loss: 0.7965586586106093\n",
      "Epoch 47, Training Loss: 0.7962113814246385\n",
      "Epoch 48, Training Loss: 0.7959688326469938\n",
      "Epoch 49, Training Loss: 0.7960118085818183\n",
      "Epoch 50, Training Loss: 0.7958873004841626\n",
      "Epoch 51, Training Loss: 0.7965070037913502\n",
      "Epoch 52, Training Loss: 0.796183387050055\n",
      "Epoch 53, Training Loss: 0.7957048027138961\n",
      "Epoch 54, Training Loss: 0.7963405263155027\n",
      "Epoch 55, Training Loss: 0.7956885429253255\n",
      "Epoch 56, Training Loss: 0.796467886114479\n",
      "Epoch 57, Training Loss: 0.7956521984329797\n",
      "Epoch 58, Training Loss: 0.7954418930792271\n",
      "Epoch 59, Training Loss: 0.7958364350455148\n",
      "Epoch 60, Training Loss: 0.7956134531730996\n",
      "Epoch 61, Training Loss: 0.7946955098245377\n",
      "Epoch 62, Training Loss: 0.7958954233872263\n",
      "Epoch 63, Training Loss: 0.7949666407771576\n",
      "Epoch 64, Training Loss: 0.7956738577749496\n",
      "Epoch 65, Training Loss: 0.795363305072139\n",
      "Epoch 66, Training Loss: 0.7947914064378667\n",
      "Epoch 67, Training Loss: 0.7959481341498239\n",
      "Epoch 68, Training Loss: 0.7962127109219257\n",
      "Epoch 69, Training Loss: 0.7946736885192699\n",
      "Epoch 70, Training Loss: 0.7960812691459083\n",
      "Epoch 71, Training Loss: 0.7952288468977562\n",
      "Epoch 72, Training Loss: 0.7953764207381054\n",
      "Epoch 73, Training Loss: 0.7950449697057107\n",
      "Epoch 74, Training Loss: 0.7949695145277152\n",
      "Epoch 75, Training Loss: 0.7955306862529955\n",
      "Epoch 76, Training Loss: 0.7946684032454526\n",
      "Epoch 77, Training Loss: 0.7948586507847435\n",
      "Epoch 78, Training Loss: 0.7952644075666155\n",
      "Epoch 79, Training Loss: 0.7957955542363618\n",
      "Epoch 80, Training Loss: 0.7945256000174615\n",
      "Epoch 81, Training Loss: 0.7952791709648935\n",
      "Epoch 82, Training Loss: 0.7946198067270723\n",
      "Epoch 83, Training Loss: 0.7951516467825811\n",
      "Epoch 84, Training Loss: 0.794743475788518\n",
      "Epoch 85, Training Loss: 0.7941896842834645\n",
      "Epoch 86, Training Loss: 0.7946084122908743\n",
      "Epoch 87, Training Loss: 0.794106878553118\n",
      "Epoch 88, Training Loss: 0.7953438576002766\n",
      "Epoch 89, Training Loss: 0.7935705972793407\n",
      "Epoch 90, Training Loss: 0.7948896954830428\n",
      "Epoch 91, Training Loss: 0.7935607845621898\n",
      "Epoch 92, Training Loss: 0.7944463263776965\n",
      "Epoch 93, Training Loss: 0.7942926116455766\n",
      "Epoch 94, Training Loss: 0.7941126642370583\n",
      "Epoch 95, Training Loss: 0.7946170712772168\n",
      "Epoch 96, Training Loss: 0.7942644634641203\n",
      "Epoch 97, Training Loss: 0.7952286226408822\n",
      "Epoch 98, Training Loss: 0.7942931344634608\n",
      "Epoch 99, Training Loss: 0.794638208607982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:47:55,870] Trial 215 finished with value: 0.5598666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7942246831449351\n",
      "Epoch 1, Training Loss: 0.8500901095306173\n",
      "Epoch 2, Training Loss: 0.8180424234446357\n",
      "Epoch 3, Training Loss: 0.8130586403958938\n",
      "Epoch 4, Training Loss: 0.8100570046200471\n",
      "Epoch 5, Training Loss: 0.8079944429678075\n",
      "Epoch 6, Training Loss: 0.8110221286380992\n",
      "Epoch 7, Training Loss: 0.808216021972544\n",
      "Epoch 8, Training Loss: 0.8064791969692006\n",
      "Epoch 9, Training Loss: 0.8058750248656553\n",
      "Epoch 10, Training Loss: 0.8051173822318807\n",
      "Epoch 11, Training Loss: 0.8061640125863693\n",
      "Epoch 12, Training Loss: 0.8052706796281478\n",
      "Epoch 13, Training Loss: 0.8052325098654803\n",
      "Epoch 14, Training Loss: 0.803472431898117\n",
      "Epoch 15, Training Loss: 0.8023455772680395\n",
      "Epoch 16, Training Loss: 0.8019383045505075\n",
      "Epoch 17, Training Loss: 0.8028021407828612\n",
      "Epoch 18, Training Loss: 0.8029673086194431\n",
      "Epoch 19, Training Loss: 0.803285342945772\n",
      "Epoch 20, Training Loss: 0.8014632250280941\n",
      "Epoch 21, Training Loss: 0.8022929235065684\n",
      "Epoch 22, Training Loss: 0.8007959862316356\n",
      "Epoch 23, Training Loss: 0.8011293601288515\n",
      "Epoch 24, Training Loss: 0.8020758910740123\n",
      "Epoch 25, Training Loss: 0.8020764371928046\n",
      "Epoch 26, Training Loss: 0.8028956805958467\n",
      "Epoch 27, Training Loss: 0.8031925660021165\n",
      "Epoch 28, Training Loss: 0.8015450260218452\n",
      "Epoch 29, Training Loss: 0.8014788709668552\n",
      "Epoch 30, Training Loss: 0.8018659913539886\n",
      "Epoch 31, Training Loss: 0.802047425999361\n",
      "Epoch 32, Training Loss: 0.8002698180955999\n",
      "Epoch 33, Training Loss: 0.7994359821431777\n",
      "Epoch 34, Training Loss: 0.7985398966424605\n",
      "Epoch 35, Training Loss: 0.7998790520780227\n",
      "Epoch 36, Training Loss: 0.7993947302593905\n",
      "Epoch 37, Training Loss: 0.8008923461156733\n",
      "Epoch 38, Training Loss: 0.798729897737503\n",
      "Epoch 39, Training Loss: 0.8007735749553232\n",
      "Epoch 40, Training Loss: 0.7984543705687803\n",
      "Epoch 41, Training Loss: 0.7999152974521413\n",
      "Epoch 42, Training Loss: 0.799371986319037\n",
      "Epoch 43, Training Loss: 0.7999110174179077\n",
      "Epoch 44, Training Loss: 0.79967521499185\n",
      "Epoch 45, Training Loss: 0.7982327566427343\n",
      "Epoch 46, Training Loss: 0.7999900272313286\n",
      "Epoch 47, Training Loss: 0.7983296193796046\n",
      "Epoch 48, Training Loss: 0.7985920623470755\n",
      "Epoch 49, Training Loss: 0.7990969591982224\n",
      "Epoch 50, Training Loss: 0.797883425319896\n",
      "Epoch 51, Training Loss: 0.7970344936847686\n",
      "Epoch 52, Training Loss: 0.798799577811185\n",
      "Epoch 53, Training Loss: 0.7971996771588045\n",
      "Epoch 54, Training Loss: 0.7992997822340797\n",
      "Epoch 55, Training Loss: 0.799313371181488\n",
      "Epoch 56, Training Loss: 0.7988685762882233\n",
      "Epoch 57, Training Loss: 0.798005323339911\n",
      "Epoch 58, Training Loss: 0.7989481852335089\n",
      "Epoch 59, Training Loss: 0.799557943834978\n",
      "Epoch 60, Training Loss: 0.7981864943223841\n",
      "Epoch 61, Training Loss: 0.7997484417522654\n",
      "Epoch 62, Training Loss: 0.7982031106247621\n",
      "Epoch 63, Training Loss: 0.7975683139352238\n",
      "Epoch 64, Training Loss: 0.799894676769481\n",
      "Epoch 65, Training Loss: 0.7983711383623235\n",
      "Epoch 66, Training Loss: 0.7994951593174654\n",
      "Epoch 67, Training Loss: 0.7997329455263474\n",
      "Epoch 68, Training Loss: 0.7982459920995376\n",
      "Epoch 69, Training Loss: 0.7984431835483102\n",
      "Epoch 70, Training Loss: 0.7987955636136672\n",
      "Epoch 71, Training Loss: 0.7978927735721364\n",
      "Epoch 72, Training Loss: 0.7978918705267065\n",
      "Epoch 73, Training Loss: 0.7973211672025569\n",
      "Epoch 74, Training Loss: 0.796805502877516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:49:49,404] Trial 216 finished with value: 0.6352 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7987798955861259\n",
      "Epoch 1, Training Loss: 0.8672422732625689\n",
      "Epoch 2, Training Loss: 0.827043584594153\n",
      "Epoch 3, Training Loss: 0.8180250952118322\n",
      "Epoch 4, Training Loss: 0.8106101690378404\n",
      "Epoch 5, Training Loss: 0.8096063369198849\n",
      "Epoch 6, Training Loss: 0.8065833028994108\n",
      "Epoch 7, Training Loss: 0.8060525682635773\n",
      "Epoch 8, Training Loss: 0.8039012428513147\n",
      "Epoch 9, Training Loss: 0.8035509253803053\n",
      "Epoch 10, Training Loss: 0.8036084713792442\n",
      "Epoch 11, Training Loss: 0.8017129145170513\n",
      "Epoch 12, Training Loss: 0.8009429251789151\n",
      "Epoch 13, Training Loss: 0.8012926662774911\n",
      "Epoch 14, Training Loss: 0.8001760577796994\n",
      "Epoch 15, Training Loss: 0.7999436000235995\n",
      "Epoch 16, Training Loss: 0.7985817578502168\n",
      "Epoch 17, Training Loss: 0.8007956968214279\n",
      "Epoch 18, Training Loss: 0.7987719974123446\n",
      "Epoch 19, Training Loss: 0.7983944361371206\n",
      "Epoch 20, Training Loss: 0.7992788719055348\n",
      "Epoch 21, Training Loss: 0.7993094091128586\n",
      "Epoch 22, Training Loss: 0.798077494757516\n",
      "Epoch 23, Training Loss: 0.7979022949261773\n",
      "Epoch 24, Training Loss: 0.7972106145288711\n",
      "Epoch 25, Training Loss: 0.7972921906557299\n",
      "Epoch 26, Training Loss: 0.796513693583639\n",
      "Epoch 27, Training Loss: 0.7978095159494787\n",
      "Epoch 28, Training Loss: 0.7968881466334924\n",
      "Epoch 29, Training Loss: 0.7970318464408244\n",
      "Epoch 30, Training Loss: 0.7959348154247256\n",
      "Epoch 31, Training Loss: 0.7966735277857099\n",
      "Epoch 32, Training Loss: 0.7956781896433436\n",
      "Epoch 33, Training Loss: 0.7967768723803356\n",
      "Epoch 34, Training Loss: 0.7965343016430848\n",
      "Epoch 35, Training Loss: 0.7959023768740489\n",
      "Epoch 36, Training Loss: 0.7959985859412\n",
      "Epoch 37, Training Loss: 0.7957360245231399\n",
      "Epoch 38, Training Loss: 0.7958112011278482\n",
      "Epoch 39, Training Loss: 0.795171575102591\n",
      "Epoch 40, Training Loss: 0.7957127671492727\n",
      "Epoch 41, Training Loss: 0.7950873675202965\n",
      "Epoch 42, Training Loss: 0.7941292897202915\n",
      "Epoch 43, Training Loss: 0.7956954423646282\n",
      "Epoch 44, Training Loss: 0.7953097006432096\n",
      "Epoch 45, Training Loss: 0.7953335014501013\n",
      "Epoch 46, Training Loss: 0.7953650949592878\n",
      "Epoch 47, Training Loss: 0.7949645444862825\n",
      "Epoch 48, Training Loss: 0.7954229988549885\n",
      "Epoch 49, Training Loss: 0.7950768437600674\n",
      "Epoch 50, Training Loss: 0.7959768892230844\n",
      "Epoch 51, Training Loss: 0.7941218118918569\n",
      "Epoch 52, Training Loss: 0.7960600261401413\n",
      "Epoch 53, Training Loss: 0.7946797344021331\n",
      "Epoch 54, Training Loss: 0.7947376261976429\n",
      "Epoch 55, Training Loss: 0.7938007752698167\n",
      "Epoch 56, Training Loss: 0.7950983979648217\n",
      "Epoch 57, Training Loss: 0.7945910123057831\n",
      "Epoch 58, Training Loss: 0.7938159521361042\n",
      "Epoch 59, Training Loss: 0.7941753758523697\n",
      "Epoch 60, Training Loss: 0.7942440241799319\n",
      "Epoch 61, Training Loss: 0.7957537044259838\n",
      "Epoch 62, Training Loss: 0.7944022669828028\n",
      "Epoch 63, Training Loss: 0.794167233051214\n",
      "Epoch 64, Training Loss: 0.7946958403838308\n",
      "Epoch 65, Training Loss: 0.7938183265521114\n",
      "Epoch 66, Training Loss: 0.7943780144354454\n",
      "Epoch 67, Training Loss: 0.7947252458199523\n",
      "Epoch 68, Training Loss: 0.7941197964481841\n",
      "Epoch 69, Training Loss: 0.7937571847349181\n",
      "Epoch 70, Training Loss: 0.7938243839077483\n",
      "Epoch 71, Training Loss: 0.7936976219478407\n",
      "Epoch 72, Training Loss: 0.7933543013450794\n",
      "Epoch 73, Training Loss: 0.793093903172285\n",
      "Epoch 74, Training Loss: 0.7939765736572725\n",
      "Epoch 75, Training Loss: 0.7931272057662333\n",
      "Epoch 76, Training Loss: 0.7936440395233326\n",
      "Epoch 77, Training Loss: 0.7932825974952009\n",
      "Epoch 78, Training Loss: 0.7934871561545178\n",
      "Epoch 79, Training Loss: 0.793733551000294\n",
      "Epoch 80, Training Loss: 0.7944108094487872\n",
      "Epoch 81, Training Loss: 0.7934930212515637\n",
      "Epoch 82, Training Loss: 0.7933846454871328\n",
      "Epoch 83, Training Loss: 0.7927304684667659\n",
      "Epoch 84, Training Loss: 0.7925783317788203\n",
      "Epoch 85, Training Loss: 0.793862673662659\n",
      "Epoch 86, Training Loss: 0.7929210942490656\n",
      "Epoch 87, Training Loss: 0.7930795383632632\n",
      "Epoch 88, Training Loss: 0.7928791764983557\n",
      "Epoch 89, Training Loss: 0.7931862507547651\n",
      "Epoch 90, Training Loss: 0.7936572323168131\n",
      "Epoch 91, Training Loss: 0.7923954488639545\n",
      "Epoch 92, Training Loss: 0.7934373187839536\n",
      "Epoch 93, Training Loss: 0.7928417670995669\n",
      "Epoch 94, Training Loss: 0.792795687600186\n",
      "Epoch 95, Training Loss: 0.7921201084789476\n",
      "Epoch 96, Training Loss: 0.7920231513959125\n",
      "Epoch 97, Training Loss: 0.7927211520367099\n",
      "Epoch 98, Training Loss: 0.7932944837369417\n",
      "Epoch 99, Training Loss: 0.7927939257227389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:51:38,745] Trial 217 finished with value: 0.6124666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7923678527201029\n",
      "Epoch 1, Training Loss: 0.9067367976769469\n",
      "Epoch 2, Training Loss: 0.820334461846746\n",
      "Epoch 3, Training Loss: 0.8129047409036105\n",
      "Epoch 4, Training Loss: 0.8105401763342377\n",
      "Epoch 5, Training Loss: 0.8089391209129104\n",
      "Epoch 6, Training Loss: 0.805616919976428\n",
      "Epoch 7, Training Loss: 0.8038618474974668\n",
      "Epoch 8, Training Loss: 0.8037715647453653\n",
      "Epoch 9, Training Loss: 0.803908113787945\n",
      "Epoch 10, Training Loss: 0.8010436683669127\n",
      "Epoch 11, Training Loss: 0.8006296679489595\n",
      "Epoch 12, Training Loss: 0.8016343581049066\n",
      "Epoch 13, Training Loss: 0.8004538760149389\n",
      "Epoch 14, Training Loss: 0.8002725188893484\n",
      "Epoch 15, Training Loss: 0.7998091045178866\n",
      "Epoch 16, Training Loss: 0.7989375370785706\n",
      "Epoch 17, Training Loss: 0.7986296458351881\n",
      "Epoch 18, Training Loss: 0.7981810793840796\n",
      "Epoch 19, Training Loss: 0.7978946505632616\n",
      "Epoch 20, Training Loss: 0.7970372364037019\n",
      "Epoch 21, Training Loss: 0.7968273265917498\n",
      "Epoch 22, Training Loss: 0.7970327557477735\n",
      "Epoch 23, Training Loss: 0.7965116507128666\n",
      "Epoch 24, Training Loss: 0.796591071437176\n",
      "Epoch 25, Training Loss: 0.7955252007434243\n",
      "Epoch 26, Training Loss: 0.7952787907500016\n",
      "Epoch 27, Training Loss: 0.7944173928042103\n",
      "Epoch 28, Training Loss: 0.7942203359048169\n",
      "Epoch 29, Training Loss: 0.7933578821053182\n",
      "Epoch 30, Training Loss: 0.7923891746460047\n",
      "Epoch 31, Training Loss: 0.7929559178818437\n",
      "Epoch 32, Training Loss: 0.7923116532483495\n",
      "Epoch 33, Training Loss: 0.7919112277210207\n",
      "Epoch 34, Training Loss: 0.7911019896206103\n",
      "Epoch 35, Training Loss: 0.7899512637826733\n",
      "Epoch 36, Training Loss: 0.7905951784069377\n",
      "Epoch 37, Training Loss: 0.7904225551992431\n",
      "Epoch 38, Training Loss: 0.7888221830353701\n",
      "Epoch 39, Training Loss: 0.7882269295534693\n",
      "Epoch 40, Training Loss: 0.7880647994521865\n",
      "Epoch 41, Training Loss: 0.7877690805528397\n",
      "Epoch 42, Training Loss: 0.787614387318604\n",
      "Epoch 43, Training Loss: 0.7867672754409618\n",
      "Epoch 44, Training Loss: 0.7868383156625848\n",
      "Epoch 45, Training Loss: 0.7863534106347794\n",
      "Epoch 46, Training Loss: 0.7864687031373045\n",
      "Epoch 47, Training Loss: 0.7854703541088821\n",
      "Epoch 48, Training Loss: 0.7855200429608051\n",
      "Epoch 49, Training Loss: 0.7848912862010469\n",
      "Epoch 50, Training Loss: 0.7845258712768555\n",
      "Epoch 51, Training Loss: 0.7842140050758993\n",
      "Epoch 52, Training Loss: 0.784254266713795\n",
      "Epoch 53, Training Loss: 0.7840424423827265\n",
      "Epoch 54, Training Loss: 0.7840446990235408\n",
      "Epoch 55, Training Loss: 0.7837297577607004\n",
      "Epoch 56, Training Loss: 0.7839330979755946\n",
      "Epoch 57, Training Loss: 0.7837085960951067\n",
      "Epoch 58, Training Loss: 0.784275960474086\n",
      "Epoch 59, Training Loss: 0.783136273640439\n",
      "Epoch 60, Training Loss: 0.7837684750556946\n",
      "Epoch 61, Training Loss: 0.783268846515426\n",
      "Epoch 62, Training Loss: 0.7836767416251333\n",
      "Epoch 63, Training Loss: 0.7836251085862181\n",
      "Epoch 64, Training Loss: 0.7830134702804393\n",
      "Epoch 65, Training Loss: 0.7828623682932746\n",
      "Epoch 66, Training Loss: 0.782848164192716\n",
      "Epoch 67, Training Loss: 0.7831081705882137\n",
      "Epoch 68, Training Loss: 0.7836625954262296\n",
      "Epoch 69, Training Loss: 0.7824650384429702\n",
      "Epoch 70, Training Loss: 0.7829269010321538\n",
      "Epoch 71, Training Loss: 0.7825914503936481\n",
      "Epoch 72, Training Loss: 0.7833477949737606\n",
      "Epoch 73, Training Loss: 0.7829522717267947\n",
      "Epoch 74, Training Loss: 0.7829865241409244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:53:05,524] Trial 218 finished with value: 0.6385333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7831832144493447\n",
      "Epoch 1, Training Loss: 0.9436891106296988\n",
      "Epoch 2, Training Loss: 0.8722408599012038\n",
      "Epoch 3, Training Loss: 0.8291157860615674\n",
      "Epoch 4, Training Loss: 0.8163578662451576\n",
      "Epoch 5, Training Loss: 0.812289602616254\n",
      "Epoch 6, Training Loss: 0.8102792337361504\n",
      "Epoch 7, Training Loss: 0.8091037450117223\n",
      "Epoch 8, Training Loss: 0.808783958939945\n",
      "Epoch 9, Training Loss: 0.8081032119779026\n",
      "Epoch 10, Training Loss: 0.8075936283083524\n",
      "Epoch 11, Training Loss: 0.8069059972202076\n",
      "Epoch 12, Training Loss: 0.8065132810087765\n",
      "Epoch 13, Training Loss: 0.8057842172594631\n",
      "Epoch 14, Training Loss: 0.8054139935970306\n",
      "Epoch 15, Training Loss: 0.8053680659742917\n",
      "Epoch 16, Training Loss: 0.8054923647992751\n",
      "Epoch 17, Training Loss: 0.8049391257762909\n",
      "Epoch 18, Training Loss: 0.8047236595434301\n",
      "Epoch 19, Training Loss: 0.8043511848589954\n",
      "Epoch 20, Training Loss: 0.8038497491443859\n",
      "Epoch 21, Training Loss: 0.8038678072480594\n",
      "Epoch 22, Training Loss: 0.8035570299625396\n",
      "Epoch 23, Training Loss: 0.8031144795698278\n",
      "Epoch 24, Training Loss: 0.8029251745869131\n",
      "Epoch 25, Training Loss: 0.8024843477501589\n",
      "Epoch 26, Training Loss: 0.8025816287012661\n",
      "Epoch 27, Training Loss: 0.8020954149610856\n",
      "Epoch 28, Training Loss: 0.8020457051080816\n",
      "Epoch 29, Training Loss: 0.8020317458405214\n",
      "Epoch 30, Training Loss: 0.801864521222956\n",
      "Epoch 31, Training Loss: 0.8011570170346428\n",
      "Epoch 32, Training Loss: 0.8014605808258056\n",
      "Epoch 33, Training Loss: 0.8006930241164039\n",
      "Epoch 34, Training Loss: 0.8006992007704342\n",
      "Epoch 35, Training Loss: 0.8007194190165576\n",
      "Epoch 36, Training Loss: 0.8006074598957511\n",
      "Epoch 37, Training Loss: 0.8005666314854342\n",
      "Epoch 38, Training Loss: 0.8000930058956146\n",
      "Epoch 39, Training Loss: 0.7999520578805138\n",
      "Epoch 40, Training Loss: 0.799810130035176\n",
      "Epoch 41, Training Loss: 0.799923361119102\n",
      "Epoch 42, Training Loss: 0.799684261784834\n",
      "Epoch 43, Training Loss: 0.7997852814898772\n",
      "Epoch 44, Training Loss: 0.7994780140764574\n",
      "Epoch 45, Training Loss: 0.7992783383060904\n",
      "Epoch 46, Training Loss: 0.799202498478048\n",
      "Epoch 47, Training Loss: 0.7989689036677865\n",
      "Epoch 48, Training Loss: 0.7992116589405958\n",
      "Epoch 49, Training Loss: 0.7990268074764925\n",
      "Epoch 50, Training Loss: 0.7991533726804396\n",
      "Epoch 51, Training Loss: 0.7993687169692095\n",
      "Epoch 52, Training Loss: 0.798875755702748\n",
      "Epoch 53, Training Loss: 0.7988710622927722\n",
      "Epoch 54, Training Loss: 0.7987061439542209\n",
      "Epoch 55, Training Loss: 0.7984445542447707\n",
      "Epoch 56, Training Loss: 0.7986829394452712\n",
      "Epoch 57, Training Loss: 0.7986940789222717\n",
      "Epoch 58, Training Loss: 0.7982818936600404\n",
      "Epoch 59, Training Loss: 0.7986631197087904\n",
      "Epoch 60, Training Loss: 0.7978888097230126\n",
      "Epoch 61, Training Loss: 0.7982564461231232\n",
      "Epoch 62, Training Loss: 0.798428411063026\n",
      "Epoch 63, Training Loss: 0.7983370684876161\n",
      "Epoch 64, Training Loss: 0.7979453588233275\n",
      "Epoch 65, Training Loss: 0.7982456946372986\n",
      "Epoch 66, Training Loss: 0.7982976473780239\n",
      "Epoch 67, Training Loss: 0.7979193037397722\n",
      "Epoch 68, Training Loss: 0.7982105154149672\n",
      "Epoch 69, Training Loss: 0.7978734193128698\n",
      "Epoch 70, Training Loss: 0.7980216585187351\n",
      "Epoch 71, Training Loss: 0.798073638046489\n",
      "Epoch 72, Training Loss: 0.7976663009559407\n",
      "Epoch 73, Training Loss: 0.7979790426703061\n",
      "Epoch 74, Training Loss: 0.797655197802712\n",
      "Epoch 75, Training Loss: 0.7977626792823567\n",
      "Epoch 76, Training Loss: 0.7978205321816837\n",
      "Epoch 77, Training Loss: 0.7976614344821257\n",
      "Epoch 78, Training Loss: 0.797719385062947\n",
      "Epoch 79, Training Loss: 0.7976882900209988\n",
      "Epoch 80, Training Loss: 0.7976631402969361\n",
      "Epoch 81, Training Loss: 0.7973918809610254\n",
      "Epoch 82, Training Loss: 0.7976337376762839\n",
      "Epoch 83, Training Loss: 0.7975885671026567\n",
      "Epoch 84, Training Loss: 0.7973103261695189\n",
      "Epoch 85, Training Loss: 0.7974260975332821\n",
      "Epoch 86, Training Loss: 0.7975800819256726\n",
      "Epoch 87, Training Loss: 0.797329046235365\n",
      "Epoch 88, Training Loss: 0.7970591738644768\n",
      "Epoch 89, Training Loss: 0.7972992300286013\n",
      "Epoch 90, Training Loss: 0.7973622412541334\n",
      "Epoch 91, Training Loss: 0.7974683294576758\n",
      "Epoch 92, Training Loss: 0.7973594040029189\n",
      "Epoch 93, Training Loss: 0.7974576997756958\n",
      "Epoch 94, Training Loss: 0.7970603615396163\n",
      "Epoch 95, Training Loss: 0.797257687624763\n",
      "Epoch 96, Training Loss: 0.7970985501654008\n",
      "Epoch 97, Training Loss: 0.7972939895882326\n",
      "Epoch 98, Training Loss: 0.7971252836900599\n",
      "Epoch 99, Training Loss: 0.7970415330634397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:55:03,518] Trial 219 finished with value: 0.6344 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7971680880995358\n",
      "Epoch 1, Training Loss: 0.9853067395266365\n",
      "Epoch 2, Training Loss: 0.9512863151466145\n",
      "Epoch 3, Training Loss: 0.9417916773347293\n",
      "Epoch 4, Training Loss: 0.932471607502769\n",
      "Epoch 5, Training Loss: 0.9231879373858957\n",
      "Epoch 6, Training Loss: 0.9137340896971086\n",
      "Epoch 7, Training Loss: 0.9040707660422606\n",
      "Epoch 8, Training Loss: 0.8945329913672279\n",
      "Epoch 9, Training Loss: 0.885091096723781\n",
      "Epoch 10, Training Loss: 0.8758168047315934\n",
      "Epoch 11, Training Loss: 0.8668592153577244\n",
      "Epoch 12, Training Loss: 0.8587471682885114\n",
      "Epoch 13, Training Loss: 0.8510920451669132\n",
      "Epoch 14, Training Loss: 0.8443278477472418\n",
      "Epoch 15, Training Loss: 0.8383070110573488\n",
      "Epoch 16, Training Loss: 0.8331933510303497\n",
      "Epoch 17, Training Loss: 0.8287833192769218\n",
      "Epoch 18, Training Loss: 0.8252243186445797\n",
      "Epoch 19, Training Loss: 0.8221716474084293\n",
      "Epoch 20, Training Loss: 0.819605416059494\n",
      "Epoch 21, Training Loss: 0.8176005676213433\n",
      "Epoch 22, Training Loss: 0.8159356268013225\n",
      "Epoch 23, Training Loss: 0.8145907390117645\n",
      "Epoch 24, Training Loss: 0.8134480224637424\n",
      "Epoch 25, Training Loss: 0.8124823805163889\n",
      "Epoch 26, Training Loss: 0.8117600536346435\n",
      "Epoch 27, Training Loss: 0.811191547758439\n",
      "Epoch 28, Training Loss: 0.8105519864839666\n",
      "Epoch 29, Training Loss: 0.8100966882004458\n",
      "Epoch 30, Training Loss: 0.8096840930686278\n",
      "Epoch 31, Training Loss: 0.8093656416500316\n",
      "Epoch 32, Training Loss: 0.8089545859308804\n",
      "Epoch 33, Training Loss: 0.8087345757203943\n",
      "Epoch 34, Training Loss: 0.8084174874249627\n",
      "Epoch 35, Training Loss: 0.808212696944966\n",
      "Epoch 36, Training Loss: 0.8079744892260607\n",
      "Epoch 37, Training Loss: 0.8076816621948691\n",
      "Epoch 38, Training Loss: 0.8075473245452433\n",
      "Epoch 39, Training Loss: 0.8074531036965987\n",
      "Epoch 40, Training Loss: 0.8071554927966174\n",
      "Epoch 41, Training Loss: 0.8069777060957516\n",
      "Epoch 42, Training Loss: 0.8068985743382397\n",
      "Epoch 43, Training Loss: 0.8066106174272649\n",
      "Epoch 44, Training Loss: 0.8064221142320072\n",
      "Epoch 45, Training Loss: 0.8062147287761464\n",
      "Epoch 46, Training Loss: 0.8061933788832496\n",
      "Epoch 47, Training Loss: 0.806022844805437\n",
      "Epoch 48, Training Loss: 0.8058540472563576\n",
      "Epoch 49, Training Loss: 0.8057898761244381\n",
      "Epoch 50, Training Loss: 0.805578402000315\n",
      "Epoch 51, Training Loss: 0.8055076316524954\n",
      "Epoch 52, Training Loss: 0.8054188818090102\n",
      "Epoch 53, Training Loss: 0.8052274178757387\n",
      "Epoch 54, Training Loss: 0.8049347462373622\n",
      "Epoch 55, Training Loss: 0.8049000113851884\n",
      "Epoch 56, Training Loss: 0.8048225611097672\n",
      "Epoch 57, Training Loss: 0.8047093928561491\n",
      "Epoch 58, Training Loss: 0.804502309701022\n",
      "Epoch 59, Training Loss: 0.8043724963945501\n",
      "Epoch 60, Training Loss: 0.8042523649860831\n",
      "Epoch 61, Training Loss: 0.8041828611317803\n",
      "Epoch 62, Training Loss: 0.8039777146367466\n",
      "Epoch 63, Training Loss: 0.8038994890100816\n",
      "Epoch 64, Training Loss: 0.8037708099449382\n",
      "Epoch 65, Training Loss: 0.8036827842628255\n",
      "Epoch 66, Training Loss: 0.8034894991622251\n",
      "Epoch 67, Training Loss: 0.8033902407393736\n",
      "Epoch 68, Training Loss: 0.8033253301592435\n",
      "Epoch 69, Training Loss: 0.8032179833860958\n",
      "Epoch 70, Training Loss: 0.8031151045771207\n",
      "Epoch 71, Training Loss: 0.8028645436202778\n",
      "Epoch 72, Training Loss: 0.8027718132383683\n",
      "Epoch 73, Training Loss: 0.8027604974017424\n",
      "Epoch 74, Training Loss: 0.802667860213448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:56:14,230] Trial 220 finished with value: 0.6318 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8025177525071536\n",
      "Epoch 1, Training Loss: 0.8640346714328317\n",
      "Epoch 2, Training Loss: 0.8155181882661932\n",
      "Epoch 3, Training Loss: 0.8088413490267361\n",
      "Epoch 4, Training Loss: 0.8037291886525996\n",
      "Epoch 5, Training Loss: 0.798765779172673\n",
      "Epoch 6, Training Loss: 0.7953518626970403\n",
      "Epoch 7, Training Loss: 0.792480984435362\n",
      "Epoch 8, Training Loss: 0.7919791363968569\n",
      "Epoch 9, Training Loss: 0.7906926227317137\n",
      "Epoch 10, Training Loss: 0.7899083096139571\n",
      "Epoch 11, Training Loss: 0.7896275417944965\n",
      "Epoch 12, Training Loss: 0.7891388076193192\n",
      "Epoch 13, Training Loss: 0.7886892661627601\n",
      "Epoch 14, Training Loss: 0.7882568191079532\n",
      "Epoch 15, Training Loss: 0.7880118437374339\n",
      "Epoch 16, Training Loss: 0.7880482675748713\n",
      "Epoch 17, Training Loss: 0.7881039488315582\n",
      "Epoch 18, Training Loss: 0.7869036350530737\n",
      "Epoch 19, Training Loss: 0.7869244443669039\n",
      "Epoch 20, Training Loss: 0.787294082220863\n",
      "Epoch 21, Training Loss: 0.7869175942505107\n",
      "Epoch 22, Training Loss: 0.7870215376685648\n",
      "Epoch 23, Training Loss: 0.7863229282463298\n",
      "Epoch 24, Training Loss: 0.785792846399195\n",
      "Epoch 25, Training Loss: 0.7858142232894898\n",
      "Epoch 26, Training Loss: 0.7853623921730939\n",
      "Epoch 27, Training Loss: 0.7855266397139605\n",
      "Epoch 28, Training Loss: 0.785765919194502\n",
      "Epoch 29, Training Loss: 0.7848521029949188\n",
      "Epoch 30, Training Loss: 0.7850377697804395\n",
      "Epoch 31, Training Loss: 0.7848325746900895\n",
      "Epoch 32, Training Loss: 0.7839163975154653\n",
      "Epoch 33, Training Loss: 0.785011471229441\n",
      "Epoch 34, Training Loss: 0.7846778900483076\n",
      "Epoch 35, Training Loss: 0.7842703999491298\n",
      "Epoch 36, Training Loss: 0.7842446178548477\n",
      "Epoch 37, Training Loss: 0.7843594619105844\n",
      "Epoch 38, Training Loss: 0.7844626834112055\n",
      "Epoch 39, Training Loss: 0.7834654978443595\n",
      "Epoch 40, Training Loss: 0.7834986239321091\n",
      "Epoch 41, Training Loss: 0.7833540181552663\n",
      "Epoch 42, Training Loss: 0.7838494113613578\n",
      "Epoch 43, Training Loss: 0.7825844602023854\n",
      "Epoch 44, Training Loss: 0.7832961921832141\n",
      "Epoch 45, Training Loss: 0.7835457596358131\n",
      "Epoch 46, Training Loss: 0.7826846281219931\n",
      "Epoch 47, Training Loss: 0.7825563937776229\n",
      "Epoch 48, Training Loss: 0.7828506119812236\n",
      "Epoch 49, Training Loss: 0.7826827177580665\n",
      "Epoch 50, Training Loss: 0.7823092234835906\n",
      "Epoch 51, Training Loss: 0.7823800940373364\n",
      "Epoch 52, Training Loss: 0.7824706712189843\n",
      "Epoch 53, Training Loss: 0.7822836549141827\n",
      "Epoch 54, Training Loss: 0.7821374048204983\n",
      "Epoch 55, Training Loss: 0.7822244114735547\n",
      "Epoch 56, Training Loss: 0.782244615204194\n",
      "Epoch 57, Training Loss: 0.7818366657986361\n",
      "Epoch 58, Training Loss: 0.7816672366506913\n",
      "Epoch 59, Training Loss: 0.7817493581070619\n",
      "Epoch 60, Training Loss: 0.781793731731527\n",
      "Epoch 61, Training Loss: 0.7813260226389941\n",
      "Epoch 62, Training Loss: 0.781861279221142\n",
      "Epoch 63, Training Loss: 0.7815507289942573\n",
      "Epoch 64, Training Loss: 0.781224088528577\n",
      "Epoch 65, Training Loss: 0.7814907829901752\n",
      "Epoch 66, Training Loss: 0.7811156009926515\n",
      "Epoch 67, Training Loss: 0.7809928362509784\n",
      "Epoch 68, Training Loss: 0.7813213203935062\n",
      "Epoch 69, Training Loss: 0.7808629335375393\n",
      "Epoch 70, Training Loss: 0.7813594068499172\n",
      "Epoch 71, Training Loss: 0.7808439481258392\n",
      "Epoch 72, Training Loss: 0.7810422410684473\n",
      "Epoch 73, Training Loss: 0.7808202959509457\n",
      "Epoch 74, Training Loss: 0.7809227486918954\n",
      "Epoch 75, Training Loss: 0.7811278428049648\n",
      "Epoch 76, Training Loss: 0.7807532660400166\n",
      "Epoch 77, Training Loss: 0.7809909312865313\n",
      "Epoch 78, Training Loss: 0.780342160954195\n",
      "Epoch 79, Training Loss: 0.7805375140554764\n",
      "Epoch 80, Training Loss: 0.7807885564776028\n",
      "Epoch 81, Training Loss: 0.7797039812452653\n",
      "Epoch 82, Training Loss: 0.7803450549350065\n",
      "Epoch 83, Training Loss: 0.7803836786045748\n",
      "Epoch 84, Training Loss: 0.7798771992150475\n",
      "Epoch 85, Training Loss: 0.7803793382644654\n",
      "Epoch 86, Training Loss: 0.7802545856027042\n",
      "Epoch 87, Training Loss: 0.7802150225639344\n",
      "Epoch 88, Training Loss: 0.7803546590664807\n",
      "Epoch 89, Training Loss: 0.7793460215540493\n",
      "Epoch 90, Training Loss: 0.7802459941892063\n",
      "Epoch 91, Training Loss: 0.7797764726246105\n",
      "Epoch 92, Training Loss: 0.7796663297625149\n",
      "Epoch 93, Training Loss: 0.7797783367774066\n",
      "Epoch 94, Training Loss: 0.7798213853555567\n",
      "Epoch 95, Training Loss: 0.779626803818871\n",
      "Epoch 96, Training Loss: 0.7795193679192487\n",
      "Epoch 97, Training Loss: 0.7796747940428117\n",
      "Epoch 98, Training Loss: 0.779549222202862\n",
      "Epoch 99, Training Loss: 0.7795570192617528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 01:58:47,587] Trial 221 finished with value: 0.6402666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7796686258736779\n",
      "Epoch 1, Training Loss: 1.0436083237031348\n",
      "Epoch 2, Training Loss: 0.909323645534372\n",
      "Epoch 3, Training Loss: 0.8957104278686351\n",
      "Epoch 4, Training Loss: 0.8880381290177654\n",
      "Epoch 5, Training Loss: 0.8878849348627535\n",
      "Epoch 6, Training Loss: 0.8778848059195324\n",
      "Epoch 7, Training Loss: 0.8812562007653085\n",
      "Epoch 8, Training Loss: 0.8774598342135437\n",
      "Epoch 9, Training Loss: 0.8797389501915839\n",
      "Epoch 10, Training Loss: 0.87803022928704\n",
      "Epoch 11, Training Loss: 0.8776417718794113\n",
      "Epoch 12, Training Loss: 0.8725475166973314\n",
      "Epoch 13, Training Loss: 0.8780011076676218\n",
      "Epoch 14, Training Loss: 0.877199939856852\n",
      "Epoch 15, Training Loss: 0.8742975644599226\n",
      "Epoch 16, Training Loss: 0.8826397487095424\n",
      "Epoch 17, Training Loss: 0.8711021918999521\n",
      "Epoch 18, Training Loss: 0.8727944642081297\n",
      "Epoch 19, Training Loss: 0.8724207307163038\n",
      "Epoch 20, Training Loss: 0.8701799267216732\n",
      "Epoch 21, Training Loss: 0.8687030865733785\n",
      "Epoch 22, Training Loss: 0.871915470388599\n",
      "Epoch 23, Training Loss: 0.8726715837206159\n",
      "Epoch 24, Training Loss: 0.8789142924143856\n",
      "Epoch 25, Training Loss: 0.8681842081528858\n",
      "Epoch 26, Training Loss: 0.8759461444123346\n",
      "Epoch 27, Training Loss: 0.877357252199847\n",
      "Epoch 28, Training Loss: 0.8741403013243711\n",
      "Epoch 29, Training Loss: 0.8771971246353666\n",
      "Epoch 30, Training Loss: 0.8711172424760977\n",
      "Epoch 31, Training Loss: 0.8703489699758085\n",
      "Epoch 32, Training Loss: 0.8718907196718947\n",
      "Epoch 33, Training Loss: 0.8744976193385017\n",
      "Epoch 34, Training Loss: 0.877383864284458\n",
      "Epoch 35, Training Loss: 0.8740440777369908\n",
      "Epoch 36, Training Loss: 0.8723908819650349\n",
      "Epoch 37, Training Loss: 0.8761182091289893\n",
      "Epoch 38, Training Loss: 0.8791801238418522\n",
      "Epoch 39, Training Loss: 0.8703219230013682\n",
      "Epoch 40, Training Loss: 0.8732293796718569\n",
      "Epoch 41, Training Loss: 0.8712068195629837\n",
      "Epoch 42, Training Loss: 0.8769462402601887\n",
      "Epoch 43, Training Loss: 0.8712433861610585\n",
      "Epoch 44, Training Loss: 0.8760059603174827\n",
      "Epoch 45, Training Loss: 0.8729808141414385\n",
      "Epoch 46, Training Loss: 0.8684840811822647\n",
      "Epoch 47, Training Loss: 0.8645998113137439\n",
      "Epoch 48, Training Loss: 0.8706320495533764\n",
      "Epoch 49, Training Loss: 0.8677452077542929\n",
      "Epoch 50, Training Loss: 0.8756426037702345\n",
      "Epoch 51, Training Loss: 0.8710842484818365\n",
      "Epoch 52, Training Loss: 0.877546461094591\n",
      "Epoch 53, Training Loss: 0.871368129988362\n",
      "Epoch 54, Training Loss: 0.8744983613042903\n",
      "Epoch 55, Training Loss: 0.8720892601443413\n",
      "Epoch 56, Training Loss: 0.8847912918356128\n",
      "Epoch 57, Training Loss: 0.8715037023214469\n",
      "Epoch 58, Training Loss: 0.8738390412545742\n",
      "Epoch 59, Training Loss: 0.8691966764012674\n",
      "Epoch 60, Training Loss: 0.872258444567372\n",
      "Epoch 61, Training Loss: 0.8627892591003189\n",
      "Epoch 62, Training Loss: 0.8666145746869253\n",
      "Epoch 63, Training Loss: 0.8691541355355341\n",
      "Epoch 64, Training Loss: 0.8670771115704586\n",
      "Epoch 65, Training Loss: 0.8738478439194816\n",
      "Epoch 66, Training Loss: 0.8708015835374818\n",
      "Epoch 67, Training Loss: 0.8710262732398241\n",
      "Epoch 68, Training Loss: 0.8708865161228897\n",
      "Epoch 69, Training Loss: 0.873543547687674\n",
      "Epoch 70, Training Loss: 0.8719732176092334\n",
      "Epoch 71, Training Loss: 0.8717406162641999\n",
      "Epoch 72, Training Loss: 0.871555987874368\n",
      "Epoch 73, Training Loss: 0.8703064743737529\n",
      "Epoch 74, Training Loss: 0.8665611170288315\n",
      "Epoch 75, Training Loss: 0.8631939626277838\n",
      "Epoch 76, Training Loss: 0.8674326500498263\n",
      "Epoch 77, Training Loss: 0.8683120399489439\n",
      "Epoch 78, Training Loss: 0.8715325756180555\n",
      "Epoch 79, Training Loss: 0.8694589313707853\n",
      "Epoch 80, Training Loss: 0.8683289843394344\n",
      "Epoch 81, Training Loss: 0.8661461838205954\n",
      "Epoch 82, Training Loss: 0.8686349195645268\n",
      "Epoch 83, Training Loss: 0.8659910786420779\n",
      "Epoch 84, Training Loss: 0.8677602898805661\n",
      "Epoch 85, Training Loss: 0.8655327443789719\n",
      "Epoch 86, Training Loss: 0.8684156397231539\n",
      "Epoch 87, Training Loss: 0.868933637966787\n",
      "Epoch 88, Training Loss: 0.8704478193942765\n",
      "Epoch 89, Training Loss: 0.8715489011061819\n",
      "Epoch 90, Training Loss: 0.8805881943917813\n",
      "Epoch 91, Training Loss: 0.8701089032610556\n",
      "Epoch 92, Training Loss: 0.8741869036416362\n",
      "Epoch 93, Training Loss: 0.8713854590752967\n",
      "Epoch 94, Training Loss: 0.8716069124695054\n",
      "Epoch 95, Training Loss: 0.8816736466902539\n",
      "Epoch 96, Training Loss: 0.8730462154051415\n",
      "Epoch 97, Training Loss: 0.8848648149268071\n",
      "Epoch 98, Training Loss: 0.8754517066747622\n",
      "Epoch 99, Training Loss: 0.8689919334605224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:00:53,553] Trial 222 finished with value: 0.49966666666666665 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8741016924829411\n",
      "Epoch 1, Training Loss: 1.0808306785190807\n",
      "Epoch 2, Training Loss: 1.0582287312956418\n",
      "Epoch 3, Training Loss: 1.041028160908643\n",
      "Epoch 4, Training Loss: 1.0265464140387142\n",
      "Epoch 5, Training Loss: 1.0139730337086845\n",
      "Epoch 6, Training Loss: 1.002913998926387\n",
      "Epoch 7, Training Loss: 0.993141650241964\n",
      "Epoch 8, Training Loss: 0.9845127091688268\n",
      "Epoch 9, Training Loss: 0.9769437766075134\n",
      "Epoch 10, Training Loss: 0.9703317370835473\n",
      "Epoch 11, Training Loss: 0.9645977425575256\n",
      "Epoch 12, Training Loss: 0.9596548003308913\n",
      "Epoch 13, Training Loss: 0.9553974202099969\n",
      "Epoch 14, Training Loss: 0.9517385841117185\n",
      "Epoch 15, Training Loss: 0.9485791390783647\n",
      "Epoch 16, Training Loss: 0.9458363864001106\n",
      "Epoch 17, Training Loss: 0.9434447471534505\n",
      "Epoch 18, Training Loss: 0.9413290499238407\n",
      "Epoch 19, Training Loss: 0.939438851370531\n",
      "Epoch 20, Training Loss: 0.9377385586850784\n",
      "Epoch 21, Training Loss: 0.9361863170651828\n",
      "Epoch 22, Training Loss: 0.9347646152973175\n",
      "Epoch 23, Training Loss: 0.9334364816721747\n",
      "Epoch 24, Training Loss: 0.9321915648965274\n",
      "Epoch 25, Training Loss: 0.9310032866281621\n",
      "Epoch 26, Training Loss: 0.9298807710058549\n",
      "Epoch 27, Training Loss: 0.9287965470201829\n",
      "Epoch 28, Training Loss: 0.9277516947774326\n",
      "Epoch 29, Training Loss: 0.9267459613435408\n",
      "Epoch 30, Training Loss: 0.9257754998347338\n",
      "Epoch 31, Training Loss: 0.9248482866146985\n",
      "Epoch 32, Training Loss: 0.9239467007272384\n",
      "Epoch 33, Training Loss: 0.9230654828688678\n",
      "Epoch 34, Training Loss: 0.9222040149744819\n",
      "Epoch 35, Training Loss: 0.9213554456654717\n",
      "Epoch 36, Training Loss: 0.920536134593627\n",
      "Epoch 37, Training Loss: 0.9197333489446079\n",
      "Epoch 38, Training Loss: 0.9189454202792223\n",
      "Epoch 39, Training Loss: 0.918174385393367\n",
      "Epoch 40, Training Loss: 0.9174096998747657\n",
      "Epoch 41, Training Loss: 0.9166526416470023\n",
      "Epoch 42, Training Loss: 0.9158986610524794\n",
      "Epoch 43, Training Loss: 0.9151574974901536\n",
      "Epoch 44, Training Loss: 0.9144202648190891\n",
      "Epoch 45, Training Loss: 0.9136885770629434\n",
      "Epoch 46, Training Loss: 0.9129672114288105\n",
      "Epoch 47, Training Loss: 0.9122508154897129\n",
      "Epoch 48, Training Loss: 0.9115385643173667\n",
      "Epoch 49, Training Loss: 0.9108274073460523\n",
      "Epoch 50, Training Loss: 0.9101219444415148\n",
      "Epoch 51, Training Loss: 0.9094189156504239\n",
      "Epoch 52, Training Loss: 0.9087249033591327\n",
      "Epoch 53, Training Loss: 0.9080298177634969\n",
      "Epoch 54, Training Loss: 0.9073391285363366\n",
      "Epoch 55, Training Loss: 0.9066423036771663\n",
      "Epoch 56, Training Loss: 0.905945728596519\n",
      "Epoch 57, Training Loss: 0.9052536500201506\n",
      "Epoch 58, Training Loss: 0.9045591301076552\n",
      "Epoch 59, Training Loss: 0.903866452960407\n",
      "Epoch 60, Training Loss: 0.9031632721424103\n",
      "Epoch 61, Training Loss: 0.9024701062370749\n",
      "Epoch 62, Training Loss: 0.9017664393957924\n",
      "Epoch 63, Training Loss: 0.9010666703476625\n",
      "Epoch 64, Training Loss: 0.9003552778328167\n",
      "Epoch 65, Training Loss: 0.8996399446094737\n",
      "Epoch 66, Training Loss: 0.8989315360433915\n",
      "Epoch 67, Training Loss: 0.8982105719341952\n",
      "Epoch 68, Training Loss: 0.8974920844330507\n",
      "Epoch 69, Training Loss: 0.896768032873378\n",
      "Epoch 70, Training Loss: 0.8960354570080252\n",
      "Epoch 71, Training Loss: 0.8953093531552483\n",
      "Epoch 72, Training Loss: 0.8945752688015208\n",
      "Epoch 73, Training Loss: 0.8938408334816204\n",
      "Epoch 74, Training Loss: 0.8930926921087153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:02:03,411] Trial 223 finished with value: 0.5801333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8923570813150967\n",
      "Epoch 1, Training Loss: 0.9686263034218237\n",
      "Epoch 2, Training Loss: 0.9389073391606036\n",
      "Epoch 3, Training Loss: 0.9241635978670049\n",
      "Epoch 4, Training Loss: 0.9087273332409392\n",
      "Epoch 5, Training Loss: 0.8934552173865469\n",
      "Epoch 6, Training Loss: 0.878953616780446\n",
      "Epoch 7, Training Loss: 0.8641640915010208\n",
      "Epoch 8, Training Loss: 0.8514721648585527\n",
      "Epoch 9, Training Loss: 0.8421692493266629\n",
      "Epoch 10, Training Loss: 0.8335407268732113\n",
      "Epoch 11, Training Loss: 0.8274568248512153\n",
      "Epoch 12, Training Loss: 0.8231707414290063\n",
      "Epoch 13, Training Loss: 0.8194448852897587\n",
      "Epoch 14, Training Loss: 0.8171105814159365\n",
      "Epoch 15, Training Loss: 0.814971857949307\n",
      "Epoch 16, Training Loss: 0.8140264243111575\n",
      "Epoch 17, Training Loss: 0.8118191286585384\n",
      "Epoch 18, Training Loss: 0.8115029180856576\n",
      "Epoch 19, Training Loss: 0.8106355374917051\n",
      "Epoch 20, Training Loss: 0.8100345431413866\n",
      "Epoch 21, Training Loss: 0.8088045038675007\n",
      "Epoch 22, Training Loss: 0.8083035212710388\n",
      "Epoch 23, Training Loss: 0.8082054256496572\n",
      "Epoch 24, Training Loss: 0.8068458238042386\n",
      "Epoch 25, Training Loss: 0.8079396219181835\n",
      "Epoch 26, Training Loss: 0.8061593852545086\n",
      "Epoch 27, Training Loss: 0.8063510210890519\n",
      "Epoch 28, Training Loss: 0.8058712336353789\n",
      "Epoch 29, Training Loss: 0.8050734220590806\n",
      "Epoch 30, Training Loss: 0.8051956622224105\n",
      "Epoch 31, Training Loss: 0.804695068893576\n",
      "Epoch 32, Training Loss: 0.8048388468591791\n",
      "Epoch 33, Training Loss: 0.8041710142802475\n",
      "Epoch 34, Training Loss: 0.803789220268565\n",
      "Epoch 35, Training Loss: 0.8038667720063288\n",
      "Epoch 36, Training Loss: 0.8044718788082438\n",
      "Epoch 37, Training Loss: 0.8032951525279454\n",
      "Epoch 38, Training Loss: 0.8030596210544271\n",
      "Epoch 39, Training Loss: 0.8029493134720881\n",
      "Epoch 40, Training Loss: 0.8025825902035362\n",
      "Epoch 41, Training Loss: 0.8027348447563056\n",
      "Epoch 42, Training Loss: 0.8027987362746906\n",
      "Epoch 43, Training Loss: 0.8017851622481095\n",
      "Epoch 44, Training Loss: 0.8015650190805134\n",
      "Epoch 45, Training Loss: 0.8017217240835491\n",
      "Epoch 46, Training Loss: 0.8012949376177967\n",
      "Epoch 47, Training Loss: 0.8010659464319846\n",
      "Epoch 48, Training Loss: 0.8013418896753985\n",
      "Epoch 49, Training Loss: 0.8009027769691066\n",
      "Epoch 50, Training Loss: 0.8017459328013256\n",
      "Epoch 51, Training Loss: 0.8009180613030168\n",
      "Epoch 52, Training Loss: 0.801096082719645\n",
      "Epoch 53, Training Loss: 0.8003768501425148\n",
      "Epoch 54, Training Loss: 0.8006609623593496\n",
      "Epoch 55, Training Loss: 0.8006516241489496\n",
      "Epoch 56, Training Loss: 0.7999497078415146\n",
      "Epoch 57, Training Loss: 0.7997340654520164\n",
      "Epoch 58, Training Loss: 0.7994758998989162\n",
      "Epoch 59, Training Loss: 0.799579878409106\n",
      "Epoch 60, Training Loss: 0.799719922793539\n",
      "Epoch 61, Training Loss: 0.7998499698208686\n",
      "Epoch 62, Training Loss: 0.800534164726286\n",
      "Epoch 63, Training Loss: 0.7996745605217783\n",
      "Epoch 64, Training Loss: 0.7992789889636792\n",
      "Epoch 65, Training Loss: 0.7992995047927799\n",
      "Epoch 66, Training Loss: 0.799696756036658\n",
      "Epoch 67, Training Loss: 0.8001305766571734\n",
      "Epoch 68, Training Loss: 0.7993813762091156\n",
      "Epoch 69, Training Loss: 0.798819635416332\n",
      "Epoch 70, Training Loss: 0.7991267278678436\n",
      "Epoch 71, Training Loss: 0.7987351219456895\n",
      "Epoch 72, Training Loss: 0.798537061985274\n",
      "Epoch 73, Training Loss: 0.7991299169404166\n",
      "Epoch 74, Training Loss: 0.7988924583098046\n",
      "Epoch 75, Training Loss: 0.7990000478307108\n",
      "Epoch 76, Training Loss: 0.7989819056109378\n",
      "Epoch 77, Training Loss: 0.7989839829000315\n",
      "Epoch 78, Training Loss: 0.7983887268188304\n",
      "Epoch 79, Training Loss: 0.7983774502474562\n",
      "Epoch 80, Training Loss: 0.7985470133616512\n",
      "Epoch 81, Training Loss: 0.7991453494344439\n",
      "Epoch 82, Training Loss: 0.7984664859628319\n",
      "Epoch 83, Training Loss: 0.7979484767842113\n",
      "Epoch 84, Training Loss: 0.7982605122085801\n",
      "Epoch 85, Training Loss: 0.7983452163244549\n",
      "Epoch 86, Training Loss: 0.7981769775089465\n",
      "Epoch 87, Training Loss: 0.7983789273670742\n",
      "Epoch 88, Training Loss: 0.7994131655621349\n",
      "Epoch 89, Training Loss: 0.7984614822201263\n",
      "Epoch 90, Training Loss: 0.7981722228509143\n",
      "Epoch 91, Training Loss: 0.7977879834354372\n",
      "Epoch 92, Training Loss: 0.7977778346018684\n",
      "Epoch 93, Training Loss: 0.7982005856090919\n",
      "Epoch 94, Training Loss: 0.7971249812527706\n",
      "Epoch 95, Training Loss: 0.7977121768141151\n",
      "Epoch 96, Training Loss: 0.7974943903155793\n",
      "Epoch 97, Training Loss: 0.7976087843565116\n",
      "Epoch 98, Training Loss: 0.7976405119537411\n",
      "Epoch 99, Training Loss: 0.7970808716196763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:03:27,079] Trial 224 finished with value: 0.6334666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7981836624611589\n",
      "Epoch 1, Training Loss: 0.8703377380791832\n",
      "Epoch 2, Training Loss: 0.8224215420554666\n",
      "Epoch 3, Training Loss: 0.8162592545677634\n",
      "Epoch 4, Training Loss: 0.8112215293154997\n",
      "Epoch 5, Training Loss: 0.807071866498274\n",
      "Epoch 6, Training Loss: 0.8027605044140536\n",
      "Epoch 7, Training Loss: 0.7983362800233504\n",
      "Epoch 8, Training Loss: 0.7958691772993873\n",
      "Epoch 9, Training Loss: 0.7931152911045972\n",
      "Epoch 10, Training Loss: 0.7920707617787753\n",
      "Epoch 11, Training Loss: 0.7915288043723386\n",
      "Epoch 12, Training Loss: 0.7904003526182736\n",
      "Epoch 13, Training Loss: 0.7898813909642837\n",
      "Epoch 14, Training Loss: 0.7894153775187099\n",
      "Epoch 15, Training Loss: 0.7890761806684382\n",
      "Epoch 16, Training Loss: 0.789111980901045\n",
      "Epoch 17, Training Loss: 0.788340038061142\n",
      "Epoch 18, Training Loss: 0.7882936756751117\n",
      "Epoch 19, Training Loss: 0.7878671659441555\n",
      "Epoch 20, Training Loss: 0.7877694040887496\n",
      "Epoch 21, Training Loss: 0.7877876051734476\n",
      "Epoch 22, Training Loss: 0.7876225153137656\n",
      "Epoch 23, Training Loss: 0.7873353456048404\n",
      "Epoch 24, Training Loss: 0.787016268828336\n",
      "Epoch 25, Training Loss: 0.7868002963066101\n",
      "Epoch 26, Training Loss: 0.7862789107070249\n",
      "Epoch 27, Training Loss: 0.786419465822332\n",
      "Epoch 28, Training Loss: 0.7864838128931382\n",
      "Epoch 29, Training Loss: 0.7861137350867776\n",
      "Epoch 30, Training Loss: 0.7857221250674303\n",
      "Epoch 31, Training Loss: 0.7855536404777975\n",
      "Epoch 32, Training Loss: 0.7853077462841482\n",
      "Epoch 33, Training Loss: 0.7853206946569331\n",
      "Epoch 34, Training Loss: 0.7851967226757722\n",
      "Epoch 35, Training Loss: 0.7848078097315395\n",
      "Epoch 36, Training Loss: 0.7850753327678232\n",
      "Epoch 37, Training Loss: 0.7848576009273529\n",
      "Epoch 38, Training Loss: 0.7845858218389399\n",
      "Epoch 39, Training Loss: 0.7846405335033642\n",
      "Epoch 40, Training Loss: 0.7842723191485685\n",
      "Epoch 41, Training Loss: 0.7844323852482964\n",
      "Epoch 42, Training Loss: 0.7843128958870383\n",
      "Epoch 43, Training Loss: 0.7841253202101763\n",
      "Epoch 44, Training Loss: 0.78376131141887\n",
      "Epoch 45, Training Loss: 0.7838287129121668\n",
      "Epoch 46, Training Loss: 0.7837500291010913\n",
      "Epoch 47, Training Loss: 0.7842135741430171\n",
      "Epoch 48, Training Loss: 0.7832940719408148\n",
      "Epoch 49, Training Loss: 0.78340366805301\n",
      "Epoch 50, Training Loss: 0.7836077781284556\n",
      "Epoch 51, Training Loss: 0.7833133743791019\n",
      "Epoch 52, Training Loss: 0.7828613284054925\n",
      "Epoch 53, Training Loss: 0.7829500668890336\n",
      "Epoch 54, Training Loss: 0.7833497083888334\n",
      "Epoch 55, Training Loss: 0.7831969039580401\n",
      "Epoch 56, Training Loss: 0.7826544745529399\n",
      "Epoch 57, Training Loss: 0.7827605032920837\n",
      "Epoch 58, Training Loss: 0.7826786164676441\n",
      "Epoch 59, Training Loss: 0.7827782609182246\n",
      "Epoch 60, Training Loss: 0.7824354119861827\n",
      "Epoch 61, Training Loss: 0.7823141630256877\n",
      "Epoch 62, Training Loss: 0.7824255642470191\n",
      "Epoch 63, Training Loss: 0.7825959128492018\n",
      "Epoch 64, Training Loss: 0.7819838370295132\n",
      "Epoch 65, Training Loss: 0.7821298041063196\n",
      "Epoch 66, Training Loss: 0.7821819006695467\n",
      "Epoch 67, Training Loss: 0.7820862733616548\n",
      "Epoch 68, Training Loss: 0.7814762440849753\n",
      "Epoch 69, Training Loss: 0.7818677553709815\n",
      "Epoch 70, Training Loss: 0.781870178825715\n",
      "Epoch 71, Training Loss: 0.7820063722834868\n",
      "Epoch 72, Training Loss: 0.7818048568332896\n",
      "Epoch 73, Training Loss: 0.7816322788771461\n",
      "Epoch 74, Training Loss: 0.7814740173255696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:05:12,868] Trial 225 finished with value: 0.6400666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7815026171067182\n",
      "Epoch 1, Training Loss: 0.845772375737814\n",
      "Epoch 2, Training Loss: 0.8116525003784582\n",
      "Epoch 3, Training Loss: 0.8067065936282165\n",
      "Epoch 4, Training Loss: 0.8019933076729452\n",
      "Epoch 5, Training Loss: 0.7987267905608155\n",
      "Epoch 6, Training Loss: 0.8004455900730048\n",
      "Epoch 7, Training Loss: 0.7986986162070941\n",
      "Epoch 8, Training Loss: 0.7972515808908562\n",
      "Epoch 9, Training Loss: 0.7954511314406431\n",
      "Epoch 10, Training Loss: 0.7946741277113893\n",
      "Epoch 11, Training Loss: 0.7948974601308206\n",
      "Epoch 12, Training Loss: 0.7958371093398646\n",
      "Epoch 13, Training Loss: 0.7936112688000041\n",
      "Epoch 14, Training Loss: 0.7955240440547915\n",
      "Epoch 15, Training Loss: 0.7929560634874759\n",
      "Epoch 16, Training Loss: 0.7927093461940163\n",
      "Epoch 17, Training Loss: 0.7943707939377405\n",
      "Epoch 18, Training Loss: 0.7930772035641778\n",
      "Epoch 19, Training Loss: 0.7912129969525158\n",
      "Epoch 20, Training Loss: 0.791706991643834\n",
      "Epoch 21, Training Loss: 0.7921825007388467\n",
      "Epoch 22, Training Loss: 0.7913981042410198\n",
      "Epoch 23, Training Loss: 0.7917719754957615\n",
      "Epoch 24, Training Loss: 0.7895623628358196\n",
      "Epoch 25, Training Loss: 0.7915361600710933\n",
      "Epoch 26, Training Loss: 0.7921429146501354\n",
      "Epoch 27, Training Loss: 0.7907112541951631\n",
      "Epoch 28, Training Loss: 0.789990136318637\n",
      "Epoch 29, Training Loss: 0.7906932987664875\n",
      "Epoch 30, Training Loss: 0.789878069009996\n",
      "Epoch 31, Training Loss: 0.7897997037808698\n",
      "Epoch 32, Training Loss: 0.7901470729282924\n",
      "Epoch 33, Training Loss: 0.7896050039090609\n",
      "Epoch 34, Training Loss: 0.7898101051050918\n",
      "Epoch 35, Training Loss: 0.789464074866216\n",
      "Epoch 36, Training Loss: 0.7893480256087798\n",
      "Epoch 37, Training Loss: 0.7895047591144877\n",
      "Epoch 38, Training Loss: 0.7894520715663308\n",
      "Epoch 39, Training Loss: 0.7883454140415765\n",
      "Epoch 40, Training Loss: 0.7892779180878087\n",
      "Epoch 41, Training Loss: 0.7895275232487156\n",
      "Epoch 42, Training Loss: 0.7888042891832222\n",
      "Epoch 43, Training Loss: 0.7887759907801348\n",
      "Epoch 44, Training Loss: 0.7889503720111417\n",
      "Epoch 45, Training Loss: 0.7883640111837172\n",
      "Epoch 46, Training Loss: 0.789574082274186\n",
      "Epoch 47, Training Loss: 0.78966270434229\n",
      "Epoch 48, Training Loss: 0.7893935215204282\n",
      "Epoch 49, Training Loss: 0.7885418914314499\n",
      "Epoch 50, Training Loss: 0.788479577060929\n",
      "Epoch 51, Training Loss: 0.7883633439702199\n",
      "Epoch 52, Training Loss: 0.787731549076568\n",
      "Epoch 53, Training Loss: 0.7876508039639408\n",
      "Epoch 54, Training Loss: 0.7874151138434733\n",
      "Epoch 55, Training Loss: 0.7893721290997097\n",
      "Epoch 56, Training Loss: 0.7892147009534047\n",
      "Epoch 57, Training Loss: 0.7880181346620833\n",
      "Epoch 58, Training Loss: 0.7872479352735935\n",
      "Epoch 59, Training Loss: 0.7890064416075111\n",
      "Epoch 60, Training Loss: 0.7879031450228584\n",
      "Epoch 61, Training Loss: 0.7890326446160338\n",
      "Epoch 62, Training Loss: 0.7883578860670104\n",
      "Epoch 63, Training Loss: 0.7890091519606741\n",
      "Epoch 64, Training Loss: 0.7867626792506168\n",
      "Epoch 65, Training Loss: 0.787860988315783\n",
      "Epoch 66, Training Loss: 0.7892223650351503\n",
      "Epoch 67, Training Loss: 0.7886909496515317\n",
      "Epoch 68, Training Loss: 0.787852732758773\n",
      "Epoch 69, Training Loss: 0.7882773601022878\n",
      "Epoch 70, Training Loss: 0.7881245527948652\n",
      "Epoch 71, Training Loss: 0.7877682716326606\n",
      "Epoch 72, Training Loss: 0.7870453474216892\n",
      "Epoch 73, Training Loss: 0.7866342860505097\n",
      "Epoch 74, Training Loss: 0.7870622507611612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:06:54,857] Trial 226 finished with value: 0.6336 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7885242163686824\n",
      "Epoch 1, Training Loss: 1.0888497707539035\n",
      "Epoch 2, Training Loss: 1.0826725002518274\n",
      "Epoch 3, Training Loss: 1.0748591279624997\n",
      "Epoch 4, Training Loss: 1.0638083737595636\n",
      "Epoch 5, Training Loss: 1.0479683607144463\n",
      "Epoch 6, Training Loss: 1.0276786002001368\n",
      "Epoch 7, Training Loss: 1.0069332490290017\n",
      "Epoch 8, Training Loss: 0.9909744755666059\n",
      "Epoch 9, Training Loss: 0.9808142502505081\n",
      "Epoch 10, Training Loss: 0.9741642271665703\n",
      "Epoch 11, Training Loss: 0.9692939171217438\n",
      "Epoch 12, Training Loss: 0.9654460543976691\n",
      "Epoch 13, Training Loss: 0.9631973633192535\n",
      "Epoch 14, Training Loss: 0.9608094277238487\n",
      "Epoch 15, Training Loss: 0.9592567513759871\n",
      "Epoch 16, Training Loss: 0.9583196320928129\n",
      "Epoch 17, Training Loss: 0.9566314138864216\n",
      "Epoch 18, Training Loss: 0.9555297726079037\n",
      "Epoch 19, Training Loss: 0.9541579721565533\n",
      "Epoch 20, Training Loss: 0.9531103655807954\n",
      "Epoch 21, Training Loss: 0.9520350285042497\n",
      "Epoch 22, Training Loss: 0.9504131432762719\n",
      "Epoch 23, Training Loss: 0.9491422253443782\n",
      "Epoch 24, Training Loss: 0.9476658718030255\n",
      "Epoch 25, Training Loss: 0.9465366728323743\n",
      "Epoch 26, Training Loss: 0.9444531913090469\n",
      "Epoch 27, Training Loss: 0.9429123975280532\n",
      "Epoch 28, Training Loss: 0.9411835218730725\n",
      "Epoch 29, Training Loss: 0.939308650511548\n",
      "Epoch 30, Training Loss: 0.9371807021305973\n",
      "Epoch 31, Training Loss: 0.9357546250622971\n",
      "Epoch 32, Training Loss: 0.932837594720654\n",
      "Epoch 33, Training Loss: 0.9302554516864002\n",
      "Epoch 34, Training Loss: 0.9281116126175214\n",
      "Epoch 35, Training Loss: 0.9248774711350749\n",
      "Epoch 36, Training Loss: 0.9220268982693665\n",
      "Epoch 37, Training Loss: 0.9190445190981815\n",
      "Epoch 38, Training Loss: 0.9152986089986069\n",
      "Epoch 39, Training Loss: 0.9115707691450764\n",
      "Epoch 40, Training Loss: 0.9073940481458391\n",
      "Epoch 41, Training Loss: 0.9028206844975177\n",
      "Epoch 42, Training Loss: 0.8978863665932103\n",
      "Epoch 43, Training Loss: 0.8930062420386121\n",
      "Epoch 44, Training Loss: 0.8875673750289401\n",
      "Epoch 45, Training Loss: 0.8827500833604569\n",
      "Epoch 46, Training Loss: 0.8774247965418307\n",
      "Epoch 47, Training Loss: 0.8717338382749629\n",
      "Epoch 48, Training Loss: 0.8664556974755194\n",
      "Epoch 49, Training Loss: 0.8607696506313811\n",
      "Epoch 50, Training Loss: 0.8556868083494946\n",
      "Epoch 51, Training Loss: 0.8510794156476071\n",
      "Epoch 52, Training Loss: 0.846597943628641\n",
      "Epoch 53, Training Loss: 0.8422741098511488\n",
      "Epoch 54, Training Loss: 0.8387440945869101\n",
      "Epoch 55, Training Loss: 0.8349369555487669\n",
      "Epoch 56, Training Loss: 0.8327696670266919\n",
      "Epoch 57, Training Loss: 0.8293760778312397\n",
      "Epoch 58, Training Loss: 0.8275853835550466\n",
      "Epoch 59, Training Loss: 0.8250075166386769\n",
      "Epoch 60, Training Loss: 0.8235863282268209\n",
      "Epoch 61, Training Loss: 0.8221979372483447\n",
      "Epoch 62, Training Loss: 0.8208152425916572\n",
      "Epoch 63, Training Loss: 0.8196736004119529\n",
      "Epoch 64, Training Loss: 0.8184258885849688\n",
      "Epoch 65, Training Loss: 0.8177361586936435\n",
      "Epoch 66, Training Loss: 0.8177527295019393\n",
      "Epoch 67, Training Loss: 0.816457646502588\n",
      "Epoch 68, Training Loss: 0.8159539578552533\n",
      "Epoch 69, Training Loss: 0.815131359889095\n",
      "Epoch 70, Training Loss: 0.8147103881477413\n",
      "Epoch 71, Training Loss: 0.8141606422295248\n",
      "Epoch 72, Training Loss: 0.8133352089645272\n",
      "Epoch 73, Training Loss: 0.8135886709492905\n",
      "Epoch 74, Training Loss: 0.8128302017548927\n",
      "Epoch 75, Training Loss: 0.8127885419623296\n",
      "Epoch 76, Training Loss: 0.8125464183943613\n",
      "Epoch 77, Training Loss: 0.8116730312655743\n",
      "Epoch 78, Training Loss: 0.812220137101367\n",
      "Epoch 79, Training Loss: 0.8112962911003514\n",
      "Epoch 80, Training Loss: 0.8113378730931676\n",
      "Epoch 81, Training Loss: 0.8109795868844915\n",
      "Epoch 82, Training Loss: 0.8113025927902164\n",
      "Epoch 83, Training Loss: 0.8103788315801692\n",
      "Epoch 84, Training Loss: 0.8106775488172259\n",
      "Epoch 85, Training Loss: 0.8109155729300994\n",
      "Epoch 86, Training Loss: 0.8104405908656299\n",
      "Epoch 87, Training Loss: 0.8100370651797244\n",
      "Epoch 88, Training Loss: 0.8103418009621757\n",
      "Epoch 89, Training Loss: 0.8099844599128666\n",
      "Epoch 90, Training Loss: 0.8100604751056298\n",
      "Epoch 91, Training Loss: 0.8092274992089522\n",
      "Epoch 92, Training Loss: 0.8088271385744998\n",
      "Epoch 93, Training Loss: 0.8087291977459327\n",
      "Epoch 94, Training Loss: 0.8086146282970457\n",
      "Epoch 95, Training Loss: 0.8086782978889637\n",
      "Epoch 96, Training Loss: 0.8087799621703929\n",
      "Epoch 97, Training Loss: 0.8095775928712429\n",
      "Epoch 98, Training Loss: 0.8085004404075163\n",
      "Epoch 99, Training Loss: 0.808282626930036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:08:28,932] Trial 227 finished with value: 0.6284666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8085598673139299\n",
      "Epoch 1, Training Loss: 0.9735283272406634\n",
      "Epoch 2, Training Loss: 0.9424597515779383\n",
      "Epoch 3, Training Loss: 0.9225812960372252\n",
      "Epoch 4, Training Loss: 0.8991023645681493\n",
      "Epoch 5, Training Loss: 0.8746801146338967\n",
      "Epoch 6, Training Loss: 0.8536841377090005\n",
      "Epoch 7, Training Loss: 0.8381872333498562\n",
      "Epoch 8, Training Loss: 0.8277865771686329\n",
      "Epoch 9, Training Loss: 0.8214297556175905\n",
      "Epoch 10, Training Loss: 0.8174185748661266\n",
      "Epoch 11, Training Loss: 0.814946182124755\n",
      "Epoch 12, Training Loss: 0.8131696672299329\n",
      "Epoch 13, Training Loss: 0.8119932754600749\n",
      "Epoch 14, Training Loss: 0.8112902218454024\n",
      "Epoch 15, Training Loss: 0.8102274888403276\n",
      "Epoch 16, Training Loss: 0.8097106280747582\n",
      "Epoch 17, Training Loss: 0.8093165272824905\n",
      "Epoch 18, Training Loss: 0.8090422422745649\n",
      "Epoch 19, Training Loss: 0.8084109013922074\n",
      "Epoch 20, Training Loss: 0.8082141480726355\n",
      "Epoch 21, Training Loss: 0.807603598762961\n",
      "Epoch 22, Training Loss: 0.8075139511332793\n",
      "Epoch 23, Training Loss: 0.807084670978434\n",
      "Epoch 24, Training Loss: 0.8070089669087354\n",
      "Epoch 25, Training Loss: 0.806814407601076\n",
      "Epoch 26, Training Loss: 0.8066113460063934\n",
      "Epoch 27, Training Loss: 0.806520998618182\n",
      "Epoch 28, Training Loss: 0.8063314390883726\n",
      "Epoch 29, Training Loss: 0.8058118376311134\n",
      "Epoch 30, Training Loss: 0.8059198175458347\n",
      "Epoch 31, Training Loss: 0.8055936033585492\n",
      "Epoch 32, Training Loss: 0.805583695453756\n",
      "Epoch 33, Training Loss: 0.8054549471771015\n",
      "Epoch 34, Training Loss: 0.804907603894963\n",
      "Epoch 35, Training Loss: 0.8048512946858125\n",
      "Epoch 36, Training Loss: 0.8045522810431087\n",
      "Epoch 37, Training Loss: 0.8045538837068221\n",
      "Epoch 38, Training Loss: 0.8044030974893009\n",
      "Epoch 39, Training Loss: 0.8041463028683382\n",
      "Epoch 40, Training Loss: 0.8041829392489265\n",
      "Epoch 41, Training Loss: 0.804020574794096\n",
      "Epoch 42, Training Loss: 0.8037467124181635\n",
      "Epoch 43, Training Loss: 0.803556486788918\n",
      "Epoch 44, Training Loss: 0.8034134121502147\n",
      "Epoch 45, Training Loss: 0.8032227758099051\n",
      "Epoch 46, Training Loss: 0.8031778714236091\n",
      "Epoch 47, Training Loss: 0.8030137160946341\n",
      "Epoch 48, Training Loss: 0.8028485548496246\n",
      "Epoch 49, Training Loss: 0.8024935495853424\n",
      "Epoch 50, Training Loss: 0.8025343660046073\n",
      "Epoch 51, Training Loss: 0.8024156487689299\n",
      "Epoch 52, Training Loss: 0.8023923638287712\n",
      "Epoch 53, Training Loss: 0.8022906481518465\n",
      "Epoch 54, Training Loss: 0.8020852654821733\n",
      "Epoch 55, Training Loss: 0.8019192248933456\n",
      "Epoch 56, Training Loss: 0.8017570679328021\n",
      "Epoch 57, Training Loss: 0.8018273225251367\n",
      "Epoch 58, Training Loss: 0.8015341898974251\n",
      "Epoch 59, Training Loss: 0.8014968289347256\n",
      "Epoch 60, Training Loss: 0.8012951909794527\n",
      "Epoch 61, Training Loss: 0.8012273234479568\n",
      "Epoch 62, Training Loss: 0.801282559633255\n",
      "Epoch 63, Training Loss: 0.8011640934383168\n",
      "Epoch 64, Training Loss: 0.800846656070036\n",
      "Epoch 65, Training Loss: 0.8007896595141467\n",
      "Epoch 66, Training Loss: 0.8006949843378628\n",
      "Epoch 67, Training Loss: 0.8004886212068446\n",
      "Epoch 68, Training Loss: 0.8003472575720618\n",
      "Epoch 69, Training Loss: 0.8005437947020811\n",
      "Epoch 70, Training Loss: 0.8002318649432238\n",
      "Epoch 71, Training Loss: 0.800261050602969\n",
      "Epoch 72, Training Loss: 0.8003009345250971\n",
      "Epoch 73, Training Loss: 0.8001044492160573\n",
      "Epoch 74, Training Loss: 0.7999117137404049\n",
      "Epoch 75, Training Loss: 0.7997729774082408\n",
      "Epoch 76, Training Loss: 0.7999298806751476\n",
      "Epoch 77, Training Loss: 0.7995208632946015\n",
      "Epoch 78, Training Loss: 0.7998671779913061\n",
      "Epoch 79, Training Loss: 0.7998296162661385\n",
      "Epoch 80, Training Loss: 0.7995594442591948\n",
      "Epoch 81, Training Loss: 0.7995530143906089\n",
      "Epoch 82, Training Loss: 0.7993952250480652\n",
      "Epoch 83, Training Loss: 0.7992965761352988\n",
      "Epoch 84, Training Loss: 0.7991551562617807\n",
      "Epoch 85, Training Loss: 0.7990439013172599\n",
      "Epoch 86, Training Loss: 0.7991539892729591\n",
      "Epoch 87, Training Loss: 0.7991688027101405\n",
      "Epoch 88, Training Loss: 0.7990911562302533\n",
      "Epoch 89, Training Loss: 0.7987437664060032\n",
      "Epoch 90, Training Loss: 0.7989746841262368\n",
      "Epoch 91, Training Loss: 0.7989291275248808\n",
      "Epoch 92, Training Loss: 0.7988578433149001\n",
      "Epoch 93, Training Loss: 0.7987593515480266\n",
      "Epoch 94, Training Loss: 0.7987786695536445\n",
      "Epoch 95, Training Loss: 0.7985960439373465\n",
      "Epoch 96, Training Loss: 0.7987779280718635\n",
      "Epoch 97, Training Loss: 0.7986099487192491\n",
      "Epoch 98, Training Loss: 0.7986112841437845\n",
      "Epoch 99, Training Loss: 0.7986004711599911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:10:20,495] Trial 228 finished with value: 0.6343333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.798268915344687\n",
      "Epoch 1, Training Loss: 0.8587143980755525\n",
      "Epoch 2, Training Loss: 0.8156056629910189\n",
      "Epoch 3, Training Loss: 0.8075514165794149\n",
      "Epoch 4, Training Loss: 0.8022205659922431\n",
      "Epoch 5, Training Loss: 0.796757999237846\n",
      "Epoch 6, Training Loss: 0.7939994037852568\n",
      "Epoch 7, Training Loss: 0.7917945949470295\n",
      "Epoch 8, Training Loss: 0.7917399316675523\n",
      "Epoch 9, Training Loss: 0.7911432708010954\n",
      "Epoch 10, Training Loss: 0.7891783369288725\n",
      "Epoch 11, Training Loss: 0.788551031701705\n",
      "Epoch 12, Training Loss: 0.7911100757823271\n",
      "Epoch 13, Training Loss: 0.7884977401004118\n",
      "Epoch 14, Training Loss: 0.7879792097736807\n",
      "Epoch 15, Training Loss: 0.7885618162856383\n",
      "Epoch 16, Training Loss: 0.7869893646941466\n",
      "Epoch 17, Training Loss: 0.787203568500631\n",
      "Epoch 18, Training Loss: 0.7867137056238511\n",
      "Epoch 19, Training Loss: 0.7865480034491595\n",
      "Epoch 20, Training Loss: 0.7861475224354688\n",
      "Epoch 21, Training Loss: 0.78528052463251\n",
      "Epoch 22, Training Loss: 0.7860154485001284\n",
      "Epoch 23, Training Loss: 0.7859825673524071\n",
      "Epoch 24, Training Loss: 0.7851590732265922\n",
      "Epoch 25, Training Loss: 0.7850293575314915\n",
      "Epoch 26, Training Loss: 0.7855744335230659\n",
      "Epoch 27, Training Loss: 0.7851396717043484\n",
      "Epoch 28, Training Loss: 0.7843469123980579\n",
      "Epoch 29, Training Loss: 0.7845854574792526\n",
      "Epoch 30, Training Loss: 0.7839711650680093\n",
      "Epoch 31, Training Loss: 0.7844504799562342\n",
      "Epoch 32, Training Loss: 0.7839764996135936\n",
      "Epoch 33, Training Loss: 0.7838225152913262\n",
      "Epoch 34, Training Loss: 0.7828829727453344\n",
      "Epoch 35, Training Loss: 0.783081663496354\n",
      "Epoch 36, Training Loss: 0.7832703921374152\n",
      "Epoch 37, Training Loss: 0.7829379823628594\n",
      "Epoch 38, Training Loss: 0.7826167423584882\n",
      "Epoch 39, Training Loss: 0.7822143489473006\n",
      "Epoch 40, Training Loss: 0.7826156454927781\n",
      "Epoch 41, Training Loss: 0.7829715570982765\n",
      "Epoch 42, Training Loss: 0.7821973854653975\n",
      "Epoch 43, Training Loss: 0.7826352222526775\n",
      "Epoch 44, Training Loss: 0.7821421759970048\n",
      "Epoch 45, Training Loss: 0.7824247646331787\n",
      "Epoch 46, Training Loss: 0.781907862775466\n",
      "Epoch 47, Training Loss: 0.7811184090025285\n",
      "Epoch 48, Training Loss: 0.7820509695305544\n",
      "Epoch 49, Training Loss: 0.7812536538348478\n",
      "Epoch 50, Training Loss: 0.7811288454953362\n",
      "Epoch 51, Training Loss: 0.7816187270949868\n",
      "Epoch 52, Training Loss: 0.7815568928858813\n",
      "Epoch 53, Training Loss: 0.7810930499609778\n",
      "Epoch 54, Training Loss: 0.7809863170455483\n",
      "Epoch 55, Training Loss: 0.7801960962660173\n",
      "Epoch 56, Training Loss: 0.7816882844532237\n",
      "Epoch 57, Training Loss: 0.7812190264112809\n",
      "Epoch 58, Training Loss: 0.7811091724564048\n",
      "Epoch 59, Training Loss: 0.7807970079253702\n",
      "Epoch 60, Training Loss: 0.7803741355503306\n",
      "Epoch 61, Training Loss: 0.7807933385933147\n",
      "Epoch 62, Training Loss: 0.7807678288571975\n",
      "Epoch 63, Training Loss: 0.7801331067085266\n",
      "Epoch 64, Training Loss: 0.7797686599983888\n",
      "Epoch 65, Training Loss: 0.7796369589777554\n",
      "Epoch 66, Training Loss: 0.7796186649098116\n",
      "Epoch 67, Training Loss: 0.7795835720791536\n",
      "Epoch 68, Training Loss: 0.779344908840516\n",
      "Epoch 69, Training Loss: 0.7796993195309359\n",
      "Epoch 70, Training Loss: 0.7794900698521557\n",
      "Epoch 71, Training Loss: 0.7792392992973327\n",
      "Epoch 72, Training Loss: 0.7790244924320894\n",
      "Epoch 73, Training Loss: 0.778626339786193\n",
      "Epoch 74, Training Loss: 0.7787740267725551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:12:16,498] Trial 229 finished with value: 0.6388 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7788928066281712\n",
      "Epoch 1, Training Loss: 0.8785804370571585\n",
      "Epoch 2, Training Loss: 0.8344208386365105\n",
      "Epoch 3, Training Loss: 0.8251535532053779\n",
      "Epoch 4, Training Loss: 0.8241437740185682\n",
      "Epoch 5, Training Loss: 0.8223749839558321\n",
      "Epoch 6, Training Loss: 0.8199411419559928\n",
      "Epoch 7, Training Loss: 0.8183183576780207\n",
      "Epoch 8, Training Loss: 0.8176314872853896\n",
      "Epoch 9, Training Loss: 0.8167806513870464\n",
      "Epoch 10, Training Loss: 0.8172991791893455\n",
      "Epoch 11, Training Loss: 0.8161353454870336\n",
      "Epoch 12, Training Loss: 0.8155879695275251\n",
      "Epoch 13, Training Loss: 0.8145577237886541\n",
      "Epoch 14, Training Loss: 0.8161915842925801\n",
      "Epoch 15, Training Loss: 0.8145675287527196\n",
      "Epoch 16, Training Loss: 0.8140163304525263\n",
      "Epoch 17, Training Loss: 0.8133860515846926\n",
      "Epoch 18, Training Loss: 0.8137089256679311\n",
      "Epoch 19, Training Loss: 0.8125265182467067\n",
      "Epoch 20, Training Loss: 0.8112851311879999\n",
      "Epoch 21, Training Loss: 0.8118831094573525\n",
      "Epoch 22, Training Loss: 0.8120796615235946\n",
      "Epoch 23, Training Loss: 0.8113040171651279\n",
      "Epoch 24, Training Loss: 0.8104042413655449\n",
      "Epoch 25, Training Loss: 0.8113719107122982\n",
      "Epoch 26, Training Loss: 0.8096842978281134\n",
      "Epoch 27, Training Loss: 0.8105630499475143\n",
      "Epoch 28, Training Loss: 0.8099332533163183\n",
      "Epoch 29, Training Loss: 0.8101193898565628\n",
      "Epoch 30, Training Loss: 0.809551606388653\n",
      "Epoch 31, Training Loss: 0.8091426020510056\n",
      "Epoch 32, Training Loss: 0.8095464329158558\n",
      "Epoch 33, Training Loss: 0.8086779100754682\n",
      "Epoch 34, Training Loss: 0.8078683260609122\n",
      "Epoch 35, Training Loss: 0.8090429087246166\n",
      "Epoch 36, Training Loss: 0.8088289842184853\n",
      "Epoch 37, Training Loss: 0.8078816495222204\n",
      "Epoch 38, Training Loss: 0.8086626557742849\n",
      "Epoch 39, Training Loss: 0.807938024927588\n",
      "Epoch 40, Training Loss: 0.8073920552169576\n",
      "Epoch 41, Training Loss: 0.8071861931155709\n",
      "Epoch 42, Training Loss: 0.8064724312810336\n",
      "Epoch 43, Training Loss: 0.8066887863243327\n",
      "Epoch 44, Training Loss: 0.8069639470997979\n",
      "Epoch 45, Training Loss: 0.8065492198747747\n",
      "Epoch 46, Training Loss: 0.8060941332929275\n",
      "Epoch 47, Training Loss: 0.8056725352651932\n",
      "Epoch 48, Training Loss: 0.8059134991028729\n",
      "Epoch 49, Training Loss: 0.8070005238757414\n",
      "Epoch 50, Training Loss: 0.8056432806744295\n",
      "Epoch 51, Training Loss: 0.8053622765400831\n",
      "Epoch 52, Training Loss: 0.8056235002770143\n",
      "Epoch 53, Training Loss: 0.805307202128803\n",
      "Epoch 54, Training Loss: 0.8049826141665963\n",
      "Epoch 55, Training Loss: 0.804961016739116\n",
      "Epoch 56, Training Loss: 0.805354589434231\n",
      "Epoch 57, Training Loss: 0.8043828511939329\n",
      "Epoch 58, Training Loss: 0.8042567586197572\n",
      "Epoch 59, Training Loss: 0.8039392027434181\n",
      "Epoch 60, Training Loss: 0.8053798004458932\n",
      "Epoch 61, Training Loss: 0.8039100675723132\n",
      "Epoch 62, Training Loss: 0.804352646855747\n",
      "Epoch 63, Training Loss: 0.8042746977946338\n",
      "Epoch 64, Training Loss: 0.8040530228614807\n",
      "Epoch 65, Training Loss: 0.8045569989961736\n",
      "Epoch 66, Training Loss: 0.803793592523126\n",
      "Epoch 67, Training Loss: 0.8040842565368204\n",
      "Epoch 68, Training Loss: 0.8035708757007823\n",
      "Epoch 69, Training Loss: 0.8043841645998113\n",
      "Epoch 70, Training Loss: 0.8032329150508432\n",
      "Epoch 71, Training Loss: 0.8036955643401427\n",
      "Epoch 72, Training Loss: 0.8041492980367997\n",
      "Epoch 73, Training Loss: 0.8034760846811182\n",
      "Epoch 74, Training Loss: 0.8034881888417637\n",
      "Epoch 75, Training Loss: 0.8032527375221252\n",
      "Epoch 76, Training Loss: 0.8027744664865382\n",
      "Epoch 77, Training Loss: 0.8035001733723809\n",
      "Epoch 78, Training Loss: 0.8029695835534264\n",
      "Epoch 79, Training Loss: 0.8036226152672488\n",
      "Epoch 80, Training Loss: 0.8036735435794381\n",
      "Epoch 81, Training Loss: 0.8032054027389077\n",
      "Epoch 82, Training Loss: 0.8026886576764724\n",
      "Epoch 83, Training Loss: 0.8032627900207744\n",
      "Epoch 84, Training Loss: 0.8031686801770154\n",
      "Epoch 85, Training Loss: 0.8032856693688561\n",
      "Epoch 86, Training Loss: 0.8029724508874556\n",
      "Epoch 87, Training Loss: 0.8029808400659\n",
      "Epoch 88, Training Loss: 0.8029236466043136\n",
      "Epoch 89, Training Loss: 0.8019312086526086\n",
      "Epoch 90, Training Loss: 0.8027539425737718\n",
      "Epoch 91, Training Loss: 0.8026269664483912\n",
      "Epoch 92, Training Loss: 0.8035489580911749\n",
      "Epoch 93, Training Loss: 0.802416935528026\n",
      "Epoch 94, Training Loss: 0.8029686025310965\n",
      "Epoch 95, Training Loss: 0.8024809493036831\n",
      "Epoch 96, Training Loss: 0.8027768281628104\n",
      "Epoch 97, Training Loss: 0.8026777381055495\n",
      "Epoch 98, Training Loss: 0.8021957119773416\n",
      "Epoch 99, Training Loss: 0.8020063243894016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:14:08,679] Trial 230 finished with value: 0.6374 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8015832770571989\n",
      "Epoch 1, Training Loss: 0.8772954077580396\n",
      "Epoch 2, Training Loss: 0.8146966868989608\n",
      "Epoch 3, Training Loss: 0.8080038580473732\n",
      "Epoch 4, Training Loss: 0.8030428750374737\n",
      "Epoch 5, Training Loss: 0.7991557319725261\n",
      "Epoch 6, Training Loss: 0.7956286483652452\n",
      "Epoch 7, Training Loss: 0.7929440115479862\n",
      "Epoch 8, Training Loss: 0.7909815047768985\n",
      "Epoch 9, Training Loss: 0.7896747693594764\n",
      "Epoch 10, Training Loss: 0.7888504615250755\n",
      "Epoch 11, Training Loss: 0.7883389271006864\n",
      "Epoch 12, Training Loss: 0.7871806627161363\n",
      "Epoch 13, Training Loss: 0.7867426686427172\n",
      "Epoch 14, Training Loss: 0.7856755234213436\n",
      "Epoch 15, Training Loss: 0.7854342909420238\n",
      "Epoch 16, Training Loss: 0.7851553913424997\n",
      "Epoch 17, Training Loss: 0.784651783213896\n",
      "Epoch 18, Training Loss: 0.7843573459456948\n",
      "Epoch 19, Training Loss: 0.7843077136488522\n",
      "Epoch 20, Training Loss: 0.7835295133730944\n",
      "Epoch 21, Training Loss: 0.7837848309909596\n",
      "Epoch 22, Training Loss: 0.7832067796763251\n",
      "Epoch 23, Training Loss: 0.7827251865583308\n",
      "Epoch 24, Training Loss: 0.7828455857669606\n",
      "Epoch 25, Training Loss: 0.7825184229542227\n",
      "Epoch 26, Training Loss: 0.7818656443147098\n",
      "Epoch 27, Training Loss: 0.7819731425537783\n",
      "Epoch 28, Training Loss: 0.7816006972509272\n",
      "Epoch 29, Training Loss: 0.781783812677159\n",
      "Epoch 30, Training Loss: 0.7810384820489322\n",
      "Epoch 31, Training Loss: 0.7811648312035729\n",
      "Epoch 32, Training Loss: 0.7809704433469211\n",
      "Epoch 33, Training Loss: 0.7805945812253391\n",
      "Epoch 34, Training Loss: 0.7804422095943899\n",
      "Epoch 35, Training Loss: 0.7802404387558208\n",
      "Epoch 36, Training Loss: 0.7800974620089811\n",
      "Epoch 37, Training Loss: 0.7799260241143844\n",
      "Epoch 38, Training Loss: 0.7800502752556521\n",
      "Epoch 39, Training Loss: 0.7796668318439932\n",
      "Epoch 40, Training Loss: 0.779595744118971\n",
      "Epoch 41, Training Loss: 0.779130807133282\n",
      "Epoch 42, Training Loss: 0.7790203679309172\n",
      "Epoch 43, Training Loss: 0.7785743655176723\n",
      "Epoch 44, Training Loss: 0.7786543281639323\n",
      "Epoch 45, Training Loss: 0.7785378398614772\n",
      "Epoch 46, Training Loss: 0.7785326514524572\n",
      "Epoch 47, Training Loss: 0.7781040040885701\n",
      "Epoch 48, Training Loss: 0.7781851185770596\n",
      "Epoch 49, Training Loss: 0.7782959790089551\n",
      "Epoch 50, Training Loss: 0.7779676541160134\n",
      "Epoch 51, Training Loss: 0.7780162107944488\n",
      "Epoch 52, Training Loss: 0.7771476875333225\n",
      "Epoch 53, Training Loss: 0.7774249355933246\n",
      "Epoch 54, Training Loss: 0.7776129017156713\n",
      "Epoch 55, Training Loss: 0.7770810128660763\n",
      "Epoch 56, Training Loss: 0.7768722385518692\n",
      "Epoch 57, Training Loss: 0.7769539643035216\n",
      "Epoch 58, Training Loss: 0.7766145173942341\n",
      "Epoch 59, Training Loss: 0.7767263824098251\n",
      "Epoch 60, Training Loss: 0.776601917182698\n",
      "Epoch 61, Training Loss: 0.7765134549842161\n",
      "Epoch 62, Training Loss: 0.7762214534422931\n",
      "Epoch 63, Training Loss: 0.7759243462366217\n",
      "Epoch 64, Training Loss: 0.7759981258476482\n",
      "Epoch 65, Training Loss: 0.776088922584758\n",
      "Epoch 66, Training Loss: 0.7760153831453884\n",
      "Epoch 67, Training Loss: 0.7754823576702791\n",
      "Epoch 68, Training Loss: 0.7754466728603139\n",
      "Epoch 69, Training Loss: 0.7758588565097135\n",
      "Epoch 70, Training Loss: 0.7751340661329381\n",
      "Epoch 71, Training Loss: 0.7751988263691173\n",
      "Epoch 72, Training Loss: 0.7748875401300542\n",
      "Epoch 73, Training Loss: 0.7748606957407559\n",
      "Epoch 74, Training Loss: 0.7746784507527071\n",
      "Epoch 75, Training Loss: 0.7747104640568004\n",
      "Epoch 76, Training Loss: 0.7748087974856882\n",
      "Epoch 77, Training Loss: 0.7742022630046396\n",
      "Epoch 78, Training Loss: 0.7743866781627431\n",
      "Epoch 79, Training Loss: 0.7746244576398064\n",
      "Epoch 80, Training Loss: 0.7739841316728031\n",
      "Epoch 81, Training Loss: 0.773902623723535\n",
      "Epoch 82, Training Loss: 0.7734175680665409\n",
      "Epoch 83, Training Loss: 0.7738212417854983\n",
      "Epoch 84, Training Loss: 0.7732414174079895\n",
      "Epoch 85, Training Loss: 0.7735092428151299\n",
      "Epoch 86, Training Loss: 0.7731807872828316\n",
      "Epoch 87, Training Loss: 0.7732228584850536\n",
      "Epoch 88, Training Loss: 0.7732846495684456\n",
      "Epoch 89, Training Loss: 0.7729739213691038\n",
      "Epoch 90, Training Loss: 0.7726945142886218\n",
      "Epoch 91, Training Loss: 0.7728157058884115\n",
      "Epoch 92, Training Loss: 0.7727003244091483\n",
      "Epoch 93, Training Loss: 0.772801770883448\n",
      "Epoch 94, Training Loss: 0.7724317376052632\n",
      "Epoch 95, Training Loss: 0.7725851676744573\n",
      "Epoch 96, Training Loss: 0.7725903468973496\n",
      "Epoch 97, Training Loss: 0.772349425834768\n",
      "Epoch 98, Training Loss: 0.7722184880340801\n",
      "Epoch 99, Training Loss: 0.7720630891884075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:16:29,554] Trial 231 finished with value: 0.6384 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7720858949773451\n",
      "Epoch 1, Training Loss: 1.0989547146890397\n",
      "Epoch 2, Training Loss: 1.083479745226695\n",
      "Epoch 3, Training Loss: 1.0725495476471751\n",
      "Epoch 4, Training Loss: 1.0631672061475597\n",
      "Epoch 5, Training Loss: 1.054215694011602\n",
      "Epoch 6, Training Loss: 1.0451540073954073\n",
      "Epoch 7, Training Loss: 1.0362412137196477\n",
      "Epoch 8, Training Loss: 1.0273314095081243\n",
      "Epoch 9, Training Loss: 1.0186049527691718\n",
      "Epoch 10, Training Loss: 1.009780188072893\n",
      "Epoch 11, Training Loss: 1.0014546987705661\n",
      "Epoch 12, Training Loss: 0.9929716629193241\n",
      "Epoch 13, Training Loss: 0.9854734200283997\n",
      "Epoch 14, Training Loss: 0.9781273532630805\n",
      "Epoch 15, Training Loss: 0.9718578186250271\n",
      "Epoch 16, Training Loss: 0.9661533777875112\n",
      "Epoch 17, Training Loss: 0.9609742039128354\n",
      "Epoch 18, Training Loss: 0.9564566618517826\n",
      "Epoch 19, Training Loss: 0.9531068404814355\n",
      "Epoch 20, Training Loss: 0.949434419054734\n",
      "Epoch 21, Training Loss: 0.9464019147973312\n",
      "Epoch 22, Training Loss: 0.9442262740959798\n",
      "Epoch 23, Training Loss: 0.9418290819440569\n",
      "Epoch 24, Training Loss: 0.9402734408701273\n",
      "Epoch 25, Training Loss: 0.9386938899979556\n",
      "Epoch 26, Training Loss: 0.9369362995140534\n",
      "Epoch 27, Training Loss: 0.9359412818026722\n",
      "Epoch 28, Training Loss: 0.9345509252153841\n",
      "Epoch 29, Training Loss: 0.9331217251325908\n",
      "Epoch 30, Training Loss: 0.9321576168662623\n",
      "Epoch 31, Training Loss: 0.9308756223298553\n",
      "Epoch 32, Training Loss: 0.9298416626184507\n",
      "Epoch 33, Training Loss: 0.9287294705111281\n",
      "Epoch 34, Training Loss: 0.927992642284336\n",
      "Epoch 35, Training Loss: 0.9274411065237863\n",
      "Epoch 36, Training Loss: 0.9261682118688311\n",
      "Epoch 37, Training Loss: 0.9249628296472077\n",
      "Epoch 38, Training Loss: 0.9240895114446941\n",
      "Epoch 39, Training Loss: 0.9234741173292461\n",
      "Epoch 40, Training Loss: 0.9220432442830021\n",
      "Epoch 41, Training Loss: 0.9215773225726938\n",
      "Epoch 42, Training Loss: 0.9206009855844024\n",
      "Epoch 43, Training Loss: 0.9195309992123367\n",
      "Epoch 44, Training Loss: 0.9186342722491214\n",
      "Epoch 45, Training Loss: 0.9176966572166385\n",
      "Epoch 46, Training Loss: 0.917242475649468\n",
      "Epoch 47, Training Loss: 0.9159148564016012\n",
      "Epoch 48, Training Loss: 0.9149352651789673\n",
      "Epoch 49, Training Loss: 0.9140241260815384\n",
      "Epoch 50, Training Loss: 0.9130798238560669\n",
      "Epoch 51, Training Loss: 0.9124522404563158\n",
      "Epoch 52, Training Loss: 0.9112371410642351\n",
      "Epoch 53, Training Loss: 0.9107531704400715\n",
      "Epoch 54, Training Loss: 0.9092332226889474\n",
      "Epoch 55, Training Loss: 0.9082757801041568\n",
      "Epoch 56, Training Loss: 0.9075553884183554\n",
      "Epoch 57, Training Loss: 0.906430563353058\n",
      "Epoch 58, Training Loss: 0.9055512443520969\n",
      "Epoch 59, Training Loss: 0.9050305476762298\n",
      "Epoch 60, Training Loss: 0.9038214110790339\n",
      "Epoch 61, Training Loss: 0.9024929895436853\n",
      "Epoch 62, Training Loss: 0.9018127024621891\n",
      "Epoch 63, Training Loss: 0.900821824808766\n",
      "Epoch 64, Training Loss: 0.8994619382055182\n",
      "Epoch 65, Training Loss: 0.8990250681575976\n",
      "Epoch 66, Training Loss: 0.8973093957829296\n",
      "Epoch 67, Training Loss: 0.8963941725573146\n",
      "Epoch 68, Training Loss: 0.8951367242891985\n",
      "Epoch 69, Training Loss: 0.8941848647325559\n",
      "Epoch 70, Training Loss: 0.893699636315941\n",
      "Epoch 71, Training Loss: 0.8922640356802403\n",
      "Epoch 72, Training Loss: 0.8908041627783524\n",
      "Epoch 73, Training Loss: 0.8897777972364784\n",
      "Epoch 74, Training Loss: 0.8888551429698341\n",
      "Epoch 75, Training Loss: 0.8876241831851185\n",
      "Epoch 76, Training Loss: 0.8863879461037485\n",
      "Epoch 77, Training Loss: 0.8852181729517485\n",
      "Epoch 78, Training Loss: 0.8839713288429089\n",
      "Epoch 79, Training Loss: 0.8828760375653891\n",
      "Epoch 80, Training Loss: 0.8817008100954213\n",
      "Epoch 81, Training Loss: 0.8806263280094118\n",
      "Epoch 82, Training Loss: 0.8794424731928603\n",
      "Epoch 83, Training Loss: 0.8779475027457215\n",
      "Epoch 84, Training Loss: 0.8765408797371657\n",
      "Epoch 85, Training Loss: 0.8755607192677662\n",
      "Epoch 86, Training Loss: 0.8736605284805584\n",
      "Epoch 87, Training Loss: 0.8726700696730076\n",
      "Epoch 88, Training Loss: 0.8711683285863776\n",
      "Epoch 89, Training Loss: 0.8700998074130009\n",
      "Epoch 90, Training Loss: 0.8683594388173039\n",
      "Epoch 91, Training Loss: 0.8669875065186866\n",
      "Epoch 92, Training Loss: 0.8659136004914019\n",
      "Epoch 93, Training Loss: 0.8637508667501291\n",
      "Epoch 94, Training Loss: 0.8629199209069847\n",
      "Epoch 95, Training Loss: 0.8612666481419613\n",
      "Epoch 96, Training Loss: 0.8595147239534479\n",
      "Epoch 97, Training Loss: 0.8583140712931641\n",
      "Epoch 98, Training Loss: 0.8568494090460297\n",
      "Epoch 99, Training Loss: 0.85550365958895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:18:00,596] Trial 232 finished with value: 0.5996666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8540169968640894\n",
      "Epoch 1, Training Loss: 0.8791250083025764\n",
      "Epoch 2, Training Loss: 0.8154217365910025\n",
      "Epoch 3, Training Loss: 0.8103324287779191\n",
      "Epoch 4, Training Loss: 0.808681588383282\n",
      "Epoch 5, Training Loss: 0.8046640812649446\n",
      "Epoch 6, Training Loss: 0.8016978322758395\n",
      "Epoch 7, Training Loss: 0.798795192452038\n",
      "Epoch 8, Training Loss: 0.7967924359966727\n",
      "Epoch 9, Training Loss: 0.7957343218607061\n",
      "Epoch 10, Training Loss: 0.7943333828449249\n",
      "Epoch 11, Training Loss: 0.7928635271857767\n",
      "Epoch 12, Training Loss: 0.793395517503514\n",
      "Epoch 13, Training Loss: 0.7913003409610075\n",
      "Epoch 14, Training Loss: 0.7918081064785228\n",
      "Epoch 15, Training Loss: 0.7910639992882224\n",
      "Epoch 16, Training Loss: 0.7898238354570726\n",
      "Epoch 17, Training Loss: 0.7898749135522282\n",
      "Epoch 18, Training Loss: 0.7897261859388912\n",
      "Epoch 19, Training Loss: 0.7898413453382604\n",
      "Epoch 20, Training Loss: 0.7893052120068494\n",
      "Epoch 21, Training Loss: 0.7881194431641523\n",
      "Epoch 22, Training Loss: 0.7874530271221609\n",
      "Epoch 23, Training Loss: 0.788702856723\n",
      "Epoch 24, Training Loss: 0.787863673322341\n",
      "Epoch 25, Training Loss: 0.7878558470922358\n",
      "Epoch 26, Training Loss: 0.7869767144848319\n",
      "Epoch 27, Training Loss: 0.7876324298802544\n",
      "Epoch 28, Training Loss: 0.7866936921372133\n",
      "Epoch 29, Training Loss: 0.7862334974373089\n",
      "Epoch 30, Training Loss: 0.7872837042808533\n",
      "Epoch 31, Training Loss: 0.7864438073775347\n",
      "Epoch 32, Training Loss: 0.7861036543285146\n",
      "Epoch 33, Training Loss: 0.7862732157286476\n",
      "Epoch 34, Training Loss: 0.7866333912400638\n",
      "Epoch 35, Training Loss: 0.7855610173590043\n",
      "Epoch 36, Training Loss: 0.7858266812212327\n",
      "Epoch 37, Training Loss: 0.7852772836124196\n",
      "Epoch 38, Training Loss: 0.78513728667708\n",
      "Epoch 39, Training Loss: 0.7856016514581793\n",
      "Epoch 40, Training Loss: 0.7858565071049859\n",
      "Epoch 41, Training Loss: 0.7849219337631674\n",
      "Epoch 42, Training Loss: 0.7848620114606969\n",
      "Epoch 43, Training Loss: 0.7839722316405352\n",
      "Epoch 44, Training Loss: 0.7847267692930558\n",
      "Epoch 45, Training Loss: 0.7843937762344585\n",
      "Epoch 46, Training Loss: 0.7845545960875119\n",
      "Epoch 47, Training Loss: 0.783808732734007\n",
      "Epoch 48, Training Loss: 0.7834051165160011\n",
      "Epoch 49, Training Loss: 0.7834893863341388\n",
      "Epoch 50, Training Loss: 0.7837333314559038\n",
      "Epoch 51, Training Loss: 0.7838949298858643\n",
      "Epoch 52, Training Loss: 0.7836734015801373\n",
      "Epoch 53, Training Loss: 0.7836705646094154\n",
      "Epoch 54, Training Loss: 0.7830844503290513\n",
      "Epoch 55, Training Loss: 0.7841041396645939\n",
      "Epoch 56, Training Loss: 0.7835578262104708\n",
      "Epoch 57, Training Loss: 0.7825765308913063\n",
      "Epoch 58, Training Loss: 0.782632267405005\n",
      "Epoch 59, Training Loss: 0.7823832801510306\n",
      "Epoch 60, Training Loss: 0.7823436453763176\n",
      "Epoch 61, Training Loss: 0.7830287152178147\n",
      "Epoch 62, Training Loss: 0.7829096297656789\n",
      "Epoch 63, Training Loss: 0.7827568039473365\n",
      "Epoch 64, Training Loss: 0.782508279505898\n",
      "Epoch 65, Training Loss: 0.7819750135085162\n",
      "Epoch 66, Training Loss: 0.7823909076522378\n",
      "Epoch 67, Training Loss: 0.7816176499338711\n",
      "Epoch 68, Training Loss: 0.7819750125267927\n",
      "Epoch 69, Training Loss: 0.78186958824887\n",
      "Epoch 70, Training Loss: 0.7820395668113933\n",
      "Epoch 71, Training Loss: 0.78248274473583\n",
      "Epoch 72, Training Loss: 0.7814583840089686\n",
      "Epoch 73, Training Loss: 0.7806051085976994\n",
      "Epoch 74, Training Loss: 0.7813763075716356\n",
      "Epoch 75, Training Loss: 0.780981497484095\n",
      "Epoch 76, Training Loss: 0.780867463981404\n",
      "Epoch 77, Training Loss: 0.7805949594694025\n",
      "Epoch 78, Training Loss: 0.7811632433358361\n",
      "Epoch 79, Training Loss: 0.7804744586523842\n",
      "Epoch 80, Training Loss: 0.7808284274970784\n",
      "Epoch 81, Training Loss: 0.7807830630330479\n",
      "Epoch 82, Training Loss: 0.7806671111723956\n",
      "Epoch 83, Training Loss: 0.7805248712792116\n",
      "Epoch 84, Training Loss: 0.7802825537849875\n",
      "Epoch 85, Training Loss: 0.7806670884525074\n",
      "Epoch 86, Training Loss: 0.779941527633106\n",
      "Epoch 87, Training Loss: 0.7807909950789284\n",
      "Epoch 88, Training Loss: 0.7801333199529087\n",
      "Epoch 89, Training Loss: 0.7801564123350031\n",
      "Epoch 90, Training Loss: 0.77973638604669\n",
      "Epoch 91, Training Loss: 0.7805477070808411\n",
      "Epoch 92, Training Loss: 0.7794786294067607\n",
      "Epoch 93, Training Loss: 0.7796720789460575\n",
      "Epoch 94, Training Loss: 0.7794043066922356\n",
      "Epoch 95, Training Loss: 0.7789925749862895\n",
      "Epoch 96, Training Loss: 0.779576851830763\n",
      "Epoch 97, Training Loss: 0.7797632912327261\n",
      "Epoch 98, Training Loss: 0.7793951528913835\n",
      "Epoch 99, Training Loss: 0.7794924034090603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:20:17,387] Trial 233 finished with value: 0.6376666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7795773548939648\n",
      "Epoch 1, Training Loss: 0.9253570411485784\n",
      "Epoch 2, Training Loss: 0.8319390208580915\n",
      "Epoch 3, Training Loss: 0.8224338808480431\n",
      "Epoch 4, Training Loss: 0.8164900194897371\n",
      "Epoch 5, Training Loss: 0.8121117506307715\n",
      "Epoch 6, Training Loss: 0.8075778086045209\n",
      "Epoch 7, Training Loss: 0.8058627036739798\n",
      "Epoch 8, Training Loss: 0.8037177132157718\n",
      "Epoch 9, Training Loss: 0.8027917845810161\n",
      "Epoch 10, Training Loss: 0.8021518039002138\n",
      "Epoch 11, Training Loss: 0.8014611394265119\n",
      "Epoch 12, Training Loss: 0.8003977198460522\n",
      "Epoch 13, Training Loss: 0.7997008218484767\n",
      "Epoch 14, Training Loss: 0.7985071979550754\n",
      "Epoch 15, Training Loss: 0.7984706695640789\n",
      "Epoch 16, Training Loss: 0.797397257860969\n",
      "Epoch 17, Training Loss: 0.7968774102715885\n",
      "Epoch 18, Training Loss: 0.796790734389249\n",
      "Epoch 19, Training Loss: 0.7972548580870908\n",
      "Epoch 20, Training Loss: 0.7956663198330823\n",
      "Epoch 21, Training Loss: 0.7965916546653299\n",
      "Epoch 22, Training Loss: 0.7958279761847328\n",
      "Epoch 23, Training Loss: 0.7952770153915181\n",
      "Epoch 24, Training Loss: 0.7942839461915633\n",
      "Epoch 25, Training Loss: 0.7953805045520558\n",
      "Epoch 26, Training Loss: 0.7954270430172191\n",
      "Epoch 27, Training Loss: 0.793993876471239\n",
      "Epoch 28, Training Loss: 0.7945116165105034\n",
      "Epoch 29, Training Loss: 0.7943500172390657\n",
      "Epoch 30, Training Loss: 0.7941010998978334\n",
      "Epoch 31, Training Loss: 0.7936992142480962\n",
      "Epoch 32, Training Loss: 0.794443725347519\n",
      "Epoch 33, Training Loss: 0.7943536435856539\n",
      "Epoch 34, Training Loss: 0.7931685749923482\n",
      "Epoch 35, Training Loss: 0.793604866546743\n",
      "Epoch 36, Training Loss: 0.7927023236891803\n",
      "Epoch 37, Training Loss: 0.7929161653097938\n",
      "Epoch 38, Training Loss: 0.7928803175337175\n",
      "Epoch 39, Training Loss: 0.7925784772985122\n",
      "Epoch 40, Training Loss: 0.7937048179261824\n",
      "Epoch 41, Training Loss: 0.7932140736720141\n",
      "Epoch 42, Training Loss: 0.7921703541980071\n",
      "Epoch 43, Training Loss: 0.7931310934178969\n",
      "Epoch 44, Training Loss: 0.7927852111704209\n",
      "Epoch 45, Training Loss: 0.7923445058570189\n",
      "Epoch 46, Training Loss: 0.7934244359240813\n",
      "Epoch 47, Training Loss: 0.7939096666784847\n",
      "Epoch 48, Training Loss: 0.7919819153056425\n",
      "Epoch 49, Training Loss: 0.7917819909488454\n",
      "Epoch 50, Training Loss: 0.7932371035042931\n",
      "Epoch 51, Training Loss: 0.7932377588047701\n",
      "Epoch 52, Training Loss: 0.7920755401779623\n",
      "Epoch 53, Training Loss: 0.793034101934994\n",
      "Epoch 54, Training Loss: 0.7932730984687805\n",
      "Epoch 55, Training Loss: 0.7943587592770072\n",
      "Epoch 56, Training Loss: 0.7924942385449129\n",
      "Epoch 57, Training Loss: 0.7940681782189537\n",
      "Epoch 58, Training Loss: 0.7925900804996491\n",
      "Epoch 59, Training Loss: 0.7926461374759675\n",
      "Epoch 60, Training Loss: 0.7936554122672361\n",
      "Epoch 61, Training Loss: 0.7929339438326218\n",
      "Epoch 62, Training Loss: 0.7926441738184761\n",
      "Epoch 63, Training Loss: 0.79094542797874\n",
      "Epoch 64, Training Loss: 0.7921293061621049\n",
      "Epoch 65, Training Loss: 0.7922663347861346\n",
      "Epoch 66, Training Loss: 0.7933219836038702\n",
      "Epoch 67, Training Loss: 0.7926398894366096\n",
      "Epoch 68, Training Loss: 0.791864360220292\n",
      "Epoch 69, Training Loss: 0.7920493585221907\n",
      "Epoch 70, Training Loss: 0.7929606688723845\n",
      "Epoch 71, Training Loss: 0.7920313502059263\n",
      "Epoch 72, Training Loss: 0.7926366826365976\n",
      "Epoch 73, Training Loss: 0.7923276635478524\n",
      "Epoch 74, Training Loss: 0.7942225480780882\n",
      "Epoch 75, Training Loss: 0.7928777370032142\n",
      "Epoch 76, Training Loss: 0.7923585104942322\n",
      "Epoch 77, Training Loss: 0.7920464252023136\n",
      "Epoch 78, Training Loss: 0.7926783669696135\n",
      "Epoch 79, Training Loss: 0.793539014563841\n",
      "Epoch 80, Training Loss: 0.7915886236639584\n",
      "Epoch 81, Training Loss: 0.7928800717522116\n",
      "Epoch 82, Training Loss: 0.7933640362935908\n",
      "Epoch 83, Training Loss: 0.7933159589066225\n",
      "Epoch 84, Training Loss: 0.7920644509792328\n",
      "Epoch 85, Training Loss: 0.7931595629103043\n",
      "Epoch 86, Training Loss: 0.7940870148294112\n",
      "Epoch 87, Training Loss: 0.792812550909379\n",
      "Epoch 88, Training Loss: 0.7928539190572851\n",
      "Epoch 89, Training Loss: 0.7916639979446636\n",
      "Epoch 90, Training Loss: 0.7931016821721021\n",
      "Epoch 91, Training Loss: 0.7938011001839357\n",
      "Epoch 92, Training Loss: 0.7923047261378344\n",
      "Epoch 93, Training Loss: 0.7920043180269354\n",
      "Epoch 94, Training Loss: 0.7919574159033158\n",
      "Epoch 95, Training Loss: 0.7920150137648863\n",
      "Epoch 96, Training Loss: 0.7929140227682451\n",
      "Epoch 97, Training Loss: 0.7927972790072946\n",
      "Epoch 98, Training Loss: 0.7917176421249614\n",
      "Epoch 99, Training Loss: 0.7910720278235043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:22:41,314] Trial 234 finished with value: 0.6369333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7925929730779985\n",
      "Epoch 1, Training Loss: 1.0074261270071332\n",
      "Epoch 2, Training Loss: 0.935457954639779\n",
      "Epoch 3, Training Loss: 0.9194530565935867\n",
      "Epoch 4, Training Loss: 0.9028713774860353\n",
      "Epoch 5, Training Loss: 0.880827786510152\n",
      "Epoch 6, Training Loss: 0.8520257875435334\n",
      "Epoch 7, Training Loss: 0.826589286237731\n",
      "Epoch 8, Training Loss: 0.8132220039690348\n",
      "Epoch 9, Training Loss: 0.8083197989858183\n",
      "Epoch 10, Training Loss: 0.8055020948101703\n",
      "Epoch 11, Training Loss: 0.8053041256460032\n",
      "Epoch 12, Training Loss: 0.8043133763442362\n",
      "Epoch 13, Training Loss: 0.8037919310698832\n",
      "Epoch 14, Training Loss: 0.8026215780050234\n",
      "Epoch 15, Training Loss: 0.8016773188920846\n",
      "Epoch 16, Training Loss: 0.8010115646778193\n",
      "Epoch 17, Training Loss: 0.8008566756893818\n",
      "Epoch 18, Training Loss: 0.8008029817638541\n",
      "Epoch 19, Training Loss: 0.7993858942411896\n",
      "Epoch 20, Training Loss: 0.7997181940795783\n",
      "Epoch 21, Training Loss: 0.7992971205173578\n",
      "Epoch 22, Training Loss: 0.7990477861318374\n",
      "Epoch 23, Training Loss: 0.7991413435541598\n",
      "Epoch 24, Training Loss: 0.7975803400340833\n",
      "Epoch 25, Training Loss: 0.7973473532755572\n",
      "Epoch 26, Training Loss: 0.7973586316395523\n",
      "Epoch 27, Training Loss: 0.7968815020152501\n",
      "Epoch 28, Training Loss: 0.7966602143488433\n",
      "Epoch 29, Training Loss: 0.7958185445993466\n",
      "Epoch 30, Training Loss: 0.7956465916526049\n",
      "Epoch 31, Training Loss: 0.796326688537024\n",
      "Epoch 32, Training Loss: 0.7955758222959992\n",
      "Epoch 33, Training Loss: 0.7943609704648642\n",
      "Epoch 34, Training Loss: 0.7947699381892842\n",
      "Epoch 35, Training Loss: 0.7945509589704356\n",
      "Epoch 36, Training Loss: 0.7936649358362183\n",
      "Epoch 37, Training Loss: 0.7939463619002722\n",
      "Epoch 38, Training Loss: 0.7932964627007792\n",
      "Epoch 39, Training Loss: 0.7931667832503642\n",
      "Epoch 40, Training Loss: 0.7932584852204287\n",
      "Epoch 41, Training Loss: 0.7925295861143815\n",
      "Epoch 42, Training Loss: 0.7913342359370755\n",
      "Epoch 43, Training Loss: 0.7911666176372901\n",
      "Epoch 44, Training Loss: 0.7914153073963366\n",
      "Epoch 45, Training Loss: 0.7907843413657711\n",
      "Epoch 46, Training Loss: 0.790755017538716\n",
      "Epoch 47, Training Loss: 0.7904305181108919\n",
      "Epoch 48, Training Loss: 0.7900116034916469\n",
      "Epoch 49, Training Loss: 0.7895531445517576\n",
      "Epoch 50, Training Loss: 0.7900599151625669\n",
      "Epoch 51, Training Loss: 0.7889715141371677\n",
      "Epoch 52, Training Loss: 0.7888028751638599\n",
      "Epoch 53, Training Loss: 0.7889027975555649\n",
      "Epoch 54, Training Loss: 0.7886842452493825\n",
      "Epoch 55, Training Loss: 0.7883821575265182\n",
      "Epoch 56, Training Loss: 0.788147685402318\n",
      "Epoch 57, Training Loss: 0.7872108033725194\n",
      "Epoch 58, Training Loss: 0.7872266431051985\n",
      "Epoch 59, Training Loss: 0.7870462181424736\n",
      "Epoch 60, Training Loss: 0.7870576892580304\n",
      "Epoch 61, Training Loss: 0.787441394831005\n",
      "Epoch 62, Training Loss: 0.7872188255302888\n",
      "Epoch 63, Training Loss: 0.7867361352855998\n",
      "Epoch 64, Training Loss: 0.7864735592576794\n",
      "Epoch 65, Training Loss: 0.7869306265859676\n",
      "Epoch 66, Training Loss: 0.7866073534004671\n",
      "Epoch 67, Training Loss: 0.7862435997876905\n",
      "Epoch 68, Training Loss: 0.7858181689914904\n",
      "Epoch 69, Training Loss: 0.7860762336200341\n",
      "Epoch 70, Training Loss: 0.7859778206151231\n",
      "Epoch 71, Training Loss: 0.7851489420223953\n",
      "Epoch 72, Training Loss: 0.7853548178995462\n",
      "Epoch 73, Training Loss: 0.7858385303863009\n",
      "Epoch 74, Training Loss: 0.7843959548867735\n",
      "Epoch 75, Training Loss: 0.7856280448741483\n",
      "Epoch 76, Training Loss: 0.7852916077563638\n",
      "Epoch 77, Training Loss: 0.7850137262416065\n",
      "Epoch 78, Training Loss: 0.7849388775968911\n",
      "Epoch 79, Training Loss: 0.7844723138145935\n",
      "Epoch 80, Training Loss: 0.784904784905283\n",
      "Epoch 81, Training Loss: 0.7847801330394315\n",
      "Epoch 82, Training Loss: 0.7839967050946745\n",
      "Epoch 83, Training Loss: 0.784273522240775\n",
      "Epoch 84, Training Loss: 0.7840610774836145\n",
      "Epoch 85, Training Loss: 0.78436344153899\n",
      "Epoch 86, Training Loss: 0.7838703855536038\n",
      "Epoch 87, Training Loss: 0.7842295427071421\n",
      "Epoch 88, Training Loss: 0.7839467468118309\n",
      "Epoch 89, Training Loss: 0.784274817588634\n",
      "Epoch 90, Training Loss: 0.7845896996053537\n",
      "Epoch 91, Training Loss: 0.7839350382188209\n",
      "Epoch 92, Training Loss: 0.7841560983120051\n",
      "Epoch 93, Training Loss: 0.7836783068520682\n",
      "Epoch 94, Training Loss: 0.7830523893797308\n",
      "Epoch 95, Training Loss: 0.7836405073789726\n",
      "Epoch 96, Training Loss: 0.783953731310995\n",
      "Epoch 97, Training Loss: 0.7837380871736914\n",
      "Epoch 98, Training Loss: 0.7827840756204791\n",
      "Epoch 99, Training Loss: 0.783240532875061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:24:15,088] Trial 235 finished with value: 0.6314666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7837067415839747\n",
      "Epoch 1, Training Loss: 1.0929315520408458\n",
      "Epoch 2, Training Loss: 1.090911272593907\n",
      "Epoch 3, Training Loss: 1.0908108714827918\n",
      "Epoch 4, Training Loss: 1.0907732065458944\n",
      "Epoch 5, Training Loss: 1.0906572623360427\n",
      "Epoch 6, Training Loss: 1.0904942967837914\n",
      "Epoch 7, Training Loss: 1.0904909938797915\n",
      "Epoch 8, Training Loss: 1.0904457529684655\n",
      "Epoch 9, Training Loss: 1.090412948364602\n",
      "Epoch 10, Training Loss: 1.0902551659964081\n",
      "Epoch 11, Training Loss: 1.0901625972045095\n",
      "Epoch 12, Training Loss: 1.0900288585433386\n",
      "Epoch 13, Training Loss: 1.0900169904966999\n",
      "Epoch 14, Training Loss: 1.0898399252640574\n",
      "Epoch 15, Training Loss: 1.0895751001243303\n",
      "Epoch 16, Training Loss: 1.0896767881579865\n",
      "Epoch 17, Training Loss: 1.089443734893225\n",
      "Epoch 18, Training Loss: 1.089226738313087\n",
      "Epoch 19, Training Loss: 1.0893288931452243\n",
      "Epoch 20, Training Loss: 1.0889509460979834\n",
      "Epoch 21, Training Loss: 1.0888866440694136\n",
      "Epoch 22, Training Loss: 1.088818135297388\n",
      "Epoch 23, Training Loss: 1.0884316024923684\n",
      "Epoch 24, Training Loss: 1.0883177038422205\n",
      "Epoch 25, Training Loss: 1.0880306329942286\n",
      "Epoch 26, Training Loss: 1.0876953117829515\n",
      "Epoch 27, Training Loss: 1.0874062529183868\n",
      "Epoch 28, Training Loss: 1.0870491169449081\n",
      "Epoch 29, Training Loss: 1.0866691083836377\n",
      "Epoch 30, Training Loss: 1.0861964908757604\n",
      "Epoch 31, Training Loss: 1.085761989686722\n",
      "Epoch 32, Training Loss: 1.085297417640686\n",
      "Epoch 33, Training Loss: 1.0846209310947503\n",
      "Epoch 34, Training Loss: 1.0839566033585628\n",
      "Epoch 35, Training Loss: 1.0830623782667002\n",
      "Epoch 36, Training Loss: 1.0820195977849172\n",
      "Epoch 37, Training Loss: 1.0808948758849524\n",
      "Epoch 38, Training Loss: 1.0795466078851457\n",
      "Epoch 39, Training Loss: 1.0779456529402196\n",
      "Epoch 40, Training Loss: 1.0759886985434626\n",
      "Epoch 41, Training Loss: 1.073683862757862\n",
      "Epoch 42, Training Loss: 1.0710044461085384\n",
      "Epoch 43, Training Loss: 1.0677606604153052\n",
      "Epoch 44, Training Loss: 1.0638023776219303\n",
      "Epoch 45, Training Loss: 1.0590451453861438\n",
      "Epoch 46, Training Loss: 1.053524599218727\n",
      "Epoch 47, Training Loss: 1.047031545280514\n",
      "Epoch 48, Training Loss: 1.0396476198856095\n",
      "Epoch 49, Training Loss: 1.0319282874128872\n",
      "Epoch 50, Training Loss: 1.0237330539782243\n",
      "Epoch 51, Training Loss: 1.016563005286052\n",
      "Epoch 52, Training Loss: 1.0102369774553113\n",
      "Epoch 53, Training Loss: 1.0054045955041297\n",
      "Epoch 54, Training Loss: 1.0015554525798425\n",
      "Epoch 55, Training Loss: 0.9993428313642516\n",
      "Epoch 56, Training Loss: 0.9974999499500246\n",
      "Epoch 57, Training Loss: 0.9960588093090774\n",
      "Epoch 58, Training Loss: 0.9957589767032996\n",
      "Epoch 59, Training Loss: 0.9946846148125211\n",
      "Epoch 60, Training Loss: 0.9939002333727098\n",
      "Epoch 61, Training Loss: 0.9931040155260187\n",
      "Epoch 62, Training Loss: 0.9925633297827011\n",
      "Epoch 63, Training Loss: 0.9924963538808034\n",
      "Epoch 64, Training Loss: 0.9911808073968815\n",
      "Epoch 65, Training Loss: 0.9907101191076121\n",
      "Epoch 66, Training Loss: 0.9904304292865266\n",
      "Epoch 67, Training Loss: 0.9895125637377115\n",
      "Epoch 68, Training Loss: 0.9886842157607688\n",
      "Epoch 69, Training Loss: 0.9884253314563206\n",
      "Epoch 70, Training Loss: 0.9876969860908681\n",
      "Epoch 71, Training Loss: 0.9866954807948349\n",
      "Epoch 72, Training Loss: 0.9859708405078802\n",
      "Epoch 73, Training Loss: 0.9852091622531862\n",
      "Epoch 74, Training Loss: 0.9846083435797154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:25:30,508] Trial 236 finished with value: 0.49866666666666665 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9839039571303174\n",
      "Epoch 1, Training Loss: 0.8847456993776209\n",
      "Epoch 2, Training Loss: 0.8185038573601666\n",
      "Epoch 3, Training Loss: 0.8129405399631051\n",
      "Epoch 4, Training Loss: 0.8100835894135868\n",
      "Epoch 5, Training Loss: 0.8076711655364317\n",
      "Epoch 6, Training Loss: 0.8061672951894648\n",
      "Epoch 7, Training Loss: 0.8050611499477835\n",
      "Epoch 8, Training Loss: 0.8034872348869548\n",
      "Epoch 9, Training Loss: 0.8026752107984879\n",
      "Epoch 10, Training Loss: 0.8022626805305481\n",
      "Epoch 11, Training Loss: 0.80160059837734\n",
      "Epoch 12, Training Loss: 0.8013880223386428\n",
      "Epoch 13, Training Loss: 0.8007247999135185\n",
      "Epoch 14, Training Loss: 0.8004371586266686\n",
      "Epoch 15, Training Loss: 0.7997764164559982\n",
      "Epoch 16, Training Loss: 0.7993701250412885\n",
      "Epoch 17, Training Loss: 0.7992978102319381\n",
      "Epoch 18, Training Loss: 0.7988617423702689\n",
      "Epoch 19, Training Loss: 0.7982225788340849\n",
      "Epoch 20, Training Loss: 0.7978563848663779\n",
      "Epoch 21, Training Loss: 0.7973256097120397\n",
      "Epoch 22, Training Loss: 0.7969767951264101\n",
      "Epoch 23, Training Loss: 0.7962372951647815\n",
      "Epoch 24, Training Loss: 0.7959738621992224\n",
      "Epoch 25, Training Loss: 0.7950056153185228\n",
      "Epoch 26, Training Loss: 0.7946213105846854\n",
      "Epoch 27, Training Loss: 0.7937962555885315\n",
      "Epoch 28, Training Loss: 0.7932208413937513\n",
      "Epoch 29, Training Loss: 0.7923548766444711\n",
      "Epoch 30, Training Loss: 0.7918684158605688\n",
      "Epoch 31, Training Loss: 0.7908163965449614\n",
      "Epoch 32, Training Loss: 0.7904509824163773\n",
      "Epoch 33, Training Loss: 0.7896273810022018\n",
      "Epoch 34, Training Loss: 0.789018267883974\n",
      "Epoch 35, Training Loss: 0.7885551102722392\n",
      "Epoch 36, Training Loss: 0.7881834286100724\n",
      "Epoch 37, Training Loss: 0.7878823710189146\n",
      "Epoch 38, Training Loss: 0.7873730269600363\n",
      "Epoch 39, Training Loss: 0.7870930640837726\n",
      "Epoch 40, Training Loss: 0.7863593649162965\n",
      "Epoch 41, Training Loss: 0.7864383165275349\n",
      "Epoch 42, Training Loss: 0.7860851951907663\n",
      "Epoch 43, Training Loss: 0.7857440815252417\n",
      "Epoch 44, Training Loss: 0.7854696704359616\n",
      "Epoch 45, Training Loss: 0.7853172499993268\n",
      "Epoch 46, Training Loss: 0.7851622413887697\n",
      "Epoch 47, Training Loss: 0.7851458649775561\n",
      "Epoch 48, Training Loss: 0.7850531684651094\n",
      "Epoch 49, Training Loss: 0.7846692392405341\n",
      "Epoch 50, Training Loss: 0.7846680597697987\n",
      "Epoch 51, Training Loss: 0.7846636023942162\n",
      "Epoch 52, Training Loss: 0.7843898145591511\n",
      "Epoch 53, Training Loss: 0.7845720350041109\n",
      "Epoch 54, Training Loss: 0.7839681494937224\n",
      "Epoch 55, Training Loss: 0.7840312240404241\n",
      "Epoch 56, Training Loss: 0.78389484167099\n",
      "Epoch 57, Training Loss: 0.7840606133376851\n",
      "Epoch 58, Training Loss: 0.783809645877165\n",
      "Epoch 59, Training Loss: 0.7838935265821569\n",
      "Epoch 60, Training Loss: 0.7837188396033119\n",
      "Epoch 61, Training Loss: 0.7834154402508455\n",
      "Epoch 62, Training Loss: 0.7838166602218852\n",
      "Epoch 63, Training Loss: 0.7831299804238712\n",
      "Epoch 64, Training Loss: 0.7834176233235528\n",
      "Epoch 65, Training Loss: 0.7836341748518102\n",
      "Epoch 66, Training Loss: 0.7835974646315855\n",
      "Epoch 67, Training Loss: 0.783210685042774\n",
      "Epoch 68, Training Loss: 0.7832212909530191\n",
      "Epoch 69, Training Loss: 0.783352268373265\n",
      "Epoch 70, Training Loss: 0.7830951199110816\n",
      "Epoch 71, Training Loss: 0.782667996041915\n",
      "Epoch 72, Training Loss: 0.7830258984425489\n",
      "Epoch 73, Training Loss: 0.7830457366214079\n",
      "Epoch 74, Training Loss: 0.7829021422302022\n",
      "Epoch 75, Training Loss: 0.7827383812736063\n",
      "Epoch 76, Training Loss: 0.7827299308776855\n",
      "Epoch 77, Training Loss: 0.7825789769256816\n",
      "Epoch 78, Training Loss: 0.7827316258935367\n",
      "Epoch 79, Training Loss: 0.7828076087026035\n",
      "Epoch 80, Training Loss: 0.7824845630281112\n",
      "Epoch 81, Training Loss: 0.7824714457287508\n",
      "Epoch 82, Training Loss: 0.7826280824577108\n",
      "Epoch 83, Training Loss: 0.7828062006305245\n",
      "Epoch 84, Training Loss: 0.782428821255179\n",
      "Epoch 85, Training Loss: 0.7823708742506363\n",
      "Epoch 86, Training Loss: 0.7826917099952698\n",
      "Epoch 87, Training Loss: 0.7824170189745286\n",
      "Epoch 88, Training Loss: 0.7823028093927047\n",
      "Epoch 89, Training Loss: 0.7821884727478028\n",
      "Epoch 90, Training Loss: 0.7823187142961165\n",
      "Epoch 91, Training Loss: 0.7817597009153927\n",
      "Epoch 92, Training Loss: 0.7820146754909965\n",
      "Epoch 93, Training Loss: 0.7822495794997496\n",
      "Epoch 94, Training Loss: 0.7820592802412369\n",
      "Epoch 95, Training Loss: 0.782020545426537\n",
      "Epoch 96, Training Loss: 0.7820302774625666\n",
      "Epoch 97, Training Loss: 0.782004214665469\n",
      "Epoch 98, Training Loss: 0.7817728240349714\n",
      "Epoch 99, Training Loss: 0.7818278250974767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:27:36,748] Trial 237 finished with value: 0.6406 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7821895662476035\n",
      "Epoch 1, Training Loss: 1.0635165798036676\n",
      "Epoch 2, Training Loss: 0.9643252816415371\n",
      "Epoch 3, Training Loss: 0.9229620862724189\n",
      "Epoch 4, Training Loss: 0.9036397823713775\n",
      "Epoch 5, Training Loss: 0.876337564798226\n",
      "Epoch 6, Training Loss: 0.8368366139275687\n",
      "Epoch 7, Training Loss: 0.8154477071045036\n",
      "Epoch 8, Training Loss: 0.8098981975612783\n",
      "Epoch 9, Training Loss: 0.8080498839679517\n",
      "Epoch 10, Training Loss: 0.8056159628065009\n",
      "Epoch 11, Training Loss: 0.8042301070421262\n",
      "Epoch 12, Training Loss: 0.8026657696057083\n",
      "Epoch 13, Training Loss: 0.8007792453120526\n",
      "Epoch 14, Training Loss: 0.8014987000845428\n",
      "Epoch 15, Training Loss: 0.8003405111176627\n",
      "Epoch 16, Training Loss: 0.7998467491085368\n",
      "Epoch 17, Training Loss: 0.7986476569247425\n",
      "Epoch 18, Training Loss: 0.7981680574273704\n",
      "Epoch 19, Training Loss: 0.7977075946958442\n",
      "Epoch 20, Training Loss: 0.7968387060595634\n",
      "Epoch 21, Training Loss: 0.7971029850773346\n",
      "Epoch 22, Training Loss: 0.7961024752236847\n",
      "Epoch 23, Training Loss: 0.7958825824852277\n",
      "Epoch 24, Training Loss: 0.79466710072711\n",
      "Epoch 25, Training Loss: 0.7939904530245558\n",
      "Epoch 26, Training Loss: 0.7933278882413879\n",
      "Epoch 27, Training Loss: 0.7927809447274172\n",
      "Epoch 28, Training Loss: 0.7918527165749916\n",
      "Epoch 29, Training Loss: 0.7920122618962051\n",
      "Epoch 30, Training Loss: 0.7906029167480039\n",
      "Epoch 31, Training Loss: 0.7910380727366397\n",
      "Epoch 32, Training Loss: 0.790070188672919\n",
      "Epoch 33, Training Loss: 0.7896614021824715\n",
      "Epoch 34, Training Loss: 0.7893624779873325\n",
      "Epoch 35, Training Loss: 0.7888696909847116\n",
      "Epoch 36, Training Loss: 0.7884892586478613\n",
      "Epoch 37, Training Loss: 0.7879682921825495\n",
      "Epoch 38, Training Loss: 0.7873893972626306\n",
      "Epoch 39, Training Loss: 0.7882300652955708\n",
      "Epoch 40, Training Loss: 0.7875678576921162\n",
      "Epoch 41, Training Loss: 0.7870986683924396\n",
      "Epoch 42, Training Loss: 0.7863759098196388\n",
      "Epoch 43, Training Loss: 0.7869910163090641\n",
      "Epoch 44, Training Loss: 0.7862349076378614\n",
      "Epoch 45, Training Loss: 0.7861656089474384\n",
      "Epoch 46, Training Loss: 0.7863170383568097\n",
      "Epoch 47, Training Loss: 0.7854058006652316\n",
      "Epoch 48, Training Loss: 0.7850758880600893\n",
      "Epoch 49, Training Loss: 0.7845501829807023\n",
      "Epoch 50, Training Loss: 0.7852918546002611\n",
      "Epoch 51, Training Loss: 0.7846703307072919\n",
      "Epoch 52, Training Loss: 0.784947113166178\n",
      "Epoch 53, Training Loss: 0.7846023661749704\n",
      "Epoch 54, Training Loss: 0.7843999857293036\n",
      "Epoch 55, Training Loss: 0.7835198705357717\n",
      "Epoch 56, Training Loss: 0.7846389334004624\n",
      "Epoch 57, Training Loss: 0.7832886401423834\n",
      "Epoch 58, Training Loss: 0.7831899114121171\n",
      "Epoch 59, Training Loss: 0.7836418096284221\n",
      "Epoch 60, Training Loss: 0.7832855480057853\n",
      "Epoch 61, Training Loss: 0.7837767696918402\n",
      "Epoch 62, Training Loss: 0.7830919681635118\n",
      "Epoch 63, Training Loss: 0.7832037512521098\n",
      "Epoch 64, Training Loss: 0.782803539763716\n",
      "Epoch 65, Training Loss: 0.783229903350199\n",
      "Epoch 66, Training Loss: 0.7829876609314653\n",
      "Epoch 67, Training Loss: 0.7830895133484576\n",
      "Epoch 68, Training Loss: 0.7829671978950501\n",
      "Epoch 69, Training Loss: 0.7823034853863536\n",
      "Epoch 70, Training Loss: 0.7825812948377509\n",
      "Epoch 71, Training Loss: 0.7823545314315566\n",
      "Epoch 72, Training Loss: 0.7820236501837136\n",
      "Epoch 73, Training Loss: 0.7821538422340737\n",
      "Epoch 74, Training Loss: 0.782433441348542\n",
      "Epoch 75, Training Loss: 0.7823634594006645\n",
      "Epoch 76, Training Loss: 0.781931057460326\n",
      "Epoch 77, Training Loss: 0.781772760251411\n",
      "Epoch 78, Training Loss: 0.7814313819982055\n",
      "Epoch 79, Training Loss: 0.7814694116886397\n",
      "Epoch 80, Training Loss: 0.7820181616266867\n",
      "Epoch 81, Training Loss: 0.7820226492738365\n",
      "Epoch 82, Training Loss: 0.7814311471200527\n",
      "Epoch 83, Training Loss: 0.7809588670730591\n",
      "Epoch 84, Training Loss: 0.7812259070855334\n",
      "Epoch 85, Training Loss: 0.7814897320324317\n",
      "Epoch 86, Training Loss: 0.7814900021804007\n",
      "Epoch 87, Training Loss: 0.7813842559667458\n",
      "Epoch 88, Training Loss: 0.7810790933164439\n",
      "Epoch 89, Training Loss: 0.7811354074263035\n",
      "Epoch 90, Training Loss: 0.7807382911667788\n",
      "Epoch 91, Training Loss: 0.7810226552468493\n",
      "Epoch 92, Training Loss: 0.7810638120299891\n",
      "Epoch 93, Training Loss: 0.7811602285930088\n",
      "Epoch 94, Training Loss: 0.7809965441997786\n",
      "Epoch 95, Training Loss: 0.7809520768043691\n",
      "Epoch 96, Training Loss: 0.7803042842481369\n",
      "Epoch 97, Training Loss: 0.7804125806442777\n",
      "Epoch 98, Training Loss: 0.780933524164042\n",
      "Epoch 99, Training Loss: 0.7809468228117864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:29:15,848] Trial 238 finished with value: 0.6342666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7811664172581264\n",
      "Epoch 1, Training Loss: 1.082485198133132\n",
      "Epoch 2, Training Loss: 1.061277361056384\n",
      "Epoch 3, Training Loss: 1.0419640463240007\n",
      "Epoch 4, Training Loss: 1.0231700210711536\n",
      "Epoch 5, Training Loss: 1.0052808049145867\n",
      "Epoch 6, Training Loss: 0.9894929880254408\n",
      "Epoch 7, Training Loss: 0.9768446360616123\n",
      "Epoch 8, Training Loss: 0.9675872983652003\n",
      "Epoch 9, Training Loss: 0.961216575678657\n",
      "Epoch 10, Training Loss: 0.9568971143049352\n",
      "Epoch 11, Training Loss: 0.9538879032695995\n",
      "Epoch 12, Training Loss: 0.9516452916930703\n",
      "Epoch 13, Training Loss: 0.9498486376509947\n",
      "Epoch 14, Training Loss: 0.9482743972189286\n",
      "Epoch 15, Training Loss: 0.9468201409367955\n",
      "Epoch 16, Training Loss: 0.94542124320479\n",
      "Epoch 17, Training Loss: 0.9440318846001344\n",
      "Epoch 18, Training Loss: 0.9426524153176477\n",
      "Epoch 19, Training Loss: 0.9412520618999706\n",
      "Epoch 20, Training Loss: 0.9398104969894184\n",
      "Epoch 21, Training Loss: 0.9383566577995525\n",
      "Epoch 22, Training Loss: 0.9368621415250442\n",
      "Epoch 23, Training Loss: 0.9353181257668663\n",
      "Epoch 24, Training Loss: 0.9337299049601835\n",
      "Epoch 25, Training Loss: 0.9320813022641574\n",
      "Epoch 26, Training Loss: 0.9303743229192846\n",
      "Epoch 27, Training Loss: 0.9286189930579242\n",
      "Epoch 28, Training Loss: 0.9267746220616734\n",
      "Epoch 29, Training Loss: 0.9248536017361809\n",
      "Epoch 30, Training Loss: 0.9228676191498252\n",
      "Epoch 31, Training Loss: 0.9207744798940771\n",
      "Epoch 32, Training Loss: 0.9185831161807565\n",
      "Epoch 33, Training Loss: 0.9162984198682449\n",
      "Epoch 34, Training Loss: 0.9138980326933019\n",
      "Epoch 35, Training Loss: 0.9113833991219016\n",
      "Epoch 36, Training Loss: 0.9087308550582213\n",
      "Epoch 37, Training Loss: 0.9059581206826602\n",
      "Epoch 38, Training Loss: 0.9030555176734925\n",
      "Epoch 39, Training Loss: 0.9000282543547014\n",
      "Epoch 40, Training Loss: 0.8968538815834943\n",
      "Epoch 41, Training Loss: 0.8935763162023881\n",
      "Epoch 42, Training Loss: 0.8901556850180906\n",
      "Epoch 43, Training Loss: 0.8866828478083891\n",
      "Epoch 44, Training Loss: 0.8830876029940212\n",
      "Epoch 45, Training Loss: 0.8794467307539547\n",
      "Epoch 46, Training Loss: 0.8757483339309693\n",
      "Epoch 47, Training Loss: 0.8720609901231878\n",
      "Epoch 48, Training Loss: 0.8683848999528324\n",
      "Epoch 49, Training Loss: 0.8647788509901833\n",
      "Epoch 50, Training Loss: 0.8612310764368842\n",
      "Epoch 51, Training Loss: 0.857798655944712\n",
      "Epoch 52, Training Loss: 0.8545005121651817\n",
      "Epoch 53, Training Loss: 0.8514150775881375\n",
      "Epoch 54, Training Loss: 0.848467961619882\n",
      "Epoch 55, Training Loss: 0.8457030416937436\n",
      "Epoch 56, Training Loss: 0.8431301999092102\n",
      "Epoch 57, Training Loss: 0.8407640078488519\n",
      "Epoch 58, Training Loss: 0.8385622867415933\n",
      "Epoch 59, Training Loss: 0.8366130138846005\n",
      "Epoch 60, Training Loss: 0.8348083595668568\n",
      "Epoch 61, Training Loss: 0.8331024322089027\n",
      "Epoch 62, Training Loss: 0.8315876784745385\n",
      "Epoch 63, Training Loss: 0.830244794102276\n",
      "Epoch 64, Training Loss: 0.8289803978976081\n",
      "Epoch 65, Training Loss: 0.8278107710445628\n",
      "Epoch 66, Training Loss: 0.8267226676379933\n",
      "Epoch 67, Training Loss: 0.8257753311886507\n",
      "Epoch 68, Training Loss: 0.824854333821465\n",
      "Epoch 69, Training Loss: 0.8240210634820602\n",
      "Epoch 70, Training Loss: 0.8232356694165398\n",
      "Epoch 71, Training Loss: 0.8225048504156225\n",
      "Epoch 72, Training Loss: 0.8218617255547468\n",
      "Epoch 73, Training Loss: 0.8211397791610044\n",
      "Epoch 74, Training Loss: 0.8205195896064534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:30:45,274] Trial 239 finished with value: 0.624 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8199408801864175\n",
      "Epoch 1, Training Loss: 1.0977286584237043\n",
      "Epoch 2, Training Loss: 1.090694996469161\n",
      "Epoch 3, Training Loss: 1.0904488274630377\n",
      "Epoch 4, Training Loss: 1.0901960473902086\n",
      "Epoch 5, Training Loss: 1.089953525907853\n",
      "Epoch 6, Training Loss: 1.089714566819808\n",
      "Epoch 7, Training Loss: 1.0894544869310716\n",
      "Epoch 8, Training Loss: 1.0892181473619797\n",
      "Epoch 9, Training Loss: 1.0889686827098621\n",
      "Epoch 10, Training Loss: 1.0887282774027656\n",
      "Epoch 11, Training Loss: 1.0884720805112054\n",
      "Epoch 12, Training Loss: 1.0882073302829967\n",
      "Epoch 13, Training Loss: 1.087968719847062\n",
      "Epoch 14, Training Loss: 1.0877098516856922\n",
      "Epoch 15, Training Loss: 1.0874551129341126\n",
      "Epoch 16, Training Loss: 1.0871917288443622\n",
      "Epoch 17, Training Loss: 1.0869064378738402\n",
      "Epoch 18, Training Loss: 1.0866596684736365\n",
      "Epoch 19, Training Loss: 1.0863834740133846\n",
      "Epoch 20, Training Loss: 1.0861075364842134\n",
      "Epoch 21, Training Loss: 1.0858297979130465\n",
      "Epoch 22, Training Loss: 1.0855528417755576\n",
      "Epoch 23, Training Loss: 1.08524164648617\n",
      "Epoch 24, Training Loss: 1.0849833311754113\n",
      "Epoch 25, Training Loss: 1.0846789668588077\n",
      "Epoch 26, Training Loss: 1.084369767553666\n",
      "Epoch 27, Training Loss: 1.0840847579170676\n",
      "Epoch 28, Training Loss: 1.0837696571911082\n",
      "Epoch 29, Training Loss: 1.083442820520962\n",
      "Epoch 30, Training Loss: 1.0831320409213796\n",
      "Epoch 31, Training Loss: 1.0827647691614488\n",
      "Epoch 32, Training Loss: 1.0824631575977102\n",
      "Epoch 33, Training Loss: 1.0821234200982486\n",
      "Epoch 34, Training Loss: 1.0817569157656501\n",
      "Epoch 35, Training Loss: 1.0814041189586414\n",
      "Epoch 36, Training Loss: 1.0810339077781228\n",
      "Epoch 37, Training Loss: 1.0806649032761069\n",
      "Epoch 38, Training Loss: 1.0802877442977008\n",
      "Epoch 39, Training Loss: 1.0798904803219964\n",
      "Epoch 40, Training Loss: 1.079482231981614\n",
      "Epoch 41, Training Loss: 1.0790897865856395\n",
      "Epoch 42, Training Loss: 1.078644110735725\n",
      "Epoch 43, Training Loss: 1.0782104336514193\n",
      "Epoch 44, Training Loss: 1.077794131531435\n",
      "Epoch 45, Training Loss: 1.0773292596199933\n",
      "Epoch 46, Training Loss: 1.076870439753813\n",
      "Epoch 47, Training Loss: 1.0764024730289683\n",
      "Epoch 48, Training Loss: 1.075912723681506\n",
      "Epoch 49, Training Loss: 1.075412773104275\n",
      "Epoch 50, Training Loss: 1.0749122176450843\n",
      "Epoch 51, Training Loss: 1.0743685145939097\n",
      "Epoch 52, Training Loss: 1.0738359452696407\n",
      "Epoch 53, Training Loss: 1.0732770994130303\n",
      "Epoch 54, Training Loss: 1.0727257874432732\n",
      "Epoch 55, Training Loss: 1.072137095226961\n",
      "Epoch 56, Training Loss: 1.0715333910549387\n",
      "Epoch 57, Training Loss: 1.0709410419183618\n",
      "Epoch 58, Training Loss: 1.0703017505477457\n",
      "Epoch 59, Training Loss: 1.0696672251645256\n",
      "Epoch 60, Training Loss: 1.0689964401020724\n",
      "Epoch 61, Training Loss: 1.0683361908968758\n",
      "Epoch 62, Training Loss: 1.0676326605852913\n",
      "Epoch 63, Training Loss: 1.0669395265859716\n",
      "Epoch 64, Training Loss: 1.0662094266274396\n",
      "Epoch 65, Training Loss: 1.06546045850305\n",
      "Epoch 66, Training Loss: 1.0646907889141757\n",
      "Epoch 67, Training Loss: 1.0639118186165304\n",
      "Epoch 68, Training Loss: 1.0631150667807636\n",
      "Epoch 69, Training Loss: 1.0623013384201947\n",
      "Epoch 70, Training Loss: 1.0614604630189783\n",
      "Epoch 71, Training Loss: 1.0606030656309688\n",
      "Epoch 72, Training Loss: 1.0597271337228662\n",
      "Epoch 73, Training Loss: 1.058829651299645\n",
      "Epoch 74, Training Loss: 1.0579141169435837\n",
      "Epoch 75, Training Loss: 1.0569714196990518\n",
      "Epoch 76, Training Loss: 1.0560168661790736\n",
      "Epoch 77, Training Loss: 1.055053824116202\n",
      "Epoch 78, Training Loss: 1.0540647177135243\n",
      "Epoch 79, Training Loss: 1.0530413052614997\n",
      "Epoch 80, Training Loss: 1.0520223460477942\n",
      "Epoch 81, Training Loss: 1.050979656191433\n",
      "Epoch 82, Training Loss: 1.0499160986788132\n",
      "Epoch 83, Training Loss: 1.048830113551196\n",
      "Epoch 84, Training Loss: 1.0477283390830545\n",
      "Epoch 85, Training Loss: 1.0466132312662462\n",
      "Epoch 86, Training Loss: 1.0454873671251186\n",
      "Epoch 87, Training Loss: 1.0443228805766387\n",
      "Epoch 88, Training Loss: 1.0431709743948543\n",
      "Epoch 89, Training Loss: 1.0420011727248921\n",
      "Epoch 90, Training Loss: 1.0408161945904002\n",
      "Epoch 91, Training Loss: 1.0396140046680675\n",
      "Epoch 92, Training Loss: 1.038416388946421\n",
      "Epoch 93, Training Loss: 1.0371986694896922\n",
      "Epoch 94, Training Loss: 1.0359748639779933\n",
      "Epoch 95, Training Loss: 1.0347465756360221\n",
      "Epoch 96, Training Loss: 1.0335169805498685\n",
      "Epoch 97, Training Loss: 1.0322673062717214\n",
      "Epoch 98, Training Loss: 1.031024879988502\n",
      "Epoch 99, Training Loss: 1.029777425667819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:32:32,424] Trial 240 finished with value: 0.46313333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.0285563921928407\n",
      "Epoch 1, Training Loss: 1.0047629794653725\n",
      "Epoch 2, Training Loss: 0.8941106032623964\n",
      "Epoch 3, Training Loss: 0.8850934723545524\n",
      "Epoch 4, Training Loss: 0.8762926645138684\n",
      "Epoch 5, Training Loss: 0.8745211412626154\n",
      "Epoch 6, Training Loss: 0.8753132516496321\n",
      "Epoch 7, Training Loss: 0.8702464063027325\n",
      "Epoch 8, Training Loss: 0.8694561330009909\n",
      "Epoch 9, Training Loss: 0.87162368164343\n",
      "Epoch 10, Training Loss: 0.8718130506487454\n",
      "Epoch 11, Training Loss: 0.8697015927118413\n",
      "Epoch 12, Training Loss: 0.8680495508979349\n",
      "Epoch 13, Training Loss: 0.8676551824457506\n",
      "Epoch 14, Training Loss: 0.8691585880868575\n",
      "Epoch 15, Training Loss: 0.8664736722497379\n",
      "Epoch 16, Training Loss: 0.868075942993164\n",
      "Epoch 17, Training Loss: 0.8676923148071065\n",
      "Epoch 18, Training Loss: 0.8673422626888051\n",
      "Epoch 19, Training Loss: 0.8682555740019854\n",
      "Epoch 20, Training Loss: 0.867413435122546\n",
      "Epoch 21, Training Loss: 0.8680660243595347\n",
      "Epoch 22, Training Loss: 0.8669558365204755\n",
      "Epoch 23, Training Loss: 0.8671486234664917\n",
      "Epoch 24, Training Loss: 0.8741969339987811\n",
      "Epoch 25, Training Loss: 0.8701395703764523\n",
      "Epoch 26, Training Loss: 0.8698264169692993\n",
      "Epoch 27, Training Loss: 0.8700847134870642\n",
      "Epoch 28, Training Loss: 0.8725524189892937\n",
      "Epoch 29, Training Loss: 0.8678807554525487\n",
      "Epoch 30, Training Loss: 0.867739242736031\n",
      "Epoch 31, Training Loss: 0.8640825568227207\n",
      "Epoch 32, Training Loss: 0.8650571078412673\n",
      "Epoch 33, Training Loss: 0.8739845193834865\n",
      "Epoch 34, Training Loss: 0.8738229696189656\n",
      "Epoch 35, Training Loss: 0.8686583622764139\n",
      "Epoch 36, Training Loss: 0.867065946635078\n",
      "Epoch 37, Training Loss: 0.8610882465979632\n",
      "Epoch 38, Training Loss: 0.873806654845967\n",
      "Epoch 39, Training Loss: 0.8714634321016423\n",
      "Epoch 40, Training Loss: 0.8684882357541253\n",
      "Epoch 41, Training Loss: 0.8706723814852098\n",
      "Epoch 42, Training Loss: 0.8702050547038808\n",
      "Epoch 43, Training Loss: 0.8688564820850597\n",
      "Epoch 44, Training Loss: 0.872332826572306\n",
      "Epoch 45, Training Loss: 0.8744143838040969\n",
      "Epoch 46, Training Loss: 0.8697926690999199\n",
      "Epoch 47, Training Loss: 0.8681495987667757\n",
      "Epoch 48, Training Loss: 0.8695783854933345\n",
      "Epoch 49, Training Loss: 0.8645254795691546\n",
      "Epoch 50, Training Loss: 0.8671028126688565\n",
      "Epoch 51, Training Loss: 0.8688418278273414\n",
      "Epoch 52, Training Loss: 0.8624061123062583\n",
      "Epoch 53, Training Loss: 0.8694325758429134\n",
      "Epoch 54, Training Loss: 0.8680222631903256\n",
      "Epoch 55, Training Loss: 0.8731257624485913\n",
      "Epoch 56, Training Loss: 0.8717755820470697\n",
      "Epoch 57, Training Loss: 0.8684250504129073\n",
      "Epoch 58, Training Loss: 0.8653104606796713\n",
      "Epoch 59, Training Loss: 0.8740774102771983\n",
      "Epoch 60, Training Loss: 0.8666871732823989\n",
      "Epoch 61, Training Loss: 0.8641057264103609\n",
      "Epoch 62, Training Loss: 0.865423939929289\n",
      "Epoch 63, Training Loss: 0.8707206442075617\n",
      "Epoch 64, Training Loss: 0.8674805421688977\n",
      "Epoch 65, Training Loss: 0.8705686773272122\n",
      "Epoch 66, Training Loss: 0.8802723140576306\n",
      "Epoch 67, Training Loss: 0.8629308978249045\n",
      "Epoch 68, Training Loss: 0.868879961827222\n",
      "Epoch 69, Training Loss: 0.8822516735161052\n",
      "Epoch 70, Training Loss: 0.8712110089554507\n",
      "Epoch 71, Training Loss: 0.8775466019967023\n",
      "Epoch 72, Training Loss: 0.8676854531203999\n",
      "Epoch 73, Training Loss: 0.8711451312373666\n",
      "Epoch 74, Training Loss: 0.8680687414197361\n",
      "Epoch 75, Training Loss: 0.8710607423501856\n",
      "Epoch 76, Training Loss: 0.8755301563178792\n",
      "Epoch 77, Training Loss: 0.8720583734792822\n",
      "Epoch 78, Training Loss: 0.8675590563521666\n",
      "Epoch 79, Training Loss: 0.8678669451264774\n",
      "Epoch 80, Training Loss: 0.869787242272321\n",
      "Epoch 81, Training Loss: 0.8756293531025158\n",
      "Epoch 82, Training Loss: 0.8699795659850625\n",
      "Epoch 83, Training Loss: 0.8725585548316731\n",
      "Epoch 84, Training Loss: 0.8657341893280254\n",
      "Epoch 85, Training Loss: 0.8724973307637607\n",
      "Epoch 86, Training Loss: 0.8712601151185877\n",
      "Epoch 87, Training Loss: 0.8705849722553702\n",
      "Epoch 88, Training Loss: 0.8634389853477478\n",
      "Epoch 89, Training Loss: 0.8652064116562114\n",
      "Epoch 90, Training Loss: 0.8641282048646142\n",
      "Epoch 91, Training Loss: 0.8693742438624887\n",
      "Epoch 92, Training Loss: 0.8653349791554844\n",
      "Epoch 93, Training Loss: 0.8680102626015158\n",
      "Epoch 94, Training Loss: 0.8683615476944867\n",
      "Epoch 95, Training Loss: 0.8653656627851374\n",
      "Epoch 96, Training Loss: 0.8682284512239344\n",
      "Epoch 97, Training Loss: 0.8633395608032451\n",
      "Epoch 98, Training Loss: 0.8690054580744575\n",
      "Epoch 99, Training Loss: 0.8745026289715486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:34:54,516] Trial 241 finished with value: 0.604 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8722388519960291\n",
      "Epoch 1, Training Loss: 0.8775630432016709\n",
      "Epoch 2, Training Loss: 0.8559206119705649\n",
      "Epoch 3, Training Loss: 0.8534798089896931\n",
      "Epoch 4, Training Loss: 0.8518952199992011\n",
      "Epoch 5, Training Loss: 0.8564422110950246\n",
      "Epoch 6, Training Loss: 0.8601496052040773\n",
      "Epoch 7, Training Loss: 0.8601320174161126\n",
      "Epoch 8, Training Loss: 0.8565295389119316\n",
      "Epoch 9, Training Loss: 0.8564949997733621\n",
      "Epoch 10, Training Loss: 0.861442657919491\n",
      "Epoch 11, Training Loss: 0.857463910299189\n",
      "Epoch 12, Training Loss: 0.8574651267248041\n",
      "Epoch 13, Training Loss: 0.8590252430298749\n",
      "Epoch 14, Training Loss: 0.8590776478543001\n",
      "Epoch 15, Training Loss: 0.8630143318456762\n",
      "Epoch 16, Training Loss: 0.8521780655664556\n",
      "Epoch 17, Training Loss: 0.8570181749848759\n",
      "Epoch 18, Training Loss: 0.8567241456228144\n",
      "Epoch 19, Training Loss: 0.857381664724911\n",
      "Epoch 20, Training Loss: 0.8491322929718915\n",
      "Epoch 21, Training Loss: 0.8536879828396966\n",
      "Epoch 22, Training Loss: 0.8554654112984152\n",
      "Epoch 23, Training Loss: 0.8643541188801036\n",
      "Epoch 24, Training Loss: 0.8558045205649207\n",
      "Epoch 25, Training Loss: 0.8520282119863174\n",
      "Epoch 26, Training Loss: 0.857690385089201\n",
      "Epoch 27, Training Loss: 0.8564430977316464\n",
      "Epoch 28, Training Loss: 0.8618835740229662\n",
      "Epoch 29, Training Loss: 0.8603603279590607\n",
      "Epoch 30, Training Loss: 0.860197001695633\n",
      "Epoch 31, Training Loss: 0.8676098057802986\n",
      "Epoch 32, Training Loss: 0.8626925756650813\n",
      "Epoch 33, Training Loss: 0.8602270305156707\n",
      "Epoch 34, Training Loss: 0.8579057857569526\n",
      "Epoch 35, Training Loss: 0.8639275289984311\n",
      "Epoch 36, Training Loss: 0.8601542192346909\n",
      "Epoch 37, Training Loss: 0.8631148325695711\n",
      "Epoch 38, Training Loss: 0.8535525436261121\n",
      "Epoch 39, Training Loss: 0.8533082420685713\n",
      "Epoch 40, Training Loss: 0.8622364325383131\n",
      "Epoch 41, Training Loss: 0.8588955931102529\n",
      "Epoch 42, Training Loss: 0.8494194300034467\n",
      "Epoch 43, Training Loss: 0.8581152269419502\n",
      "Epoch 44, Training Loss: 0.8554943557346568\n",
      "Epoch 45, Training Loss: 0.8587540449114407\n",
      "Epoch 46, Training Loss: 0.8507021204864278\n",
      "Epoch 47, Training Loss: 0.8584563321225783\n",
      "Epoch 48, Training Loss: 0.8568838283594917\n",
      "Epoch 49, Training Loss: 0.8578479935842401\n",
      "Epoch 50, Training Loss: 0.8611188165580526\n",
      "Epoch 51, Training Loss: 0.8672225097347709\n",
      "Epoch 52, Training Loss: 0.8521232123935923\n",
      "Epoch 53, Training Loss: 0.8540068345210131\n",
      "Epoch 54, Training Loss: 0.8522053325176239\n",
      "Epoch 55, Training Loss: 0.8479199687873616\n",
      "Epoch 56, Training Loss: 0.8476970692241893\n",
      "Epoch 57, Training Loss: 0.8531661915779114\n",
      "Epoch 58, Training Loss: 0.8578823607809404\n",
      "Epoch 59, Training Loss: 0.8563849985599518\n",
      "Epoch 60, Training Loss: 0.8541058661657221\n",
      "Epoch 61, Training Loss: 0.8558866901257459\n",
      "Epoch 62, Training Loss: 0.8661349755174973\n",
      "Epoch 63, Training Loss: 0.8642943957973929\n",
      "Epoch 64, Training Loss: 0.8616034741962657\n",
      "Epoch 65, Training Loss: 0.858899759124307\n",
      "Epoch 66, Training Loss: 0.8591998421444612\n",
      "Epoch 67, Training Loss: 0.8592575904201059\n",
      "Epoch 68, Training Loss: 0.8647955950568704\n",
      "Epoch 69, Training Loss: 0.8524169191192178\n",
      "Epoch 70, Training Loss: 0.8512325829618117\n",
      "Epoch 71, Training Loss: 0.8510510197807761\n",
      "Epoch 72, Training Loss: 0.860940018752042\n",
      "Epoch 73, Training Loss: 0.8578385017899905\n",
      "Epoch 74, Training Loss: 0.8543764303712283\n",
      "Epoch 75, Training Loss: 0.8572558166700252\n",
      "Epoch 76, Training Loss: 0.853759268171647\n",
      "Epoch 77, Training Loss: 0.8708827861617593\n",
      "Epoch 78, Training Loss: 0.865958068300696\n",
      "Epoch 79, Training Loss: 0.8564072687485639\n",
      "Epoch 80, Training Loss: 0.8523076677322388\n",
      "Epoch 81, Training Loss: 0.8511051763506496\n",
      "Epoch 82, Training Loss: 0.8453840728367076\n",
      "Epoch 83, Training Loss: 0.8537915985023274\n",
      "Epoch 84, Training Loss: 0.8518287308777079\n",
      "Epoch 85, Training Loss: 0.8614881254644955\n",
      "Epoch 86, Training Loss: 0.8524019654358135\n",
      "Epoch 87, Training Loss: 0.8549057041897493\n",
      "Epoch 88, Training Loss: 0.855022553135367\n",
      "Epoch 89, Training Loss: 0.8604590509218328\n",
      "Epoch 90, Training Loss: 0.8616985919195063\n",
      "Epoch 91, Training Loss: 0.8536641736591564\n",
      "Epoch 92, Training Loss: 0.8595049128111671\n",
      "Epoch 93, Training Loss: 0.8533631542850943\n",
      "Epoch 94, Training Loss: 0.8565598311844994\n",
      "Epoch 95, Training Loss: 0.8558842216519749\n",
      "Epoch 96, Training Loss: 0.8630846951989567\n",
      "Epoch 97, Training Loss: 0.859252286378075\n",
      "Epoch 98, Training Loss: 0.8619199125907\n",
      "Epoch 99, Training Loss: 0.8616194369512445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:37:30,839] Trial 242 finished with value: 0.6088666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8591233824281131\n",
      "Epoch 1, Training Loss: 0.8853534047042623\n",
      "Epoch 2, Training Loss: 0.8308636726351345\n",
      "Epoch 3, Training Loss: 0.8214647011897144\n",
      "Epoch 4, Training Loss: 0.8173522658207837\n",
      "Epoch 5, Training Loss: 0.8136252039320329\n",
      "Epoch 6, Training Loss: 0.8113151867249433\n",
      "Epoch 7, Training Loss: 0.8101677410041584\n",
      "Epoch 8, Training Loss: 0.8086462981560651\n",
      "Epoch 9, Training Loss: 0.8081793815949384\n",
      "Epoch 10, Training Loss: 0.8080865780746236\n",
      "Epoch 11, Training Loss: 0.8076385077308206\n",
      "Epoch 12, Training Loss: 0.8070515270092908\n",
      "Epoch 13, Training Loss: 0.8061480982163373\n",
      "Epoch 14, Training Loss: 0.8055520076611463\n",
      "Epoch 15, Training Loss: 0.8052054488658905\n",
      "Epoch 16, Training Loss: 0.8043720526555005\n",
      "Epoch 17, Training Loss: 0.8049708197397344\n",
      "Epoch 18, Training Loss: 0.8037314009666443\n",
      "Epoch 19, Training Loss: 0.8039397828719195\n",
      "Epoch 20, Training Loss: 0.802191650797339\n",
      "Epoch 21, Training Loss: 0.8027767723448136\n",
      "Epoch 22, Training Loss: 0.8031179640573614\n",
      "Epoch 23, Training Loss: 0.8027630301784067\n",
      "Epoch 24, Training Loss: 0.8021429753303528\n",
      "Epoch 25, Training Loss: 0.8017393179500805\n",
      "Epoch 26, Training Loss: 0.8015745461688322\n",
      "Epoch 27, Training Loss: 0.8009598807026358\n",
      "Epoch 28, Training Loss: 0.8013823385098401\n",
      "Epoch 29, Training Loss: 0.8017583651402417\n",
      "Epoch 30, Training Loss: 0.8015965522036833\n",
      "Epoch 31, Training Loss: 0.8017014474027297\n",
      "Epoch 32, Training Loss: 0.8013778542069828\n",
      "Epoch 33, Training Loss: 0.8007603348703946\n",
      "Epoch 34, Training Loss: 0.8012600713617661\n",
      "Epoch 35, Training Loss: 0.8015375095255235\n",
      "Epoch 36, Training Loss: 0.8014121941257926\n",
      "Epoch 37, Training Loss: 0.8013203662984512\n",
      "Epoch 38, Training Loss: 0.8003839147090912\n",
      "Epoch 39, Training Loss: 0.8006268737596623\n",
      "Epoch 40, Training Loss: 0.8012172210917753\n",
      "Epoch 41, Training Loss: 0.801072342746398\n",
      "Epoch 42, Training Loss: 0.8003163359445684\n",
      "Epoch 43, Training Loss: 0.8002622811934528\n",
      "Epoch 44, Training Loss: 0.801174140256994\n",
      "Epoch 45, Training Loss: 0.8003219995779149\n",
      "Epoch 46, Training Loss: 0.8011670704448924\n",
      "Epoch 47, Training Loss: 0.80046927578309\n",
      "Epoch 48, Training Loss: 0.8009478794827181\n",
      "Epoch 49, Training Loss: 0.7997966880658094\n",
      "Epoch 50, Training Loss: 0.8010432274201337\n",
      "Epoch 51, Training Loss: 0.8000045198552749\n",
      "Epoch 52, Training Loss: 0.8004432789718403\n",
      "Epoch 53, Training Loss: 0.80017358324107\n",
      "Epoch 54, Training Loss: 0.7999447666897493\n",
      "Epoch 55, Training Loss: 0.8002200442201951\n",
      "Epoch 56, Training Loss: 0.7998996292843538\n",
      "Epoch 57, Training Loss: 0.8002911651134491\n",
      "Epoch 58, Training Loss: 0.8006436371102053\n",
      "Epoch 59, Training Loss: 0.799558352652718\n",
      "Epoch 60, Training Loss: 0.7997498543823467\n",
      "Epoch 61, Training Loss: 0.8000131918402279\n",
      "Epoch 62, Training Loss: 0.8001080463914311\n",
      "Epoch 63, Training Loss: 0.7995497543671551\n",
      "Epoch 64, Training Loss: 0.7998245267307057\n",
      "Epoch 65, Training Loss: 0.7998140006205615\n",
      "Epoch 66, Training Loss: 0.799481969019946\n",
      "Epoch 67, Training Loss: 0.8004492733057808\n",
      "Epoch 68, Training Loss: 0.7996613465337192\n",
      "Epoch 69, Training Loss: 0.7993803477287292\n",
      "Epoch 70, Training Loss: 0.7995497751937193\n",
      "Epoch 71, Training Loss: 0.799726631711511\n",
      "Epoch 72, Training Loss: 0.7993619097681607\n",
      "Epoch 73, Training Loss: 0.8003533044983359\n",
      "Epoch 74, Training Loss: 0.7996043230505551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:39:08,419] Trial 243 finished with value: 0.6369333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7995110076315263\n",
      "Epoch 1, Training Loss: 0.9475822176252092\n",
      "Epoch 2, Training Loss: 0.8943482445146804\n",
      "Epoch 3, Training Loss: 0.8548064124315304\n",
      "Epoch 4, Training Loss: 0.825310864932555\n",
      "Epoch 5, Training Loss: 0.8121505757023517\n",
      "Epoch 6, Training Loss: 0.806947251997496\n",
      "Epoch 7, Training Loss: 0.8050845078955916\n",
      "Epoch 8, Training Loss: 0.8035779812282189\n",
      "Epoch 9, Training Loss: 0.8026115840539\n",
      "Epoch 10, Training Loss: 0.801992098191627\n",
      "Epoch 11, Training Loss: 0.8014436432293484\n",
      "Epoch 12, Training Loss: 0.800655842813334\n",
      "Epoch 13, Training Loss: 0.8007746157789589\n",
      "Epoch 14, Training Loss: 0.8003928179131414\n",
      "Epoch 15, Training Loss: 0.7999698195242344\n",
      "Epoch 16, Training Loss: 0.7992800579035193\n",
      "Epoch 17, Training Loss: 0.7995517442100927\n",
      "Epoch 18, Training Loss: 0.7991419404072869\n",
      "Epoch 19, Training Loss: 0.7988886705914834\n",
      "Epoch 20, Training Loss: 0.7980701471629895\n",
      "Epoch 21, Training Loss: 0.7986091108250438\n",
      "Epoch 22, Training Loss: 0.7980700867516654\n",
      "Epoch 23, Training Loss: 0.7973469062407214\n",
      "Epoch 24, Training Loss: 0.7973897687474588\n",
      "Epoch 25, Training Loss: 0.7986500947098983\n",
      "Epoch 26, Training Loss: 0.7974131131530704\n",
      "Epoch 27, Training Loss: 0.7968778316239665\n",
      "Epoch 28, Training Loss: 0.7967523596340552\n",
      "Epoch 29, Training Loss: 0.7970152013283923\n",
      "Epoch 30, Training Loss: 0.7972020478176891\n",
      "Epoch 31, Training Loss: 0.7968448551973902\n",
      "Epoch 32, Training Loss: 0.7961390459447875\n",
      "Epoch 33, Training Loss: 0.7959846490307858\n",
      "Epoch 34, Training Loss: 0.79542610457069\n",
      "Epoch 35, Training Loss: 0.7957693177954595\n",
      "Epoch 36, Training Loss: 0.7952104546969995\n",
      "Epoch 37, Training Loss: 0.7957578382993999\n",
      "Epoch 38, Training Loss: 0.7951734809050882\n",
      "Epoch 39, Training Loss: 0.794629514844794\n",
      "Epoch 40, Training Loss: 0.794762543000673\n",
      "Epoch 41, Training Loss: 0.7951188848430949\n",
      "Epoch 42, Training Loss: 0.7942187783413364\n",
      "Epoch 43, Training Loss: 0.7943358823769074\n",
      "Epoch 44, Training Loss: 0.7938323526454151\n",
      "Epoch 45, Training Loss: 0.7937873661966253\n",
      "Epoch 46, Training Loss: 0.7936554375447725\n",
      "Epoch 47, Training Loss: 0.7942199295624754\n",
      "Epoch 48, Training Loss: 0.7929151829920317\n",
      "Epoch 49, Training Loss: 0.7929195192523468\n",
      "Epoch 50, Training Loss: 0.7926049856314982\n",
      "Epoch 51, Training Loss: 0.7927655600067368\n",
      "Epoch 52, Training Loss: 0.7923767563095666\n",
      "Epoch 53, Training Loss: 0.7924748171541027\n",
      "Epoch 54, Training Loss: 0.7920003954629252\n",
      "Epoch 55, Training Loss: 0.7918616172962619\n",
      "Epoch 56, Training Loss: 0.7920385175181511\n",
      "Epoch 57, Training Loss: 0.7919707534008457\n",
      "Epoch 58, Training Loss: 0.7915499264136293\n",
      "Epoch 59, Training Loss: 0.7919911115689385\n",
      "Epoch 60, Training Loss: 0.7913713476711646\n",
      "Epoch 61, Training Loss: 0.7917983384956991\n",
      "Epoch 62, Training Loss: 0.7906093529292515\n",
      "Epoch 63, Training Loss: 0.7907258184332596\n",
      "Epoch 64, Training Loss: 0.7909621030764472\n",
      "Epoch 65, Training Loss: 0.7902580908366612\n",
      "Epoch 66, Training Loss: 0.7906859381754595\n",
      "Epoch 67, Training Loss: 0.7905067134620551\n",
      "Epoch 68, Training Loss: 0.7904687452137023\n",
      "Epoch 69, Training Loss: 0.7907555924322373\n",
      "Epoch 70, Training Loss: 0.7905881385157879\n",
      "Epoch 71, Training Loss: 0.7904950734367944\n",
      "Epoch 72, Training Loss: 0.790959958653701\n",
      "Epoch 73, Training Loss: 0.7904331169630352\n",
      "Epoch 74, Training Loss: 0.7901059599747335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:40:21,597] Trial 244 finished with value: 0.6363333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7908063312222187\n",
      "Epoch 1, Training Loss: 0.8462247265908951\n",
      "Epoch 2, Training Loss: 0.814487298001024\n",
      "Epoch 3, Training Loss: 0.8117523928333942\n",
      "Epoch 4, Training Loss: 0.8072319376737551\n",
      "Epoch 5, Training Loss: 0.8059332575116839\n",
      "Epoch 6, Training Loss: 0.8043648257291407\n",
      "Epoch 7, Training Loss: 0.8020204242010762\n",
      "Epoch 8, Training Loss: 0.8014514107453196\n",
      "Epoch 9, Training Loss: 0.7989515643370779\n",
      "Epoch 10, Training Loss: 0.8010775109879056\n",
      "Epoch 11, Training Loss: 0.8002439958708627\n",
      "Epoch 12, Training Loss: 0.7993448717253548\n",
      "Epoch 13, Training Loss: 0.7988023684437113\n",
      "Epoch 14, Training Loss: 0.8010270411806896\n",
      "Epoch 15, Training Loss: 0.7987878077908566\n",
      "Epoch 16, Training Loss: 0.7982606679873359\n",
      "Epoch 17, Training Loss: 0.8000452860853726\n",
      "Epoch 18, Training Loss: 0.7992589506887852\n",
      "Epoch 19, Training Loss: 0.7973858064278624\n",
      "Epoch 20, Training Loss: 0.7976867127239256\n",
      "Epoch 21, Training Loss: 0.7968844999944357\n",
      "Epoch 22, Training Loss: 0.7984353112995176\n",
      "Epoch 23, Training Loss: 0.7973606300533266\n",
      "Epoch 24, Training Loss: 0.7957092918847737\n",
      "Epoch 25, Training Loss: 0.797093143140463\n",
      "Epoch 26, Training Loss: 0.7971238276115934\n",
      "Epoch 27, Training Loss: 0.7953314369782469\n",
      "Epoch 28, Training Loss: 0.7970663525108108\n",
      "Epoch 29, Training Loss: 0.7962463895181068\n",
      "Epoch 30, Training Loss: 0.7961376161503613\n",
      "Epoch 31, Training Loss: 0.7973202408704543\n",
      "Epoch 32, Training Loss: 0.7953205885743736\n",
      "Epoch 33, Training Loss: 0.7970087037946945\n",
      "Epoch 34, Training Loss: 0.7957851940527895\n",
      "Epoch 35, Training Loss: 0.7948439751352583\n",
      "Epoch 36, Training Loss: 0.7967026910387484\n",
      "Epoch 37, Training Loss: 0.7955260724949658\n",
      "Epoch 38, Training Loss: 0.7950421707074445\n",
      "Epoch 39, Training Loss: 0.796053715935327\n",
      "Epoch 40, Training Loss: 0.7970047969567148\n",
      "Epoch 41, Training Loss: 0.796209854828684\n",
      "Epoch 42, Training Loss: 0.7950285319098853\n",
      "Epoch 43, Training Loss: 0.7953832853109317\n",
      "Epoch 44, Training Loss: 0.7966001059775962\n",
      "Epoch 45, Training Loss: 0.7956985714740323\n",
      "Epoch 46, Training Loss: 0.7955393719493895\n",
      "Epoch 47, Training Loss: 0.7941821671966324\n",
      "Epoch 48, Training Loss: 0.7968561722819967\n",
      "Epoch 49, Training Loss: 0.7941353296875058\n",
      "Epoch 50, Training Loss: 0.7962399508720054\n",
      "Epoch 51, Training Loss: 0.7949539541301871\n",
      "Epoch 52, Training Loss: 0.7931169028568985\n",
      "Epoch 53, Training Loss: 0.7950082832709291\n",
      "Epoch 54, Training Loss: 0.7944167792348933\n",
      "Epoch 55, Training Loss: 0.795800308894394\n",
      "Epoch 56, Training Loss: 0.7946969760987992\n",
      "Epoch 57, Training Loss: 0.7936000447524222\n",
      "Epoch 58, Training Loss: 0.7951273997027175\n",
      "Epoch 59, Training Loss: 0.7946538941304486\n",
      "Epoch 60, Training Loss: 0.794575830240895\n",
      "Epoch 61, Training Loss: 0.7948308460694506\n",
      "Epoch 62, Training Loss: 0.7950254275386495\n",
      "Epoch 63, Training Loss: 0.792956667018116\n",
      "Epoch 64, Training Loss: 0.7940197236555859\n",
      "Epoch 65, Training Loss: 0.7936845959577346\n",
      "Epoch 66, Training Loss: 0.7946218790864585\n",
      "Epoch 67, Training Loss: 0.795372410017745\n",
      "Epoch 68, Training Loss: 0.7942417836727056\n",
      "Epoch 69, Training Loss: 0.7965094178242791\n",
      "Epoch 70, Training Loss: 0.7945292557092537\n",
      "Epoch 71, Training Loss: 0.7936395544754832\n",
      "Epoch 72, Training Loss: 0.7956219506442995\n",
      "Epoch 73, Training Loss: 0.7944457670799772\n",
      "Epoch 74, Training Loss: 0.795081277090804\n",
      "Epoch 75, Training Loss: 0.7937505763276179\n",
      "Epoch 76, Training Loss: 0.7934732946238123\n",
      "Epoch 77, Training Loss: 0.7950598092007458\n",
      "Epoch 78, Training Loss: 0.7954157137333002\n",
      "Epoch 79, Training Loss: 0.7937776684761048\n",
      "Epoch 80, Training Loss: 0.7946825758855146\n",
      "Epoch 81, Training Loss: 0.79548943204091\n",
      "Epoch 82, Training Loss: 0.794216761822091\n",
      "Epoch 83, Training Loss: 0.7938621866971927\n",
      "Epoch 84, Training Loss: 0.7961256651950062\n",
      "Epoch 85, Training Loss: 0.794941981125595\n",
      "Epoch 86, Training Loss: 0.7947943120970762\n",
      "Epoch 87, Training Loss: 0.7948964055319477\n",
      "Epoch 88, Training Loss: 0.7937406247719786\n",
      "Epoch 89, Training Loss: 0.795554004217449\n",
      "Epoch 90, Training Loss: 0.7935499757752382\n",
      "Epoch 91, Training Loss: 0.79387812363474\n",
      "Epoch 92, Training Loss: 0.7943524999726087\n",
      "Epoch 93, Training Loss: 0.7951926283370283\n",
      "Epoch 94, Training Loss: 0.7942254945747834\n",
      "Epoch 95, Training Loss: 0.7935474077561744\n",
      "Epoch 96, Training Loss: 0.7954572369281511\n",
      "Epoch 97, Training Loss: 0.7938376060105804\n",
      "Epoch 98, Training Loss: 0.7953849534343059\n",
      "Epoch 99, Training Loss: 0.795337840757872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:42:31,208] Trial 245 finished with value: 0.639 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7920905160724668\n",
      "Epoch 1, Training Loss: 1.0899977698045618\n",
      "Epoch 2, Training Loss: 1.0557403941715464\n",
      "Epoch 3, Training Loss: 1.033505608965369\n",
      "Epoch 4, Training Loss: 1.0162894227224237\n",
      "Epoch 5, Training Loss: 1.002121970022426\n",
      "Epoch 6, Training Loss: 0.9904666590690613\n",
      "Epoch 7, Training Loss: 0.9811209241782918\n",
      "Epoch 8, Training Loss: 0.9738742243542391\n",
      "Epoch 9, Training Loss: 0.9684321861407336\n",
      "Epoch 10, Training Loss: 0.9644202655904434\n",
      "Epoch 11, Training Loss: 0.9614794055153342\n",
      "Epoch 12, Training Loss: 0.9592867761499742\n",
      "Epoch 13, Training Loss: 0.9576061541893903\n",
      "Epoch 14, Training Loss: 0.9562703086348141\n",
      "Epoch 15, Training Loss: 0.9551360687087563\n",
      "Epoch 16, Training Loss: 0.9541476064569809\n",
      "Epoch 17, Training Loss: 0.9532390540487626\n",
      "Epoch 18, Training Loss: 0.9523805872833028\n",
      "Epoch 19, Training Loss: 0.9515527004354141\n",
      "Epoch 20, Training Loss: 0.9507466650710387\n",
      "Epoch 21, Training Loss: 0.9499374442942002\n",
      "Epoch 22, Training Loss: 0.9491390661632313\n",
      "Epoch 23, Training Loss: 0.9483325305405785\n",
      "Epoch 24, Training Loss: 0.9475241847599254\n",
      "Epoch 25, Training Loss: 0.9467080016697155\n",
      "Epoch 26, Training Loss: 0.9458857687781839\n",
      "Epoch 27, Training Loss: 0.945052373269025\n",
      "Epoch 28, Training Loss: 0.9442079954988816\n",
      "Epoch 29, Training Loss: 0.9433498112594381\n",
      "Epoch 30, Training Loss: 0.9424797796501833\n",
      "Epoch 31, Training Loss: 0.9415998324927162\n",
      "Epoch 32, Training Loss: 0.9407049061270321\n",
      "Epoch 33, Training Loss: 0.9397918557419497\n",
      "Epoch 34, Training Loss: 0.938864279424443\n",
      "Epoch 35, Training Loss: 0.9379177657996907\n",
      "Epoch 36, Training Loss: 0.9369520636165843\n",
      "Epoch 37, Training Loss: 0.9359716880321503\n",
      "Epoch 38, Training Loss: 0.9349735713706298\n",
      "Epoch 39, Training Loss: 0.9339527325069203\n",
      "Epoch 40, Training Loss: 0.9329073722923503\n",
      "Epoch 41, Training Loss: 0.9318363048749811\n",
      "Epoch 42, Training Loss: 0.9307529430529651\n",
      "Epoch 43, Training Loss: 0.9296334259650286\n",
      "Epoch 44, Training Loss: 0.9285029193934272\n",
      "Epoch 45, Training Loss: 0.9273302891675164\n",
      "Epoch 46, Training Loss: 0.9261390464446124\n",
      "Epoch 47, Training Loss: 0.9249180509062375\n",
      "Epoch 48, Training Loss: 0.9236645960106569\n",
      "Epoch 49, Training Loss: 0.9223802955711589\n",
      "Epoch 50, Training Loss: 0.9210600071093615\n",
      "Epoch 51, Training Loss: 0.9197204247642966\n",
      "Epoch 52, Training Loss: 0.9183303464861478\n",
      "Epoch 53, Training Loss: 0.9169124127135557\n",
      "Epoch 54, Training Loss: 0.9154688875114216\n",
      "Epoch 55, Training Loss: 0.913965395338395\n",
      "Epoch 56, Training Loss: 0.9124437075502733\n",
      "Epoch 57, Training Loss: 0.9108639104226056\n",
      "Epoch 58, Training Loss: 0.9092629930552314\n",
      "Epoch 59, Training Loss: 0.9076182042149936\n",
      "Epoch 60, Training Loss: 0.9059204748097588\n",
      "Epoch 61, Training Loss: 0.9041986120448393\n",
      "Epoch 62, Training Loss: 0.9024194538593292\n",
      "Epoch 63, Training Loss: 0.900616785848842\n",
      "Epoch 64, Training Loss: 0.8987543373248157\n",
      "Epoch 65, Training Loss: 0.8968647700898787\n",
      "Epoch 66, Training Loss: 0.8949211071519291\n",
      "Epoch 67, Training Loss: 0.8929666911153232\n",
      "Epoch 68, Training Loss: 0.8909688450308407\n",
      "Epoch 69, Training Loss: 0.8889259214260999\n",
      "Epoch 70, Training Loss: 0.8868504987043493\n",
      "Epoch 71, Training Loss: 0.8847429154900943\n",
      "Epoch 72, Training Loss: 0.8826257752670962\n",
      "Epoch 73, Training Loss: 0.880477098647286\n",
      "Epoch 74, Training Loss: 0.8783159485985251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:43:51,280] Trial 246 finished with value: 0.5901333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8761383407256182\n",
      "Epoch 1, Training Loss: 0.9344184485593237\n",
      "Epoch 2, Training Loss: 0.8271351829507297\n",
      "Epoch 3, Training Loss: 0.8185376711357805\n",
      "Epoch 4, Training Loss: 0.8112015596906045\n",
      "Epoch 5, Training Loss: 0.806443480979231\n",
      "Epoch 6, Training Loss: 0.805415559890575\n",
      "Epoch 7, Training Loss: 0.8012438266797173\n",
      "Epoch 8, Training Loss: 0.8006990153090399\n",
      "Epoch 9, Training Loss: 0.7992423587275627\n",
      "Epoch 10, Training Loss: 0.7983551213615819\n",
      "Epoch 11, Training Loss: 0.7971326034768184\n",
      "Epoch 12, Training Loss: 0.7963475947093247\n",
      "Epoch 13, Training Loss: 0.7952612994308759\n",
      "Epoch 14, Training Loss: 0.7940261359501603\n",
      "Epoch 15, Training Loss: 0.7947757762177546\n",
      "Epoch 16, Training Loss: 0.7935944124271995\n",
      "Epoch 17, Training Loss: 0.7921046599409634\n",
      "Epoch 18, Training Loss: 0.7932017381926229\n",
      "Epoch 19, Training Loss: 0.7919004137354686\n",
      "Epoch 20, Training Loss: 0.7923402102370011\n",
      "Epoch 21, Training Loss: 0.791546148554723\n",
      "Epoch 22, Training Loss: 0.7918291626119972\n",
      "Epoch 23, Training Loss: 0.7910057410261685\n",
      "Epoch 24, Training Loss: 0.7910349133319424\n",
      "Epoch 25, Training Loss: 0.7897008609950991\n",
      "Epoch 26, Training Loss: 0.7906638647380628\n",
      "Epoch 27, Training Loss: 0.7900776959899672\n",
      "Epoch 28, Training Loss: 0.7891046345682072\n",
      "Epoch 29, Training Loss: 0.7892684513464906\n",
      "Epoch 30, Training Loss: 0.788788384333589\n",
      "Epoch 31, Training Loss: 0.7882174261530539\n",
      "Epoch 32, Training Loss: 0.7891815527041156\n",
      "Epoch 33, Training Loss: 0.7891081450577069\n",
      "Epoch 34, Training Loss: 0.7902092945306821\n",
      "Epoch 35, Training Loss: 0.7881750394527177\n",
      "Epoch 36, Training Loss: 0.7876104074313228\n",
      "Epoch 37, Training Loss: 0.7875543256451313\n",
      "Epoch 38, Training Loss: 0.7887022290910993\n",
      "Epoch 39, Training Loss: 0.7886848316156775\n",
      "Epoch 40, Training Loss: 0.7882201632162682\n",
      "Epoch 41, Training Loss: 0.7877679749987179\n",
      "Epoch 42, Training Loss: 0.7879298608105881\n",
      "Epoch 43, Training Loss: 0.7874377212130037\n",
      "Epoch 44, Training Loss: 0.7884859275997134\n",
      "Epoch 45, Training Loss: 0.789200422817603\n",
      "Epoch 46, Training Loss: 0.7877165394618099\n",
      "Epoch 47, Training Loss: 0.7881467547631802\n",
      "Epoch 48, Training Loss: 0.7880844671027105\n",
      "Epoch 49, Training Loss: 0.7877796659792277\n",
      "Epoch 50, Training Loss: 0.7879487360330453\n",
      "Epoch 51, Training Loss: 0.787514086773521\n",
      "Epoch 52, Training Loss: 0.7868413476119364\n",
      "Epoch 53, Training Loss: 0.7865643835605536\n",
      "Epoch 54, Training Loss: 0.7867933903421674\n",
      "Epoch 55, Training Loss: 0.7868952807627226\n",
      "Epoch 56, Training Loss: 0.7878564212555276\n",
      "Epoch 57, Training Loss: 0.7864638192313058\n",
      "Epoch 58, Training Loss: 0.7867816635540553\n",
      "Epoch 59, Training Loss: 0.7878258306281011\n",
      "Epoch 60, Training Loss: 0.7872543783116162\n",
      "Epoch 61, Training Loss: 0.7868011709442713\n",
      "Epoch 62, Training Loss: 0.7869927681478343\n",
      "Epoch 63, Training Loss: 0.7869256382598017\n",
      "Epoch 64, Training Loss: 0.7865518748312068\n",
      "Epoch 65, Training Loss: 0.7872178687188859\n",
      "Epoch 66, Training Loss: 0.7871655554699718\n",
      "Epoch 67, Training Loss: 0.7877489062180196\n",
      "Epoch 68, Training Loss: 0.7870723923346153\n",
      "Epoch 69, Training Loss: 0.7873324716001525\n",
      "Epoch 70, Training Loss: 0.7868849351890105\n",
      "Epoch 71, Training Loss: 0.7879777362472132\n",
      "Epoch 72, Training Loss: 0.7876434465996305\n",
      "Epoch 73, Training Loss: 0.7882916005034196\n",
      "Epoch 74, Training Loss: 0.7871597918352686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:45:23,978] Trial 247 finished with value: 0.6164666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7868975986215405\n",
      "Epoch 1, Training Loss: 0.8975546565476585\n",
      "Epoch 2, Training Loss: 0.8236241658996133\n",
      "Epoch 3, Training Loss: 0.8140424642843359\n",
      "Epoch 4, Training Loss: 0.8084812738614924\n",
      "Epoch 5, Training Loss: 0.8042696626046124\n",
      "Epoch 6, Training Loss: 0.8009115106218001\n",
      "Epoch 7, Training Loss: 0.7982992736031027\n",
      "Epoch 8, Training Loss: 0.7975902893262751\n",
      "Epoch 9, Training Loss: 0.7956560086502749\n",
      "Epoch 10, Training Loss: 0.794940333927379\n",
      "Epoch 11, Training Loss: 0.7938214106419507\n",
      "Epoch 12, Training Loss: 0.7939755295304691\n",
      "Epoch 13, Training Loss: 0.7933223589027629\n",
      "Epoch 14, Training Loss: 0.791750108424355\n",
      "Epoch 15, Training Loss: 0.7913514791516697\n",
      "Epoch 16, Training Loss: 0.7905946083629832\n",
      "Epoch 17, Training Loss: 0.7904269689672133\n",
      "Epoch 18, Training Loss: 0.7902683443181655\n",
      "Epoch 19, Training Loss: 0.7898117033874288\n",
      "Epoch 20, Training Loss: 0.7893433865378885\n",
      "Epoch 21, Training Loss: 0.7887385643930996\n",
      "Epoch 22, Training Loss: 0.7885555966461406\n",
      "Epoch 23, Training Loss: 0.7884187821780934\n",
      "Epoch 24, Training Loss: 0.7878582935473498\n",
      "Epoch 25, Training Loss: 0.7877881139166215\n",
      "Epoch 26, Training Loss: 0.7878177562180687\n",
      "Epoch 27, Training Loss: 0.7874400702644797\n",
      "Epoch 28, Training Loss: 0.7864701646917006\n",
      "Epoch 29, Training Loss: 0.7867957678261925\n",
      "Epoch 30, Training Loss: 0.7861903846263886\n",
      "Epoch 31, Training Loss: 0.786849392301896\n",
      "Epoch 32, Training Loss: 0.7865341252439162\n",
      "Epoch 33, Training Loss: 0.7860316859974581\n",
      "Epoch 34, Training Loss: 0.7855495017416337\n",
      "Epoch 35, Training Loss: 0.7855040801973904\n",
      "Epoch 36, Training Loss: 0.7853113749447991\n",
      "Epoch 37, Training Loss: 0.7851467048420625\n",
      "Epoch 38, Training Loss: 0.7850804828896242\n",
      "Epoch 39, Training Loss: 0.7847834629171034\n",
      "Epoch 40, Training Loss: 0.7847446267043843\n",
      "Epoch 41, Training Loss: 0.7847863478520337\n",
      "Epoch 42, Training Loss: 0.7846922208982355\n",
      "Epoch 43, Training Loss: 0.7849015427336973\n",
      "Epoch 44, Training Loss: 0.784692111155566\n",
      "Epoch 45, Training Loss: 0.7840223764671999\n",
      "Epoch 46, Training Loss: 0.7840167680908652\n",
      "Epoch 47, Training Loss: 0.7838334685914656\n",
      "Epoch 48, Training Loss: 0.7834776332097895\n",
      "Epoch 49, Training Loss: 0.7838744744132546\n",
      "Epoch 50, Training Loss: 0.7833152303976171\n",
      "Epoch 51, Training Loss: 0.7832766032218933\n",
      "Epoch 52, Training Loss: 0.7834163973611944\n",
      "Epoch 53, Training Loss: 0.7823951141273274\n",
      "Epoch 54, Training Loss: 0.7824254756114062\n",
      "Epoch 55, Training Loss: 0.7830205582169926\n",
      "Epoch 56, Training Loss: 0.7830163227109348\n",
      "Epoch 57, Training Loss: 0.782422161733403\n",
      "Epoch 58, Training Loss: 0.7828696356801426\n",
      "Epoch 59, Training Loss: 0.7826966770256267\n",
      "Epoch 60, Training Loss: 0.7824134435373195\n",
      "Epoch 61, Training Loss: 0.7823662787325242\n",
      "Epoch 62, Training Loss: 0.782321002553491\n",
      "Epoch 63, Training Loss: 0.7816204248456394\n",
      "Epoch 64, Training Loss: 0.7821039091138279\n",
      "Epoch 65, Training Loss: 0.7821132109445684\n",
      "Epoch 66, Training Loss: 0.7816056205945856\n",
      "Epoch 67, Training Loss: 0.7818318782133215\n",
      "Epoch 68, Training Loss: 0.7815564158383538\n",
      "Epoch 69, Training Loss: 0.7817679015327902\n",
      "Epoch 70, Training Loss: 0.7814447762685663\n",
      "Epoch 71, Training Loss: 0.7812426168077132\n",
      "Epoch 72, Training Loss: 0.7816765137279735\n",
      "Epoch 73, Training Loss: 0.7809700537429136\n",
      "Epoch 74, Training Loss: 0.7813628904258504\n",
      "Epoch 75, Training Loss: 0.7806542932285983\n",
      "Epoch 76, Training Loss: 0.7809919503155877\n",
      "Epoch 77, Training Loss: 0.7813637584798476\n",
      "Epoch 78, Training Loss: 0.7815075851889217\n",
      "Epoch 79, Training Loss: 0.7809465480552\n",
      "Epoch 80, Training Loss: 0.7809890392247368\n",
      "Epoch 81, Training Loss: 0.7809293869663687\n",
      "Epoch 82, Training Loss: 0.7807460702166837\n",
      "Epoch 83, Training Loss: 0.7802495137382957\n",
      "Epoch 84, Training Loss: 0.7804065106896794\n",
      "Epoch 85, Training Loss: 0.7800896757490494\n",
      "Epoch 86, Training Loss: 0.7806435221083023\n",
      "Epoch 87, Training Loss: 0.7797914391405443\n",
      "Epoch 88, Training Loss: 0.78066809499965\n",
      "Epoch 89, Training Loss: 0.7801277553333955\n",
      "Epoch 90, Training Loss: 0.7801103329658509\n",
      "Epoch 91, Training Loss: 0.7798436915874482\n",
      "Epoch 92, Training Loss: 0.7799018617237315\n",
      "Epoch 93, Training Loss: 0.780153494792826\n",
      "Epoch 94, Training Loss: 0.7798953583661248\n",
      "Epoch 95, Training Loss: 0.7806247997985166\n",
      "Epoch 96, Training Loss: 0.7796665610986597\n",
      "Epoch 97, Training Loss: 0.7807304460160872\n",
      "Epoch 98, Training Loss: 0.7797931225159589\n",
      "Epoch 99, Training Loss: 0.7800335285242866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:47:42,086] Trial 248 finished with value: 0.64 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7799673967501697\n",
      "Epoch 1, Training Loss: 0.8565710090156785\n",
      "Epoch 2, Training Loss: 0.8262970451125525\n",
      "Epoch 3, Training Loss: 0.8273725476480068\n",
      "Epoch 4, Training Loss: 0.8435651512074291\n",
      "Epoch 5, Training Loss: 0.8394265934040672\n",
      "Epoch 6, Training Loss: 0.8322421112454923\n",
      "Epoch 7, Training Loss: 0.8435420905737052\n",
      "Epoch 8, Training Loss: 0.840134126261661\n",
      "Epoch 9, Training Loss: 0.8419978358691796\n",
      "Epoch 10, Training Loss: 0.8404297084736645\n",
      "Epoch 11, Training Loss: 0.8382561989296647\n",
      "Epoch 12, Training Loss: 0.8347431462510188\n",
      "Epoch 13, Training Loss: 0.8456950807929935\n",
      "Epoch 14, Training Loss: 0.8404876845223563\n",
      "Epoch 15, Training Loss: 0.8432361650287656\n",
      "Epoch 16, Training Loss: 0.8428047517188509\n",
      "Epoch 17, Training Loss: 0.8440373331980597\n",
      "Epoch 18, Training Loss: 0.8494353314091388\n",
      "Epoch 19, Training Loss: 0.8449131260240884\n",
      "Epoch 20, Training Loss: 0.8375285370009286\n",
      "Epoch 21, Training Loss: 0.8426736707974197\n",
      "Epoch 22, Training Loss: 0.843448737868689\n",
      "Epoch 23, Training Loss: 0.8427065000498205\n",
      "Epoch 24, Training Loss: 0.8469091437813034\n",
      "Epoch 25, Training Loss: 0.8481095578437461\n",
      "Epoch 26, Training Loss: 0.8471050071537046\n",
      "Epoch 27, Training Loss: 0.8444943175280004\n",
      "Epoch 28, Training Loss: 0.8433348233538462\n",
      "Epoch 29, Training Loss: 0.8484179779102928\n",
      "Epoch 30, Training Loss: 0.8514053152916127\n",
      "Epoch 31, Training Loss: 0.842952375214799\n",
      "Epoch 32, Training Loss: 0.8429153186038024\n",
      "Epoch 33, Training Loss: 0.8403767863610634\n",
      "Epoch 34, Training Loss: 0.8445985944647538\n",
      "Epoch 35, Training Loss: 0.8516166159084865\n",
      "Epoch 36, Training Loss: 0.842869528343803\n",
      "Epoch 37, Training Loss: 0.8375555609401903\n",
      "Epoch 38, Training Loss: 0.852014726624453\n",
      "Epoch 39, Training Loss: 0.8454861637344934\n",
      "Epoch 40, Training Loss: 0.8366245885988823\n",
      "Epoch 41, Training Loss: 0.8389623066536466\n",
      "Epoch 42, Training Loss: 0.8406353167125157\n",
      "Epoch 43, Training Loss: 0.8424928511892046\n",
      "Epoch 44, Training Loss: 0.8385358312972506\n",
      "Epoch 45, Training Loss: 0.839978812063547\n",
      "Epoch 46, Training Loss: 0.8488701158000115\n",
      "Epoch 47, Training Loss: 0.8471419760159083\n",
      "Epoch 48, Training Loss: 0.8509502816917305\n",
      "Epoch 49, Training Loss: 0.8427604586558235\n",
      "Epoch 50, Training Loss: 0.8444475635550076\n",
      "Epoch 51, Training Loss: 0.8420011228188536\n",
      "Epoch 52, Training Loss: 0.8382564226487526\n",
      "Epoch 53, Training Loss: 0.8490639684791852\n",
      "Epoch 54, Training Loss: 0.8481023133249211\n",
      "Epoch 55, Training Loss: 0.8396766185760498\n",
      "Epoch 56, Training Loss: 0.8417430614170275\n",
      "Epoch 57, Training Loss: 0.8448620478909715\n",
      "Epoch 58, Training Loss: 0.8522365537801183\n",
      "Epoch 59, Training Loss: 0.8481098041498571\n",
      "Epoch 60, Training Loss: 0.8472249147587253\n",
      "Epoch 61, Training Loss: 0.8514109007397989\n",
      "Epoch 62, Training Loss: 0.8444447745057874\n",
      "Epoch 63, Training Loss: 0.8426455108742965\n",
      "Epoch 64, Training Loss: 0.8448868637694452\n",
      "Epoch 65, Training Loss: 0.8545246837730694\n",
      "Epoch 66, Training Loss: 0.8486554956973944\n",
      "Epoch 67, Training Loss: 0.8432425567978307\n",
      "Epoch 68, Training Loss: 0.8444082163330308\n",
      "Epoch 69, Training Loss: 0.8390731850961097\n",
      "Epoch 70, Training Loss: 0.8362212372005434\n",
      "Epoch 71, Training Loss: 0.8462542655772732\n",
      "Epoch 72, Training Loss: 0.8461855237645314\n",
      "Epoch 73, Training Loss: 0.8419993760890531\n",
      "Epoch 74, Training Loss: 0.8405822802307015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:49:21,185] Trial 249 finished with value: 0.6084 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8503897134522747\n",
      "Epoch 1, Training Loss: 0.8830680297729664\n",
      "Epoch 2, Training Loss: 0.829801947461035\n",
      "Epoch 3, Training Loss: 0.8218690459889577\n",
      "Epoch 4, Training Loss: 0.8170233141210742\n",
      "Epoch 5, Training Loss: 0.8133403942100984\n",
      "Epoch 6, Training Loss: 0.8101840779297333\n",
      "Epoch 7, Training Loss: 0.8102402511395906\n",
      "Epoch 8, Training Loss: 0.8085865756622831\n",
      "Epoch 9, Training Loss: 0.8076746090014179\n",
      "Epoch 10, Training Loss: 0.8055538313729422\n",
      "Epoch 11, Training Loss: 0.8052438137226535\n",
      "Epoch 12, Training Loss: 0.8042304574995113\n",
      "Epoch 13, Training Loss: 0.8028264315504777\n",
      "Epoch 14, Training Loss: 0.8033780670703802\n",
      "Epoch 15, Training Loss: 0.8016166185974178\n",
      "Epoch 16, Training Loss: 0.8014579259363332\n",
      "Epoch 17, Training Loss: 0.8025442239037134\n",
      "Epoch 18, Training Loss: 0.8022894359172735\n",
      "Epoch 19, Training Loss: 0.8024838784583529\n",
      "Epoch 20, Training Loss: 0.8027610387120928\n",
      "Epoch 21, Training Loss: 0.7997124670143414\n",
      "Epoch 22, Training Loss: 0.8014489121006844\n",
      "Epoch 23, Training Loss: 0.8006511885420721\n",
      "Epoch 24, Training Loss: 0.8003222462826205\n",
      "Epoch 25, Training Loss: 0.7993359188388165\n",
      "Epoch 26, Training Loss: 0.7983745228079029\n",
      "Epoch 27, Training Loss: 0.7993392570574481\n",
      "Epoch 28, Training Loss: 0.7986035812169986\n",
      "Epoch 29, Training Loss: 0.7986051211679789\n",
      "Epoch 30, Training Loss: 0.7989487097675639\n",
      "Epoch 31, Training Loss: 0.798696434139309\n",
      "Epoch 32, Training Loss: 0.7988908746188744\n",
      "Epoch 33, Training Loss: 0.7989486086637454\n",
      "Epoch 34, Training Loss: 0.7976137924015074\n",
      "Epoch 35, Training Loss: 0.7980330626767381\n",
      "Epoch 36, Training Loss: 0.7980614560887329\n",
      "Epoch 37, Training Loss: 0.7987298134574317\n",
      "Epoch 38, Training Loss: 0.7977297108872492\n",
      "Epoch 39, Training Loss: 0.7970175195457344\n",
      "Epoch 40, Training Loss: 0.7972543175507308\n",
      "Epoch 41, Training Loss: 0.7977144014566464\n",
      "Epoch 42, Training Loss: 0.7969479603874953\n",
      "Epoch 43, Training Loss: 0.7974274593188351\n",
      "Epoch 44, Training Loss: 0.7958684133407765\n",
      "Epoch 45, Training Loss: 0.7954743770728434\n",
      "Epoch 46, Training Loss: 0.7978819575524868\n",
      "Epoch 47, Training Loss: 0.7959612408078702\n",
      "Epoch 48, Training Loss: 0.7962795616988849\n",
      "Epoch 49, Training Loss: 0.7959530745233808\n",
      "Epoch 50, Training Loss: 0.7963237489972795\n",
      "Epoch 51, Training Loss: 0.7970564931855166\n",
      "Epoch 52, Training Loss: 0.795947379635689\n",
      "Epoch 53, Training Loss: 0.7964439631404733\n",
      "Epoch 54, Training Loss: 0.7966661007780778\n",
      "Epoch 55, Training Loss: 0.7957937894907213\n",
      "Epoch 56, Training Loss: 0.7965082458087376\n",
      "Epoch 57, Training Loss: 0.7959624992277389\n",
      "Epoch 58, Training Loss: 0.7957959594583153\n",
      "Epoch 59, Training Loss: 0.7966015705488678\n",
      "Epoch 60, Training Loss: 0.7956725816977651\n",
      "Epoch 61, Training Loss: 0.794737872862278\n",
      "Epoch 62, Training Loss: 0.7960928889145529\n",
      "Epoch 63, Training Loss: 0.7946327419209301\n",
      "Epoch 64, Training Loss: 0.7950000106840205\n",
      "Epoch 65, Training Loss: 0.7956798734521507\n",
      "Epoch 66, Training Loss: 0.7948412417469168\n",
      "Epoch 67, Training Loss: 0.7955492174715028\n",
      "Epoch 68, Training Loss: 0.7955060826208359\n",
      "Epoch 69, Training Loss: 0.7952072803239177\n",
      "Epoch 70, Training Loss: 0.795427087464727\n",
      "Epoch 71, Training Loss: 0.7955721945690929\n",
      "Epoch 72, Training Loss: 0.7948993704372779\n",
      "Epoch 73, Training Loss: 0.7945577710194696\n",
      "Epoch 74, Training Loss: 0.7952375087522923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:50:45,512] Trial 250 finished with value: 0.6257333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7946956514415885\n",
      "Epoch 1, Training Loss: 1.1171368717250967\n",
      "Epoch 2, Training Loss: 1.061593042638965\n",
      "Epoch 3, Training Loss: 1.027489619774926\n",
      "Epoch 4, Training Loss: 1.0051972424177298\n",
      "Epoch 5, Training Loss: 0.9905459115379736\n",
      "Epoch 6, Training Loss: 0.9805160558313355\n",
      "Epoch 7, Training Loss: 0.9734806100228676\n",
      "Epoch 8, Training Loss: 0.9684885805710814\n",
      "Epoch 9, Training Loss: 0.9646968457931863\n",
      "Epoch 10, Training Loss: 0.9622599517492424\n",
      "Epoch 11, Training Loss: 0.9601246826630786\n",
      "Epoch 12, Training Loss: 0.9583499758763421\n",
      "Epoch 13, Training Loss: 0.957199560251451\n",
      "Epoch 14, Training Loss: 0.9560970712425118\n",
      "Epoch 15, Training Loss: 0.9552879130033622\n",
      "Epoch 16, Training Loss: 0.954086104640387\n",
      "Epoch 17, Training Loss: 0.953202920390251\n",
      "Epoch 18, Training Loss: 0.9521476123566018\n",
      "Epoch 19, Training Loss: 0.9517167436449151\n",
      "Epoch 20, Training Loss: 0.9506344565771576\n",
      "Epoch 21, Training Loss: 0.9504099075059246\n",
      "Epoch 22, Training Loss: 0.9497821254837782\n",
      "Epoch 23, Training Loss: 0.9492398406329908\n",
      "Epoch 24, Training Loss: 0.9482828650259434\n",
      "Epoch 25, Training Loss: 0.9474651864596776\n",
      "Epoch 26, Training Loss: 0.9471757713117097\n",
      "Epoch 27, Training Loss: 0.9461614960118344\n",
      "Epoch 28, Training Loss: 0.9459976568257898\n",
      "Epoch 29, Training Loss: 0.9449898175727156\n",
      "Epoch 30, Training Loss: 0.9446431569587019\n",
      "Epoch 31, Training Loss: 0.9439578382592452\n",
      "Epoch 32, Training Loss: 0.9430729564867522\n",
      "Epoch 33, Training Loss: 0.9423852492992143\n",
      "Epoch 34, Training Loss: 0.94214273144428\n",
      "Epoch 35, Training Loss: 0.9413826254077424\n",
      "Epoch 36, Training Loss: 0.9407835383164255\n",
      "Epoch 37, Training Loss: 0.9400353980243654\n",
      "Epoch 38, Training Loss: 0.9387004893077048\n",
      "Epoch 39, Training Loss: 0.9390282894435682\n",
      "Epoch 40, Training Loss: 0.9384211492717714\n",
      "Epoch 41, Training Loss: 0.937356167897246\n",
      "Epoch 42, Training Loss: 0.9367612128867242\n",
      "Epoch 43, Training Loss: 0.9365997012396504\n",
      "Epoch 44, Training Loss: 0.9354802775203733\n",
      "Epoch 45, Training Loss: 0.9345881323169049\n",
      "Epoch 46, Training Loss: 0.9342061481081454\n",
      "Epoch 47, Training Loss: 0.9336811614215822\n",
      "Epoch 48, Training Loss: 0.9327192951862077\n",
      "Epoch 49, Training Loss: 0.9321416773294148\n",
      "Epoch 50, Training Loss: 0.9317108061080589\n",
      "Epoch 51, Training Loss: 0.9306864339606207\n",
      "Epoch 52, Training Loss: 0.9297646915105949\n",
      "Epoch 53, Training Loss: 0.9300286910587684\n",
      "Epoch 54, Training Loss: 0.9292327346658348\n",
      "Epoch 55, Training Loss: 0.9279439877746697\n",
      "Epoch 56, Training Loss: 0.9275396036922483\n",
      "Epoch 57, Training Loss: 0.927318703231955\n",
      "Epoch 58, Training Loss: 0.9261676735447761\n",
      "Epoch 59, Training Loss: 0.9251895815806281\n",
      "Epoch 60, Training Loss: 0.9248082525748059\n",
      "Epoch 61, Training Loss: 0.9239975627203633\n",
      "Epoch 62, Training Loss: 0.9233636600630624\n",
      "Epoch 63, Training Loss: 0.9223357409462893\n",
      "Epoch 64, Training Loss: 0.921645696808521\n",
      "Epoch 65, Training Loss: 0.9212459951415097\n",
      "Epoch 66, Training Loss: 0.920728279683823\n",
      "Epoch 67, Training Loss: 0.9194277642364789\n",
      "Epoch 68, Training Loss: 0.9189793318734133\n",
      "Epoch 69, Training Loss: 0.9180606262128156\n",
      "Epoch 70, Training Loss: 0.9177782411862137\n",
      "Epoch 71, Training Loss: 0.9168231015814874\n",
      "Epoch 72, Training Loss: 0.9160879214007155\n",
      "Epoch 73, Training Loss: 0.9160096231259798\n",
      "Epoch 74, Training Loss: 0.9144089479195444\n",
      "Epoch 75, Training Loss: 0.9141870833877334\n",
      "Epoch 76, Training Loss: 0.9134231093234586\n",
      "Epoch 77, Training Loss: 0.9125518471674812\n",
      "Epoch 78, Training Loss: 0.9118525375100903\n",
      "Epoch 79, Training Loss: 0.9110784256368651\n",
      "Epoch 80, Training Loss: 0.9107144857707776\n",
      "Epoch 81, Training Loss: 0.9091662236622402\n",
      "Epoch 82, Training Loss: 0.9090751446279368\n",
      "Epoch 83, Training Loss: 0.9082024033804585\n",
      "Epoch 84, Training Loss: 0.9073657553895076\n",
      "Epoch 85, Training Loss: 0.9067089197330905\n",
      "Epoch 86, Training Loss: 0.905656699847458\n",
      "Epoch 87, Training Loss: 0.9048917020173898\n",
      "Epoch 88, Training Loss: 0.904243428635418\n",
      "Epoch 89, Training Loss: 0.9035201596138173\n",
      "Epoch 90, Training Loss: 0.9027459430515318\n",
      "Epoch 91, Training Loss: 0.9022614138466971\n",
      "Epoch 92, Training Loss: 0.9018121570572817\n",
      "Epoch 93, Training Loss: 0.9003165426110863\n",
      "Epoch 94, Training Loss: 0.899781848882374\n",
      "Epoch 95, Training Loss: 0.8990730194220865\n",
      "Epoch 96, Training Loss: 0.8985039731613675\n",
      "Epoch 97, Training Loss: 0.8973325352023419\n",
      "Epoch 98, Training Loss: 0.8966661782193005\n",
      "Epoch 99, Training Loss: 0.8959864418309434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:52:06,817] Trial 251 finished with value: 0.58 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8945876717567444\n",
      "Epoch 1, Training Loss: 0.9757220706545321\n",
      "Epoch 2, Training Loss: 0.9450441174937371\n",
      "Epoch 3, Training Loss: 0.9321359641569897\n",
      "Epoch 4, Training Loss: 0.9188541037695749\n",
      "Epoch 5, Training Loss: 0.9045407880517773\n",
      "Epoch 6, Training Loss: 0.8897372064733864\n",
      "Epoch 7, Training Loss: 0.8748770499587956\n",
      "Epoch 8, Training Loss: 0.8612738108276424\n",
      "Epoch 9, Training Loss: 0.8492284940597706\n",
      "Epoch 10, Training Loss: 0.8395581197021599\n",
      "Epoch 11, Training Loss: 0.8321635300055482\n",
      "Epoch 12, Training Loss: 0.8259860875014972\n",
      "Epoch 13, Training Loss: 0.8214949569307772\n",
      "Epoch 14, Training Loss: 0.8180473834948432\n",
      "Epoch 15, Training Loss: 0.8158846892808613\n",
      "Epoch 16, Training Loss: 0.8140645645614853\n",
      "Epoch 17, Training Loss: 0.8126003073570424\n",
      "Epoch 18, Training Loss: 0.8108682125134575\n",
      "Epoch 19, Training Loss: 0.8103573278376931\n",
      "Epoch 20, Training Loss: 0.8095735332123319\n",
      "Epoch 21, Training Loss: 0.8090454608874214\n",
      "Epoch 22, Training Loss: 0.8078923889568874\n",
      "Epoch 23, Training Loss: 0.8076877621779764\n",
      "Epoch 24, Training Loss: 0.8074308547758519\n",
      "Epoch 25, Training Loss: 0.8071155188675213\n",
      "Epoch 26, Training Loss: 0.8072979841017185\n",
      "Epoch 27, Training Loss: 0.8060088257592424\n",
      "Epoch 28, Training Loss: 0.8059432598881255\n",
      "Epoch 29, Training Loss: 0.8058231662090559\n",
      "Epoch 30, Training Loss: 0.8052511484103095\n",
      "Epoch 31, Training Loss: 0.8051750435865015\n",
      "Epoch 32, Training Loss: 0.8055725381786661\n",
      "Epoch 33, Training Loss: 0.8053829201182029\n",
      "Epoch 34, Training Loss: 0.8040781242506845\n",
      "Epoch 35, Training Loss: 0.8040677749124685\n",
      "Epoch 36, Training Loss: 0.8037744462938237\n",
      "Epoch 37, Training Loss: 0.8032118506897661\n",
      "Epoch 38, Training Loss: 0.803394147059075\n",
      "Epoch 39, Training Loss: 0.802679551454415\n",
      "Epoch 40, Training Loss: 0.8032989114747011\n",
      "Epoch 41, Training Loss: 0.8037105452745481\n",
      "Epoch 42, Training Loss: 0.8027000049899395\n",
      "Epoch 43, Training Loss: 0.80240914498953\n",
      "Epoch 44, Training Loss: 0.8022793622841512\n",
      "Epoch 45, Training Loss: 0.8022281926377375\n",
      "Epoch 46, Training Loss: 0.8025673133986336\n",
      "Epoch 47, Training Loss: 0.801578012086395\n",
      "Epoch 48, Training Loss: 0.8018072197311803\n",
      "Epoch 49, Training Loss: 0.8014034959606658\n",
      "Epoch 50, Training Loss: 0.8015260146076518\n",
      "Epoch 51, Training Loss: 0.8012805548825659\n",
      "Epoch 52, Training Loss: 0.8009521024567741\n",
      "Epoch 53, Training Loss: 0.8016100241725607\n",
      "Epoch 54, Training Loss: 0.8009462445302117\n",
      "Epoch 55, Training Loss: 0.800501206942967\n",
      "Epoch 56, Training Loss: 0.8005512979693878\n",
      "Epoch 57, Training Loss: 0.8001087396664727\n",
      "Epoch 58, Training Loss: 0.7999889604131082\n",
      "Epoch 59, Training Loss: 0.8001700265066964\n",
      "Epoch 60, Training Loss: 0.8001121749555258\n",
      "Epoch 61, Training Loss: 0.8002519515223969\n",
      "Epoch 62, Training Loss: 0.7999387672969274\n",
      "Epoch 63, Training Loss: 0.8007328623219541\n",
      "Epoch 64, Training Loss: 0.7996724453187527\n",
      "Epoch 65, Training Loss: 0.7994125272994651\n",
      "Epoch 66, Training Loss: 0.7995356791897824\n",
      "Epoch 67, Training Loss: 0.7991652309894561\n",
      "Epoch 68, Training Loss: 0.7991164571360538\n",
      "Epoch 69, Training Loss: 0.7996690375464303\n",
      "Epoch 70, Training Loss: 0.799132536138807\n",
      "Epoch 71, Training Loss: 0.7989241302461553\n",
      "Epoch 72, Training Loss: 0.798864984422698\n",
      "Epoch 73, Training Loss: 0.7988125680084516\n",
      "Epoch 74, Training Loss: 0.7985213046683405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:53:08,691] Trial 252 finished with value: 0.6338666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7991204168563498\n",
      "Epoch 1, Training Loss: 1.06721122946058\n",
      "Epoch 2, Training Loss: 1.0175861254670566\n",
      "Epoch 3, Training Loss: 0.9944389654281444\n",
      "Epoch 4, Training Loss: 0.9813018694856113\n",
      "Epoch 5, Training Loss: 0.9732521111803843\n",
      "Epoch 6, Training Loss: 0.9682024759457524\n",
      "Epoch 7, Training Loss: 0.964729014375156\n",
      "Epoch 8, Training Loss: 0.9625259541031114\n",
      "Epoch 9, Training Loss: 0.9604529192573146\n",
      "Epoch 10, Training Loss: 0.9588512899284076\n",
      "Epoch 11, Training Loss: 0.9578213148547294\n",
      "Epoch 12, Training Loss: 0.9566823621441547\n",
      "Epoch 13, Training Loss: 0.9555172767854274\n",
      "Epoch 14, Training Loss: 0.9555227830894011\n",
      "Epoch 15, Training Loss: 0.954418282670186\n",
      "Epoch 16, Training Loss: 0.9529846549034119\n",
      "Epoch 17, Training Loss: 0.9525467770440238\n",
      "Epoch 18, Training Loss: 0.9515411098200576\n",
      "Epoch 19, Training Loss: 0.9510594491671799\n",
      "Epoch 20, Training Loss: 0.9501084053426757\n",
      "Epoch 21, Training Loss: 0.9494757112703825\n",
      "Epoch 22, Training Loss: 0.9487944130610703\n",
      "Epoch 23, Training Loss: 0.9482155080128433\n",
      "Epoch 24, Training Loss: 0.9475258138843049\n",
      "Epoch 25, Training Loss: 0.946553801862817\n",
      "Epoch 26, Training Loss: 0.9458199917821956\n",
      "Epoch 27, Training Loss: 0.9452533358918097\n",
      "Epoch 28, Training Loss: 0.9448133356589123\n",
      "Epoch 29, Training Loss: 0.9441256069599238\n",
      "Epoch 30, Training Loss: 0.9428243919422752\n",
      "Epoch 31, Training Loss: 0.9424121443490336\n",
      "Epoch 32, Training Loss: 0.9412572797079731\n",
      "Epoch 33, Training Loss: 0.9413187156046243\n",
      "Epoch 34, Training Loss: 0.9408513038678277\n",
      "Epoch 35, Training Loss: 0.939745881503686\n",
      "Epoch 36, Training Loss: 0.9388439608695812\n",
      "Epoch 37, Training Loss: 0.9380228514061835\n",
      "Epoch 38, Training Loss: 0.937195498871624\n",
      "Epoch 39, Training Loss: 0.9366278051433706\n",
      "Epoch 40, Training Loss: 0.9357518594067796\n",
      "Epoch 41, Training Loss: 0.9351642173932011\n",
      "Epoch 42, Training Loss: 0.9343951461010409\n",
      "Epoch 43, Training Loss: 0.9338844609439821\n",
      "Epoch 44, Training Loss: 0.9335243057487602\n",
      "Epoch 45, Training Loss: 0.9326104868623547\n",
      "Epoch 46, Training Loss: 0.931548277507151\n",
      "Epoch 47, Training Loss: 0.9309093950386335\n",
      "Epoch 48, Training Loss: 0.9298738578208408\n",
      "Epoch 49, Training Loss: 0.929086590978436\n",
      "Epoch 50, Training Loss: 0.9284046354150414\n",
      "Epoch 51, Training Loss: 0.9277437040680333\n",
      "Epoch 52, Training Loss: 0.9270852261916139\n",
      "Epoch 53, Training Loss: 0.926417216681\n",
      "Epoch 54, Training Loss: 0.9259396958171873\n",
      "Epoch 55, Training Loss: 0.9248643962064184\n",
      "Epoch 56, Training Loss: 0.9240612581260222\n",
      "Epoch 57, Training Loss: 0.923160395586401\n",
      "Epoch 58, Training Loss: 0.9224382541233436\n",
      "Epoch 59, Training Loss: 0.9217905068756046\n",
      "Epoch 60, Training Loss: 0.9205286128180368\n",
      "Epoch 61, Training Loss: 0.9200503876334742\n",
      "Epoch 62, Training Loss: 0.9196047941544898\n",
      "Epoch 63, Training Loss: 0.9188435734662794\n",
      "Epoch 64, Training Loss: 0.91771349530471\n",
      "Epoch 65, Training Loss: 0.9170318880475553\n",
      "Epoch 66, Training Loss: 0.9165662127329891\n",
      "Epoch 67, Training Loss: 0.9156460244852798\n",
      "Epoch 68, Training Loss: 0.914681625545473\n",
      "Epoch 69, Training Loss: 0.9142272327179299\n",
      "Epoch 70, Training Loss: 0.9133696660959632\n",
      "Epoch 71, Training Loss: 0.9131061914271879\n",
      "Epoch 72, Training Loss: 0.9114699577926693\n",
      "Epoch 73, Training Loss: 0.9111454392734327\n",
      "Epoch 74, Training Loss: 0.9106329002774748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:54:10,316] Trial 253 finished with value: 0.5674 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9098281111036028\n",
      "Epoch 1, Training Loss: 0.8823125900941736\n",
      "Epoch 2, Training Loss: 0.8117825141373802\n",
      "Epoch 3, Training Loss: 0.805032393581727\n",
      "Epoch 4, Training Loss: 0.8021257925033569\n",
      "Epoch 5, Training Loss: 0.799274542612188\n",
      "Epoch 6, Training Loss: 0.7969342561329112\n",
      "Epoch 7, Training Loss: 0.7942502661312327\n",
      "Epoch 8, Training Loss: 0.7924444870387807\n",
      "Epoch 9, Training Loss: 0.7906313074336332\n",
      "Epoch 10, Training Loss: 0.7904509380284478\n",
      "Epoch 11, Training Loss: 0.7893080444896923\n",
      "Epoch 12, Training Loss: 0.7879990177294788\n",
      "Epoch 13, Training Loss: 0.7873677754402161\n",
      "Epoch 14, Training Loss: 0.7854731238589567\n",
      "Epoch 15, Training Loss: 0.7861115770480213\n",
      "Epoch 16, Training Loss: 0.785830595072578\n",
      "Epoch 17, Training Loss: 0.7854252240237067\n",
      "Epoch 18, Training Loss: 0.7848993430418126\n",
      "Epoch 19, Training Loss: 0.7840975457780501\n",
      "Epoch 20, Training Loss: 0.7843171453475952\n",
      "Epoch 21, Training Loss: 0.7837678637224085\n",
      "Epoch 22, Training Loss: 0.7837573564753813\n",
      "Epoch 23, Training Loss: 0.7834770254527821\n",
      "Epoch 24, Training Loss: 0.7836230773084304\n",
      "Epoch 25, Training Loss: 0.7829484513927909\n",
      "Epoch 26, Training Loss: 0.7830645266701194\n",
      "Epoch 27, Training Loss: 0.7825291775955874\n",
      "Epoch 28, Training Loss: 0.7823119359156665\n",
      "Epoch 29, Training Loss: 0.7820769643082338\n",
      "Epoch 30, Training Loss: 0.782140054843005\n",
      "Epoch 31, Training Loss: 0.7817570522252251\n",
      "Epoch 32, Training Loss: 0.7813781483033124\n",
      "Epoch 33, Training Loss: 0.7820501895511851\n",
      "Epoch 34, Training Loss: 0.781018172712887\n",
      "Epoch 35, Training Loss: 0.7814341923068552\n",
      "Epoch 36, Training Loss: 0.7805892260635601\n",
      "Epoch 37, Training Loss: 0.7803492635839125\n",
      "Epoch 38, Training Loss: 0.781000174073612\n",
      "Epoch 39, Training Loss: 0.7810707734612857\n",
      "Epoch 40, Training Loss: 0.7807412538808934\n",
      "Epoch 41, Training Loss: 0.7800976050601286\n",
      "Epoch 42, Training Loss: 0.7802682066664977\n",
      "Epoch 43, Training Loss: 0.7809163410523359\n",
      "Epoch 44, Training Loss: 0.7797233113821815\n",
      "Epoch 45, Training Loss: 0.7794747410101049\n",
      "Epoch 46, Training Loss: 0.7793394033347859\n",
      "Epoch 47, Training Loss: 0.7792740183016833\n",
      "Epoch 48, Training Loss: 0.7795156343544231\n",
      "Epoch 49, Training Loss: 0.7789193018744973\n",
      "Epoch 50, Training Loss: 0.779322182080325\n",
      "Epoch 51, Training Loss: 0.7785160693000345\n",
      "Epoch 52, Training Loss: 0.7795141462718739\n",
      "Epoch 53, Training Loss: 0.7787614529273089\n",
      "Epoch 54, Training Loss: 0.7787262537198908\n",
      "Epoch 55, Training Loss: 0.7783393129180459\n",
      "Epoch 56, Training Loss: 0.7786568752457114\n",
      "Epoch 57, Training Loss: 0.7787697146219366\n",
      "Epoch 58, Training Loss: 0.7787386682454277\n",
      "Epoch 59, Training Loss: 0.7785220045201918\n",
      "Epoch 60, Training Loss: 0.7779131596228656\n",
      "Epoch 61, Training Loss: 0.7782121364509358\n",
      "Epoch 62, Training Loss: 0.7782685019689448\n",
      "Epoch 63, Training Loss: 0.7780321604364059\n",
      "Epoch 64, Training Loss: 0.7773448265300078\n",
      "Epoch 65, Training Loss: 0.7780918104508344\n",
      "Epoch 66, Training Loss: 0.7777866432947271\n",
      "Epoch 67, Training Loss: 0.7775465613954208\n",
      "Epoch 68, Training Loss: 0.7776889727396123\n",
      "Epoch 69, Training Loss: 0.7773073726541856\n",
      "Epoch 70, Training Loss: 0.7771417118521298\n",
      "Epoch 71, Training Loss: 0.7773055242089664\n",
      "Epoch 72, Training Loss: 0.7768484820337856\n",
      "Epoch 73, Training Loss: 0.77660763705478\n",
      "Epoch 74, Training Loss: 0.7765043483060949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:56:01,401] Trial 254 finished with value: 0.6387333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7772430465501897\n",
      "Epoch 1, Training Loss: 0.8433176596725688\n",
      "Epoch 2, Training Loss: 0.8138174953180201\n",
      "Epoch 3, Training Loss: 0.8085102686461281\n",
      "Epoch 4, Training Loss: 0.807978553982342\n",
      "Epoch 5, Training Loss: 0.8059437279140248\n",
      "Epoch 6, Training Loss: 0.8037862503528594\n",
      "Epoch 7, Training Loss: 0.802985267498914\n",
      "Epoch 8, Training Loss: 0.8027122981408064\n",
      "Epoch 9, Training Loss: 0.8018014340540942\n",
      "Epoch 10, Training Loss: 0.8010070455074311\n",
      "Epoch 11, Training Loss: 0.8011467963106492\n",
      "Epoch 12, Training Loss: 0.8018757824336781\n",
      "Epoch 13, Training Loss: 0.8029545476857354\n",
      "Epoch 14, Training Loss: 0.8007985420086805\n",
      "Epoch 15, Training Loss: 0.8003305297739366\n",
      "Epoch 16, Training Loss: 0.7994737429478589\n",
      "Epoch 17, Training Loss: 0.8003197240829468\n",
      "Epoch 18, Training Loss: 0.7996543634639067\n",
      "Epoch 19, Training Loss: 0.7999997079372406\n",
      "Epoch 20, Training Loss: 0.8009562045686385\n",
      "Epoch 21, Training Loss: 0.7991798635090098\n",
      "Epoch 22, Training Loss: 0.799929305455264\n",
      "Epoch 23, Training Loss: 0.7992984920389512\n",
      "Epoch 24, Training Loss: 0.7985151527208441\n",
      "Epoch 25, Training Loss: 0.7994577220608207\n",
      "Epoch 26, Training Loss: 0.798118263833663\n",
      "Epoch 27, Training Loss: 0.7979870519217322\n",
      "Epoch 28, Training Loss: 0.7980277231861563\n",
      "Epoch 29, Training Loss: 0.7982095692438238\n",
      "Epoch 30, Training Loss: 0.7978629956525914\n",
      "Epoch 31, Training Loss: 0.7978000469768749\n",
      "Epoch 32, Training Loss: 0.798322152039584\n",
      "Epoch 33, Training Loss: 0.7978684344011194\n",
      "Epoch 34, Training Loss: 0.7977481894633349\n",
      "Epoch 35, Training Loss: 0.798062962363748\n",
      "Epoch 36, Training Loss: 0.7982954772780924\n",
      "Epoch 37, Training Loss: 0.7995647128189312\n",
      "Epoch 38, Training Loss: 0.7971891359020682\n",
      "Epoch 39, Training Loss: 0.7984185568725362\n",
      "Epoch 40, Training Loss: 0.7977762709645664\n",
      "Epoch 41, Training Loss: 0.7991406251402462\n",
      "Epoch 42, Training Loss: 0.7966387268374948\n",
      "Epoch 43, Training Loss: 0.7970564619933858\n",
      "Epoch 44, Training Loss: 0.7975318805610432\n",
      "Epoch 45, Training Loss: 0.7976747721784255\n",
      "Epoch 46, Training Loss: 0.7971490615956923\n",
      "Epoch 47, Training Loss: 0.7973596668944639\n",
      "Epoch 48, Training Loss: 0.7967514647455777\n",
      "Epoch 49, Training Loss: 0.7969934997137855\n",
      "Epoch 50, Training Loss: 0.7976382216986488\n",
      "Epoch 51, Training Loss: 0.7966891526474672\n",
      "Epoch 52, Training Loss: 0.7975297385804794\n",
      "Epoch 53, Training Loss: 0.7985507667064666\n",
      "Epoch 54, Training Loss: 0.7974410336859086\n",
      "Epoch 55, Training Loss: 0.7968252402193406\n",
      "Epoch 56, Training Loss: 0.7959387486121233\n",
      "Epoch 57, Training Loss: 0.7973850681501277\n",
      "Epoch 58, Training Loss: 0.7973164650973151\n",
      "Epoch 59, Training Loss: 0.796243068190182\n",
      "Epoch 60, Training Loss: 0.7968248824512257\n",
      "Epoch 61, Training Loss: 0.7970915020213407\n",
      "Epoch 62, Training Loss: 0.7972021641450769\n",
      "Epoch 63, Training Loss: 0.7972778111345628\n",
      "Epoch 64, Training Loss: 0.7976704871654511\n",
      "Epoch 65, Training Loss: 0.7975716731127571\n",
      "Epoch 66, Training Loss: 0.7982464964249555\n",
      "Epoch 67, Training Loss: 0.7989745876368355\n",
      "Epoch 68, Training Loss: 0.7973624370378607\n",
      "Epoch 69, Training Loss: 0.7959146687563727\n",
      "Epoch 70, Training Loss: 0.7976817131743712\n",
      "Epoch 71, Training Loss: 0.7970325016274171\n",
      "Epoch 72, Training Loss: 0.7962368384529562\n",
      "Epoch 73, Training Loss: 0.7970523071990293\n",
      "Epoch 74, Training Loss: 0.7972916766475229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:57:40,901] Trial 255 finished with value: 0.6342 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7969100210947149\n",
      "Epoch 1, Training Loss: 0.893312100242166\n",
      "Epoch 2, Training Loss: 0.8194506054766038\n",
      "Epoch 3, Training Loss: 0.8154336248425876\n",
      "Epoch 4, Training Loss: 0.8121150199805989\n",
      "Epoch 5, Training Loss: 0.8097871862439548\n",
      "Epoch 6, Training Loss: 0.8096034213374643\n",
      "Epoch 7, Training Loss: 0.8077522563233095\n",
      "Epoch 8, Training Loss: 0.807036375578712\n",
      "Epoch 9, Training Loss: 0.8065832558800192\n",
      "Epoch 10, Training Loss: 0.8067971752671634\n",
      "Epoch 11, Training Loss: 0.8053064774765688\n",
      "Epoch 12, Training Loss: 0.8048149207760306\n",
      "Epoch 13, Training Loss: 0.8050872844808242\n",
      "Epoch 14, Training Loss: 0.8049365774322959\n",
      "Epoch 15, Training Loss: 0.8044911134944243\n",
      "Epoch 16, Training Loss: 0.8034273410544676\n",
      "Epoch 17, Training Loss: 0.8037981279457317\n",
      "Epoch 18, Training Loss: 0.804558971489177\n",
      "Epoch 19, Training Loss: 0.8032108002550462\n",
      "Epoch 20, Training Loss: 0.8019961257541881\n",
      "Epoch 21, Training Loss: 0.8024983828909257\n",
      "Epoch 22, Training Loss: 0.8022506880058962\n",
      "Epoch 23, Training Loss: 0.8016657413454616\n",
      "Epoch 24, Training Loss: 0.8013048840971554\n",
      "Epoch 25, Training Loss: 0.8011291368568645\n",
      "Epoch 26, Training Loss: 0.8011403759788064\n",
      "Epoch 27, Training Loss: 0.8010070241900051\n",
      "Epoch 28, Training Loss: 0.8011023828562568\n",
      "Epoch 29, Training Loss: 0.8011147776070763\n",
      "Epoch 30, Training Loss: 0.800308554312762\n",
      "Epoch 31, Training Loss: 0.8005817503788892\n",
      "Epoch 32, Training Loss: 0.8002108300433439\n",
      "Epoch 33, Training Loss: 0.8011962926387787\n",
      "Epoch 34, Training Loss: 0.799687862887102\n",
      "Epoch 35, Training Loss: 0.8001845321935765\n",
      "Epoch 36, Training Loss: 0.7992016939555897\n",
      "Epoch 37, Training Loss: 0.799631311122109\n",
      "Epoch 38, Training Loss: 0.7995676032935872\n",
      "Epoch 39, Training Loss: 0.7995604627272662\n",
      "Epoch 40, Training Loss: 0.7989520809930913\n",
      "Epoch 41, Training Loss: 0.7985808613721063\n",
      "Epoch 42, Training Loss: 0.7994379685205572\n",
      "Epoch 43, Training Loss: 0.7989747059345246\n",
      "Epoch 44, Training Loss: 0.7990190631501815\n",
      "Epoch 45, Training Loss: 0.7981091026698842\n",
      "Epoch 46, Training Loss: 0.7978480543809778\n",
      "Epoch 47, Training Loss: 0.7980116924818824\n",
      "Epoch 48, Training Loss: 0.7987097039643456\n",
      "Epoch 49, Training Loss: 0.7971428963016062\n",
      "Epoch 50, Training Loss: 0.7977279701653649\n",
      "Epoch 51, Training Loss: 0.7968606549852034\n",
      "Epoch 52, Training Loss: 0.7968662131533903\n",
      "Epoch 53, Training Loss: 0.7963731236317578\n",
      "Epoch 54, Training Loss: 0.7964126167577855\n",
      "Epoch 55, Training Loss: 0.7961217817839454\n",
      "Epoch 56, Training Loss: 0.7957570105440477\n",
      "Epoch 57, Training Loss: 0.796209470314138\n",
      "Epoch 58, Training Loss: 0.795796913469539\n",
      "Epoch 59, Training Loss: 0.7961901049754199\n",
      "Epoch 60, Training Loss: 0.7962308514118195\n",
      "Epoch 61, Training Loss: 0.7953756203370935\n",
      "Epoch 62, Training Loss: 0.7960234898679397\n",
      "Epoch 63, Training Loss: 0.7952157174138462\n",
      "Epoch 64, Training Loss: 0.7955738672088174\n",
      "Epoch 65, Training Loss: 0.7947065992916331\n",
      "Epoch 66, Training Loss: 0.7949582517147065\n",
      "Epoch 67, Training Loss: 0.7950589647713829\n",
      "Epoch 68, Training Loss: 0.7949358920490041\n",
      "Epoch 69, Training Loss: 0.7954970763010137\n",
      "Epoch 70, Training Loss: 0.7950341892242432\n",
      "Epoch 71, Training Loss: 0.7944939102846034\n",
      "Epoch 72, Training Loss: 0.7941950140981113\n",
      "Epoch 73, Training Loss: 0.7946835538219003\n",
      "Epoch 74, Training Loss: 0.7945876204266268\n",
      "Epoch 75, Training Loss: 0.7938418504771064\n",
      "Epoch 76, Training Loss: 0.7935501276044284\n",
      "Epoch 77, Training Loss: 0.7939746624581954\n",
      "Epoch 78, Training Loss: 0.7931122463590958\n",
      "Epoch 79, Training Loss: 0.7933973534668193\n",
      "Epoch 80, Training Loss: 0.7929835916266722\n",
      "Epoch 81, Training Loss: 0.7928258685504689\n",
      "Epoch 82, Training Loss: 0.7934660116363974\n",
      "Epoch 83, Training Loss: 0.7928452584322762\n",
      "Epoch 84, Training Loss: 0.7933851279230679\n",
      "Epoch 85, Training Loss: 0.7926163521233727\n",
      "Epoch 86, Training Loss: 0.7920722342939938\n",
      "Epoch 87, Training Loss: 0.7927250128633836\n",
      "Epoch 88, Training Loss: 0.7921709355887244\n",
      "Epoch 89, Training Loss: 0.792050756075803\n",
      "Epoch 90, Training Loss: 0.7917156424943138\n",
      "Epoch 91, Training Loss: 0.7914529607576483\n",
      "Epoch 92, Training Loss: 0.7919732586776509\n",
      "Epoch 93, Training Loss: 0.7915622896306655\n",
      "Epoch 94, Training Loss: 0.7925368463993072\n",
      "Epoch 95, Training Loss: 0.7916790735721588\n",
      "Epoch 96, Training Loss: 0.7921611305545359\n",
      "Epoch 97, Training Loss: 0.7918375784509322\n",
      "Epoch 98, Training Loss: 0.7909045977452221\n",
      "Epoch 99, Training Loss: 0.7917150639085209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 02:59:37,863] Trial 256 finished with value: 0.6378 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7905193976794972\n",
      "Epoch 1, Training Loss: 1.015083212422249\n",
      "Epoch 2, Training Loss: 0.9485499310314207\n",
      "Epoch 3, Training Loss: 0.9342814887376656\n",
      "Epoch 4, Training Loss: 0.9273738094738552\n",
      "Epoch 5, Training Loss: 0.9216403450284686\n",
      "Epoch 6, Training Loss: 0.917080216927636\n",
      "Epoch 7, Training Loss: 0.9127426431591349\n",
      "Epoch 8, Training Loss: 0.9082601170790823\n",
      "Epoch 9, Training Loss: 0.9038956513082175\n",
      "Epoch 10, Training Loss: 0.8988327366068847\n",
      "Epoch 11, Training Loss: 0.893360414003071\n",
      "Epoch 12, Training Loss: 0.8878028165128894\n",
      "Epoch 13, Training Loss: 0.8820969414890261\n",
      "Epoch 14, Training Loss: 0.875784340836948\n",
      "Epoch 15, Training Loss: 0.8695753296515099\n",
      "Epoch 16, Training Loss: 0.8623038094742854\n",
      "Epoch 17, Training Loss: 0.856785930188975\n",
      "Epoch 18, Training Loss: 0.8499544344450298\n",
      "Epoch 19, Training Loss: 0.8443890379783803\n",
      "Epoch 20, Training Loss: 0.8390408418232337\n",
      "Epoch 21, Training Loss: 0.834635251177881\n",
      "Epoch 22, Training Loss: 0.8300831552734949\n",
      "Epoch 23, Training Loss: 0.8263829661491222\n",
      "Epoch 24, Training Loss: 0.8234276779612204\n",
      "Epoch 25, Training Loss: 0.820425387880856\n",
      "Epoch 26, Training Loss: 0.8179395819068851\n",
      "Epoch 27, Training Loss: 0.815655129296439\n",
      "Epoch 28, Training Loss: 0.81411049939636\n",
      "Epoch 29, Training Loss: 0.8126578278111336\n",
      "Epoch 30, Training Loss: 0.8114704952204138\n",
      "Epoch 31, Training Loss: 0.8101185663302142\n",
      "Epoch 32, Training Loss: 0.809376016864203\n",
      "Epoch 33, Training Loss: 0.8088996126239462\n",
      "Epoch 34, Training Loss: 0.8072188740386103\n",
      "Epoch 35, Training Loss: 0.8068754742020054\n",
      "Epoch 36, Training Loss: 0.8070311308803415\n",
      "Epoch 37, Training Loss: 0.8059473400725458\n",
      "Epoch 38, Training Loss: 0.8061054111423349\n",
      "Epoch 39, Training Loss: 0.8052547318594796\n",
      "Epoch 40, Training Loss: 0.805850705168301\n",
      "Epoch 41, Training Loss: 0.8043266144013943\n",
      "Epoch 42, Training Loss: 0.8045959595450781\n",
      "Epoch 43, Training Loss: 0.8044480077306131\n",
      "Epoch 44, Training Loss: 0.8035546898841858\n",
      "Epoch 45, Training Loss: 0.8038672990368722\n",
      "Epoch 46, Training Loss: 0.8035634649427313\n",
      "Epoch 47, Training Loss: 0.8033232368024669\n",
      "Epoch 48, Training Loss: 0.8032005543995621\n",
      "Epoch 49, Training Loss: 0.8031488513588009\n",
      "Epoch 50, Training Loss: 0.8025313058293851\n",
      "Epoch 51, Training Loss: 0.8029746745762072\n",
      "Epoch 52, Training Loss: 0.8024204970302439\n",
      "Epoch 53, Training Loss: 0.8025273816926138\n",
      "Epoch 54, Training Loss: 0.801990400758901\n",
      "Epoch 55, Training Loss: 0.8024103244444482\n",
      "Epoch 56, Training Loss: 0.8022599548325503\n",
      "Epoch 57, Training Loss: 0.8020999300748782\n",
      "Epoch 58, Training Loss: 0.8018342122995764\n",
      "Epoch 59, Training Loss: 0.8013318321758643\n",
      "Epoch 60, Training Loss: 0.8011329675975599\n",
      "Epoch 61, Training Loss: 0.8024745272514515\n",
      "Epoch 62, Training Loss: 0.8011362119724876\n",
      "Epoch 63, Training Loss: 0.8009163743571232\n",
      "Epoch 64, Training Loss: 0.8006130477987734\n",
      "Epoch 65, Training Loss: 0.8004090798080415\n",
      "Epoch 66, Training Loss: 0.800546810142976\n",
      "Epoch 67, Training Loss: 0.800676204925193\n",
      "Epoch 68, Training Loss: 0.8002969440660979\n",
      "Epoch 69, Training Loss: 0.8004082604458458\n",
      "Epoch 70, Training Loss: 0.8002940264859594\n",
      "Epoch 71, Training Loss: 0.8005284987894216\n",
      "Epoch 72, Training Loss: 0.8000379114222705\n",
      "Epoch 73, Training Loss: 0.8005869622517349\n",
      "Epoch 74, Training Loss: 0.8000537041434668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:00:37,829] Trial 257 finished with value: 0.6338 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7999222550177036\n",
      "Epoch 1, Training Loss: 0.847914829738158\n",
      "Epoch 2, Training Loss: 0.8190652403616367\n",
      "Epoch 3, Training Loss: 0.8138411065689604\n",
      "Epoch 4, Training Loss: 0.8117404113138529\n",
      "Epoch 5, Training Loss: 0.8064998805074763\n",
      "Epoch 6, Training Loss: 0.8022828042059016\n",
      "Epoch 7, Training Loss: 0.8061113889952352\n",
      "Epoch 8, Training Loss: 0.8038393062756474\n",
      "Epoch 9, Training Loss: 0.8041463871199386\n",
      "Epoch 10, Training Loss: 0.8002892835695941\n",
      "Epoch 11, Training Loss: 0.8010829571494482\n",
      "Epoch 12, Training Loss: 0.8003632387720553\n",
      "Epoch 13, Training Loss: 0.8021230069318213\n",
      "Epoch 14, Training Loss: 0.8020009064136591\n",
      "Epoch 15, Training Loss: 0.8005847644088859\n",
      "Epoch 16, Training Loss: 0.8008864855407772\n",
      "Epoch 17, Training Loss: 0.8024867219136174\n",
      "Epoch 18, Training Loss: 0.8011576470575834\n",
      "Epoch 19, Training Loss: 0.8001505436753868\n",
      "Epoch 20, Training Loss: 0.8012878141008821\n",
      "Epoch 21, Training Loss: 0.8010688385569064\n",
      "Epoch 22, Training Loss: 0.7992519024619482\n",
      "Epoch 23, Training Loss: 0.8045827951646389\n",
      "Epoch 24, Training Loss: 0.8004233363875769\n",
      "Epoch 25, Training Loss: 0.8004020544819366\n",
      "Epoch 26, Training Loss: 0.8017852491005919\n",
      "Epoch 27, Training Loss: 0.8011502865561866\n",
      "Epoch 28, Training Loss: 0.8020449467171403\n",
      "Epoch 29, Training Loss: 0.8043398651861606\n",
      "Epoch 30, Training Loss: 0.802136956928368\n",
      "Epoch 31, Training Loss: 0.8006620013624206\n",
      "Epoch 32, Training Loss: 0.8005371125120866\n",
      "Epoch 33, Training Loss: 0.7992831659496279\n",
      "Epoch 34, Training Loss: 0.8020630459140118\n",
      "Epoch 35, Training Loss: 0.7996214570855735\n",
      "Epoch 36, Training Loss: 0.799526081856032\n",
      "Epoch 37, Training Loss: 0.8010862733188429\n",
      "Epoch 38, Training Loss: 0.801322687389259\n",
      "Epoch 39, Training Loss: 0.801732241390343\n",
      "Epoch 40, Training Loss: 0.8011489836793196\n",
      "Epoch 41, Training Loss: 0.800105709509742\n",
      "Epoch 42, Training Loss: 0.8012307650164554\n",
      "Epoch 43, Training Loss: 0.8054166820712556\n",
      "Epoch 44, Training Loss: 0.7996585184470155\n",
      "Epoch 45, Training Loss: 0.7991731881199027\n",
      "Epoch 46, Training Loss: 0.7990481105962194\n",
      "Epoch 47, Training Loss: 0.801547862622971\n",
      "Epoch 48, Training Loss: 0.7977107082094465\n",
      "Epoch 49, Training Loss: 0.7994704249209927\n",
      "Epoch 50, Training Loss: 0.8009492529962295\n",
      "Epoch 51, Training Loss: 0.801028265182237\n",
      "Epoch 52, Training Loss: 0.8031275068906913\n",
      "Epoch 53, Training Loss: 0.8001779092881912\n",
      "Epoch 54, Training Loss: 0.8008166404595053\n",
      "Epoch 55, Training Loss: 0.7989511537372618\n",
      "Epoch 56, Training Loss: 0.8027308343944694\n",
      "Epoch 57, Training Loss: 0.8009625052150927\n",
      "Epoch 58, Training Loss: 0.8005303498497582\n",
      "Epoch 59, Training Loss: 0.8006039842627102\n",
      "Epoch 60, Training Loss: 0.8013230233264149\n",
      "Epoch 61, Training Loss: 0.8022058002930835\n",
      "Epoch 62, Training Loss: 0.8021789264858217\n",
      "Epoch 63, Training Loss: 0.8013300963810512\n",
      "Epoch 64, Training Loss: 0.801621711523013\n",
      "Epoch 65, Training Loss: 0.8026060757780433\n",
      "Epoch 66, Training Loss: 0.8025380626656955\n",
      "Epoch 67, Training Loss: 0.8054005222213\n",
      "Epoch 68, Training Loss: 0.8041436962615278\n",
      "Epoch 69, Training Loss: 0.8026494635675187\n",
      "Epoch 70, Training Loss: 0.8016120043016017\n",
      "Epoch 71, Training Loss: 0.8038948068045135\n",
      "Epoch 72, Training Loss: 0.8066535398476106\n",
      "Epoch 73, Training Loss: 0.8075760515560781\n",
      "Epoch 74, Training Loss: 0.8085355556100831\n",
      "Epoch 75, Training Loss: 0.807875346689296\n",
      "Epoch 76, Training Loss: 0.8092319178401975\n",
      "Epoch 77, Training Loss: 0.8102867389083805\n",
      "Epoch 78, Training Loss: 0.8107616515087902\n",
      "Epoch 79, Training Loss: 0.811496066778226\n",
      "Epoch 80, Training Loss: 0.8106406014664729\n",
      "Epoch 81, Training Loss: 0.809572325209926\n",
      "Epoch 82, Training Loss: 0.8127293665606277\n",
      "Epoch 83, Training Loss: 0.8102902539690634\n",
      "Epoch 84, Training Loss: 0.8124197198932332\n",
      "Epoch 85, Training Loss: 0.8098107907108795\n",
      "Epoch 86, Training Loss: 0.8112054883985591\n",
      "Epoch 87, Training Loss: 0.8113168987116419\n",
      "Epoch 88, Training Loss: 0.8140890984606922\n",
      "Epoch 89, Training Loss: 0.8160887819483764\n",
      "Epoch 90, Training Loss: 0.8108110172856123\n",
      "Epoch 91, Training Loss: 0.8128664377936743\n",
      "Epoch 92, Training Loss: 0.8154129609129482\n",
      "Epoch 93, Training Loss: 0.8135195930201308\n",
      "Epoch 94, Training Loss: 0.811217365049778\n",
      "Epoch 95, Training Loss: 0.8133003425777406\n",
      "Epoch 96, Training Loss: 0.8128119545771664\n",
      "Epoch 97, Training Loss: 0.8151057751555192\n",
      "Epoch 98, Training Loss: 0.8143756666577848\n",
      "Epoch 99, Training Loss: 0.8137874830038028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:02:51,458] Trial 258 finished with value: 0.631 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8127832973810066\n",
      "Epoch 1, Training Loss: 1.0948098383451763\n",
      "Epoch 2, Training Loss: 1.0856620677431723\n",
      "Epoch 3, Training Loss: 1.0792700557780446\n",
      "Epoch 4, Training Loss: 1.0737308276327033\n",
      "Epoch 5, Training Loss: 1.0685115520219157\n",
      "Epoch 6, Training Loss: 1.0630493056505246\n",
      "Epoch 7, Training Loss: 1.0571595156103148\n",
      "Epoch 8, Training Loss: 1.0509218043850777\n",
      "Epoch 9, Training Loss: 1.0440774627197953\n",
      "Epoch 10, Training Loss: 1.0364986297779513\n",
      "Epoch 11, Training Loss: 1.028770401155142\n",
      "Epoch 12, Training Loss: 1.0208850697467202\n",
      "Epoch 13, Training Loss: 1.012444484144225\n",
      "Epoch 14, Training Loss: 1.004079103918004\n",
      "Epoch 15, Training Loss: 0.9958841032551643\n",
      "Epoch 16, Training Loss: 0.9880197404918815\n",
      "Epoch 17, Training Loss: 0.9807894742578492\n",
      "Epoch 18, Training Loss: 0.9740864214144255\n",
      "Epoch 19, Training Loss: 0.9679841876926278\n",
      "Epoch 20, Training Loss: 0.9622745840173018\n",
      "Epoch 21, Training Loss: 0.9572466855658625\n",
      "Epoch 22, Training Loss: 0.9532458037362063\n",
      "Epoch 23, Training Loss: 0.9487973246359287\n",
      "Epoch 24, Training Loss: 0.9455558540229511\n",
      "Epoch 25, Training Loss: 0.9427164689042514\n",
      "Epoch 26, Training Loss: 0.9405360341072082\n",
      "Epoch 27, Training Loss: 0.939058538397452\n",
      "Epoch 28, Training Loss: 0.9370432723733716\n",
      "Epoch 29, Training Loss: 0.9354243291051765\n",
      "Epoch 30, Training Loss: 0.9343275419751504\n",
      "Epoch 31, Training Loss: 0.9330419616591662\n",
      "Epoch 32, Training Loss: 0.9315887907393893\n",
      "Epoch 33, Training Loss: 0.9308485457771702\n",
      "Epoch 34, Training Loss: 0.9297168168806492\n",
      "Epoch 35, Training Loss: 0.9287585261172818\n",
      "Epoch 36, Training Loss: 0.9280123823567441\n",
      "Epoch 37, Training Loss: 0.9265808876295735\n",
      "Epoch 38, Training Loss: 0.9254757261814032\n",
      "Epoch 39, Training Loss: 0.924470483450065\n",
      "Epoch 40, Training Loss: 0.9235749004478742\n",
      "Epoch 41, Training Loss: 0.9229696440517454\n",
      "Epoch 42, Training Loss: 0.922176384298425\n",
      "Epoch 43, Training Loss: 0.9211925822989385\n",
      "Epoch 44, Training Loss: 0.9202863564168601\n",
      "Epoch 45, Training Loss: 0.9191376241526209\n",
      "Epoch 46, Training Loss: 0.9187499654920478\n",
      "Epoch 47, Training Loss: 0.9175496509200648\n",
      "Epoch 48, Training Loss: 0.917330445203566\n",
      "Epoch 49, Training Loss: 0.9161045825571046\n",
      "Epoch 50, Training Loss: 0.9152685127760235\n",
      "Epoch 51, Training Loss: 0.9143809529175436\n",
      "Epoch 52, Training Loss: 0.9134820550904238\n",
      "Epoch 53, Training Loss: 0.9125676995829533\n",
      "Epoch 54, Training Loss: 0.9120596723448962\n",
      "Epoch 55, Training Loss: 0.9108430382900669\n",
      "Epoch 56, Training Loss: 0.9102289951833568\n",
      "Epoch 57, Training Loss: 0.9095258706494381\n",
      "Epoch 58, Training Loss: 0.9086141411523173\n",
      "Epoch 59, Training Loss: 0.9073598793574742\n",
      "Epoch 60, Training Loss: 0.9067830645052114\n",
      "Epoch 61, Training Loss: 0.9058464527130127\n",
      "Epoch 62, Training Loss: 0.9050688873556324\n",
      "Epoch 63, Training Loss: 0.9039464393056424\n",
      "Epoch 64, Training Loss: 0.9027400895168907\n",
      "Epoch 65, Training Loss: 0.9019784403922863\n",
      "Epoch 66, Training Loss: 0.9005288661870742\n",
      "Epoch 67, Training Loss: 0.8997151511952393\n",
      "Epoch 68, Training Loss: 0.8981691719894123\n",
      "Epoch 69, Training Loss: 0.897317832305019\n",
      "Epoch 70, Training Loss: 0.8963860967105493\n",
      "Epoch 71, Training Loss: 0.8950814919364184\n",
      "Epoch 72, Training Loss: 0.8934245931474786\n",
      "Epoch 73, Training Loss: 0.8921126483974601\n",
      "Epoch 74, Training Loss: 0.8913725739134881\n",
      "Epoch 75, Training Loss: 0.8892921792833428\n",
      "Epoch 76, Training Loss: 0.8880167967394779\n",
      "Epoch 77, Training Loss: 0.8865925156084219\n",
      "Epoch 78, Training Loss: 0.8848336377538236\n",
      "Epoch 79, Training Loss: 0.8832862851314975\n",
      "Epoch 80, Training Loss: 0.8812411208798114\n",
      "Epoch 81, Training Loss: 0.8794975912660584\n",
      "Epoch 82, Training Loss: 0.8782790702984745\n",
      "Epoch 83, Training Loss: 0.8754936880635139\n",
      "Epoch 84, Training Loss: 0.8736069497309233\n",
      "Epoch 85, Training Loss: 0.871449981954761\n",
      "Epoch 86, Training Loss: 0.8691540626654948\n",
      "Epoch 87, Training Loss: 0.867044546640009\n",
      "Epoch 88, Training Loss: 0.8647884863659852\n",
      "Epoch 89, Training Loss: 0.8623533148514597\n",
      "Epoch 90, Training Loss: 0.8597748059975473\n",
      "Epoch 91, Training Loss: 0.857479981641124\n",
      "Epoch 92, Training Loss: 0.8546712178932993\n",
      "Epoch 93, Training Loss: 0.8527639563818623\n",
      "Epoch 94, Training Loss: 0.8494670611575134\n",
      "Epoch 95, Training Loss: 0.8469628074115381\n",
      "Epoch 96, Training Loss: 0.8453167570264716\n",
      "Epoch 97, Training Loss: 0.8422867324119224\n",
      "Epoch 98, Training Loss: 0.8403014663466833\n",
      "Epoch 99, Training Loss: 0.8373113871517038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:04:34,965] Trial 259 finished with value: 0.6146666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8351475738941279\n",
      "Epoch 1, Training Loss: 0.9805977631332283\n",
      "Epoch 2, Training Loss: 0.9399312022037076\n",
      "Epoch 3, Training Loss: 0.9178341965926321\n",
      "Epoch 4, Training Loss: 0.8885163135098335\n",
      "Epoch 5, Training Loss: 0.8571709966301022\n",
      "Epoch 6, Training Loss: 0.835902330212127\n",
      "Epoch 7, Training Loss: 0.8245847184855238\n",
      "Epoch 8, Training Loss: 0.8190182623110319\n",
      "Epoch 9, Training Loss: 0.8154073539533113\n",
      "Epoch 10, Training Loss: 0.8130123405528248\n",
      "Epoch 11, Training Loss: 0.8118026091640157\n",
      "Epoch 12, Training Loss: 0.8100022907543899\n",
      "Epoch 13, Training Loss: 0.8078893400224528\n",
      "Epoch 14, Training Loss: 0.8074104076041315\n",
      "Epoch 15, Training Loss: 0.8071573725320342\n",
      "Epoch 16, Training Loss: 0.8059684047125336\n",
      "Epoch 17, Training Loss: 0.8056202280790286\n",
      "Epoch 18, Training Loss: 0.8042781504473292\n",
      "Epoch 19, Training Loss: 0.8040135537771355\n",
      "Epoch 20, Training Loss: 0.8035802937091742\n",
      "Epoch 21, Training Loss: 0.8023880046113093\n",
      "Epoch 22, Training Loss: 0.8025621784360786\n",
      "Epoch 23, Training Loss: 0.8017105834824698\n",
      "Epoch 24, Training Loss: 0.8021605298035127\n",
      "Epoch 25, Training Loss: 0.8010470754221866\n",
      "Epoch 26, Training Loss: 0.8012366043893915\n",
      "Epoch 27, Training Loss: 0.8008870797049731\n",
      "Epoch 28, Training Loss: 0.8011185729414001\n",
      "Epoch 29, Training Loss: 0.8005497954841844\n",
      "Epoch 30, Training Loss: 0.800273431064491\n",
      "Epoch 31, Training Loss: 0.8001249444215818\n",
      "Epoch 32, Training Loss: 0.7997016975754185\n",
      "Epoch 33, Training Loss: 0.7992489981472044\n",
      "Epoch 34, Training Loss: 0.7994397351616307\n",
      "Epoch 35, Training Loss: 0.79965897180084\n",
      "Epoch 36, Training Loss: 0.7990687916153356\n",
      "Epoch 37, Training Loss: 0.7991371614592416\n",
      "Epoch 38, Training Loss: 0.7982706053812701\n",
      "Epoch 39, Training Loss: 0.798916450927132\n",
      "Epoch 40, Training Loss: 0.7991718645382645\n",
      "Epoch 41, Training Loss: 0.7984906049151169\n",
      "Epoch 42, Training Loss: 0.798378106346704\n",
      "Epoch 43, Training Loss: 0.7985971828152363\n",
      "Epoch 44, Training Loss: 0.7980885261879828\n",
      "Epoch 45, Training Loss: 0.7985633428831745\n",
      "Epoch 46, Training Loss: 0.7974125387077045\n",
      "Epoch 47, Training Loss: 0.7973062109230156\n",
      "Epoch 48, Training Loss: 0.7975776002819377\n",
      "Epoch 49, Training Loss: 0.7973996570235804\n",
      "Epoch 50, Training Loss: 0.7972584430436442\n",
      "Epoch 51, Training Loss: 0.797791958661904\n",
      "Epoch 52, Training Loss: 0.7979400886628861\n",
      "Epoch 53, Training Loss: 0.7970962301232761\n",
      "Epoch 54, Training Loss: 0.7970335383164255\n",
      "Epoch 55, Training Loss: 0.797278736767016\n",
      "Epoch 56, Training Loss: 0.7970645297738842\n",
      "Epoch 57, Training Loss: 0.7965020658378315\n",
      "Epoch 58, Training Loss: 0.7972105915385082\n",
      "Epoch 59, Training Loss: 0.7970103411746204\n",
      "Epoch 60, Training Loss: 0.7966260037027804\n",
      "Epoch 61, Training Loss: 0.7965270879573392\n",
      "Epoch 62, Training Loss: 0.7967490110182225\n",
      "Epoch 63, Training Loss: 0.7960819344771536\n",
      "Epoch 64, Training Loss: 0.7957656342284124\n",
      "Epoch 65, Training Loss: 0.7957635559085616\n",
      "Epoch 66, Training Loss: 0.7960666578515132\n",
      "Epoch 67, Training Loss: 0.7958692957584123\n",
      "Epoch 68, Training Loss: 0.7958669053880791\n",
      "Epoch 69, Training Loss: 0.7959479470898334\n",
      "Epoch 70, Training Loss: 0.7956331652806218\n",
      "Epoch 71, Training Loss: 0.7954113225739702\n",
      "Epoch 72, Training Loss: 0.7951937040888277\n",
      "Epoch 73, Training Loss: 0.7953988846083333\n",
      "Epoch 74, Training Loss: 0.7952128914065827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:05:44,537] Trial 260 finished with value: 0.6310666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.79535175741167\n",
      "Epoch 1, Training Loss: 1.0025796525460438\n",
      "Epoch 2, Training Loss: 0.9277853399291074\n",
      "Epoch 3, Training Loss: 0.8798710450193936\n",
      "Epoch 4, Training Loss: 0.8235269230111201\n",
      "Epoch 5, Training Loss: 0.8154609172864068\n",
      "Epoch 6, Training Loss: 0.811958832937972\n",
      "Epoch 7, Training Loss: 0.8124443905694144\n",
      "Epoch 8, Training Loss: 0.8113372683525085\n",
      "Epoch 9, Training Loss: 0.8097855919285825\n",
      "Epoch 10, Training Loss: 0.8089316541987254\n",
      "Epoch 11, Training Loss: 0.8082022334848131\n",
      "Epoch 12, Training Loss: 0.8073459454048845\n",
      "Epoch 13, Training Loss: 0.8077832230051657\n",
      "Epoch 14, Training Loss: 0.8063619673700261\n",
      "Epoch 15, Training Loss: 0.805379231173293\n",
      "Epoch 16, Training Loss: 0.8059357250543465\n",
      "Epoch 17, Training Loss: 0.8044683613275226\n",
      "Epoch 18, Training Loss: 0.8038953927226533\n",
      "Epoch 19, Training Loss: 0.8039497085980006\n",
      "Epoch 20, Training Loss: 0.8029206831652419\n",
      "Epoch 21, Training Loss: 0.8026166696297495\n",
      "Epoch 22, Training Loss: 0.8017360614654713\n",
      "Epoch 23, Training Loss: 0.8023257139033841\n",
      "Epoch 24, Training Loss: 0.8017763592246779\n",
      "Epoch 25, Training Loss: 0.8018258708760254\n",
      "Epoch 26, Training Loss: 0.8016471141263058\n",
      "Epoch 27, Training Loss: 0.8006610930414129\n",
      "Epoch 28, Training Loss: 0.801038022059247\n",
      "Epoch 29, Training Loss: 0.8000363481672187\n",
      "Epoch 30, Training Loss: 0.8002439933611935\n",
      "Epoch 31, Training Loss: 0.7995703870192507\n",
      "Epoch 32, Training Loss: 0.7994067822183881\n",
      "Epoch 33, Training Loss: 0.798739271638985\n",
      "Epoch 34, Training Loss: 0.7996030102995105\n",
      "Epoch 35, Training Loss: 0.7989831037987444\n",
      "Epoch 36, Training Loss: 0.7990302414822399\n",
      "Epoch 37, Training Loss: 0.7983438595793301\n",
      "Epoch 38, Training Loss: 0.7971523921292527\n",
      "Epoch 39, Training Loss: 0.7967407253451814\n",
      "Epoch 40, Training Loss: 0.7970407195557329\n",
      "Epoch 41, Training Loss: 0.7970175165879099\n",
      "Epoch 42, Training Loss: 0.796564862737082\n",
      "Epoch 43, Training Loss: 0.7958208525987496\n",
      "Epoch 44, Training Loss: 0.7947610861376713\n",
      "Epoch 45, Training Loss: 0.7948162173866329\n",
      "Epoch 46, Training Loss: 0.7949117327991285\n",
      "Epoch 47, Training Loss: 0.7944354607646626\n",
      "Epoch 48, Training Loss: 0.7935342596885854\n",
      "Epoch 49, Training Loss: 0.7945769932037009\n",
      "Epoch 50, Training Loss: 0.7934555730425326\n",
      "Epoch 51, Training Loss: 0.7917409846657201\n",
      "Epoch 52, Training Loss: 0.791663208222927\n",
      "Epoch 53, Training Loss: 0.7914904078146568\n",
      "Epoch 54, Training Loss: 0.7903034044387646\n",
      "Epoch 55, Training Loss: 0.7909752647679551\n",
      "Epoch 56, Training Loss: 0.7898474376004442\n",
      "Epoch 57, Training Loss: 0.7899472407828596\n",
      "Epoch 58, Training Loss: 0.7886182258003637\n",
      "Epoch 59, Training Loss: 0.7892941838816593\n",
      "Epoch 60, Training Loss: 0.7887150488401714\n",
      "Epoch 61, Training Loss: 0.7880366312381917\n",
      "Epoch 62, Training Loss: 0.7890248177643109\n",
      "Epoch 63, Training Loss: 0.7884879119414135\n",
      "Epoch 64, Training Loss: 0.7884131997151482\n",
      "Epoch 65, Training Loss: 0.7875575586369163\n",
      "Epoch 66, Training Loss: 0.78756290279833\n",
      "Epoch 67, Training Loss: 0.7868353136500021\n",
      "Epoch 68, Training Loss: 0.7864863411824506\n",
      "Epoch 69, Training Loss: 0.7868640602979445\n",
      "Epoch 70, Training Loss: 0.785936567299348\n",
      "Epoch 71, Training Loss: 0.7858644709551245\n",
      "Epoch 72, Training Loss: 0.7862936028860565\n",
      "Epoch 73, Training Loss: 0.7863963130721472\n",
      "Epoch 74, Training Loss: 0.7852035936556364\n",
      "Epoch 75, Training Loss: 0.7863644611566587\n",
      "Epoch 76, Training Loss: 0.7845689386353457\n",
      "Epoch 77, Training Loss: 0.785582629271916\n",
      "Epoch 78, Training Loss: 0.7849305788377174\n",
      "Epoch 79, Training Loss: 0.7848675589812429\n",
      "Epoch 80, Training Loss: 0.7848751181946662\n",
      "Epoch 81, Training Loss: 0.7838088831507174\n",
      "Epoch 82, Training Loss: 0.7844620074544634\n",
      "Epoch 83, Training Loss: 0.784069425898387\n",
      "Epoch 84, Training Loss: 0.7843256926178036\n",
      "Epoch 85, Training Loss: 0.7838885017803737\n",
      "Epoch 86, Training Loss: 0.7838271282669297\n",
      "Epoch 87, Training Loss: 0.7840371678646346\n",
      "Epoch 88, Training Loss: 0.783839219674132\n",
      "Epoch 89, Training Loss: 0.783106603658289\n",
      "Epoch 90, Training Loss: 0.7850971029217082\n",
      "Epoch 91, Training Loss: 0.7835446415539075\n",
      "Epoch 92, Training Loss: 0.7836058558378004\n",
      "Epoch 93, Training Loss: 0.7833033543780334\n",
      "Epoch 94, Training Loss: 0.783425413755546\n",
      "Epoch 95, Training Loss: 0.7838707758968038\n",
      "Epoch 96, Training Loss: 0.7834128897889217\n",
      "Epoch 97, Training Loss: 0.7827429309823459\n",
      "Epoch 98, Training Loss: 0.7832010461871786\n",
      "Epoch 99, Training Loss: 0.7839251297757142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:07:58,580] Trial 261 finished with value: 0.6396 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7828857042735681\n",
      "Epoch 1, Training Loss: 0.89833235726637\n",
      "Epoch 2, Training Loss: 0.8202579114016365\n",
      "Epoch 3, Training Loss: 0.813745771225761\n",
      "Epoch 4, Training Loss: 0.8112573597010444\n",
      "Epoch 5, Training Loss: 0.8092188790265251\n",
      "Epoch 6, Training Loss: 0.8077067969827091\n",
      "Epoch 7, Training Loss: 0.8071963505183949\n",
      "Epoch 8, Training Loss: 0.8063766342050889\n",
      "Epoch 9, Training Loss: 0.8056341546423295\n",
      "Epoch 10, Training Loss: 0.8058980474051307\n",
      "Epoch 11, Training Loss: 0.8052043972997105\n",
      "Epoch 12, Training Loss: 0.8045524254967185\n",
      "Epoch 13, Training Loss: 0.8040842049262102\n",
      "Epoch 14, Training Loss: 0.8036394133287318\n",
      "Epoch 15, Training Loss: 0.8043398238630856\n",
      "Epoch 16, Training Loss: 0.8032097634848426\n",
      "Epoch 17, Training Loss: 0.8028520598131068\n",
      "Epoch 18, Training Loss: 0.8029306324089275\n",
      "Epoch 19, Training Loss: 0.802527447237688\n",
      "Epoch 20, Training Loss: 0.80259589658064\n",
      "Epoch 21, Training Loss: 0.8026510042302749\n",
      "Epoch 22, Training Loss: 0.8024349102553199\n",
      "Epoch 23, Training Loss: 0.80271813231356\n",
      "Epoch 24, Training Loss: 0.8021362832013299\n",
      "Epoch 25, Training Loss: 0.8014322491954354\n",
      "Epoch 26, Training Loss: 0.8012573367707869\n",
      "Epoch 27, Training Loss: 0.8017158637327306\n",
      "Epoch 28, Training Loss: 0.8012233744649326\n",
      "Epoch 29, Training Loss: 0.8012638883029713\n",
      "Epoch 30, Training Loss: 0.8012686530982747\n",
      "Epoch 31, Training Loss: 0.8007386098889744\n",
      "Epoch 32, Training Loss: 0.8009809102030362\n",
      "Epoch 33, Training Loss: 0.8007357523721808\n",
      "Epoch 34, Training Loss: 0.800380351823919\n",
      "Epoch 35, Training Loss: 0.8002949560389799\n",
      "Epoch 36, Training Loss: 0.8002145238483653\n",
      "Epoch 37, Training Loss: 0.8001321810834549\n",
      "Epoch 38, Training Loss: 0.7999960158151739\n",
      "Epoch 39, Training Loss: 0.7998070916007547\n",
      "Epoch 40, Training Loss: 0.7997684920535368\n",
      "Epoch 41, Training Loss: 0.7990488649817074\n",
      "Epoch 42, Training Loss: 0.7994912294079276\n",
      "Epoch 43, Training Loss: 0.798954829748939\n",
      "Epoch 44, Training Loss: 0.7990747292602763\n",
      "Epoch 45, Training Loss: 0.7988525022478664\n",
      "Epoch 46, Training Loss: 0.7988215766934788\n",
      "Epoch 47, Training Loss: 0.798293596015257\n",
      "Epoch 48, Training Loss: 0.7974463566611795\n",
      "Epoch 49, Training Loss: 0.7982079634245705\n",
      "Epoch 50, Training Loss: 0.7979647686902215\n",
      "Epoch 51, Training Loss: 0.7979984793943518\n",
      "Epoch 52, Training Loss: 0.7974024767735425\n",
      "Epoch 53, Training Loss: 0.7979784758651958\n",
      "Epoch 54, Training Loss: 0.796782481880749\n",
      "Epoch 55, Training Loss: 0.7966106072594138\n",
      "Epoch 56, Training Loss: 0.7969807317677666\n",
      "Epoch 57, Training Loss: 0.7969066223677467\n",
      "Epoch 58, Training Loss: 0.7962649860101587\n",
      "Epoch 59, Training Loss: 0.7967957938418669\n",
      "Epoch 60, Training Loss: 0.7967423608022578\n",
      "Epoch 61, Training Loss: 0.7965095002510968\n",
      "Epoch 62, Training Loss: 0.7964141369567198\n",
      "Epoch 63, Training Loss: 0.7964522126842948\n",
      "Epoch 64, Training Loss: 0.7964845999549417\n",
      "Epoch 65, Training Loss: 0.796625819907469\n",
      "Epoch 66, Training Loss: 0.7961487607394948\n",
      "Epoch 67, Training Loss: 0.795393156935187\n",
      "Epoch 68, Training Loss: 0.7959120246943305\n",
      "Epoch 69, Training Loss: 0.7956480069020215\n",
      "Epoch 70, Training Loss: 0.7953647591787226\n",
      "Epoch 71, Training Loss: 0.7952922919918509\n",
      "Epoch 72, Training Loss: 0.7948753224400913\n",
      "Epoch 73, Training Loss: 0.7947093644562889\n",
      "Epoch 74, Training Loss: 0.795065460134955\n",
      "Epoch 75, Training Loss: 0.7953018975257874\n",
      "Epoch 76, Training Loss: 0.79505540525212\n",
      "Epoch 77, Training Loss: 0.7950058862041025\n",
      "Epoch 78, Training Loss: 0.7945901036963743\n",
      "Epoch 79, Training Loss: 0.7949938899629256\n",
      "Epoch 80, Training Loss: 0.7948804501926198\n",
      "Epoch 81, Training Loss: 0.7941160950239967\n",
      "Epoch 82, Training Loss: 0.7946459145405713\n",
      "Epoch 83, Training Loss: 0.7941731220133165\n",
      "Epoch 84, Training Loss: 0.7938068807125092\n",
      "Epoch 85, Training Loss: 0.793916220664978\n",
      "Epoch 86, Training Loss: 0.7944253629796645\n",
      "Epoch 87, Training Loss: 0.7933738174157984\n",
      "Epoch 88, Training Loss: 0.7934279915164498\n",
      "Epoch 89, Training Loss: 0.7937178911882289\n",
      "Epoch 90, Training Loss: 0.7929454567853143\n",
      "Epoch 91, Training Loss: 0.7929639072277966\n",
      "Epoch 92, Training Loss: 0.7930852124270271\n",
      "Epoch 93, Training Loss: 0.7926238557170419\n",
      "Epoch 94, Training Loss: 0.7926248846334569\n",
      "Epoch 95, Training Loss: 0.7929887457454906\n",
      "Epoch 96, Training Loss: 0.7924149535684024\n",
      "Epoch 97, Training Loss: 0.7918571735830868\n",
      "Epoch 98, Training Loss: 0.7925243195365457\n",
      "Epoch 99, Training Loss: 0.7920282095320085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:09:55,863] Trial 262 finished with value: 0.6380666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7917905693194446\n",
      "Epoch 1, Training Loss: 0.8386774506989647\n",
      "Epoch 2, Training Loss: 0.8086652177221635\n",
      "Epoch 3, Training Loss: 0.8025900908077465\n",
      "Epoch 4, Training Loss: 0.8005040509560529\n",
      "Epoch 5, Training Loss: 0.8005039156885708\n",
      "Epoch 6, Training Loss: 0.7999034559726715\n",
      "Epoch 7, Training Loss: 0.7967489705366246\n",
      "Epoch 8, Training Loss: 0.7966604509774377\n",
      "Epoch 9, Training Loss: 0.7948730093591353\n",
      "Epoch 10, Training Loss: 0.7958021603612339\n",
      "Epoch 11, Training Loss: 0.7952784500402563\n",
      "Epoch 12, Training Loss: 0.7955188743507161\n",
      "Epoch 13, Training Loss: 0.7939791747401742\n",
      "Epoch 14, Training Loss: 0.7947427476854886\n",
      "Epoch 15, Training Loss: 0.7935767507553101\n",
      "Epoch 16, Training Loss: 0.7928774562302757\n",
      "Epoch 17, Training Loss: 0.7931505084037781\n",
      "Epoch 18, Training Loss: 0.7938755987672245\n",
      "Epoch 19, Training Loss: 0.79208811991355\n",
      "Epoch 20, Training Loss: 0.7924203315201928\n",
      "Epoch 21, Training Loss: 0.7923704312128179\n",
      "Epoch 22, Training Loss: 0.7919930425812216\n",
      "Epoch 23, Training Loss: 0.7921228821137373\n",
      "Epoch 24, Training Loss: 0.7916274534954745\n",
      "Epoch 25, Training Loss: 0.7920364455615773\n",
      "Epoch 26, Training Loss: 0.7917632224980523\n",
      "Epoch 27, Training Loss: 0.7909839949888342\n",
      "Epoch 28, Training Loss: 0.7914664067941554\n",
      "Epoch 29, Training Loss: 0.792221802893807\n",
      "Epoch 30, Training Loss: 0.7911365715195151\n",
      "Epoch 31, Training Loss: 0.7921633948999293\n",
      "Epoch 32, Training Loss: 0.7919023420530207\n",
      "Epoch 33, Training Loss: 0.7912520198962267\n",
      "Epoch 34, Training Loss: 0.7924131744749405\n",
      "Epoch 35, Training Loss: 0.7912096510915195\n",
      "Epoch 36, Training Loss: 0.7917136983310475\n",
      "Epoch 37, Training Loss: 0.791087886095047\n",
      "Epoch 38, Training Loss: 0.7912689946679508\n",
      "Epoch 39, Training Loss: 0.7912098125149222\n",
      "Epoch 40, Training Loss: 0.7914046056130353\n",
      "Epoch 41, Training Loss: 0.7898326678837047\n",
      "Epoch 42, Training Loss: 0.7903218854174895\n",
      "Epoch 43, Training Loss: 0.7897320663928986\n",
      "Epoch 44, Training Loss: 0.7914116260584663\n",
      "Epoch 45, Training Loss: 0.7904635911829332\n",
      "Epoch 46, Training Loss: 0.7898603515765247\n",
      "Epoch 47, Training Loss: 0.789751667064779\n",
      "Epoch 48, Training Loss: 0.7901911090402042\n",
      "Epoch 49, Training Loss: 0.7900735602659338\n",
      "Epoch 50, Training Loss: 0.7900998849027298\n",
      "Epoch 51, Training Loss: 0.7897164462594425\n",
      "Epoch 52, Training Loss: 0.7904188523573034\n",
      "Epoch 53, Training Loss: 0.7892958900507758\n",
      "Epoch 54, Training Loss: 0.7903408483897938\n",
      "Epoch 55, Training Loss: 0.7896362688962151\n",
      "Epoch 56, Training Loss: 0.79017288888202\n",
      "Epoch 57, Training Loss: 0.7906149852275849\n",
      "Epoch 58, Training Loss: 0.7903325522647184\n",
      "Epoch 59, Training Loss: 0.7891426103255328\n",
      "Epoch 60, Training Loss: 0.7893312601482168\n",
      "Epoch 61, Training Loss: 0.7897444081306457\n",
      "Epoch 62, Training Loss: 0.7889915311336517\n",
      "Epoch 63, Training Loss: 0.790130779953564\n",
      "Epoch 64, Training Loss: 0.7894595836190617\n",
      "Epoch 65, Training Loss: 0.7895127757857827\n",
      "Epoch 66, Training Loss: 0.7891803859261906\n",
      "Epoch 67, Training Loss: 0.7895070148215574\n",
      "Epoch 68, Training Loss: 0.7891117143630981\n",
      "Epoch 69, Training Loss: 0.7890421459254097\n",
      "Epoch 70, Training Loss: 0.7897479183533612\n",
      "Epoch 71, Training Loss: 0.7884442863043617\n",
      "Epoch 72, Training Loss: 0.7894314461595872\n",
      "Epoch 73, Training Loss: 0.7889115508163677\n",
      "Epoch 74, Training Loss: 0.7888567554249483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:11:36,854] Trial 263 finished with value: 0.6374666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7896577894687653\n",
      "Epoch 1, Training Loss: 0.8532494106687101\n",
      "Epoch 2, Training Loss: 0.8200919571675752\n",
      "Epoch 3, Training Loss: 0.8165932567496049\n",
      "Epoch 4, Training Loss: 0.8138537575427751\n",
      "Epoch 5, Training Loss: 0.8112945679435156\n",
      "Epoch 6, Training Loss: 0.8117163927035225\n",
      "Epoch 7, Training Loss: 0.8098037010744998\n",
      "Epoch 8, Training Loss: 0.8096186507017092\n",
      "Epoch 9, Training Loss: 0.8074233113374926\n",
      "Epoch 10, Training Loss: 0.8062949401991708\n",
      "Epoch 11, Training Loss: 0.8059088329623516\n",
      "Epoch 12, Training Loss: 0.806178713472266\n",
      "Epoch 13, Training Loss: 0.8066654937607901\n",
      "Epoch 14, Training Loss: 0.8057484421514927\n",
      "Epoch 15, Training Loss: 0.8062106573492065\n",
      "Epoch 16, Training Loss: 0.8061599438351796\n",
      "Epoch 17, Training Loss: 0.8060613243203414\n",
      "Epoch 18, Training Loss: 0.8051875613685837\n",
      "Epoch 19, Training Loss: 0.8050059884114373\n",
      "Epoch 20, Training Loss: 0.8049045619211699\n",
      "Epoch 21, Training Loss: 0.8050638934723416\n",
      "Epoch 22, Training Loss: 0.8053141999961738\n",
      "Epoch 23, Training Loss: 0.8041183132874338\n",
      "Epoch 24, Training Loss: 0.8063377944150365\n",
      "Epoch 25, Training Loss: 0.8050132949549452\n",
      "Epoch 26, Training Loss: 0.8034827692168099\n",
      "Epoch 27, Training Loss: 0.8032336169615724\n",
      "Epoch 28, Training Loss: 0.8045744600152611\n",
      "Epoch 29, Training Loss: 0.8039279509307746\n",
      "Epoch 30, Training Loss: 0.8026833581745176\n",
      "Epoch 31, Training Loss: 0.8045823228986639\n",
      "Epoch 32, Training Loss: 0.8029252874223809\n",
      "Epoch 33, Training Loss: 0.8033443288695543\n",
      "Epoch 34, Training Loss: 0.8033354173925586\n",
      "Epoch 35, Training Loss: 0.8035962603145972\n",
      "Epoch 36, Training Loss: 0.8034914614562702\n",
      "Epoch 37, Training Loss: 0.8035340792254397\n",
      "Epoch 38, Training Loss: 0.8024983855118429\n",
      "Epoch 39, Training Loss: 0.8031501304834409\n",
      "Epoch 40, Training Loss: 0.8027041790180637\n",
      "Epoch 41, Training Loss: 0.8031676160661798\n",
      "Epoch 42, Training Loss: 0.8019144993079336\n",
      "Epoch 43, Training Loss: 0.8019918486587984\n",
      "Epoch 44, Training Loss: 0.8026991597691873\n",
      "Epoch 45, Training Loss: 0.8010487386158535\n",
      "Epoch 46, Training Loss: 0.8020125945707909\n",
      "Epoch 47, Training Loss: 0.8017386175636062\n",
      "Epoch 48, Training Loss: 0.8021192017354464\n",
      "Epoch 49, Training Loss: 0.8013371812669854\n",
      "Epoch 50, Training Loss: 0.8010694208001732\n",
      "Epoch 51, Training Loss: 0.7999461507438717\n",
      "Epoch 52, Training Loss: 0.7999298911345633\n",
      "Epoch 53, Training Loss: 0.7999296453662385\n",
      "Epoch 54, Training Loss: 0.8009993540613275\n",
      "Epoch 55, Training Loss: 0.7993694966000722\n",
      "Epoch 56, Training Loss: 0.8003730577633793\n",
      "Epoch 57, Training Loss: 0.8001876682267153\n",
      "Epoch 58, Training Loss: 0.7990282754252728\n",
      "Epoch 59, Training Loss: 0.7997992427725541\n",
      "Epoch 60, Training Loss: 0.7978279595984552\n",
      "Epoch 61, Training Loss: 0.7986031989405926\n",
      "Epoch 62, Training Loss: 0.7984746102103614\n",
      "Epoch 63, Training Loss: 0.7982702851295471\n",
      "Epoch 64, Training Loss: 0.7977078070317892\n",
      "Epoch 65, Training Loss: 0.7983579327289323\n",
      "Epoch 66, Training Loss: 0.7974510350621733\n",
      "Epoch 67, Training Loss: 0.7977695328848703\n",
      "Epoch 68, Training Loss: 0.7979041216965008\n",
      "Epoch 69, Training Loss: 0.7969963568493836\n",
      "Epoch 70, Training Loss: 0.7981298180451071\n",
      "Epoch 71, Training Loss: 0.7964061714652786\n",
      "Epoch 72, Training Loss: 0.7962924387221946\n",
      "Epoch 73, Training Loss: 0.7973825002971449\n",
      "Epoch 74, Training Loss: 0.796258468825118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:12:50,723] Trial 264 finished with value: 0.6316666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7955353124697405\n",
      "Epoch 1, Training Loss: 0.8492322900715996\n",
      "Epoch 2, Training Loss: 0.8189908942054299\n",
      "Epoch 3, Training Loss: 0.8151992257202373\n",
      "Epoch 4, Training Loss: 0.8113136927520528\n",
      "Epoch 5, Training Loss: 0.8077978483368369\n",
      "Epoch 6, Training Loss: 0.8063879581058726\n",
      "Epoch 7, Training Loss: 0.8070182472116807\n",
      "Epoch 8, Training Loss: 0.8074346797606524\n",
      "Epoch 9, Training Loss: 0.8052026055840885\n",
      "Epoch 10, Training Loss: 0.8058903396129609\n",
      "Epoch 11, Training Loss: 0.8045898111427532\n",
      "Epoch 12, Training Loss: 0.8033424984006321\n",
      "Epoch 13, Training Loss: 0.8050522466968087\n",
      "Epoch 14, Training Loss: 0.8036818225243513\n",
      "Epoch 15, Training Loss: 0.8020770785387824\n",
      "Epoch 16, Training Loss: 0.8017012770035687\n",
      "Epoch 17, Training Loss: 0.8018895839242374\n",
      "Epoch 18, Training Loss: 0.8024127637638765\n",
      "Epoch 19, Training Loss: 0.8005165816755856\n",
      "Epoch 20, Training Loss: 0.8003982096559861\n",
      "Epoch 21, Training Loss: 0.8020383559255039\n",
      "Epoch 22, Training Loss: 0.799927570328993\n",
      "Epoch 23, Training Loss: 0.7998750073068283\n",
      "Epoch 24, Training Loss: 0.800228751336827\n",
      "Epoch 25, Training Loss: 0.8001664544554318\n",
      "Epoch 26, Training Loss: 0.7991856379368726\n",
      "Epoch 27, Training Loss: 0.8004124304126291\n",
      "Epoch 28, Training Loss: 0.8006453317754408\n",
      "Epoch 29, Training Loss: 0.7989601310561685\n",
      "Epoch 30, Training Loss: 0.7988038091098562\n",
      "Epoch 31, Training Loss: 0.7984184202025918\n",
      "Epoch 32, Training Loss: 0.8001492595672608\n",
      "Epoch 33, Training Loss: 0.8008603081282447\n",
      "Epoch 34, Training Loss: 0.8001428879709804\n",
      "Epoch 35, Training Loss: 0.798475460094564\n",
      "Epoch 36, Training Loss: 0.7985483413584092\n",
      "Epoch 37, Training Loss: 0.7996435931149651\n",
      "Epoch 38, Training Loss: 0.7989702245067147\n",
      "Epoch 39, Training Loss: 0.80039209912805\n",
      "Epoch 40, Training Loss: 0.79851638155825\n",
      "Epoch 41, Training Loss: 0.7986879341742572\n",
      "Epoch 42, Training Loss: 0.7982035165674546\n",
      "Epoch 43, Training Loss: 0.7984683745047625\n",
      "Epoch 44, Training Loss: 0.7981625742772046\n",
      "Epoch 45, Training Loss: 0.798433100195492\n",
      "Epoch 46, Training Loss: 0.7984346905175377\n",
      "Epoch 47, Training Loss: 0.7989455526716569\n",
      "Epoch 48, Training Loss: 0.7999848520755768\n",
      "Epoch 49, Training Loss: 0.7979033202984753\n",
      "Epoch 50, Training Loss: 0.7974545141528634\n",
      "Epoch 51, Training Loss: 0.7983637095198912\n",
      "Epoch 52, Training Loss: 0.7983727582763223\n",
      "Epoch 53, Training Loss: 0.798476712212843\n",
      "Epoch 54, Training Loss: 0.7985704959140104\n",
      "Epoch 55, Training Loss: 0.7976279619862051\n",
      "Epoch 56, Training Loss: 0.7993474228943096\n",
      "Epoch 57, Training Loss: 0.7980726943997776\n",
      "Epoch 58, Training Loss: 0.7977335129064672\n",
      "Epoch 59, Training Loss: 0.7976858141843011\n",
      "Epoch 60, Training Loss: 0.7984450025418225\n",
      "Epoch 61, Training Loss: 0.7988328753499424\n",
      "Epoch 62, Training Loss: 0.7984261600410237\n",
      "Epoch 63, Training Loss: 0.7985587063256432\n",
      "Epoch 64, Training Loss: 0.7980222388576059\n",
      "Epoch 65, Training Loss: 0.7979928797132829\n",
      "Epoch 66, Training Loss: 0.7984520255818086\n",
      "Epoch 67, Training Loss: 0.7984527700087604\n",
      "Epoch 68, Training Loss: 0.7977185514393975\n",
      "Epoch 69, Training Loss: 0.7973188358194688\n",
      "Epoch 70, Training Loss: 0.7973848084842458\n",
      "Epoch 71, Training Loss: 0.7983537694987128\n",
      "Epoch 72, Training Loss: 0.7971856715398676\n",
      "Epoch 73, Training Loss: 0.7991721493356368\n",
      "Epoch 74, Training Loss: 0.7983847984145669\n",
      "Epoch 75, Training Loss: 0.7972401501851923\n",
      "Epoch 76, Training Loss: 0.7979314548127792\n",
      "Epoch 77, Training Loss: 0.7971993180583505\n",
      "Epoch 78, Training Loss: 0.7967327429266537\n",
      "Epoch 79, Training Loss: 0.7991222868947422\n",
      "Epoch 80, Training Loss: 0.7988607354023878\n",
      "Epoch 81, Training Loss: 0.7978110215243172\n",
      "Epoch 82, Training Loss: 0.799140896656934\n",
      "Epoch 83, Training Loss: 0.7976979785105761\n",
      "Epoch 84, Training Loss: 0.7983347615073709\n",
      "Epoch 85, Training Loss: 0.798042905961766\n",
      "Epoch 86, Training Loss: 0.7977309041864732\n",
      "Epoch 87, Training Loss: 0.7989372345980476\n",
      "Epoch 88, Training Loss: 0.7986286213818719\n",
      "Epoch 89, Training Loss: 0.7974639787393458\n",
      "Epoch 90, Training Loss: 0.7982917733052197\n",
      "Epoch 91, Training Loss: 0.7990827129167669\n",
      "Epoch 92, Training Loss: 0.7975383169510786\n",
      "Epoch 93, Training Loss: 0.7977901691548964\n",
      "Epoch 94, Training Loss: 0.7973670247021843\n",
      "Epoch 95, Training Loss: 0.796814259150449\n",
      "Epoch 96, Training Loss: 0.7973756924096276\n",
      "Epoch 97, Training Loss: 0.7969385640761432\n",
      "Epoch 98, Training Loss: 0.7966611365710988\n",
      "Epoch 99, Training Loss: 0.7964215085085701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:15:22,880] Trial 265 finished with value: 0.6350666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7971373004773084\n",
      "Epoch 1, Training Loss: 0.8744856153215681\n",
      "Epoch 2, Training Loss: 0.8080984894494365\n",
      "Epoch 3, Training Loss: 0.8023782749821369\n",
      "Epoch 4, Training Loss: 0.7982319515450557\n",
      "Epoch 5, Training Loss: 0.7944868892655337\n",
      "Epoch 6, Training Loss: 0.7920993017074757\n",
      "Epoch 7, Training Loss: 0.7893999544301428\n",
      "Epoch 8, Training Loss: 0.7893264213899025\n",
      "Epoch 9, Training Loss: 0.7884389670271622\n",
      "Epoch 10, Training Loss: 0.7877803592753589\n",
      "Epoch 11, Training Loss: 0.7864382353044094\n",
      "Epoch 12, Training Loss: 0.7856200865336826\n",
      "Epoch 13, Training Loss: 0.7851853809858623\n",
      "Epoch 14, Training Loss: 0.7846160271113977\n",
      "Epoch 15, Training Loss: 0.7841168203748259\n",
      "Epoch 16, Training Loss: 0.7840957672076118\n",
      "Epoch 17, Training Loss: 0.7840087081256666\n",
      "Epoch 18, Training Loss: 0.7831235843493526\n",
      "Epoch 19, Training Loss: 0.7822829601459933\n",
      "Epoch 20, Training Loss: 0.7825920372080982\n",
      "Epoch 21, Training Loss: 0.7822915678633783\n",
      "Epoch 22, Training Loss: 0.7826752035241378\n",
      "Epoch 23, Training Loss: 0.7819518113494816\n",
      "Epoch 24, Training Loss: 0.7821247663713039\n",
      "Epoch 25, Training Loss: 0.7812871803018383\n",
      "Epoch 26, Training Loss: 0.7816409388879189\n",
      "Epoch 27, Training Loss: 0.780481161121139\n",
      "Epoch 28, Training Loss: 0.7802154969452019\n",
      "Epoch 29, Training Loss: 0.7812762469277346\n",
      "Epoch 30, Training Loss: 0.780581331790838\n",
      "Epoch 31, Training Loss: 0.7804718179810316\n",
      "Epoch 32, Training Loss: 0.7800776043332609\n",
      "Epoch 33, Training Loss: 0.7794094399402016\n",
      "Epoch 34, Training Loss: 0.7808626973539367\n",
      "Epoch 35, Training Loss: 0.7797843244739044\n",
      "Epoch 36, Training Loss: 0.7791381661156963\n",
      "Epoch 37, Training Loss: 0.7789824743916217\n",
      "Epoch 38, Training Loss: 0.7783355267424332\n",
      "Epoch 39, Training Loss: 0.7782302040802805\n",
      "Epoch 40, Training Loss: 0.7773203017120075\n",
      "Epoch 41, Training Loss: 0.7777309147038854\n",
      "Epoch 42, Training Loss: 0.777968672612556\n",
      "Epoch 43, Training Loss: 0.7776041739865354\n",
      "Epoch 44, Training Loss: 0.7780686315737273\n",
      "Epoch 45, Training Loss: 0.7778257191629339\n",
      "Epoch 46, Training Loss: 0.778584238998872\n",
      "Epoch 47, Training Loss: 0.7770580427987235\n",
      "Epoch 48, Training Loss: 0.7772701610299878\n",
      "Epoch 49, Training Loss: 0.7774696041766862\n",
      "Epoch 50, Training Loss: 0.776869265298198\n",
      "Epoch 51, Training Loss: 0.7774204361707644\n",
      "Epoch 52, Training Loss: 0.7767502021072502\n",
      "Epoch 53, Training Loss: 0.7755504002248435\n",
      "Epoch 54, Training Loss: 0.7761344286732208\n",
      "Epoch 55, Training Loss: 0.7765004563152342\n",
      "Epoch 56, Training Loss: 0.7764687879641253\n",
      "Epoch 57, Training Loss: 0.7754720652013793\n",
      "Epoch 58, Training Loss: 0.7761418229655216\n",
      "Epoch 59, Training Loss: 0.7757935075831592\n",
      "Epoch 60, Training Loss: 0.7758055169779555\n",
      "Epoch 61, Training Loss: 0.7753410966772782\n",
      "Epoch 62, Training Loss: 0.7754494803292411\n",
      "Epoch 63, Training Loss: 0.774987410602713\n",
      "Epoch 64, Training Loss: 0.7748325883894038\n",
      "Epoch 65, Training Loss: 0.7746695538212482\n",
      "Epoch 66, Training Loss: 0.7748105782315247\n",
      "Epoch 67, Training Loss: 0.7744708312185187\n",
      "Epoch 68, Training Loss: 0.7738220308060036\n",
      "Epoch 69, Training Loss: 0.7742254394337647\n",
      "Epoch 70, Training Loss: 0.7739954333556326\n",
      "Epoch 71, Training Loss: 0.7746742915390129\n",
      "Epoch 72, Training Loss: 0.7735335609966651\n",
      "Epoch 73, Training Loss: 0.7731701041522779\n",
      "Epoch 74, Training Loss: 0.7735181907065829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:17:02,491] Trial 266 finished with value: 0.6377333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.773343416981231\n",
      "Epoch 1, Training Loss: 0.8394070462176674\n",
      "Epoch 2, Training Loss: 0.8112603278984701\n",
      "Epoch 3, Training Loss: 0.8061811200658181\n",
      "Epoch 4, Training Loss: 0.807060936877602\n",
      "Epoch 5, Training Loss: 0.8038894201579847\n",
      "Epoch 6, Training Loss: 0.8020172601355646\n",
      "Epoch 7, Training Loss: 0.8040164521762303\n",
      "Epoch 8, Training Loss: 0.7996769490994905\n",
      "Epoch 9, Training Loss: 0.802689700646508\n",
      "Epoch 10, Training Loss: 0.8018720078289061\n",
      "Epoch 11, Training Loss: 0.8008221727564819\n",
      "Epoch 12, Training Loss: 0.8001391510766251\n",
      "Epoch 13, Training Loss: 0.799130211826554\n",
      "Epoch 14, Training Loss: 0.8009196539570514\n",
      "Epoch 15, Training Loss: 0.7997940095743739\n",
      "Epoch 16, Training Loss: 0.8002777238537494\n",
      "Epoch 17, Training Loss: 0.7989948553250248\n",
      "Epoch 18, Training Loss: 0.7986343541539701\n",
      "Epoch 19, Training Loss: 0.798294610905468\n",
      "Epoch 20, Training Loss: 0.7974588504411224\n",
      "Epoch 21, Training Loss: 0.7967714585756001\n",
      "Epoch 22, Training Loss: 0.7979591008415796\n",
      "Epoch 23, Training Loss: 0.7968366626510046\n",
      "Epoch 24, Training Loss: 0.7983247388574414\n",
      "Epoch 25, Training Loss: 0.7973753463953062\n",
      "Epoch 26, Training Loss: 0.797472728105416\n",
      "Epoch 27, Training Loss: 0.7981149848242451\n",
      "Epoch 28, Training Loss: 0.7966065462370564\n",
      "Epoch 29, Training Loss: 0.799256281924427\n",
      "Epoch 30, Training Loss: 0.7973480104503775\n",
      "Epoch 31, Training Loss: 0.7975018337256926\n",
      "Epoch 32, Training Loss: 0.7986990675890356\n",
      "Epoch 33, Training Loss: 0.7954097456053684\n",
      "Epoch 34, Training Loss: 0.7974012323788234\n",
      "Epoch 35, Training Loss: 0.7966063808677788\n",
      "Epoch 36, Training Loss: 0.7966828852667844\n",
      "Epoch 37, Training Loss: 0.7967041743429084\n",
      "Epoch 38, Training Loss: 0.7977556415070268\n",
      "Epoch 39, Training Loss: 0.7984249378505506\n",
      "Epoch 40, Training Loss: 0.7955184488368213\n",
      "Epoch 41, Training Loss: 0.7955563998760138\n",
      "Epoch 42, Training Loss: 0.7960569544842369\n",
      "Epoch 43, Training Loss: 0.7974138631856531\n",
      "Epoch 44, Training Loss: 0.795404999507101\n",
      "Epoch 45, Training Loss: 0.7973658017646101\n",
      "Epoch 46, Training Loss: 0.7965989113750315\n",
      "Epoch 47, Training Loss: 0.7956390848733429\n",
      "Epoch 48, Training Loss: 0.7955146402344668\n",
      "Epoch 49, Training Loss: 0.7962694725595919\n",
      "Epoch 50, Training Loss: 0.7955425747355124\n",
      "Epoch 51, Training Loss: 0.7962469919283587\n",
      "Epoch 52, Training Loss: 0.797000161149448\n",
      "Epoch 53, Training Loss: 0.7948785886728674\n",
      "Epoch 54, Training Loss: 0.7973742804132906\n",
      "Epoch 55, Training Loss: 0.79654825065369\n",
      "Epoch 56, Training Loss: 0.7945560867625071\n",
      "Epoch 57, Training Loss: 0.7964738671940969\n",
      "Epoch 58, Training Loss: 0.7963533320821318\n",
      "Epoch 59, Training Loss: 0.7960120357965168\n",
      "Epoch 60, Training Loss: 0.796947965854989\n",
      "Epoch 61, Training Loss: 0.7964370965957641\n",
      "Epoch 62, Training Loss: 0.7958805615740611\n",
      "Epoch 63, Training Loss: 0.7953106052893445\n",
      "Epoch 64, Training Loss: 0.7945347119991044\n",
      "Epoch 65, Training Loss: 0.7945895599243337\n",
      "Epoch 66, Training Loss: 0.795666171643967\n",
      "Epoch 67, Training Loss: 0.7957439746175493\n",
      "Epoch 68, Training Loss: 0.7966198028478407\n",
      "Epoch 69, Training Loss: 0.7944402593867224\n",
      "Epoch 70, Training Loss: 0.7962455830179659\n",
      "Epoch 71, Training Loss: 0.7963966085498494\n",
      "Epoch 72, Training Loss: 0.7969538676111322\n",
      "Epoch 73, Training Loss: 0.7978065824150142\n",
      "Epoch 74, Training Loss: 0.7964065221915567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:18:30,796] Trial 267 finished with value: 0.6361333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7958021287631272\n",
      "Epoch 1, Training Loss: 1.091510358838474\n",
      "Epoch 2, Training Loss: 1.0911629343032836\n",
      "Epoch 3, Training Loss: 1.0911067509651184\n",
      "Epoch 4, Training Loss: 1.090956960846396\n",
      "Epoch 5, Training Loss: 1.0909132347387427\n",
      "Epoch 6, Training Loss: 1.090874758608201\n",
      "Epoch 7, Training Loss: 1.0905911719097812\n",
      "Epoch 8, Training Loss: 1.090561983164619\n",
      "Epoch 9, Training Loss: 1.09035654853372\n",
      "Epoch 10, Training Loss: 1.0902947365536408\n",
      "Epoch 11, Training Loss: 1.0900231843836168\n",
      "Epoch 12, Training Loss: 1.089854621887207\n",
      "Epoch 13, Training Loss: 1.0896315490498263\n",
      "Epoch 14, Training Loss: 1.0892330865299\n",
      "Epoch 15, Training Loss: 1.0888530337109286\n",
      "Epoch 16, Training Loss: 1.0883701087446773\n",
      "Epoch 17, Training Loss: 1.0877385490080889\n",
      "Epoch 18, Training Loss: 1.0868572794689852\n",
      "Epoch 19, Training Loss: 1.0857835552271675\n",
      "Epoch 20, Training Loss: 1.0843070873092202\n",
      "Epoch 21, Training Loss: 1.0822449800547431\n",
      "Epoch 22, Training Loss: 1.0792031760776744\n",
      "Epoch 23, Training Loss: 1.0745609662112068\n",
      "Epoch 24, Training Loss: 1.0674985860375796\n",
      "Epoch 25, Training Loss: 1.0562076581225677\n",
      "Epoch 26, Training Loss: 1.0398595733502332\n",
      "Epoch 27, Training Loss: 1.0208751388157116\n",
      "Epoch 28, Training Loss: 1.0055818436426276\n",
      "Epoch 29, Training Loss: 0.9973644159120671\n",
      "Epoch 30, Training Loss: 0.9930250892919653\n",
      "Epoch 31, Training Loss: 0.9899949176171247\n",
      "Epoch 32, Training Loss: 0.9869124589948093\n",
      "Epoch 33, Training Loss: 0.9840112383926616\n",
      "Epoch 34, Training Loss: 0.9804928926860585\n",
      "Epoch 35, Training Loss: 0.9765591540056117\n",
      "Epoch 36, Training Loss: 0.9720170140266419\n",
      "Epoch 37, Training Loss: 0.9668342584020951\n",
      "Epoch 38, Training Loss: 0.9618565279595992\n",
      "Epoch 39, Training Loss: 0.9566548158140743\n",
      "Epoch 40, Training Loss: 0.9521622112218071\n",
      "Epoch 41, Training Loss: 0.9482473925983205\n",
      "Epoch 42, Training Loss: 0.9452660151088939\n",
      "Epoch 43, Training Loss: 0.9425006355257596\n",
      "Epoch 44, Training Loss: 0.939925373582279\n",
      "Epoch 45, Training Loss: 0.9370383997524486\n",
      "Epoch 46, Training Loss: 0.9338606025190914\n",
      "Epoch 47, Training Loss: 0.9305185440007379\n",
      "Epoch 48, Training Loss: 0.9263717145779553\n",
      "Epoch 49, Training Loss: 0.9216916394934934\n",
      "Epoch 50, Training Loss: 0.9163590313406551\n",
      "Epoch 51, Training Loss: 0.9102599196574267\n",
      "Epoch 52, Training Loss: 0.90311251689406\n",
      "Epoch 53, Training Loss: 0.8952727173356448\n",
      "Epoch 54, Training Loss: 0.8869772310817943\n",
      "Epoch 55, Training Loss: 0.8783006429672241\n",
      "Epoch 56, Training Loss: 0.8700376766569474\n",
      "Epoch 57, Training Loss: 0.8626842047186459\n",
      "Epoch 58, Training Loss: 0.8565737602289986\n",
      "Epoch 59, Training Loss: 0.8514707685218138\n",
      "Epoch 60, Training Loss: 0.8474478013375226\n",
      "Epoch 61, Training Loss: 0.8449298835501952\n",
      "Epoch 62, Training Loss: 0.842625010504442\n",
      "Epoch 63, Training Loss: 0.8411722828360165\n",
      "Epoch 64, Training Loss: 0.8395953356518465\n",
      "Epoch 65, Training Loss: 0.8384961541961221\n",
      "Epoch 66, Training Loss: 0.8375417773162618\n",
      "Epoch 67, Training Loss: 0.8365548037080204\n",
      "Epoch 68, Training Loss: 0.8357987666130066\n",
      "Epoch 69, Training Loss: 0.8348097435165854\n",
      "Epoch 70, Training Loss: 0.8339766550064087\n",
      "Epoch 71, Training Loss: 0.8329355293161729\n",
      "Epoch 72, Training Loss: 0.8320302298489739\n",
      "Epoch 73, Training Loss: 0.831246505204369\n",
      "Epoch 74, Training Loss: 0.8303751132067512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:19:56,266] Trial 268 finished with value: 0.6172666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.829540345879162\n",
      "Epoch 1, Training Loss: 0.8951840873051407\n",
      "Epoch 2, Training Loss: 0.8238735989520424\n",
      "Epoch 3, Training Loss: 0.8161567371590693\n",
      "Epoch 4, Training Loss: 0.8126366045241965\n",
      "Epoch 5, Training Loss: 0.8084162232571078\n",
      "Epoch 6, Training Loss: 0.8049672090917601\n",
      "Epoch 7, Training Loss: 0.8019634733522745\n",
      "Epoch 8, Training Loss: 0.7998043926138627\n",
      "Epoch 9, Training Loss: 0.7975706533381813\n",
      "Epoch 10, Training Loss: 0.7962041467652285\n",
      "Epoch 11, Training Loss: 0.7953092006812418\n",
      "Epoch 12, Training Loss: 0.7939317394019966\n",
      "Epoch 13, Training Loss: 0.7930051077577405\n",
      "Epoch 14, Training Loss: 0.7927503464813519\n",
      "Epoch 15, Training Loss: 0.793093973174131\n",
      "Epoch 16, Training Loss: 0.7918708686541794\n",
      "Epoch 17, Training Loss: 0.7912183389627844\n",
      "Epoch 18, Training Loss: 0.7911279406762661\n",
      "Epoch 19, Training Loss: 0.7907844070205114\n",
      "Epoch 20, Training Loss: 0.7900923509346811\n",
      "Epoch 21, Training Loss: 0.7900850996935278\n",
      "Epoch 22, Training Loss: 0.7894427497584121\n",
      "Epoch 23, Training Loss: 0.7896668891261395\n",
      "Epoch 24, Training Loss: 0.7892659541359521\n",
      "Epoch 25, Training Loss: 0.7886088383377047\n",
      "Epoch 26, Training Loss: 0.7887712519867975\n",
      "Epoch 27, Training Loss: 0.7883936796869551\n",
      "Epoch 28, Training Loss: 0.7885378774843718\n",
      "Epoch 29, Training Loss: 0.7881917663086626\n",
      "Epoch 30, Training Loss: 0.7880044427133144\n",
      "Epoch 31, Training Loss: 0.7878926842732537\n",
      "Epoch 32, Training Loss: 0.7870219853587617\n",
      "Epoch 33, Training Loss: 0.7870346519283782\n",
      "Epoch 34, Training Loss: 0.787731615493172\n",
      "Epoch 35, Training Loss: 0.7871645425495348\n",
      "Epoch 36, Training Loss: 0.7873000486452777\n",
      "Epoch 37, Training Loss: 0.7866997494733423\n",
      "Epoch 38, Training Loss: 0.7877907999476096\n",
      "Epoch 39, Training Loss: 0.7870212465300596\n",
      "Epoch 40, Training Loss: 0.7868475686338611\n",
      "Epoch 41, Training Loss: 0.7858372303776275\n",
      "Epoch 42, Training Loss: 0.7860592728270623\n",
      "Epoch 43, Training Loss: 0.7854702666289824\n",
      "Epoch 44, Training Loss: 0.7858504003151915\n",
      "Epoch 45, Training Loss: 0.7860438025087342\n",
      "Epoch 46, Training Loss: 0.7862475138857848\n",
      "Epoch 47, Training Loss: 0.7853099854368912\n",
      "Epoch 48, Training Loss: 0.7857574881467604\n",
      "Epoch 49, Training Loss: 0.7851382013550379\n",
      "Epoch 50, Training Loss: 0.7848546107460682\n",
      "Epoch 51, Training Loss: 0.7853169330080649\n",
      "Epoch 52, Training Loss: 0.7855995193459934\n",
      "Epoch 53, Training Loss: 0.7853392538271452\n",
      "Epoch 54, Training Loss: 0.7851547740455856\n",
      "Epoch 55, Training Loss: 0.7845127654254884\n",
      "Epoch 56, Training Loss: 0.7843570451987417\n",
      "Epoch 57, Training Loss: 0.7847419223391023\n",
      "Epoch 58, Training Loss: 0.7843606187884969\n",
      "Epoch 59, Training Loss: 0.7844144433960879\n",
      "Epoch 60, Training Loss: 0.7848802850658733\n",
      "Epoch 61, Training Loss: 0.7844866873626423\n",
      "Epoch 62, Training Loss: 0.7840827895286387\n",
      "Epoch 63, Training Loss: 0.7846205558095659\n",
      "Epoch 64, Training Loss: 0.7837537428490201\n",
      "Epoch 65, Training Loss: 0.7842927800085312\n",
      "Epoch 66, Training Loss: 0.7837034442370996\n",
      "Epoch 67, Training Loss: 0.7839643002452706\n",
      "Epoch 68, Training Loss: 0.7835992057520644\n",
      "Epoch 69, Training Loss: 0.7835826246361983\n",
      "Epoch 70, Training Loss: 0.7837912587294901\n",
      "Epoch 71, Training Loss: 0.7834786251075285\n",
      "Epoch 72, Training Loss: 0.7833117498491043\n",
      "Epoch 73, Training Loss: 0.7835153008762159\n",
      "Epoch 74, Training Loss: 0.7833712287415239\n",
      "Epoch 75, Training Loss: 0.7837331062869022\n",
      "Epoch 76, Training Loss: 0.7831721554125162\n",
      "Epoch 77, Training Loss: 0.7826748044867264\n",
      "Epoch 78, Training Loss: 0.7829948109791691\n",
      "Epoch 79, Training Loss: 0.7834885567650759\n",
      "Epoch 80, Training Loss: 0.7834162032693849\n",
      "Epoch 81, Training Loss: 0.78365174648457\n",
      "Epoch 82, Training Loss: 0.7827278937612261\n",
      "Epoch 83, Training Loss: 0.7825755224192052\n",
      "Epoch 84, Training Loss: 0.7822986115190319\n",
      "Epoch 85, Training Loss: 0.7827945343533853\n",
      "Epoch 86, Training Loss: 0.7830323161039138\n",
      "Epoch 87, Training Loss: 0.7822687319346837\n",
      "Epoch 88, Training Loss: 0.7834325974148916\n",
      "Epoch 89, Training Loss: 0.7819555637531711\n",
      "Epoch 90, Training Loss: 0.7821221205524932\n",
      "Epoch 91, Training Loss: 0.7816161404874988\n",
      "Epoch 92, Training Loss: 0.7823335878831104\n",
      "Epoch 93, Training Loss: 0.7823724436580687\n",
      "Epoch 94, Training Loss: 0.7824559991521046\n",
      "Epoch 95, Training Loss: 0.7822094772095071\n",
      "Epoch 96, Training Loss: 0.7823004540644194\n",
      "Epoch 97, Training Loss: 0.7820027766371132\n",
      "Epoch 98, Training Loss: 0.7826935557494487\n",
      "Epoch 99, Training Loss: 0.7822866536620864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:21:47,287] Trial 269 finished with value: 0.5978666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7820636330690599\n",
      "Epoch 1, Training Loss: 0.9221260431117582\n",
      "Epoch 2, Training Loss: 0.8795690013053722\n",
      "Epoch 3, Training Loss: 0.8464016309358123\n",
      "Epoch 4, Training Loss: 0.8236694823530384\n",
      "Epoch 5, Training Loss: 0.8124840026511285\n",
      "Epoch 6, Training Loss: 0.8076029361638808\n",
      "Epoch 7, Training Loss: 0.8048966173838852\n",
      "Epoch 8, Training Loss: 0.8040140963138495\n",
      "Epoch 9, Training Loss: 0.8032294691953444\n",
      "Epoch 10, Training Loss: 0.8028751924521941\n",
      "Epoch 11, Training Loss: 0.8017764206219437\n",
      "Epoch 12, Training Loss: 0.8014421578636743\n",
      "Epoch 13, Training Loss: 0.8001812317765745\n",
      "Epoch 14, Training Loss: 0.8003826053518998\n",
      "Epoch 15, Training Loss: 0.8003790615196514\n",
      "Epoch 16, Training Loss: 0.7998545315032615\n",
      "Epoch 17, Training Loss: 0.7998736734677078\n",
      "Epoch 18, Training Loss: 0.7990620208862133\n",
      "Epoch 19, Training Loss: 0.7988955755879108\n",
      "Epoch 20, Training Loss: 0.799325840813773\n",
      "Epoch 21, Training Loss: 0.7990234022749994\n",
      "Epoch 22, Training Loss: 0.7989378160103819\n",
      "Epoch 23, Training Loss: 0.798566122431504\n",
      "Epoch 24, Training Loss: 0.7987405244569133\n",
      "Epoch 25, Training Loss: 0.7974479365617709\n",
      "Epoch 26, Training Loss: 0.7982834513026072\n",
      "Epoch 27, Training Loss: 0.797044126700638\n",
      "Epoch 28, Training Loss: 0.7979426644798508\n",
      "Epoch 29, Training Loss: 0.7961471117528758\n",
      "Epoch 30, Training Loss: 0.7966409212664554\n",
      "Epoch 31, Training Loss: 0.7967058845928737\n",
      "Epoch 32, Training Loss: 0.796046665378083\n",
      "Epoch 33, Training Loss: 0.7952557814748664\n",
      "Epoch 34, Training Loss: 0.7944389079746447\n",
      "Epoch 35, Training Loss: 0.7947242007219701\n",
      "Epoch 36, Training Loss: 0.794057341805078\n",
      "Epoch 37, Training Loss: 0.7942507500935317\n",
      "Epoch 38, Training Loss: 0.7936757819096845\n",
      "Epoch 39, Training Loss: 0.793550113358892\n",
      "Epoch 40, Training Loss: 0.7927168410523493\n",
      "Epoch 41, Training Loss: 0.7930178459425617\n",
      "Epoch 42, Training Loss: 0.7926015463090481\n",
      "Epoch 43, Training Loss: 0.7933217502178106\n",
      "Epoch 44, Training Loss: 0.7922725640741506\n",
      "Epoch 45, Training Loss: 0.7920840395124336\n",
      "Epoch 46, Training Loss: 0.7923154062794563\n",
      "Epoch 47, Training Loss: 0.7917023362969994\n",
      "Epoch 48, Training Loss: 0.7922973809385658\n",
      "Epoch 49, Training Loss: 0.7922008387128213\n",
      "Epoch 50, Training Loss: 0.792541465992318\n",
      "Epoch 51, Training Loss: 0.7915924381492729\n",
      "Epoch 52, Training Loss: 0.791146871649233\n",
      "Epoch 53, Training Loss: 0.7911488647747756\n",
      "Epoch 54, Training Loss: 0.7911857032238092\n",
      "Epoch 55, Training Loss: 0.7909925327265173\n",
      "Epoch 56, Training Loss: 0.7908940934597102\n",
      "Epoch 57, Training Loss: 0.7907875119295336\n",
      "Epoch 58, Training Loss: 0.7905514559351412\n",
      "Epoch 59, Training Loss: 0.7907602386367052\n",
      "Epoch 60, Training Loss: 0.7911300168897872\n",
      "Epoch 61, Training Loss: 0.7906843161224423\n",
      "Epoch 62, Training Loss: 0.7906824966122333\n",
      "Epoch 63, Training Loss: 0.7905601476368151\n",
      "Epoch 64, Training Loss: 0.7905180047329207\n",
      "Epoch 65, Training Loss: 0.7901582696384057\n",
      "Epoch 66, Training Loss: 0.7898671876218982\n",
      "Epoch 67, Training Loss: 0.7912913777774437\n",
      "Epoch 68, Training Loss: 0.7901448078621599\n",
      "Epoch 69, Training Loss: 0.7901319171253004\n",
      "Epoch 70, Training Loss: 0.7903898918539062\n",
      "Epoch 71, Training Loss: 0.7902862770216805\n",
      "Epoch 72, Training Loss: 0.7903276778701552\n",
      "Epoch 73, Training Loss: 0.7897946479625272\n",
      "Epoch 74, Training Loss: 0.7906161397919619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:22:56,512] Trial 270 finished with value: 0.6385333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7899293526670986\n",
      "Epoch 1, Training Loss: 0.9840376082588644\n",
      "Epoch 2, Training Loss: 0.9283064505633186\n",
      "Epoch 3, Training Loss: 0.879870139500674\n",
      "Epoch 4, Training Loss: 0.8327844460571513\n",
      "Epoch 5, Training Loss: 0.8168667324150309\n",
      "Epoch 6, Training Loss: 0.8133578824295717\n",
      "Epoch 7, Training Loss: 0.8117257131548489\n",
      "Epoch 8, Training Loss: 0.8109942741955027\n",
      "Epoch 9, Training Loss: 0.8102359515077927\n",
      "Epoch 10, Training Loss: 0.8085160761720994\n",
      "Epoch 11, Training Loss: 0.8082118502083947\n",
      "Epoch 12, Training Loss: 0.8071113920913023\n",
      "Epoch 13, Training Loss: 0.8061080107268165\n",
      "Epoch 14, Training Loss: 0.8055923610575059\n",
      "Epoch 15, Training Loss: 0.8048823040373185\n",
      "Epoch 16, Training Loss: 0.8044947765855228\n",
      "Epoch 17, Training Loss: 0.8037883177224328\n",
      "Epoch 18, Training Loss: 0.8041527246727663\n",
      "Epoch 19, Training Loss: 0.8032151992882\n",
      "Epoch 20, Training Loss: 0.8027991110437056\n",
      "Epoch 21, Training Loss: 0.8029669044298284\n",
      "Epoch 22, Training Loss: 0.8020826131456038\n",
      "Epoch 23, Training Loss: 0.802004169365939\n",
      "Epoch 24, Training Loss: 0.8015299888919382\n",
      "Epoch 25, Training Loss: 0.8015955616446102\n",
      "Epoch 26, Training Loss: 0.8014615570096408\n",
      "Epoch 27, Training Loss: 0.8010714354935814\n",
      "Epoch 28, Training Loss: 0.800827221028945\n",
      "Epoch 29, Training Loss: 0.800898271518595\n",
      "Epoch 30, Training Loss: 0.800444866068223\n",
      "Epoch 31, Training Loss: 0.8004688095345217\n",
      "Epoch 32, Training Loss: 0.8002625679969788\n",
      "Epoch 33, Training Loss: 0.7998185174605426\n",
      "Epoch 34, Training Loss: 0.799939265461529\n",
      "Epoch 35, Training Loss: 0.7996917712688446\n",
      "Epoch 36, Training Loss: 0.7998401623613695\n",
      "Epoch 37, Training Loss: 0.7993839043729446\n",
      "Epoch 38, Training Loss: 0.7992611759550431\n",
      "Epoch 39, Training Loss: 0.7990100988920997\n",
      "Epoch 40, Training Loss: 0.799075056104099\n",
      "Epoch 41, Training Loss: 0.7988518289958729\n",
      "Epoch 42, Training Loss: 0.7986863283550039\n",
      "Epoch 43, Training Loss: 0.7987297224998474\n",
      "Epoch 44, Training Loss: 0.7976748582194834\n",
      "Epoch 45, Training Loss: 0.7984862823346082\n",
      "Epoch 46, Training Loss: 0.7981944341519299\n",
      "Epoch 47, Training Loss: 0.7980996384340174\n",
      "Epoch 48, Training Loss: 0.7976483986658208\n",
      "Epoch 49, Training Loss: 0.7983687509508693\n",
      "Epoch 50, Training Loss: 0.7978572833538056\n",
      "Epoch 51, Training Loss: 0.7976497274286607\n",
      "Epoch 52, Training Loss: 0.7975174851277296\n",
      "Epoch 53, Training Loss: 0.7973939821299385\n",
      "Epoch 54, Training Loss: 0.7974442355071797\n",
      "Epoch 55, Training Loss: 0.797196453739615\n",
      "Epoch 56, Training Loss: 0.7969388459009282\n",
      "Epoch 57, Training Loss: 0.79720026549171\n",
      "Epoch 58, Training Loss: 0.7965492286401636\n",
      "Epoch 59, Training Loss: 0.7964733951933244\n",
      "Epoch 60, Training Loss: 0.796810575653525\n",
      "Epoch 61, Training Loss: 0.796442496916827\n",
      "Epoch 62, Training Loss: 0.7964866579981411\n",
      "Epoch 63, Training Loss: 0.7959958745451534\n",
      "Epoch 64, Training Loss: 0.7962592999374165\n",
      "Epoch 65, Training Loss: 0.7961679039281957\n",
      "Epoch 66, Training Loss: 0.7960658013820648\n",
      "Epoch 67, Training Loss: 0.7955517799714033\n",
      "Epoch 68, Training Loss: 0.7949713906119852\n",
      "Epoch 69, Training Loss: 0.7949658816000994\n",
      "Epoch 70, Training Loss: 0.7954604469327365\n",
      "Epoch 71, Training Loss: 0.7951782423608443\n",
      "Epoch 72, Training Loss: 0.7948989094706143\n",
      "Epoch 73, Training Loss: 0.7946578275456148\n",
      "Epoch 74, Training Loss: 0.7946508903363172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:24:38,368] Trial 271 finished with value: 0.6343333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7949174760369694\n",
      "Epoch 1, Training Loss: 0.89011226254351\n",
      "Epoch 2, Training Loss: 0.8158574888986699\n",
      "Epoch 3, Training Loss: 0.8105692658704869\n",
      "Epoch 4, Training Loss: 0.808441801071167\n",
      "Epoch 5, Training Loss: 0.8065894314120797\n",
      "Epoch 6, Training Loss: 0.8056837360999164\n",
      "Epoch 7, Training Loss: 0.803833947392071\n",
      "Epoch 8, Training Loss: 0.8027484454126919\n",
      "Epoch 9, Training Loss: 0.8019839107289034\n",
      "Epoch 10, Training Loss: 0.80167100611855\n",
      "Epoch 11, Training Loss: 0.8010477271500756\n",
      "Epoch 12, Training Loss: 0.800314720658695\n",
      "Epoch 13, Training Loss: 0.7997900793832892\n",
      "Epoch 14, Training Loss: 0.7993976480820599\n",
      "Epoch 15, Training Loss: 0.7987431263923646\n",
      "Epoch 16, Training Loss: 0.7982497647229363\n",
      "Epoch 17, Training Loss: 0.7977518076756421\n",
      "Epoch 18, Training Loss: 0.797943098264582\n",
      "Epoch 19, Training Loss: 0.7980382289605982\n",
      "Epoch 20, Training Loss: 0.7965758517209222\n",
      "Epoch 21, Training Loss: 0.7964060144564685\n",
      "Epoch 22, Training Loss: 0.7949202999647926\n",
      "Epoch 23, Training Loss: 0.7947770540153279\n",
      "Epoch 24, Training Loss: 0.7942442398912767\n",
      "Epoch 25, Training Loss: 0.7935244800763972\n",
      "Epoch 26, Training Loss: 0.7924915975682876\n",
      "Epoch 27, Training Loss: 0.791607286789838\n",
      "Epoch 28, Training Loss: 0.7908479083986844\n",
      "Epoch 29, Training Loss: 0.7900416767597198\n",
      "Epoch 30, Training Loss: 0.78944659317241\n",
      "Epoch 31, Training Loss: 0.7886595406953026\n",
      "Epoch 32, Training Loss: 0.7883298488925485\n",
      "Epoch 33, Training Loss: 0.7877719810429742\n",
      "Epoch 34, Training Loss: 0.7872588191312903\n",
      "Epoch 35, Training Loss: 0.7871733703332788\n",
      "Epoch 36, Training Loss: 0.7864786039380466\n",
      "Epoch 37, Training Loss: 0.786026016403647\n",
      "Epoch 38, Training Loss: 0.7859397894494674\n",
      "Epoch 39, Training Loss: 0.7860422109155094\n",
      "Epoch 40, Training Loss: 0.7853758886982413\n",
      "Epoch 41, Training Loss: 0.7853964375748353\n",
      "Epoch 42, Training Loss: 0.7849911506035748\n",
      "Epoch 43, Training Loss: 0.7848178780078888\n",
      "Epoch 44, Training Loss: 0.784715159780839\n",
      "Epoch 45, Training Loss: 0.7842698970261742\n",
      "Epoch 46, Training Loss: 0.7846252707874074\n",
      "Epoch 47, Training Loss: 0.7837051627215217\n",
      "Epoch 48, Training Loss: 0.7839170498006484\n",
      "Epoch 49, Training Loss: 0.7843273736448849\n",
      "Epoch 50, Training Loss: 0.783954415251227\n",
      "Epoch 51, Training Loss: 0.7833810999112971\n",
      "Epoch 52, Training Loss: 0.783806362993577\n",
      "Epoch 53, Training Loss: 0.7837157318872564\n",
      "Epoch 54, Training Loss: 0.7832062407100902\n",
      "Epoch 55, Training Loss: 0.7834263848557191\n",
      "Epoch 56, Training Loss: 0.7832712897833656\n",
      "Epoch 57, Training Loss: 0.7837904324251063\n",
      "Epoch 58, Training Loss: 0.7831563330397886\n",
      "Epoch 59, Training Loss: 0.7829793715476989\n",
      "Epoch 60, Training Loss: 0.7834609103904051\n",
      "Epoch 61, Training Loss: 0.7823955893516541\n",
      "Epoch 62, Training Loss: 0.783001110904357\n",
      "Epoch 63, Training Loss: 0.7827195312696344\n",
      "Epoch 64, Training Loss: 0.7825686959659353\n",
      "Epoch 65, Training Loss: 0.7821546903077294\n",
      "Epoch 66, Training Loss: 0.7825538210307851\n",
      "Epoch 67, Training Loss: 0.7826001420441796\n",
      "Epoch 68, Training Loss: 0.7827871559647953\n",
      "Epoch 69, Training Loss: 0.7825727762194241\n",
      "Epoch 70, Training Loss: 0.7824712618659524\n",
      "Epoch 71, Training Loss: 0.7820659347842721\n",
      "Epoch 72, Training Loss: 0.7820689922220567\n",
      "Epoch 73, Training Loss: 0.7821669877977933\n",
      "Epoch 74, Training Loss: 0.7818283483561348\n",
      "Epoch 75, Training Loss: 0.7822738982649411\n",
      "Epoch 76, Training Loss: 0.7817026391450097\n",
      "Epoch 77, Training Loss: 0.78172711316277\n",
      "Epoch 78, Training Loss: 0.7817541968121248\n",
      "Epoch 79, Training Loss: 0.7821352131226483\n",
      "Epoch 80, Training Loss: 0.7820279182406032\n",
      "Epoch 81, Training Loss: 0.7817953454045689\n",
      "Epoch 82, Training Loss: 0.7816584118674783\n",
      "Epoch 83, Training Loss: 0.7814801007158616\n",
      "Epoch 84, Training Loss: 0.781848623752594\n",
      "Epoch 85, Training Loss: 0.7816557422806235\n",
      "Epoch 86, Training Loss: 0.7814387500987333\n",
      "Epoch 87, Training Loss: 0.781380166165969\n",
      "Epoch 88, Training Loss: 0.7814626707750209\n",
      "Epoch 89, Training Loss: 0.7816852871109458\n",
      "Epoch 90, Training Loss: 0.7813338330913993\n",
      "Epoch 91, Training Loss: 0.7818812194992514\n",
      "Epoch 92, Training Loss: 0.7812405490174013\n",
      "Epoch 93, Training Loss: 0.7810111204315634\n",
      "Epoch 94, Training Loss: 0.7812830592604244\n",
      "Epoch 95, Training Loss: 0.78116226848434\n",
      "Epoch 96, Training Loss: 0.7811555399614222\n",
      "Epoch 97, Training Loss: 0.7808180802008685\n",
      "Epoch 98, Training Loss: 0.7812865560896256\n",
      "Epoch 99, Training Loss: 0.7810956640804515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:26:53,188] Trial 272 finished with value: 0.6400666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7807704678703757\n",
      "Epoch 1, Training Loss: 0.9237032761251119\n",
      "Epoch 2, Training Loss: 0.8317601480878385\n",
      "Epoch 3, Training Loss: 0.8218305424640053\n",
      "Epoch 4, Training Loss: 0.8189101583079288\n",
      "Epoch 5, Training Loss: 0.8141788142068046\n",
      "Epoch 6, Training Loss: 0.8112099070746199\n",
      "Epoch 7, Training Loss: 0.8104343138243023\n",
      "Epoch 8, Training Loss: 0.8092227294929045\n",
      "Epoch 9, Training Loss: 0.8078061857617888\n",
      "Epoch 10, Training Loss: 0.8082620479110488\n",
      "Epoch 11, Training Loss: 0.8082238416026409\n",
      "Epoch 12, Training Loss: 0.8070700620350085\n",
      "Epoch 13, Training Loss: 0.8056050897540903\n",
      "Epoch 14, Training Loss: 0.8053296030015874\n",
      "Epoch 15, Training Loss: 0.8053207107952662\n",
      "Epoch 16, Training Loss: 0.8050432308275897\n",
      "Epoch 17, Training Loss: 0.8044911854249194\n",
      "Epoch 18, Training Loss: 0.8031365195611366\n",
      "Epoch 19, Training Loss: 0.8051410216137879\n",
      "Epoch 20, Training Loss: 0.8043432021499577\n",
      "Epoch 21, Training Loss: 0.8034473144022146\n",
      "Epoch 22, Training Loss: 0.8048894750444513\n",
      "Epoch 23, Training Loss: 0.8030927239504075\n",
      "Epoch 24, Training Loss: 0.8020149742750297\n",
      "Epoch 25, Training Loss: 0.8020628989190983\n",
      "Epoch 26, Training Loss: 0.8022900647686836\n",
      "Epoch 27, Training Loss: 0.8022823968328031\n",
      "Epoch 28, Training Loss: 0.8026707169704868\n",
      "Epoch 29, Training Loss: 0.80156301945672\n",
      "Epoch 30, Training Loss: 0.8027654303643936\n",
      "Epoch 31, Training Loss: 0.8015776445094804\n",
      "Epoch 32, Training Loss: 0.8016756800332464\n",
      "Epoch 33, Training Loss: 0.80124013881038\n",
      "Epoch 34, Training Loss: 0.8005321334627338\n",
      "Epoch 35, Training Loss: 0.8020731899075042\n",
      "Epoch 36, Training Loss: 0.8016559465487201\n",
      "Epoch 37, Training Loss: 0.8019540013227248\n",
      "Epoch 38, Training Loss: 0.8015188429588662\n",
      "Epoch 39, Training Loss: 0.801497381074088\n",
      "Epoch 40, Training Loss: 0.8021663044628344\n",
      "Epoch 41, Training Loss: 0.7998670665841353\n",
      "Epoch 42, Training Loss: 0.8010040059125513\n",
      "Epoch 43, Training Loss: 0.8020005154430419\n",
      "Epoch 44, Training Loss: 0.8015799665809574\n",
      "Epoch 45, Training Loss: 0.799654941989067\n",
      "Epoch 46, Training Loss: 0.8005495791148423\n",
      "Epoch 47, Training Loss: 0.8010272839015588\n",
      "Epoch 48, Training Loss: 0.8007767634284227\n",
      "Epoch 49, Training Loss: 0.8005686757259799\n",
      "Epoch 50, Training Loss: 0.8000374767117034\n",
      "Epoch 51, Training Loss: 0.8003273531906587\n",
      "Epoch 52, Training Loss: 0.8002867023747666\n",
      "Epoch 53, Training Loss: 0.8007156943020067\n",
      "Epoch 54, Training Loss: 0.8000085294694829\n",
      "Epoch 55, Training Loss: 0.7996412613338097\n",
      "Epoch 56, Training Loss: 0.7995985255205542\n",
      "Epoch 57, Training Loss: 0.7998457495431255\n",
      "Epoch 58, Training Loss: 0.7990687602444699\n",
      "Epoch 59, Training Loss: 0.7998404386348295\n",
      "Epoch 60, Training Loss: 0.8005054533033442\n",
      "Epoch 61, Training Loss: 0.7998456968848867\n",
      "Epoch 62, Training Loss: 0.7997771680803227\n",
      "Epoch 63, Training Loss: 0.7988069905374283\n",
      "Epoch 64, Training Loss: 0.8000684126875455\n",
      "Epoch 65, Training Loss: 0.7989567779060593\n",
      "Epoch 66, Training Loss: 0.7995256690154399\n",
      "Epoch 67, Training Loss: 0.7993604808821714\n",
      "Epoch 68, Training Loss: 0.8004069123949323\n",
      "Epoch 69, Training Loss: 0.7992808572331765\n",
      "Epoch 70, Training Loss: 0.7996627921448615\n",
      "Epoch 71, Training Loss: 0.8001538053491062\n",
      "Epoch 72, Training Loss: 0.8000243323189872\n",
      "Epoch 73, Training Loss: 0.7991813948279933\n",
      "Epoch 74, Training Loss: 0.7991153303841899\n",
      "Epoch 75, Training Loss: 0.7996528463256091\n",
      "Epoch 76, Training Loss: 0.7992098586003583\n",
      "Epoch 77, Training Loss: 0.7999499692952723\n",
      "Epoch 78, Training Loss: 0.7996818057576517\n",
      "Epoch 79, Training Loss: 0.7990477632759209\n",
      "Epoch 80, Training Loss: 0.7996594194182776\n",
      "Epoch 81, Training Loss: 0.7997416118930157\n",
      "Epoch 82, Training Loss: 0.7989588768858659\n",
      "Epoch 83, Training Loss: 0.7993767159325736\n",
      "Epoch 84, Training Loss: 0.7990381232777932\n",
      "Epoch 85, Training Loss: 0.7998240723645776\n",
      "Epoch 86, Training Loss: 0.7998123071247474\n",
      "Epoch 87, Training Loss: 0.7984387575235582\n",
      "Epoch 88, Training Loss: 0.7997647482649725\n",
      "Epoch 89, Training Loss: 0.7995315738190386\n",
      "Epoch 90, Training Loss: 0.7989789903612066\n",
      "Epoch 91, Training Loss: 0.7994627647830131\n",
      "Epoch 92, Training Loss: 0.8006791413278508\n",
      "Epoch 93, Training Loss: 0.798903126824171\n",
      "Epoch 94, Training Loss: 0.7990309440103689\n",
      "Epoch 95, Training Loss: 0.8003175764155567\n",
      "Epoch 96, Training Loss: 0.7990910664536899\n",
      "Epoch 97, Training Loss: 0.7999341158042277\n",
      "Epoch 98, Training Loss: 0.7988541593228964\n",
      "Epoch 99, Training Loss: 0.7997864210964145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:28:56,448] Trial 273 finished with value: 0.6296666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.80017612733339\n",
      "Epoch 1, Training Loss: 1.0090904726701624\n",
      "Epoch 2, Training Loss: 0.9469309217088363\n",
      "Epoch 3, Training Loss: 0.9336508093160741\n",
      "Epoch 4, Training Loss: 0.9262103396303514\n",
      "Epoch 5, Training Loss: 0.9198805701031404\n",
      "Epoch 6, Training Loss: 0.9138999604477602\n",
      "Epoch 7, Training Loss: 0.9080856511172126\n",
      "Epoch 8, Training Loss: 0.9021007682996638\n",
      "Epoch 9, Training Loss: 0.8958052618363325\n",
      "Epoch 10, Training Loss: 0.8890411689000971\n",
      "Epoch 11, Training Loss: 0.8818375724904678\n",
      "Epoch 12, Training Loss: 0.8740598778864916\n",
      "Epoch 13, Training Loss: 0.8660493817048914\n",
      "Epoch 14, Training Loss: 0.8580758544977973\n",
      "Epoch 15, Training Loss: 0.8503791825911579\n",
      "Epoch 16, Training Loss: 0.8432295175159679\n",
      "Epoch 17, Training Loss: 0.8368539930792416\n",
      "Epoch 18, Training Loss: 0.8312261624195997\n",
      "Epoch 19, Training Loss: 0.8264347767128664\n",
      "Epoch 20, Training Loss: 0.8223027571509867\n",
      "Epoch 21, Training Loss: 0.8188879257089952\n",
      "Epoch 22, Training Loss: 0.8160557680270251\n",
      "Epoch 23, Training Loss: 0.8138755280831281\n",
      "Epoch 24, Training Loss: 0.8120753395557404\n",
      "Epoch 25, Training Loss: 0.8106193092991324\n",
      "Epoch 26, Training Loss: 0.8095102285637575\n",
      "Epoch 27, Training Loss: 0.808515820643481\n",
      "Epoch 28, Training Loss: 0.8075376632634331\n",
      "Epoch 29, Training Loss: 0.8069298836764167\n",
      "Epoch 30, Training Loss: 0.8062915394586675\n",
      "Epoch 31, Training Loss: 0.8057894372238832\n",
      "Epoch 32, Training Loss: 0.8051677772578071\n",
      "Epoch 33, Training Loss: 0.804685845936046\n",
      "Epoch 34, Training Loss: 0.8043617237315458\n",
      "Epoch 35, Training Loss: 0.8039083154762492\n",
      "Epoch 36, Training Loss: 0.8036197510186364\n",
      "Epoch 37, Training Loss: 0.8032146755386801\n",
      "Epoch 38, Training Loss: 0.8028885016020606\n",
      "Epoch 39, Training Loss: 0.8026670284832225\n",
      "Epoch 40, Training Loss: 0.8024197261473712\n",
      "Epoch 41, Training Loss: 0.8021782247459187\n",
      "Epoch 42, Training Loss: 0.8018908899671892\n",
      "Epoch 43, Training Loss: 0.8016993088581983\n",
      "Epoch 44, Training Loss: 0.801542724721572\n",
      "Epoch 45, Training Loss: 0.8012293690092424\n",
      "Epoch 46, Training Loss: 0.8011388614598443\n",
      "Epoch 47, Training Loss: 0.8010892084766836\n",
      "Epoch 48, Training Loss: 0.8009765894973979\n",
      "Epoch 49, Training Loss: 0.8007048690319061\n",
      "Epoch 50, Training Loss: 0.8006683084544014\n",
      "Epoch 51, Training Loss: 0.800506777973736\n",
      "Epoch 52, Training Loss: 0.8003879008573644\n",
      "Epoch 53, Training Loss: 0.8003302626750048\n",
      "Epoch 54, Training Loss: 0.8001790648348192\n",
      "Epoch 55, Training Loss: 0.800116044773775\n",
      "Epoch 56, Training Loss: 0.8000543986348545\n",
      "Epoch 57, Training Loss: 0.7999084010544946\n",
      "Epoch 58, Training Loss: 0.7997725688008701\n",
      "Epoch 59, Training Loss: 0.7995729409947114\n",
      "Epoch 60, Training Loss: 0.7995525649014641\n",
      "Epoch 61, Training Loss: 0.7994843492087196\n",
      "Epoch 62, Training Loss: 0.799332493613748\n",
      "Epoch 63, Training Loss: 0.7993286993924309\n",
      "Epoch 64, Training Loss: 0.7992868755845463\n",
      "Epoch 65, Training Loss: 0.7989905637152055\n",
      "Epoch 66, Training Loss: 0.798990912507562\n",
      "Epoch 67, Training Loss: 0.798771837458891\n",
      "Epoch 68, Training Loss: 0.7986649000644683\n",
      "Epoch 69, Training Loss: 0.7986191857562346\n",
      "Epoch 70, Training Loss: 0.7984662286674276\n",
      "Epoch 71, Training Loss: 0.7982953073697932\n",
      "Epoch 72, Training Loss: 0.7982616461725796\n",
      "Epoch 73, Training Loss: 0.7980741807993721\n",
      "Epoch 74, Training Loss: 0.7979010142298306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:30:04,438] Trial 274 finished with value: 0.6345333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7978963736926808\n",
      "Epoch 1, Training Loss: 0.8453017904477961\n",
      "Epoch 2, Training Loss: 0.8164477091677048\n",
      "Epoch 3, Training Loss: 0.8155975506586187\n",
      "Epoch 4, Training Loss: 0.8135001843115862\n",
      "Epoch 5, Training Loss: 0.8106875801787657\n",
      "Epoch 6, Training Loss: 0.8085000085830688\n",
      "Epoch 7, Training Loss: 0.8099141626498279\n",
      "Epoch 8, Training Loss: 0.8069411038651186\n",
      "Epoch 9, Training Loss: 0.8052560091719908\n",
      "Epoch 10, Training Loss: 0.8047514139203464\n",
      "Epoch 11, Training Loss: 0.8062598170252407\n",
      "Epoch 12, Training Loss: 0.8037770690637477\n",
      "Epoch 13, Training Loss: 0.8052769196033478\n",
      "Epoch 14, Training Loss: 0.8034006022004521\n",
      "Epoch 15, Training Loss: 0.8038783227696138\n",
      "Epoch 16, Training Loss: 0.802039405037375\n",
      "Epoch 17, Training Loss: 0.8030131699057186\n",
      "Epoch 18, Training Loss: 0.8023239022142747\n",
      "Epoch 19, Training Loss: 0.8015387448843788\n",
      "Epoch 20, Training Loss: 0.8017528480641982\n",
      "Epoch 21, Training Loss: 0.8009736190122717\n",
      "Epoch 22, Training Loss: 0.8005298810145435\n",
      "Epoch 23, Training Loss: 0.8018400170522577\n",
      "Epoch 24, Training Loss: 0.8006184057628407\n",
      "Epoch 25, Training Loss: 0.7991442003670861\n",
      "Epoch 26, Training Loss: 0.7995824203771703\n",
      "Epoch 27, Training Loss: 0.8000569926289951\n",
      "Epoch 28, Training Loss: 0.799870511153165\n",
      "Epoch 29, Training Loss: 0.801109224768246\n",
      "Epoch 30, Training Loss: 0.7997589081876418\n",
      "Epoch 31, Training Loss: 0.7991001113022075\n",
      "Epoch 32, Training Loss: 0.8003790146463058\n",
      "Epoch 33, Training Loss: 0.7992702489039477\n",
      "Epoch 34, Training Loss: 0.799598067928763\n",
      "Epoch 35, Training Loss: 0.7993951865504769\n",
      "Epoch 36, Training Loss: 0.8000035553118762\n",
      "Epoch 37, Training Loss: 0.7994359128615436\n",
      "Epoch 38, Training Loss: 0.7996111985515145\n",
      "Epoch 39, Training Loss: 0.799926616935169\n",
      "Epoch 40, Training Loss: 0.7994539774866665\n",
      "Epoch 41, Training Loss: 0.7998617888899411\n",
      "Epoch 42, Training Loss: 0.7989201894227196\n",
      "Epoch 43, Training Loss: 0.7996990374256583\n",
      "Epoch 44, Training Loss: 0.7985080596979927\n",
      "Epoch 45, Training Loss: 0.7989731412775376\n",
      "Epoch 46, Training Loss: 0.7989986174948075\n",
      "Epoch 47, Training Loss: 0.7989391418765573\n",
      "Epoch 48, Training Loss: 0.7980924878400915\n",
      "Epoch 49, Training Loss: 0.8002230231200947\n",
      "Epoch 50, Training Loss: 0.7988677589332356\n",
      "Epoch 51, Training Loss: 0.7981906399306129\n",
      "Epoch 52, Training Loss: 0.7995443913515876\n",
      "Epoch 53, Training Loss: 0.7989514303207398\n",
      "Epoch 54, Training Loss: 0.7993654675343458\n",
      "Epoch 55, Training Loss: 0.7981785188001745\n",
      "Epoch 56, Training Loss: 0.7992011266596177\n",
      "Epoch 57, Training Loss: 0.7984260579417733\n",
      "Epoch 58, Training Loss: 0.7982349899937125\n",
      "Epoch 59, Training Loss: 0.798543786722071\n",
      "Epoch 60, Training Loss: 0.7989978648634518\n",
      "Epoch 61, Training Loss: 0.7984337384560529\n",
      "Epoch 62, Training Loss: 0.7981380961221807\n",
      "Epoch 63, Training Loss: 0.7982046804708594\n",
      "Epoch 64, Training Loss: 0.7968274206273696\n",
      "Epoch 65, Training Loss: 0.7985936277754166\n",
      "Epoch 66, Training Loss: 0.7974632591359756\n",
      "Epoch 67, Training Loss: 0.7996705269813538\n",
      "Epoch 68, Training Loss: 0.7979980099902434\n",
      "Epoch 69, Training Loss: 0.7983765750772813\n",
      "Epoch 70, Training Loss: 0.7984394421296961\n",
      "Epoch 71, Training Loss: 0.7985335585650276\n",
      "Epoch 72, Training Loss: 0.7985463000045103\n",
      "Epoch 73, Training Loss: 0.7980104678518631\n",
      "Epoch 74, Training Loss: 0.7990369052746716\n",
      "Epoch 75, Training Loss: 0.7980658718417672\n",
      "Epoch 76, Training Loss: 0.7986564313664156\n",
      "Epoch 77, Training Loss: 0.7973676006934222\n",
      "Epoch 78, Training Loss: 0.7983482064219082\n",
      "Epoch 79, Training Loss: 0.7984019016518312\n",
      "Epoch 80, Training Loss: 0.7981815606706283\n",
      "Epoch 81, Training Loss: 0.7979825755427865\n",
      "Epoch 82, Training Loss: 0.7991984093890471\n",
      "Epoch 83, Training Loss: 0.7979452246778151\n",
      "Epoch 84, Training Loss: 0.797900713191313\n",
      "Epoch 85, Training Loss: 0.7983654945036944\n",
      "Epoch 86, Training Loss: 0.7982672901013318\n",
      "Epoch 87, Training Loss: 0.7975335200393902\n",
      "Epoch 88, Training Loss: 0.7986362899752224\n",
      "Epoch 89, Training Loss: 0.798168724564945\n",
      "Epoch 90, Training Loss: 0.7977911409209756\n",
      "Epoch 91, Training Loss: 0.797076792085872\n",
      "Epoch 92, Training Loss: 0.7986095779082354\n",
      "Epoch 93, Training Loss: 0.798448641650817\n",
      "Epoch 94, Training Loss: 0.7983415995625889\n",
      "Epoch 95, Training Loss: 0.7984342565957238\n",
      "Epoch 96, Training Loss: 0.7968630765466129\n",
      "Epoch 97, Training Loss: 0.7975586469033186\n",
      "Epoch 98, Training Loss: 0.797351859106737\n",
      "Epoch 99, Training Loss: 0.7985999451665318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:32:01,695] Trial 275 finished with value: 0.6336666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7970498770826003\n",
      "Epoch 1, Training Loss: 0.855525951385498\n",
      "Epoch 2, Training Loss: 0.8192527585169849\n",
      "Epoch 3, Training Loss: 0.812595767694361\n",
      "Epoch 4, Training Loss: 0.8082695295530207\n",
      "Epoch 5, Training Loss: 0.8049146244105171\n",
      "Epoch 6, Training Loss: 0.8033723640441894\n",
      "Epoch 7, Training Loss: 0.8016016828312593\n",
      "Epoch 8, Training Loss: 0.8008452590072856\n",
      "Epoch 9, Training Loss: 0.7992891925222734\n",
      "Epoch 10, Training Loss: 0.7993213431975421\n",
      "Epoch 11, Training Loss: 0.797947002509061\n",
      "Epoch 12, Training Loss: 0.7975954243716071\n",
      "Epoch 13, Training Loss: 0.7964761383393232\n",
      "Epoch 14, Training Loss: 0.7973731172786039\n",
      "Epoch 15, Training Loss: 0.7954454344160417\n",
      "Epoch 16, Training Loss: 0.7959836786634782\n",
      "Epoch 17, Training Loss: 0.7950625032537124\n",
      "Epoch 18, Training Loss: 0.7949752762738396\n",
      "Epoch 19, Training Loss: 0.7947099736858817\n",
      "Epoch 20, Training Loss: 0.794439993465648\n",
      "Epoch 21, Training Loss: 0.7942781821419211\n",
      "Epoch 22, Training Loss: 0.7934279001460356\n",
      "Epoch 23, Training Loss: 0.7931748101991766\n",
      "Epoch 24, Training Loss: 0.7930701680744395\n",
      "Epoch 25, Training Loss: 0.7934404183135313\n",
      "Epoch 26, Training Loss: 0.7929720427709467\n",
      "Epoch 27, Training Loss: 0.7926313106452717\n",
      "Epoch 28, Training Loss: 0.7924524598963121\n",
      "Epoch 29, Training Loss: 0.7927501045956331\n",
      "Epoch 30, Training Loss: 0.7930826450796689\n",
      "Epoch 31, Training Loss: 0.7921784626035129\n",
      "Epoch 32, Training Loss: 0.7921130800948424\n",
      "Epoch 33, Training Loss: 0.7918401648016536\n",
      "Epoch 34, Training Loss: 0.7921291789587807\n",
      "Epoch 35, Training Loss: 0.7922857936690836\n",
      "Epoch 36, Training Loss: 0.7912627263630138\n",
      "Epoch 37, Training Loss: 0.7914119600548464\n",
      "Epoch 38, Training Loss: 0.7914436054930968\n",
      "Epoch 39, Training Loss: 0.7919848749918096\n",
      "Epoch 40, Training Loss: 0.7917165761835435\n",
      "Epoch 41, Training Loss: 0.7912806393819697\n",
      "Epoch 42, Training Loss: 0.7916322049673866\n",
      "Epoch 43, Training Loss: 0.7913249876919914\n",
      "Epoch 44, Training Loss: 0.7913364389363458\n",
      "Epoch 45, Training Loss: 0.7913513044048758\n",
      "Epoch 46, Training Loss: 0.792107275093303\n",
      "Epoch 47, Training Loss: 0.7914292866342207\n",
      "Epoch 48, Training Loss: 0.7913358776008381\n",
      "Epoch 49, Training Loss: 0.7912274227422826\n",
      "Epoch 50, Training Loss: 0.791471544083427\n",
      "Epoch 51, Training Loss: 0.7904262625469881\n",
      "Epoch 52, Training Loss: 0.7918584627263686\n",
      "Epoch 53, Training Loss: 0.7918469463376437\n",
      "Epoch 54, Training Loss: 0.7911574803380406\n",
      "Epoch 55, Training Loss: 0.7903457715932061\n",
      "Epoch 56, Training Loss: 0.790505713224411\n",
      "Epoch 57, Training Loss: 0.7909675368140725\n",
      "Epoch 58, Training Loss: 0.7913068961395937\n",
      "Epoch 59, Training Loss: 0.7914336539717282\n",
      "Epoch 60, Training Loss: 0.7905793040640214\n",
      "Epoch 61, Training Loss: 0.7905148671655093\n",
      "Epoch 62, Training Loss: 0.7917366557962754\n",
      "Epoch 63, Training Loss: 0.7917460676501779\n",
      "Epoch 64, Training Loss: 0.7910343916977153\n",
      "Epoch 65, Training Loss: 0.7906574410550734\n",
      "Epoch 66, Training Loss: 0.789988105577581\n",
      "Epoch 67, Training Loss: 0.7910232180006364\n",
      "Epoch 68, Training Loss: 0.7911411651443032\n",
      "Epoch 69, Training Loss: 0.7906793396613178\n",
      "Epoch 70, Training Loss: 0.7906744147048277\n",
      "Epoch 71, Training Loss: 0.7901066685424132\n",
      "Epoch 72, Training Loss: 0.791085261036368\n",
      "Epoch 73, Training Loss: 0.7907985598900739\n",
      "Epoch 74, Training Loss: 0.7910042813946219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:33:50,199] Trial 276 finished with value: 0.6344666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7906237720741945\n",
      "Epoch 1, Training Loss: 0.9740930440846611\n",
      "Epoch 2, Training Loss: 0.9335198544754701\n",
      "Epoch 3, Training Loss: 0.8948186305691214\n",
      "Epoch 4, Training Loss: 0.8509732176275814\n",
      "Epoch 5, Training Loss: 0.8246816177929149\n",
      "Epoch 6, Training Loss: 0.8158135847484365\n",
      "Epoch 7, Training Loss: 0.8133156380933874\n",
      "Epoch 8, Training Loss: 0.8119428244759055\n",
      "Epoch 9, Training Loss: 0.8109237485072192\n",
      "Epoch 10, Training Loss: 0.810419683526544\n",
      "Epoch 11, Training Loss: 0.8092062307806576\n",
      "Epoch 12, Training Loss: 0.8084324324832243\n",
      "Epoch 13, Training Loss: 0.8074983029505786\n",
      "Epoch 14, Training Loss: 0.8071890072261586\n",
      "Epoch 15, Training Loss: 0.8067147837666904\n",
      "Epoch 16, Training Loss: 0.80599769431002\n",
      "Epoch 17, Training Loss: 0.8054684833919301\n",
      "Epoch 18, Training Loss: 0.8050276487715105\n",
      "Epoch 19, Training Loss: 0.8044354529941783\n",
      "Epoch 20, Training Loss: 0.8043352415281183\n",
      "Epoch 21, Training Loss: 0.8039012680334203\n",
      "Epoch 22, Training Loss: 0.8037485243292416\n",
      "Epoch 23, Training Loss: 0.8036128078488742\n",
      "Epoch 24, Training Loss: 0.8031254617606892\n",
      "Epoch 25, Training Loss: 0.8029918277964873\n",
      "Epoch 26, Training Loss: 0.8029654813514037\n",
      "Epoch 27, Training Loss: 0.8025476905177621\n",
      "Epoch 28, Training Loss: 0.8023974511903875\n",
      "Epoch 29, Training Loss: 0.8019913798220017\n",
      "Epoch 30, Training Loss: 0.8021573061802808\n",
      "Epoch 31, Training Loss: 0.8018988029395833\n",
      "Epoch 32, Training Loss: 0.8021440965287826\n",
      "Epoch 33, Training Loss: 0.8015003212059245\n",
      "Epoch 34, Training Loss: 0.8013674338424907\n",
      "Epoch 35, Training Loss: 0.8010586869015414\n",
      "Epoch 36, Training Loss: 0.8014155171899234\n",
      "Epoch 37, Training Loss: 0.8009174287319183\n",
      "Epoch 38, Training Loss: 0.8007066301037283\n",
      "Epoch 39, Training Loss: 0.8009447793399587\n",
      "Epoch 40, Training Loss: 0.800711470561869\n",
      "Epoch 41, Training Loss: 0.800736635502647\n",
      "Epoch 42, Training Loss: 0.8002607438844793\n",
      "Epoch 43, Training Loss: 0.8005068835791419\n",
      "Epoch 44, Training Loss: 0.8001751833803513\n",
      "Epoch 45, Training Loss: 0.7999988822376027\n",
      "Epoch 46, Training Loss: 0.8000066606437459\n",
      "Epoch 47, Training Loss: 0.7996507948286393\n",
      "Epoch 48, Training Loss: 0.7995241450562197\n",
      "Epoch 49, Training Loss: 0.7994298500874463\n",
      "Epoch 50, Training Loss: 0.7996055395462933\n",
      "Epoch 51, Training Loss: 0.7992469730797936\n",
      "Epoch 52, Training Loss: 0.799353429219302\n",
      "Epoch 53, Training Loss: 0.7988772387364331\n",
      "Epoch 54, Training Loss: 0.7991546934492448\n",
      "Epoch 55, Training Loss: 0.7988968936134787\n",
      "Epoch 56, Training Loss: 0.7988265875507804\n",
      "Epoch 57, Training Loss: 0.7988906307080212\n",
      "Epoch 58, Training Loss: 0.7982471203102784\n",
      "Epoch 59, Training Loss: 0.798582311518052\n",
      "Epoch 60, Training Loss: 0.7987286069112666\n",
      "Epoch 61, Training Loss: 0.7985430228008944\n",
      "Epoch 62, Training Loss: 0.798348196815042\n",
      "Epoch 63, Training Loss: 0.7982572822711047\n",
      "Epoch 64, Training Loss: 0.7978257437313304\n",
      "Epoch 65, Training Loss: 0.797892809194677\n",
      "Epoch 66, Training Loss: 0.7977081181722528\n",
      "Epoch 67, Training Loss: 0.7979878473983092\n",
      "Epoch 68, Training Loss: 0.7978255464750178\n",
      "Epoch 69, Training Loss: 0.7974942611946779\n",
      "Epoch 70, Training Loss: 0.7974705400186427\n",
      "Epoch 71, Training Loss: 0.7972047742675332\n",
      "Epoch 72, Training Loss: 0.7974865162372589\n",
      "Epoch 73, Training Loss: 0.7973917659591226\n",
      "Epoch 74, Training Loss: 0.7970453761605656\n",
      "Epoch 75, Training Loss: 0.797035950422287\n",
      "Epoch 76, Training Loss: 0.7971425885312697\n",
      "Epoch 77, Training Loss: 0.7969531972969279\n",
      "Epoch 78, Training Loss: 0.7970043484603657\n",
      "Epoch 79, Training Loss: 0.7967392058933482\n",
      "Epoch 80, Training Loss: 0.7966501292761634\n",
      "Epoch 81, Training Loss: 0.7964950194078333\n",
      "Epoch 82, Training Loss: 0.7964776632365058\n",
      "Epoch 83, Training Loss: 0.7962250183610355\n",
      "Epoch 84, Training Loss: 0.7964144589620478\n",
      "Epoch 85, Training Loss: 0.7960062606895671\n",
      "Epoch 86, Training Loss: 0.796108072084539\n",
      "Epoch 87, Training Loss: 0.7955888703991385\n",
      "Epoch 88, Training Loss: 0.7956871623151442\n",
      "Epoch 89, Training Loss: 0.7955168280180763\n",
      "Epoch 90, Training Loss: 0.795773613803527\n",
      "Epoch 91, Training Loss: 0.795368250538321\n",
      "Epoch 92, Training Loss: 0.7955940318107605\n",
      "Epoch 93, Training Loss: 0.7951726873481975\n",
      "Epoch 94, Training Loss: 0.795152622461319\n",
      "Epoch 95, Training Loss: 0.7948462458217845\n",
      "Epoch 96, Training Loss: 0.7949543652814978\n",
      "Epoch 97, Training Loss: 0.7947545030537774\n",
      "Epoch 98, Training Loss: 0.7945254987127641\n",
      "Epoch 99, Training Loss: 0.7943907669011284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:35:56,180] Trial 277 finished with value: 0.6344 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7941628827768213\n",
      "Epoch 1, Training Loss: 0.8816185834127314\n",
      "Epoch 2, Training Loss: 0.8168351070319905\n",
      "Epoch 3, Training Loss: 0.812110686232062\n",
      "Epoch 4, Training Loss: 0.8088496039895451\n",
      "Epoch 5, Training Loss: 0.8056428191241096\n",
      "Epoch 6, Training Loss: 0.8023294964958639\n",
      "Epoch 7, Training Loss: 0.8005555328200845\n",
      "Epoch 8, Training Loss: 0.7964939505913678\n",
      "Epoch 9, Training Loss: 0.7958004501987906\n",
      "Epoch 10, Training Loss: 0.7940932857289034\n",
      "Epoch 11, Training Loss: 0.7934702441271614\n",
      "Epoch 12, Training Loss: 0.7919999983030207\n",
      "Epoch 13, Training Loss: 0.7925380377208485\n",
      "Epoch 14, Training Loss: 0.7907496106624603\n",
      "Epoch 15, Training Loss: 0.7904259569504681\n",
      "Epoch 16, Training Loss: 0.7910245600167443\n",
      "Epoch 17, Training Loss: 0.789030647277832\n",
      "Epoch 18, Training Loss: 0.7892538548217101\n",
      "Epoch 19, Training Loss: 0.7899050542887519\n",
      "Epoch 20, Training Loss: 0.7886245749277228\n",
      "Epoch 21, Training Loss: 0.7893241155147552\n",
      "Epoch 22, Training Loss: 0.7882130729450899\n",
      "Epoch 23, Training Loss: 0.787665946413489\n",
      "Epoch 24, Training Loss: 0.7882942376417272\n",
      "Epoch 25, Training Loss: 0.7876621935648077\n",
      "Epoch 26, Training Loss: 0.7879502999081331\n",
      "Epoch 27, Training Loss: 0.7870094431147856\n",
      "Epoch 28, Training Loss: 0.7878603402306051\n",
      "Epoch 29, Training Loss: 0.7870118755452773\n",
      "Epoch 30, Training Loss: 0.7869458436264711\n",
      "Epoch 31, Training Loss: 0.7865656929857591\n",
      "Epoch 32, Training Loss: 0.7877308121849509\n",
      "Epoch 33, Training Loss: 0.7860507156568415\n",
      "Epoch 34, Training Loss: 0.7863821784888997\n",
      "Epoch 35, Training Loss: 0.7872834571670083\n",
      "Epoch 36, Training Loss: 0.7861912880925571\n",
      "Epoch 37, Training Loss: 0.7861450181989109\n",
      "Epoch 38, Training Loss: 0.7861821931951186\n",
      "Epoch 39, Training Loss: 0.7860858248261844\n",
      "Epoch 40, Training Loss: 0.7857822572483736\n",
      "Epoch 41, Training Loss: 0.7857638527365292\n",
      "Epoch 42, Training Loss: 0.7852406887447133\n",
      "Epoch 43, Training Loss: 0.7852339749476489\n",
      "Epoch 44, Training Loss: 0.7851249467625337\n",
      "Epoch 45, Training Loss: 0.7848823786483091\n",
      "Epoch 46, Training Loss: 0.7841423411229077\n",
      "Epoch 47, Training Loss: 0.7848620134241441\n",
      "Epoch 48, Training Loss: 0.7846277004830977\n",
      "Epoch 49, Training Loss: 0.7847899648021249\n",
      "Epoch 50, Training Loss: 0.7844941872007707\n",
      "Epoch 51, Training Loss: 0.7844274447244757\n",
      "Epoch 52, Training Loss: 0.7844869456571691\n",
      "Epoch 53, Training Loss: 0.7838189435706419\n",
      "Epoch 54, Training Loss: 0.7835853261807385\n",
      "Epoch 55, Training Loss: 0.7849095913943123\n",
      "Epoch 56, Training Loss: 0.784347159652149\n",
      "Epoch 57, Training Loss: 0.784081782873939\n",
      "Epoch 58, Training Loss: 0.7839181701576009\n",
      "Epoch 59, Training Loss: 0.7832194333216723\n",
      "Epoch 60, Training Loss: 0.7839073133468628\n",
      "Epoch 61, Training Loss: 0.7826246284737306\n",
      "Epoch 62, Training Loss: 0.783021910611321\n",
      "Epoch 63, Training Loss: 0.783021520726821\n",
      "Epoch 64, Training Loss: 0.7839490285340478\n",
      "Epoch 65, Training Loss: 0.7835038281889523\n",
      "Epoch 66, Training Loss: 0.7831822832191692\n",
      "Epoch 67, Training Loss: 0.7834473400256213\n",
      "Epoch 68, Training Loss: 0.7826078810411341\n",
      "Epoch 69, Training Loss: 0.7826581841356615\n",
      "Epoch 70, Training Loss: 0.7835446645231808\n",
      "Epoch 71, Training Loss: 0.7823336500280044\n",
      "Epoch 72, Training Loss: 0.7823455898901995\n",
      "Epoch 73, Training Loss: 0.7820871975141414\n",
      "Epoch 74, Training Loss: 0.7827258730635923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:37:38,578] Trial 278 finished with value: 0.6382666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7824451768398285\n",
      "Epoch 1, Training Loss: 0.9793777023939262\n",
      "Epoch 2, Training Loss: 0.9431629347621946\n",
      "Epoch 3, Training Loss: 0.9243578761143791\n",
      "Epoch 4, Training Loss: 0.8970764633408166\n",
      "Epoch 5, Training Loss: 0.866031241058407\n",
      "Epoch 6, Training Loss: 0.8413545742070765\n",
      "Epoch 7, Training Loss: 0.8280872647923635\n",
      "Epoch 8, Training Loss: 0.8203327560783329\n",
      "Epoch 9, Training Loss: 0.8156246669310376\n",
      "Epoch 10, Training Loss: 0.8126321275431411\n",
      "Epoch 11, Training Loss: 0.8110755450743482\n",
      "Epoch 12, Training Loss: 0.8092201697198969\n",
      "Epoch 13, Training Loss: 0.8084725972405054\n",
      "Epoch 14, Training Loss: 0.8075237039336585\n",
      "Epoch 15, Training Loss: 0.8071394084987784\n",
      "Epoch 16, Training Loss: 0.8062658248091102\n",
      "Epoch 17, Training Loss: 0.805468117563348\n",
      "Epoch 18, Training Loss: 0.8048739036223046\n",
      "Epoch 19, Training Loss: 0.8050483166723323\n",
      "Epoch 20, Training Loss: 0.8042313655516259\n",
      "Epoch 21, Training Loss: 0.8034381411129371\n",
      "Epoch 22, Training Loss: 0.8023165843092409\n",
      "Epoch 23, Training Loss: 0.8022842713764735\n",
      "Epoch 24, Training Loss: 0.8021495522413039\n",
      "Epoch 25, Training Loss: 0.8013738504029755\n",
      "Epoch 26, Training Loss: 0.8011503053787059\n",
      "Epoch 27, Training Loss: 0.8006395201934011\n",
      "Epoch 28, Training Loss: 0.8005848206075511\n",
      "Epoch 29, Training Loss: 0.8001017894959987\n",
      "Epoch 30, Training Loss: 0.8002134102627747\n",
      "Epoch 31, Training Loss: 0.7995514159812067\n",
      "Epoch 32, Training Loss: 0.7995075030434401\n",
      "Epoch 33, Training Loss: 0.799473952530022\n",
      "Epoch 34, Training Loss: 0.7992057054562676\n",
      "Epoch 35, Training Loss: 0.7995158812157194\n",
      "Epoch 36, Training Loss: 0.799563671682114\n",
      "Epoch 37, Training Loss: 0.7990686914974585\n",
      "Epoch 38, Training Loss: 0.7989011752874331\n",
      "Epoch 39, Training Loss: 0.7986093409079358\n",
      "Epoch 40, Training Loss: 0.7982894981714119\n",
      "Epoch 41, Training Loss: 0.7981501905541671\n",
      "Epoch 42, Training Loss: 0.7979240209536445\n",
      "Epoch 43, Training Loss: 0.7981293092096658\n",
      "Epoch 44, Training Loss: 0.798376468877147\n",
      "Epoch 45, Training Loss: 0.7980534774916512\n",
      "Epoch 46, Training Loss: 0.7983340188076622\n",
      "Epoch 47, Training Loss: 0.7976527508936431\n",
      "Epoch 48, Training Loss: 0.7973706417513969\n",
      "Epoch 49, Training Loss: 0.7977919220924378\n",
      "Epoch 50, Training Loss: 0.7978368364778676\n",
      "Epoch 51, Training Loss: 0.7974360701733065\n",
      "Epoch 52, Training Loss: 0.7974231138265222\n",
      "Epoch 53, Training Loss: 0.7968988202568283\n",
      "Epoch 54, Training Loss: 0.7973905825077143\n",
      "Epoch 55, Training Loss: 0.7967976447334864\n",
      "Epoch 56, Training Loss: 0.7971106003101607\n",
      "Epoch 57, Training Loss: 0.7975154804107838\n",
      "Epoch 58, Training Loss: 0.796850910043358\n",
      "Epoch 59, Training Loss: 0.7962239873588534\n",
      "Epoch 60, Training Loss: 0.7963806361184085\n",
      "Epoch 61, Training Loss: 0.7969867817441324\n",
      "Epoch 62, Training Loss: 0.7966730841120383\n",
      "Epoch 63, Training Loss: 0.7958965362462782\n",
      "Epoch 64, Training Loss: 0.796221765808593\n",
      "Epoch 65, Training Loss: 0.7956402809996354\n",
      "Epoch 66, Training Loss: 0.7959402978868413\n",
      "Epoch 67, Training Loss: 0.7955965797703966\n",
      "Epoch 68, Training Loss: 0.796090176768769\n",
      "Epoch 69, Training Loss: 0.7962982384782088\n",
      "Epoch 70, Training Loss: 0.7958348362069381\n",
      "Epoch 71, Training Loss: 0.7952289193196405\n",
      "Epoch 72, Training Loss: 0.7958084170083354\n",
      "Epoch 73, Training Loss: 0.7950302779226375\n",
      "Epoch 74, Training Loss: 0.7947045220468277\n",
      "Epoch 75, Training Loss: 0.795604920387268\n",
      "Epoch 76, Training Loss: 0.7957828960024325\n",
      "Epoch 77, Training Loss: 0.7951611324360496\n",
      "Epoch 78, Training Loss: 0.7952240738653599\n",
      "Epoch 79, Training Loss: 0.7951670624259719\n",
      "Epoch 80, Training Loss: 0.7958409040494072\n",
      "Epoch 81, Training Loss: 0.7944841311390238\n",
      "Epoch 82, Training Loss: 0.794136412116818\n",
      "Epoch 83, Training Loss: 0.7942605214907711\n",
      "Epoch 84, Training Loss: 0.7945343081216166\n",
      "Epoch 85, Training Loss: 0.7936833101107662\n",
      "Epoch 86, Training Loss: 0.7937311816036253\n",
      "Epoch 87, Training Loss: 0.7941381096839905\n",
      "Epoch 88, Training Loss: 0.7931012760427661\n",
      "Epoch 89, Training Loss: 0.7936702622506852\n",
      "Epoch 90, Training Loss: 0.7934443496223679\n",
      "Epoch 91, Training Loss: 0.7933352899730653\n",
      "Epoch 92, Training Loss: 0.7930501731714807\n",
      "Epoch 93, Training Loss: 0.7929653666073219\n",
      "Epoch 94, Training Loss: 0.7927883392886111\n",
      "Epoch 95, Training Loss: 0.7925654993021398\n",
      "Epoch 96, Training Loss: 0.792902952477448\n",
      "Epoch 97, Training Loss: 0.7924542280068075\n",
      "Epoch 98, Training Loss: 0.7924812140321373\n",
      "Epoch 99, Training Loss: 0.7923174605333716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:39:09,178] Trial 279 finished with value: 0.6368 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7919144354368511\n",
      "Epoch 1, Training Loss: 0.8520340263395381\n",
      "Epoch 2, Training Loss: 0.811073769304089\n",
      "Epoch 3, Training Loss: 0.8103411607276229\n",
      "Epoch 4, Training Loss: 0.8066780335024784\n",
      "Epoch 5, Training Loss: 0.8049907989968035\n",
      "Epoch 6, Training Loss: 0.8035357875035222\n",
      "Epoch 7, Training Loss: 0.8034722733318358\n",
      "Epoch 8, Training Loss: 0.8019309119174355\n",
      "Epoch 9, Training Loss: 0.8021408729983451\n",
      "Epoch 10, Training Loss: 0.8016926245581835\n",
      "Epoch 11, Training Loss: 0.8005904803598733\n",
      "Epoch 12, Training Loss: 0.7992272970371677\n",
      "Epoch 13, Training Loss: 0.7995161148838531\n",
      "Epoch 14, Training Loss: 0.7981052016853389\n",
      "Epoch 15, Training Loss: 0.7980043219444447\n",
      "Epoch 16, Training Loss: 0.7970731683243486\n",
      "Epoch 17, Training Loss: 0.7961112406468929\n",
      "Epoch 18, Training Loss: 0.7970648202681003\n",
      "Epoch 19, Training Loss: 0.7963284052404246\n",
      "Epoch 20, Training Loss: 0.7955643063201043\n",
      "Epoch 21, Training Loss: 0.7959180859694803\n",
      "Epoch 22, Training Loss: 0.7945035985537938\n",
      "Epoch 23, Training Loss: 0.7942762702031243\n",
      "Epoch 24, Training Loss: 0.7953684601568638\n",
      "Epoch 25, Training Loss: 0.7956951905910233\n",
      "Epoch 26, Training Loss: 0.7952147162946543\n",
      "Epoch 27, Training Loss: 0.794691114407733\n",
      "Epoch 28, Training Loss: 0.7938539409099665\n",
      "Epoch 29, Training Loss: 0.7953220356675915\n",
      "Epoch 30, Training Loss: 0.7938828650273775\n",
      "Epoch 31, Training Loss: 0.7951183483116608\n",
      "Epoch 32, Training Loss: 0.7932678107032203\n",
      "Epoch 33, Training Loss: 0.7938820488470838\n",
      "Epoch 34, Training Loss: 0.7943651153628988\n",
      "Epoch 35, Training Loss: 0.7929241461861403\n",
      "Epoch 36, Training Loss: 0.7933152259740615\n",
      "Epoch 37, Training Loss: 0.7931999146490168\n",
      "Epoch 38, Training Loss: 0.7933967503389918\n",
      "Epoch 39, Training Loss: 0.7940371241784634\n",
      "Epoch 40, Training Loss: 0.7928393616712183\n",
      "Epoch 41, Training Loss: 0.7927812915995605\n",
      "Epoch 42, Training Loss: 0.7928227418347409\n",
      "Epoch 43, Training Loss: 0.792257092769881\n",
      "Epoch 44, Training Loss: 0.7938408804119081\n",
      "Epoch 45, Training Loss: 0.7940865826786012\n",
      "Epoch 46, Training Loss: 0.7949401995293179\n",
      "Epoch 47, Training Loss: 0.7925925645613132\n",
      "Epoch 48, Training Loss: 0.7920923007161994\n",
      "Epoch 49, Training Loss: 0.7932691135800871\n",
      "Epoch 50, Training Loss: 0.7915740173562129\n",
      "Epoch 51, Training Loss: 0.7920921836580549\n",
      "Epoch 52, Training Loss: 0.79319832351871\n",
      "Epoch 53, Training Loss: 0.7918553922409401\n",
      "Epoch 54, Training Loss: 0.7918524375535492\n",
      "Epoch 55, Training Loss: 0.7908713255609785\n",
      "Epoch 56, Training Loss: 0.791563261899733\n",
      "Epoch 57, Training Loss: 0.790781771688533\n",
      "Epoch 58, Training Loss: 0.7915020114497134\n",
      "Epoch 59, Training Loss: 0.7930915500884665\n",
      "Epoch 60, Training Loss: 0.7916440015448664\n",
      "Epoch 61, Training Loss: 0.7915772854833675\n",
      "Epoch 62, Training Loss: 0.792262117575882\n",
      "Epoch 63, Training Loss: 0.7916256025321502\n",
      "Epoch 64, Training Loss: 0.7919345277592652\n",
      "Epoch 65, Training Loss: 0.7910223512721241\n",
      "Epoch 66, Training Loss: 0.7912543452771983\n",
      "Epoch 67, Training Loss: 0.7914528353769976\n",
      "Epoch 68, Training Loss: 0.7908822331213413\n",
      "Epoch 69, Training Loss: 0.7913363670944271\n",
      "Epoch 70, Training Loss: 0.7906045601780253\n",
      "Epoch 71, Training Loss: 0.791666156994669\n",
      "Epoch 72, Training Loss: 0.7896234450035525\n",
      "Epoch 73, Training Loss: 0.7916292086579746\n",
      "Epoch 74, Training Loss: 0.7908916277096684\n",
      "Epoch 75, Training Loss: 0.7903048512630892\n",
      "Epoch 76, Training Loss: 0.7907529339754492\n",
      "Epoch 77, Training Loss: 0.7908231414350352\n",
      "Epoch 78, Training Loss: 0.7909527720813464\n",
      "Epoch 79, Training Loss: 0.791305483104591\n",
      "Epoch 80, Training Loss: 0.7911147388300501\n",
      "Epoch 81, Training Loss: 0.7907464170814457\n",
      "Epoch 82, Training Loss: 0.790865387325\n",
      "Epoch 83, Training Loss: 0.7903596701926755\n",
      "Epoch 84, Training Loss: 0.7913352532494337\n",
      "Epoch 85, Training Loss: 0.7908173671342377\n",
      "Epoch 86, Training Loss: 0.7897127644460004\n",
      "Epoch 87, Training Loss: 0.7910743022323551\n",
      "Epoch 88, Training Loss: 0.7898603192845681\n",
      "Epoch 89, Training Loss: 0.790149620421847\n",
      "Epoch 90, Training Loss: 0.7906196418561433\n",
      "Epoch 91, Training Loss: 0.7903712925158048\n",
      "Epoch 92, Training Loss: 0.7899017197745187\n",
      "Epoch 93, Training Loss: 0.7897343164996097\n",
      "Epoch 94, Training Loss: 0.7906464857266362\n",
      "Epoch 95, Training Loss: 0.7902911360998799\n",
      "Epoch 96, Training Loss: 0.7902687525838837\n",
      "Epoch 97, Training Loss: 0.790238961151668\n",
      "Epoch 98, Training Loss: 0.7905786095705247\n",
      "Epoch 99, Training Loss: 0.7890451625773781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:40:46,708] Trial 280 finished with value: 0.6338 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7906930443039514\n",
      "Epoch 1, Training Loss: 0.9932053066734085\n",
      "Epoch 2, Training Loss: 0.945466771430539\n",
      "Epoch 3, Training Loss: 0.9242370540038087\n",
      "Epoch 4, Training Loss: 0.893481116904352\n",
      "Epoch 5, Training Loss: 0.858075814049943\n",
      "Epoch 6, Training Loss: 0.8313906119282084\n",
      "Epoch 7, Training Loss: 0.8186788601982863\n",
      "Epoch 8, Training Loss: 0.8140411618060636\n",
      "Epoch 9, Training Loss: 0.8123319206381203\n",
      "Epoch 10, Training Loss: 0.8113829447810811\n",
      "Epoch 11, Training Loss: 0.8106705394006313\n",
      "Epoch 12, Training Loss: 0.8095423820323514\n",
      "Epoch 13, Training Loss: 0.8094018399267269\n",
      "Epoch 14, Training Loss: 0.8081866037576718\n",
      "Epoch 15, Training Loss: 0.8074589508816712\n",
      "Epoch 16, Training Loss: 0.8074003867636946\n",
      "Epoch 17, Training Loss: 0.8062028365923946\n",
      "Epoch 18, Training Loss: 0.8052794715515653\n",
      "Epoch 19, Training Loss: 0.8051154628732151\n",
      "Epoch 20, Training Loss: 0.8046881307336621\n",
      "Epoch 21, Training Loss: 0.8044302381967243\n",
      "Epoch 22, Training Loss: 0.8042159664003472\n",
      "Epoch 23, Training Loss: 0.8041092186045826\n",
      "Epoch 24, Training Loss: 0.8026060031768971\n",
      "Epoch 25, Training Loss: 0.8025768763140628\n",
      "Epoch 26, Training Loss: 0.8026863283680794\n",
      "Epoch 27, Training Loss: 0.802475088789947\n",
      "Epoch 28, Training Loss: 0.8015238767279718\n",
      "Epoch 29, Training Loss: 0.8022112160697019\n",
      "Epoch 30, Training Loss: 0.8015715385738172\n",
      "Epoch 31, Training Loss: 0.8017190765617486\n",
      "Epoch 32, Training Loss: 0.8015881655807782\n",
      "Epoch 33, Training Loss: 0.8010305475471611\n",
      "Epoch 34, Training Loss: 0.8010944895278242\n",
      "Epoch 35, Training Loss: 0.8010324577639873\n",
      "Epoch 36, Training Loss: 0.8003706919519524\n",
      "Epoch 37, Training Loss: 0.8004805735179357\n",
      "Epoch 38, Training Loss: 0.8009460479693306\n",
      "Epoch 39, Training Loss: 0.8003265626448438\n",
      "Epoch 40, Training Loss: 0.7999368554667423\n",
      "Epoch 41, Training Loss: 0.7996485896576616\n",
      "Epoch 42, Training Loss: 0.8003281571811303\n",
      "Epoch 43, Training Loss: 0.7996118056146722\n",
      "Epoch 44, Training Loss: 0.7995333387439413\n",
      "Epoch 45, Training Loss: 0.799516919591373\n",
      "Epoch 46, Training Loss: 0.799367432486742\n",
      "Epoch 47, Training Loss: 0.7989397783476607\n",
      "Epoch 48, Training Loss: 0.7992544134756676\n",
      "Epoch 49, Training Loss: 0.7989833499255933\n",
      "Epoch 50, Training Loss: 0.7990874117478393\n",
      "Epoch 51, Training Loss: 0.7991631602882443\n",
      "Epoch 52, Training Loss: 0.798875399639732\n",
      "Epoch 53, Training Loss: 0.798874999257855\n",
      "Epoch 54, Training Loss: 0.7988671650563864\n",
      "Epoch 55, Training Loss: 0.7984506607055664\n",
      "Epoch 56, Training Loss: 0.7982565273915915\n",
      "Epoch 57, Training Loss: 0.7977974373595159\n",
      "Epoch 58, Training Loss: 0.7983653966645549\n",
      "Epoch 59, Training Loss: 0.7987679189309141\n",
      "Epoch 60, Training Loss: 0.7985423585526029\n",
      "Epoch 61, Training Loss: 0.7977343011619453\n",
      "Epoch 62, Training Loss: 0.79838019412263\n",
      "Epoch 63, Training Loss: 0.7979052709457569\n",
      "Epoch 64, Training Loss: 0.7980542571921098\n",
      "Epoch 65, Training Loss: 0.7976071870416627\n",
      "Epoch 66, Training Loss: 0.7969425554562332\n",
      "Epoch 67, Training Loss: 0.7977111053646059\n",
      "Epoch 68, Training Loss: 0.7974596913595845\n",
      "Epoch 69, Training Loss: 0.7972145792236902\n",
      "Epoch 70, Training Loss: 0.7973010983682216\n",
      "Epoch 71, Training Loss: 0.7976126249571492\n",
      "Epoch 72, Training Loss: 0.7970802811751688\n",
      "Epoch 73, Training Loss: 0.7969803768889349\n",
      "Epoch 74, Training Loss: 0.7973818656197168\n",
      "Epoch 75, Training Loss: 0.7972392883515895\n",
      "Epoch 76, Training Loss: 0.7974213176203849\n",
      "Epoch 77, Training Loss: 0.7964581248455478\n",
      "Epoch 78, Training Loss: 0.7966547956144003\n",
      "Epoch 79, Training Loss: 0.7972225121985701\n",
      "Epoch 80, Training Loss: 0.7976475336497888\n",
      "Epoch 81, Training Loss: 0.7968774894126376\n",
      "Epoch 82, Training Loss: 0.7965173485583829\n",
      "Epoch 83, Training Loss: 0.7962462857253569\n",
      "Epoch 84, Training Loss: 0.7965806909073564\n",
      "Epoch 85, Training Loss: 0.7966260597221834\n",
      "Epoch 86, Training Loss: 0.7969743189058806\n",
      "Epoch 87, Training Loss: 0.7958399314629404\n",
      "Epoch 88, Training Loss: 0.796555219019266\n",
      "Epoch 89, Training Loss: 0.7968647123279428\n",
      "Epoch 90, Training Loss: 0.7954615298070405\n",
      "Epoch 91, Training Loss: 0.7957227481935257\n",
      "Epoch 92, Training Loss: 0.795643238913744\n",
      "Epoch 93, Training Loss: 0.7955621151995839\n",
      "Epoch 94, Training Loss: 0.7955266709614517\n",
      "Epoch 95, Training Loss: 0.7957689150831753\n",
      "Epoch 96, Training Loss: 0.7954579862436854\n",
      "Epoch 97, Training Loss: 0.7959376223105237\n",
      "Epoch 98, Training Loss: 0.795438918224851\n",
      "Epoch 99, Training Loss: 0.7952507784492091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:42:34,668] Trial 281 finished with value: 0.6253333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7952885977307657\n",
      "Epoch 1, Training Loss: 1.094989594481045\n",
      "Epoch 2, Training Loss: 1.0910578385331577\n",
      "Epoch 3, Training Loss: 1.0909246351485862\n",
      "Epoch 4, Training Loss: 1.090995344362761\n",
      "Epoch 5, Training Loss: 1.090926616711724\n",
      "Epoch 6, Training Loss: 1.0909675544365904\n",
      "Epoch 7, Training Loss: 1.0908474074270493\n",
      "Epoch 8, Training Loss: 1.0909387776726172\n",
      "Epoch 9, Training Loss: 1.0907749328398166\n",
      "Epoch 10, Training Loss: 1.0906485595201192\n",
      "Epoch 11, Training Loss: 1.0905973482849007\n",
      "Epoch 12, Training Loss: 1.090678095817566\n",
      "Epoch 13, Training Loss: 1.0907402592494075\n",
      "Epoch 14, Training Loss: 1.0905109961230055\n",
      "Epoch 15, Training Loss: 1.0904301005198542\n",
      "Epoch 16, Training Loss: 1.0905550117779494\n",
      "Epoch 17, Training Loss: 1.0902538442970218\n",
      "Epoch 18, Training Loss: 1.0903586027317478\n",
      "Epoch 19, Training Loss: 1.0902920294525031\n",
      "Epoch 20, Training Loss: 1.0902343782267176\n",
      "Epoch 21, Training Loss: 1.090126387696517\n",
      "Epoch 22, Training Loss: 1.0901022902108672\n",
      "Epoch 23, Training Loss: 1.0899894608590837\n",
      "Epoch 24, Training Loss: 1.0901318679178569\n",
      "Epoch 25, Training Loss: 1.0899043061679468\n",
      "Epoch 26, Training Loss: 1.0898784841809954\n",
      "Epoch 27, Training Loss: 1.0897833990871457\n",
      "Epoch 28, Training Loss: 1.089640040146677\n",
      "Epoch 29, Training Loss: 1.089633682437409\n",
      "Epoch 30, Training Loss: 1.0894550981378197\n",
      "Epoch 31, Training Loss: 1.0892649790398161\n",
      "Epoch 32, Training Loss: 1.0892723565711115\n",
      "Epoch 33, Training Loss: 1.0891688827285193\n",
      "Epoch 34, Training Loss: 1.0889035907903113\n",
      "Epoch 35, Training Loss: 1.088767381001236\n",
      "Epoch 36, Training Loss: 1.0886501428776219\n",
      "Epoch 37, Training Loss: 1.0885222675208759\n",
      "Epoch 38, Training Loss: 1.0884659951790832\n",
      "Epoch 39, Training Loss: 1.088093087726966\n",
      "Epoch 40, Training Loss: 1.0880875573122413\n",
      "Epoch 41, Training Loss: 1.0877131775805824\n",
      "Epoch 42, Training Loss: 1.0875991765717814\n",
      "Epoch 43, Training Loss: 1.0872698873505557\n",
      "Epoch 44, Training Loss: 1.0869974684894532\n",
      "Epoch 45, Training Loss: 1.0866595035208795\n",
      "Epoch 46, Training Loss: 1.08634172095392\n",
      "Epoch 47, Training Loss: 1.0858393602801444\n",
      "Epoch 48, Training Loss: 1.0855732984112618\n",
      "Epoch 49, Training Loss: 1.0849203672624173\n",
      "Epoch 50, Training Loss: 1.0845770520375186\n",
      "Epoch 51, Training Loss: 1.0839243867343529\n",
      "Epoch 52, Training Loss: 1.0832689819479346\n",
      "Epoch 53, Training Loss: 1.0823262818773887\n",
      "Epoch 54, Training Loss: 1.0815671852656774\n",
      "Epoch 55, Training Loss: 1.0804327418033342\n",
      "Epoch 56, Training Loss: 1.0790456409741165\n",
      "Epoch 57, Training Loss: 1.0776992053913892\n",
      "Epoch 58, Training Loss: 1.0761699267796108\n",
      "Epoch 59, Training Loss: 1.0740873049972648\n",
      "Epoch 60, Training Loss: 1.071878000668117\n",
      "Epoch 61, Training Loss: 1.0692277307797196\n",
      "Epoch 62, Training Loss: 1.065899914547913\n",
      "Epoch 63, Training Loss: 1.0621307107738982\n",
      "Epoch 64, Training Loss: 1.057631316400112\n",
      "Epoch 65, Training Loss: 1.0523417075773827\n",
      "Epoch 66, Training Loss: 1.04635513608617\n",
      "Epoch 67, Training Loss: 1.0396669291015854\n",
      "Epoch 68, Training Loss: 1.032437873334813\n",
      "Epoch 69, Training Loss: 1.025284851224799\n",
      "Epoch 70, Training Loss: 1.0181734654240142\n",
      "Epoch 71, Training Loss: 1.0117038471358164\n",
      "Epoch 72, Training Loss: 1.0058602989168095\n",
      "Epoch 73, Training Loss: 1.0016470574794856\n",
      "Epoch 74, Training Loss: 0.9972639278361671\n",
      "Epoch 75, Training Loss: 0.9940559267997742\n",
      "Epoch 76, Training Loss: 0.9909616865609822\n",
      "Epoch 77, Training Loss: 0.9881955364592989\n",
      "Epoch 78, Training Loss: 0.9853819606895734\n",
      "Epoch 79, Training Loss: 0.9826827526988839\n",
      "Epoch 80, Training Loss: 0.9800042095937227\n",
      "Epoch 81, Training Loss: 0.9777293026895452\n",
      "Epoch 82, Training Loss: 0.975106718396782\n",
      "Epoch 83, Training Loss: 0.9722173354679481\n",
      "Epoch 84, Training Loss: 0.9700181682307021\n",
      "Epoch 85, Training Loss: 0.9675793011385695\n",
      "Epoch 86, Training Loss: 0.964854573755336\n",
      "Epoch 87, Training Loss: 0.9626359397307375\n",
      "Epoch 88, Training Loss: 0.9610256351026377\n",
      "Epoch 89, Training Loss: 0.9586517539239467\n",
      "Epoch 90, Training Loss: 0.9569408980527319\n",
      "Epoch 91, Training Loss: 0.9557683533295653\n",
      "Epoch 92, Training Loss: 0.9545306831374204\n",
      "Epoch 93, Training Loss: 0.95310548454299\n",
      "Epoch 94, Training Loss: 0.9521245434768217\n",
      "Epoch 95, Training Loss: 0.9508660476906855\n",
      "Epoch 96, Training Loss: 0.9505691523838761\n",
      "Epoch 97, Training Loss: 0.9490694222593666\n",
      "Epoch 98, Training Loss: 0.9484445917875247\n",
      "Epoch 99, Training Loss: 0.9473951544080462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:44:13,993] Trial 282 finished with value: 0.5366666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.9468214709955947\n",
      "Epoch 1, Training Loss: 0.9929024624824524\n",
      "Epoch 2, Training Loss: 0.9237976731272305\n",
      "Epoch 3, Training Loss: 0.8615859822665943\n",
      "Epoch 4, Training Loss: 0.8203097062952378\n",
      "Epoch 5, Training Loss: 0.8137179519849665\n",
      "Epoch 6, Training Loss: 0.811192116316627\n",
      "Epoch 7, Training Loss: 0.8108105000327616\n",
      "Epoch 8, Training Loss: 0.8087936424507814\n",
      "Epoch 9, Training Loss: 0.8084732324936811\n",
      "Epoch 10, Training Loss: 0.8071140482846428\n",
      "Epoch 11, Training Loss: 0.8059547258124632\n",
      "Epoch 12, Training Loss: 0.8056048215136808\n",
      "Epoch 13, Training Loss: 0.8045694407294778\n",
      "Epoch 14, Training Loss: 0.8046793215415057\n",
      "Epoch 15, Training Loss: 0.8036689467289868\n",
      "Epoch 16, Training Loss: 0.8028561265328351\n",
      "Epoch 17, Training Loss: 0.802863160091288\n",
      "Epoch 18, Training Loss: 0.8024437090929817\n",
      "Epoch 19, Training Loss: 0.8015775400049546\n",
      "Epoch 20, Training Loss: 0.8009303088749157\n",
      "Epoch 21, Training Loss: 0.8012639948901008\n",
      "Epoch 22, Training Loss: 0.8013079615200267\n",
      "Epoch 23, Training Loss: 0.8006273412704468\n",
      "Epoch 24, Training Loss: 0.8006000572092393\n",
      "Epoch 25, Training Loss: 0.7997307450631086\n",
      "Epoch 26, Training Loss: 0.8004411056462456\n",
      "Epoch 27, Training Loss: 0.7995049304821912\n",
      "Epoch 28, Training Loss: 0.7994839732085958\n",
      "Epoch 29, Training Loss: 0.7989849324787365\n",
      "Epoch 30, Training Loss: 0.7987740883406471\n",
      "Epoch 31, Training Loss: 0.7987004111093633\n",
      "Epoch 32, Training Loss: 0.7983460074312547\n",
      "Epoch 33, Training Loss: 0.7976954738532795\n",
      "Epoch 34, Training Loss: 0.7979212266557357\n",
      "Epoch 35, Training Loss: 0.7971423700977774\n",
      "Epoch 36, Training Loss: 0.796760280342663\n",
      "Epoch 37, Training Loss: 0.7973323428630829\n",
      "Epoch 38, Training Loss: 0.7969141168454114\n",
      "Epoch 39, Training Loss: 0.7961573844797472\n",
      "Epoch 40, Training Loss: 0.7955278423253228\n",
      "Epoch 41, Training Loss: 0.7954746141854454\n",
      "Epoch 42, Training Loss: 0.7950568308549769\n",
      "Epoch 43, Training Loss: 0.7950644950305714\n",
      "Epoch 44, Training Loss: 0.7941409791217131\n",
      "Epoch 45, Training Loss: 0.7940233158364015\n",
      "Epoch 46, Training Loss: 0.7938378808077644\n",
      "Epoch 47, Training Loss: 0.7934730699483086\n",
      "Epoch 48, Training Loss: 0.7928731107711792\n",
      "Epoch 49, Training Loss: 0.7924944183405708\n",
      "Epoch 50, Training Loss: 0.7921871927906485\n",
      "Epoch 51, Training Loss: 0.7915756056589239\n",
      "Epoch 52, Training Loss: 0.7909745965284459\n",
      "Epoch 53, Training Loss: 0.7908843785874984\n",
      "Epoch 54, Training Loss: 0.7904042070052203\n",
      "Epoch 55, Training Loss: 0.7899036317011889\n",
      "Epoch 56, Training Loss: 0.7903154507805319\n",
      "Epoch 57, Training Loss: 0.7895420903318069\n",
      "Epoch 58, Training Loss: 0.7899195566598107\n",
      "Epoch 59, Training Loss: 0.7895140924173243\n",
      "Epoch 60, Training Loss: 0.7892202362593482\n",
      "Epoch 61, Training Loss: 0.788402280106264\n",
      "Epoch 62, Training Loss: 0.7884216124871198\n",
      "Epoch 63, Training Loss: 0.7880763816132265\n",
      "Epoch 64, Training Loss: 0.7879674949365504\n",
      "Epoch 65, Training Loss: 0.7877920623386607\n",
      "Epoch 66, Training Loss: 0.7877603745460511\n",
      "Epoch 67, Training Loss: 0.7878392162743737\n",
      "Epoch 68, Training Loss: 0.7868010985150057\n",
      "Epoch 69, Training Loss: 0.7869605478819679\n",
      "Epoch 70, Training Loss: 0.7869528368641349\n",
      "Epoch 71, Training Loss: 0.786904075496337\n",
      "Epoch 72, Training Loss: 0.7864799956013174\n",
      "Epoch 73, Training Loss: 0.7860512263634626\n",
      "Epoch 74, Training Loss: 0.7860790442719179\n",
      "Epoch 75, Training Loss: 0.7856618466096766\n",
      "Epoch 76, Training Loss: 0.7855924142809475\n",
      "Epoch 77, Training Loss: 0.7854198284710154\n",
      "Epoch 78, Training Loss: 0.7851974718710956\n",
      "Epoch 79, Training Loss: 0.7845865309238433\n",
      "Epoch 80, Training Loss: 0.7850096452236176\n",
      "Epoch 81, Training Loss: 0.7850961821920731\n",
      "Epoch 82, Training Loss: 0.7846772437235888\n",
      "Epoch 83, Training Loss: 0.7848776788571301\n",
      "Epoch 84, Training Loss: 0.784274742603302\n",
      "Epoch 85, Training Loss: 0.784437234471826\n",
      "Epoch 86, Training Loss: 0.7840173146303963\n",
      "Epoch 87, Training Loss: 0.7845127320990843\n",
      "Epoch 88, Training Loss: 0.7841524004234988\n",
      "Epoch 89, Training Loss: 0.7844726328990038\n",
      "Epoch 90, Training Loss: 0.7840002383204068\n",
      "Epoch 91, Training Loss: 0.7837146998153013\n",
      "Epoch 92, Training Loss: 0.7833825882743387\n",
      "Epoch 93, Training Loss: 0.7837045257933\n",
      "Epoch 94, Training Loss: 0.7837317585945129\n",
      "Epoch 95, Training Loss: 0.7834531082125271\n",
      "Epoch 96, Training Loss: 0.7835968644478741\n",
      "Epoch 97, Training Loss: 0.7834501074342166\n",
      "Epoch 98, Training Loss: 0.7831954888736501\n",
      "Epoch 99, Training Loss: 0.7830275741745444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:46:44,293] Trial 283 finished with value: 0.6386666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7830450472410987\n",
      "Epoch 1, Training Loss: 0.9780911215265891\n",
      "Epoch 2, Training Loss: 0.9414150264030112\n",
      "Epoch 3, Training Loss: 0.9117935111648158\n",
      "Epoch 4, Training Loss: 0.8702673906670477\n",
      "Epoch 5, Training Loss: 0.8361383896124991\n",
      "Epoch 6, Training Loss: 0.82072062097994\n",
      "Epoch 7, Training Loss: 0.8158371652875628\n",
      "Epoch 8, Training Loss: 0.8137319466225187\n",
      "Epoch 9, Training Loss: 0.811996052587839\n",
      "Epoch 10, Training Loss: 0.811050956679466\n",
      "Epoch 11, Training Loss: 0.8104240058059979\n",
      "Epoch 12, Training Loss: 0.809360939129851\n",
      "Epoch 13, Training Loss: 0.8083648534645712\n",
      "Epoch 14, Training Loss: 0.8073604885796856\n",
      "Epoch 15, Training Loss: 0.8065084048679897\n",
      "Epoch 16, Training Loss: 0.8066215689020946\n",
      "Epoch 17, Training Loss: 0.8064405360616239\n",
      "Epoch 18, Training Loss: 0.8049897845526387\n",
      "Epoch 19, Training Loss: 0.8054502055160981\n",
      "Epoch 20, Training Loss: 0.8042409640505798\n",
      "Epoch 21, Training Loss: 0.8037166398270685\n",
      "Epoch 22, Training Loss: 0.803422066322843\n",
      "Epoch 23, Training Loss: 0.8032130159829792\n",
      "Epoch 24, Training Loss: 0.8034074941075834\n",
      "Epoch 25, Training Loss: 0.8031028588911644\n",
      "Epoch 26, Training Loss: 0.8027259079137243\n",
      "Epoch 27, Training Loss: 0.8020263126021937\n",
      "Epoch 28, Training Loss: 0.8022905093386657\n",
      "Epoch 29, Training Loss: 0.8020353962604264\n",
      "Epoch 30, Training Loss: 0.8017929823775041\n",
      "Epoch 31, Training Loss: 0.801975066769392\n",
      "Epoch 32, Training Loss: 0.8006441372677796\n",
      "Epoch 33, Training Loss: 0.8008170981156199\n",
      "Epoch 34, Training Loss: 0.8011160958978466\n",
      "Epoch 35, Training Loss: 0.8011171324808795\n",
      "Epoch 36, Training Loss: 0.8012535114933673\n",
      "Epoch 37, Training Loss: 0.8010322377197725\n",
      "Epoch 38, Training Loss: 0.8008357387736328\n",
      "Epoch 39, Training Loss: 0.800862125077642\n",
      "Epoch 40, Training Loss: 0.8007624686212468\n",
      "Epoch 41, Training Loss: 0.800470319098996\n",
      "Epoch 42, Training Loss: 0.799888863509759\n",
      "Epoch 43, Training Loss: 0.7996927551757124\n",
      "Epoch 44, Training Loss: 0.8004151252875651\n",
      "Epoch 45, Training Loss: 0.7998499446345452\n",
      "Epoch 46, Training Loss: 0.7997389769195614\n",
      "Epoch 47, Training Loss: 0.7992011725902557\n",
      "Epoch 48, Training Loss: 0.7989919249276469\n",
      "Epoch 49, Training Loss: 0.7997898688889984\n",
      "Epoch 50, Training Loss: 0.7991679623610991\n",
      "Epoch 51, Training Loss: 0.7989941605051657\n",
      "Epoch 52, Training Loss: 0.7989950822708302\n",
      "Epoch 53, Training Loss: 0.798816480493187\n",
      "Epoch 54, Training Loss: 0.7986142968772946\n",
      "Epoch 55, Training Loss: 0.7991061334323166\n",
      "Epoch 56, Training Loss: 0.799270863192422\n",
      "Epoch 57, Training Loss: 0.7985713771411351\n",
      "Epoch 58, Training Loss: 0.7979641936775437\n",
      "Epoch 59, Training Loss: 0.7987163542804862\n",
      "Epoch 60, Training Loss: 0.7988862514495849\n",
      "Epoch 61, Training Loss: 0.7979880946919434\n",
      "Epoch 62, Training Loss: 0.7980299289961507\n",
      "Epoch 63, Training Loss: 0.79777081541549\n",
      "Epoch 64, Training Loss: 0.7977677395469264\n",
      "Epoch 65, Training Loss: 0.7979976601170418\n",
      "Epoch 66, Training Loss: 0.7977046810594717\n",
      "Epoch 67, Training Loss: 0.7976433217077327\n",
      "Epoch 68, Training Loss: 0.7975980567752867\n",
      "Epoch 69, Training Loss: 0.7979273113989293\n",
      "Epoch 70, Training Loss: 0.7977707238125622\n",
      "Epoch 71, Training Loss: 0.7972922889809859\n",
      "Epoch 72, Training Loss: 0.7967507242260122\n",
      "Epoch 73, Training Loss: 0.7974975379786097\n",
      "Epoch 74, Training Loss: 0.7967845473970686\n",
      "Epoch 75, Training Loss: 0.7971779066817205\n",
      "Epoch 76, Training Loss: 0.7964419871344602\n",
      "Epoch 77, Training Loss: 0.797253558062073\n",
      "Epoch 78, Training Loss: 0.7964269128957189\n",
      "Epoch 79, Training Loss: 0.7961299517100915\n",
      "Epoch 80, Training Loss: 0.7960553499092733\n",
      "Epoch 81, Training Loss: 0.7965409684001952\n",
      "Epoch 82, Training Loss: 0.7963218852989655\n",
      "Epoch 83, Training Loss: 0.7962246966541262\n",
      "Epoch 84, Training Loss: 0.7956201616086458\n",
      "Epoch 85, Training Loss: 0.7966344971405832\n",
      "Epoch 86, Training Loss: 0.7954271648163186\n",
      "Epoch 87, Training Loss: 0.7952989160566402\n",
      "Epoch 88, Training Loss: 0.7955441402313405\n",
      "Epoch 89, Training Loss: 0.7950918817878666\n",
      "Epoch 90, Training Loss: 0.7953119840837063\n",
      "Epoch 91, Training Loss: 0.7952796139215168\n",
      "Epoch 92, Training Loss: 0.7951991482784874\n",
      "Epoch 93, Training Loss: 0.7947617150787124\n",
      "Epoch 94, Training Loss: 0.7952101072870699\n",
      "Epoch 95, Training Loss: 0.7946717274816413\n",
      "Epoch 96, Training Loss: 0.7944362306953373\n",
      "Epoch 97, Training Loss: 0.7954553733194681\n",
      "Epoch 98, Training Loss: 0.7948845287014668\n",
      "Epoch 99, Training Loss: 0.79431432790326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:48:37,162] Trial 284 finished with value: 0.6291333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7938841399393584\n",
      "Epoch 1, Training Loss: 0.9890377082322773\n",
      "Epoch 2, Training Loss: 0.9446633595273011\n",
      "Epoch 3, Training Loss: 0.926309553573006\n",
      "Epoch 4, Training Loss: 0.9038093209266662\n",
      "Epoch 5, Training Loss: 0.8790370810300784\n",
      "Epoch 6, Training Loss: 0.8567784874959099\n",
      "Epoch 7, Training Loss: 0.840092260676219\n",
      "Epoch 8, Training Loss: 0.829493480517452\n",
      "Epoch 9, Training Loss: 0.8220467348744098\n",
      "Epoch 10, Training Loss: 0.8178599971577637\n",
      "Epoch 11, Training Loss: 0.8150653036913478\n",
      "Epoch 12, Training Loss: 0.8130187953325142\n",
      "Epoch 13, Training Loss: 0.8124714002573401\n",
      "Epoch 14, Training Loss: 0.8112110605813507\n",
      "Epoch 15, Training Loss: 0.8104007195709343\n",
      "Epoch 16, Training Loss: 0.8106010997205748\n",
      "Epoch 17, Training Loss: 0.8094181083198777\n",
      "Epoch 18, Training Loss: 0.808710000927287\n",
      "Epoch 19, Training Loss: 0.8089264056736365\n",
      "Epoch 20, Training Loss: 0.8085523210970083\n",
      "Epoch 21, Training Loss: 0.8086589302335466\n",
      "Epoch 22, Training Loss: 0.8085181307075615\n",
      "Epoch 23, Training Loss: 0.8072573627296247\n",
      "Epoch 24, Training Loss: 0.8074497244411841\n",
      "Epoch 25, Training Loss: 0.8071156765285291\n",
      "Epoch 26, Training Loss: 0.8071454530371759\n",
      "Epoch 27, Training Loss: 0.8073893392892708\n",
      "Epoch 28, Training Loss: 0.8063445596766651\n",
      "Epoch 29, Training Loss: 0.8071023005291932\n",
      "Epoch 30, Training Loss: 0.8063651516921538\n",
      "Epoch 31, Training Loss: 0.8065995409076375\n",
      "Epoch 32, Training Loss: 0.8064853013906264\n",
      "Epoch 33, Training Loss: 0.8055817364750052\n",
      "Epoch 34, Training Loss: 0.8056395421350809\n",
      "Epoch 35, Training Loss: 0.805463635383692\n",
      "Epoch 36, Training Loss: 0.8052353654588972\n",
      "Epoch 37, Training Loss: 0.8050055561209084\n",
      "Epoch 38, Training Loss: 0.8051448082565364\n",
      "Epoch 39, Training Loss: 0.8042572429305629\n",
      "Epoch 40, Training Loss: 0.8047515143129162\n",
      "Epoch 41, Training Loss: 0.8043445481393571\n",
      "Epoch 42, Training Loss: 0.8045345485658574\n",
      "Epoch 43, Training Loss: 0.8041834497810306\n",
      "Epoch 44, Training Loss: 0.8042355056992151\n",
      "Epoch 45, Training Loss: 0.8046139870371137\n",
      "Epoch 46, Training Loss: 0.803576726303961\n",
      "Epoch 47, Training Loss: 0.803928572253177\n",
      "Epoch 48, Training Loss: 0.8037590472321762\n",
      "Epoch 49, Training Loss: 0.803998935850043\n",
      "Epoch 50, Training Loss: 0.8035582021663064\n",
      "Epoch 51, Training Loss: 0.8033456824775925\n",
      "Epoch 52, Training Loss: 0.8030429695781909\n",
      "Epoch 53, Training Loss: 0.8038515826813261\n",
      "Epoch 54, Training Loss: 0.803512964571329\n",
      "Epoch 55, Training Loss: 0.802690199174379\n",
      "Epoch 56, Training Loss: 0.8030256627197553\n",
      "Epoch 57, Training Loss: 0.8028789565975505\n",
      "Epoch 58, Training Loss: 0.8023089528980112\n",
      "Epoch 59, Training Loss: 0.8030495349625896\n",
      "Epoch 60, Training Loss: 0.8025538727753144\n",
      "Epoch 61, Training Loss: 0.8016467593218151\n",
      "Epoch 62, Training Loss: 0.8016774341576082\n",
      "Epoch 63, Training Loss: 0.8020234831293723\n",
      "Epoch 64, Training Loss: 0.8019658206997061\n",
      "Epoch 65, Training Loss: 0.8013666137716824\n",
      "Epoch 66, Training Loss: 0.8020429505441422\n",
      "Epoch 67, Training Loss: 0.8016294146838941\n",
      "Epoch 68, Training Loss: 0.8011845326961431\n",
      "Epoch 69, Training Loss: 0.8008247489319709\n",
      "Epoch 70, Training Loss: 0.8006548646697424\n",
      "Epoch 71, Training Loss: 0.8011293014189355\n",
      "Epoch 72, Training Loss: 0.8005260520411613\n",
      "Epoch 73, Training Loss: 0.8002971113176274\n",
      "Epoch 74, Training Loss: 0.8006760990709291\n",
      "Epoch 75, Training Loss: 0.8005940483028727\n",
      "Epoch 76, Training Loss: 0.800657756256878\n",
      "Epoch 77, Training Loss: 0.8000196079562482\n",
      "Epoch 78, Training Loss: 0.8002553281927467\n",
      "Epoch 79, Training Loss: 0.8001990874907128\n",
      "Epoch 80, Training Loss: 0.800099329930499\n",
      "Epoch 81, Training Loss: 0.8000113376101157\n",
      "Epoch 82, Training Loss: 0.8004572709700218\n",
      "Epoch 83, Training Loss: 0.7999440196761511\n",
      "Epoch 84, Training Loss: 0.8000694495394713\n",
      "Epoch 85, Training Loss: 0.7994607945133869\n",
      "Epoch 86, Training Loss: 0.7998676857553927\n",
      "Epoch 87, Training Loss: 0.8000339434559184\n",
      "Epoch 88, Training Loss: 0.8005494564099419\n",
      "Epoch 89, Training Loss: 0.7994831579968446\n",
      "Epoch 90, Training Loss: 0.7995243431930256\n",
      "Epoch 91, Training Loss: 0.800026708258722\n",
      "Epoch 92, Training Loss: 0.799493120039316\n",
      "Epoch 93, Training Loss: 0.7996606845604746\n",
      "Epoch 94, Training Loss: 0.798926736179151\n",
      "Epoch 95, Training Loss: 0.7996210387774876\n",
      "Epoch 96, Training Loss: 0.7995403028968582\n",
      "Epoch 97, Training Loss: 0.7994261989019867\n",
      "Epoch 98, Training Loss: 0.7991226232141481\n",
      "Epoch 99, Training Loss: 0.7988236058027225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:50:15,295] Trial 285 finished with value: 0.6339333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7985248445568228\n",
      "Epoch 1, Training Loss: 0.8930835203563465\n",
      "Epoch 2, Training Loss: 0.8276421363914714\n",
      "Epoch 3, Training Loss: 0.8195674943923951\n",
      "Epoch 4, Training Loss: 0.8152773855714237\n",
      "Epoch 5, Training Loss: 0.8134623617284438\n",
      "Epoch 6, Training Loss: 0.8115946636480443\n",
      "Epoch 7, Training Loss: 0.8103466697300181\n",
      "Epoch 8, Training Loss: 0.8092778559993296\n",
      "Epoch 9, Training Loss: 0.8082238840355592\n",
      "Epoch 10, Training Loss: 0.8074131459348342\n",
      "Epoch 11, Training Loss: 0.8065518253691056\n",
      "Epoch 12, Training Loss: 0.806186761154848\n",
      "Epoch 13, Training Loss: 0.8056889622351703\n",
      "Epoch 14, Training Loss: 0.8056012652902043\n",
      "Epoch 15, Training Loss: 0.8053023416855756\n",
      "Epoch 16, Training Loss: 0.8048803945849924\n",
      "Epoch 17, Training Loss: 0.8042823847602395\n",
      "Epoch 18, Training Loss: 0.8037415070393507\n",
      "Epoch 19, Training Loss: 0.8032475301097421\n",
      "Epoch 20, Training Loss: 0.8031696737513823\n",
      "Epoch 21, Training Loss: 0.8032787574740017\n",
      "Epoch 22, Training Loss: 0.8026870130791384\n",
      "Epoch 23, Training Loss: 0.8020855650481056\n",
      "Epoch 24, Training Loss: 0.8018681692375856\n",
      "Epoch 25, Training Loss: 0.8012684103320626\n",
      "Epoch 26, Training Loss: 0.8010969761539908\n",
      "Epoch 27, Training Loss: 0.8007128737253302\n",
      "Epoch 28, Training Loss: 0.8003296433476841\n",
      "Epoch 29, Training Loss: 0.8002265578858992\n",
      "Epoch 30, Training Loss: 0.7999330080957974\n",
      "Epoch 31, Training Loss: 0.7997367296499365\n",
      "Epoch 32, Training Loss: 0.7992639687482048\n",
      "Epoch 33, Training Loss: 0.7984992743940914\n",
      "Epoch 34, Training Loss: 0.7981401860713959\n",
      "Epoch 35, Training Loss: 0.798315825953203\n",
      "Epoch 36, Training Loss: 0.7982410119561588\n",
      "Epoch 37, Training Loss: 0.7982870474282433\n",
      "Epoch 38, Training Loss: 0.7978836030819837\n",
      "Epoch 39, Training Loss: 0.7975613526035757\n",
      "Epoch 40, Training Loss: 0.7978383616840138\n",
      "Epoch 41, Training Loss: 0.7971303022609038\n",
      "Epoch 42, Training Loss: 0.796806438109454\n",
      "Epoch 43, Training Loss: 0.7970425687116736\n",
      "Epoch 44, Training Loss: 0.7967962701180402\n",
      "Epoch 45, Training Loss: 0.7966354039136101\n",
      "Epoch 46, Training Loss: 0.7961279930086697\n",
      "Epoch 47, Training Loss: 0.7955268729434294\n",
      "Epoch 48, Training Loss: 0.7955864350935992\n",
      "Epoch 49, Training Loss: 0.7959060608639437\n",
      "Epoch 50, Training Loss: 0.795108789696413\n",
      "Epoch 51, Training Loss: 0.7955766977983363\n",
      "Epoch 52, Training Loss: 0.7951160223343793\n",
      "Epoch 53, Training Loss: 0.7947345803064458\n",
      "Epoch 54, Training Loss: 0.7950796486349667\n",
      "Epoch 55, Training Loss: 0.7945084605497472\n",
      "Epoch 56, Training Loss: 0.7942825264790478\n",
      "Epoch 57, Training Loss: 0.7946452745269327\n",
      "Epoch 58, Training Loss: 0.794358791884254\n",
      "Epoch 59, Training Loss: 0.7939562438516056\n",
      "Epoch 60, Training Loss: 0.7939698734002955\n",
      "Epoch 61, Training Loss: 0.7939525070611169\n",
      "Epoch 62, Training Loss: 0.7943355354140786\n",
      "Epoch 63, Training Loss: 0.7934728384017944\n",
      "Epoch 64, Training Loss: 0.793690422002007\n",
      "Epoch 65, Training Loss: 0.7930892701709972\n",
      "Epoch 66, Training Loss: 0.7928839105718276\n",
      "Epoch 67, Training Loss: 0.7931964738228742\n",
      "Epoch 68, Training Loss: 0.7928629652191611\n",
      "Epoch 69, Training Loss: 0.7930469755565419\n",
      "Epoch 70, Training Loss: 0.7929507173510159\n",
      "Epoch 71, Training Loss: 0.7933350035723518\n",
      "Epoch 72, Training Loss: 0.7927463087614844\n",
      "Epoch 73, Training Loss: 0.7926863173176261\n",
      "Epoch 74, Training Loss: 0.793182725205141\n",
      "Epoch 75, Training Loss: 0.7925543665885926\n",
      "Epoch 76, Training Loss: 0.7924655608569875\n",
      "Epoch 77, Training Loss: 0.7929763625649845\n",
      "Epoch 78, Training Loss: 0.7927878448542427\n",
      "Epoch 79, Training Loss: 0.7922600488101735\n",
      "Epoch 80, Training Loss: 0.7924912278091206\n",
      "Epoch 81, Training Loss: 0.7928700623091529\n",
      "Epoch 82, Training Loss: 0.7923987538674299\n",
      "Epoch 83, Training Loss: 0.7922693005028894\n",
      "Epoch 84, Training Loss: 0.7921311604275423\n",
      "Epoch 85, Training Loss: 0.7918949030427371\n",
      "Epoch 86, Training Loss: 0.7922002861780278\n",
      "Epoch 87, Training Loss: 0.7922719579584458\n",
      "Epoch 88, Training Loss: 0.7917895805835724\n",
      "Epoch 89, Training Loss: 0.7918725347518921\n",
      "Epoch 90, Training Loss: 0.7920679734033697\n",
      "Epoch 91, Training Loss: 0.7916804364849539\n",
      "Epoch 92, Training Loss: 0.791841874964097\n",
      "Epoch 93, Training Loss: 0.7916331160068512\n",
      "Epoch 94, Training Loss: 0.7914932458540973\n",
      "Epoch 95, Training Loss: 0.7917770878707662\n",
      "Epoch 96, Training Loss: 0.7919013063346638\n",
      "Epoch 97, Training Loss: 0.7914163233953364\n",
      "Epoch 98, Training Loss: 0.7916155998847064\n",
      "Epoch 99, Training Loss: 0.7914752365561093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:52:06,001] Trial 286 finished with value: 0.6387333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7910956089636859\n",
      "Epoch 1, Training Loss: 0.8628206419243533\n",
      "Epoch 2, Training Loss: 0.8252178926327649\n",
      "Epoch 3, Training Loss: 0.8205516390239491\n",
      "Epoch 4, Training Loss: 0.8169038058028502\n",
      "Epoch 5, Training Loss: 0.8144158801611732\n",
      "Epoch 6, Training Loss: 0.8129762789081124\n",
      "Epoch 7, Training Loss: 0.8115743047349593\n",
      "Epoch 8, Training Loss: 0.8106706136114457\n",
      "Epoch 9, Training Loss: 0.8096686900363249\n",
      "Epoch 10, Training Loss: 0.8084022634169634\n",
      "Epoch 11, Training Loss: 0.8078900811952703\n",
      "Epoch 12, Training Loss: 0.8072694643806009\n",
      "Epoch 13, Training Loss: 0.8063105122482075\n",
      "Epoch 14, Training Loss: 0.8058342970118804\n",
      "Epoch 15, Training Loss: 0.8058633720173555\n",
      "Epoch 16, Training Loss: 0.805614663993611\n",
      "Epoch 17, Training Loss: 0.8050369529864367\n",
      "Epoch 18, Training Loss: 0.8051352598386652\n",
      "Epoch 19, Training Loss: 0.8045712952754077\n",
      "Epoch 20, Training Loss: 0.8047811011005851\n",
      "Epoch 21, Training Loss: 0.8041672832124374\n",
      "Epoch 22, Training Loss: 0.8040681108306436\n",
      "Epoch 23, Training Loss: 0.803216555258807\n",
      "Epoch 24, Training Loss: 0.803676750870312\n",
      "Epoch 25, Training Loss: 0.8038104511709774\n",
      "Epoch 26, Training Loss: 0.8035570538745207\n",
      "Epoch 27, Training Loss: 0.803354049430174\n",
      "Epoch 28, Training Loss: 0.8028712595911587\n",
      "Epoch 29, Training Loss: 0.8023836425472708\n",
      "Epoch 30, Training Loss: 0.8017631333715776\n",
      "Epoch 31, Training Loss: 0.802614373179043\n",
      "Epoch 32, Training Loss: 0.8023643630392411\n",
      "Epoch 33, Training Loss: 0.8019484092908747\n",
      "Epoch 34, Training Loss: 0.8010588268672719\n",
      "Epoch 35, Training Loss: 0.8023072804422939\n",
      "Epoch 36, Training Loss: 0.8015105971168069\n",
      "Epoch 37, Training Loss: 0.8018158059961655\n",
      "Epoch 38, Training Loss: 0.8020693220811732\n",
      "Epoch 39, Training Loss: 0.8016518143345328\n",
      "Epoch 40, Training Loss: 0.8012108166077557\n",
      "Epoch 41, Training Loss: 0.8016210976067711\n",
      "Epoch 42, Training Loss: 0.8009412465376012\n",
      "Epoch 43, Training Loss: 0.8017982212234945\n",
      "Epoch 44, Training Loss: 0.8005734093750224\n",
      "Epoch 45, Training Loss: 0.8017905397976146\n",
      "Epoch 46, Training Loss: 0.801608399293002\n",
      "Epoch 47, Training Loss: 0.8015435438997606\n",
      "Epoch 48, Training Loss: 0.8004084187395433\n",
      "Epoch 49, Training Loss: 0.8006463571857003\n",
      "Epoch 50, Training Loss: 0.8012311863198\n",
      "Epoch 51, Training Loss: 0.8013308913567487\n",
      "Epoch 52, Training Loss: 0.8013454928818871\n",
      "Epoch 53, Training Loss: 0.8011707188101376\n",
      "Epoch 54, Training Loss: 0.8007465248949388\n",
      "Epoch 55, Training Loss: 0.8007516945109648\n",
      "Epoch 56, Training Loss: 0.8010110228201922\n",
      "Epoch 57, Training Loss: 0.8005113206891452\n",
      "Epoch 58, Training Loss: 0.8007237274506512\n",
      "Epoch 59, Training Loss: 0.8008415073506973\n",
      "Epoch 60, Training Loss: 0.8011310373334324\n",
      "Epoch 61, Training Loss: 0.8011014692923601\n",
      "Epoch 62, Training Loss: 0.8000931108699125\n",
      "Epoch 63, Training Loss: 0.8008547877564149\n",
      "Epoch 64, Training Loss: 0.8006361868802239\n",
      "Epoch 65, Training Loss: 0.8003550146607792\n",
      "Epoch 66, Training Loss: 0.8004559051990509\n",
      "Epoch 67, Training Loss: 0.8008138641189126\n",
      "Epoch 68, Training Loss: 0.8002961734463186\n",
      "Epoch 69, Training Loss: 0.7997613124286427\n",
      "Epoch 70, Training Loss: 0.8000003371519201\n",
      "Epoch 71, Training Loss: 0.8001828648763545\n",
      "Epoch 72, Training Loss: 0.8005044091449064\n",
      "Epoch 73, Training Loss: 0.8001338928587296\n",
      "Epoch 74, Training Loss: 0.8005244079758139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:53:29,600] Trial 287 finished with value: 0.6358666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8004303767400629\n",
      "Epoch 1, Training Loss: 0.8931014697354539\n",
      "Epoch 2, Training Loss: 0.8152574986443484\n",
      "Epoch 3, Training Loss: 0.8093830116709372\n",
      "Epoch 4, Training Loss: 0.8067976678224434\n",
      "Epoch 5, Training Loss: 0.8051406201563384\n",
      "Epoch 6, Training Loss: 0.8001089727968201\n",
      "Epoch 7, Training Loss: 0.796007187742936\n",
      "Epoch 8, Training Loss: 0.7952939803438975\n",
      "Epoch 9, Training Loss: 0.7950472804836761\n",
      "Epoch 10, Training Loss: 0.7944411462410949\n",
      "Epoch 11, Training Loss: 0.7936052564391516\n",
      "Epoch 12, Training Loss: 0.7903643419868067\n",
      "Epoch 13, Training Loss: 0.7914993793444526\n",
      "Epoch 14, Training Loss: 0.7902465883950541\n",
      "Epoch 15, Training Loss: 0.7901536648434804\n",
      "Epoch 16, Training Loss: 0.7901794905053046\n",
      "Epoch 17, Training Loss: 0.7889659658410495\n",
      "Epoch 18, Training Loss: 0.7886152583853643\n",
      "Epoch 19, Training Loss: 0.7905600839987733\n",
      "Epoch 20, Training Loss: 0.7891708678768989\n",
      "Epoch 21, Training Loss: 0.7888040910986133\n",
      "Epoch 22, Training Loss: 0.7880657686326736\n",
      "Epoch 23, Training Loss: 0.7887865358725527\n",
      "Epoch 24, Training Loss: 0.7866713141140185\n",
      "Epoch 25, Training Loss: 0.7864556585039412\n",
      "Epoch 26, Training Loss: 0.7873300673370075\n",
      "Epoch 27, Training Loss: 0.7851674380159019\n",
      "Epoch 28, Training Loss: 0.787223666413386\n",
      "Epoch 29, Training Loss: 0.785005391719646\n",
      "Epoch 30, Training Loss: 0.7860147171450736\n",
      "Epoch 31, Training Loss: 0.7859815980258741\n",
      "Epoch 32, Training Loss: 0.7849664352890244\n",
      "Epoch 33, Training Loss: 0.7857381240765852\n",
      "Epoch 34, Training Loss: 0.7860103963012982\n",
      "Epoch 35, Training Loss: 0.7849840926496606\n",
      "Epoch 36, Training Loss: 0.7843377031778035\n",
      "Epoch 37, Training Loss: 0.783359328069185\n",
      "Epoch 38, Training Loss: 0.7849004317047005\n",
      "Epoch 39, Training Loss: 0.784454864039457\n",
      "Epoch 40, Training Loss: 0.784281119009606\n",
      "Epoch 41, Training Loss: 0.7847450414098295\n",
      "Epoch 42, Training Loss: 0.7839748491918234\n",
      "Epoch 43, Training Loss: 0.7841932931340727\n",
      "Epoch 44, Training Loss: 0.7822397399665718\n",
      "Epoch 45, Training Loss: 0.7830064650765038\n",
      "Epoch 46, Training Loss: 0.7826114319320908\n",
      "Epoch 47, Training Loss: 0.7828830381085102\n",
      "Epoch 48, Training Loss: 0.7820945472645581\n",
      "Epoch 49, Training Loss: 0.7814366285065959\n",
      "Epoch 50, Training Loss: 0.7808699109052357\n",
      "Epoch 51, Training Loss: 0.7821557000167387\n",
      "Epoch 52, Training Loss: 0.7815485464899163\n",
      "Epoch 53, Training Loss: 0.7808308050148469\n",
      "Epoch 54, Training Loss: 0.7804860741572273\n",
      "Epoch 55, Training Loss: 0.7808366277164086\n",
      "Epoch 56, Training Loss: 0.7809602067882854\n",
      "Epoch 57, Training Loss: 0.7797945405307569\n",
      "Epoch 58, Training Loss: 0.7812395128092371\n",
      "Epoch 59, Training Loss: 0.7806452695588421\n",
      "Epoch 60, Training Loss: 0.7787276415896595\n",
      "Epoch 61, Training Loss: 0.7795389904115433\n",
      "Epoch 62, Training Loss: 0.7798165531983053\n",
      "Epoch 63, Training Loss: 0.7790510911690561\n",
      "Epoch 64, Training Loss: 0.7779138403055363\n",
      "Epoch 65, Training Loss: 0.7788076900001756\n",
      "Epoch 66, Training Loss: 0.7798284877511792\n",
      "Epoch 67, Training Loss: 0.7786910046312145\n",
      "Epoch 68, Training Loss: 0.7777175128011775\n",
      "Epoch 69, Training Loss: 0.7772510499882519\n",
      "Epoch 70, Training Loss: 0.7782094053756026\n",
      "Epoch 71, Training Loss: 0.7782244517390889\n",
      "Epoch 72, Training Loss: 0.7768227051075239\n",
      "Epoch 73, Training Loss: 0.778048606922752\n",
      "Epoch 74, Training Loss: 0.7776440388277958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:55:09,599] Trial 288 finished with value: 0.6353333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7775952795394381\n",
      "Epoch 1, Training Loss: 0.8815629871452556\n",
      "Epoch 2, Training Loss: 0.8361275449921103\n",
      "Epoch 3, Training Loss: 0.8317149835474351\n",
      "Epoch 4, Training Loss: 0.8241517465956071\n",
      "Epoch 5, Training Loss: 0.8251362621082979\n",
      "Epoch 6, Training Loss: 0.8220425990048577\n",
      "Epoch 7, Training Loss: 0.8220341150199666\n",
      "Epoch 8, Training Loss: 0.8223588322190677\n",
      "Epoch 9, Training Loss: 0.8194868854915395\n",
      "Epoch 10, Training Loss: 0.8190887852977304\n",
      "Epoch 11, Training Loss: 0.8190478177631603\n",
      "Epoch 12, Training Loss: 0.8181401965898626\n",
      "Epoch 13, Training Loss: 0.8180099108639886\n",
      "Epoch 14, Training Loss: 0.8174356702496024\n",
      "Epoch 15, Training Loss: 0.82057160728118\n",
      "Epoch 16, Training Loss: 0.8157047430907979\n",
      "Epoch 17, Training Loss: 0.8178617295798133\n",
      "Epoch 18, Training Loss: 0.8197899353504181\n",
      "Epoch 19, Training Loss: 0.8173911632509793\n",
      "Epoch 20, Training Loss: 0.8170083064191481\n",
      "Epoch 21, Training Loss: 0.818602999939638\n",
      "Epoch 22, Training Loss: 0.818751061593785\n",
      "Epoch 23, Training Loss: 0.8145289477180032\n",
      "Epoch 24, Training Loss: 0.8149504189631518\n",
      "Epoch 25, Training Loss: 0.8159281253113466\n",
      "Epoch 26, Training Loss: 0.8143169360301074\n",
      "Epoch 27, Training Loss: 0.8167077966998605\n",
      "Epoch 28, Training Loss: 0.8148984110355377\n",
      "Epoch 29, Training Loss: 0.815595454748939\n",
      "Epoch 30, Training Loss: 0.8160658276081085\n",
      "Epoch 31, Training Loss: 0.8138384988728692\n",
      "Epoch 32, Training Loss: 0.8128471376615413\n",
      "Epoch 33, Training Loss: 0.8141794708195854\n",
      "Epoch 34, Training Loss: 0.8141801228943993\n",
      "Epoch 35, Training Loss: 0.8142410783206715\n",
      "Epoch 36, Training Loss: 0.8131177696059732\n",
      "Epoch 37, Training Loss: 0.8111929512725157\n",
      "Epoch 38, Training Loss: 0.8147543731857748\n",
      "Epoch 39, Training Loss: 0.8123032984312842\n",
      "Epoch 40, Training Loss: 0.8130619795182172\n",
      "Epoch 41, Training Loss: 0.810411794536254\n",
      "Epoch 42, Training Loss: 0.8116105726185967\n",
      "Epoch 43, Training Loss: 0.8134022588589612\n",
      "Epoch 44, Training Loss: 0.8122319952179404\n",
      "Epoch 45, Training Loss: 0.8117204026614918\n",
      "Epoch 46, Training Loss: 0.8130172436377582\n",
      "Epoch 47, Training Loss: 0.811838190906188\n",
      "Epoch 48, Training Loss: 0.8107036636156194\n",
      "Epoch 49, Training Loss: 0.8101938118654138\n",
      "Epoch 50, Training Loss: 0.8112672422913944\n",
      "Epoch 51, Training Loss: 0.8119577833484201\n",
      "Epoch 52, Training Loss: 0.8104317463145536\n",
      "Epoch 53, Training Loss: 0.8109282213800094\n",
      "Epoch 54, Training Loss: 0.8102295759846182\n",
      "Epoch 55, Training Loss: 0.8105580479958479\n",
      "Epoch 56, Training Loss: 0.8114925437815049\n",
      "Epoch 57, Training Loss: 0.8119800530461704\n",
      "Epoch 58, Training Loss: 0.8103832460150999\n",
      "Epoch 59, Training Loss: 0.8112001797732185\n",
      "Epoch 60, Training Loss: 0.8099710148923537\n",
      "Epoch 61, Training Loss: 0.8083067469035878\n",
      "Epoch 62, Training Loss: 0.8092251444564146\n",
      "Epoch 63, Training Loss: 0.8110553314405329\n",
      "Epoch 64, Training Loss: 0.8088817508781657\n",
      "Epoch 65, Training Loss: 0.8107323108701145\n",
      "Epoch 66, Training Loss: 0.8109339319958406\n",
      "Epoch 67, Training Loss: 0.8080877115445978\n",
      "Epoch 68, Training Loss: 0.8091622048967024\n",
      "Epoch 69, Training Loss: 0.8080046479141011\n",
      "Epoch 70, Training Loss: 0.8093294502005858\n",
      "Epoch 71, Training Loss: 0.808813282882466\n",
      "Epoch 72, Training Loss: 0.8091174243478214\n",
      "Epoch 73, Training Loss: 0.8074856660646551\n",
      "Epoch 74, Training Loss: 0.8101507138504701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:56:34,448] Trial 289 finished with value: 0.6286 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8077156171377967\n",
      "Epoch 1, Training Loss: 0.8519626707890454\n",
      "Epoch 2, Training Loss: 0.8183138430819792\n",
      "Epoch 3, Training Loss: 0.8107747643134173\n",
      "Epoch 4, Training Loss: 0.8043252951257369\n",
      "Epoch 5, Training Loss: 0.7994597644665662\n",
      "Epoch 6, Training Loss: 0.7960611325853011\n",
      "Epoch 7, Training Loss: 0.7949846680725322\n",
      "Epoch 8, Training Loss: 0.79358593583107\n",
      "Epoch 9, Training Loss: 0.7922551942572874\n",
      "Epoch 10, Training Loss: 0.7916543972492218\n",
      "Epoch 11, Training Loss: 0.7912042373769423\n",
      "Epoch 12, Training Loss: 0.7905640181373147\n",
      "Epoch 13, Training Loss: 0.7904324222312255\n",
      "Epoch 14, Training Loss: 0.7898557319360621\n",
      "Epoch 15, Training Loss: 0.7893075503321255\n",
      "Epoch 16, Training Loss: 0.7889763949197881\n",
      "Epoch 17, Training Loss: 0.7887977107132182\n",
      "Epoch 18, Training Loss: 0.7883588821747723\n",
      "Epoch 19, Training Loss: 0.7877679573788362\n",
      "Epoch 20, Training Loss: 0.7878674318509943\n",
      "Epoch 21, Training Loss: 0.7870111651981578\n",
      "Epoch 22, Training Loss: 0.7870912247545578\n",
      "Epoch 23, Training Loss: 0.7865708696842194\n",
      "Epoch 24, Training Loss: 0.7863912061382743\n",
      "Epoch 25, Training Loss: 0.7867738066000097\n",
      "Epoch 26, Training Loss: 0.7858415316132938\n",
      "Epoch 27, Training Loss: 0.7860160710531122\n",
      "Epoch 28, Training Loss: 0.7853225138608148\n",
      "Epoch 29, Training Loss: 0.7856964206695557\n",
      "Epoch 30, Training Loss: 0.7852211186465095\n",
      "Epoch 31, Training Loss: 0.7845762383937835\n",
      "Epoch 32, Training Loss: 0.7844185179822585\n",
      "Epoch 33, Training Loss: 0.7845741248130799\n",
      "Epoch 34, Training Loss: 0.7843768685705521\n",
      "Epoch 35, Training Loss: 0.7839438397042892\n",
      "Epoch 36, Training Loss: 0.7836944678951713\n",
      "Epoch 37, Training Loss: 0.7838103062265059\n",
      "Epoch 38, Training Loss: 0.7835916048638961\n",
      "Epoch 39, Training Loss: 0.7834015213040745\n",
      "Epoch 40, Training Loss: 0.7834502117072835\n",
      "Epoch 41, Training Loss: 0.7833351922736448\n",
      "Epoch 42, Training Loss: 0.7833698387706981\n",
      "Epoch 43, Training Loss: 0.7831224377015058\n",
      "Epoch 44, Training Loss: 0.7831009139734156\n",
      "Epoch 45, Training Loss: 0.782713202308206\n",
      "Epoch 46, Training Loss: 0.7827668067988227\n",
      "Epoch 47, Training Loss: 0.7826990219424753\n",
      "Epoch 48, Training Loss: 0.7821422926117392\n",
      "Epoch 49, Training Loss: 0.7823295048405142\n",
      "Epoch 50, Training Loss: 0.7822445159098681\n",
      "Epoch 51, Training Loss: 0.7820827923802769\n",
      "Epoch 52, Training Loss: 0.7819993784848381\n",
      "Epoch 53, Training Loss: 0.7817610813589657\n",
      "Epoch 54, Training Loss: 0.7812946829375099\n",
      "Epoch 55, Training Loss: 0.7816035741216996\n",
      "Epoch 56, Training Loss: 0.7814435825628393\n",
      "Epoch 57, Training Loss: 0.7811429817536298\n",
      "Epoch 58, Training Loss: 0.7814425132555121\n",
      "Epoch 59, Training Loss: 0.7812838156784282\n",
      "Epoch 60, Training Loss: 0.7810640266362359\n",
      "Epoch 61, Training Loss: 0.7811349856853486\n",
      "Epoch 62, Training Loss: 0.7809429210775038\n",
      "Epoch 63, Training Loss: 0.7808675706386566\n",
      "Epoch 64, Training Loss: 0.7807160225335289\n",
      "Epoch 65, Training Loss: 0.7807658012474284\n",
      "Epoch 66, Training Loss: 0.7809302736029905\n",
      "Epoch 67, Training Loss: 0.7801932065627154\n",
      "Epoch 68, Training Loss: 0.7801062023639679\n",
      "Epoch 69, Training Loss: 0.7800955448431127\n",
      "Epoch 70, Training Loss: 0.7800718722623937\n",
      "Epoch 71, Training Loss: 0.7799201446421006\n",
      "Epoch 72, Training Loss: 0.7796131701329175\n",
      "Epoch 73, Training Loss: 0.7800681769146639\n",
      "Epoch 74, Training Loss: 0.7796995503060958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 03:58:22,481] Trial 290 finished with value: 0.6384666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7796602898485521\n",
      "Epoch 1, Training Loss: 0.8378522695455336\n",
      "Epoch 2, Training Loss: 0.8076895933402212\n",
      "Epoch 3, Training Loss: 0.8020042568221128\n",
      "Epoch 4, Training Loss: 0.8014882081433347\n",
      "Epoch 5, Training Loss: 0.7978692371146123\n",
      "Epoch 6, Training Loss: 0.7986476363992332\n",
      "Epoch 7, Training Loss: 0.797692822872248\n",
      "Epoch 8, Training Loss: 0.7977650809108763\n",
      "Epoch 9, Training Loss: 0.7954500490561464\n",
      "Epoch 10, Training Loss: 0.794012775456995\n",
      "Epoch 11, Training Loss: 0.7941328340903261\n",
      "Epoch 12, Training Loss: 0.7936773748326122\n",
      "Epoch 13, Training Loss: 0.7936373885412862\n",
      "Epoch 14, Training Loss: 0.7931936307060987\n",
      "Epoch 15, Training Loss: 0.7935720233988941\n",
      "Epoch 16, Training Loss: 0.7920606338888183\n",
      "Epoch 17, Training Loss: 0.7923529814060469\n",
      "Epoch 18, Training Loss: 0.7940271377563477\n",
      "Epoch 19, Training Loss: 0.7921024773353921\n",
      "Epoch 20, Training Loss: 0.7925755285678949\n",
      "Epoch 21, Training Loss: 0.7912164387846352\n",
      "Epoch 22, Training Loss: 0.7908459032388558\n",
      "Epoch 23, Training Loss: 0.7910406045447614\n",
      "Epoch 24, Training Loss: 0.7907065698975011\n",
      "Epoch 25, Training Loss: 0.7905426953071938\n",
      "Epoch 26, Training Loss: 0.7910064028618031\n",
      "Epoch 27, Training Loss: 0.7902773422406132\n",
      "Epoch 28, Training Loss: 0.7899780989589548\n",
      "Epoch 29, Training Loss: 0.790882408618927\n",
      "Epoch 30, Training Loss: 0.7898963416429391\n",
      "Epoch 31, Training Loss: 0.7887953214179304\n",
      "Epoch 32, Training Loss: 0.7901781177162228\n",
      "Epoch 33, Training Loss: 0.7887837244155712\n",
      "Epoch 34, Training Loss: 0.7900291069109637\n",
      "Epoch 35, Training Loss: 0.7903656321360653\n",
      "Epoch 36, Training Loss: 0.7895677409673992\n",
      "Epoch 37, Training Loss: 0.7892617954795522\n",
      "Epoch 38, Training Loss: 0.7896756166802313\n",
      "Epoch 39, Training Loss: 0.7878551351396661\n",
      "Epoch 40, Training Loss: 0.788290398282216\n",
      "Epoch 41, Training Loss: 0.7882028605704917\n",
      "Epoch 42, Training Loss: 0.7889444007909387\n",
      "Epoch 43, Training Loss: 0.7899515226371306\n",
      "Epoch 44, Training Loss: 0.788892192141454\n",
      "Epoch 45, Training Loss: 0.7873288237062612\n",
      "Epoch 46, Training Loss: 0.7888380017495693\n",
      "Epoch 47, Training Loss: 0.7886741004492107\n",
      "Epoch 48, Training Loss: 0.7877008751819008\n",
      "Epoch 49, Training Loss: 0.7885945276210182\n",
      "Epoch 50, Training Loss: 0.787709944633613\n",
      "Epoch 51, Training Loss: 0.7888076815390049\n",
      "Epoch 52, Training Loss: 0.7891850977015674\n",
      "Epoch 53, Training Loss: 0.7882748775912407\n",
      "Epoch 54, Training Loss: 0.7886831222620225\n",
      "Epoch 55, Training Loss: 0.7885851799097277\n",
      "Epoch 56, Training Loss: 0.7877284437193907\n",
      "Epoch 57, Training Loss: 0.7891035972681261\n",
      "Epoch 58, Training Loss: 0.7873887691282688\n",
      "Epoch 59, Training Loss: 0.7888296839886142\n",
      "Epoch 60, Training Loss: 0.788600334457885\n",
      "Epoch 61, Training Loss: 0.7883965052160106\n",
      "Epoch 62, Training Loss: 0.7875034808216238\n",
      "Epoch 63, Training Loss: 0.7879278119345356\n",
      "Epoch 64, Training Loss: 0.7879012039729527\n",
      "Epoch 65, Training Loss: 0.7878195028556021\n",
      "Epoch 66, Training Loss: 0.7874357393809728\n",
      "Epoch 67, Training Loss: 0.7865853793638989\n",
      "Epoch 68, Training Loss: 0.7878502521299778\n",
      "Epoch 69, Training Loss: 0.7880991770808858\n",
      "Epoch 70, Training Loss: 0.7869457105944927\n",
      "Epoch 71, Training Loss: 0.7880663232695787\n",
      "Epoch 72, Training Loss: 0.7871575516865665\n",
      "Epoch 73, Training Loss: 0.7868339007958434\n",
      "Epoch 74, Training Loss: 0.7873653161794619\n",
      "Epoch 75, Training Loss: 0.7869642716601379\n",
      "Epoch 76, Training Loss: 0.7872029187087726\n",
      "Epoch 77, Training Loss: 0.7879925009003259\n",
      "Epoch 78, Training Loss: 0.7878938060057791\n",
      "Epoch 79, Training Loss: 0.7872756115475992\n",
      "Epoch 80, Training Loss: 0.7869072322110484\n",
      "Epoch 81, Training Loss: 0.7870338396022194\n",
      "Epoch 82, Training Loss: 0.7868942409529722\n",
      "Epoch 83, Training Loss: 0.7871353876321836\n",
      "Epoch 84, Training Loss: 0.7869718810669462\n",
      "Epoch 85, Training Loss: 0.787033502410229\n",
      "Epoch 86, Training Loss: 0.7865663940745189\n",
      "Epoch 87, Training Loss: 0.7862810375098895\n",
      "Epoch 88, Training Loss: 0.7870777905435491\n",
      "Epoch 89, Training Loss: 0.7866086400541148\n",
      "Epoch 90, Training Loss: 0.7868658722791456\n",
      "Epoch 91, Training Loss: 0.7878301198321177\n",
      "Epoch 92, Training Loss: 0.7872495780313822\n",
      "Epoch 93, Training Loss: 0.7870932049321052\n",
      "Epoch 94, Training Loss: 0.7874736704324421\n",
      "Epoch 95, Training Loss: 0.7872207330581837\n",
      "Epoch 96, Training Loss: 0.7861017220898678\n",
      "Epoch 97, Training Loss: 0.7857620495602601\n",
      "Epoch 98, Training Loss: 0.7857369742895427\n",
      "Epoch 99, Training Loss: 0.786735977624592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:00:20,531] Trial 291 finished with value: 0.6366 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7866943185490773\n",
      "Epoch 1, Training Loss: 0.8714669584526735\n",
      "Epoch 2, Training Loss: 0.8197321529949413\n",
      "Epoch 3, Training Loss: 0.8119175268622005\n",
      "Epoch 4, Training Loss: 0.8087349163083469\n",
      "Epoch 5, Training Loss: 0.8069149395998787\n",
      "Epoch 6, Training Loss: 0.8048133445487303\n",
      "Epoch 7, Training Loss: 0.8037769664035124\n",
      "Epoch 8, Training Loss: 0.8019939777430366\n",
      "Epoch 9, Training Loss: 0.8006782699332518\n",
      "Epoch 10, Training Loss: 0.8001225352287292\n",
      "Epoch 11, Training Loss: 0.8009645104408264\n",
      "Epoch 12, Training Loss: 0.7998614330151502\n",
      "Epoch 13, Training Loss: 0.7988128481191747\n",
      "Epoch 14, Training Loss: 0.797967451951083\n",
      "Epoch 15, Training Loss: 0.7981460501867182\n",
      "Epoch 16, Training Loss: 0.7979177491805133\n",
      "Epoch 17, Training Loss: 0.7971953866061042\n",
      "Epoch 18, Training Loss: 0.7975895122920765\n",
      "Epoch 19, Training Loss: 0.796970433066873\n",
      "Epoch 20, Training Loss: 0.7964580481192645\n",
      "Epoch 21, Training Loss: 0.7957825969247257\n",
      "Epoch 22, Training Loss: 0.795784612122704\n",
      "Epoch 23, Training Loss: 0.7959610708320842\n",
      "Epoch 24, Training Loss: 0.7952494455786312\n",
      "Epoch 25, Training Loss: 0.7957369504255407\n",
      "Epoch 26, Training Loss: 0.7942342631255879\n",
      "Epoch 27, Training Loss: 0.7942793008159189\n",
      "Epoch 28, Training Loss: 0.79529435543453\n",
      "Epoch 29, Training Loss: 0.7946450652094448\n",
      "Epoch 30, Training Loss: 0.7944784865659826\n",
      "Epoch 31, Training Loss: 0.7940044388350318\n",
      "Epoch 32, Training Loss: 0.7944422239415786\n",
      "Epoch 33, Training Loss: 0.7942624829797184\n",
      "Epoch 34, Training Loss: 0.7961064169687383\n",
      "Epoch 35, Training Loss: 0.7939834324051352\n",
      "Epoch 36, Training Loss: 0.794196223301046\n",
      "Epoch 37, Training Loss: 0.7940171525057624\n",
      "Epoch 38, Training Loss: 0.7938412347961875\n",
      "Epoch 39, Training Loss: 0.7937719723757576\n",
      "Epoch 40, Training Loss: 0.7934625556889703\n",
      "Epoch 41, Training Loss: 0.793650211726918\n",
      "Epoch 42, Training Loss: 0.7932318220419042\n",
      "Epoch 43, Training Loss: 0.7935879967493169\n",
      "Epoch 44, Training Loss: 0.7926466357006746\n",
      "Epoch 45, Training Loss: 0.7935604805806104\n",
      "Epoch 46, Training Loss: 0.7942777924677905\n",
      "Epoch 47, Training Loss: 0.7933517876092125\n",
      "Epoch 48, Training Loss: 0.793422152785694\n",
      "Epoch 49, Training Loss: 0.7935630822181702\n",
      "Epoch 50, Training Loss: 0.7926253330006319\n",
      "Epoch 51, Training Loss: 0.7932291293845457\n",
      "Epoch 52, Training Loss: 0.7924536168575287\n",
      "Epoch 53, Training Loss: 0.7930776910220876\n",
      "Epoch 54, Training Loss: 0.7932355713844299\n",
      "Epoch 55, Training Loss: 0.7929866593725541\n",
      "Epoch 56, Training Loss: 0.7932976891713984\n",
      "Epoch 57, Training Loss: 0.792852950446746\n",
      "Epoch 58, Training Loss: 0.7926319041672875\n",
      "Epoch 59, Training Loss: 0.7924703569973216\n",
      "Epoch 60, Training Loss: 0.7932705461277682\n",
      "Epoch 61, Training Loss: 0.7926632706558003\n",
      "Epoch 62, Training Loss: 0.7928898388497969\n",
      "Epoch 63, Training Loss: 0.7923803870116963\n",
      "Epoch 64, Training Loss: 0.7928333775436177\n",
      "Epoch 65, Training Loss: 0.7933964997880599\n",
      "Epoch 66, Training Loss: 0.7924537085785586\n",
      "Epoch 67, Training Loss: 0.7927543983038734\n",
      "Epoch 68, Training Loss: 0.7931219607942245\n",
      "Epoch 69, Training Loss: 0.7930742315685048\n",
      "Epoch 70, Training Loss: 0.7932217636529137\n",
      "Epoch 71, Training Loss: 0.7923598121194279\n",
      "Epoch 72, Training Loss: 0.7922349875814775\n",
      "Epoch 73, Training Loss: 0.793210366192986\n",
      "Epoch 74, Training Loss: 0.7920018124580384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:02:11,399] Trial 292 finished with value: 0.6383333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7928836012587828\n",
      "Epoch 1, Training Loss: 1.0062622536631192\n",
      "Epoch 2, Training Loss: 0.9356352861488567\n",
      "Epoch 3, Training Loss: 0.9190835290796616\n",
      "Epoch 4, Training Loss: 0.9012707457822912\n",
      "Epoch 5, Training Loss: 0.8739375009957482\n",
      "Epoch 6, Training Loss: 0.8389526481488172\n",
      "Epoch 7, Training Loss: 0.8171476510693045\n",
      "Epoch 8, Training Loss: 0.8094223431979909\n",
      "Epoch 9, Training Loss: 0.806819991644691\n",
      "Epoch 10, Training Loss: 0.8052442051382626\n",
      "Epoch 11, Training Loss: 0.8037210302493152\n",
      "Epoch 12, Training Loss: 0.8027482198266422\n",
      "Epoch 13, Training Loss: 0.8017541740221136\n",
      "Epoch 14, Training Loss: 0.8010709135672626\n",
      "Epoch 15, Training Loss: 0.8007171909248127\n",
      "Epoch 16, Training Loss: 0.8005066327487721\n",
      "Epoch 17, Training Loss: 0.7996246658353244\n",
      "Epoch 18, Training Loss: 0.7990799892649931\n",
      "Epoch 19, Training Loss: 0.7986716619659873\n",
      "Epoch 20, Training Loss: 0.7981439738413867\n",
      "Epoch 21, Training Loss: 0.7978283999246709\n",
      "Epoch 22, Training Loss: 0.7971822483399335\n",
      "Epoch 23, Training Loss: 0.7969570715287152\n",
      "Epoch 24, Training Loss: 0.7960716070147121\n",
      "Epoch 25, Training Loss: 0.7956985701532925\n",
      "Epoch 26, Training Loss: 0.795618150584838\n",
      "Epoch 27, Training Loss: 0.7950072993951686\n",
      "Epoch 28, Training Loss: 0.7945139874430264\n",
      "Epoch 29, Training Loss: 0.7939559519290924\n",
      "Epoch 30, Training Loss: 0.7938631622931537\n",
      "Epoch 31, Training Loss: 0.7931088307324577\n",
      "Epoch 32, Training Loss: 0.7928601852585287\n",
      "Epoch 33, Training Loss: 0.7922221423597897\n",
      "Epoch 34, Training Loss: 0.7918419503464418\n",
      "Epoch 35, Training Loss: 0.7915604367676903\n",
      "Epoch 36, Training Loss: 0.7911273227018468\n",
      "Epoch 37, Training Loss: 0.7906838049608118\n",
      "Epoch 38, Training Loss: 0.7903239069966709\n",
      "Epoch 39, Training Loss: 0.7899056769819821\n",
      "Epoch 40, Training Loss: 0.789978804027333\n",
      "Epoch 41, Training Loss: 0.7897629392848295\n",
      "Epoch 42, Training Loss: 0.7889828991188722\n",
      "Epoch 43, Training Loss: 0.7891521239280701\n",
      "Epoch 44, Training Loss: 0.7885876728506649\n",
      "Epoch 45, Training Loss: 0.7883119212178623\n",
      "Epoch 46, Training Loss: 0.7881849990171544\n",
      "Epoch 47, Training Loss: 0.7882063557821162\n",
      "Epoch 48, Training Loss: 0.787730171399958\n",
      "Epoch 49, Training Loss: 0.7875970416910508\n",
      "Epoch 50, Training Loss: 0.7872910738692565\n",
      "Epoch 51, Training Loss: 0.7873248037169961\n",
      "Epoch 52, Training Loss: 0.7870517491593081\n",
      "Epoch 53, Training Loss: 0.7867327744119308\n",
      "Epoch 54, Training Loss: 0.7866336191401763\n",
      "Epoch 55, Training Loss: 0.7864340294108672\n",
      "Epoch 56, Training Loss: 0.7864078861825606\n",
      "Epoch 57, Training Loss: 0.7861460976039663\n",
      "Epoch 58, Training Loss: 0.7858910616005168\n",
      "Epoch 59, Training Loss: 0.7858284277775708\n",
      "Epoch 60, Training Loss: 0.7858169613866245\n",
      "Epoch 61, Training Loss: 0.7855599099047044\n",
      "Epoch 62, Training Loss: 0.7854862512560452\n",
      "Epoch 63, Training Loss: 0.7852734276126413\n",
      "Epoch 64, Training Loss: 0.7852449731265797\n",
      "Epoch 65, Training Loss: 0.7849587868241703\n",
      "Epoch 66, Training Loss: 0.7850423679632299\n",
      "Epoch 67, Training Loss: 0.7847439054180594\n",
      "Epoch 68, Training Loss: 0.7849841047735775\n",
      "Epoch 69, Training Loss: 0.7845989990935606\n",
      "Epoch 70, Training Loss: 0.7846685815558714\n",
      "Epoch 71, Training Loss: 0.7845495325677535\n",
      "Epoch 72, Training Loss: 0.7841068996401395\n",
      "Epoch 73, Training Loss: 0.7842212491175707\n",
      "Epoch 74, Training Loss: 0.7842696574856253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:03:29,613] Trial 293 finished with value: 0.6385333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7842528274480034\n",
      "Epoch 1, Training Loss: 0.9357271673984098\n",
      "Epoch 2, Training Loss: 0.880096435726137\n",
      "Epoch 3, Training Loss: 0.8390484216517972\n",
      "Epoch 4, Training Loss: 0.8204017132744753\n",
      "Epoch 5, Training Loss: 0.8140200510957187\n",
      "Epoch 6, Training Loss: 0.8118673746747181\n",
      "Epoch 7, Training Loss: 0.8107204629066295\n",
      "Epoch 8, Training Loss: 0.8096582041647201\n",
      "Epoch 9, Training Loss: 0.8088774011547404\n",
      "Epoch 10, Training Loss: 0.8078030173491715\n",
      "Epoch 11, Training Loss: 0.8077464036475447\n",
      "Epoch 12, Training Loss: 0.807726178043767\n",
      "Epoch 13, Training Loss: 0.8062066673336172\n",
      "Epoch 14, Training Loss: 0.8065514980402207\n",
      "Epoch 15, Training Loss: 0.8059441916028359\n",
      "Epoch 16, Training Loss: 0.8064796401145763\n",
      "Epoch 17, Training Loss: 0.8059063867518776\n",
      "Epoch 18, Training Loss: 0.8052726966994149\n",
      "Epoch 19, Training Loss: 0.8051161637879852\n",
      "Epoch 20, Training Loss: 0.8049410217686703\n",
      "Epoch 21, Training Loss: 0.8045389888878155\n",
      "Epoch 22, Training Loss: 0.8042822334999429\n",
      "Epoch 23, Training Loss: 0.8040433310028305\n",
      "Epoch 24, Training Loss: 0.8037261358777383\n",
      "Epoch 25, Training Loss: 0.8031864724661174\n",
      "Epoch 26, Training Loss: 0.8033303120978793\n",
      "Epoch 27, Training Loss: 0.8031247278801481\n",
      "Epoch 28, Training Loss: 0.8032188016669195\n",
      "Epoch 29, Training Loss: 0.8022424198631057\n",
      "Epoch 30, Training Loss: 0.8023382864500347\n",
      "Epoch 31, Training Loss: 0.8027976669763264\n",
      "Epoch 32, Training Loss: 0.8019484285125159\n",
      "Epoch 33, Training Loss: 0.8021302821044635\n",
      "Epoch 34, Training Loss: 0.8013377529337891\n",
      "Epoch 35, Training Loss: 0.8026142662629149\n",
      "Epoch 36, Training Loss: 0.8013782059339652\n",
      "Epoch 37, Training Loss: 0.8014159102188914\n",
      "Epoch 38, Training Loss: 0.801404185044138\n",
      "Epoch 39, Training Loss: 0.8007403578077044\n",
      "Epoch 40, Training Loss: 0.8009364389835444\n",
      "Epoch 41, Training Loss: 0.8006299819265094\n",
      "Epoch 42, Training Loss: 0.8007295835287052\n",
      "Epoch 43, Training Loss: 0.8003513260891563\n",
      "Epoch 44, Training Loss: 0.800621866014667\n",
      "Epoch 45, Training Loss: 0.8006616337855059\n",
      "Epoch 46, Training Loss: 0.8006238130698526\n",
      "Epoch 47, Training Loss: 0.799616667740327\n",
      "Epoch 48, Training Loss: 0.8002310437367375\n",
      "Epoch 49, Training Loss: 0.8000055239612894\n",
      "Epoch 50, Training Loss: 0.7996202002790638\n",
      "Epoch 51, Training Loss: 0.7996255611118518\n",
      "Epoch 52, Training Loss: 0.8001976508843271\n",
      "Epoch 53, Training Loss: 0.7995278579848153\n",
      "Epoch 54, Training Loss: 0.7991859878812517\n",
      "Epoch 55, Training Loss: 0.8001438147143314\n",
      "Epoch 56, Training Loss: 0.7987275104773672\n",
      "Epoch 57, Training Loss: 0.7991234597406889\n",
      "Epoch 58, Training Loss: 0.7986941717621079\n",
      "Epoch 59, Training Loss: 0.7986893702270393\n",
      "Epoch 60, Training Loss: 0.7987278901544729\n",
      "Epoch 61, Training Loss: 0.7986716486457596\n",
      "Epoch 62, Training Loss: 0.7986853275980268\n",
      "Epoch 63, Training Loss: 0.7984530892587246\n",
      "Epoch 64, Training Loss: 0.7989797948894645\n",
      "Epoch 65, Training Loss: 0.7987042654725842\n",
      "Epoch 66, Training Loss: 0.798568884770673\n",
      "Epoch 67, Training Loss: 0.7987201159161733\n",
      "Epoch 68, Training Loss: 0.7985073265276457\n",
      "Epoch 69, Training Loss: 0.7981184486159705\n",
      "Epoch 70, Training Loss: 0.7984663691735805\n",
      "Epoch 71, Training Loss: 0.7986553484335878\n",
      "Epoch 72, Training Loss: 0.7979093925397199\n",
      "Epoch 73, Training Loss: 0.7985164077658402\n",
      "Epoch 74, Training Loss: 0.797430229814429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:04:41,123] Trial 294 finished with value: 0.6212666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7985571016942649\n",
      "Epoch 1, Training Loss: 0.9808151021935886\n",
      "Epoch 2, Training Loss: 0.9378435739000938\n",
      "Epoch 3, Training Loss: 0.9103607714624333\n",
      "Epoch 4, Training Loss: 0.8728881521332533\n",
      "Epoch 5, Training Loss: 0.8385909567650099\n",
      "Epoch 6, Training Loss: 0.8208405529646049\n",
      "Epoch 7, Training Loss: 0.8152287437503499\n",
      "Epoch 8, Training Loss: 0.8129013077657026\n",
      "Epoch 9, Training Loss: 0.8116030851701148\n",
      "Epoch 10, Training Loss: 0.8107331886327356\n",
      "Epoch 11, Training Loss: 0.8107862931445129\n",
      "Epoch 12, Training Loss: 0.8096545163850138\n",
      "Epoch 13, Training Loss: 0.808839724296914\n",
      "Epoch 14, Training Loss: 0.8089699166161674\n",
      "Epoch 15, Training Loss: 0.8078328708957012\n",
      "Epoch 16, Training Loss: 0.8071142559661004\n",
      "Epoch 17, Training Loss: 0.8067870510251899\n",
      "Epoch 18, Training Loss: 0.8071966378312362\n",
      "Epoch 19, Training Loss: 0.8062480108182233\n",
      "Epoch 20, Training Loss: 0.8054458807285567\n",
      "Epoch 21, Training Loss: 0.8049832922175415\n",
      "Epoch 22, Training Loss: 0.8038645825887981\n",
      "Epoch 23, Training Loss: 0.8040658854900447\n",
      "Epoch 24, Training Loss: 0.803028512897348\n",
      "Epoch 25, Training Loss: 0.8026304797122353\n",
      "Epoch 26, Training Loss: 0.8033963905241256\n",
      "Epoch 27, Training Loss: 0.8020737225399878\n",
      "Epoch 28, Training Loss: 0.8019698674517467\n",
      "Epoch 29, Training Loss: 0.8025326230472192\n",
      "Epoch 30, Training Loss: 0.8015833230843221\n",
      "Epoch 31, Training Loss: 0.8019079496985988\n",
      "Epoch 32, Training Loss: 0.8020158989985187\n",
      "Epoch 33, Training Loss: 0.80176467052976\n",
      "Epoch 34, Training Loss: 0.8013853453155747\n",
      "Epoch 35, Training Loss: 0.8009428269881055\n",
      "Epoch 36, Training Loss: 0.80076330192107\n",
      "Epoch 37, Training Loss: 0.8009038795205884\n",
      "Epoch 38, Training Loss: 0.8004781128768634\n",
      "Epoch 39, Training Loss: 0.8006386284541367\n",
      "Epoch 40, Training Loss: 0.8007268111508592\n",
      "Epoch 41, Training Loss: 0.8007140317357573\n",
      "Epoch 42, Training Loss: 0.7997320725057359\n",
      "Epoch 43, Training Loss: 0.799884413418017\n",
      "Epoch 44, Training Loss: 0.8005525807689007\n",
      "Epoch 45, Training Loss: 0.7997238387738852\n",
      "Epoch 46, Training Loss: 0.7993503860064916\n",
      "Epoch 47, Training Loss: 0.8007286449124043\n",
      "Epoch 48, Training Loss: 0.7998012013901445\n",
      "Epoch 49, Training Loss: 0.7992771017820315\n",
      "Epoch 50, Training Loss: 0.7993548722195446\n",
      "Epoch 51, Training Loss: 0.7992983429055465\n",
      "Epoch 52, Training Loss: 0.7997262081705538\n",
      "Epoch 53, Training Loss: 0.7988474274040165\n",
      "Epoch 54, Training Loss: 0.7990854069702608\n",
      "Epoch 55, Training Loss: 0.7984700239690623\n",
      "Epoch 56, Training Loss: 0.7985057810195406\n",
      "Epoch 57, Training Loss: 0.7987896892361175\n",
      "Epoch 58, Training Loss: 0.7994603411595624\n",
      "Epoch 59, Training Loss: 0.7981529239425086\n",
      "Epoch 60, Training Loss: 0.797790938615799\n",
      "Epoch 61, Training Loss: 0.7982546192362793\n",
      "Epoch 62, Training Loss: 0.7976965096660127\n",
      "Epoch 63, Training Loss: 0.7973959658378945\n",
      "Epoch 64, Training Loss: 0.7978638874857049\n",
      "Epoch 65, Training Loss: 0.7977271656344708\n",
      "Epoch 66, Training Loss: 0.797475826919527\n",
      "Epoch 67, Training Loss: 0.7978813442072474\n",
      "Epoch 68, Training Loss: 0.7976054222960222\n",
      "Epoch 69, Training Loss: 0.7972641202740203\n",
      "Epoch 70, Training Loss: 0.7973244987036052\n",
      "Epoch 71, Training Loss: 0.7975538664294365\n",
      "Epoch 72, Training Loss: 0.7976584445265003\n",
      "Epoch 73, Training Loss: 0.7972090821517142\n",
      "Epoch 74, Training Loss: 0.7974119494732161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:06:05,263] Trial 295 finished with value: 0.6289333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7966436935546702\n",
      "Epoch 1, Training Loss: 0.9937948925154549\n",
      "Epoch 2, Training Loss: 0.9538072387078651\n",
      "Epoch 3, Training Loss: 0.9458477786609105\n",
      "Epoch 4, Training Loss: 0.9387353119993569\n",
      "Epoch 5, Training Loss: 0.9319658844990838\n",
      "Epoch 6, Training Loss: 0.9247483092143123\n",
      "Epoch 7, Training Loss: 0.9170436535562788\n",
      "Epoch 8, Training Loss: 0.9096103939794956\n",
      "Epoch 9, Training Loss: 0.9021049720004088\n",
      "Epoch 10, Training Loss: 0.8942510965175199\n",
      "Epoch 11, Training Loss: 0.8862050615755239\n",
      "Epoch 12, Training Loss: 0.8789412732411148\n",
      "Epoch 13, Training Loss: 0.8718435488249127\n",
      "Epoch 14, Training Loss: 0.8640740235945336\n",
      "Epoch 15, Training Loss: 0.8581697887047789\n",
      "Epoch 16, Training Loss: 0.8509198385073726\n",
      "Epoch 17, Training Loss: 0.8457072970562411\n",
      "Epoch 18, Training Loss: 0.8405462061552177\n",
      "Epoch 19, Training Loss: 0.8365141199047404\n",
      "Epoch 20, Training Loss: 0.8325615557512842\n",
      "Epoch 21, Training Loss: 0.8285709634759373\n",
      "Epoch 22, Training Loss: 0.8273114459855216\n",
      "Epoch 23, Training Loss: 0.8229856187687781\n",
      "Epoch 24, Training Loss: 0.8216135734005978\n",
      "Epoch 25, Training Loss: 0.8195396014622279\n",
      "Epoch 26, Training Loss: 0.8179260866086285\n",
      "Epoch 27, Training Loss: 0.8169493510310811\n",
      "Epoch 28, Training Loss: 0.8150240568290079\n",
      "Epoch 29, Training Loss: 0.8142003509335052\n",
      "Epoch 30, Training Loss: 0.8136234368596759\n",
      "Epoch 31, Training Loss: 0.8125313221063829\n",
      "Epoch 32, Training Loss: 0.8119767165721807\n",
      "Epoch 33, Training Loss: 0.8118534755886049\n",
      "Epoch 34, Training Loss: 0.8113976313655538\n",
      "Epoch 35, Training Loss: 0.8108291950440945\n",
      "Epoch 36, Training Loss: 0.8101593152921003\n",
      "Epoch 37, Training Loss: 0.8097008508847172\n",
      "Epoch 38, Training Loss: 0.809772387124542\n",
      "Epoch 39, Training Loss: 0.8095586878912789\n",
      "Epoch 40, Training Loss: 0.8088968535114948\n",
      "Epoch 41, Training Loss: 0.8085165328549263\n",
      "Epoch 42, Training Loss: 0.8083118148316119\n",
      "Epoch 43, Training Loss: 0.8081156871372596\n",
      "Epoch 44, Training Loss: 0.8077101001165863\n",
      "Epoch 45, Training Loss: 0.8083323457187279\n",
      "Epoch 46, Training Loss: 0.8073409232878147\n",
      "Epoch 47, Training Loss: 0.8071134859457948\n",
      "Epoch 48, Training Loss: 0.8074858876995574\n",
      "Epoch 49, Training Loss: 0.8066199992832385\n",
      "Epoch 50, Training Loss: 0.807206101883623\n",
      "Epoch 51, Training Loss: 0.8064274448201172\n",
      "Epoch 52, Training Loss: 0.8063946620862287\n",
      "Epoch 53, Training Loss: 0.8066253429068658\n",
      "Epoch 54, Training Loss: 0.8060920661553405\n",
      "Epoch 55, Training Loss: 0.8056598276123965\n",
      "Epoch 56, Training Loss: 0.8063045189792949\n",
      "Epoch 57, Training Loss: 0.8060208247120219\n",
      "Epoch 58, Training Loss: 0.8054191678090203\n",
      "Epoch 59, Training Loss: 0.8058317728508684\n",
      "Epoch 60, Training Loss: 0.8051213706346383\n",
      "Epoch 61, Training Loss: 0.8054129441877953\n",
      "Epoch 62, Training Loss: 0.8053794738941623\n",
      "Epoch 63, Training Loss: 0.8051206640731123\n",
      "Epoch 64, Training Loss: 0.805416270754391\n",
      "Epoch 65, Training Loss: 0.8044026930529372\n",
      "Epoch 66, Training Loss: 0.8049481259252792\n",
      "Epoch 67, Training Loss: 0.8050191838938491\n",
      "Epoch 68, Training Loss: 0.8040525993906465\n",
      "Epoch 69, Training Loss: 0.8045806485907476\n",
      "Epoch 70, Training Loss: 0.8042497414395325\n",
      "Epoch 71, Training Loss: 0.8037552156842741\n",
      "Epoch 72, Training Loss: 0.803422491935859\n",
      "Epoch 73, Training Loss: 0.804059594645536\n",
      "Epoch 74, Training Loss: 0.8034811142691992\n",
      "Epoch 75, Training Loss: 0.8035771989284601\n",
      "Epoch 76, Training Loss: 0.8036800782483323\n",
      "Epoch 77, Training Loss: 0.8035161779339152\n",
      "Epoch 78, Training Loss: 0.803014956649981\n",
      "Epoch 79, Training Loss: 0.8028679991126957\n",
      "Epoch 80, Training Loss: 0.8034910114187943\n",
      "Epoch 81, Training Loss: 0.8029543014397299\n",
      "Epoch 82, Training Loss: 0.8030792843130298\n",
      "Epoch 83, Training Loss: 0.803100101571334\n",
      "Epoch 84, Training Loss: 0.8027904093713689\n",
      "Epoch 85, Training Loss: 0.8025694993205537\n",
      "Epoch 86, Training Loss: 0.8024225745882306\n",
      "Epoch 87, Training Loss: 0.8023535274921503\n",
      "Epoch 88, Training Loss: 0.8017487616467297\n",
      "Epoch 89, Training Loss: 0.801607384627923\n",
      "Epoch 90, Training Loss: 0.8014187008814704\n",
      "Epoch 91, Training Loss: 0.8015740778213157\n",
      "Epoch 92, Training Loss: 0.8016887022140331\n",
      "Epoch 93, Training Loss: 0.8012870353415497\n",
      "Epoch 94, Training Loss: 0.8033168099876633\n",
      "Epoch 95, Training Loss: 0.8017249225673819\n",
      "Epoch 96, Training Loss: 0.8012556544820169\n",
      "Epoch 97, Training Loss: 0.8011719422232836\n",
      "Epoch 98, Training Loss: 0.8012639138035308\n",
      "Epoch 99, Training Loss: 0.8011048018484187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:07:28,551] Trial 296 finished with value: 0.6341333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8010150248843028\n",
      "Epoch 1, Training Loss: 0.862124421526404\n",
      "Epoch 2, Training Loss: 0.8099317018424763\n",
      "Epoch 3, Training Loss: 0.8050424771449145\n",
      "Epoch 4, Training Loss: 0.8022027199408587\n",
      "Epoch 5, Training Loss: 0.7996981432157404\n",
      "Epoch 6, Training Loss: 0.7975343156562132\n",
      "Epoch 7, Training Loss: 0.7952819864890155\n",
      "Epoch 8, Training Loss: 0.7939694531524882\n",
      "Epoch 9, Training Loss: 0.7924401858273674\n",
      "Epoch 10, Training Loss: 0.7910827628303977\n",
      "Epoch 11, Training Loss: 0.7905715767776265\n",
      "Epoch 12, Training Loss: 0.7897734362237594\n",
      "Epoch 13, Training Loss: 0.7892772415105034\n",
      "Epoch 14, Training Loss: 0.7885509840179892\n",
      "Epoch 15, Training Loss: 0.7880431659782634\n",
      "Epoch 16, Training Loss: 0.7877227210998535\n",
      "Epoch 17, Training Loss: 0.7870840165194343\n",
      "Epoch 18, Training Loss: 0.7867325403409846\n",
      "Epoch 19, Training Loss: 0.7863149454313166\n",
      "Epoch 20, Training Loss: 0.7858783053650575\n",
      "Epoch 21, Training Loss: 0.7858416687039768\n",
      "Epoch 22, Training Loss: 0.785344295501709\n",
      "Epoch 23, Training Loss: 0.7845580549801097\n",
      "Epoch 24, Training Loss: 0.7847063914467307\n",
      "Epoch 25, Training Loss: 0.7844308533388026\n",
      "Epoch 26, Training Loss: 0.7841868580088895\n",
      "Epoch 27, Training Loss: 0.7839078116416931\n",
      "Epoch 28, Training Loss: 0.783569871818318\n",
      "Epoch 29, Training Loss: 0.7835266893751481\n",
      "Epoch 30, Training Loss: 0.7826221314598533\n",
      "Epoch 31, Training Loss: 0.7828886436714846\n",
      "Epoch 32, Training Loss: 0.7828912955171922\n",
      "Epoch 33, Training Loss: 0.782364544097115\n",
      "Epoch 34, Training Loss: 0.7821368465704076\n",
      "Epoch 35, Training Loss: 0.7822832850848928\n",
      "Epoch 36, Training Loss: 0.7820541854465709\n",
      "Epoch 37, Training Loss: 0.7816355339218588\n",
      "Epoch 38, Training Loss: 0.7812907296769759\n",
      "Epoch 39, Training Loss: 0.7812672374528997\n",
      "Epoch 40, Training Loss: 0.7813637902456171\n",
      "Epoch 41, Training Loss: 0.7809481773656958\n",
      "Epoch 42, Training Loss: 0.7813355200430926\n",
      "Epoch 43, Training Loss: 0.7808407388715183\n",
      "Epoch 44, Training Loss: 0.7810701889150283\n",
      "Epoch 45, Training Loss: 0.7805069865899927\n",
      "Epoch 46, Training Loss: 0.7802028469478383\n",
      "Epoch 47, Training Loss: 0.7802688549546635\n",
      "Epoch 48, Training Loss: 0.7803707216767704\n",
      "Epoch 49, Training Loss: 0.7803513740090763\n",
      "Epoch 50, Training Loss: 0.7799514662518221\n",
      "Epoch 51, Training Loss: 0.7798842430815978\n",
      "Epoch 52, Training Loss: 0.7799882299759808\n",
      "Epoch 53, Training Loss: 0.7796346427412594\n",
      "Epoch 54, Training Loss: 0.7794359225385329\n",
      "Epoch 55, Training Loss: 0.779850498788497\n",
      "Epoch 56, Training Loss: 0.7795521430408253\n",
      "Epoch 57, Training Loss: 0.7790463483333587\n",
      "Epoch 58, Training Loss: 0.7791123196657966\n",
      "Epoch 59, Training Loss: 0.7791417179388158\n",
      "Epoch 60, Training Loss: 0.7784504287383136\n",
      "Epoch 61, Training Loss: 0.7788299303896287\n",
      "Epoch 62, Training Loss: 0.7784937621565426\n",
      "Epoch 63, Training Loss: 0.7785927611940048\n",
      "Epoch 64, Training Loss: 0.7782299462486716\n",
      "Epoch 65, Training Loss: 0.7784867188509773\n",
      "Epoch 66, Training Loss: 0.7784217391294591\n",
      "Epoch 67, Training Loss: 0.7784790126015159\n",
      "Epoch 68, Training Loss: 0.7779631145561443\n",
      "Epoch 69, Training Loss: 0.7779636560468113\n",
      "Epoch 70, Training Loss: 0.7780170723269968\n",
      "Epoch 71, Training Loss: 0.7780382813425625\n",
      "Epoch 72, Training Loss: 0.777537379545324\n",
      "Epoch 73, Training Loss: 0.7776152781879201\n",
      "Epoch 74, Training Loss: 0.7777139081674463\n",
      "Epoch 75, Training Loss: 0.7777043281583225\n",
      "Epoch 76, Training Loss: 0.7778260293427636\n",
      "Epoch 77, Training Loss: 0.7773350800486172\n",
      "Epoch 78, Training Loss: 0.7772888456372654\n",
      "Epoch 79, Training Loss: 0.777294473437702\n",
      "Epoch 80, Training Loss: 0.7775145051058601\n",
      "Epoch 81, Training Loss: 0.776791384009754\n",
      "Epoch 82, Training Loss: 0.7772207870904138\n",
      "Epoch 83, Training Loss: 0.7771558219545027\n",
      "Epoch 84, Training Loss: 0.7771687899617588\n",
      "Epoch 85, Training Loss: 0.7766837021182565\n",
      "Epoch 86, Training Loss: 0.7767975314925699\n",
      "Epoch 87, Training Loss: 0.7771180269297432\n",
      "Epoch 88, Training Loss: 0.7768069614382351\n",
      "Epoch 89, Training Loss: 0.7768140083902022\n",
      "Epoch 90, Training Loss: 0.7765456707337324\n",
      "Epoch 91, Training Loss: 0.7766055809049045\n",
      "Epoch 92, Training Loss: 0.7763201297731961\n",
      "Epoch 93, Training Loss: 0.7763201409227708\n",
      "Epoch 94, Training Loss: 0.7762256812348085\n",
      "Epoch 95, Training Loss: 0.7759660251701579\n",
      "Epoch 96, Training Loss: 0.7760094342512243\n",
      "Epoch 97, Training Loss: 0.7759185843607959\n",
      "Epoch 98, Training Loss: 0.776093316078186\n",
      "Epoch 99, Training Loss: 0.7756636956860038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:09:37,057] Trial 297 finished with value: 0.6391333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7759382474422455\n",
      "Epoch 1, Training Loss: 0.877980352389185\n",
      "Epoch 2, Training Loss: 0.8251539942913486\n",
      "Epoch 3, Training Loss: 0.8177856880919377\n",
      "Epoch 4, Training Loss: 0.8154837992854584\n",
      "Epoch 5, Training Loss: 0.8122378057106994\n",
      "Epoch 6, Training Loss: 0.810755733619059\n",
      "Epoch 7, Training Loss: 0.8102603863056441\n",
      "Epoch 8, Training Loss: 0.8074800459962143\n",
      "Epoch 9, Training Loss: 0.8056669212821731\n",
      "Epoch 10, Training Loss: 0.8062452847796275\n",
      "Epoch 11, Training Loss: 0.8040870985590426\n",
      "Epoch 12, Training Loss: 0.8050872487233097\n",
      "Epoch 13, Training Loss: 0.8046728196897005\n",
      "Epoch 14, Training Loss: 0.8032113410476455\n",
      "Epoch 15, Training Loss: 0.8031507304736546\n",
      "Epoch 16, Training Loss: 0.8028389100741623\n",
      "Epoch 17, Training Loss: 0.8020677883822219\n",
      "Epoch 18, Training Loss: 0.8023511120251247\n",
      "Epoch 19, Training Loss: 0.8020622503488584\n",
      "Epoch 20, Training Loss: 0.8022157372388624\n",
      "Epoch 21, Training Loss: 0.8003269468035017\n",
      "Epoch 22, Training Loss: 0.8015020822223864\n",
      "Epoch 23, Training Loss: 0.8017079183929845\n",
      "Epoch 24, Training Loss: 0.8008709121019321\n",
      "Epoch 25, Training Loss: 0.8008900160179999\n",
      "Epoch 26, Training Loss: 0.7997340131523017\n",
      "Epoch 27, Training Loss: 0.8010541601288588\n",
      "Epoch 28, Training Loss: 0.8002918946115594\n",
      "Epoch 29, Training Loss: 0.8006466438895777\n",
      "Epoch 30, Training Loss: 0.800600578731164\n",
      "Epoch 31, Training Loss: 0.8000474552462872\n",
      "Epoch 32, Training Loss: 0.8000456266385272\n",
      "Epoch 33, Training Loss: 0.7998670087721115\n",
      "Epoch 34, Training Loss: 0.8000403477285142\n",
      "Epoch 35, Training Loss: 0.7988348393063797\n",
      "Epoch 36, Training Loss: 0.7993436815147114\n",
      "Epoch 37, Training Loss: 0.7998220849754218\n",
      "Epoch 38, Training Loss: 0.7991152012258544\n",
      "Epoch 39, Training Loss: 0.7994117628362842\n",
      "Epoch 40, Training Loss: 0.8000958858576036\n",
      "Epoch 41, Training Loss: 0.7989855193105856\n",
      "Epoch 42, Training Loss: 0.7996424821086396\n",
      "Epoch 43, Training Loss: 0.7989355445804452\n",
      "Epoch 44, Training Loss: 0.7988315778567379\n",
      "Epoch 45, Training Loss: 0.7993527322783506\n",
      "Epoch 46, Training Loss: 0.799312004290129\n",
      "Epoch 47, Training Loss: 0.7994690309789844\n",
      "Epoch 48, Training Loss: 0.799235715364155\n",
      "Epoch 49, Training Loss: 0.7996773112985425\n",
      "Epoch 50, Training Loss: 0.7986892781759564\n",
      "Epoch 51, Training Loss: 0.7994867644811932\n",
      "Epoch 52, Training Loss: 0.7988964350599992\n",
      "Epoch 53, Training Loss: 0.798710851023968\n",
      "Epoch 54, Training Loss: 0.7990096421170055\n",
      "Epoch 55, Training Loss: 0.8001565831944458\n",
      "Epoch 56, Training Loss: 0.799146943164051\n",
      "Epoch 57, Training Loss: 0.7988611093141083\n",
      "Epoch 58, Training Loss: 0.7989357193609825\n",
      "Epoch 59, Training Loss: 0.7979032725319827\n",
      "Epoch 60, Training Loss: 0.79797634719906\n",
      "Epoch 61, Training Loss: 0.7991838551105414\n",
      "Epoch 62, Training Loss: 0.798400168311327\n",
      "Epoch 63, Training Loss: 0.7987823503358024\n",
      "Epoch 64, Training Loss: 0.7983458197206483\n",
      "Epoch 65, Training Loss: 0.7984016202446214\n",
      "Epoch 66, Training Loss: 0.7977038614731983\n",
      "Epoch 67, Training Loss: 0.7986159610569029\n",
      "Epoch 68, Training Loss: 0.7976854931142994\n",
      "Epoch 69, Training Loss: 0.7978028708382656\n",
      "Epoch 70, Training Loss: 0.7987214709583081\n",
      "Epoch 71, Training Loss: 0.7986256036543309\n",
      "Epoch 72, Training Loss: 0.7985122832140528\n",
      "Epoch 73, Training Loss: 0.7979200992817269\n",
      "Epoch 74, Training Loss: 0.7974635649444466\n",
      "Epoch 75, Training Loss: 0.798082897448002\n",
      "Epoch 76, Training Loss: 0.797776747870266\n",
      "Epoch 77, Training Loss: 0.7983057410197151\n",
      "Epoch 78, Training Loss: 0.7985230569552658\n",
      "Epoch 79, Training Loss: 0.7980752047739531\n",
      "Epoch 80, Training Loss: 0.7986350260282817\n",
      "Epoch 81, Training Loss: 0.7977764252881359\n",
      "Epoch 82, Training Loss: 0.7980401217489315\n",
      "Epoch 83, Training Loss: 0.7980847489564938\n",
      "Epoch 84, Training Loss: 0.7978911547732532\n",
      "Epoch 85, Training Loss: 0.7984633740625884\n",
      "Epoch 86, Training Loss: 0.798037860805827\n",
      "Epoch 87, Training Loss: 0.7975046752090741\n",
      "Epoch 88, Training Loss: 0.7974880689068844\n",
      "Epoch 89, Training Loss: 0.7977948514142431\n",
      "Epoch 90, Training Loss: 0.7980516360218364\n",
      "Epoch 91, Training Loss: 0.7979722181657203\n",
      "Epoch 92, Training Loss: 0.797919800138115\n",
      "Epoch 93, Training Loss: 0.7973197995271898\n",
      "Epoch 94, Training Loss: 0.7978540836420275\n",
      "Epoch 95, Training Loss: 0.7980182825174547\n",
      "Epoch 96, Training Loss: 0.7979318554240061\n",
      "Epoch 97, Training Loss: 0.7977952822706753\n",
      "Epoch 98, Training Loss: 0.7982895732822275\n",
      "Epoch 99, Training Loss: 0.7975830481464702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:11:29,714] Trial 298 finished with value: 0.6224 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.798190666858415\n",
      "Epoch 1, Training Loss: 1.0428913827503428\n",
      "Epoch 2, Training Loss: 0.9463446757372688\n",
      "Epoch 3, Training Loss: 0.9211552082089817\n",
      "Epoch 4, Training Loss: 0.9009649333533118\n",
      "Epoch 5, Training Loss: 0.8647080464924083\n",
      "Epoch 6, Training Loss: 0.8228717715599958\n",
      "Epoch 7, Training Loss: 0.8115774231097278\n",
      "Epoch 8, Training Loss: 0.8073405261600719\n",
      "Epoch 9, Training Loss: 0.8052834691019619\n",
      "Epoch 10, Training Loss: 0.8036431947175194\n",
      "Epoch 11, Training Loss: 0.8024814126070808\n",
      "Epoch 12, Training Loss: 0.8009775985689724\n",
      "Epoch 13, Training Loss: 0.8000224623960607\n",
      "Epoch 14, Training Loss: 0.798918005438412\n",
      "Epoch 15, Training Loss: 0.7982130234381731\n",
      "Epoch 16, Training Loss: 0.7977881154593299\n",
      "Epoch 17, Training Loss: 0.7972357033280766\n",
      "Epoch 18, Training Loss: 0.7958807735583362\n",
      "Epoch 19, Training Loss: 0.7952675152526182\n",
      "Epoch 20, Training Loss: 0.7945521262112786\n",
      "Epoch 21, Training Loss: 0.7941713409564074\n",
      "Epoch 22, Training Loss: 0.7932515673777636\n",
      "Epoch 23, Training Loss: 0.7927068084828994\n",
      "Epoch 24, Training Loss: 0.7923447756206288\n",
      "Epoch 25, Training Loss: 0.7917514070342568\n",
      "Epoch 26, Training Loss: 0.7909862009216757\n",
      "Epoch 27, Training Loss: 0.7903635384054745\n",
      "Epoch 28, Training Loss: 0.7897165345444399\n",
      "Epoch 29, Training Loss: 0.7891516070506152\n",
      "Epoch 30, Training Loss: 0.7889197537478279\n",
      "Epoch 31, Training Loss: 0.7881005464581883\n",
      "Epoch 32, Training Loss: 0.7876700715457692\n",
      "Epoch 33, Training Loss: 0.7868975754345164\n",
      "Epoch 34, Training Loss: 0.7870112953466527\n",
      "Epoch 35, Training Loss: 0.786519912481308\n",
      "Epoch 36, Training Loss: 0.7859724164710326\n",
      "Epoch 37, Training Loss: 0.785648566975313\n",
      "Epoch 38, Training Loss: 0.7856760133014006\n",
      "Epoch 39, Training Loss: 0.7854371917949003\n",
      "Epoch 40, Training Loss: 0.7851673924922943\n",
      "Epoch 41, Training Loss: 0.7846837831945981\n",
      "Epoch 42, Training Loss: 0.7844179517381331\n",
      "Epoch 43, Training Loss: 0.7840928031416501\n",
      "Epoch 44, Training Loss: 0.7840619676954605\n",
      "Epoch 45, Training Loss: 0.7841714536442476\n",
      "Epoch 46, Training Loss: 0.7837717389359193\n",
      "Epoch 47, Training Loss: 0.783492281366797\n",
      "Epoch 48, Training Loss: 0.7833558919149287\n",
      "Epoch 49, Training Loss: 0.7834288027006037\n",
      "Epoch 50, Training Loss: 0.7828979749539319\n",
      "Epoch 51, Training Loss: 0.7831009613065159\n",
      "Epoch 52, Training Loss: 0.7826717476283803\n",
      "Epoch 53, Training Loss: 0.7827174911078285\n",
      "Epoch 54, Training Loss: 0.7827567258301903\n",
      "Epoch 55, Training Loss: 0.7825821308528675\n",
      "Epoch 56, Training Loss: 0.7825838846318862\n",
      "Epoch 57, Training Loss: 0.782446744371863\n",
      "Epoch 58, Training Loss: 0.7824768004697912\n",
      "Epoch 59, Training Loss: 0.782158011969398\n",
      "Epoch 60, Training Loss: 0.7821893484452191\n",
      "Epoch 61, Training Loss: 0.7819391188200783\n",
      "Epoch 62, Training Loss: 0.7820084479275872\n",
      "Epoch 63, Training Loss: 0.7817274401468389\n",
      "Epoch 64, Training Loss: 0.7813583371919745\n",
      "Epoch 65, Training Loss: 0.78153488166192\n",
      "Epoch 66, Training Loss: 0.7814799084382898\n",
      "Epoch 67, Training Loss: 0.7817297129771289\n",
      "Epoch 68, Training Loss: 0.7814469359902775\n",
      "Epoch 69, Training Loss: 0.7814069023553063\n",
      "Epoch 70, Training Loss: 0.7814755540735582\n",
      "Epoch 71, Training Loss: 0.7814012432098388\n",
      "Epoch 72, Training Loss: 0.7812932304073783\n",
      "Epoch 73, Training Loss: 0.78112084255499\n",
      "Epoch 74, Training Loss: 0.7809618535462548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:12:57,043] Trial 299 finished with value: 0.6412 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7809874867691713\n",
      "Epoch 1, Training Loss: 0.8464227196048287\n",
      "Epoch 2, Training Loss: 0.8130696446054122\n",
      "Epoch 3, Training Loss: 0.8088963916722466\n",
      "Epoch 4, Training Loss: 0.8085810148014742\n",
      "Epoch 5, Training Loss: 0.8080734403694377\n",
      "Epoch 6, Training Loss: 0.8062436504924999\n",
      "Epoch 7, Training Loss: 0.8032354983161477\n",
      "Epoch 8, Training Loss: 0.8033915748315699\n",
      "Epoch 9, Training Loss: 0.8032878260752734\n",
      "Epoch 10, Training Loss: 0.8023476511590621\n",
      "Epoch 11, Training Loss: 0.8036807671014\n",
      "Epoch 12, Training Loss: 0.7999531433161567\n",
      "Epoch 13, Training Loss: 0.800348952587913\n",
      "Epoch 14, Training Loss: 0.8014144537028144\n",
      "Epoch 15, Training Loss: 0.8019515348182005\n",
      "Epoch 16, Training Loss: 0.8007970690727234\n",
      "Epoch 17, Training Loss: 0.8004814627591301\n",
      "Epoch 18, Training Loss: 0.8006761276721954\n",
      "Epoch 19, Training Loss: 0.8005878722667694\n",
      "Epoch 20, Training Loss: 0.8005654240355772\n",
      "Epoch 21, Training Loss: 0.7998284322374007\n",
      "Epoch 22, Training Loss: 0.7995934402942657\n",
      "Epoch 23, Training Loss: 0.7989416841899647\n",
      "Epoch 24, Training Loss: 0.7997266185283661\n",
      "Epoch 25, Training Loss: 0.799613110037411\n",
      "Epoch 26, Training Loss: 0.7983158577890958\n",
      "Epoch 27, Training Loss: 0.7999540743407081\n",
      "Epoch 28, Training Loss: 0.7996099886473488\n",
      "Epoch 29, Training Loss: 0.7985895912787494\n",
      "Epoch 30, Training Loss: 0.7990958838603076\n",
      "Epoch 31, Training Loss: 0.7999723871315226\n",
      "Epoch 32, Training Loss: 0.7986584535065819\n",
      "Epoch 33, Training Loss: 0.7981357491717619\n",
      "Epoch 34, Training Loss: 0.7984647327310899\n",
      "Epoch 35, Training Loss: 0.7982474643342635\n",
      "Epoch 36, Training Loss: 0.7982025512527017\n",
      "Epoch 37, Training Loss: 0.7989656240098617\n",
      "Epoch 38, Training Loss: 0.7972701705203337\n",
      "Epoch 39, Training Loss: 0.7999677184750053\n",
      "Epoch 40, Training Loss: 0.797353426989387\n",
      "Epoch 41, Training Loss: 0.797229247934678\n",
      "Epoch 42, Training Loss: 0.7976694320931154\n",
      "Epoch 43, Training Loss: 0.7976237516543444\n",
      "Epoch 44, Training Loss: 0.7984226155281067\n",
      "Epoch 45, Training Loss: 0.7973572379701278\n",
      "Epoch 46, Training Loss: 0.7981544617344352\n",
      "Epoch 47, Training Loss: 0.7979552165199728\n",
      "Epoch 48, Training Loss: 0.7968035296832814\n",
      "Epoch 49, Training Loss: 0.7975250897688024\n",
      "Epoch 50, Training Loss: 0.7973874917451074\n",
      "Epoch 51, Training Loss: 0.7979532488654641\n",
      "Epoch 52, Training Loss: 0.797222348241245\n",
      "Epoch 53, Training Loss: 0.7974097709796008\n",
      "Epoch 54, Training Loss: 0.7969734898034264\n",
      "Epoch 55, Training Loss: 0.7970498203530031\n",
      "Epoch 56, Training Loss: 0.7986254731346579\n",
      "Epoch 57, Training Loss: 0.7987895928410923\n",
      "Epoch 58, Training Loss: 0.798294922393911\n",
      "Epoch 59, Training Loss: 0.7968812216029447\n",
      "Epoch 60, Training Loss: 0.7981125635960523\n",
      "Epoch 61, Training Loss: 0.7969839401104871\n",
      "Epoch 62, Training Loss: 0.7972204751828138\n",
      "Epoch 63, Training Loss: 0.7976236905771144\n",
      "Epoch 64, Training Loss: 0.7974657157589408\n",
      "Epoch 65, Training Loss: 0.798025895357132\n",
      "Epoch 66, Training Loss: 0.7974075226923999\n",
      "Epoch 67, Training Loss: 0.7969717672993155\n",
      "Epoch 68, Training Loss: 0.7975168402054731\n",
      "Epoch 69, Training Loss: 0.7980188139747171\n",
      "Epoch 70, Training Loss: 0.7975569723634159\n",
      "Epoch 71, Training Loss: 0.7966165158327888\n",
      "Epoch 72, Training Loss: 0.797982365103329\n",
      "Epoch 73, Training Loss: 0.7978915779029622\n",
      "Epoch 74, Training Loss: 0.7974784961868735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:14:38,894] Trial 300 finished with value: 0.6364 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7977107949116651\n",
      "Epoch 1, Training Loss: 0.9394689950522255\n",
      "Epoch 2, Training Loss: 0.8610779862544116\n",
      "Epoch 3, Training Loss: 0.82421493880889\n",
      "Epoch 4, Training Loss: 0.8146344046732958\n",
      "Epoch 5, Training Loss: 0.8114926022641799\n",
      "Epoch 6, Training Loss: 0.8101662976601545\n",
      "Epoch 7, Training Loss: 0.8093505073996151\n",
      "Epoch 8, Training Loss: 0.808275415616877\n",
      "Epoch 9, Training Loss: 0.8075172834536608\n",
      "Epoch 10, Training Loss: 0.8071605816308189\n",
      "Epoch 11, Training Loss: 0.8064139105993159\n",
      "Epoch 12, Training Loss: 0.805771678686142\n",
      "Epoch 13, Training Loss: 0.8057548386209151\n",
      "Epoch 14, Training Loss: 0.8052471462418052\n",
      "Epoch 15, Training Loss: 0.8050692143159754\n",
      "Epoch 16, Training Loss: 0.8045140370200662\n",
      "Epoch 17, Training Loss: 0.8043161606087404\n",
      "Epoch 18, Training Loss: 0.8039307707898757\n",
      "Epoch 19, Training Loss: 0.8037866987200344\n",
      "Epoch 20, Training Loss: 0.8034450089931489\n",
      "Epoch 21, Training Loss: 0.8027387598682852\n",
      "Epoch 22, Training Loss: 0.8030496276827419\n",
      "Epoch 23, Training Loss: 0.8027518400023965\n",
      "Epoch 24, Training Loss: 0.8022903427656959\n",
      "Epoch 25, Training Loss: 0.8019634051883922\n",
      "Epoch 26, Training Loss: 0.8022626770243925\n",
      "Epoch 27, Training Loss: 0.8016150532750522\n",
      "Epoch 28, Training Loss: 0.8012923395633698\n",
      "Epoch 29, Training Loss: 0.8011875065635232\n",
      "Epoch 30, Training Loss: 0.8011189739143147\n",
      "Epoch 31, Training Loss: 0.8008891329344581\n",
      "Epoch 32, Training Loss: 0.8010620416613186\n",
      "Epoch 33, Training Loss: 0.8007858387161704\n",
      "Epoch 34, Training Loss: 0.8008919143676758\n",
      "Epoch 35, Training Loss: 0.8003662202638738\n",
      "Epoch 36, Training Loss: 0.8005088477274951\n",
      "Epoch 37, Training Loss: 0.8002459437706891\n",
      "Epoch 38, Training Loss: 0.7999460723119624\n",
      "Epoch 39, Training Loss: 0.8001910232095157\n",
      "Epoch 40, Training Loss: 0.8000791891883401\n",
      "Epoch 41, Training Loss: 0.7997276531948763\n",
      "Epoch 42, Training Loss: 0.7997932133253883\n",
      "Epoch 43, Training Loss: 0.7993784769142376\n",
      "Epoch 44, Training Loss: 0.7997575153322781\n",
      "Epoch 45, Training Loss: 0.7993611640088698\n",
      "Epoch 46, Training Loss: 0.7993526904021993\n",
      "Epoch 47, Training Loss: 0.7987691025172963\n",
      "Epoch 48, Training Loss: 0.7990677616876715\n",
      "Epoch 49, Training Loss: 0.7991739432250752\n",
      "Epoch 50, Training Loss: 0.798745525374132\n",
      "Epoch 51, Training Loss: 0.7988832475157345\n",
      "Epoch 52, Training Loss: 0.7988593379188986\n",
      "Epoch 53, Training Loss: 0.7987228034524356\n",
      "Epoch 54, Training Loss: 0.7985139757044175\n",
      "Epoch 55, Training Loss: 0.7986260918308706\n",
      "Epoch 56, Training Loss: 0.7986682774038876\n",
      "Epoch 57, Training Loss: 0.7984176606991712\n",
      "Epoch 58, Training Loss: 0.7986561449135051\n",
      "Epoch 59, Training Loss: 0.7980619137427386\n",
      "Epoch 60, Training Loss: 0.7980505068161908\n",
      "Epoch 61, Training Loss: 0.798348666008781\n",
      "Epoch 62, Training Loss: 0.7984999543779037\n",
      "Epoch 63, Training Loss: 0.7981038987636566\n",
      "Epoch 64, Training Loss: 0.7983521059681388\n",
      "Epoch 65, Training Loss: 0.7980487052833333\n",
      "Epoch 66, Training Loss: 0.797953985859366\n",
      "Epoch 67, Training Loss: 0.7982220983505249\n",
      "Epoch 68, Training Loss: 0.7980038421294269\n",
      "Epoch 69, Training Loss: 0.7984513207042918\n",
      "Epoch 70, Training Loss: 0.7978885326665991\n",
      "Epoch 71, Training Loss: 0.7976906439837288\n",
      "Epoch 72, Training Loss: 0.7979497340847465\n",
      "Epoch 73, Training Loss: 0.7980553807230557\n",
      "Epoch 74, Training Loss: 0.7977039802775664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:16:08,311] Trial 301 finished with value: 0.6322666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7975918718646554\n",
      "Epoch 1, Training Loss: 0.8679579127103763\n",
      "Epoch 2, Training Loss: 0.8245097018722305\n",
      "Epoch 3, Training Loss: 0.8193053476792529\n",
      "Epoch 4, Training Loss: 0.8173264029330777\n",
      "Epoch 5, Training Loss: 0.8141778409929203\n",
      "Epoch 6, Training Loss: 0.8123685999920494\n",
      "Epoch 7, Training Loss: 0.8116901712310045\n",
      "Epoch 8, Training Loss: 0.8099677270516417\n",
      "Epoch 9, Training Loss: 0.8086699980542176\n",
      "Epoch 10, Training Loss: 0.8082987783546735\n",
      "Epoch 11, Training Loss: 0.8076443837101298\n",
      "Epoch 12, Training Loss: 0.8070681908076867\n",
      "Epoch 13, Training Loss: 0.8078767112323216\n",
      "Epoch 14, Training Loss: 0.8061508423403689\n",
      "Epoch 15, Training Loss: 0.8052000323632607\n",
      "Epoch 16, Training Loss: 0.8048884361310112\n",
      "Epoch 17, Training Loss: 0.8047536952154977\n",
      "Epoch 18, Training Loss: 0.8035790137778548\n",
      "Epoch 19, Training Loss: 0.8049595173140217\n",
      "Epoch 20, Training Loss: 0.803417348234277\n",
      "Epoch 21, Training Loss: 0.8038702962093783\n",
      "Epoch 22, Training Loss: 0.8035355469337979\n",
      "Epoch 23, Training Loss: 0.8022585931577181\n",
      "Epoch 24, Training Loss: 0.8037669658660889\n",
      "Epoch 25, Training Loss: 0.8022564641515115\n",
      "Epoch 26, Training Loss: 0.801655744251452\n",
      "Epoch 27, Training Loss: 0.8018496903261744\n",
      "Epoch 28, Training Loss: 0.8023748335981727\n",
      "Epoch 29, Training Loss: 0.8028008800700195\n",
      "Epoch 30, Training Loss: 0.8022052534540793\n",
      "Epoch 31, Training Loss: 0.8019312711586629\n",
      "Epoch 32, Training Loss: 0.801098202404223\n",
      "Epoch 33, Training Loss: 0.8007530614845735\n",
      "Epoch 34, Training Loss: 0.8010739454649445\n",
      "Epoch 35, Training Loss: 0.8017191328500447\n",
      "Epoch 36, Training Loss: 0.8012375706120541\n",
      "Epoch 37, Training Loss: 0.8014636401843308\n",
      "Epoch 38, Training Loss: 0.8004661725876027\n",
      "Epoch 39, Training Loss: 0.79996968445025\n",
      "Epoch 40, Training Loss: 0.800981046651539\n",
      "Epoch 41, Training Loss: 0.800136061001541\n",
      "Epoch 42, Training Loss: 0.8004450052304375\n",
      "Epoch 43, Training Loss: 0.7999233069276451\n",
      "Epoch 44, Training Loss: 0.801371691639262\n",
      "Epoch 45, Training Loss: 0.800982245556394\n",
      "Epoch 46, Training Loss: 0.8002688251939931\n",
      "Epoch 47, Training Loss: 0.80068241671512\n",
      "Epoch 48, Training Loss: 0.8000597309349174\n",
      "Epoch 49, Training Loss: 0.7999130361062243\n",
      "Epoch 50, Training Loss: 0.8004921015940214\n",
      "Epoch 51, Training Loss: 0.7995140069409421\n",
      "Epoch 52, Training Loss: 0.8006585781735586\n",
      "Epoch 53, Training Loss: 0.7999671207334762\n",
      "Epoch 54, Training Loss: 0.799802607611606\n",
      "Epoch 55, Training Loss: 0.8002638249468983\n",
      "Epoch 56, Training Loss: 0.8000624138609808\n",
      "Epoch 57, Training Loss: 0.7998398404372365\n",
      "Epoch 58, Training Loss: 0.7997688503193676\n",
      "Epoch 59, Training Loss: 0.7997856932475155\n",
      "Epoch 60, Training Loss: 0.7997091950330519\n",
      "Epoch 61, Training Loss: 0.7989766476745892\n",
      "Epoch 62, Training Loss: 0.7995527522008222\n",
      "Epoch 63, Training Loss: 0.7996344358401191\n",
      "Epoch 64, Training Loss: 0.7999022959766532\n",
      "Epoch 65, Training Loss: 0.7994400343500582\n",
      "Epoch 66, Training Loss: 0.8001884887092992\n",
      "Epoch 67, Training Loss: 0.8003644789968218\n",
      "Epoch 68, Training Loss: 0.7996053662515225\n",
      "Epoch 69, Training Loss: 0.7996362981043363\n",
      "Epoch 70, Training Loss: 0.7991150101772825\n",
      "Epoch 71, Training Loss: 0.799375810121235\n",
      "Epoch 72, Training Loss: 0.7988109308077876\n",
      "Epoch 73, Training Loss: 0.7988260275439212\n",
      "Epoch 74, Training Loss: 0.7995365093525191\n",
      "Epoch 75, Training Loss: 0.7995326452685478\n",
      "Epoch 76, Training Loss: 0.7986555107554099\n",
      "Epoch 77, Training Loss: 0.7992201170526949\n",
      "Epoch 78, Training Loss: 0.7988991203164696\n",
      "Epoch 79, Training Loss: 0.7987977309334547\n",
      "Epoch 80, Training Loss: 0.8002082445567712\n",
      "Epoch 81, Training Loss: 0.7987142164904372\n",
      "Epoch 82, Training Loss: 0.7988608683858599\n",
      "Epoch 83, Training Loss: 0.7980043214962895\n",
      "Epoch 84, Training Loss: 0.798654347971866\n",
      "Epoch 85, Training Loss: 0.7989442543875902\n",
      "Epoch 86, Training Loss: 0.7988450366751592\n",
      "Epoch 87, Training Loss: 0.7994100835090293\n",
      "Epoch 88, Training Loss: 0.7984079139573234\n",
      "Epoch 89, Training Loss: 0.7987999719784672\n",
      "Epoch 90, Training Loss: 0.7994164362885898\n",
      "Epoch 91, Training Loss: 0.7984560613345383\n",
      "Epoch 92, Training Loss: 0.7984337149706102\n",
      "Epoch 93, Training Loss: 0.7983218720532897\n",
      "Epoch 94, Training Loss: 0.7995400881408748\n",
      "Epoch 95, Training Loss: 0.7989977752355705\n",
      "Epoch 96, Training Loss: 0.7987659654222933\n",
      "Epoch 97, Training Loss: 0.7990247577652896\n",
      "Epoch 98, Training Loss: 0.7998780911130117\n",
      "Epoch 99, Training Loss: 0.7993110821659404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:17:41,225] Trial 302 finished with value: 0.588 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7989668032280485\n",
      "Epoch 1, Training Loss: 1.0102534240834853\n",
      "Epoch 2, Training Loss: 0.9319770208527061\n",
      "Epoch 3, Training Loss: 0.8827954480227302\n",
      "Epoch 4, Training Loss: 0.8243891354869394\n",
      "Epoch 5, Training Loss: 0.8144138639814713\n",
      "Epoch 6, Training Loss: 0.8117777905043434\n",
      "Epoch 7, Training Loss: 0.810661420471528\n",
      "Epoch 8, Training Loss: 0.8092786779123194\n",
      "Epoch 9, Training Loss: 0.8089511182728936\n",
      "Epoch 10, Training Loss: 0.806897307213615\n",
      "Epoch 11, Training Loss: 0.8066373211496016\n",
      "Epoch 12, Training Loss: 0.8056244469390196\n",
      "Epoch 13, Training Loss: 0.8045198352897869\n",
      "Epoch 14, Training Loss: 0.8041287430594949\n",
      "Epoch 15, Training Loss: 0.8031141716592453\n",
      "Epoch 16, Training Loss: 0.8030874064389397\n",
      "Epoch 17, Training Loss: 0.8022890525004444\n",
      "Epoch 18, Training Loss: 0.8019876340557547\n",
      "Epoch 19, Training Loss: 0.8009833637405844\n",
      "Epoch 20, Training Loss: 0.8015728190365959\n",
      "Epoch 21, Training Loss: 0.8010559826738695\n",
      "Epoch 22, Training Loss: 0.8011823935368482\n",
      "Epoch 23, Training Loss: 0.799849894397399\n",
      "Epoch 24, Training Loss: 0.8007659970311558\n",
      "Epoch 25, Training Loss: 0.8003992960733526\n",
      "Epoch 26, Training Loss: 0.7996218553010155\n",
      "Epoch 27, Training Loss: 0.7991254278491525\n",
      "Epoch 28, Training Loss: 0.7993687678084654\n",
      "Epoch 29, Training Loss: 0.7991623490698198\n",
      "Epoch 30, Training Loss: 0.7993361089510076\n",
      "Epoch 31, Training Loss: 0.7989054900057175\n",
      "Epoch 32, Training Loss: 0.7993317937149721\n",
      "Epoch 33, Training Loss: 0.798297841128181\n",
      "Epoch 34, Training Loss: 0.7980196147806504\n",
      "Epoch 35, Training Loss: 0.7977967349921956\n",
      "Epoch 36, Training Loss: 0.7976392649903017\n",
      "Epoch 37, Training Loss: 0.7972584967052235\n",
      "Epoch 38, Training Loss: 0.7972493020927205\n",
      "Epoch 39, Training Loss: 0.7967228367749383\n",
      "Epoch 40, Training Loss: 0.7968918490409851\n",
      "Epoch 41, Training Loss: 0.796052744809319\n",
      "Epoch 42, Training Loss: 0.7969910468774684\n",
      "Epoch 43, Training Loss: 0.7965184366703033\n",
      "Epoch 44, Training Loss: 0.7964151158753563\n",
      "Epoch 45, Training Loss: 0.7960858265792622\n",
      "Epoch 46, Training Loss: 0.795874530918458\n",
      "Epoch 47, Training Loss: 0.7959943929840537\n",
      "Epoch 48, Training Loss: 0.7951535216499778\n",
      "Epoch 49, Training Loss: 0.7958332586288452\n",
      "Epoch 50, Training Loss: 0.7954005255418666\n",
      "Epoch 51, Training Loss: 0.7953368755649118\n",
      "Epoch 52, Training Loss: 0.7947414866615744\n",
      "Epoch 53, Training Loss: 0.7946981270874248\n",
      "Epoch 54, Training Loss: 0.7941576982946957\n",
      "Epoch 55, Training Loss: 0.7939003727015327\n",
      "Epoch 56, Training Loss: 0.7935212624774259\n",
      "Epoch 57, Training Loss: 0.7938938630328459\n",
      "Epoch 58, Training Loss: 0.7930278356636272\n",
      "Epoch 59, Training Loss: 0.7922608154661516\n",
      "Epoch 60, Training Loss: 0.7918238221897799\n",
      "Epoch 61, Training Loss: 0.7916422615331762\n",
      "Epoch 62, Training Loss: 0.7908485467293683\n",
      "Epoch 63, Training Loss: 0.7912018923198476\n",
      "Epoch 64, Training Loss: 0.7906063547555138\n",
      "Epoch 65, Training Loss: 0.7901125322369968\n",
      "Epoch 66, Training Loss: 0.7896750682943008\n",
      "Epoch 67, Training Loss: 0.7896476824143354\n",
      "Epoch 68, Training Loss: 0.7886357204353108\n",
      "Epoch 69, Training Loss: 0.7888922075664296\n",
      "Epoch 70, Training Loss: 0.788715695142746\n",
      "Epoch 71, Training Loss: 0.7880268364092883\n",
      "Epoch 72, Training Loss: 0.788256236595266\n",
      "Epoch 73, Training Loss: 0.7877028294871835\n",
      "Epoch 74, Training Loss: 0.7874894509595983\n",
      "Epoch 75, Training Loss: 0.7872287896100213\n",
      "Epoch 76, Training Loss: 0.7869532517825856\n",
      "Epoch 77, Training Loss: 0.7870085211361155\n",
      "Epoch 78, Training Loss: 0.7871029391709496\n",
      "Epoch 79, Training Loss: 0.7864592388798208\n",
      "Epoch 80, Training Loss: 0.7867129870723275\n",
      "Epoch 81, Training Loss: 0.7861634412933799\n",
      "Epoch 82, Training Loss: 0.7864610799621133\n",
      "Epoch 83, Training Loss: 0.7860783999106463\n",
      "Epoch 84, Training Loss: 0.7864433696690728\n",
      "Epoch 85, Training Loss: 0.7858439144667457\n",
      "Epoch 86, Training Loss: 0.7858483752783607\n",
      "Epoch 87, Training Loss: 0.785662603448419\n",
      "Epoch 88, Training Loss: 0.785468580863055\n",
      "Epoch 89, Training Loss: 0.7851978506060208\n",
      "Epoch 90, Training Loss: 0.7854914419090047\n",
      "Epoch 91, Training Loss: 0.7856208367207471\n",
      "Epoch 92, Training Loss: 0.7855180174462936\n",
      "Epoch 93, Training Loss: 0.7848784123448764\n",
      "Epoch 94, Training Loss: 0.7856375778422636\n",
      "Epoch 95, Training Loss: 0.7849305627626532\n",
      "Epoch 96, Training Loss: 0.7851312835076276\n",
      "Epoch 97, Training Loss: 0.7850477975256303\n",
      "Epoch 98, Training Loss: 0.784882792374667\n",
      "Epoch 99, Training Loss: 0.7849026562185848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:20:08,780] Trial 303 finished with value: 0.6394 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7848459232554716\n",
      "Epoch 1, Training Loss: 0.8686829030961919\n",
      "Epoch 2, Training Loss: 0.814899682460871\n",
      "Epoch 3, Training Loss: 0.8089273069137918\n",
      "Epoch 4, Training Loss: 0.8054530542147788\n",
      "Epoch 5, Training Loss: 0.8016599645291952\n",
      "Epoch 6, Training Loss: 0.7982908751731529\n",
      "Epoch 7, Training Loss: 0.7963683526318772\n",
      "Epoch 8, Training Loss: 0.7961606634290594\n",
      "Epoch 9, Training Loss: 0.7942520925873204\n",
      "Epoch 10, Training Loss: 0.7942097252472899\n",
      "Epoch 11, Training Loss: 0.7928138763384711\n",
      "Epoch 12, Training Loss: 0.7932427023586474\n",
      "Epoch 13, Training Loss: 0.7929803378599927\n",
      "Epoch 14, Training Loss: 0.7923285746036616\n",
      "Epoch 15, Training Loss: 0.7917103029731521\n",
      "Epoch 16, Training Loss: 0.7917484580126024\n",
      "Epoch 17, Training Loss: 0.7908496559114384\n",
      "Epoch 18, Training Loss: 0.7906167936504336\n",
      "Epoch 19, Training Loss: 0.7900315170897577\n",
      "Epoch 20, Training Loss: 0.7903156993980694\n",
      "Epoch 21, Training Loss: 0.7899945809428853\n",
      "Epoch 22, Training Loss: 0.7896877897413154\n",
      "Epoch 23, Training Loss: 0.7896813294045011\n",
      "Epoch 24, Training Loss: 0.7887687378359917\n",
      "Epoch 25, Training Loss: 0.7902371636906961\n",
      "Epoch 26, Training Loss: 0.7901335630201756\n",
      "Epoch 27, Training Loss: 0.7887171611750037\n",
      "Epoch 28, Training Loss: 0.7878287595913822\n",
      "Epoch 29, Training Loss: 0.787996056294979\n",
      "Epoch 30, Training Loss: 0.787562108129487\n",
      "Epoch 31, Training Loss: 0.7880768019453923\n",
      "Epoch 32, Training Loss: 0.787460650447616\n",
      "Epoch 33, Training Loss: 0.7875982578535725\n",
      "Epoch 34, Training Loss: 0.7870998692691774\n",
      "Epoch 35, Training Loss: 0.7869208384277229\n",
      "Epoch 36, Training Loss: 0.7863143291688504\n",
      "Epoch 37, Training Loss: 0.7879869538142269\n",
      "Epoch 38, Training Loss: 0.785944926649108\n",
      "Epoch 39, Training Loss: 0.7852330361990104\n",
      "Epoch 40, Training Loss: 0.7868880236059204\n",
      "Epoch 41, Training Loss: 0.7865502629064975\n",
      "Epoch 42, Training Loss: 0.7862728415575243\n",
      "Epoch 43, Training Loss: 0.7858442914217039\n",
      "Epoch 44, Training Loss: 0.7854805962483685\n",
      "Epoch 45, Training Loss: 0.7858386029874472\n",
      "Epoch 46, Training Loss: 0.7849481890972395\n",
      "Epoch 47, Training Loss: 0.7867077334482867\n",
      "Epoch 48, Training Loss: 0.7855048427904459\n",
      "Epoch 49, Training Loss: 0.784321349366267\n",
      "Epoch 50, Training Loss: 0.784947068888442\n",
      "Epoch 51, Training Loss: 0.784696698726568\n",
      "Epoch 52, Training Loss: 0.7845383162785293\n",
      "Epoch 53, Training Loss: 0.7846415863001257\n",
      "Epoch 54, Training Loss: 0.7853033279117785\n",
      "Epoch 55, Training Loss: 0.7846205717638919\n",
      "Epoch 56, Training Loss: 0.7847936212568355\n",
      "Epoch 57, Training Loss: 0.7845411814245066\n",
      "Epoch 58, Training Loss: 0.7853696270096571\n",
      "Epoch 59, Training Loss: 0.7849187567718047\n",
      "Epoch 60, Training Loss: 0.7852156561120112\n",
      "Epoch 61, Training Loss: 0.7833547672831026\n",
      "Epoch 62, Training Loss: 0.7844088776667315\n",
      "Epoch 63, Training Loss: 0.7842496653248493\n",
      "Epoch 64, Training Loss: 0.7833079066491665\n",
      "Epoch 65, Training Loss: 0.7839090824127197\n",
      "Epoch 66, Training Loss: 0.7840042642184666\n",
      "Epoch 67, Training Loss: 0.7836735682379931\n",
      "Epoch 68, Training Loss: 0.7827304516519819\n",
      "Epoch 69, Training Loss: 0.7833763878148301\n",
      "Epoch 70, Training Loss: 0.7835451965045211\n",
      "Epoch 71, Training Loss: 0.7832251023529168\n",
      "Epoch 72, Training Loss: 0.7835988638992596\n",
      "Epoch 73, Training Loss: 0.7832828498424445\n",
      "Epoch 74, Training Loss: 0.7837182788920581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:21:37,502] Trial 304 finished with value: 0.6402 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7829845062772134\n",
      "Epoch 1, Training Loss: 0.8743489770960987\n",
      "Epoch 2, Training Loss: 0.8202720641193534\n",
      "Epoch 3, Training Loss: 0.8133102730700844\n",
      "Epoch 4, Training Loss: 0.8110761625426156\n",
      "Epoch 5, Training Loss: 0.811312412857113\n",
      "Epoch 6, Training Loss: 0.8089916026681886\n",
      "Epoch 7, Training Loss: 0.8085351786219088\n",
      "Epoch 8, Training Loss: 0.807631966942235\n",
      "Epoch 9, Training Loss: 0.8065723115340211\n",
      "Epoch 10, Training Loss: 0.8078072805153697\n",
      "Epoch 11, Training Loss: 0.8069225289767846\n",
      "Epoch 12, Training Loss: 0.8066667875849215\n",
      "Epoch 13, Training Loss: 0.8052223203773785\n",
      "Epoch 14, Training Loss: 0.8044061235019139\n",
      "Epoch 15, Training Loss: 0.8050675520323273\n",
      "Epoch 16, Training Loss: 0.8031016234168433\n",
      "Epoch 17, Training Loss: 0.8039903185421363\n",
      "Epoch 18, Training Loss: 0.8028407874860262\n",
      "Epoch 19, Training Loss: 0.8036072294514879\n",
      "Epoch 20, Training Loss: 0.8018059543200902\n",
      "Epoch 21, Training Loss: 0.8028367572260978\n",
      "Epoch 22, Training Loss: 0.8021348195864741\n",
      "Epoch 23, Training Loss: 0.8026555683379782\n",
      "Epoch 24, Training Loss: 0.8012644199500406\n",
      "Epoch 25, Training Loss: 0.8012983497820403\n",
      "Epoch 26, Training Loss: 0.8010794446880656\n",
      "Epoch 27, Training Loss: 0.8013881946864881\n",
      "Epoch 28, Training Loss: 0.8016235826607038\n",
      "Epoch 29, Training Loss: 0.8007829471638328\n",
      "Epoch 30, Training Loss: 0.8000463441798561\n",
      "Epoch 31, Training Loss: 0.8001948194396227\n",
      "Epoch 32, Training Loss: 0.8001101982324643\n",
      "Epoch 33, Training Loss: 0.800638433058459\n",
      "Epoch 34, Training Loss: 0.8004954798777301\n",
      "Epoch 35, Training Loss: 0.7991126517604168\n",
      "Epoch 36, Training Loss: 0.7979149271671037\n",
      "Epoch 37, Training Loss: 0.797646994653501\n",
      "Epoch 38, Training Loss: 0.7991149620005958\n",
      "Epoch 39, Training Loss: 0.7978077493215862\n",
      "Epoch 40, Training Loss: 0.7978227897694237\n",
      "Epoch 41, Training Loss: 0.7989323776467402\n",
      "Epoch 42, Training Loss: 0.7977177864626834\n",
      "Epoch 43, Training Loss: 0.797060091244547\n",
      "Epoch 44, Training Loss: 0.7965687959714043\n",
      "Epoch 45, Training Loss: 0.7964736001832144\n",
      "Epoch 46, Training Loss: 0.7966627177439238\n",
      "Epoch 47, Training Loss: 0.7961656457499454\n",
      "Epoch 48, Training Loss: 0.7962246851813525\n",
      "Epoch 49, Training Loss: 0.7953888556114713\n",
      "Epoch 50, Training Loss: 0.7940119459664912\n",
      "Epoch 51, Training Loss: 0.7941066075984696\n",
      "Epoch 52, Training Loss: 0.7954466850237739\n",
      "Epoch 53, Training Loss: 0.7952489323185798\n",
      "Epoch 54, Training Loss: 0.7938703019816177\n",
      "Epoch 55, Training Loss: 0.7953920251444766\n",
      "Epoch 56, Training Loss: 0.79426210679506\n",
      "Epoch 57, Training Loss: 0.7944898021848579\n",
      "Epoch 58, Training Loss: 0.793908160969727\n",
      "Epoch 59, Training Loss: 0.794653538384832\n",
      "Epoch 60, Training Loss: 0.794337298726677\n",
      "Epoch 61, Training Loss: 0.7937503348615833\n",
      "Epoch 62, Training Loss: 0.7937213851096935\n",
      "Epoch 63, Training Loss: 0.7935153429669545\n",
      "Epoch 64, Training Loss: 0.7939256417123894\n",
      "Epoch 65, Training Loss: 0.791680175469334\n",
      "Epoch 66, Training Loss: 0.7931489900538796\n",
      "Epoch 67, Training Loss: 0.7940283817456181\n",
      "Epoch 68, Training Loss: 0.7938077958902918\n",
      "Epoch 69, Training Loss: 0.7925499268940517\n",
      "Epoch 70, Training Loss: 0.7922488820283933\n",
      "Epoch 71, Training Loss: 0.793006435910562\n",
      "Epoch 72, Training Loss: 0.7923062912503579\n",
      "Epoch 73, Training Loss: 0.7931556325209769\n",
      "Epoch 74, Training Loss: 0.7917834690638951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:22:50,691] Trial 305 finished with value: 0.6352666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7918556238475599\n",
      "Epoch 1, Training Loss: 1.0731318768332987\n",
      "Epoch 2, Training Loss: 1.0083987860819872\n",
      "Epoch 3, Training Loss: 0.947537160059985\n",
      "Epoch 4, Training Loss: 0.9303258957582362\n",
      "Epoch 5, Training Loss: 0.9207547197622411\n",
      "Epoch 6, Training Loss: 0.910950922054403\n",
      "Epoch 7, Training Loss: 0.8985887913142934\n",
      "Epoch 8, Training Loss: 0.8803449305366068\n",
      "Epoch 9, Training Loss: 0.8527639116960414\n",
      "Epoch 10, Training Loss: 0.8257672598081477\n",
      "Epoch 11, Training Loss: 0.8127361243612626\n",
      "Epoch 12, Training Loss: 0.8089917478841894\n",
      "Epoch 13, Training Loss: 0.8071474972191979\n",
      "Epoch 14, Training Loss: 0.8059104803029229\n",
      "Epoch 15, Training Loss: 0.8050883563125835\n",
      "Epoch 16, Training Loss: 0.8044225717292113\n",
      "Epoch 17, Training Loss: 0.8034412385435665\n",
      "Epoch 18, Training Loss: 0.8029637392829446\n",
      "Epoch 19, Training Loss: 0.8022281648832209\n",
      "Epoch 20, Training Loss: 0.8014285827384275\n",
      "Epoch 21, Training Loss: 0.8008209937460282\n",
      "Epoch 22, Training Loss: 0.800732777890037\n",
      "Epoch 23, Training Loss: 0.800392124933355\n",
      "Epoch 24, Training Loss: 0.7997814968754263\n",
      "Epoch 25, Training Loss: 0.7995168333895066\n",
      "Epoch 26, Training Loss: 0.7992394191377303\n",
      "Epoch 27, Training Loss: 0.7990816337220809\n",
      "Epoch 28, Training Loss: 0.7985803059269401\n",
      "Epoch 29, Training Loss: 0.7985208834620083\n",
      "Epoch 30, Training Loss: 0.7979090006211225\n",
      "Epoch 31, Training Loss: 0.7975006981456981\n",
      "Epoch 32, Training Loss: 0.797125492516686\n",
      "Epoch 33, Training Loss: 0.7966202136348276\n",
      "Epoch 34, Training Loss: 0.7963496632435743\n",
      "Epoch 35, Training Loss: 0.7958623515157138\n",
      "Epoch 36, Training Loss: 0.7954264460591709\n",
      "Epoch 37, Training Loss: 0.7951745340403389\n",
      "Epoch 38, Training Loss: 0.7948333944292629\n",
      "Epoch 39, Training Loss: 0.7945765765274272\n",
      "Epoch 40, Training Loss: 0.7944244114090414\n",
      "Epoch 41, Training Loss: 0.7940521360846127\n",
      "Epoch 42, Training Loss: 0.793272761667476\n",
      "Epoch 43, Training Loss: 0.7930236927901997\n",
      "Epoch 44, Training Loss: 0.7926950974324171\n",
      "Epoch 45, Training Loss: 0.7927797622540418\n",
      "Epoch 46, Training Loss: 0.792456729552325\n",
      "Epoch 47, Training Loss: 0.7923401490379782\n",
      "Epoch 48, Training Loss: 0.7919634645826676\n",
      "Epoch 49, Training Loss: 0.7915847696276272\n",
      "Epoch 50, Training Loss: 0.7912555327836205\n",
      "Epoch 51, Training Loss: 0.7912857791956733\n",
      "Epoch 52, Training Loss: 0.790655836427913\n",
      "Epoch 53, Training Loss: 0.7903061637457679\n",
      "Epoch 54, Training Loss: 0.7903329603111042\n",
      "Epoch 55, Training Loss: 0.7903282794531654\n",
      "Epoch 56, Training Loss: 0.789751470229205\n",
      "Epoch 57, Training Loss: 0.7897372644087848\n",
      "Epoch 58, Training Loss: 0.7894095434862025\n",
      "Epoch 59, Training Loss: 0.7891889686444227\n",
      "Epoch 60, Training Loss: 0.7887835255089928\n",
      "Epoch 61, Training Loss: 0.7884889303936677\n",
      "Epoch 62, Training Loss: 0.7885806456734152\n",
      "Epoch 63, Training Loss: 0.7882129306652966\n",
      "Epoch 64, Training Loss: 0.7880050439694348\n",
      "Epoch 65, Training Loss: 0.7877551021295436\n",
      "Epoch 66, Training Loss: 0.7876638053445255\n",
      "Epoch 67, Training Loss: 0.7875263334021849\n",
      "Epoch 68, Training Loss: 0.7875974756128648\n",
      "Epoch 69, Training Loss: 0.7867445555855246\n",
      "Epoch 70, Training Loss: 0.7868446615163017\n",
      "Epoch 71, Training Loss: 0.7865492017128888\n",
      "Epoch 72, Training Loss: 0.7862407044102164\n",
      "Epoch 73, Training Loss: 0.7865710337723003\n",
      "Epoch 74, Training Loss: 0.7862433062581455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:24:17,217] Trial 306 finished with value: 0.6389333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7860438072681427\n",
      "Epoch 1, Training Loss: 0.9021975029917324\n",
      "Epoch 2, Training Loss: 0.8231313333090614\n",
      "Epoch 3, Training Loss: 0.8154811089880326\n",
      "Epoch 4, Training Loss: 0.8100770948213689\n",
      "Epoch 5, Training Loss: 0.8053003575521357\n",
      "Epoch 6, Training Loss: 0.8010399852079504\n",
      "Epoch 7, Training Loss: 0.7995991350622738\n",
      "Epoch 8, Training Loss: 0.7983478284583372\n",
      "Epoch 9, Training Loss: 0.7962743215701159\n",
      "Epoch 10, Training Loss: 0.7947133377720328\n",
      "Epoch 11, Training Loss: 0.7944231353787815\n",
      "Epoch 12, Training Loss: 0.7933071318794699\n",
      "Epoch 13, Training Loss: 0.7931930064453798\n",
      "Epoch 14, Training Loss: 0.7922923516525942\n",
      "Epoch 15, Training Loss: 0.7918003682529225\n",
      "Epoch 16, Training Loss: 0.7914990763103261\n",
      "Epoch 17, Training Loss: 0.7908464278193081\n",
      "Epoch 18, Training Loss: 0.7899704529958613\n",
      "Epoch 19, Training Loss: 0.7902841680190142\n",
      "Epoch 20, Training Loss: 0.7896998719608083\n",
      "Epoch 21, Training Loss: 0.7893699392150431\n",
      "Epoch 22, Training Loss: 0.7897038187700159\n",
      "Epoch 23, Training Loss: 0.7888388692631441\n",
      "Epoch 24, Training Loss: 0.7884923960180844\n",
      "Epoch 25, Training Loss: 0.7884092779019299\n",
      "Epoch 26, Training Loss: 0.7883551936991074\n",
      "Epoch 27, Training Loss: 0.7877474538718953\n",
      "Epoch 28, Training Loss: 0.7876470080543967\n",
      "Epoch 29, Training Loss: 0.787354211176143\n",
      "Epoch 30, Training Loss: 0.7873625460091759\n",
      "Epoch 31, Training Loss: 0.7869951028683606\n",
      "Epoch 32, Training Loss: 0.7872927219026229\n",
      "Epoch 33, Training Loss: 0.7870959644457873\n",
      "Epoch 34, Training Loss: 0.7866197210900924\n",
      "Epoch 35, Training Loss: 0.7867964623254888\n",
      "Epoch 36, Training Loss: 0.7868586287778967\n",
      "Epoch 37, Training Loss: 0.7863142939174876\n",
      "Epoch 38, Training Loss: 0.7861083671625922\n",
      "Epoch 39, Training Loss: 0.7857786495545331\n",
      "Epoch 40, Training Loss: 0.7853610709835501\n",
      "Epoch 41, Training Loss: 0.7857787157507504\n",
      "Epoch 42, Training Loss: 0.7853576547959271\n",
      "Epoch 43, Training Loss: 0.7859891931449666\n",
      "Epoch 44, Training Loss: 0.7855042928807876\n",
      "Epoch 45, Training Loss: 0.7851007425083834\n",
      "Epoch 46, Training Loss: 0.7845069551467896\n",
      "Epoch 47, Training Loss: 0.7850412182246937\n",
      "Epoch 48, Training Loss: 0.7850461146410774\n",
      "Epoch 49, Training Loss: 0.7845952463851256\n",
      "Epoch 50, Training Loss: 0.7849655185026281\n",
      "Epoch 51, Training Loss: 0.7850390066118801\n",
      "Epoch 52, Training Loss: 0.784531355675529\n",
      "Epoch 53, Training Loss: 0.7844465629493489\n",
      "Epoch 54, Training Loss: 0.784306350034826\n",
      "Epoch 55, Training Loss: 0.7843650175543393\n",
      "Epoch 56, Training Loss: 0.7840307576516096\n",
      "Epoch 57, Training Loss: 0.7844785769546733\n",
      "Epoch 58, Training Loss: 0.784247460715911\n",
      "Epoch 59, Training Loss: 0.7837411529877607\n",
      "Epoch 60, Training Loss: 0.7839526224136353\n",
      "Epoch 61, Training Loss: 0.7840073618468116\n",
      "Epoch 62, Training Loss: 0.7840969744850608\n",
      "Epoch 63, Training Loss: 0.7838102682197795\n",
      "Epoch 64, Training Loss: 0.7835912419767941\n",
      "Epoch 65, Training Loss: 0.7837411687654607\n",
      "Epoch 66, Training Loss: 0.7838185768267688\n",
      "Epoch 67, Training Loss: 0.7833441779893987\n",
      "Epoch 68, Training Loss: 0.7840844380154329\n",
      "Epoch 69, Training Loss: 0.7837388806483325\n",
      "Epoch 70, Training Loss: 0.7828593465159921\n",
      "Epoch 71, Training Loss: 0.7832236555043389\n",
      "Epoch 72, Training Loss: 0.7828459704623503\n",
      "Epoch 73, Training Loss: 0.782994862093645\n",
      "Epoch 74, Training Loss: 0.7827484928159153\n",
      "Epoch 75, Training Loss: 0.783329027821036\n",
      "Epoch 76, Training Loss: 0.7829917646856869\n",
      "Epoch 77, Training Loss: 0.783260984210407\n",
      "Epoch 78, Training Loss: 0.7829461159425624\n",
      "Epoch 79, Training Loss: 0.7828768894251655\n",
      "Epoch 80, Training Loss: 0.7828710868078119\n",
      "Epoch 81, Training Loss: 0.7823744586636039\n",
      "Epoch 82, Training Loss: 0.7824324314734515\n",
      "Epoch 83, Training Loss: 0.7827996727999519\n",
      "Epoch 84, Training Loss: 0.7825673230956582\n",
      "Epoch 85, Training Loss: 0.7829142512994655\n",
      "Epoch 86, Training Loss: 0.7825649813343497\n",
      "Epoch 87, Training Loss: 0.7822008337694056\n",
      "Epoch 88, Training Loss: 0.7821089447245878\n",
      "Epoch 89, Training Loss: 0.7822664859014399\n",
      "Epoch 90, Training Loss: 0.7824361930173986\n",
      "Epoch 91, Training Loss: 0.7820573271022123\n",
      "Epoch 92, Training Loss: 0.7819256237675162\n",
      "Epoch 93, Training Loss: 0.7822150580322041\n",
      "Epoch 94, Training Loss: 0.7820469212532043\n",
      "Epoch 95, Training Loss: 0.7818128690298866\n",
      "Epoch 96, Training Loss: 0.7819438444866854\n",
      "Epoch 97, Training Loss: 0.7819048808602725\n",
      "Epoch 98, Training Loss: 0.781805518655216\n",
      "Epoch 99, Training Loss: 0.7817527582364924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:26:25,067] Trial 307 finished with value: 0.6374666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7814901199060328\n",
      "Epoch 1, Training Loss: 0.9998277368265039\n",
      "Epoch 2, Training Loss: 0.9262841301104602\n",
      "Epoch 3, Training Loss: 0.9058180413526647\n",
      "Epoch 4, Training Loss: 0.8796214757246129\n",
      "Epoch 5, Training Loss: 0.8450801641099593\n",
      "Epoch 6, Training Loss: 0.8190769081255969\n",
      "Epoch 7, Training Loss: 0.8097988423179178\n",
      "Epoch 8, Training Loss: 0.8060675282338087\n",
      "Epoch 9, Training Loss: 0.8045286994120654\n",
      "Epoch 10, Training Loss: 0.8033174609436708\n",
      "Epoch 11, Training Loss: 0.8022917617068571\n",
      "Epoch 12, Training Loss: 0.8017992764360764\n",
      "Epoch 13, Training Loss: 0.8013072907924652\n",
      "Epoch 14, Training Loss: 0.8005421385344337\n",
      "Epoch 15, Training Loss: 0.8001843170558705\n",
      "Epoch 16, Training Loss: 0.7998618968094097\n",
      "Epoch 17, Training Loss: 0.799324366345125\n",
      "Epoch 18, Training Loss: 0.7994082109367147\n",
      "Epoch 19, Training Loss: 0.7988542539231918\n",
      "Epoch 20, Training Loss: 0.7985823969280018\n",
      "Epoch 21, Training Loss: 0.798068757688298\n",
      "Epoch 22, Training Loss: 0.7979201575587778\n",
      "Epoch 23, Training Loss: 0.7978913075082442\n",
      "Epoch 24, Training Loss: 0.797441642074024\n",
      "Epoch 25, Training Loss: 0.7970849513306337\n",
      "Epoch 26, Training Loss: 0.7964193978029139\n",
      "Epoch 27, Training Loss: 0.796305852357079\n",
      "Epoch 28, Training Loss: 0.7963655107161578\n",
      "Epoch 29, Training Loss: 0.7959161726166221\n",
      "Epoch 30, Training Loss: 0.7954706245310166\n",
      "Epoch 31, Training Loss: 0.7948941606633804\n",
      "Epoch 32, Training Loss: 0.7948883326614604\n",
      "Epoch 33, Training Loss: 0.7943618920971366\n",
      "Epoch 34, Training Loss: 0.7942597332421472\n",
      "Epoch 35, Training Loss: 0.7936154155871448\n",
      "Epoch 36, Training Loss: 0.7930249556373148\n",
      "Epoch 37, Training Loss: 0.7930145950177137\n",
      "Epoch 38, Training Loss: 0.7926918097103344\n",
      "Epoch 39, Training Loss: 0.7923628078488743\n",
      "Epoch 40, Training Loss: 0.7921864746598637\n",
      "Epoch 41, Training Loss: 0.7921541038681479\n",
      "Epoch 42, Training Loss: 0.791611115581849\n",
      "Epoch 43, Training Loss: 0.791185494521085\n",
      "Epoch 44, Training Loss: 0.7910002641116871\n",
      "Epoch 45, Training Loss: 0.790394012296901\n",
      "Epoch 46, Training Loss: 0.7901732378146228\n",
      "Epoch 47, Training Loss: 0.7902025817422306\n",
      "Epoch 48, Training Loss: 0.789937235327328\n",
      "Epoch 49, Training Loss: 0.7896075552351335\n",
      "Epoch 50, Training Loss: 0.7890974547582514\n",
      "Epoch 51, Training Loss: 0.7885011986423941\n",
      "Epoch 52, Training Loss: 0.7885143920253305\n",
      "Epoch 53, Training Loss: 0.7884712689063128\n",
      "Epoch 54, Training Loss: 0.7879850025036755\n",
      "Epoch 55, Training Loss: 0.7874488665075863\n",
      "Epoch 56, Training Loss: 0.787648605038138\n",
      "Epoch 57, Training Loss: 0.7872313207037309\n",
      "Epoch 58, Training Loss: 0.7869055479414323\n",
      "Epoch 59, Training Loss: 0.7870273739450118\n",
      "Epoch 60, Training Loss: 0.7865320738624124\n",
      "Epoch 61, Training Loss: 0.7862817248877357\n",
      "Epoch 62, Training Loss: 0.7865880408707787\n",
      "Epoch 63, Training Loss: 0.7862525201545042\n",
      "Epoch 64, Training Loss: 0.7858247943485485\n",
      "Epoch 65, Training Loss: 0.7860181786733516\n",
      "Epoch 66, Training Loss: 0.7856447286465589\n",
      "Epoch 67, Training Loss: 0.7854994943562676\n",
      "Epoch 68, Training Loss: 0.7854012467580683\n",
      "Epoch 69, Training Loss: 0.7855559015274047\n",
      "Epoch 70, Training Loss: 0.7849324142932892\n",
      "Epoch 71, Training Loss: 0.7848382233872133\n",
      "Epoch 72, Training Loss: 0.7847743530133191\n",
      "Epoch 73, Training Loss: 0.784618801018771\n",
      "Epoch 74, Training Loss: 0.7844903326735777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:27:46,903] Trial 308 finished with value: 0.6400666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7843148089857662\n",
      "Epoch 1, Training Loss: 1.1086839110711042\n",
      "Epoch 2, Training Loss: 1.0435092846085043\n",
      "Epoch 3, Training Loss: 1.0111169496003318\n",
      "Epoch 4, Training Loss: 0.992842823266983\n",
      "Epoch 5, Training Loss: 0.9818943917050081\n",
      "Epoch 6, Training Loss: 0.9750388488348792\n",
      "Epoch 7, Training Loss: 0.9705611491904539\n",
      "Epoch 8, Training Loss: 0.967497124671936\n",
      "Epoch 9, Training Loss: 0.9652991739441367\n",
      "Epoch 10, Training Loss: 0.9636242302025065\n",
      "Epoch 11, Training Loss: 0.9622766947045046\n",
      "Epoch 12, Training Loss: 0.9611240554557127\n",
      "Epoch 13, Training Loss: 0.9601033560668721\n",
      "Epoch 14, Training Loss: 0.959160676353118\n",
      "Epoch 15, Training Loss: 0.9582760660087362\n",
      "Epoch 16, Training Loss: 0.9574226424974553\n",
      "Epoch 17, Training Loss: 0.956589622146943\n",
      "Epoch 18, Training Loss: 0.9557843358376447\n",
      "Epoch 19, Training Loss: 0.9549833738102632\n",
      "Epoch 20, Training Loss: 0.9541925874177147\n",
      "Epoch 21, Training Loss: 0.9534052123041714\n",
      "Epoch 22, Training Loss: 0.9526256310939789\n",
      "Epoch 23, Training Loss: 0.9518471459080191\n",
      "Epoch 24, Training Loss: 0.9510756750667796\n",
      "Epoch 25, Training Loss: 0.9503022306806901\n",
      "Epoch 26, Training Loss: 0.9495278915938209\n",
      "Epoch 27, Training Loss: 0.948756238151999\n",
      "Epoch 28, Training Loss: 0.9479832960577572\n",
      "Epoch 29, Training Loss: 0.9472175924217\n",
      "Epoch 30, Training Loss: 0.9464398315373589\n",
      "Epoch 31, Training Loss: 0.9456708404597114\n",
      "Epoch 32, Training Loss: 0.944897543963264\n",
      "Epoch 33, Training Loss: 0.9441223408194149\n",
      "Epoch 34, Training Loss: 0.9433491762946634\n",
      "Epoch 35, Training Loss: 0.9425650127495037\n",
      "Epoch 36, Training Loss: 0.9417882132530212\n",
      "Epoch 37, Training Loss: 0.9410061638495502\n",
      "Epoch 38, Training Loss: 0.9402182393214282\n",
      "Epoch 39, Training Loss: 0.9394320349833545\n",
      "Epoch 40, Training Loss: 0.9386416195420658\n",
      "Epoch 41, Training Loss: 0.937849379567539\n",
      "Epoch 42, Training Loss: 0.9370505525785334\n",
      "Epoch 43, Training Loss: 0.93625029816347\n",
      "Epoch 44, Training Loss: 0.935446508842356\n",
      "Epoch 45, Training Loss: 0.9346362568350399\n",
      "Epoch 46, Training Loss: 0.933829896660412\n",
      "Epoch 47, Training Loss: 0.9330152358027065\n",
      "Epoch 48, Training Loss: 0.9321948677652022\n",
      "Epoch 49, Training Loss: 0.9313676317299113\n",
      "Epoch 50, Training Loss: 0.9305435253592098\n",
      "Epoch 51, Training Loss: 0.9297146377142738\n",
      "Epoch 52, Training Loss: 0.9288719854635351\n",
      "Epoch 53, Training Loss: 0.9280381767890032\n",
      "Epoch 54, Training Loss: 0.9271890896909377\n",
      "Epoch 55, Training Loss: 0.9263461283375235\n",
      "Epoch 56, Training Loss: 0.9254933437179117\n",
      "Epoch 57, Training Loss: 0.9246357804186204\n",
      "Epoch 58, Training Loss: 0.923770366416258\n",
      "Epoch 59, Training Loss: 0.9228977476148045\n",
      "Epoch 60, Training Loss: 0.9220343523165759\n",
      "Epoch 61, Training Loss: 0.9211489993684432\n",
      "Epoch 62, Training Loss: 0.9202721320180332\n",
      "Epoch 63, Training Loss: 0.9193897276064928\n",
      "Epoch 64, Training Loss: 0.9184961729189929\n",
      "Epoch 65, Training Loss: 0.91760418176651\n",
      "Epoch 66, Training Loss: 0.9167046877917121\n",
      "Epoch 67, Training Loss: 0.9157980054266313\n",
      "Epoch 68, Training Loss: 0.9148958567310782\n",
      "Epoch 69, Training Loss: 0.9139690373925602\n",
      "Epoch 70, Training Loss: 0.913058581422357\n",
      "Epoch 71, Training Loss: 0.9121416709703558\n",
      "Epoch 72, Training Loss: 0.9112113663028268\n",
      "Epoch 73, Training Loss: 0.9102809965610504\n",
      "Epoch 74, Training Loss: 0.9093530519569621\n",
      "Epoch 75, Training Loss: 0.9084178525560043\n",
      "Epoch 76, Training Loss: 0.9074722367174485\n",
      "Epoch 77, Training Loss: 0.9065243828997892\n",
      "Epoch 78, Training Loss: 0.9055816498223473\n",
      "Epoch 79, Training Loss: 0.9046282137141508\n",
      "Epoch 80, Training Loss: 0.9036721892917857\n",
      "Epoch 81, Training Loss: 0.9027111446857452\n",
      "Epoch 82, Training Loss: 0.9017453514828402\n",
      "Epoch 83, Training Loss: 0.9007934860622182\n",
      "Epoch 84, Training Loss: 0.8998248771358939\n",
      "Epoch 85, Training Loss: 0.8988536971456864\n",
      "Epoch 86, Training Loss: 0.8978767275810242\n",
      "Epoch 87, Training Loss: 0.8969148884100072\n",
      "Epoch 88, Training Loss: 0.8959354583656086\n",
      "Epoch 89, Training Loss: 0.8949607401735642\n",
      "Epoch 90, Training Loss: 0.8939837621941286\n",
      "Epoch 91, Training Loss: 0.8929964057838216\n",
      "Epoch 92, Training Loss: 0.8920251844209783\n",
      "Epoch 93, Training Loss: 0.891043903056313\n",
      "Epoch 94, Training Loss: 0.8900397515998167\n",
      "Epoch 95, Training Loss: 0.889092428263496\n",
      "Epoch 96, Training Loss: 0.8881043400483973\n",
      "Epoch 97, Training Loss: 0.8871207397825578\n",
      "Epoch 98, Training Loss: 0.8861345896300148\n",
      "Epoch 99, Training Loss: 0.8851642143726349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:29:18,840] Trial 309 finished with value: 0.5865333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8841870984610389\n",
      "Epoch 1, Training Loss: 1.0300200421669905\n",
      "Epoch 2, Training Loss: 0.9433617939668543\n",
      "Epoch 3, Training Loss: 0.9252475768678329\n",
      "Epoch 4, Training Loss: 0.9160505781454199\n",
      "Epoch 5, Training Loss: 0.907243687545552\n",
      "Epoch 6, Training Loss: 0.8974889786804423\n",
      "Epoch 7, Training Loss: 0.8852816510200501\n",
      "Epoch 8, Training Loss: 0.8697958057768205\n",
      "Epoch 9, Training Loss: 0.8519320917129517\n",
      "Epoch 10, Training Loss: 0.8349066156499526\n",
      "Epoch 11, Training Loss: 0.821647910861408\n",
      "Epoch 12, Training Loss: 0.8135751630278195\n",
      "Epoch 13, Training Loss: 0.8090330966781167\n",
      "Epoch 14, Training Loss: 0.8066505806586322\n",
      "Epoch 15, Training Loss: 0.8052235678364249\n",
      "Epoch 16, Training Loss: 0.8041800244415508\n",
      "Epoch 17, Training Loss: 0.8033859557965223\n",
      "Epoch 18, Training Loss: 0.8027302507091971\n",
      "Epoch 19, Training Loss: 0.802317034777473\n",
      "Epoch 20, Training Loss: 0.8019593726186192\n",
      "Epoch 21, Training Loss: 0.8015748377407298\n",
      "Epoch 22, Training Loss: 0.8010407582451315\n",
      "Epoch 23, Training Loss: 0.8007170040467206\n",
      "Epoch 24, Training Loss: 0.8000287579788882\n",
      "Epoch 25, Training Loss: 0.7999776395629434\n",
      "Epoch 26, Training Loss: 0.799437252072727\n",
      "Epoch 27, Training Loss: 0.7993516415708205\n",
      "Epoch 28, Training Loss: 0.798893503371407\n",
      "Epoch 29, Training Loss: 0.7986936081157011\n",
      "Epoch 30, Training Loss: 0.7984393003407647\n",
      "Epoch 31, Training Loss: 0.7981242749270271\n",
      "Epoch 32, Training Loss: 0.7977797398146461\n",
      "Epoch 33, Training Loss: 0.7975348667537465\n",
      "Epoch 34, Training Loss: 0.7974417511855855\n",
      "Epoch 35, Training Loss: 0.7970588618867538\n",
      "Epoch 36, Training Loss: 0.7969145371633417\n",
      "Epoch 37, Training Loss: 0.7968227156470804\n",
      "Epoch 38, Training Loss: 0.7964604737478144\n",
      "Epoch 39, Training Loss: 0.796245209048776\n",
      "Epoch 40, Training Loss: 0.7961247777938842\n",
      "Epoch 41, Training Loss: 0.7958630354965435\n",
      "Epoch 42, Training Loss: 0.7956347332281225\n",
      "Epoch 43, Training Loss: 0.7953495395884794\n",
      "Epoch 44, Training Loss: 0.7953369475813473\n",
      "Epoch 45, Training Loss: 0.7952425021283767\n",
      "Epoch 46, Training Loss: 0.7948942637443542\n",
      "Epoch 47, Training Loss: 0.7947449722009546\n",
      "Epoch 48, Training Loss: 0.7945620252805597\n",
      "Epoch 49, Training Loss: 0.7944560604936937\n",
      "Epoch 50, Training Loss: 0.794202158871819\n",
      "Epoch 51, Training Loss: 0.7941377756174873\n",
      "Epoch 52, Training Loss: 0.7936479888944065\n",
      "Epoch 53, Training Loss: 0.793578097820282\n",
      "Epoch 54, Training Loss: 0.7934887515096103\n",
      "Epoch 55, Training Loss: 0.7930643526946797\n",
      "Epoch 56, Training Loss: 0.7929275852792403\n",
      "Epoch 57, Training Loss: 0.7924296962513643\n",
      "Epoch 58, Training Loss: 0.7925941813693327\n",
      "Epoch 59, Training Loss: 0.7922317444576936\n",
      "Epoch 60, Training Loss: 0.7918894783889546\n",
      "Epoch 61, Training Loss: 0.7915741395950318\n",
      "Epoch 62, Training Loss: 0.791487834593829\n",
      "Epoch 63, Training Loss: 0.7912275714734022\n",
      "Epoch 64, Training Loss: 0.7912176961057327\n",
      "Epoch 65, Training Loss: 0.7910899305343628\n",
      "Epoch 66, Training Loss: 0.7910240885089426\n",
      "Epoch 67, Training Loss: 0.7907060226272135\n",
      "Epoch 68, Training Loss: 0.7904181850657743\n",
      "Epoch 69, Training Loss: 0.790207146616543\n",
      "Epoch 70, Training Loss: 0.7901560295329374\n",
      "Epoch 71, Training Loss: 0.7900600267859066\n",
      "Epoch 72, Training Loss: 0.7898631230522605\n",
      "Epoch 73, Training Loss: 0.7897670252884136\n",
      "Epoch 74, Training Loss: 0.7894831451948952\n",
      "Epoch 75, Training Loss: 0.7893381860676933\n",
      "Epoch 76, Training Loss: 0.7892048287391663\n",
      "Epoch 77, Training Loss: 0.788962902952643\n",
      "Epoch 78, Training Loss: 0.7887969739998089\n",
      "Epoch 79, Training Loss: 0.7886815355104558\n",
      "Epoch 80, Training Loss: 0.7885884995320264\n",
      "Epoch 81, Training Loss: 0.7884960286757525\n",
      "Epoch 82, Training Loss: 0.788260273582795\n",
      "Epoch 83, Training Loss: 0.788351062115501\n",
      "Epoch 84, Training Loss: 0.7878477479429806\n",
      "Epoch 85, Training Loss: 0.7878743236205157\n",
      "Epoch 86, Training Loss: 0.7878473585493424\n",
      "Epoch 87, Training Loss: 0.7875704167870914\n",
      "Epoch 88, Training Loss: 0.787622705487644\n",
      "Epoch 89, Training Loss: 0.7873704808599808\n",
      "Epoch 90, Training Loss: 0.7873808077503653\n",
      "Epoch 91, Training Loss: 0.7872948772767011\n",
      "Epoch 92, Training Loss: 0.7871135750938865\n",
      "Epoch 93, Training Loss: 0.7870707885658039\n",
      "Epoch 94, Training Loss: 0.7867660432703355\n",
      "Epoch 95, Training Loss: 0.7867125082015991\n",
      "Epoch 96, Training Loss: 0.786695551030776\n",
      "Epoch 97, Training Loss: 0.7867348324551302\n",
      "Epoch 98, Training Loss: 0.7865541039494908\n",
      "Epoch 99, Training Loss: 0.7863399832388934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:31:07,136] Trial 310 finished with value: 0.6410666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7863429008511936\n",
      "Epoch 1, Training Loss: 0.8678534928121064\n",
      "Epoch 2, Training Loss: 0.8174106061906743\n",
      "Epoch 3, Training Loss: 0.8120622533604615\n",
      "Epoch 4, Training Loss: 0.8075180751040466\n",
      "Epoch 5, Training Loss: 0.8040005179276144\n",
      "Epoch 6, Training Loss: 0.7987906736538822\n",
      "Epoch 7, Training Loss: 0.7953944897293148\n",
      "Epoch 8, Training Loss: 0.7924540634442093\n",
      "Epoch 9, Training Loss: 0.7924699667701147\n",
      "Epoch 10, Training Loss: 0.7916592066449331\n",
      "Epoch 11, Training Loss: 0.7910873415774868\n",
      "Epoch 12, Training Loss: 0.7900148909791072\n",
      "Epoch 13, Training Loss: 0.7913423317715638\n",
      "Epoch 14, Training Loss: 0.7897353387416753\n",
      "Epoch 15, Training Loss: 0.7905609491176175\n",
      "Epoch 16, Training Loss: 0.7891861179717501\n",
      "Epoch 17, Training Loss: 0.7891627332321683\n",
      "Epoch 18, Training Loss: 0.7883762181253362\n",
      "Epoch 19, Training Loss: 0.7869779616370237\n",
      "Epoch 20, Training Loss: 0.7869833916649782\n",
      "Epoch 21, Training Loss: 0.7869821656467323\n",
      "Epoch 22, Training Loss: 0.7870432980974814\n",
      "Epoch 23, Training Loss: 0.787097153986307\n",
      "Epoch 24, Training Loss: 0.7862498225126051\n",
      "Epoch 25, Training Loss: 0.7858038100981175\n",
      "Epoch 26, Training Loss: 0.7871083053431116\n",
      "Epoch 27, Training Loss: 0.7858988501075516\n",
      "Epoch 28, Training Loss: 0.7848971027180665\n",
      "Epoch 29, Training Loss: 0.7867210227744024\n",
      "Epoch 30, Training Loss: 0.7864010022995167\n",
      "Epoch 31, Training Loss: 0.785059462633348\n",
      "Epoch 32, Training Loss: 0.7851234287247622\n",
      "Epoch 33, Training Loss: 0.7843092497130085\n",
      "Epoch 34, Training Loss: 0.7848150424491194\n",
      "Epoch 35, Training Loss: 0.7831686847192004\n",
      "Epoch 36, Training Loss: 0.784112684977682\n",
      "Epoch 37, Training Loss: 0.7844117797406992\n",
      "Epoch 38, Training Loss: 0.7836619742830894\n",
      "Epoch 39, Training Loss: 0.7836140788587412\n",
      "Epoch 40, Training Loss: 0.7835285297013763\n",
      "Epoch 41, Training Loss: 0.7832954923013099\n",
      "Epoch 42, Training Loss: 0.7829475248666634\n",
      "Epoch 43, Training Loss: 0.7829715824664983\n",
      "Epoch 44, Training Loss: 0.7832883630480085\n",
      "Epoch 45, Training Loss: 0.7820016276567502\n",
      "Epoch 46, Training Loss: 0.7827368506811615\n",
      "Epoch 47, Training Loss: 0.7825148427396789\n",
      "Epoch 48, Training Loss: 0.7819721824244449\n",
      "Epoch 49, Training Loss: 0.7817584213457609\n",
      "Epoch 50, Training Loss: 0.7818185326748325\n",
      "Epoch 51, Training Loss: 0.7826288433003247\n",
      "Epoch 52, Training Loss: 0.7826155581868681\n",
      "Epoch 53, Training Loss: 0.7816642048663662\n",
      "Epoch 54, Training Loss: 0.7813776275269071\n",
      "Epoch 55, Training Loss: 0.7826286065847354\n",
      "Epoch 56, Training Loss: 0.78247020468676\n",
      "Epoch 57, Training Loss: 0.7814151418836494\n",
      "Epoch 58, Training Loss: 0.7814736576008617\n",
      "Epoch 59, Training Loss: 0.7812551311084203\n",
      "Epoch 60, Training Loss: 0.7810842151032354\n",
      "Epoch 61, Training Loss: 0.7811395143207751\n",
      "Epoch 62, Training Loss: 0.782240767407238\n",
      "Epoch 63, Training Loss: 0.7806433913402988\n",
      "Epoch 64, Training Loss: 0.7806086755336675\n",
      "Epoch 65, Training Loss: 0.7803995705188665\n",
      "Epoch 66, Training Loss: 0.7802053728498014\n",
      "Epoch 67, Training Loss: 0.7806569891764705\n",
      "Epoch 68, Training Loss: 0.7806378213982833\n",
      "Epoch 69, Training Loss: 0.7808803428384594\n",
      "Epoch 70, Training Loss: 0.7804094401517309\n",
      "Epoch 71, Training Loss: 0.7800501404848313\n",
      "Epoch 72, Training Loss: 0.7798271367424413\n",
      "Epoch 73, Training Loss: 0.7797384971066526\n",
      "Epoch 74, Training Loss: 0.780214458211024\n",
      "Epoch 75, Training Loss: 0.7805682411767486\n",
      "Epoch 76, Training Loss: 0.7805691932377062\n",
      "Epoch 77, Training Loss: 0.7795221931055972\n",
      "Epoch 78, Training Loss: 0.7794820759529458\n",
      "Epoch 79, Training Loss: 0.7794144461925765\n",
      "Epoch 80, Training Loss: 0.779342838516809\n",
      "Epoch 81, Training Loss: 0.7803835167024369\n",
      "Epoch 82, Training Loss: 0.7792456271056842\n",
      "Epoch 83, Training Loss: 0.7791760580880301\n",
      "Epoch 84, Training Loss: 0.7793404412448854\n",
      "Epoch 85, Training Loss: 0.7795025002687497\n",
      "Epoch 86, Training Loss: 0.7792326987237859\n",
      "Epoch 87, Training Loss: 0.7798589008195059\n",
      "Epoch 88, Training Loss: 0.7791055276877898\n",
      "Epoch 89, Training Loss: 0.7790663535433604\n",
      "Epoch 90, Training Loss: 0.7791520462000281\n",
      "Epoch 91, Training Loss: 0.7783536819587077\n",
      "Epoch 92, Training Loss: 0.7787799702551131\n",
      "Epoch 93, Training Loss: 0.7790104344375152\n",
      "Epoch 94, Training Loss: 0.7789329218685179\n",
      "Epoch 95, Training Loss: 0.7788176977544798\n",
      "Epoch 96, Training Loss: 0.7783641103515051\n",
      "Epoch 97, Training Loss: 0.7781403870510876\n",
      "Epoch 98, Training Loss: 0.7789136297720716\n",
      "Epoch 99, Training Loss: 0.7781465720413323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:33:20,646] Trial 311 finished with value: 0.6317333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.77815197231178\n",
      "Epoch 1, Training Loss: 0.9011938189205371\n",
      "Epoch 2, Training Loss: 0.8289715525799228\n",
      "Epoch 3, Training Loss: 0.8218534280482988\n",
      "Epoch 4, Training Loss: 0.8167342628751483\n",
      "Epoch 5, Training Loss: 0.8151619126025895\n",
      "Epoch 6, Training Loss: 0.812023107001656\n",
      "Epoch 7, Training Loss: 0.8102506305938376\n",
      "Epoch 8, Training Loss: 0.8099545090718376\n",
      "Epoch 9, Training Loss: 0.8099269718155825\n",
      "Epoch 10, Training Loss: 0.808080083714392\n",
      "Epoch 11, Training Loss: 0.808096903069575\n",
      "Epoch 12, Training Loss: 0.8077912220381256\n",
      "Epoch 13, Training Loss: 0.8074645048693607\n",
      "Epoch 14, Training Loss: 0.8067214353640276\n",
      "Epoch 15, Training Loss: 0.8063623871122088\n",
      "Epoch 16, Training Loss: 0.8062825170674719\n",
      "Epoch 17, Training Loss: 0.8049024856180177\n",
      "Epoch 18, Training Loss: 0.804779034718535\n",
      "Epoch 19, Training Loss: 0.8038166859096154\n",
      "Epoch 20, Training Loss: 0.803894267135993\n",
      "Epoch 21, Training Loss: 0.8029242579202006\n",
      "Epoch 22, Training Loss: 0.8035979528176157\n",
      "Epoch 23, Training Loss: 0.8034521319812402\n",
      "Epoch 24, Training Loss: 0.8024344094713828\n",
      "Epoch 25, Training Loss: 0.8028000718668887\n",
      "Epoch 26, Training Loss: 0.8022652528339759\n",
      "Epoch 27, Training Loss: 0.8018728189898613\n",
      "Epoch 28, Training Loss: 0.8014359759208851\n",
      "Epoch 29, Training Loss: 0.8006337443688758\n",
      "Epoch 30, Training Loss: 0.8004412111483122\n",
      "Epoch 31, Training Loss: 0.8008214927257452\n",
      "Epoch 32, Training Loss: 0.7994608702516197\n",
      "Epoch 33, Training Loss: 0.7996743388642046\n",
      "Epoch 34, Training Loss: 0.7998018803453086\n",
      "Epoch 35, Training Loss: 0.7985968034070238\n",
      "Epoch 36, Training Loss: 0.7989181374248705\n",
      "Epoch 37, Training Loss: 0.7987324079176537\n",
      "Epoch 38, Training Loss: 0.7980446446210818\n",
      "Epoch 39, Training Loss: 0.7977830170688772\n",
      "Epoch 40, Training Loss: 0.7972066461591792\n",
      "Epoch 41, Training Loss: 0.7973218451765247\n",
      "Epoch 42, Training Loss: 0.7971456940909077\n",
      "Epoch 43, Training Loss: 0.7974940824329405\n",
      "Epoch 44, Training Loss: 0.7959091957350423\n",
      "Epoch 45, Training Loss: 0.795690959916079\n",
      "Epoch 46, Training Loss: 0.7959815362342318\n",
      "Epoch 47, Training Loss: 0.7953910914578832\n",
      "Epoch 48, Training Loss: 0.7952496482017345\n",
      "Epoch 49, Training Loss: 0.7955649565933343\n",
      "Epoch 50, Training Loss: 0.7949454483681155\n",
      "Epoch 51, Training Loss: 0.7947390122521193\n",
      "Epoch 52, Training Loss: 0.7948855033494476\n",
      "Epoch 53, Training Loss: 0.7950131665495105\n",
      "Epoch 54, Training Loss: 0.7948188677766269\n",
      "Epoch 55, Training Loss: 0.7950748348594608\n",
      "Epoch 56, Training Loss: 0.7946363326302148\n",
      "Epoch 57, Training Loss: 0.794519977730916\n",
      "Epoch 58, Training Loss: 0.7939851116417046\n",
      "Epoch 59, Training Loss: 0.7948440351880582\n",
      "Epoch 60, Training Loss: 0.7942858176123827\n",
      "Epoch 61, Training Loss: 0.794051392275588\n",
      "Epoch 62, Training Loss: 0.7947341355166041\n",
      "Epoch 63, Training Loss: 0.7932283604055419\n",
      "Epoch 64, Training Loss: 0.7932976294729046\n",
      "Epoch 65, Training Loss: 0.7939968421046896\n",
      "Epoch 66, Training Loss: 0.7938246424933125\n",
      "Epoch 67, Training Loss: 0.7934881681786444\n",
      "Epoch 68, Training Loss: 0.7931045561804807\n",
      "Epoch 69, Training Loss: 0.7925959632809001\n",
      "Epoch 70, Training Loss: 0.7934690879699879\n",
      "Epoch 71, Training Loss: 0.792762909975267\n",
      "Epoch 72, Training Loss: 0.7930832030181598\n",
      "Epoch 73, Training Loss: 0.7929272754748065\n",
      "Epoch 74, Training Loss: 0.7929749699463522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:34:29,760] Trial 312 finished with value: 0.6061333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7925837066836823\n",
      "Epoch 1, Training Loss: 1.0977683156952822\n",
      "Epoch 2, Training Loss: 1.0907648993614025\n",
      "Epoch 3, Training Loss: 1.0906558642710062\n",
      "Epoch 4, Training Loss: 1.0905654399914848\n",
      "Epoch 5, Training Loss: 1.090546142606807\n",
      "Epoch 6, Training Loss: 1.0905082620176159\n",
      "Epoch 7, Training Loss: 1.0905534681520963\n",
      "Epoch 8, Training Loss: 1.090632752906111\n",
      "Epoch 9, Training Loss: 1.0905306817893694\n",
      "Epoch 10, Training Loss: 1.0906146542470259\n",
      "Epoch 11, Training Loss: 1.0906312205737694\n",
      "Epoch 12, Training Loss: 1.090557162743762\n",
      "Epoch 13, Training Loss: 1.0905630843083662\n",
      "Epoch 14, Training Loss: 1.090560641862396\n",
      "Epoch 15, Training Loss: 1.0905463534190243\n",
      "Epoch 16, Training Loss: 1.090689568412035\n",
      "Epoch 17, Training Loss: 1.0905838059303457\n",
      "Epoch 18, Training Loss: 1.0905299175950818\n",
      "Epoch 19, Training Loss: 1.0905298453524597\n",
      "Epoch 20, Training Loss: 1.0904728962962789\n",
      "Epoch 21, Training Loss: 1.0905221727557648\n",
      "Epoch 22, Training Loss: 1.0905578663474635\n",
      "Epoch 23, Training Loss: 1.0906109621650295\n",
      "Epoch 24, Training Loss: 1.0905920068124184\n",
      "Epoch 25, Training Loss: 1.0904356583616788\n",
      "Epoch 26, Training Loss: 1.0905379872573049\n",
      "Epoch 27, Training Loss: 1.090587037846558\n",
      "Epoch 28, Training Loss: 1.0906221927556776\n",
      "Epoch 29, Training Loss: 1.0903776493287625\n",
      "Epoch 30, Training Loss: 1.0906055335711715\n",
      "Epoch 31, Training Loss: 1.0905116325034234\n",
      "Epoch 32, Training Loss: 1.090459126099608\n",
      "Epoch 33, Training Loss: 1.0904989717598248\n",
      "Epoch 34, Training Loss: 1.0904940764706834\n",
      "Epoch 35, Training Loss: 1.0904870126480446\n",
      "Epoch 36, Training Loss: 1.0903908613032864\n",
      "Epoch 37, Training Loss: 1.0904745496305308\n",
      "Epoch 38, Training Loss: 1.090459598634476\n",
      "Epoch 39, Training Loss: 1.0904706574920424\n",
      "Epoch 40, Training Loss: 1.0904501785909322\n",
      "Epoch 41, Training Loss: 1.0904043410953723\n",
      "Epoch 42, Training Loss: 1.0903155977564647\n",
      "Epoch 43, Training Loss: 1.09049018648334\n",
      "Epoch 44, Training Loss: 1.0904731454705834\n",
      "Epoch 45, Training Loss: 1.0904671077441452\n",
      "Epoch 46, Training Loss: 1.090267943260365\n",
      "Epoch 47, Training Loss: 1.0903790931056316\n",
      "Epoch 48, Training Loss: 1.0903067689192922\n",
      "Epoch 49, Training Loss: 1.0903518664209466\n",
      "Epoch 50, Training Loss: 1.0902937994863755\n",
      "Epoch 51, Training Loss: 1.090394495483628\n",
      "Epoch 52, Training Loss: 1.090348090085768\n",
      "Epoch 53, Training Loss: 1.090388264333395\n",
      "Epoch 54, Training Loss: 1.0902800857572628\n",
      "Epoch 55, Training Loss: 1.0903128303083263\n",
      "Epoch 56, Training Loss: 1.0903212312468908\n",
      "Epoch 57, Training Loss: 1.0903499119263842\n",
      "Epoch 58, Training Loss: 1.0902058054630022\n",
      "Epoch 59, Training Loss: 1.0903043671658164\n",
      "Epoch 60, Training Loss: 1.0902389583731056\n",
      "Epoch 61, Training Loss: 1.0902858210685558\n",
      "Epoch 62, Training Loss: 1.0902705249929787\n",
      "Epoch 63, Training Loss: 1.0902241982911762\n",
      "Epoch 64, Training Loss: 1.0902649931441573\n",
      "Epoch 65, Training Loss: 1.0902020527904195\n",
      "Epoch 66, Training Loss: 1.090241689251778\n",
      "Epoch 67, Training Loss: 1.0901968566994917\n",
      "Epoch 68, Training Loss: 1.0903334513642735\n",
      "Epoch 69, Training Loss: 1.0902804354975995\n",
      "Epoch 70, Training Loss: 1.0902571090181967\n",
      "Epoch 71, Training Loss: 1.0901708192395088\n",
      "Epoch 72, Training Loss: 1.0903490109551222\n",
      "Epoch 73, Training Loss: 1.0902460990991807\n",
      "Epoch 74, Training Loss: 1.0901359687174172\n",
      "Epoch 75, Training Loss: 1.090108097226996\n",
      "Epoch 76, Training Loss: 1.0903335313151654\n",
      "Epoch 77, Training Loss: 1.0902145697658223\n",
      "Epoch 78, Training Loss: 1.0903123523955955\n",
      "Epoch 79, Training Loss: 1.0902482966731366\n",
      "Epoch 80, Training Loss: 1.0901563673091115\n",
      "Epoch 81, Training Loss: 1.0901034132878584\n",
      "Epoch 82, Training Loss: 1.0901296836092955\n",
      "Epoch 83, Training Loss: 1.0901719129175171\n",
      "Epoch 84, Training Loss: 1.090169216277904\n",
      "Epoch 85, Training Loss: 1.0901181703223322\n",
      "Epoch 86, Training Loss: 1.0901541041252307\n",
      "Epoch 87, Training Loss: 1.09014834497208\n",
      "Epoch 88, Training Loss: 1.090042365224738\n",
      "Epoch 89, Training Loss: 1.089991979849966\n",
      "Epoch 90, Training Loss: 1.090129854625329\n",
      "Epoch 91, Training Loss: 1.09010790452025\n",
      "Epoch 92, Training Loss: 1.090071826052845\n",
      "Epoch 93, Training Loss: 1.090062507830168\n",
      "Epoch 94, Training Loss: 1.0899554785032917\n",
      "Epoch 95, Training Loss: 1.0900552661795364\n",
      "Epoch 96, Training Loss: 1.0900503172910303\n",
      "Epoch 97, Training Loss: 1.090067744255066\n",
      "Epoch 98, Training Loss: 1.0900771725446659\n",
      "Epoch 99, Training Loss: 1.0902074720626487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:36:06,880] Trial 313 finished with value: 0.3558 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.0899603268257658\n",
      "Epoch 1, Training Loss: 0.9951698781852435\n",
      "Epoch 2, Training Loss: 0.9389083734132293\n",
      "Epoch 3, Training Loss: 0.9030702538956377\n",
      "Epoch 4, Training Loss: 0.8527164039755226\n",
      "Epoch 5, Training Loss: 0.8233541514640464\n",
      "Epoch 6, Training Loss: 0.8139623799718412\n",
      "Epoch 7, Training Loss: 0.8131346664930644\n",
      "Epoch 8, Training Loss: 0.8109973918226429\n",
      "Epoch 9, Training Loss: 0.8097554770627416\n",
      "Epoch 10, Training Loss: 0.8086823019766269\n",
      "Epoch 11, Training Loss: 0.8078493224050766\n",
      "Epoch 12, Training Loss: 0.8066326223817983\n",
      "Epoch 13, Training Loss: 0.8054831401746075\n",
      "Epoch 14, Training Loss: 0.8056198348676352\n",
      "Epoch 15, Training Loss: 0.8050707401189588\n",
      "Epoch 16, Training Loss: 0.8039635968387575\n",
      "Epoch 17, Training Loss: 0.8041611014452196\n",
      "Epoch 18, Training Loss: 0.8028324684254209\n",
      "Epoch 19, Training Loss: 0.8034603593044711\n",
      "Epoch 20, Training Loss: 0.8030614690673082\n",
      "Epoch 21, Training Loss: 0.8024042315052864\n",
      "Epoch 22, Training Loss: 0.8025882163442167\n",
      "Epoch 23, Training Loss: 0.8018656087997265\n",
      "Epoch 24, Training Loss: 0.8022707827557298\n",
      "Epoch 25, Training Loss: 0.8014322840181508\n",
      "Epoch 26, Training Loss: 0.8020849008309213\n",
      "Epoch 27, Training Loss: 0.8015969574899602\n",
      "Epoch 28, Training Loss: 0.8011259508760352\n",
      "Epoch 29, Training Loss: 0.8011170449113487\n",
      "Epoch 30, Training Loss: 0.8009965057659866\n",
      "Epoch 31, Training Loss: 0.8006442973488256\n",
      "Epoch 32, Training Loss: 0.8005788714365851\n",
      "Epoch 33, Training Loss: 0.8004938966349552\n",
      "Epoch 34, Training Loss: 0.8008228679348651\n",
      "Epoch 35, Training Loss: 0.7998985026115761\n",
      "Epoch 36, Training Loss: 0.7998686796740482\n",
      "Epoch 37, Training Loss: 0.8009098277056128\n",
      "Epoch 38, Training Loss: 0.8006204652606993\n",
      "Epoch 39, Training Loss: 0.7993662965028806\n",
      "Epoch 40, Training Loss: 0.7993010770109363\n",
      "Epoch 41, Training Loss: 0.7989325428367557\n",
      "Epoch 42, Training Loss: 0.800301011971065\n",
      "Epoch 43, Training Loss: 0.7991299661478601\n",
      "Epoch 44, Training Loss: 0.7987132490129399\n",
      "Epoch 45, Training Loss: 0.7994786373654702\n",
      "Epoch 46, Training Loss: 0.7992289716139772\n",
      "Epoch 47, Training Loss: 0.7985196163779811\n",
      "Epoch 48, Training Loss: 0.7986379419054304\n",
      "Epoch 49, Training Loss: 0.7987929255442512\n",
      "Epoch 50, Training Loss: 0.7983503941306495\n",
      "Epoch 51, Training Loss: 0.7980085450903813\n",
      "Epoch 52, Training Loss: 0.7976556860414663\n",
      "Epoch 53, Training Loss: 0.7979408302701505\n",
      "Epoch 54, Training Loss: 0.7979625190111032\n",
      "Epoch 55, Training Loss: 0.7981779312728939\n",
      "Epoch 56, Training Loss: 0.797993454628421\n",
      "Epoch 57, Training Loss: 0.7971339292096016\n",
      "Epoch 58, Training Loss: 0.7971741255064656\n",
      "Epoch 59, Training Loss: 0.7971767543850089\n",
      "Epoch 60, Training Loss: 0.7974616780316919\n",
      "Epoch 61, Training Loss: 0.7975876338499829\n",
      "Epoch 62, Training Loss: 0.7974439862975501\n",
      "Epoch 63, Training Loss: 0.7969639287855392\n",
      "Epoch 64, Training Loss: 0.7971322613551204\n",
      "Epoch 65, Training Loss: 0.7973762011169491\n",
      "Epoch 66, Training Loss: 0.7982330477327333\n",
      "Epoch 67, Training Loss: 0.7976766377463377\n",
      "Epoch 68, Training Loss: 0.7969192961104831\n",
      "Epoch 69, Training Loss: 0.7966520358745317\n",
      "Epoch 70, Training Loss: 0.796080722217273\n",
      "Epoch 71, Training Loss: 0.797363167239311\n",
      "Epoch 72, Training Loss: 0.7955032334291845\n",
      "Epoch 73, Training Loss: 0.7962810858748013\n",
      "Epoch 74, Training Loss: 0.7962952813707797\n",
      "Epoch 75, Training Loss: 0.7961161459298959\n",
      "Epoch 76, Training Loss: 0.7964645778326164\n",
      "Epoch 77, Training Loss: 0.7962978553054925\n",
      "Epoch 78, Training Loss: 0.7961110681519472\n",
      "Epoch 79, Training Loss: 0.7956460586167816\n",
      "Epoch 80, Training Loss: 0.7959880522319249\n",
      "Epoch 81, Training Loss: 0.7963195704876032\n",
      "Epoch 82, Training Loss: 0.7951043854978748\n",
      "Epoch 83, Training Loss: 0.7952789173986679\n",
      "Epoch 84, Training Loss: 0.7949236829478041\n",
      "Epoch 85, Training Loss: 0.7956570342071074\n",
      "Epoch 86, Training Loss: 0.7951487235556868\n",
      "Epoch 87, Training Loss: 0.7953077013331248\n",
      "Epoch 88, Training Loss: 0.7944390562243928\n",
      "Epoch 89, Training Loss: 0.7947278444928334\n",
      "Epoch 90, Training Loss: 0.7941310025695572\n",
      "Epoch 91, Training Loss: 0.7942152948307811\n",
      "Epoch 92, Training Loss: 0.7941319233492801\n",
      "Epoch 93, Training Loss: 0.7935944361794264\n",
      "Epoch 94, Training Loss: 0.7935299267446189\n",
      "Epoch 95, Training Loss: 0.7947308709746913\n",
      "Epoch 96, Training Loss: 0.7934704180946923\n",
      "Epoch 97, Training Loss: 0.7933577797466651\n",
      "Epoch 98, Training Loss: 0.7925959337026554\n",
      "Epoch 99, Training Loss: 0.7932434201240539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:38:07,141] Trial 314 finished with value: 0.6340666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7932647691633469\n",
      "Epoch 1, Training Loss: 0.8462596582665163\n",
      "Epoch 2, Training Loss: 0.8130659041685216\n",
      "Epoch 3, Training Loss: 0.8048534824567682\n",
      "Epoch 4, Training Loss: 0.8068724459760329\n",
      "Epoch 5, Training Loss: 0.8014569360368392\n",
      "Epoch 6, Training Loss: 0.8020417091425728\n",
      "Epoch 7, Training Loss: 0.8020525214952581\n",
      "Epoch 8, Training Loss: 0.7993486673691693\n",
      "Epoch 9, Training Loss: 0.7999024961275213\n",
      "Epoch 10, Training Loss: 0.7987359753776999\n",
      "Epoch 11, Training Loss: 0.799939954140607\n",
      "Epoch 12, Training Loss: 0.7975179301991182\n",
      "Epoch 13, Training Loss: 0.7993923133261064\n",
      "Epoch 14, Training Loss: 0.7981175692642436\n",
      "Epoch 15, Training Loss: 0.7969043463117936\n",
      "Epoch 16, Training Loss: 0.7982782253798316\n",
      "Epoch 17, Training Loss: 0.7981093976076912\n",
      "Epoch 18, Training Loss: 0.7961739020487841\n",
      "Epoch 19, Training Loss: 0.7969393216862398\n",
      "Epoch 20, Training Loss: 0.7967463426028981\n",
      "Epoch 21, Training Loss: 0.7960298082407783\n",
      "Epoch 22, Training Loss: 0.7970686007948483\n",
      "Epoch 23, Training Loss: 0.7967710136666017\n",
      "Epoch 24, Training Loss: 0.7964693274217494\n",
      "Epoch 25, Training Loss: 0.7944763993515688\n",
      "Epoch 26, Training Loss: 0.7953268952229444\n",
      "Epoch 27, Training Loss: 0.7964735989009633\n",
      "Epoch 28, Training Loss: 0.7959703074483311\n",
      "Epoch 29, Training Loss: 0.7943271656597362\n",
      "Epoch 30, Training Loss: 0.795327930801055\n",
      "Epoch 31, Training Loss: 0.7957403915769914\n",
      "Epoch 32, Training Loss: 0.7941247069835663\n",
      "Epoch 33, Training Loss: 0.7948627028745764\n",
      "Epoch 34, Training Loss: 0.7939788118530722\n",
      "Epoch 35, Training Loss: 0.7945036710711086\n",
      "Epoch 36, Training Loss: 0.795630662020515\n",
      "Epoch 37, Training Loss: 0.7940802185675677\n",
      "Epoch 38, Training Loss: 0.7957144998101627\n",
      "Epoch 39, Training Loss: 0.7952380296763252\n",
      "Epoch 40, Training Loss: 0.7939391139675589\n",
      "Epoch 41, Training Loss: 0.7949443412528319\n",
      "Epoch 42, Training Loss: 0.7937419373147628\n",
      "Epoch 43, Training Loss: 0.7958940213568071\n",
      "Epoch 44, Training Loss: 0.7939232382353615\n",
      "Epoch 45, Training Loss: 0.7945328469136183\n",
      "Epoch 46, Training Loss: 0.794990195947535\n",
      "Epoch 47, Training Loss: 0.794184380348991\n",
      "Epoch 48, Training Loss: 0.7947602853354285\n",
      "Epoch 49, Training Loss: 0.7930319023132324\n",
      "Epoch 50, Training Loss: 0.7953745011021109\n",
      "Epoch 51, Training Loss: 0.7936107578698327\n",
      "Epoch 52, Training Loss: 0.7940372032978955\n",
      "Epoch 53, Training Loss: 0.7947045336751377\n",
      "Epoch 54, Training Loss: 0.7933478518093333\n",
      "Epoch 55, Training Loss: 0.7938932423030629\n",
      "Epoch 56, Training Loss: 0.7937586857290829\n",
      "Epoch 57, Training Loss: 0.793413906658397\n",
      "Epoch 58, Training Loss: 0.7934827321417192\n",
      "Epoch 59, Training Loss: 0.7935190577366773\n",
      "Epoch 60, Training Loss: 0.7930227576283848\n",
      "Epoch 61, Training Loss: 0.7940823685421663\n",
      "Epoch 62, Training Loss: 0.7933259231202743\n",
      "Epoch 63, Training Loss: 0.7937607352873858\n",
      "Epoch 64, Training Loss: 0.7933256795125849\n",
      "Epoch 65, Training Loss: 0.7950170477698831\n",
      "Epoch 66, Training Loss: 0.7934157429021947\n",
      "Epoch 67, Training Loss: 0.7926245315635906\n",
      "Epoch 68, Training Loss: 0.7937041241281173\n",
      "Epoch 69, Training Loss: 0.7931010871073779\n",
      "Epoch 70, Training Loss: 0.7937573026909548\n",
      "Epoch 71, Training Loss: 0.7923834724286023\n",
      "Epoch 72, Training Loss: 0.7947018881405101\n",
      "Epoch 73, Training Loss: 0.7930331108850591\n",
      "Epoch 74, Training Loss: 0.7929565339228686\n",
      "Epoch 75, Training Loss: 0.7925446437386905\n",
      "Epoch 76, Training Loss: 0.7944569060381721\n",
      "Epoch 77, Training Loss: 0.7913883611735175\n",
      "Epoch 78, Training Loss: 0.7938843060241026\n",
      "Epoch 79, Training Loss: 0.7930995927838718\n",
      "Epoch 80, Training Loss: 0.7937569144894095\n",
      "Epoch 81, Training Loss: 0.7930357103488025\n",
      "Epoch 82, Training Loss: 0.7919953398844775\n",
      "Epoch 83, Training Loss: 0.7924471908457139\n",
      "Epoch 84, Training Loss: 0.7923420673258165\n",
      "Epoch 85, Training Loss: 0.7942485215383418\n",
      "Epoch 86, Training Loss: 0.7930470966591554\n",
      "Epoch 87, Training Loss: 0.7923818931158851\n",
      "Epoch 88, Training Loss: 0.7933055482191198\n",
      "Epoch 89, Training Loss: 0.7928093835886787\n",
      "Epoch 90, Training Loss: 0.7922211644929998\n",
      "Epoch 91, Training Loss: 0.7925408777068643\n",
      "Epoch 92, Training Loss: 0.7934518945217133\n",
      "Epoch 93, Training Loss: 0.7922699583277982\n",
      "Epoch 94, Training Loss: 0.7920998815929189\n",
      "Epoch 95, Training Loss: 0.7927592015266418\n",
      "Epoch 96, Training Loss: 0.7916314646075754\n",
      "Epoch 97, Training Loss: 0.7926317601344165\n",
      "Epoch 98, Training Loss: 0.7935834325762356\n",
      "Epoch 99, Training Loss: 0.7933453434355119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:40:24,973] Trial 315 finished with value: 0.6346666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7919536216118757\n",
      "Epoch 1, Training Loss: 1.0169703436599058\n",
      "Epoch 2, Training Loss: 0.9471175855748794\n",
      "Epoch 3, Training Loss: 0.932752306952196\n",
      "Epoch 4, Training Loss: 0.9241767493416281\n",
      "Epoch 5, Training Loss: 0.9166587471961976\n",
      "Epoch 6, Training Loss: 0.9089772640256321\n",
      "Epoch 7, Training Loss: 0.9003230594186222\n",
      "Epoch 8, Training Loss: 0.8890916336985195\n",
      "Epoch 9, Training Loss: 0.8744342444924748\n",
      "Epoch 10, Training Loss: 0.8565087141710169\n",
      "Epoch 11, Training Loss: 0.837893215417862\n",
      "Epoch 12, Training Loss: 0.8230062949657441\n",
      "Epoch 13, Training Loss: 0.8138936891275294\n",
      "Epoch 14, Training Loss: 0.8089509041870342\n",
      "Epoch 15, Training Loss: 0.8062886111175313\n",
      "Epoch 16, Training Loss: 0.8051334343237035\n",
      "Epoch 17, Training Loss: 0.8042209673629087\n",
      "Epoch 18, Training Loss: 0.8034325201371136\n",
      "Epoch 19, Training Loss: 0.8028062118502224\n",
      "Epoch 20, Training Loss: 0.8021214699745178\n",
      "Epoch 21, Training Loss: 0.8017853524404414\n",
      "Epoch 22, Training Loss: 0.8011705115963431\n",
      "Epoch 23, Training Loss: 0.8007063926668728\n",
      "Epoch 24, Training Loss: 0.8002557517500485\n",
      "Epoch 25, Training Loss: 0.8000081140854779\n",
      "Epoch 26, Training Loss: 0.7994630289077759\n",
      "Epoch 27, Training Loss: 0.7990626512555515\n",
      "Epoch 28, Training Loss: 0.7986586130366606\n",
      "Epoch 29, Training Loss: 0.798511798732421\n",
      "Epoch 30, Training Loss: 0.7980890391153448\n",
      "Epoch 31, Training Loss: 0.7980248965235317\n",
      "Epoch 32, Training Loss: 0.7974016119452084\n",
      "Epoch 33, Training Loss: 0.7971611855310552\n",
      "Epoch 34, Training Loss: 0.7969434414891635\n",
      "Epoch 35, Training Loss: 0.7967321688287399\n",
      "Epoch 36, Training Loss: 0.7963663160099703\n",
      "Epoch 37, Training Loss: 0.7963377902087043\n",
      "Epoch 38, Training Loss: 0.7957290364012999\n",
      "Epoch 39, Training Loss: 0.795700093087028\n",
      "Epoch 40, Training Loss: 0.7956158030734343\n",
      "Epoch 41, Training Loss: 0.7952310465363895\n",
      "Epoch 42, Training Loss: 0.7951354227346532\n",
      "Epoch 43, Training Loss: 0.7950259179227492\n",
      "Epoch 44, Training Loss: 0.7945707630409914\n",
      "Epoch 45, Training Loss: 0.7946789378979627\n",
      "Epoch 46, Training Loss: 0.7942098062178667\n",
      "Epoch 47, Training Loss: 0.7940491819381714\n",
      "Epoch 48, Training Loss: 0.7938145554065704\n",
      "Epoch 49, Training Loss: 0.7936529228967779\n",
      "Epoch 50, Training Loss: 0.7933889736147488\n",
      "Epoch 51, Training Loss: 0.7931161906438715\n",
      "Epoch 52, Training Loss: 0.7926921262460597\n",
      "Epoch 53, Training Loss: 0.7926912810521968\n",
      "Epoch 54, Training Loss: 0.7926872709919425\n",
      "Epoch 55, Training Loss: 0.7923310037921457\n",
      "Epoch 56, Training Loss: 0.7922057956106523\n",
      "Epoch 57, Training Loss: 0.7920078544756946\n",
      "Epoch 58, Training Loss: 0.7916917860507965\n",
      "Epoch 59, Training Loss: 0.7918176229561077\n",
      "Epoch 60, Training Loss: 0.791600780136445\n",
      "Epoch 61, Training Loss: 0.7914863406910616\n",
      "Epoch 62, Training Loss: 0.7912570282992195\n",
      "Epoch 63, Training Loss: 0.7911689597017625\n",
      "Epoch 64, Training Loss: 0.7910531068549437\n",
      "Epoch 65, Training Loss: 0.7907981277213377\n",
      "Epoch 66, Training Loss: 0.7907648778662962\n",
      "Epoch 67, Training Loss: 0.7904255550047931\n",
      "Epoch 68, Training Loss: 0.7902545386903426\n",
      "Epoch 69, Training Loss: 0.7904379666552824\n",
      "Epoch 70, Training Loss: 0.7901693798513973\n",
      "Epoch 71, Training Loss: 0.790060030782924\n",
      "Epoch 72, Training Loss: 0.7900558344756856\n",
      "Epoch 73, Training Loss: 0.7900116478695589\n",
      "Epoch 74, Training Loss: 0.7897685012396645\n",
      "Epoch 75, Training Loss: 0.7896863224927116\n",
      "Epoch 76, Training Loss: 0.789386913916644\n",
      "Epoch 77, Training Loss: 0.7893720302161048\n",
      "Epoch 78, Training Loss: 0.7893084669814391\n",
      "Epoch 79, Training Loss: 0.7893397934997783\n",
      "Epoch 80, Training Loss: 0.789023666381836\n",
      "Epoch 81, Training Loss: 0.7890442458321066\n",
      "Epoch 82, Training Loss: 0.7889421586429372\n",
      "Epoch 83, Training Loss: 0.7889819033005658\n",
      "Epoch 84, Training Loss: 0.7887819762790904\n",
      "Epoch 85, Training Loss: 0.7885954029419843\n",
      "Epoch 86, Training Loss: 0.7886685881895178\n",
      "Epoch 87, Training Loss: 0.7885560826694265\n",
      "Epoch 88, Training Loss: 0.7882505986971013\n",
      "Epoch 89, Training Loss: 0.7883914896319895\n",
      "Epoch 90, Training Loss: 0.7881211690341725\n",
      "Epoch 91, Training Loss: 0.7883316349281985\n",
      "Epoch 92, Training Loss: 0.7879576260903303\n",
      "Epoch 93, Training Loss: 0.7879676363047432\n",
      "Epoch 94, Training Loss: 0.7877630228856031\n",
      "Epoch 95, Training Loss: 0.7878442247474895\n",
      "Epoch 96, Training Loss: 0.7877362078778885\n",
      "Epoch 97, Training Loss: 0.7875003628169789\n",
      "Epoch 98, Training Loss: 0.7877322929746965\n",
      "Epoch 99, Training Loss: 0.7873920036063474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:42:04,862] Trial 316 finished with value: 0.6390666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7873748383101296\n",
      "Epoch 1, Training Loss: 0.8491632800943711\n",
      "Epoch 2, Training Loss: 0.8121958396014045\n",
      "Epoch 3, Training Loss: 0.8099799734003403\n",
      "Epoch 4, Training Loss: 0.807717704001595\n",
      "Epoch 5, Training Loss: 0.8060875088327071\n",
      "Epoch 6, Training Loss: 0.8043540753336513\n",
      "Epoch 7, Training Loss: 0.8022154787708732\n",
      "Epoch 8, Training Loss: 0.8023498225212097\n",
      "Epoch 9, Training Loss: 0.8012261604561525\n",
      "Epoch 10, Training Loss: 0.8009468697800356\n",
      "Epoch 11, Training Loss: 0.8010454420482411\n",
      "Epoch 12, Training Loss: 0.799679425463957\n",
      "Epoch 13, Training Loss: 0.799222624722649\n",
      "Epoch 14, Training Loss: 0.7973513315004461\n",
      "Epoch 15, Training Loss: 0.7986265383748448\n",
      "Epoch 16, Training Loss: 0.7979499012582443\n",
      "Epoch 17, Training Loss: 0.7983184131454019\n",
      "Epoch 18, Training Loss: 0.7983740922983955\n",
      "Epoch 19, Training Loss: 0.7968254177009358\n",
      "Epoch 20, Training Loss: 0.7963843680129332\n",
      "Epoch 21, Training Loss: 0.7970862991669598\n",
      "Epoch 22, Training Loss: 0.7965092990678899\n",
      "Epoch 23, Training Loss: 0.7959394673740162\n",
      "Epoch 24, Training Loss: 0.796295800559661\n",
      "Epoch 25, Training Loss: 0.7966951582011055\n",
      "Epoch 26, Training Loss: 0.7959332377069137\n",
      "Epoch 27, Training Loss: 0.7949931184684529\n",
      "Epoch 28, Training Loss: 0.7945549988746643\n",
      "Epoch 29, Training Loss: 0.7943639915830949\n",
      "Epoch 30, Training Loss: 0.7937864253100226\n",
      "Epoch 31, Training Loss: 0.7944341517195982\n",
      "Epoch 32, Training Loss: 0.7947894398605122\n",
      "Epoch 33, Training Loss: 0.7939524213005514\n",
      "Epoch 34, Training Loss: 0.795300606769674\n",
      "Epoch 35, Training Loss: 0.7937895976094639\n",
      "Epoch 36, Training Loss: 0.7934254439438091\n",
      "Epoch 37, Training Loss: 0.7933916215335621\n",
      "Epoch 38, Training Loss: 0.7943591684453628\n",
      "Epoch 39, Training Loss: 0.793228362307829\n",
      "Epoch 40, Training Loss: 0.7939318440240972\n",
      "Epoch 41, Training Loss: 0.7938059752828934\n",
      "Epoch 42, Training Loss: 0.7931796470810385\n",
      "Epoch 43, Training Loss: 0.7934058882208431\n",
      "Epoch 44, Training Loss: 0.792473489186343\n",
      "Epoch 45, Training Loss: 0.7932130223863265\n",
      "Epoch 46, Training Loss: 0.7938414056862102\n",
      "Epoch 47, Training Loss: 0.7932602317894206\n",
      "Epoch 48, Training Loss: 0.792880184510175\n",
      "Epoch 49, Training Loss: 0.7930729574315688\n",
      "Epoch 50, Training Loss: 0.7936538352685816\n",
      "Epoch 51, Training Loss: 0.793118144974989\n",
      "Epoch 52, Training Loss: 0.7938203655270969\n",
      "Epoch 53, Training Loss: 0.7931308613103979\n",
      "Epoch 54, Training Loss: 0.7933142270761377\n",
      "Epoch 55, Training Loss: 0.7927426322768716\n",
      "Epoch 56, Training Loss: 0.7934187790225534\n",
      "Epoch 57, Training Loss: 0.7928716257039239\n",
      "Epoch 58, Training Loss: 0.7925291193934048\n",
      "Epoch 59, Training Loss: 0.7929797549107496\n",
      "Epoch 60, Training Loss: 0.7923371280642116\n",
      "Epoch 61, Training Loss: 0.792344294646207\n",
      "Epoch 62, Training Loss: 0.7926596995662241\n",
      "Epoch 63, Training Loss: 0.7926775140622083\n",
      "Epoch 64, Training Loss: 0.7923096482192769\n",
      "Epoch 65, Training Loss: 0.7923017547411078\n",
      "Epoch 66, Training Loss: 0.7927636418623083\n",
      "Epoch 67, Training Loss: 0.7917589001795825\n",
      "Epoch 68, Training Loss: 0.7928291976451873\n",
      "Epoch 69, Training Loss: 0.7922260355248171\n",
      "Epoch 70, Training Loss: 0.7916713056143593\n",
      "Epoch 71, Training Loss: 0.7920751629857457\n",
      "Epoch 72, Training Loss: 0.7914061079305761\n",
      "Epoch 73, Training Loss: 0.7924983901136061\n",
      "Epoch 74, Training Loss: 0.7925256510341868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:43:32,681] Trial 317 finished with value: 0.6368666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7923766404039719\n",
      "Epoch 1, Training Loss: 1.035324903936947\n",
      "Epoch 2, Training Loss: 0.9351921694418963\n",
      "Epoch 3, Training Loss: 0.9133180016629836\n",
      "Epoch 4, Training Loss: 0.8887385538045098\n",
      "Epoch 5, Training Loss: 0.8401456027171191\n",
      "Epoch 6, Training Loss: 0.8145632043305565\n",
      "Epoch 7, Training Loss: 0.8106886959777159\n",
      "Epoch 8, Training Loss: 0.8080342705810771\n",
      "Epoch 9, Training Loss: 0.8058395461475147\n",
      "Epoch 10, Training Loss: 0.8038404319566839\n",
      "Epoch 11, Training Loss: 0.8020647440237157\n",
      "Epoch 12, Training Loss: 0.8004212317747228\n",
      "Epoch 13, Training Loss: 0.7998514435571783\n",
      "Epoch 14, Training Loss: 0.7985424745784087\n",
      "Epoch 15, Training Loss: 0.7984003381869372\n",
      "Epoch 16, Training Loss: 0.7974854848665349\n",
      "Epoch 17, Training Loss: 0.7962940630492042\n",
      "Epoch 18, Training Loss: 0.7959435330418979\n",
      "Epoch 19, Training Loss: 0.7952920427743126\n",
      "Epoch 20, Training Loss: 0.7948397597845863\n",
      "Epoch 21, Training Loss: 0.7936097939575419\n",
      "Epoch 22, Training Loss: 0.7933322727680207\n",
      "Epoch 23, Training Loss: 0.792581298491534\n",
      "Epoch 24, Training Loss: 0.7921241943275227\n",
      "Epoch 25, Training Loss: 0.7909538155443528\n",
      "Epoch 26, Training Loss: 0.7905718031350304\n",
      "Epoch 27, Training Loss: 0.7902109333346872\n",
      "Epoch 28, Training Loss: 0.7894573412222021\n",
      "Epoch 29, Training Loss: 0.7893480292488547\n",
      "Epoch 30, Training Loss: 0.7887894877265481\n",
      "Epoch 31, Training Loss: 0.7880875696855433\n",
      "Epoch 32, Training Loss: 0.787910074486452\n",
      "Epoch 33, Training Loss: 0.7876672532277949\n",
      "Epoch 34, Training Loss: 0.7868234718547148\n",
      "Epoch 35, Training Loss: 0.7866374036143807\n",
      "Epoch 36, Training Loss: 0.7863467269785264\n",
      "Epoch 37, Training Loss: 0.7861188853488249\n",
      "Epoch 38, Training Loss: 0.7860152672318851\n",
      "Epoch 39, Training Loss: 0.785948103946798\n",
      "Epoch 40, Training Loss: 0.7854810500144959\n",
      "Epoch 41, Training Loss: 0.7852133471825543\n",
      "Epoch 42, Training Loss: 0.7848659613553215\n",
      "Epoch 43, Training Loss: 0.7845150866227991\n",
      "Epoch 44, Training Loss: 0.7842334821644952\n",
      "Epoch 45, Training Loss: 0.7843015843980452\n",
      "Epoch 46, Training Loss: 0.7843541042944965\n",
      "Epoch 47, Training Loss: 0.783882500073489\n",
      "Epoch 48, Training Loss: 0.7837992432538201\n",
      "Epoch 49, Training Loss: 0.7834739523775437\n",
      "Epoch 50, Training Loss: 0.7832287241430843\n",
      "Epoch 51, Training Loss: 0.7834726648470934\n",
      "Epoch 52, Training Loss: 0.7832594134527094\n",
      "Epoch 53, Training Loss: 0.783014277640511\n",
      "Epoch 54, Training Loss: 0.7827472891527064\n",
      "Epoch 55, Training Loss: 0.7827585183171665\n",
      "Epoch 56, Training Loss: 0.7825865988871631\n",
      "Epoch 57, Training Loss: 0.7826694127391366\n",
      "Epoch 58, Training Loss: 0.7822838917199303\n",
      "Epoch 59, Training Loss: 0.782369544506073\n",
      "Epoch 60, Training Loss: 0.7825588885475607\n",
      "Epoch 61, Training Loss: 0.7823746594961952\n",
      "Epoch 62, Training Loss: 0.782167418423821\n",
      "Epoch 63, Training Loss: 0.782241341787226\n",
      "Epoch 64, Training Loss: 0.7820671979820027\n",
      "Epoch 65, Training Loss: 0.7820292687416077\n",
      "Epoch 66, Training Loss: 0.781756706868901\n",
      "Epoch 67, Training Loss: 0.7816272849896375\n",
      "Epoch 68, Training Loss: 0.7815338635444641\n",
      "Epoch 69, Training Loss: 0.7819900338789996\n",
      "Epoch 70, Training Loss: 0.7814117104866926\n",
      "Epoch 71, Training Loss: 0.781421607241911\n",
      "Epoch 72, Training Loss: 0.7813154796993032\n",
      "Epoch 73, Training Loss: 0.7812335341818193\n",
      "Epoch 74, Training Loss: 0.7811693800196928\n",
      "Epoch 75, Training Loss: 0.781416613775141\n",
      "Epoch 76, Training Loss: 0.7809398501059588\n",
      "Epoch 77, Training Loss: 0.7809187429792741\n",
      "Epoch 78, Training Loss: 0.7811934159783755\n",
      "Epoch 79, Training Loss: 0.7811827266216278\n",
      "Epoch 80, Training Loss: 0.7807897616835201\n",
      "Epoch 81, Training Loss: 0.7808646298857296\n",
      "Epoch 82, Training Loss: 0.7806119577323689\n",
      "Epoch 83, Training Loss: 0.7810102224349975\n",
      "Epoch 84, Training Loss: 0.7805761286791633\n",
      "Epoch 85, Training Loss: 0.7804409432411193\n",
      "Epoch 86, Training Loss: 0.7802991104827207\n",
      "Epoch 87, Training Loss: 0.780386232067557\n",
      "Epoch 88, Training Loss: 0.7805393376771141\n",
      "Epoch 89, Training Loss: 0.7803664902378531\n",
      "Epoch 90, Training Loss: 0.7799902099721572\n",
      "Epoch 91, Training Loss: 0.779874865027035\n",
      "Epoch 92, Training Loss: 0.7802102299998789\n",
      "Epoch 93, Training Loss: 0.7801260491679697\n",
      "Epoch 94, Training Loss: 0.7799441505179686\n",
      "Epoch 95, Training Loss: 0.7799655428353478\n",
      "Epoch 96, Training Loss: 0.7802887239876916\n",
      "Epoch 97, Training Loss: 0.7797885811328888\n",
      "Epoch 98, Training Loss: 0.7799018852149739\n",
      "Epoch 99, Training Loss: 0.7801096020025365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:45:31,173] Trial 318 finished with value: 0.6358 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7794963615782121\n",
      "Epoch 1, Training Loss: 1.0119303178787231\n",
      "Epoch 2, Training Loss: 0.9294354102190803\n",
      "Epoch 3, Training Loss: 0.9129211336724898\n",
      "Epoch 4, Training Loss: 0.8989809762730318\n",
      "Epoch 5, Training Loss: 0.8781081712245942\n",
      "Epoch 6, Training Loss: 0.8455427012724035\n",
      "Epoch 7, Training Loss: 0.8191441305945901\n",
      "Epoch 8, Training Loss: 0.8095471146527459\n",
      "Epoch 9, Training Loss: 0.8063355199729695\n",
      "Epoch 10, Training Loss: 0.8052942703050726\n",
      "Epoch 11, Training Loss: 0.8042452251209932\n",
      "Epoch 12, Training Loss: 0.8032156980037689\n",
      "Epoch 13, Training Loss: 0.8027126694426817\n",
      "Epoch 14, Training Loss: 0.8019600637520061\n",
      "Epoch 15, Training Loss: 0.8014124941825866\n",
      "Epoch 16, Training Loss: 0.8009140217304229\n",
      "Epoch 17, Training Loss: 0.8003125211070565\n",
      "Epoch 18, Training Loss: 0.8000022447109223\n",
      "Epoch 19, Training Loss: 0.7995683561353123\n",
      "Epoch 20, Training Loss: 0.799090520213632\n",
      "Epoch 21, Training Loss: 0.7986124671907986\n",
      "Epoch 22, Training Loss: 0.7983437867725597\n",
      "Epoch 23, Training Loss: 0.7979186513844658\n",
      "Epoch 24, Training Loss: 0.7976434357727276\n",
      "Epoch 25, Training Loss: 0.797411189289654\n",
      "Epoch 26, Training Loss: 0.797247215649661\n",
      "Epoch 27, Training Loss: 0.7962733282061184\n",
      "Epoch 28, Training Loss: 0.796160089338527\n",
      "Epoch 29, Training Loss: 0.7958290005431455\n",
      "Epoch 30, Training Loss: 0.7949189817204195\n",
      "Epoch 31, Training Loss: 0.7947684788703918\n",
      "Epoch 32, Training Loss: 0.7944417093080632\n",
      "Epoch 33, Training Loss: 0.7940458243033465\n",
      "Epoch 34, Training Loss: 0.7936546715568094\n",
      "Epoch 35, Training Loss: 0.7933747276137857\n",
      "Epoch 36, Training Loss: 0.7925673771605772\n",
      "Epoch 37, Training Loss: 0.7923507732503554\n",
      "Epoch 38, Training Loss: 0.7920352055044735\n",
      "Epoch 39, Training Loss: 0.7917882654947394\n",
      "Epoch 40, Training Loss: 0.7914653231115902\n",
      "Epoch 41, Training Loss: 0.7911474503489102\n",
      "Epoch 42, Training Loss: 0.7906266366032993\n",
      "Epoch 43, Training Loss: 0.7903038256308612\n",
      "Epoch 44, Training Loss: 0.7903836112162647\n",
      "Epoch 45, Training Loss: 0.7897821558924283\n",
      "Epoch 46, Training Loss: 0.7896600132128772\n",
      "Epoch 47, Training Loss: 0.7892528998851777\n",
      "Epoch 48, Training Loss: 0.7893630202377544\n",
      "Epoch 49, Training Loss: 0.7889412898877087\n",
      "Epoch 50, Training Loss: 0.788685583226821\n",
      "Epoch 51, Training Loss: 0.788527327635709\n",
      "Epoch 52, Training Loss: 0.7882167782503016\n",
      "Epoch 53, Training Loss: 0.788178526022855\n",
      "Epoch 54, Training Loss: 0.7880755878897274\n",
      "Epoch 55, Training Loss: 0.7876469414374407\n",
      "Epoch 56, Training Loss: 0.7875927221775055\n",
      "Epoch 57, Training Loss: 0.787347961734323\n",
      "Epoch 58, Training Loss: 0.786850758931216\n",
      "Epoch 59, Training Loss: 0.7869916482532726\n",
      "Epoch 60, Training Loss: 0.786781143581166\n",
      "Epoch 61, Training Loss: 0.7868668669111588\n",
      "Epoch 62, Training Loss: 0.7865877155696644\n",
      "Epoch 63, Training Loss: 0.7861900707553414\n",
      "Epoch 64, Training Loss: 0.7861934057404013\n",
      "Epoch 65, Training Loss: 0.786158994997249\n",
      "Epoch 66, Training Loss: 0.7860135249530568\n",
      "Epoch 67, Training Loss: 0.785824086595984\n",
      "Epoch 68, Training Loss: 0.785619500244365\n",
      "Epoch 69, Training Loss: 0.7855767398722031\n",
      "Epoch 70, Training Loss: 0.7856362690645106\n",
      "Epoch 71, Training Loss: 0.7855118675792918\n",
      "Epoch 72, Training Loss: 0.7851485456438626\n",
      "Epoch 73, Training Loss: 0.7852320056101855\n",
      "Epoch 74, Training Loss: 0.7851097395139582\n",
      "Epoch 75, Training Loss: 0.7850307233894572\n",
      "Epoch 76, Training Loss: 0.7847272192730623\n",
      "Epoch 77, Training Loss: 0.7851232037824742\n",
      "Epoch 78, Training Loss: 0.7846043071326088\n",
      "Epoch 79, Training Loss: 0.7844259014550378\n",
      "Epoch 80, Training Loss: 0.7843294418559354\n",
      "Epoch 81, Training Loss: 0.7847435863579021\n",
      "Epoch 82, Training Loss: 0.7844308568449582\n",
      "Epoch 83, Training Loss: 0.784483622452792\n",
      "Epoch 84, Training Loss: 0.7841022887650658\n",
      "Epoch 85, Training Loss: 0.7841449837123646\n",
      "Epoch 86, Training Loss: 0.7841954173761255\n",
      "Epoch 87, Training Loss: 0.7839829465220957\n",
      "Epoch 88, Training Loss: 0.7840417988160078\n",
      "Epoch 89, Training Loss: 0.783724145749036\n",
      "Epoch 90, Training Loss: 0.7836236224454992\n",
      "Epoch 91, Training Loss: 0.7835249115439022\n",
      "Epoch 92, Training Loss: 0.7834574392963858\n",
      "Epoch 93, Training Loss: 0.7832663325702443\n",
      "Epoch 94, Training Loss: 0.7833568460099838\n",
      "Epoch 95, Training Loss: 0.7831558970142813\n",
      "Epoch 96, Training Loss: 0.7830580568313599\n",
      "Epoch 97, Training Loss: 0.7829284112593707\n",
      "Epoch 98, Training Loss: 0.7831201497246237\n",
      "Epoch 99, Training Loss: 0.7831635126646828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:47:12,948] Trial 319 finished with value: 0.6417333333333334 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7831106925711913\n",
      "Epoch 1, Training Loss: 0.9348410824187716\n",
      "Epoch 2, Training Loss: 0.8748594747450119\n",
      "Epoch 3, Training Loss: 0.8353485748283845\n",
      "Epoch 4, Training Loss: 0.8185691494690744\n",
      "Epoch 5, Training Loss: 0.8136200807148353\n",
      "Epoch 6, Training Loss: 0.8104942215116401\n",
      "Epoch 7, Training Loss: 0.8096076287721333\n",
      "Epoch 8, Training Loss: 0.8094857750082375\n",
      "Epoch 9, Training Loss: 0.807731674936481\n",
      "Epoch 10, Training Loss: 0.807772279771647\n",
      "Epoch 11, Training Loss: 0.8070059562984265\n",
      "Epoch 12, Training Loss: 0.8071400520496799\n",
      "Epoch 13, Training Loss: 0.8063588081445909\n",
      "Epoch 14, Training Loss: 0.8060922352891219\n",
      "Epoch 15, Training Loss: 0.8052525902152958\n",
      "Epoch 16, Training Loss: 0.805888012835854\n",
      "Epoch 17, Training Loss: 0.8051560020088253\n",
      "Epoch 18, Training Loss: 0.805266517176664\n",
      "Epoch 19, Training Loss: 0.8049459072880278\n",
      "Epoch 20, Training Loss: 0.8041249548582207\n",
      "Epoch 21, Training Loss: 0.8033114818253911\n",
      "Epoch 22, Training Loss: 0.8039945355035308\n",
      "Epoch 23, Training Loss: 0.8035023739463405\n",
      "Epoch 24, Training Loss: 0.8037139039290578\n",
      "Epoch 25, Training Loss: 0.8034066755968825\n",
      "Epoch 26, Training Loss: 0.8027757769240472\n",
      "Epoch 27, Training Loss: 0.80341595097592\n",
      "Epoch 28, Training Loss: 0.8028242188288753\n",
      "Epoch 29, Training Loss: 0.8021727972460869\n",
      "Epoch 30, Training Loss: 0.802244139793224\n",
      "Epoch 31, Training Loss: 0.8020858673224772\n",
      "Epoch 32, Training Loss: 0.8022291750836194\n",
      "Epoch 33, Training Loss: 0.8019543631632525\n",
      "Epoch 34, Training Loss: 0.8018986713617368\n",
      "Epoch 35, Training Loss: 0.8008873893354173\n",
      "Epoch 36, Training Loss: 0.8010924526623318\n",
      "Epoch 37, Training Loss: 0.8008684020293386\n",
      "Epoch 38, Training Loss: 0.8018190571240016\n",
      "Epoch 39, Training Loss: 0.8006011315754482\n",
      "Epoch 40, Training Loss: 0.8002824561936515\n",
      "Epoch 41, Training Loss: 0.8005662947669065\n",
      "Epoch 42, Training Loss: 0.8003799024381135\n",
      "Epoch 43, Training Loss: 0.8002408743800974\n",
      "Epoch 44, Training Loss: 0.7998896236258342\n",
      "Epoch 45, Training Loss: 0.8003524252346583\n",
      "Epoch 46, Training Loss: 0.8002485570154692\n",
      "Epoch 47, Training Loss: 0.7998524062615588\n",
      "Epoch 48, Training Loss: 0.7996970363129351\n",
      "Epoch 49, Training Loss: 0.7998950700114544\n",
      "Epoch 50, Training Loss: 0.7996838292681185\n",
      "Epoch 51, Training Loss: 0.7996424032333201\n",
      "Epoch 52, Training Loss: 0.8000002690723964\n",
      "Epoch 53, Training Loss: 0.7990608338126562\n",
      "Epoch 54, Training Loss: 0.7987906223401091\n",
      "Epoch 55, Training Loss: 0.7992893545251144\n",
      "Epoch 56, Training Loss: 0.799072725970046\n",
      "Epoch 57, Training Loss: 0.7989486461295221\n",
      "Epoch 58, Training Loss: 0.7990388849624117\n",
      "Epoch 59, Training Loss: 0.7991053107985877\n",
      "Epoch 60, Training Loss: 0.7992004606060515\n",
      "Epoch 61, Training Loss: 0.7988104681323346\n",
      "Epoch 62, Training Loss: 0.7984653794675841\n",
      "Epoch 63, Training Loss: 0.799652041438827\n",
      "Epoch 64, Training Loss: 0.7980043257537641\n",
      "Epoch 65, Training Loss: 0.7994093929018293\n",
      "Epoch 66, Training Loss: 0.798500629116718\n",
      "Epoch 67, Training Loss: 0.7981952003966597\n",
      "Epoch 68, Training Loss: 0.7987731429867279\n",
      "Epoch 69, Training Loss: 0.7983555907593634\n",
      "Epoch 70, Training Loss: 0.7979999203430979\n",
      "Epoch 71, Training Loss: 0.7983756405966622\n",
      "Epoch 72, Training Loss: 0.7986526417552977\n",
      "Epoch 73, Training Loss: 0.7985778665184078\n",
      "Epoch 74, Training Loss: 0.7987688423995685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:48:23,741] Trial 320 finished with value: 0.6270666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7984637239821871\n",
      "Epoch 1, Training Loss: 1.0545270123876127\n",
      "Epoch 2, Training Loss: 0.9615970120394141\n",
      "Epoch 3, Training Loss: 0.9342717202982508\n",
      "Epoch 4, Training Loss: 0.9172009218007998\n",
      "Epoch 5, Training Loss: 0.8962969184818125\n",
      "Epoch 6, Training Loss: 0.8566177869201603\n",
      "Epoch 7, Training Loss: 0.8203892800144683\n",
      "Epoch 8, Training Loss: 0.8104154809973294\n",
      "Epoch 9, Training Loss: 0.8072890935087562\n",
      "Epoch 10, Training Loss: 0.8060906283837512\n",
      "Epoch 11, Training Loss: 0.8042240213630791\n",
      "Epoch 12, Training Loss: 0.8032518169933692\n",
      "Epoch 13, Training Loss: 0.8026136310477006\n",
      "Epoch 14, Training Loss: 0.8006923816257849\n",
      "Epoch 15, Training Loss: 0.7997007785883165\n",
      "Epoch 16, Training Loss: 0.7993413689441251\n",
      "Epoch 17, Training Loss: 0.7978881389574897\n",
      "Epoch 18, Training Loss: 0.797410407819246\n",
      "Epoch 19, Training Loss: 0.7969973413567794\n",
      "Epoch 20, Training Loss: 0.7967897848079079\n",
      "Epoch 21, Training Loss: 0.796163076565678\n",
      "Epoch 22, Training Loss: 0.7956942111925971\n",
      "Epoch 23, Training Loss: 0.795313829228394\n",
      "Epoch 24, Training Loss: 0.7956977366504813\n",
      "Epoch 25, Training Loss: 0.7937884320890097\n",
      "Epoch 26, Training Loss: 0.7941395738071069\n",
      "Epoch 27, Training Loss: 0.7932990409377823\n",
      "Epoch 28, Training Loss: 0.7916619246615503\n",
      "Epoch 29, Training Loss: 0.7919294160111506\n",
      "Epoch 30, Training Loss: 0.7918995074759748\n",
      "Epoch 31, Training Loss: 0.7921761915199739\n",
      "Epoch 32, Training Loss: 0.7909836947469783\n",
      "Epoch 33, Training Loss: 0.7905499375852427\n",
      "Epoch 34, Training Loss: 0.790039316783274\n",
      "Epoch 35, Training Loss: 0.7893630285012094\n",
      "Epoch 36, Training Loss: 0.7893991533078646\n",
      "Epoch 37, Training Loss: 0.7887432716394726\n",
      "Epoch 38, Training Loss: 0.7888224568582118\n",
      "Epoch 39, Training Loss: 0.7880231612607053\n",
      "Epoch 40, Training Loss: 0.7876843136056025\n",
      "Epoch 41, Training Loss: 0.7877451067580317\n",
      "Epoch 42, Training Loss: 0.7871557471447421\n",
      "Epoch 43, Training Loss: 0.7869539902622539\n",
      "Epoch 44, Training Loss: 0.7865267708785552\n",
      "Epoch 45, Training Loss: 0.7862153259435094\n",
      "Epoch 46, Training Loss: 0.7859299549482819\n",
      "Epoch 47, Training Loss: 0.7856246707134678\n",
      "Epoch 48, Training Loss: 0.7852760888580093\n",
      "Epoch 49, Training Loss: 0.785310723638176\n",
      "Epoch 50, Training Loss: 0.7856676370577704\n",
      "Epoch 51, Training Loss: 0.785517726625715\n",
      "Epoch 52, Training Loss: 0.7847260457232482\n",
      "Epoch 53, Training Loss: 0.7848497677566414\n",
      "Epoch 54, Training Loss: 0.7845981354103949\n",
      "Epoch 55, Training Loss: 0.7850359759832684\n",
      "Epoch 56, Training Loss: 0.7843882761503521\n",
      "Epoch 57, Training Loss: 0.7843044813414265\n",
      "Epoch 58, Training Loss: 0.7837265442188521\n",
      "Epoch 59, Training Loss: 0.7837298526799769\n",
      "Epoch 60, Training Loss: 0.7841479910943742\n",
      "Epoch 61, Training Loss: 0.7833222755812165\n",
      "Epoch 62, Training Loss: 0.7834035021918161\n",
      "Epoch 63, Training Loss: 0.7836138063803652\n",
      "Epoch 64, Training Loss: 0.7831439013319804\n",
      "Epoch 65, Training Loss: 0.782693550550848\n",
      "Epoch 66, Training Loss: 0.7826561537900365\n",
      "Epoch 67, Training Loss: 0.7841067703146684\n",
      "Epoch 68, Training Loss: 0.7829483101242467\n",
      "Epoch 69, Training Loss: 0.7823198739747356\n",
      "Epoch 70, Training Loss: 0.7824920365684911\n",
      "Epoch 71, Training Loss: 0.7826276959333205\n",
      "Epoch 72, Training Loss: 0.7821154365862223\n",
      "Epoch 73, Training Loss: 0.7823728380346657\n",
      "Epoch 74, Training Loss: 0.7821635141408533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:49:35,964] Trial 321 finished with value: 0.6366666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7819467324959605\n",
      "Epoch 1, Training Loss: 0.890665612633067\n",
      "Epoch 2, Training Loss: 0.817583251537237\n",
      "Epoch 3, Training Loss: 0.8123540726819433\n",
      "Epoch 4, Training Loss: 0.8092948103309574\n",
      "Epoch 5, Training Loss: 0.8069723263719029\n",
      "Epoch 6, Training Loss: 0.8034024300431847\n",
      "Epoch 7, Training Loss: 0.8028311608429242\n",
      "Epoch 8, Training Loss: 0.8032219107886006\n",
      "Epoch 9, Training Loss: 0.8017126092337128\n",
      "Epoch 10, Training Loss: 0.8003536798003921\n",
      "Epoch 11, Training Loss: 0.8002205382612415\n",
      "Epoch 12, Training Loss: 0.7997705242687598\n",
      "Epoch 13, Training Loss: 0.7996137683552907\n",
      "Epoch 14, Training Loss: 0.799453762330507\n",
      "Epoch 15, Training Loss: 0.7989662970815387\n",
      "Epoch 16, Training Loss: 0.7989525829042707\n",
      "Epoch 17, Training Loss: 0.7977407277974867\n",
      "Epoch 18, Training Loss: 0.7967777636714448\n",
      "Epoch 19, Training Loss: 0.7964004272805121\n",
      "Epoch 20, Training Loss: 0.796253096071401\n",
      "Epoch 21, Training Loss: 0.7957309758752809\n",
      "Epoch 22, Training Loss: 0.7943810957715027\n",
      "Epoch 23, Training Loss: 0.7945409852759282\n",
      "Epoch 24, Training Loss: 0.7935344454041101\n",
      "Epoch 25, Training Loss: 0.7922448464802333\n",
      "Epoch 26, Training Loss: 0.7918755473947167\n",
      "Epoch 27, Training Loss: 0.7911705272538322\n",
      "Epoch 28, Training Loss: 0.7899823489045739\n",
      "Epoch 29, Training Loss: 0.7890750847364727\n",
      "Epoch 30, Training Loss: 0.7893998492929272\n",
      "Epoch 31, Training Loss: 0.7881325464499624\n",
      "Epoch 32, Training Loss: 0.7885938689224702\n",
      "Epoch 33, Training Loss: 0.7876499283582644\n",
      "Epoch 34, Training Loss: 0.7868378268148666\n",
      "Epoch 35, Training Loss: 0.7868999901122616\n",
      "Epoch 36, Training Loss: 0.7861607975529549\n",
      "Epoch 37, Training Loss: 0.7863462883726995\n",
      "Epoch 38, Training Loss: 0.7861777169363839\n",
      "Epoch 39, Training Loss: 0.7857878864259649\n",
      "Epoch 40, Training Loss: 0.785286005278279\n",
      "Epoch 41, Training Loss: 0.7858507647550196\n",
      "Epoch 42, Training Loss: 0.7855577940331366\n",
      "Epoch 43, Training Loss: 0.7848938250900211\n",
      "Epoch 44, Training Loss: 0.7846155775220771\n",
      "Epoch 45, Training Loss: 0.784299125259084\n",
      "Epoch 46, Training Loss: 0.7837745143954915\n",
      "Epoch 47, Training Loss: 0.7837838242824813\n",
      "Epoch 48, Training Loss: 0.7840480174337114\n",
      "Epoch 49, Training Loss: 0.7834798774324861\n",
      "Epoch 50, Training Loss: 0.7836536948842213\n",
      "Epoch 51, Training Loss: 0.7843224383834609\n",
      "Epoch 52, Training Loss: 0.7833019633042185\n",
      "Epoch 53, Training Loss: 0.7837081075610971\n",
      "Epoch 54, Training Loss: 0.7829646763048674\n",
      "Epoch 55, Training Loss: 0.7828070202268156\n",
      "Epoch 56, Training Loss: 0.7827986397241291\n",
      "Epoch 57, Training Loss: 0.7837028615456775\n",
      "Epoch 58, Training Loss: 0.7831697711370942\n",
      "Epoch 59, Training Loss: 0.7829369820150217\n",
      "Epoch 60, Training Loss: 0.7837721820164444\n",
      "Epoch 61, Training Loss: 0.7832975371439654\n",
      "Epoch 62, Training Loss: 0.7824153045066318\n",
      "Epoch 63, Training Loss: 0.7828123790877206\n",
      "Epoch 64, Training Loss: 0.7828298067688045\n",
      "Epoch 65, Training Loss: 0.7822343531407808\n",
      "Epoch 66, Training Loss: 0.7823166827062019\n",
      "Epoch 67, Training Loss: 0.7823066463147788\n",
      "Epoch 68, Training Loss: 0.7828973793445673\n",
      "Epoch 69, Training Loss: 0.7821177147384873\n",
      "Epoch 70, Training Loss: 0.7826471767927471\n",
      "Epoch 71, Training Loss: 0.7814561129960799\n",
      "Epoch 72, Training Loss: 0.7822934783490977\n",
      "Epoch 73, Training Loss: 0.7819345110341123\n",
      "Epoch 74, Training Loss: 0.7820186242125088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:51:04,652] Trial 322 finished with value: 0.6396666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7817062397648518\n",
      "Epoch 1, Training Loss: 1.0677830693417025\n",
      "Epoch 2, Training Loss: 0.9628386932208126\n",
      "Epoch 3, Training Loss: 0.9216751944749875\n",
      "Epoch 4, Training Loss: 0.8935992443471923\n",
      "Epoch 5, Training Loss: 0.8485924900026249\n",
      "Epoch 6, Training Loss: 0.8172926315687653\n",
      "Epoch 7, Training Loss: 0.8091883218378053\n",
      "Epoch 8, Training Loss: 0.8060185866248338\n",
      "Epoch 9, Training Loss: 0.8046277286862968\n",
      "Epoch 10, Training Loss: 0.8033393413500678\n",
      "Epoch 11, Training Loss: 0.8031162690399285\n",
      "Epoch 12, Training Loss: 0.8021102262618847\n",
      "Epoch 13, Training Loss: 0.8014445384642235\n",
      "Epoch 14, Training Loss: 0.8002029902056644\n",
      "Epoch 15, Training Loss: 0.7998115327125205\n",
      "Epoch 16, Training Loss: 0.7980801580543805\n",
      "Epoch 17, Training Loss: 0.7978051874870644\n",
      "Epoch 18, Training Loss: 0.7977939034763135\n",
      "Epoch 19, Training Loss: 0.7962607879387705\n",
      "Epoch 20, Training Loss: 0.7955888800154951\n",
      "Epoch 21, Training Loss: 0.7959734517828863\n",
      "Epoch 22, Training Loss: 0.7947117547343548\n",
      "Epoch 23, Training Loss: 0.7943123200782259\n",
      "Epoch 24, Training Loss: 0.7935916653253082\n",
      "Epoch 25, Training Loss: 0.7931169462383242\n",
      "Epoch 26, Training Loss: 0.7931118449770418\n",
      "Epoch 27, Training Loss: 0.7923228612519745\n",
      "Epoch 28, Training Loss: 0.7920678847714474\n",
      "Epoch 29, Training Loss: 0.7919867399043606\n",
      "Epoch 30, Training Loss: 0.7913346797900093\n",
      "Epoch 31, Training Loss: 0.7905436607231772\n",
      "Epoch 32, Training Loss: 0.7895850710402754\n",
      "Epoch 33, Training Loss: 0.7896692113768785\n",
      "Epoch 34, Training Loss: 0.7888221817805354\n",
      "Epoch 35, Training Loss: 0.7896303587390068\n",
      "Epoch 36, Training Loss: 0.7886063809681656\n",
      "Epoch 37, Training Loss: 0.7883248263731935\n",
      "Epoch 38, Training Loss: 0.7878889756991451\n",
      "Epoch 39, Training Loss: 0.7874124736714184\n",
      "Epoch 40, Training Loss: 0.7867919017497759\n",
      "Epoch 41, Training Loss: 0.7865526628673525\n",
      "Epoch 42, Training Loss: 0.7863628453778145\n",
      "Epoch 43, Training Loss: 0.7868733129106966\n",
      "Epoch 44, Training Loss: 0.7854744783021453\n",
      "Epoch 45, Training Loss: 0.7868078480089518\n",
      "Epoch 46, Training Loss: 0.7854144883335085\n",
      "Epoch 47, Training Loss: 0.7849577376717015\n",
      "Epoch 48, Training Loss: 0.7856176127168469\n",
      "Epoch 49, Training Loss: 0.784974627745779\n",
      "Epoch 50, Training Loss: 0.7846935532146827\n",
      "Epoch 51, Training Loss: 0.7846005843098002\n",
      "Epoch 52, Training Loss: 0.7851194071590453\n",
      "Epoch 53, Training Loss: 0.7851062389244711\n",
      "Epoch 54, Training Loss: 0.7839462822541259\n",
      "Epoch 55, Training Loss: 0.7840553635941412\n",
      "Epoch 56, Training Loss: 0.7837343517999004\n",
      "Epoch 57, Training Loss: 0.7837082841342553\n",
      "Epoch 58, Training Loss: 0.7834277227408903\n",
      "Epoch 59, Training Loss: 0.7831541722878478\n",
      "Epoch 60, Training Loss: 0.7838524532497377\n",
      "Epoch 61, Training Loss: 0.7833429203893906\n",
      "Epoch 62, Training Loss: 0.7835313174061309\n",
      "Epoch 63, Training Loss: 0.7837118221404857\n",
      "Epoch 64, Training Loss: 0.7826679018207062\n",
      "Epoch 65, Training Loss: 0.7825532400518431\n",
      "Epoch 66, Training Loss: 0.7828393925401501\n",
      "Epoch 67, Training Loss: 0.7826008242772038\n",
      "Epoch 68, Training Loss: 0.7824257093264644\n",
      "Epoch 69, Training Loss: 0.7822775248298072\n",
      "Epoch 70, Training Loss: 0.7825326973334291\n",
      "Epoch 71, Training Loss: 0.7822541112290289\n",
      "Epoch 72, Training Loss: 0.7827062182856682\n",
      "Epoch 73, Training Loss: 0.782391075173715\n",
      "Epoch 74, Training Loss: 0.7817480714697587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:52:21,510] Trial 323 finished with value: 0.6259333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7817073303057736\n",
      "Epoch 1, Training Loss: 0.9761369108452517\n",
      "Epoch 2, Training Loss: 0.927595385242911\n",
      "Epoch 3, Training Loss: 0.8779103444604313\n",
      "Epoch 4, Training Loss: 0.8323257421745973\n",
      "Epoch 5, Training Loss: 0.8199362104079303\n",
      "Epoch 6, Training Loss: 0.8150006215712603\n",
      "Epoch 7, Training Loss: 0.8121739388213438\n",
      "Epoch 8, Training Loss: 0.8101640403971953\n",
      "Epoch 9, Training Loss: 0.8087043856172\n",
      "Epoch 10, Training Loss: 0.8066171762522529\n",
      "Epoch 11, Training Loss: 0.8050283760182998\n",
      "Epoch 12, Training Loss: 0.8045244920954985\n",
      "Epoch 13, Training Loss: 0.8035093142004575\n",
      "Epoch 14, Training Loss: 0.8025858765489915\n",
      "Epoch 15, Training Loss: 0.8019038386204663\n",
      "Epoch 16, Training Loss: 0.8010213720097261\n",
      "Epoch 17, Training Loss: 0.8008639108433443\n",
      "Epoch 18, Training Loss: 0.8002580271047705\n",
      "Epoch 19, Training Loss: 0.8002684262920828\n",
      "Epoch 20, Training Loss: 0.7999623860331143\n",
      "Epoch 21, Training Loss: 0.7995008984032799\n",
      "Epoch 22, Training Loss: 0.7990634549365324\n",
      "Epoch 23, Training Loss: 0.7986236402567695\n",
      "Epoch 24, Training Loss: 0.7983351511113784\n",
      "Epoch 25, Training Loss: 0.7977864710723652\n",
      "Epoch 26, Training Loss: 0.7976808202967924\n",
      "Epoch 27, Training Loss: 0.7971759557723999\n",
      "Epoch 28, Training Loss: 0.7967399558600258\n",
      "Epoch 29, Training Loss: 0.7964001375787398\n",
      "Epoch 30, Training Loss: 0.7955435306184432\n",
      "Epoch 31, Training Loss: 0.7952351060334374\n",
      "Epoch 32, Training Loss: 0.7951179047191844\n",
      "Epoch 33, Training Loss: 0.7945824581034043\n",
      "Epoch 34, Training Loss: 0.793697190915837\n",
      "Epoch 35, Training Loss: 0.7933677188087912\n",
      "Epoch 36, Training Loss: 0.7924788522720337\n",
      "Epoch 37, Training Loss: 0.792128814388724\n",
      "Epoch 38, Training Loss: 0.7913660157428068\n",
      "Epoch 39, Training Loss: 0.790876954162822\n",
      "Epoch 40, Training Loss: 0.7902714709674611\n",
      "Epoch 41, Training Loss: 0.7897703925301047\n",
      "Epoch 42, Training Loss: 0.7894026091519524\n",
      "Epoch 43, Training Loss: 0.7891871726512909\n",
      "Epoch 44, Training Loss: 0.7886680191404679\n",
      "Epoch 45, Training Loss: 0.7877039114166708\n",
      "Epoch 46, Training Loss: 0.7877739124438342\n",
      "Epoch 47, Training Loss: 0.7872641438596388\n",
      "Epoch 48, Training Loss: 0.7870228294064017\n",
      "Epoch 49, Training Loss: 0.7867158781079685\n",
      "Epoch 50, Training Loss: 0.7867042012074414\n",
      "Epoch 51, Training Loss: 0.786540169505512\n",
      "Epoch 52, Training Loss: 0.7862855289964115\n",
      "Epoch 53, Training Loss: 0.785783846167957\n",
      "Epoch 54, Training Loss: 0.7858637221420512\n",
      "Epoch 55, Training Loss: 0.785897202982622\n",
      "Epoch 56, Training Loss: 0.7856386056367088\n",
      "Epoch 57, Training Loss: 0.7855601413810954\n",
      "Epoch 58, Training Loss: 0.7853299312030568\n",
      "Epoch 59, Training Loss: 0.7852834216286154\n",
      "Epoch 60, Training Loss: 0.7852369775491602\n",
      "Epoch 61, Training Loss: 0.7848598594525281\n",
      "Epoch 62, Training Loss: 0.7849418013236102\n",
      "Epoch 63, Training Loss: 0.7847541553132674\n",
      "Epoch 64, Training Loss: 0.7850100520779105\n",
      "Epoch 65, Training Loss: 0.784894791841507\n",
      "Epoch 66, Training Loss: 0.7848908849323497\n",
      "Epoch 67, Training Loss: 0.7845365536212922\n",
      "Epoch 68, Training Loss: 0.7845549061719109\n",
      "Epoch 69, Training Loss: 0.7843041887704064\n",
      "Epoch 70, Training Loss: 0.7843924595327938\n",
      "Epoch 71, Training Loss: 0.7840821296327254\n",
      "Epoch 72, Training Loss: 0.7841396925028633\n",
      "Epoch 73, Training Loss: 0.7843581182816449\n",
      "Epoch 74, Training Loss: 0.7842371205722585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:53:53,380] Trial 324 finished with value: 0.639 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7840439873583177\n",
      "Epoch 1, Training Loss: 1.0384366163085488\n",
      "Epoch 2, Training Loss: 0.9905440687432009\n",
      "Epoch 3, Training Loss: 0.9716286648021025\n",
      "Epoch 4, Training Loss: 0.9627335049825556\n",
      "Epoch 5, Training Loss: 0.9579814476826611\n",
      "Epoch 6, Training Loss: 0.955119174157872\n",
      "Epoch 7, Training Loss: 0.953124345961739\n",
      "Epoch 8, Training Loss: 0.9515709879117854\n",
      "Epoch 9, Training Loss: 0.9502498349021463\n",
      "Epoch 10, Training Loss: 0.9490372751740849\n",
      "Epoch 11, Training Loss: 0.9478864926450393\n",
      "Epoch 12, Training Loss: 0.946764847460915\n",
      "Epoch 13, Training Loss: 0.9456659138202668\n",
      "Epoch 14, Training Loss: 0.9445809149040896\n",
      "Epoch 15, Training Loss: 0.9434973907470703\n",
      "Epoch 16, Training Loss: 0.9424141713450936\n",
      "Epoch 17, Training Loss: 0.9413367990185233\n",
      "Epoch 18, Training Loss: 0.9402750963323256\n",
      "Epoch 19, Training Loss: 0.9391964920829324\n",
      "Epoch 20, Training Loss: 0.9381296229362488\n",
      "Epoch 21, Training Loss: 0.9370644617781919\n",
      "Epoch 22, Training Loss: 0.9360000559862922\n",
      "Epoch 23, Training Loss: 0.9349329544516171\n",
      "Epoch 24, Training Loss: 0.9338639187812805\n",
      "Epoch 25, Training Loss: 0.9327975862867692\n",
      "Epoch 26, Training Loss: 0.9317256328638862\n",
      "Epoch 27, Training Loss: 0.9306617670900681\n",
      "Epoch 28, Training Loss: 0.9295908409006456\n",
      "Epoch 29, Training Loss: 0.9285278877090005\n",
      "Epoch 30, Training Loss: 0.9274619535838856\n",
      "Epoch 31, Training Loss: 0.9263873767151553\n",
      "Epoch 32, Training Loss: 0.9253179017235251\n",
      "Epoch 33, Training Loss: 0.9242515395669376\n",
      "Epoch 34, Training Loss: 0.9231828817199258\n",
      "Epoch 35, Training Loss: 0.9221101355552673\n",
      "Epoch 36, Training Loss: 0.9210345177089467\n",
      "Epoch 37, Training Loss: 0.9199658623162438\n",
      "Epoch 38, Training Loss: 0.9188921997126411\n",
      "Epoch 39, Training Loss: 0.9178257345452028\n",
      "Epoch 40, Training Loss: 0.9167479198820451\n",
      "Epoch 41, Training Loss: 0.91567645185134\n",
      "Epoch 42, Training Loss: 0.9146017952526317\n",
      "Epoch 43, Training Loss: 0.9135268181913039\n",
      "Epoch 44, Training Loss: 0.9124534092229956\n",
      "Epoch 45, Training Loss: 0.9113825325404896\n",
      "Epoch 46, Training Loss: 0.9103115388926337\n",
      "Epoch 47, Training Loss: 0.9092340690248153\n",
      "Epoch 48, Training Loss: 0.9081585461953107\n",
      "Epoch 49, Training Loss: 0.9070935947053572\n",
      "Epoch 50, Training Loss: 0.9060175782091477\n",
      "Epoch 51, Training Loss: 0.9049478366094477\n",
      "Epoch 52, Training Loss: 0.9038758533842424\n",
      "Epoch 53, Training Loss: 0.9028097814672134\n",
      "Epoch 54, Training Loss: 0.9017499948249144\n",
      "Epoch 55, Training Loss: 0.9006803297996521\n",
      "Epoch 56, Training Loss: 0.8996139409962822\n",
      "Epoch 57, Training Loss: 0.898555031523985\n",
      "Epoch 58, Training Loss: 0.8974912884656121\n",
      "Epoch 59, Training Loss: 0.8964334516665515\n",
      "Epoch 60, Training Loss: 0.8953802439745735\n",
      "Epoch 61, Training Loss: 0.8943142089423012\n",
      "Epoch 62, Training Loss: 0.8932767824565663\n",
      "Epoch 63, Training Loss: 0.8922254560274236\n",
      "Epoch 64, Training Loss: 0.8911841372181387\n",
      "Epoch 65, Training Loss: 0.8901427979329053\n",
      "Epoch 66, Training Loss: 0.8890961713650647\n",
      "Epoch 67, Training Loss: 0.8880742835297304\n",
      "Epoch 68, Training Loss: 0.8870433428007014\n",
      "Epoch 69, Training Loss: 0.8860165237679201\n",
      "Epoch 70, Training Loss: 0.8849973267667434\n",
      "Epoch 71, Training Loss: 0.8839750894378213\n",
      "Epoch 72, Training Loss: 0.8829558535183177\n",
      "Epoch 73, Training Loss: 0.8819584495179793\n",
      "Epoch 74, Training Loss: 0.8809528268786038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:55:02,131] Trial 325 finished with value: 0.5889333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8799607671709622\n",
      "Epoch 1, Training Loss: 1.0900180872221639\n",
      "Epoch 2, Training Loss: 1.0871776908860171\n",
      "Epoch 3, Training Loss: 1.085261971609933\n",
      "Epoch 4, Training Loss: 1.0827586817562131\n",
      "Epoch 5, Training Loss: 1.0800306892036495\n",
      "Epoch 6, Training Loss: 1.0767745165000284\n",
      "Epoch 7, Training Loss: 1.0729351965108311\n",
      "Epoch 8, Training Loss: 1.0680466628612433\n",
      "Epoch 9, Training Loss: 1.0623927874672683\n",
      "Epoch 10, Training Loss: 1.055443374913438\n",
      "Epoch 11, Training Loss: 1.0474839902461919\n",
      "Epoch 12, Training Loss: 1.0383504435532076\n",
      "Epoch 13, Training Loss: 1.0283345459099102\n",
      "Epoch 14, Training Loss: 1.018161826384695\n",
      "Epoch 15, Training Loss: 1.008233788676728\n",
      "Epoch 16, Training Loss: 0.99938827322838\n",
      "Epoch 17, Training Loss: 0.9916772541246917\n",
      "Epoch 18, Training Loss: 0.9854274857313113\n",
      "Epoch 19, Training Loss: 0.9803712377870889\n",
      "Epoch 20, Training Loss: 0.9760699456795714\n",
      "Epoch 21, Training Loss: 0.9725128236569857\n",
      "Epoch 22, Training Loss: 0.9690088635996769\n",
      "Epoch 23, Training Loss: 0.9661501908660831\n",
      "Epoch 24, Training Loss: 0.9636196644682633\n",
      "Epoch 25, Training Loss: 0.9620102944230675\n",
      "Epoch 26, Training Loss: 0.959607580192107\n",
      "Epoch 27, Training Loss: 0.9580877216238725\n",
      "Epoch 28, Training Loss: 0.9565854560163685\n",
      "Epoch 29, Training Loss: 0.9553058617097094\n",
      "Epoch 30, Training Loss: 0.9542865830256526\n",
      "Epoch 31, Training Loss: 0.9534296830793969\n",
      "Epoch 32, Training Loss: 0.952347451403625\n",
      "Epoch 33, Training Loss: 0.9513411735233508\n",
      "Epoch 34, Training Loss: 0.9503695524724802\n",
      "Epoch 35, Training Loss: 0.9497545765754872\n",
      "Epoch 36, Training Loss: 0.9488218638233672\n",
      "Epoch 37, Training Loss: 0.947740500045002\n",
      "Epoch 38, Training Loss: 0.9472784761199378\n",
      "Epoch 39, Training Loss: 0.9464238576422956\n",
      "Epoch 40, Training Loss: 0.9446077526063847\n",
      "Epoch 41, Training Loss: 0.9437980356073021\n",
      "Epoch 42, Training Loss: 0.9423231973683923\n",
      "Epoch 43, Training Loss: 0.9417079101827808\n",
      "Epoch 44, Training Loss: 0.9408469119466337\n",
      "Epoch 45, Training Loss: 0.9396584560100297\n",
      "Epoch 46, Training Loss: 0.9382580800164014\n",
      "Epoch 47, Training Loss: 0.9377243898864975\n",
      "Epoch 48, Training Loss: 0.9360439474421336\n",
      "Epoch 49, Training Loss: 0.9342272446567851\n",
      "Epoch 50, Training Loss: 0.9332914974456443\n",
      "Epoch 51, Training Loss: 0.9318344965016931\n",
      "Epoch 52, Training Loss: 0.9303855918403855\n",
      "Epoch 53, Training Loss: 0.9291789650917053\n",
      "Epoch 54, Training Loss: 0.9272321948431488\n",
      "Epoch 55, Training Loss: 0.9258402382520805\n",
      "Epoch 56, Training Loss: 0.9241917508885377\n",
      "Epoch 57, Training Loss: 0.9220359583546345\n",
      "Epoch 58, Training Loss: 0.9202716199975265\n",
      "Epoch 59, Training Loss: 0.918598931445215\n",
      "Epoch 60, Training Loss: 0.9167540237419587\n",
      "Epoch 61, Training Loss: 0.9150398166556107\n",
      "Epoch 62, Training Loss: 0.9127216711976475\n",
      "Epoch 63, Training Loss: 0.9110861970966023\n",
      "Epoch 64, Training Loss: 0.9085753072473339\n",
      "Epoch 65, Training Loss: 0.9066592794611938\n",
      "Epoch 66, Training Loss: 0.9043500141093606\n",
      "Epoch 67, Training Loss: 0.9021909293375517\n",
      "Epoch 68, Training Loss: 0.8995741128025199\n",
      "Epoch 69, Training Loss: 0.8971726845977898\n",
      "Epoch 70, Training Loss: 0.8944227727732263\n",
      "Epoch 71, Training Loss: 0.8924575629987215\n",
      "Epoch 72, Training Loss: 0.8896738766727591\n",
      "Epoch 73, Training Loss: 0.8873663540173294\n",
      "Epoch 74, Training Loss: 0.8846480917213555\n",
      "Epoch 75, Training Loss: 0.881946711701558\n",
      "Epoch 76, Training Loss: 0.8796494137971921\n",
      "Epoch 77, Training Loss: 0.8765878231005562\n",
      "Epoch 78, Training Loss: 0.8739458450697418\n",
      "Epoch 79, Training Loss: 0.8709700207064922\n",
      "Epoch 80, Training Loss: 0.8680631118609493\n",
      "Epoch 81, Training Loss: 0.8652567568578218\n",
      "Epoch 82, Training Loss: 0.8628703838900516\n",
      "Epoch 83, Training Loss: 0.8599517141965995\n",
      "Epoch 84, Training Loss: 0.8577713784418608\n",
      "Epoch 85, Training Loss: 0.8551059973867317\n",
      "Epoch 86, Training Loss: 0.8527251144100849\n",
      "Epoch 87, Training Loss: 0.8503762475530008\n",
      "Epoch 88, Training Loss: 0.8480371146273792\n",
      "Epoch 89, Training Loss: 0.8460148560373406\n",
      "Epoch 90, Training Loss: 0.8438435070496753\n",
      "Epoch 91, Training Loss: 0.8416238103594099\n",
      "Epoch 92, Training Loss: 0.8395480283220907\n",
      "Epoch 93, Training Loss: 0.8382485690869783\n",
      "Epoch 94, Training Loss: 0.8363000272808219\n",
      "Epoch 95, Training Loss: 0.8344998834724713\n",
      "Epoch 96, Training Loss: 0.8335337071490467\n",
      "Epoch 97, Training Loss: 0.8318326788737361\n",
      "Epoch 98, Training Loss: 0.8300453905772446\n",
      "Epoch 99, Training Loss: 0.8289764336177281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:56:33,117] Trial 326 finished with value: 0.6188 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8278893611484901\n",
      "Epoch 1, Training Loss: 1.0619642934404818\n",
      "Epoch 2, Training Loss: 1.0225845379936964\n",
      "Epoch 3, Training Loss: 0.9983346352899881\n",
      "Epoch 4, Training Loss: 0.9824201079239523\n",
      "Epoch 5, Training Loss: 0.9722476982532587\n",
      "Epoch 6, Training Loss: 0.9667390890587542\n",
      "Epoch 7, Training Loss: 0.9629702364591728\n",
      "Epoch 8, Training Loss: 0.9608901687134478\n",
      "Epoch 9, Training Loss: 0.9593883257163198\n",
      "Epoch 10, Training Loss: 0.9588451618538764\n",
      "Epoch 11, Training Loss: 0.9569837648169439\n",
      "Epoch 12, Training Loss: 0.9558327270629711\n",
      "Epoch 13, Training Loss: 0.9553467868862295\n",
      "Epoch 14, Training Loss: 0.9540253609642947\n",
      "Epoch 15, Training Loss: 0.9534946066096313\n",
      "Epoch 16, Training Loss: 0.9525190587330582\n",
      "Epoch 17, Training Loss: 0.9517416416254258\n",
      "Epoch 18, Training Loss: 0.9507816252851845\n",
      "Epoch 19, Training Loss: 0.9498269284578195\n",
      "Epoch 20, Training Loss: 0.9492964622669651\n",
      "Epoch 21, Training Loss: 0.9478959307634741\n",
      "Epoch 22, Training Loss: 0.9464874634169098\n",
      "Epoch 23, Training Loss: 0.9455173518424643\n",
      "Epoch 24, Training Loss: 0.9448161031966819\n",
      "Epoch 25, Training Loss: 0.943374234393127\n",
      "Epoch 26, Training Loss: 0.9424567167920278\n",
      "Epoch 27, Training Loss: 0.9417012553465994\n",
      "Epoch 28, Training Loss: 0.9398108450989974\n",
      "Epoch 29, Training Loss: 0.9387874532462959\n",
      "Epoch 30, Training Loss: 0.9378720936022307\n",
      "Epoch 31, Training Loss: 0.9368297658468547\n",
      "Epoch 32, Training Loss: 0.935216266409795\n",
      "Epoch 33, Training Loss: 0.9338287588348962\n",
      "Epoch 34, Training Loss: 0.9322191383605613\n",
      "Epoch 35, Training Loss: 0.9307542623433852\n",
      "Epoch 36, Training Loss: 0.9294949554859248\n",
      "Epoch 37, Training Loss: 0.9283577410798324\n",
      "Epoch 38, Training Loss: 0.9265383513350236\n",
      "Epoch 39, Training Loss: 0.9250795284608253\n",
      "Epoch 40, Training Loss: 0.9236921971005605\n",
      "Epoch 41, Training Loss: 0.9221163338288328\n",
      "Epoch 42, Training Loss: 0.9206347994338301\n",
      "Epoch 43, Training Loss: 0.9186041973587266\n",
      "Epoch 44, Training Loss: 0.9170603256476553\n",
      "Epoch 45, Training Loss: 0.9156211266840311\n",
      "Epoch 46, Training Loss: 0.9138056497824819\n",
      "Epoch 47, Training Loss: 0.9118559085336843\n",
      "Epoch 48, Training Loss: 0.9098139798730835\n",
      "Epoch 49, Training Loss: 0.908359938815124\n",
      "Epoch 50, Training Loss: 0.9066725142916342\n",
      "Epoch 51, Training Loss: 0.9042697376774665\n",
      "Epoch 52, Training Loss: 0.902515333218682\n",
      "Epoch 53, Training Loss: 0.9009976947217956\n",
      "Epoch 54, Training Loss: 0.8988753872706478\n",
      "Epoch 55, Training Loss: 0.8969593614563907\n",
      "Epoch 56, Training Loss: 0.8949940072862725\n",
      "Epoch 57, Training Loss: 0.8933782523736021\n",
      "Epoch 58, Training Loss: 0.8913056908693529\n",
      "Epoch 59, Training Loss: 0.889224328134293\n",
      "Epoch 60, Training Loss: 0.8870807574207621\n",
      "Epoch 61, Training Loss: 0.8850947448185512\n",
      "Epoch 62, Training Loss: 0.8834131483744858\n",
      "Epoch 63, Training Loss: 0.8815537824666589\n",
      "Epoch 64, Training Loss: 0.879171253415875\n",
      "Epoch 65, Training Loss: 0.8773425652568502\n",
      "Epoch 66, Training Loss: 0.8753005599617062\n",
      "Epoch 67, Training Loss: 0.8734705641753692\n",
      "Epoch 68, Training Loss: 0.8716643704507584\n",
      "Epoch 69, Training Loss: 0.870156424834316\n",
      "Epoch 70, Training Loss: 0.8679391400258344\n",
      "Epoch 71, Training Loss: 0.8658220037481839\n",
      "Epoch 72, Training Loss: 0.8639442637450713\n",
      "Epoch 73, Training Loss: 0.8623887945834855\n",
      "Epoch 74, Training Loss: 0.8606097351339527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:57:33,044] Trial 327 finished with value: 0.6038 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8590095354202099\n",
      "Epoch 1, Training Loss: 1.0941459368942374\n",
      "Epoch 2, Training Loss: 1.090887401337014\n",
      "Epoch 3, Training Loss: 1.0905679121949619\n",
      "Epoch 4, Training Loss: 1.0904386900421372\n",
      "Epoch 5, Training Loss: 1.0901773361335123\n",
      "Epoch 6, Training Loss: 1.0899503851295413\n",
      "Epoch 7, Training Loss: 1.0897411473711631\n",
      "Epoch 8, Training Loss: 1.0895442544965817\n",
      "Epoch 9, Training Loss: 1.0892706308149753\n",
      "Epoch 10, Training Loss: 1.0890567933706412\n",
      "Epoch 11, Training Loss: 1.0887273385112446\n",
      "Epoch 12, Training Loss: 1.0885951904425943\n",
      "Epoch 13, Training Loss: 1.0883116811737978\n",
      "Epoch 14, Training Loss: 1.0881900217300071\n",
      "Epoch 15, Training Loss: 1.088038498476932\n",
      "Epoch 16, Training Loss: 1.0875994366810735\n",
      "Epoch 17, Training Loss: 1.0874840053400598\n",
      "Epoch 18, Training Loss: 1.0872501697755397\n",
      "Epoch 19, Training Loss: 1.0869877895914521\n",
      "Epoch 20, Training Loss: 1.0867948263211358\n",
      "Epoch 21, Training Loss: 1.0865477567328545\n",
      "Epoch 22, Training Loss: 1.0863695135690217\n",
      "Epoch 23, Training Loss: 1.0860897162803134\n",
      "Epoch 24, Training Loss: 1.0860021989148363\n",
      "Epoch 25, Training Loss: 1.0856452384389432\n",
      "Epoch 26, Training Loss: 1.0852968337840603\n",
      "Epoch 27, Training Loss: 1.0850685996220524\n",
      "Epoch 28, Training Loss: 1.0848986901735005\n",
      "Epoch 29, Training Loss: 1.0845550243119548\n",
      "Epoch 30, Training Loss: 1.084342292376927\n",
      "Epoch 31, Training Loss: 1.0840363201342131\n",
      "Epoch 32, Training Loss: 1.0837844468597182\n",
      "Epoch 33, Training Loss: 1.0836608938704757\n",
      "Epoch 34, Training Loss: 1.0833099861790363\n",
      "Epoch 35, Training Loss: 1.0829868422415023\n",
      "Epoch 36, Training Loss: 1.0827045437088587\n",
      "Epoch 37, Training Loss: 1.082476538285277\n",
      "Epoch 38, Training Loss: 1.0822127202399692\n",
      "Epoch 39, Training Loss: 1.0818643559190564\n",
      "Epoch 40, Training Loss: 1.0816300978337912\n",
      "Epoch 41, Training Loss: 1.0814220109380277\n",
      "Epoch 42, Training Loss: 1.080952457198523\n",
      "Epoch 43, Training Loss: 1.0807677256433588\n",
      "Epoch 44, Training Loss: 1.08039239277517\n",
      "Epoch 45, Training Loss: 1.0800387748201987\n",
      "Epoch 46, Training Loss: 1.0797963920392488\n",
      "Epoch 47, Training Loss: 1.0793334448247924\n",
      "Epoch 48, Training Loss: 1.0790993898434746\n",
      "Epoch 49, Training Loss: 1.0788096540852596\n",
      "Epoch 50, Training Loss: 1.0785001778064813\n",
      "Epoch 51, Training Loss: 1.0780736544974765\n",
      "Epoch 52, Training Loss: 1.0777675426095947\n",
      "Epoch 53, Training Loss: 1.0774399676717314\n",
      "Epoch 54, Training Loss: 1.0770353686540646\n",
      "Epoch 55, Training Loss: 1.0765281863678666\n",
      "Epoch 56, Training Loss: 1.0761730805375522\n",
      "Epoch 57, Training Loss: 1.075838778251992\n",
      "Epoch 58, Training Loss: 1.0755575210528265\n",
      "Epoch 59, Training Loss: 1.0750565021557916\n",
      "Epoch 60, Training Loss: 1.074641520277898\n",
      "Epoch 61, Training Loss: 1.074146818756161\n",
      "Epoch 62, Training Loss: 1.0736530322777598\n",
      "Epoch 63, Training Loss: 1.0733436441062985\n",
      "Epoch 64, Training Loss: 1.0728680090796678\n",
      "Epoch 65, Training Loss: 1.0723751138027449\n",
      "Epoch 66, Training Loss: 1.0719411767515026\n",
      "Epoch 67, Training Loss: 1.07153709024415\n",
      "Epoch 68, Training Loss: 1.0710290473206598\n",
      "Epoch 69, Training Loss: 1.070526735406173\n",
      "Epoch 70, Training Loss: 1.0700475524242659\n",
      "Epoch 71, Training Loss: 1.0694672874938276\n",
      "Epoch 72, Training Loss: 1.0690270552957866\n",
      "Epoch 73, Training Loss: 1.0684012456047804\n",
      "Epoch 74, Training Loss: 1.0678732845119965\n",
      "Epoch 75, Training Loss: 1.0674968438040942\n",
      "Epoch 76, Training Loss: 1.0668301587714288\n",
      "Epoch 77, Training Loss: 1.0661625064405285\n",
      "Epoch 78, Training Loss: 1.065689620577303\n",
      "Epoch 79, Training Loss: 1.0649977657131682\n",
      "Epoch 80, Training Loss: 1.0643884298496677\n",
      "Epoch 81, Training Loss: 1.0638083495591817\n",
      "Epoch 82, Training Loss: 1.0631291875265594\n",
      "Epoch 83, Training Loss: 1.0626253160318935\n",
      "Epoch 84, Training Loss: 1.0618569101606097\n",
      "Epoch 85, Training Loss: 1.0611128558789877\n",
      "Epoch 86, Training Loss: 1.060424378581513\n",
      "Epoch 87, Training Loss: 1.059893604328758\n",
      "Epoch 88, Training Loss: 1.059182224058567\n",
      "Epoch 89, Training Loss: 1.0584403704879874\n",
      "Epoch 90, Training Loss: 1.0576768183170404\n",
      "Epoch 91, Training Loss: 1.0569568180500117\n",
      "Epoch 92, Training Loss: 1.0561958445642228\n",
      "Epoch 93, Training Loss: 1.0555034736045321\n",
      "Epoch 94, Training Loss: 1.0546836403079498\n",
      "Epoch 95, Training Loss: 1.0538915955034414\n",
      "Epoch 96, Training Loss: 1.0529099191041817\n",
      "Epoch 97, Training Loss: 1.0521419905182114\n",
      "Epoch 98, Training Loss: 1.0513109395378515\n",
      "Epoch 99, Training Loss: 1.050439437170674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 04:59:02,967] Trial 328 finished with value: 0.468 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.049598895696769\n",
      "Epoch 1, Training Loss: 1.106326413154602\n",
      "Epoch 2, Training Loss: 1.0497582277129678\n",
      "Epoch 3, Training Loss: 1.018849202815224\n",
      "Epoch 4, Training Loss: 0.9988751861628364\n",
      "Epoch 5, Training Loss: 0.9852990958971136\n",
      "Epoch 6, Training Loss: 0.9758811246647554\n",
      "Epoch 7, Training Loss: 0.9692366917694316\n",
      "Epoch 8, Training Loss: 0.9644531607627869\n",
      "Epoch 9, Training Loss: 0.9609373871719136\n",
      "Epoch 10, Training Loss: 0.9582789228242986\n",
      "Epoch 11, Training Loss: 0.9562076645738938\n",
      "Epoch 12, Training Loss: 0.9545514407578637\n",
      "Epoch 13, Training Loss: 0.9531587933091556\n",
      "Epoch 14, Training Loss: 0.951967508021523\n",
      "Epoch 15, Training Loss: 0.9509053991822636\n",
      "Epoch 16, Training Loss: 0.9499350337421193\n",
      "Epoch 17, Training Loss: 0.9490251478026895\n",
      "Epoch 18, Training Loss: 0.948158658322166\n",
      "Epoch 19, Training Loss: 0.9473196608178756\n",
      "Epoch 20, Training Loss: 0.9464984862944659\n",
      "Epoch 21, Training Loss: 0.9457109743006089\n",
      "Epoch 22, Training Loss: 0.9449147001434774\n",
      "Epoch 23, Training Loss: 0.9441331047871534\n",
      "Epoch 24, Training Loss: 0.9433539584103753\n",
      "Epoch 25, Training Loss: 0.9425694762257969\n",
      "Epoch 26, Training Loss: 0.9417962265715879\n",
      "Epoch 27, Training Loss: 0.9410249405047473\n",
      "Epoch 28, Training Loss: 0.9402505910396576\n",
      "Epoch 29, Training Loss: 0.9394688273878659\n",
      "Epoch 30, Training Loss: 0.9387016349680284\n",
      "Epoch 31, Training Loss: 0.9379239207155564\n",
      "Epoch 32, Training Loss: 0.9371485666667714\n",
      "Epoch 33, Training Loss: 0.9363667236356175\n",
      "Epoch 34, Training Loss: 0.9355870426402373\n",
      "Epoch 35, Training Loss: 0.934807769761366\n",
      "Epoch 36, Training Loss: 0.9340213774933535\n",
      "Epoch 37, Training Loss: 0.9332371896855971\n",
      "Epoch 38, Training Loss: 0.9324466866605422\n",
      "Epoch 39, Training Loss: 0.9316584338160122\n",
      "Epoch 40, Training Loss: 0.9308672342580907\n",
      "Epoch 41, Training Loss: 0.9300744393292595\n",
      "Epoch 42, Training Loss: 0.9292722946054796\n",
      "Epoch 43, Training Loss: 0.9284714220551884\n",
      "Epoch 44, Training Loss: 0.9276666736602783\n",
      "Epoch 45, Training Loss: 0.9268635761036592\n",
      "Epoch 46, Training Loss: 0.9260531967527726\n",
      "Epoch 47, Training Loss: 0.9252355602208305\n",
      "Epoch 48, Training Loss: 0.9244138796189252\n",
      "Epoch 49, Training Loss: 0.9236006574069753\n",
      "Epoch 50, Training Loss: 0.9227692773762871\n",
      "Epoch 51, Training Loss: 0.9219435437987833\n",
      "Epoch 52, Training Loss: 0.9211102708648233\n",
      "Epoch 53, Training Loss: 0.9202738293479471\n",
      "Epoch 54, Training Loss: 0.9194308513052323\n",
      "Epoch 55, Training Loss: 0.9185901788403006\n",
      "Epoch 56, Training Loss: 0.9177391873387729\n",
      "Epoch 57, Training Loss: 0.9168842714674332\n",
      "Epoch 58, Training Loss: 0.9160215405856862\n",
      "Epoch 59, Training Loss: 0.9151633904961979\n",
      "Epoch 60, Training Loss: 0.9142954489764046\n",
      "Epoch 61, Training Loss: 0.9134315125381245\n",
      "Epoch 62, Training Loss: 0.9125504943202524\n",
      "Epoch 63, Training Loss: 0.9116723665770362\n",
      "Epoch 64, Training Loss: 0.9107923424243927\n",
      "Epoch 65, Training Loss: 0.9099059323002311\n",
      "Epoch 66, Training Loss: 0.9090162580854753\n",
      "Epoch 67, Training Loss: 0.9081178457596722\n",
      "Epoch 68, Training Loss: 0.9072198292087106\n",
      "Epoch 69, Training Loss: 0.9063200069175047\n",
      "Epoch 70, Training Loss: 0.9054123726311852\n",
      "Epoch 71, Training Loss: 0.9045062926236321\n",
      "Epoch 72, Training Loss: 0.9035916078791899\n",
      "Epoch 73, Training Loss: 0.9026755741063286\n",
      "Epoch 74, Training Loss: 0.901753149172839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:00:10,887] Trial 329 finished with value: 0.5758 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.9008297023352455\n",
      "Epoch 1, Training Loss: 0.8980206365872146\n",
      "Epoch 2, Training Loss: 0.8142343978236493\n",
      "Epoch 3, Training Loss: 0.8096038864967519\n",
      "Epoch 4, Training Loss: 0.8082736953308708\n",
      "Epoch 5, Training Loss: 0.8046210448544725\n",
      "Epoch 6, Training Loss: 0.8011215788081176\n",
      "Epoch 7, Training Loss: 0.7975673426362805\n",
      "Epoch 8, Training Loss: 0.795425992621515\n",
      "Epoch 9, Training Loss: 0.7946309035882018\n",
      "Epoch 10, Training Loss: 0.7932930729891124\n",
      "Epoch 11, Training Loss: 0.7910833000240469\n",
      "Epoch 12, Training Loss: 0.792692138198623\n",
      "Epoch 13, Training Loss: 0.7925211097064772\n",
      "Epoch 14, Training Loss: 0.7907300370976441\n",
      "Epoch 15, Training Loss: 0.790828816155742\n",
      "Epoch 16, Training Loss: 0.7911840182497986\n",
      "Epoch 17, Training Loss: 0.7894141449964136\n",
      "Epoch 18, Training Loss: 0.7909820004513389\n",
      "Epoch 19, Training Loss: 0.7880292040961129\n",
      "Epoch 20, Training Loss: 0.788372836794172\n",
      "Epoch 21, Training Loss: 0.7886397615411228\n",
      "Epoch 22, Training Loss: 0.7880497765720339\n",
      "Epoch 23, Training Loss: 0.7885006192931555\n",
      "Epoch 24, Training Loss: 0.7876027535675163\n",
      "Epoch 25, Training Loss: 0.7888211768372615\n",
      "Epoch 26, Training Loss: 0.7869133001879642\n",
      "Epoch 27, Training Loss: 0.78695324184303\n",
      "Epoch 28, Training Loss: 0.7869665713238537\n",
      "Epoch 29, Training Loss: 0.7870341366395018\n",
      "Epoch 30, Training Loss: 0.7855887950811171\n",
      "Epoch 31, Training Loss: 0.7859532974716416\n",
      "Epoch 32, Training Loss: 0.7868552042129344\n",
      "Epoch 33, Training Loss: 0.7862386383508381\n",
      "Epoch 34, Training Loss: 0.7859848446415779\n",
      "Epoch 35, Training Loss: 0.7857496302826961\n",
      "Epoch 36, Training Loss: 0.78592066890315\n",
      "Epoch 37, Training Loss: 0.7859927292156936\n",
      "Epoch 38, Training Loss: 0.7847570846851607\n",
      "Epoch 39, Training Loss: 0.7850090962603576\n",
      "Epoch 40, Training Loss: 0.7847131922729034\n",
      "Epoch 41, Training Loss: 0.7850256475290858\n",
      "Epoch 42, Training Loss: 0.784686252362746\n",
      "Epoch 43, Training Loss: 0.7842733597396908\n",
      "Epoch 44, Training Loss: 0.7840601111713209\n",
      "Epoch 45, Training Loss: 0.7848469912557674\n",
      "Epoch 46, Training Loss: 0.7845067916059852\n",
      "Epoch 47, Training Loss: 0.7832972695056657\n",
      "Epoch 48, Training Loss: 0.7835153873701741\n",
      "Epoch 49, Training Loss: 0.7820985898935705\n",
      "Epoch 50, Training Loss: 0.782664500232926\n",
      "Epoch 51, Training Loss: 0.7824861098052864\n",
      "Epoch 52, Training Loss: 0.7827495939749524\n",
      "Epoch 53, Training Loss: 0.7825083160758914\n",
      "Epoch 54, Training Loss: 0.7825654469934621\n",
      "Epoch 55, Training Loss: 0.7826475710797131\n",
      "Epoch 56, Training Loss: 0.7816609583402935\n",
      "Epoch 57, Training Loss: 0.7809994417921942\n",
      "Epoch 58, Training Loss: 0.7815968019621713\n",
      "Epoch 59, Training Loss: 0.7812791336747936\n",
      "Epoch 60, Training Loss: 0.7809774134392129\n",
      "Epoch 61, Training Loss: 0.781670265717614\n",
      "Epoch 62, Training Loss: 0.7814681709260869\n",
      "Epoch 63, Training Loss: 0.7805239201488351\n",
      "Epoch 64, Training Loss: 0.780206583137799\n",
      "Epoch 65, Training Loss: 0.7803576294640849\n",
      "Epoch 66, Training Loss: 0.780742195046934\n",
      "Epoch 67, Training Loss: 0.7797602260919442\n",
      "Epoch 68, Training Loss: 0.7797423377072901\n",
      "Epoch 69, Training Loss: 0.7799692468535632\n",
      "Epoch 70, Training Loss: 0.7798483510662738\n",
      "Epoch 71, Training Loss: 0.7796660972717113\n",
      "Epoch 72, Training Loss: 0.7804547356483632\n",
      "Epoch 73, Training Loss: 0.7790703675800696\n",
      "Epoch 74, Training Loss: 0.7793208435065764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:01:46,574] Trial 330 finished with value: 0.6360666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7783565763244056\n",
      "Epoch 1, Training Loss: 1.0942794509495006\n",
      "Epoch 2, Training Loss: 1.0908960333992452\n",
      "Epoch 3, Training Loss: 1.0907063401446624\n",
      "Epoch 4, Training Loss: 1.09053017504075\n",
      "Epoch 5, Training Loss: 1.0903647978165572\n",
      "Epoch 6, Training Loss: 1.0902038860321044\n",
      "Epoch 7, Training Loss: 1.0900242886823766\n",
      "Epoch 8, Training Loss: 1.0898513656504014\n",
      "Epoch 9, Training Loss: 1.0896881966029897\n",
      "Epoch 10, Training Loss: 1.0895014855440925\n",
      "Epoch 11, Training Loss: 1.0893371950878816\n",
      "Epoch 12, Training Loss: 1.0891636917170355\n",
      "Epoch 13, Training Loss: 1.0889886296496671\n",
      "Epoch 14, Training Loss: 1.0888066468519324\n",
      "Epoch 15, Training Loss: 1.0886295930077048\n",
      "Epoch 16, Training Loss: 1.0884568328015944\n",
      "Epoch 17, Training Loss: 1.08826645191978\n",
      "Epoch 18, Training Loss: 1.088094938923331\n",
      "Epoch 19, Training Loss: 1.0879186494210187\n",
      "Epoch 20, Training Loss: 1.0877321756587308\n",
      "Epoch 21, Training Loss: 1.0875413841359756\n",
      "Epoch 22, Training Loss: 1.0873493231044096\n",
      "Epoch 23, Training Loss: 1.0871568024859708\n",
      "Epoch 24, Training Loss: 1.086970616789425\n",
      "Epoch 25, Training Loss: 1.0867835058885462\n",
      "Epoch 26, Training Loss: 1.0865833302105175\n",
      "Epoch 27, Training Loss: 1.0863869183203754\n",
      "Epoch 28, Training Loss: 1.086182101614335\n",
      "Epoch 29, Training Loss: 1.085973389709697\n",
      "Epoch 30, Training Loss: 1.0857657819635729\n",
      "Epoch 31, Training Loss: 1.0855497866518358\n",
      "Epoch 32, Training Loss: 1.0853403276555678\n",
      "Epoch 33, Training Loss: 1.0851337906893561\n",
      "Epoch 34, Training Loss: 1.0849052232854506\n",
      "Epoch 35, Training Loss: 1.0846875273480134\n",
      "Epoch 36, Training Loss: 1.084463693815119\n",
      "Epoch 37, Training Loss: 1.0842257825066062\n",
      "Epoch 38, Training Loss: 1.0839877803185407\n",
      "Epoch 39, Training Loss: 1.0837457314659567\n",
      "Epoch 40, Training Loss: 1.0835045549448798\n",
      "Epoch 41, Training Loss: 1.083253337916206\n",
      "Epoch 42, Training Loss: 1.0829792280758128\n",
      "Epoch 43, Training Loss: 1.0827448114226845\n",
      "Epoch 44, Training Loss: 1.0824811255230624\n",
      "Epoch 45, Training Loss: 1.082213510765749\n",
      "Epoch 46, Training Loss: 1.0819356853821698\n",
      "Epoch 47, Training Loss: 1.081651945254382\n",
      "Epoch 48, Training Loss: 1.0813768640686483\n",
      "Epoch 49, Training Loss: 1.0810736688445597\n",
      "Epoch 50, Training Loss: 1.0807864288722768\n",
      "Epoch 51, Training Loss: 1.0804784072146696\n",
      "Epoch 52, Training Loss: 1.0801708792237674\n",
      "Epoch 53, Training Loss: 1.0798520175148458\n",
      "Epoch 54, Training Loss: 1.0795292056308072\n",
      "Epoch 55, Training Loss: 1.079188958196079\n",
      "Epoch 56, Training Loss: 1.0788550463844748\n",
      "Epoch 57, Training Loss: 1.078515050831963\n",
      "Epoch 58, Training Loss: 1.0781587975165423\n",
      "Epoch 59, Training Loss: 1.0777951313467586\n",
      "Epoch 60, Training Loss: 1.0774238848686217\n",
      "Epoch 61, Training Loss: 1.0770444496940164\n",
      "Epoch 62, Training Loss: 1.0766613777946024\n",
      "Epoch 63, Training Loss: 1.0762437241217668\n",
      "Epoch 64, Training Loss: 1.0758604463408976\n",
      "Epoch 65, Training Loss: 1.0754290513431324\n",
      "Epoch 66, Training Loss: 1.0750112034292783\n",
      "Epoch 67, Training Loss: 1.0745770054705002\n",
      "Epoch 68, Training Loss: 1.0741267014952267\n",
      "Epoch 69, Training Loss: 1.0736573534853318\n",
      "Epoch 70, Training Loss: 1.073201006861294\n",
      "Epoch 71, Training Loss: 1.0727171179827522\n",
      "Epoch 72, Training Loss: 1.0722279036746305\n",
      "Epoch 73, Training Loss: 1.0717257499694823\n",
      "Epoch 74, Training Loss: 1.0712110480140238\n",
      "Epoch 75, Training Loss: 1.0706844618741205\n",
      "Epoch 76, Training Loss: 1.0701327915752634\n",
      "Epoch 77, Training Loss: 1.0695830304482403\n",
      "Epoch 78, Training Loss: 1.0690039058292613\n",
      "Epoch 79, Training Loss: 1.0684364201040828\n",
      "Epoch 80, Training Loss: 1.067835968241972\n",
      "Epoch 81, Training Loss: 1.0672189622766832\n",
      "Epoch 82, Training Loss: 1.066601137974683\n",
      "Epoch 83, Training Loss: 1.0659600365863127\n",
      "Epoch 84, Training Loss: 1.0652975065567913\n",
      "Epoch 85, Training Loss: 1.0646229968351477\n",
      "Epoch 86, Training Loss: 1.0639423286213594\n",
      "Epoch 87, Training Loss: 1.063242773729212\n",
      "Epoch 88, Training Loss: 1.0625123092707465\n",
      "Epoch 89, Training Loss: 1.061769111436956\n",
      "Epoch 90, Training Loss: 1.0610104121881372\n",
      "Epoch 91, Training Loss: 1.0602501866396736\n",
      "Epoch 92, Training Loss: 1.0594514637834886\n",
      "Epoch 93, Training Loss: 1.0586314551970537\n",
      "Epoch 94, Training Loss: 1.0578114607754876\n",
      "Epoch 95, Training Loss: 1.056947199456832\n",
      "Epoch 96, Training Loss: 1.05609887305428\n",
      "Epoch 97, Training Loss: 1.0551914683510275\n",
      "Epoch 98, Training Loss: 1.0543085655044107\n",
      "Epoch 99, Training Loss: 1.0533779103615706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:03:30,743] Trial 331 finished with value: 0.4732 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 1.0524177138945636\n",
      "Epoch 1, Training Loss: 0.9046855499869899\n",
      "Epoch 2, Training Loss: 0.8220232915161247\n",
      "Epoch 3, Training Loss: 0.8152651730336641\n",
      "Epoch 4, Training Loss: 0.8103434024000526\n",
      "Epoch 5, Training Loss: 0.8097966490831591\n",
      "Epoch 6, Training Loss: 0.8098321321315335\n",
      "Epoch 7, Training Loss: 0.8069872156569832\n",
      "Epoch 8, Training Loss: 0.8059232791563622\n",
      "Epoch 9, Training Loss: 0.8065026708115313\n",
      "Epoch 10, Training Loss: 0.8051436357032088\n",
      "Epoch 11, Training Loss: 0.8064720062384928\n",
      "Epoch 12, Training Loss: 0.805203826535017\n",
      "Epoch 13, Training Loss: 0.8042792478898414\n",
      "Epoch 14, Training Loss: 0.8034694965620687\n",
      "Epoch 15, Training Loss: 0.8040614317234297\n",
      "Epoch 16, Training Loss: 0.8029919275663849\n",
      "Epoch 17, Training Loss: 0.8027176444691823\n",
      "Epoch 18, Training Loss: 0.802302194537973\n",
      "Epoch 19, Training Loss: 0.8034690641819087\n",
      "Epoch 20, Training Loss: 0.8015301736673914\n",
      "Epoch 21, Training Loss: 0.8027239350447978\n",
      "Epoch 22, Training Loss: 0.8021242847119955\n",
      "Epoch 23, Training Loss: 0.8032046661341101\n",
      "Epoch 24, Training Loss: 0.8031655290969333\n",
      "Epoch 25, Training Loss: 0.8018364096046391\n",
      "Epoch 26, Training Loss: 0.8011205958244496\n",
      "Epoch 27, Training Loss: 0.801306286521424\n",
      "Epoch 28, Training Loss: 0.800127434999423\n",
      "Epoch 29, Training Loss: 0.8013009948838026\n",
      "Epoch 30, Training Loss: 0.8011410946236517\n",
      "Epoch 31, Training Loss: 0.8005931327217504\n",
      "Epoch 32, Training Loss: 0.7999777344384588\n",
      "Epoch 33, Training Loss: 0.8011648677345505\n",
      "Epoch 34, Training Loss: 0.8006600690067263\n",
      "Epoch 35, Training Loss: 0.8006576573042045\n",
      "Epoch 36, Training Loss: 0.8001022529781313\n",
      "Epoch 37, Training Loss: 0.7999391203536127\n",
      "Epoch 38, Training Loss: 0.799656335303658\n",
      "Epoch 39, Training Loss: 0.8002661118830057\n",
      "Epoch 40, Training Loss: 0.8000159982451819\n",
      "Epoch 41, Training Loss: 0.8005794285831594\n",
      "Epoch 42, Training Loss: 0.7998644694349819\n",
      "Epoch 43, Training Loss: 0.7990782285991468\n",
      "Epoch 44, Training Loss: 0.800107889426382\n",
      "Epoch 45, Training Loss: 0.7991830704803754\n",
      "Epoch 46, Training Loss: 0.7989793780154751\n",
      "Epoch 47, Training Loss: 0.7988804231012674\n",
      "Epoch 48, Training Loss: 0.7998107293494662\n",
      "Epoch 49, Training Loss: 0.7992763004804913\n",
      "Epoch 50, Training Loss: 0.7985015472075097\n",
      "Epoch 51, Training Loss: 0.798132776676264\n",
      "Epoch 52, Training Loss: 0.7978547399205372\n",
      "Epoch 53, Training Loss: 0.7989425722817729\n",
      "Epoch 54, Training Loss: 0.7981315537054736\n",
      "Epoch 55, Training Loss: 0.797821088930718\n",
      "Epoch 56, Training Loss: 0.7982677583407639\n",
      "Epoch 57, Training Loss: 0.7977252647392732\n",
      "Epoch 58, Training Loss: 0.7974549654731177\n",
      "Epoch 59, Training Loss: 0.7976712968116416\n",
      "Epoch 60, Training Loss: 0.7982166226645161\n",
      "Epoch 61, Training Loss: 0.7972139419469618\n",
      "Epoch 62, Training Loss: 0.7968247871649893\n",
      "Epoch 63, Training Loss: 0.7976415455789494\n",
      "Epoch 64, Training Loss: 0.7973054603526467\n",
      "Epoch 65, Training Loss: 0.7969232239221272\n",
      "Epoch 66, Training Loss: 0.7964790948351523\n",
      "Epoch 67, Training Loss: 0.7960755205692205\n",
      "Epoch 68, Training Loss: 0.7958873092680049\n",
      "Epoch 69, Training Loss: 0.7959557080627384\n",
      "Epoch 70, Training Loss: 0.7957053308200119\n",
      "Epoch 71, Training Loss: 0.7958820196918975\n",
      "Epoch 72, Training Loss: 0.7954383854579209\n",
      "Epoch 73, Training Loss: 0.7954603896553355\n",
      "Epoch 74, Training Loss: 0.7946012449443789\n",
      "Epoch 75, Training Loss: 0.7961416516088902\n",
      "Epoch 76, Training Loss: 0.7957581234157534\n",
      "Epoch 77, Training Loss: 0.7956890385850032\n",
      "Epoch 78, Training Loss: 0.7958918942544694\n",
      "Epoch 79, Training Loss: 0.7948861383853998\n",
      "Epoch 80, Training Loss: 0.7951050195478855\n",
      "Epoch 81, Training Loss: 0.7943711400032043\n",
      "Epoch 82, Training Loss: 0.7946912983306369\n",
      "Epoch 83, Training Loss: 0.7948595589264891\n",
      "Epoch 84, Training Loss: 0.7944093898723\n",
      "Epoch 85, Training Loss: 0.7941835554022538\n",
      "Epoch 86, Training Loss: 0.7937096599349402\n",
      "Epoch 87, Training Loss: 0.7938841938076163\n",
      "Epoch 88, Training Loss: 0.7947650295451172\n",
      "Epoch 89, Training Loss: 0.7939848385359112\n",
      "Epoch 90, Training Loss: 0.7937432227278114\n",
      "Epoch 91, Training Loss: 0.7933954898576091\n",
      "Epoch 92, Training Loss: 0.7930053689426049\n",
      "Epoch 93, Training Loss: 0.7940974996502238\n",
      "Epoch 94, Training Loss: 0.7938158150006057\n",
      "Epoch 95, Training Loss: 0.7933609862076608\n",
      "Epoch 96, Training Loss: 0.7927377857659993\n",
      "Epoch 97, Training Loss: 0.7930857903975294\n",
      "Epoch 98, Training Loss: 0.7931815294394816\n",
      "Epoch 99, Training Loss: 0.7926241438191636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:05:07,467] Trial 332 finished with value: 0.6320666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7937284241941639\n",
      "Epoch 1, Training Loss: 0.8520470399716321\n",
      "Epoch 2, Training Loss: 0.8187189337786506\n",
      "Epoch 3, Training Loss: 0.8135026622519773\n",
      "Epoch 4, Training Loss: 0.8098080589490778\n",
      "Epoch 5, Training Loss: 0.8086468593513264\n",
      "Epoch 6, Training Loss: 0.8073562719541437\n",
      "Epoch 7, Training Loss: 0.8056946924854728\n",
      "Epoch 8, Training Loss: 0.8041822338805479\n",
      "Epoch 9, Training Loss: 0.8042941059785731\n",
      "Epoch 10, Training Loss: 0.8030362758916967\n",
      "Epoch 11, Training Loss: 0.8024272952360265\n",
      "Epoch 12, Training Loss: 0.8018848597302156\n",
      "Epoch 13, Training Loss: 0.801756948372897\n",
      "Epoch 14, Training Loss: 0.8011714117667255\n",
      "Epoch 15, Training Loss: 0.8004719880749197\n",
      "Epoch 16, Training Loss: 0.7999664243529825\n",
      "Epoch 17, Training Loss: 0.7996464885683621\n",
      "Epoch 18, Training Loss: 0.7983056908495286\n",
      "Epoch 19, Training Loss: 0.7993116157195147\n",
      "Epoch 20, Training Loss: 0.798202604265774\n",
      "Epoch 21, Training Loss: 0.7985163457954632\n",
      "Epoch 22, Training Loss: 0.7971714268712436\n",
      "Epoch 23, Training Loss: 0.7979911574896644\n",
      "Epoch 24, Training Loss: 0.7969811191278345\n",
      "Epoch 25, Training Loss: 0.7973247708292568\n",
      "Epoch 26, Training Loss: 0.7966829960486468\n",
      "Epoch 27, Training Loss: 0.7968344799911274\n",
      "Epoch 28, Training Loss: 0.7969522624156055\n",
      "Epoch 29, Training Loss: 0.7954901293446036\n",
      "Epoch 30, Training Loss: 0.7965489906423232\n",
      "Epoch 31, Training Loss: 0.7955494670306935\n",
      "Epoch 32, Training Loss: 0.7952354988631081\n",
      "Epoch 33, Training Loss: 0.7959948867910048\n",
      "Epoch 34, Training Loss: 0.7956398820175844\n",
      "Epoch 35, Training Loss: 0.7952987240342533\n",
      "Epoch 36, Training Loss: 0.795400842147715\n",
      "Epoch 37, Training Loss: 0.7953118617394391\n",
      "Epoch 38, Training Loss: 0.7951557436410118\n",
      "Epoch 39, Training Loss: 0.795315453936072\n",
      "Epoch 40, Training Loss: 0.7953139473410213\n",
      "Epoch 41, Training Loss: 0.7949479162693024\n",
      "Epoch 42, Training Loss: 0.7947719782941481\n",
      "Epoch 43, Training Loss: 0.7944142570215112\n",
      "Epoch 44, Training Loss: 0.7940856176264146\n",
      "Epoch 45, Training Loss: 0.794898368400686\n",
      "Epoch 46, Training Loss: 0.7944085755768944\n",
      "Epoch 47, Training Loss: 0.7942177811790915\n",
      "Epoch 48, Training Loss: 0.7939495137859793\n",
      "Epoch 49, Training Loss: 0.7942802971250871\n",
      "Epoch 50, Training Loss: 0.7934737321909736\n",
      "Epoch 51, Training Loss: 0.7938525110833785\n",
      "Epoch 52, Training Loss: 0.794214301740422\n",
      "Epoch 53, Training Loss: 0.7931898549725027\n",
      "Epoch 54, Training Loss: 0.7936105930103975\n",
      "Epoch 55, Training Loss: 0.793192550645155\n",
      "Epoch 56, Training Loss: 0.7933151358716628\n",
      "Epoch 57, Training Loss: 0.7935919244850383\n",
      "Epoch 58, Training Loss: 0.79318323969841\n",
      "Epoch 59, Training Loss: 0.7932816975958207\n",
      "Epoch 60, Training Loss: 0.7931754201300004\n",
      "Epoch 61, Training Loss: 0.7930468337676104\n",
      "Epoch 62, Training Loss: 0.7934991225775551\n",
      "Epoch 63, Training Loss: 0.7930942130790037\n",
      "Epoch 64, Training Loss: 0.7933209908709806\n",
      "Epoch 65, Training Loss: 0.7929672180203831\n",
      "Epoch 66, Training Loss: 0.7932114684581757\n",
      "Epoch 67, Training Loss: 0.7927642863638261\n",
      "Epoch 68, Training Loss: 0.7929656508389641\n",
      "Epoch 69, Training Loss: 0.7929511068849002\n",
      "Epoch 70, Training Loss: 0.793220917337081\n",
      "Epoch 71, Training Loss: 0.7925573854586657\n",
      "Epoch 72, Training Loss: 0.7926909538577585\n",
      "Epoch 73, Training Loss: 0.7928063811274135\n",
      "Epoch 74, Training Loss: 0.7924661775897531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:06:32,339] Trial 333 finished with value: 0.6322666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7924235099904677\n",
      "Epoch 1, Training Loss: 0.8461187379700797\n",
      "Epoch 2, Training Loss: 0.816419944458438\n",
      "Epoch 3, Training Loss: 0.8127240945522051\n",
      "Epoch 4, Training Loss: 0.8127122075037849\n",
      "Epoch 5, Training Loss: 0.809071473519605\n",
      "Epoch 6, Training Loss: 0.809079138766554\n",
      "Epoch 7, Training Loss: 0.8091934761606661\n",
      "Epoch 8, Training Loss: 0.8062930993567732\n",
      "Epoch 9, Training Loss: 0.8088899366837695\n",
      "Epoch 10, Training Loss: 0.8062033627266274\n",
      "Epoch 11, Training Loss: 0.8055941520776964\n",
      "Epoch 12, Training Loss: 0.8051861021751748\n",
      "Epoch 13, Training Loss: 0.804499962365717\n",
      "Epoch 14, Training Loss: 0.8028064598714498\n",
      "Epoch 15, Training Loss: 0.8029066891598522\n",
      "Epoch 16, Training Loss: 0.8016935430971304\n",
      "Epoch 17, Training Loss: 0.8028658622189572\n",
      "Epoch 18, Training Loss: 0.8042020340611163\n",
      "Epoch 19, Training Loss: 0.8019077855841558\n",
      "Epoch 20, Training Loss: 0.8022477506694937\n",
      "Epoch 21, Training Loss: 0.8017519398739463\n",
      "Epoch 22, Training Loss: 0.8011636806609935\n",
      "Epoch 23, Training Loss: 0.8011748023499223\n",
      "Epoch 24, Training Loss: 0.802400695470939\n",
      "Epoch 25, Training Loss: 0.8010799269030865\n",
      "Epoch 26, Training Loss: 0.802067068017515\n",
      "Epoch 27, Training Loss: 0.8007520621880553\n",
      "Epoch 28, Training Loss: 0.8008602431842259\n",
      "Epoch 29, Training Loss: 0.800067739289506\n",
      "Epoch 30, Training Loss: 0.7992597421309106\n",
      "Epoch 31, Training Loss: 0.8024729182845668\n",
      "Epoch 32, Training Loss: 0.8015403728736075\n",
      "Epoch 33, Training Loss: 0.7992309984407927\n",
      "Epoch 34, Training Loss: 0.7986077333751478\n",
      "Epoch 35, Training Loss: 0.799427808944444\n",
      "Epoch 36, Training Loss: 0.7993303602799438\n",
      "Epoch 37, Training Loss: 0.8001441164124281\n",
      "Epoch 38, Training Loss: 0.799763404157825\n",
      "Epoch 39, Training Loss: 0.7990696104845606\n",
      "Epoch 40, Training Loss: 0.7977864821154372\n",
      "Epoch 41, Training Loss: 0.7987103594873185\n",
      "Epoch 42, Training Loss: 0.798169487938845\n",
      "Epoch 43, Training Loss: 0.7980468613760812\n",
      "Epoch 44, Training Loss: 0.799471950710268\n",
      "Epoch 45, Training Loss: 0.8001816318447429\n",
      "Epoch 46, Training Loss: 0.7985384510872059\n",
      "Epoch 47, Training Loss: 0.799109216919519\n",
      "Epoch 48, Training Loss: 0.7989362650347832\n",
      "Epoch 49, Training Loss: 0.7989590591954109\n",
      "Epoch 50, Training Loss: 0.7984239603343762\n",
      "Epoch 51, Training Loss: 0.7982722449123412\n",
      "Epoch 52, Training Loss: 0.7974105288211565\n",
      "Epoch 53, Training Loss: 0.7984088452238786\n",
      "Epoch 54, Training Loss: 0.7987086423357627\n",
      "Epoch 55, Training Loss: 0.7977169975302273\n",
      "Epoch 56, Training Loss: 0.7976526194049004\n",
      "Epoch 57, Training Loss: 0.7976694758673359\n",
      "Epoch 58, Training Loss: 0.7975334258007823\n",
      "Epoch 59, Training Loss: 0.7978336495564397\n",
      "Epoch 60, Training Loss: 0.799274594532816\n",
      "Epoch 61, Training Loss: 0.7982441979243343\n",
      "Epoch 62, Training Loss: 0.7989538948338731\n",
      "Epoch 63, Training Loss: 0.7969682748156383\n",
      "Epoch 64, Training Loss: 0.7985766368701046\n",
      "Epoch 65, Training Loss: 0.7973948690227997\n",
      "Epoch 66, Training Loss: 0.797989827528932\n",
      "Epoch 67, Training Loss: 0.7970819032281862\n",
      "Epoch 68, Training Loss: 0.7988181033528837\n",
      "Epoch 69, Training Loss: 0.7978733974291866\n",
      "Epoch 70, Training Loss: 0.7971510739254772\n",
      "Epoch 71, Training Loss: 0.7966195948141858\n",
      "Epoch 72, Training Loss: 0.7983295687159201\n",
      "Epoch 73, Training Loss: 0.7975987872683016\n",
      "Epoch 74, Training Loss: 0.7982302922951547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:07:45,184] Trial 334 finished with value: 0.6333333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7967372377564136\n",
      "Epoch 1, Training Loss: 1.091620627740272\n",
      "Epoch 2, Training Loss: 1.0910773530042261\n",
      "Epoch 3, Training Loss: 1.09110095411315\n",
      "Epoch 4, Training Loss: 1.0909021101499858\n",
      "Epoch 5, Training Loss: 1.0906467378587652\n",
      "Epoch 6, Training Loss: 1.0906211109089672\n",
      "Epoch 7, Training Loss: 1.0904632025195244\n",
      "Epoch 8, Training Loss: 1.0902935431415874\n",
      "Epoch 9, Training Loss: 1.0900901573941224\n",
      "Epoch 10, Training Loss: 1.0898340318436013\n",
      "Epoch 11, Training Loss: 1.089544381772665\n",
      "Epoch 12, Training Loss: 1.0891804257729896\n",
      "Epoch 13, Training Loss: 1.088985608394881\n",
      "Epoch 14, Training Loss: 1.0885532207058786\n",
      "Epoch 15, Training Loss: 1.088010956022076\n",
      "Epoch 16, Training Loss: 1.0877017797384048\n",
      "Epoch 17, Training Loss: 1.0869472679338956\n",
      "Epoch 18, Training Loss: 1.0859856252383469\n",
      "Epoch 19, Training Loss: 1.0851519374919116\n",
      "Epoch 20, Training Loss: 1.0839582059616433\n",
      "Epoch 21, Training Loss: 1.0821035082178905\n",
      "Epoch 22, Training Loss: 1.0798815965652466\n",
      "Epoch 23, Training Loss: 1.0767874278520282\n",
      "Epoch 24, Training Loss: 1.0725241374252434\n",
      "Epoch 25, Training Loss: 1.0663504439189022\n",
      "Epoch 26, Training Loss: 1.0575380413155806\n",
      "Epoch 27, Training Loss: 1.045365535854397\n",
      "Epoch 28, Training Loss: 1.0297584651108076\n",
      "Epoch 29, Training Loss: 1.0133490094564912\n",
      "Epoch 30, Training Loss: 0.9998958346538974\n",
      "Epoch 31, Training Loss: 0.9904472841356033\n",
      "Epoch 32, Training Loss: 0.9846654206290281\n",
      "Epoch 33, Training Loss: 0.9794653670232099\n",
      "Epoch 34, Training Loss: 0.9747309962609657\n",
      "Epoch 35, Training Loss: 0.9704916266570414\n",
      "Epoch 36, Training Loss: 0.9665191943484142\n",
      "Epoch 37, Training Loss: 0.9625144385753718\n",
      "Epoch 38, Training Loss: 0.9599021701884449\n",
      "Epoch 39, Training Loss: 0.9578360849753358\n",
      "Epoch 40, Training Loss: 0.9567126242738021\n",
      "Epoch 41, Training Loss: 0.9545167940003532\n",
      "Epoch 42, Training Loss: 0.9537798716609639\n",
      "Epoch 43, Training Loss: 0.9524438512952704\n",
      "Epoch 44, Training Loss: 0.9508773419193756\n",
      "Epoch 45, Training Loss: 0.9505481590901999\n",
      "Epoch 46, Training Loss: 0.948595300563296\n",
      "Epoch 47, Training Loss: 0.9470392799915228\n",
      "Epoch 48, Training Loss: 0.9454340774313847\n",
      "Epoch 49, Training Loss: 0.9442064590023872\n",
      "Epoch 50, Training Loss: 0.9426928188567771\n",
      "Epoch 51, Training Loss: 0.9411125588237791\n",
      "Epoch 52, Training Loss: 0.9390939547603292\n",
      "Epoch 53, Training Loss: 0.9368663725996376\n",
      "Epoch 54, Training Loss: 0.9348082906321475\n",
      "Epoch 55, Training Loss: 0.932492630911949\n",
      "Epoch 56, Training Loss: 0.9293547256548602\n",
      "Epoch 57, Training Loss: 0.9273069089516661\n",
      "Epoch 58, Training Loss: 0.9234213751957829\n",
      "Epoch 59, Training Loss: 0.9197905831767205\n",
      "Epoch 60, Training Loss: 0.9157304065568107\n",
      "Epoch 61, Training Loss: 0.910946219845822\n",
      "Epoch 62, Training Loss: 0.90620203663532\n",
      "Epoch 63, Training Loss: 0.8997104498676788\n",
      "Epoch 64, Training Loss: 0.8937527071264454\n",
      "Epoch 65, Training Loss: 0.8878865339702233\n",
      "Epoch 66, Training Loss: 0.8811511776501075\n",
      "Epoch 67, Training Loss: 0.8741804041360554\n",
      "Epoch 68, Training Loss: 0.8673307359666753\n",
      "Epoch 69, Training Loss: 0.8617584026845774\n",
      "Epoch 70, Training Loss: 0.8567699363357142\n",
      "Epoch 71, Training Loss: 0.85188083164674\n",
      "Epoch 72, Training Loss: 0.8486349356801887\n",
      "Epoch 73, Training Loss: 0.8456302127443758\n",
      "Epoch 74, Training Loss: 0.8430522149666808\n",
      "Epoch 75, Training Loss: 0.8415454315063648\n",
      "Epoch 76, Training Loss: 0.8402304600055953\n",
      "Epoch 77, Training Loss: 0.8388995020013107\n",
      "Epoch 78, Training Loss: 0.8383106444115029\n",
      "Epoch 79, Training Loss: 0.8372159897832943\n",
      "Epoch 80, Training Loss: 0.8362927161661305\n",
      "Epoch 81, Training Loss: 0.8358130733769639\n",
      "Epoch 82, Training Loss: 0.8349092652923182\n",
      "Epoch 83, Training Loss: 0.8345804380294972\n",
      "Epoch 84, Training Loss: 0.8341250322815171\n",
      "Epoch 85, Training Loss: 0.8332553129447134\n",
      "Epoch 86, Training Loss: 0.8328465314736043\n",
      "Epoch 87, Training Loss: 0.831714633981088\n",
      "Epoch 88, Training Loss: 0.8321411744096225\n",
      "Epoch 89, Training Loss: 0.83109616686527\n",
      "Epoch 90, Training Loss: 0.8311104250133485\n",
      "Epoch 91, Training Loss: 0.830603263700815\n",
      "Epoch 92, Training Loss: 0.8294639294308828\n",
      "Epoch 93, Training Loss: 0.8289943962168873\n",
      "Epoch 94, Training Loss: 0.8280832772864435\n",
      "Epoch 95, Training Loss: 0.8280568098663388\n",
      "Epoch 96, Training Loss: 0.8277107040684922\n",
      "Epoch 97, Training Loss: 0.8267697475906601\n",
      "Epoch 98, Training Loss: 0.8261598768987154\n",
      "Epoch 99, Training Loss: 0.8257181015229763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:09:29,042] Trial 335 finished with value: 0.6192 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8242261670585862\n",
      "Epoch 1, Training Loss: 0.9852008295760435\n",
      "Epoch 2, Training Loss: 0.9296828922804664\n",
      "Epoch 3, Training Loss: 0.8793307257399839\n",
      "Epoch 4, Training Loss: 0.8318106537005481\n",
      "Epoch 5, Training Loss: 0.8200111094643088\n",
      "Epoch 6, Training Loss: 0.8158478606448454\n",
      "Epoch 7, Training Loss: 0.8127487457499785\n",
      "Epoch 8, Training Loss: 0.8113577779601602\n",
      "Epoch 9, Training Loss: 0.8094488771522746\n",
      "Epoch 10, Training Loss: 0.8083115562971901\n",
      "Epoch 11, Training Loss: 0.8068493660057292\n",
      "Epoch 12, Training Loss: 0.8055284313594594\n",
      "Epoch 13, Training Loss: 0.8053775646406062\n",
      "Epoch 14, Training Loss: 0.8042863908234764\n",
      "Epoch 15, Training Loss: 0.8032104513224434\n",
      "Epoch 16, Training Loss: 0.8026651694494136\n",
      "Epoch 17, Training Loss: 0.8017335580377017\n",
      "Epoch 18, Training Loss: 0.8014663903853473\n",
      "Epoch 19, Training Loss: 0.8013256972677567\n",
      "Epoch 20, Training Loss: 0.8008498944254483\n",
      "Epoch 21, Training Loss: 0.8004971894095926\n",
      "Epoch 22, Training Loss: 0.7999352765083313\n",
      "Epoch 23, Training Loss: 0.799123299893211\n",
      "Epoch 24, Training Loss: 0.7991046188859379\n",
      "Epoch 25, Training Loss: 0.7987445614618414\n",
      "Epoch 26, Training Loss: 0.7984321883145501\n",
      "Epoch 27, Training Loss: 0.7978237520947176\n",
      "Epoch 28, Training Loss: 0.7978556756412282\n",
      "Epoch 29, Training Loss: 0.7976694501147551\n",
      "Epoch 30, Training Loss: 0.7968652404055876\n",
      "Epoch 31, Training Loss: 0.7969090485572815\n",
      "Epoch 32, Training Loss: 0.7961727502766778\n",
      "Epoch 33, Training Loss: 0.7960063251327066\n",
      "Epoch 34, Training Loss: 0.7955750061483944\n",
      "Epoch 35, Training Loss: 0.7951982895065757\n",
      "Epoch 36, Training Loss: 0.7945084735225229\n",
      "Epoch 37, Training Loss: 0.7941760664827684\n",
      "Epoch 38, Training Loss: 0.7936329483985901\n",
      "Epoch 39, Training Loss: 0.7926807012277491\n",
      "Epoch 40, Training Loss: 0.7923077344894409\n",
      "Epoch 41, Training Loss: 0.7916621583349565\n",
      "Epoch 42, Training Loss: 0.7914379492226769\n",
      "Epoch 43, Training Loss: 0.7907793556241428\n",
      "Epoch 44, Training Loss: 0.7904463751877056\n",
      "Epoch 45, Training Loss: 0.7898905675551471\n",
      "Epoch 46, Training Loss: 0.7893732086349936\n",
      "Epoch 47, Training Loss: 0.7891447553213905\n",
      "Epoch 48, Training Loss: 0.788440515574287\n",
      "Epoch 49, Training Loss: 0.7884948013810551\n",
      "Epoch 50, Training Loss: 0.7876840824940625\n",
      "Epoch 51, Training Loss: 0.7875799057062934\n",
      "Epoch 52, Training Loss: 0.787517426294439\n",
      "Epoch 53, Training Loss: 0.7870138573646546\n",
      "Epoch 54, Training Loss: 0.7869307229098151\n",
      "Epoch 55, Training Loss: 0.7867576564059539\n",
      "Epoch 56, Training Loss: 0.7864115045351141\n",
      "Epoch 57, Training Loss: 0.7863555394200717\n",
      "Epoch 58, Training Loss: 0.7861805990864249\n",
      "Epoch 59, Training Loss: 0.7858909311715294\n",
      "Epoch 60, Training Loss: 0.7857313264117521\n",
      "Epoch 61, Training Loss: 0.7855900850015528\n",
      "Epoch 62, Training Loss: 0.7856306945576387\n",
      "Epoch 63, Training Loss: 0.7853574139230391\n",
      "Epoch 64, Training Loss: 0.7851469281841726\n",
      "Epoch 65, Training Loss: 0.7850231930087594\n",
      "Epoch 66, Training Loss: 0.7849896995460286\n",
      "Epoch 67, Training Loss: 0.7848888574628269\n",
      "Epoch 68, Training Loss: 0.7848507155390346\n",
      "Epoch 69, Training Loss: 0.7847529084542219\n",
      "Epoch 70, Training Loss: 0.7844153432285085\n",
      "Epoch 71, Training Loss: 0.784800067508922\n",
      "Epoch 72, Training Loss: 0.7841579090847689\n",
      "Epoch 73, Training Loss: 0.7842864694314845\n",
      "Epoch 74, Training Loss: 0.7845437019011554\n",
      "Epoch 75, Training Loss: 0.7843640833742478\n",
      "Epoch 76, Training Loss: 0.7843049744297477\n",
      "Epoch 77, Training Loss: 0.7840395389584934\n",
      "Epoch 78, Training Loss: 0.7840442425363204\n",
      "Epoch 79, Training Loss: 0.7835428714050966\n",
      "Epoch 80, Training Loss: 0.7838874456461739\n",
      "Epoch 81, Training Loss: 0.7838736562869129\n",
      "Epoch 82, Training Loss: 0.7838979758234585\n",
      "Epoch 83, Training Loss: 0.7836676081489115\n",
      "Epoch 84, Training Loss: 0.7837375599496504\n",
      "Epoch 85, Training Loss: 0.7835351763052099\n",
      "Epoch 86, Training Loss: 0.783773834845599\n",
      "Epoch 87, Training Loss: 0.7830628871917724\n",
      "Epoch 88, Training Loss: 0.7834203513229594\n",
      "Epoch 89, Training Loss: 0.7833778497050791\n",
      "Epoch 90, Training Loss: 0.7831069515733158\n",
      "Epoch 91, Training Loss: 0.7832380376142614\n",
      "Epoch 92, Training Loss: 0.7834354931466719\n",
      "Epoch 93, Training Loss: 0.783218022514792\n",
      "Epoch 94, Training Loss: 0.7831417160174426\n",
      "Epoch 95, Training Loss: 0.7830387421215281\n",
      "Epoch 96, Training Loss: 0.7830223826801076\n",
      "Epoch 97, Training Loss: 0.7828949950723088\n",
      "Epoch 98, Training Loss: 0.7828299988017363\n",
      "Epoch 99, Training Loss: 0.782687847403919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:11:28,945] Trial 336 finished with value: 0.6403333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 165 with value: 0.6417333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7828349572069505\n",
      "Epoch 1, Training Loss: 0.8997839726420009\n",
      "Epoch 2, Training Loss: 0.8177487759730395\n",
      "Epoch 3, Training Loss: 0.8105246215006885\n",
      "Epoch 4, Training Loss: 0.8066781566423529\n",
      "Epoch 5, Training Loss: 0.8048743684151594\n",
      "Epoch 6, Training Loss: 0.8044538452344783\n",
      "Epoch 7, Training Loss: 0.8026005924449248\n",
      "Epoch 8, Training Loss: 0.8014790243962232\n",
      "Epoch 9, Training Loss: 0.8010794841541964\n",
      "Epoch 10, Training Loss: 0.8004418677442214\n",
      "Epoch 11, Training Loss: 0.8004963581001058\n",
      "Epoch 12, Training Loss: 0.7990807361462537\n",
      "Epoch 13, Training Loss: 0.7990590953125674\n",
      "Epoch 14, Training Loss: 0.7988631820678711\n",
      "Epoch 15, Training Loss: 0.7979925428418552\n",
      "Epoch 16, Training Loss: 0.7975917715184829\n",
      "Epoch 17, Training Loss: 0.7973461037523606\n",
      "Epoch 18, Training Loss: 0.7969593822956085\n",
      "Epoch 19, Training Loss: 0.7968653015529408\n",
      "Epoch 20, Training Loss: 0.7962417848671184\n",
      "Epoch 21, Training Loss: 0.7960115548442391\n",
      "Epoch 22, Training Loss: 0.7957406292943393\n",
      "Epoch 23, Training Loss: 0.7946040502015282\n",
      "Epoch 24, Training Loss: 0.7941748228493859\n",
      "Epoch 25, Training Loss: 0.7937358631105984\n",
      "Epoch 26, Training Loss: 0.7932311301371631\n",
      "Epoch 27, Training Loss: 0.7925138488937826\n",
      "Epoch 28, Training Loss: 0.7916875933198367\n",
      "Epoch 29, Training Loss: 0.7912759062121896\n",
      "Epoch 30, Training Loss: 0.7904322397708893\n",
      "Epoch 31, Training Loss: 0.7898666566960952\n",
      "Epoch 32, Training Loss: 0.7888537658663357\n",
      "Epoch 33, Training Loss: 0.7883910458929398\n",
      "Epoch 34, Training Loss: 0.7884636529052959\n",
      "Epoch 35, Training Loss: 0.7875467909784878\n",
      "Epoch 36, Training Loss: 0.7866743217496311\n",
      "Epoch 37, Training Loss: 0.7867360830307006\n",
      "Epoch 38, Training Loss: 0.7865409654729506\n",
      "Epoch 39, Training Loss: 0.7860238929355846\n",
      "Epoch 40, Training Loss: 0.7858806743341333\n",
      "Epoch 41, Training Loss: 0.7855407308830934\n",
      "Epoch 42, Training Loss: 0.7851210382405449\n",
      "Epoch 43, Training Loss: 0.7855112565264982\n",
      "Epoch 44, Training Loss: 0.7847465423275443\n",
      "Epoch 45, Training Loss: 0.7849010804120232\n",
      "Epoch 46, Training Loss: 0.7848222837027381\n",
      "Epoch 47, Training Loss: 0.784964066323112\n",
      "Epoch 48, Training Loss: 0.7841788510014029\n",
      "Epoch 49, Training Loss: 0.7838365141083212\n",
      "Epoch 50, Training Loss: 0.7843526221724118\n",
      "Epoch 51, Training Loss: 0.7841761072944192\n",
      "Epoch 52, Training Loss: 0.7841070267032174\n",
      "Epoch 53, Training Loss: 0.7841025933798622\n",
      "Epoch 54, Training Loss: 0.7837658452987671\n",
      "Epoch 55, Training Loss: 0.7836922172939076\n",
      "Epoch 56, Training Loss: 0.7838816887490889\n",
      "Epoch 57, Training Loss: 0.7837321092100704\n",
      "Epoch 58, Training Loss: 0.7838014001004836\n",
      "Epoch 59, Training Loss: 0.7836114216552061\n",
      "Epoch 60, Training Loss: 0.7831258872677298\n",
      "Epoch 61, Training Loss: 0.7836174249649048\n",
      "Epoch 62, Training Loss: 0.7833082556724549\n",
      "Epoch 63, Training Loss: 0.7833819096228656\n",
      "Epoch 64, Training Loss: 0.783591546311098\n",
      "Epoch 65, Training Loss: 0.7829824728124282\n",
      "Epoch 66, Training Loss: 0.7829264141531551\n",
      "Epoch 67, Training Loss: 0.7831038971508251\n",
      "Epoch 68, Training Loss: 0.7826320669230293\n",
      "Epoch 69, Training Loss: 0.7823833232066211\n",
      "Epoch 70, Training Loss: 0.7821495920770308\n",
      "Epoch 71, Training Loss: 0.7825389669923222\n",
      "Epoch 72, Training Loss: 0.7821530413627624\n",
      "Epoch 73, Training Loss: 0.7824513249537524\n",
      "Epoch 74, Training Loss: 0.7827582594927619\n",
      "Epoch 75, Training Loss: 0.7822639141363256\n",
      "Epoch 76, Training Loss: 0.7823720587001127\n",
      "Epoch 77, Training Loss: 0.7819249024110682\n",
      "Epoch 78, Training Loss: 0.782688490152359\n",
      "Epoch 79, Training Loss: 0.7819576227664947\n",
      "Epoch 80, Training Loss: 0.7820249173220466\n",
      "Epoch 81, Training Loss: 0.7825851739855374\n",
      "Epoch 82, Training Loss: 0.7816204839594224\n",
      "Epoch 83, Training Loss: 0.7821019724537345\n",
      "Epoch 84, Training Loss: 0.7813049576562994\n",
      "Epoch 85, Training Loss: 0.7817620778785033\n",
      "Epoch 86, Training Loss: 0.7821496769259958\n",
      "Epoch 87, Training Loss: 0.7822250120780048\n",
      "Epoch 88, Training Loss: 0.7815235616880305\n",
      "Epoch 89, Training Loss: 0.7812647885434768\n",
      "Epoch 90, Training Loss: 0.7814398198969223\n",
      "Epoch 91, Training Loss: 0.7817781834041371\n",
      "Epoch 92, Training Loss: 0.7817490095250746\n",
      "Epoch 93, Training Loss: 0.7815852312480702\n",
      "Epoch 94, Training Loss: 0.7816230424712686\n",
      "Epoch 95, Training Loss: 0.7813379655866062\n",
      "Epoch 96, Training Loss: 0.7812423744622399\n",
      "Epoch 97, Training Loss: 0.7808133498360129\n",
      "Epoch 98, Training Loss: 0.7812925997902366\n",
      "Epoch 99, Training Loss: 0.7814826290046467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:13:43,390] Trial 337 finished with value: 0.6421333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7812007340964149\n",
      "Epoch 1, Training Loss: 0.8489808404445648\n",
      "Epoch 2, Training Loss: 0.8215092336430269\n",
      "Epoch 3, Training Loss: 0.8128467261089998\n",
      "Epoch 4, Training Loss: 0.8134873625110177\n",
      "Epoch 5, Training Loss: 0.8111445637310253\n",
      "Epoch 6, Training Loss: 0.8083699790870442\n",
      "Epoch 7, Training Loss: 0.8082664149649003\n",
      "Epoch 8, Training Loss: 0.8071903890721939\n",
      "Epoch 9, Training Loss: 0.807529715089237\n",
      "Epoch 10, Training Loss: 0.8051499575025896\n",
      "Epoch 11, Training Loss: 0.8060356227089377\n",
      "Epoch 12, Training Loss: 0.8052132526566\n",
      "Epoch 13, Training Loss: 0.8064890944957733\n",
      "Epoch 14, Training Loss: 0.805697513958987\n",
      "Epoch 15, Training Loss: 0.8039527665166294\n",
      "Epoch 16, Training Loss: 0.8069046904059017\n",
      "Epoch 17, Training Loss: 0.807394936505486\n",
      "Epoch 18, Training Loss: 0.8072690148213331\n",
      "Epoch 19, Training Loss: 0.8069813658209408\n",
      "Epoch 20, Training Loss: 0.804747478611329\n",
      "Epoch 21, Training Loss: 0.806045036315918\n",
      "Epoch 22, Training Loss: 0.8071168428308824\n",
      "Epoch 23, Training Loss: 0.8070336166550132\n",
      "Epoch 24, Training Loss: 0.8064216456693761\n",
      "Epoch 25, Training Loss: 0.8073939010676215\n",
      "Epoch 26, Training Loss: 0.8057916422451243\n",
      "Epoch 27, Training Loss: 0.8076909210401423\n",
      "Epoch 28, Training Loss: 0.8092749494664809\n",
      "Epoch 29, Training Loss: 0.8084521689835716\n",
      "Epoch 30, Training Loss: 0.812434297800064\n",
      "Epoch 31, Training Loss: 0.8139077328233157\n",
      "Epoch 32, Training Loss: 0.8139351119013394\n",
      "Epoch 33, Training Loss: 0.815534129002515\n",
      "Epoch 34, Training Loss: 0.8157873015543994\n",
      "Epoch 35, Training Loss: 0.8152534693128922\n",
      "Epoch 36, Training Loss: 0.8150044812875635\n",
      "Epoch 37, Training Loss: 0.815983884194318\n",
      "Epoch 38, Training Loss: 0.8166293027821709\n",
      "Epoch 39, Training Loss: 0.8162406578484703\n",
      "Epoch 40, Training Loss: 0.8184008129905251\n",
      "Epoch 41, Training Loss: 0.8156309071709128\n",
      "Epoch 42, Training Loss: 0.8167049518753501\n",
      "Epoch 43, Training Loss: 0.8154529962118934\n",
      "Epoch 44, Training Loss: 0.8159424331608941\n",
      "Epoch 45, Training Loss: 0.8160206093507655\n",
      "Epoch 46, Training Loss: 0.8182551207261927\n",
      "Epoch 47, Training Loss: 0.8165618775171392\n",
      "Epoch 48, Training Loss: 0.8174557435512543\n",
      "Epoch 49, Training Loss: 0.8211291970926172\n",
      "Epoch 50, Training Loss: 0.8202530222079333\n",
      "Epoch 51, Training Loss: 0.8195202084849863\n",
      "Epoch 52, Training Loss: 0.8210777965012719\n",
      "Epoch 53, Training Loss: 0.8198996347539566\n",
      "Epoch 54, Training Loss: 0.8158378080059501\n",
      "Epoch 55, Training Loss: 0.8216063515579\n",
      "Epoch 56, Training Loss: 0.8187981318025028\n",
      "Epoch 57, Training Loss: 0.8176053658653708\n",
      "Epoch 58, Training Loss: 0.8201168506285723\n",
      "Epoch 59, Training Loss: 0.8188938818258398\n",
      "Epoch 60, Training Loss: 0.8166030088592978\n",
      "Epoch 61, Training Loss: 0.8179741396623499\n",
      "Epoch 62, Training Loss: 0.8187248038544375\n",
      "Epoch 63, Training Loss: 0.8168422157624189\n",
      "Epoch 64, Training Loss: 0.8167024225347183\n",
      "Epoch 65, Training Loss: 0.8187794048645917\n",
      "Epoch 66, Training Loss: 0.8217292951836306\n",
      "Epoch 67, Training Loss: 0.8192147266864777\n",
      "Epoch 68, Training Loss: 0.8199165287438561\n",
      "Epoch 69, Training Loss: 0.8197022176490111\n",
      "Epoch 70, Training Loss: 0.8200366508960724\n",
      "Epoch 71, Training Loss: 0.8194399558095371\n",
      "Epoch 72, Training Loss: 0.8213497938128078\n",
      "Epoch 73, Training Loss: 0.8205454052897061\n",
      "Epoch 74, Training Loss: 0.8210887678931741\n",
      "Epoch 75, Training Loss: 0.818383930570939\n",
      "Epoch 76, Training Loss: 0.8187965198124156\n",
      "Epoch 77, Training Loss: 0.8192486272839938\n",
      "Epoch 78, Training Loss: 0.8203262564715217\n",
      "Epoch 79, Training Loss: 0.8214577306018156\n",
      "Epoch 80, Training Loss: 0.8223791637841393\n",
      "Epoch 81, Training Loss: 0.8227094630634083\n",
      "Epoch 82, Training Loss: 0.8239472887796514\n",
      "Epoch 83, Training Loss: 0.8215211082907284\n",
      "Epoch 84, Training Loss: 0.8179331766156589\n",
      "Epoch 85, Training Loss: 0.8204116697171155\n",
      "Epoch 86, Training Loss: 0.8198959353390862\n",
      "Epoch 87, Training Loss: 0.8209874749884886\n",
      "Epoch 88, Training Loss: 0.8177011899387135\n",
      "Epoch 89, Training Loss: 0.819299552861382\n",
      "Epoch 90, Training Loss: 0.8236795124586891\n",
      "Epoch 91, Training Loss: 0.8174266475789688\n",
      "Epoch 92, Training Loss: 0.8251659641546362\n",
      "Epoch 93, Training Loss: 0.8222889722796047\n",
      "Epoch 94, Training Loss: 0.8206403652359457\n",
      "Epoch 95, Training Loss: 0.8194772408990298\n",
      "Epoch 96, Training Loss: 0.8222754005824818\n",
      "Epoch 97, Training Loss: 0.8200772314913133\n",
      "Epoch 98, Training Loss: 0.8243898609806509\n",
      "Epoch 99, Training Loss: 0.82208927266738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:16:19,440] Trial 338 finished with value: 0.6248 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8199453412785249\n",
      "Epoch 1, Training Loss: 0.9365048559984767\n",
      "Epoch 2, Training Loss: 0.8337919448551379\n",
      "Epoch 3, Training Loss: 0.8251483417991409\n",
      "Epoch 4, Training Loss: 0.8199598465647017\n",
      "Epoch 5, Training Loss: 0.8164861233610856\n",
      "Epoch 6, Training Loss: 0.8136915755451174\n",
      "Epoch 7, Training Loss: 0.8100593548968322\n",
      "Epoch 8, Training Loss: 0.8087576704814021\n",
      "Epoch 9, Training Loss: 0.8075000232323668\n",
      "Epoch 10, Training Loss: 0.8080881239776324\n",
      "Epoch 11, Training Loss: 0.8080086105748226\n",
      "Epoch 12, Training Loss: 0.805867244067945\n",
      "Epoch 13, Training Loss: 0.8047332459822634\n",
      "Epoch 14, Training Loss: 0.8035192099728978\n",
      "Epoch 15, Training Loss: 0.8047264234463971\n",
      "Epoch 16, Training Loss: 0.8028047168165221\n",
      "Epoch 17, Training Loss: 0.8030250231126197\n",
      "Epoch 18, Training Loss: 0.8030902393778464\n",
      "Epoch 19, Training Loss: 0.802763857608451\n",
      "Epoch 20, Training Loss: 0.8013903516127651\n",
      "Epoch 21, Training Loss: 0.8021218444171705\n",
      "Epoch 22, Training Loss: 0.8013302589717665\n",
      "Epoch 23, Training Loss: 0.8008573277552326\n",
      "Epoch 24, Training Loss: 0.8019139461947563\n",
      "Epoch 25, Training Loss: 0.8038757836012016\n",
      "Epoch 26, Training Loss: 0.8011478690276469\n",
      "Epoch 27, Training Loss: 0.8006706694911297\n",
      "Epoch 28, Training Loss: 0.8014771063524978\n",
      "Epoch 29, Training Loss: 0.8015686123890984\n",
      "Epoch 30, Training Loss: 0.8013775544955318\n",
      "Epoch 31, Training Loss: 0.7997577688747779\n",
      "Epoch 32, Training Loss: 0.8005577316857818\n",
      "Epoch 33, Training Loss: 0.8011880809203127\n",
      "Epoch 34, Training Loss: 0.80116029251787\n",
      "Epoch 35, Training Loss: 0.8004784121549219\n",
      "Epoch 36, Training Loss: 0.8009567283149949\n",
      "Epoch 37, Training Loss: 0.8006079297316702\n",
      "Epoch 38, Training Loss: 0.8001642979177317\n",
      "Epoch 39, Training Loss: 0.8001188682434254\n",
      "Epoch 40, Training Loss: 0.8008208373435458\n",
      "Epoch 41, Training Loss: 0.7999787542156707\n",
      "Epoch 42, Training Loss: 0.8001427372595421\n",
      "Epoch 43, Training Loss: 0.8004400314245009\n",
      "Epoch 44, Training Loss: 0.7994224760765419\n",
      "Epoch 45, Training Loss: 0.7991046464980993\n",
      "Epoch 46, Training Loss: 0.7999701239112624\n",
      "Epoch 47, Training Loss: 0.7999381137073488\n",
      "Epoch 48, Training Loss: 0.7998720308891812\n",
      "Epoch 49, Training Loss: 0.7991215624307332\n",
      "Epoch 50, Training Loss: 0.8000266184484152\n",
      "Epoch 51, Training Loss: 0.7993946180307776\n",
      "Epoch 52, Training Loss: 0.7998588211554334\n",
      "Epoch 53, Training Loss: 0.8027919235982393\n",
      "Epoch 54, Training Loss: 0.7989740733813523\n",
      "Epoch 55, Training Loss: 0.7997943389684634\n",
      "Epoch 56, Training Loss: 0.8001614853851777\n",
      "Epoch 57, Training Loss: 0.7997941402564371\n",
      "Epoch 58, Training Loss: 0.8022469727616561\n",
      "Epoch 59, Training Loss: 0.798652240028955\n",
      "Epoch 60, Training Loss: 0.8004347748326179\n",
      "Epoch 61, Training Loss: 0.7981183172168588\n",
      "Epoch 62, Training Loss: 0.7991218972923164\n",
      "Epoch 63, Training Loss: 0.799470901847782\n",
      "Epoch 64, Training Loss: 0.7979648835676953\n",
      "Epoch 65, Training Loss: 0.7988282607014018\n",
      "Epoch 66, Training Loss: 0.7982745160733847\n",
      "Epoch 67, Training Loss: 0.7992714239242381\n",
      "Epoch 68, Training Loss: 0.7986700278475769\n",
      "Epoch 69, Training Loss: 0.7998307036277943\n",
      "Epoch 70, Training Loss: 0.7986568774495806\n",
      "Epoch 71, Training Loss: 0.7994720017999635\n",
      "Epoch 72, Training Loss: 0.7989421968173264\n",
      "Epoch 73, Training Loss: 0.7994534728222323\n",
      "Epoch 74, Training Loss: 0.7991302062694291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:17:56,579] Trial 339 finished with value: 0.6324666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8001017377788859\n",
      "Epoch 1, Training Loss: 0.893551851035957\n",
      "Epoch 2, Training Loss: 0.81847919933778\n",
      "Epoch 3, Training Loss: 0.8106850733434348\n",
      "Epoch 4, Training Loss: 0.8066607642890815\n",
      "Epoch 5, Training Loss: 0.8039919370995429\n",
      "Epoch 6, Training Loss: 0.8023055457531061\n",
      "Epoch 7, Training Loss: 0.79759230828823\n",
      "Epoch 8, Training Loss: 0.7965373193411003\n",
      "Epoch 9, Training Loss: 0.7941703824172343\n",
      "Epoch 10, Training Loss: 0.7942617116117836\n",
      "Epoch 11, Training Loss: 0.7926758230180668\n",
      "Epoch 12, Training Loss: 0.7916741156040278\n",
      "Epoch 13, Training Loss: 0.7912539571747744\n",
      "Epoch 14, Training Loss: 0.7904356416903044\n",
      "Epoch 15, Training Loss: 0.7903640415435447\n",
      "Epoch 16, Training Loss: 0.7903263716769398\n",
      "Epoch 17, Training Loss: 0.7907406976348476\n",
      "Epoch 18, Training Loss: 0.7892619496897647\n",
      "Epoch 19, Training Loss: 0.7871103218175415\n",
      "Epoch 20, Training Loss: 0.7889083614923004\n",
      "Epoch 21, Training Loss: 0.7875829883984157\n",
      "Epoch 22, Training Loss: 0.7886886268630063\n",
      "Epoch 23, Training Loss: 0.786007569248515\n",
      "Epoch 24, Training Loss: 0.787163830341253\n",
      "Epoch 25, Training Loss: 0.7884299288118692\n",
      "Epoch 26, Training Loss: 0.7876561278687384\n",
      "Epoch 27, Training Loss: 0.7873697473590535\n",
      "Epoch 28, Training Loss: 0.7861311882062065\n",
      "Epoch 29, Training Loss: 0.7864853528209199\n",
      "Epoch 30, Training Loss: 0.7863866071952017\n",
      "Epoch 31, Training Loss: 0.7855333454626844\n",
      "Epoch 32, Training Loss: 0.7860070855993974\n",
      "Epoch 33, Training Loss: 0.7866892226656577\n",
      "Epoch 34, Training Loss: 0.7847567875582473\n",
      "Epoch 35, Training Loss: 0.7844846375006482\n",
      "Epoch 36, Training Loss: 0.7855543959409671\n",
      "Epoch 37, Training Loss: 0.7851987389693583\n",
      "Epoch 38, Training Loss: 0.784359488092867\n",
      "Epoch 39, Training Loss: 0.7850622837704824\n",
      "Epoch 40, Training Loss: 0.7840724301517458\n",
      "Epoch 41, Training Loss: 0.7844477083001818\n",
      "Epoch 42, Training Loss: 0.7840342702722191\n",
      "Epoch 43, Training Loss: 0.7842837321130853\n",
      "Epoch 44, Training Loss: 0.7830898794464599\n",
      "Epoch 45, Training Loss: 0.7836902426597767\n",
      "Epoch 46, Training Loss: 0.7832360049835722\n",
      "Epoch 47, Training Loss: 0.783609490376666\n",
      "Epoch 48, Training Loss: 0.782617033513865\n",
      "Epoch 49, Training Loss: 0.7832725141281472\n",
      "Epoch 50, Training Loss: 0.7822911079664876\n",
      "Epoch 51, Training Loss: 0.7832597501295849\n",
      "Epoch 52, Training Loss: 0.7831226148103413\n",
      "Epoch 53, Training Loss: 0.7827179291194543\n",
      "Epoch 54, Training Loss: 0.781532585351987\n",
      "Epoch 55, Training Loss: 0.7821127213929829\n",
      "Epoch 56, Training Loss: 0.7826225079988178\n",
      "Epoch 57, Training Loss: 0.781682570805227\n",
      "Epoch 58, Training Loss: 0.7819786234009535\n",
      "Epoch 59, Training Loss: 0.7806402592730701\n",
      "Epoch 60, Training Loss: 0.7815524258111652\n",
      "Epoch 61, Training Loss: 0.7806957407105238\n",
      "Epoch 62, Training Loss: 0.7815934833727385\n",
      "Epoch 63, Training Loss: 0.7812544007946675\n",
      "Epoch 64, Training Loss: 0.7815098134198584\n",
      "Epoch 65, Training Loss: 0.7800397631369139\n",
      "Epoch 66, Training Loss: 0.7803334852806607\n",
      "Epoch 67, Training Loss: 0.7814294302373901\n",
      "Epoch 68, Training Loss: 0.7806636258175499\n",
      "Epoch 69, Training Loss: 0.7794069379792178\n",
      "Epoch 70, Training Loss: 0.7798528284058535\n",
      "Epoch 71, Training Loss: 0.779887378574314\n",
      "Epoch 72, Training Loss: 0.7805760391672751\n",
      "Epoch 73, Training Loss: 0.7797497051775008\n",
      "Epoch 74, Training Loss: 0.7796531203994177\n",
      "Epoch 75, Training Loss: 0.7791899582497159\n",
      "Epoch 76, Training Loss: 0.7781681647874359\n",
      "Epoch 77, Training Loss: 0.7779638120106288\n",
      "Epoch 78, Training Loss: 0.7782222373144967\n",
      "Epoch 79, Training Loss: 0.7773988200309582\n",
      "Epoch 80, Training Loss: 0.7784234300591892\n",
      "Epoch 81, Training Loss: 0.7783217808357755\n",
      "Epoch 82, Training Loss: 0.7781896537407896\n",
      "Epoch 83, Training Loss: 0.7775620542970815\n",
      "Epoch 84, Training Loss: 0.7781899584863419\n",
      "Epoch 85, Training Loss: 0.777765751422796\n",
      "Epoch 86, Training Loss: 0.7778967891420637\n",
      "Epoch 87, Training Loss: 0.7775066728878738\n",
      "Epoch 88, Training Loss: 0.7766192398573223\n",
      "Epoch 89, Training Loss: 0.778344029860389\n",
      "Epoch 90, Training Loss: 0.7779649190436628\n",
      "Epoch 91, Training Loss: 0.7779655531833046\n",
      "Epoch 92, Training Loss: 0.7765797688548727\n",
      "Epoch 93, Training Loss: 0.7763442991371442\n",
      "Epoch 94, Training Loss: 0.7774868318909093\n",
      "Epoch 95, Training Loss: 0.77595749914198\n",
      "Epoch 96, Training Loss: 0.7764156697387982\n",
      "Epoch 97, Training Loss: 0.7765045787158765\n",
      "Epoch 98, Training Loss: 0.7761349848338536\n",
      "Epoch 99, Training Loss: 0.7763567868031953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:20:04,682] Trial 340 finished with value: 0.6366666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7752138275849192\n",
      "Epoch 1, Training Loss: 0.9163568326405116\n",
      "Epoch 2, Training Loss: 0.8675576796209006\n",
      "Epoch 3, Training Loss: 0.8336287853413058\n",
      "Epoch 4, Training Loss: 0.8149067406367538\n",
      "Epoch 5, Training Loss: 0.8076408157671304\n",
      "Epoch 6, Training Loss: 0.8043444720426001\n",
      "Epoch 7, Training Loss: 0.8029740532538048\n",
      "Epoch 8, Training Loss: 0.8019264557307824\n",
      "Epoch 9, Training Loss: 0.8013404965400696\n",
      "Epoch 10, Training Loss: 0.8010099416388605\n",
      "Epoch 11, Training Loss: 0.8003599066483347\n",
      "Epoch 12, Training Loss: 0.8002178220820606\n",
      "Epoch 13, Training Loss: 0.8007484470094953\n",
      "Epoch 14, Training Loss: 0.7989386191493586\n",
      "Epoch 15, Training Loss: 0.799852014394631\n",
      "Epoch 16, Training Loss: 0.8000455938783804\n",
      "Epoch 17, Training Loss: 0.7986543073690027\n",
      "Epoch 18, Training Loss: 0.7989738752071123\n",
      "Epoch 19, Training Loss: 0.7986229889374926\n",
      "Epoch 20, Training Loss: 0.798731085321957\n",
      "Epoch 21, Training Loss: 0.799121770285126\n",
      "Epoch 22, Training Loss: 0.7977962186462001\n",
      "Epoch 23, Training Loss: 0.7986700520479589\n",
      "Epoch 24, Training Loss: 0.797988522411289\n",
      "Epoch 25, Training Loss: 0.7978452337415595\n",
      "Epoch 26, Training Loss: 0.7972875613915292\n",
      "Epoch 27, Training Loss: 0.7986702219884199\n",
      "Epoch 28, Training Loss: 0.7979726757322039\n",
      "Epoch 29, Training Loss: 0.7973824455325765\n",
      "Epoch 30, Training Loss: 0.7969754143765099\n",
      "Epoch 31, Training Loss: 0.7973350532969138\n",
      "Epoch 32, Training Loss: 0.7969261649855994\n",
      "Epoch 33, Training Loss: 0.7969657157596789\n",
      "Epoch 34, Training Loss: 0.7969548367019883\n",
      "Epoch 35, Training Loss: 0.7969746265196263\n",
      "Epoch 36, Training Loss: 0.7965009074462087\n",
      "Epoch 37, Training Loss: 0.796377648536424\n",
      "Epoch 38, Training Loss: 0.7958452926542526\n",
      "Epoch 39, Training Loss: 0.7963825545812908\n",
      "Epoch 40, Training Loss: 0.7956977682902401\n",
      "Epoch 41, Training Loss: 0.7947329777523987\n",
      "Epoch 42, Training Loss: 0.7941781327688604\n",
      "Epoch 43, Training Loss: 0.7943136620342284\n",
      "Epoch 44, Training Loss: 0.7935514955592334\n",
      "Epoch 45, Training Loss: 0.793379677327952\n",
      "Epoch 46, Training Loss: 0.7928531117009041\n",
      "Epoch 47, Training Loss: 0.7927597528113458\n",
      "Epoch 48, Training Loss: 0.7922536945432649\n",
      "Epoch 49, Training Loss: 0.7926139770593859\n",
      "Epoch 50, Training Loss: 0.7915689947013568\n",
      "Epoch 51, Training Loss: 0.7914690370846512\n",
      "Epoch 52, Training Loss: 0.7916951316639893\n",
      "Epoch 53, Training Loss: 0.791934038911547\n",
      "Epoch 54, Training Loss: 0.7915160948172548\n",
      "Epoch 55, Training Loss: 0.791277421506724\n",
      "Epoch 56, Training Loss: 0.7911720167425342\n",
      "Epoch 57, Training Loss: 0.7911851078047788\n",
      "Epoch 58, Training Loss: 0.7910961391334247\n",
      "Epoch 59, Training Loss: 0.7908816856549199\n",
      "Epoch 60, Training Loss: 0.7906100516928766\n",
      "Epoch 61, Training Loss: 0.7906372000400285\n",
      "Epoch 62, Training Loss: 0.7905237155749385\n",
      "Epoch 63, Training Loss: 0.7899131549480266\n",
      "Epoch 64, Training Loss: 0.7902212277390903\n",
      "Epoch 65, Training Loss: 0.7905974285046857\n",
      "Epoch 66, Training Loss: 0.790064753625626\n",
      "Epoch 67, Training Loss: 0.7907622504055052\n",
      "Epoch 68, Training Loss: 0.7910808683338022\n",
      "Epoch 69, Training Loss: 0.79021860083243\n",
      "Epoch 70, Training Loss: 0.7898878062578072\n",
      "Epoch 71, Training Loss: 0.7906207334726376\n",
      "Epoch 72, Training Loss: 0.7901735122042491\n",
      "Epoch 73, Training Loss: 0.7889598700785099\n",
      "Epoch 74, Training Loss: 0.789647070268043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:21:15,536] Trial 341 finished with value: 0.6332 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7895477588911702\n",
      "Epoch 1, Training Loss: 0.9692357967670698\n",
      "Epoch 2, Training Loss: 0.9316535177087425\n",
      "Epoch 3, Training Loss: 0.9194955148194965\n",
      "Epoch 4, Training Loss: 0.9097921514869632\n",
      "Epoch 5, Training Loss: 0.9002109549995652\n",
      "Epoch 6, Training Loss: 0.8900639034751663\n",
      "Epoch 7, Training Loss: 0.8788773937332899\n",
      "Epoch 8, Training Loss: 0.8669842838344718\n",
      "Epoch 9, Training Loss: 0.8549713870636503\n",
      "Epoch 10, Training Loss: 0.843632441714294\n",
      "Epoch 11, Training Loss: 0.8342214636336592\n",
      "Epoch 12, Training Loss: 0.8260841146447605\n",
      "Epoch 13, Training Loss: 0.8206885409534426\n",
      "Epoch 14, Training Loss: 0.815889937447426\n",
      "Epoch 15, Training Loss: 0.8121712836107813\n",
      "Epoch 16, Training Loss: 0.8098784077436404\n",
      "Epoch 17, Training Loss: 0.807839696479023\n",
      "Epoch 18, Training Loss: 0.8065312207193303\n",
      "Epoch 19, Training Loss: 0.8050528546024982\n",
      "Epoch 20, Training Loss: 0.8047731468552037\n",
      "Epoch 21, Training Loss: 0.8038769980122272\n",
      "Epoch 22, Training Loss: 0.8031385130452035\n",
      "Epoch 23, Training Loss: 0.8031905139299264\n",
      "Epoch 24, Training Loss: 0.8023273169546199\n",
      "Epoch 25, Training Loss: 0.8018362304321806\n",
      "Epoch 26, Training Loss: 0.8021436290633409\n",
      "Epoch 27, Training Loss: 0.801304228503005\n",
      "Epoch 28, Training Loss: 0.8011788141458555\n",
      "Epoch 29, Training Loss: 0.8005291128517094\n",
      "Epoch 30, Training Loss: 0.8005886088636585\n",
      "Epoch 31, Training Loss: 0.7998191990350422\n",
      "Epoch 32, Training Loss: 0.8005419544707564\n",
      "Epoch 33, Training Loss: 0.7998579338080901\n",
      "Epoch 34, Training Loss: 0.7996257612579747\n",
      "Epoch 35, Training Loss: 0.7998250211091866\n",
      "Epoch 36, Training Loss: 0.7992839090806201\n",
      "Epoch 37, Training Loss: 0.7994255408308559\n",
      "Epoch 38, Training Loss: 0.7987262643369517\n",
      "Epoch 39, Training Loss: 0.7989204806492741\n",
      "Epoch 40, Training Loss: 0.7985380491816012\n",
      "Epoch 41, Training Loss: 0.7988541087709872\n",
      "Epoch 42, Training Loss: 0.7983744987867829\n",
      "Epoch 43, Training Loss: 0.7985675398568461\n",
      "Epoch 44, Training Loss: 0.799117132147452\n",
      "Epoch 45, Training Loss: 0.7978969893957439\n",
      "Epoch 46, Training Loss: 0.7981620499065945\n",
      "Epoch 47, Training Loss: 0.7979155977865807\n",
      "Epoch 48, Training Loss: 0.7973800255839986\n",
      "Epoch 49, Training Loss: 0.7979444572800084\n",
      "Epoch 50, Training Loss: 0.7978131043283563\n",
      "Epoch 51, Training Loss: 0.7967774351736656\n",
      "Epoch 52, Training Loss: 0.7972127962829475\n",
      "Epoch 53, Training Loss: 0.7970388066499753\n",
      "Epoch 54, Training Loss: 0.7972048549723805\n",
      "Epoch 55, Training Loss: 0.7974214523358453\n",
      "Epoch 56, Training Loss: 0.7968768765155534\n",
      "Epoch 57, Training Loss: 0.7971866984116404\n",
      "Epoch 58, Training Loss: 0.7974000610803303\n",
      "Epoch 59, Training Loss: 0.7965395853035432\n",
      "Epoch 60, Training Loss: 0.7959417388851481\n",
      "Epoch 61, Training Loss: 0.79643907797964\n",
      "Epoch 62, Training Loss: 0.7964036305147902\n",
      "Epoch 63, Training Loss: 0.7960768524417303\n",
      "Epoch 64, Training Loss: 0.7972796647172226\n",
      "Epoch 65, Training Loss: 0.7962621426223813\n",
      "Epoch 66, Training Loss: 0.7961973512083068\n",
      "Epoch 67, Training Loss: 0.7965143771099865\n",
      "Epoch 68, Training Loss: 0.7959533052336901\n",
      "Epoch 69, Training Loss: 0.7958660919863478\n",
      "Epoch 70, Training Loss: 0.7973412863293985\n",
      "Epoch 71, Training Loss: 0.7964196409497942\n",
      "Epoch 72, Training Loss: 0.7957323069859268\n",
      "Epoch 73, Training Loss: 0.796366000085845\n",
      "Epoch 74, Training Loss: 0.7967285812349247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:22:18,326] Trial 342 finished with value: 0.6328 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7957235332718469\n",
      "Epoch 1, Training Loss: 0.9044132105389932\n",
      "Epoch 2, Training Loss: 0.824583869858792\n",
      "Epoch 3, Training Loss: 0.8169729165564802\n",
      "Epoch 4, Training Loss: 0.8116247679057874\n",
      "Epoch 5, Training Loss: 0.806855467208346\n",
      "Epoch 6, Training Loss: 0.8018308652971023\n",
      "Epoch 7, Training Loss: 0.7995971877772109\n",
      "Epoch 8, Training Loss: 0.7979362619550605\n",
      "Epoch 9, Training Loss: 0.797168849464646\n",
      "Epoch 10, Training Loss: 0.794468597361916\n",
      "Epoch 11, Training Loss: 0.7938716208128105\n",
      "Epoch 12, Training Loss: 0.7938016956910154\n",
      "Epoch 13, Training Loss: 0.7917417923310646\n",
      "Epoch 14, Training Loss: 0.7918277183869727\n",
      "Epoch 15, Training Loss: 0.7909613733004807\n",
      "Epoch 16, Training Loss: 0.7914782812720851\n",
      "Epoch 17, Training Loss: 0.7904246665481338\n",
      "Epoch 18, Training Loss: 0.7897188156170952\n",
      "Epoch 19, Training Loss: 0.7893556164619618\n",
      "Epoch 20, Training Loss: 0.7877760688165076\n",
      "Epoch 21, Training Loss: 0.7892954103032449\n",
      "Epoch 22, Training Loss: 0.7881884066682113\n",
      "Epoch 23, Training Loss: 0.7875520016465868\n",
      "Epoch 24, Training Loss: 0.7875886274459667\n",
      "Epoch 25, Training Loss: 0.7874104403911677\n",
      "Epoch 26, Training Loss: 0.7880172660476283\n",
      "Epoch 27, Training Loss: 0.7867896895659597\n",
      "Epoch 28, Training Loss: 0.7870863623188851\n",
      "Epoch 29, Training Loss: 0.7868031257077267\n",
      "Epoch 30, Training Loss: 0.7866595261975339\n",
      "Epoch 31, Training Loss: 0.7866562342285214\n",
      "Epoch 32, Training Loss: 0.7864305540135033\n",
      "Epoch 33, Training Loss: 0.7860110656659406\n",
      "Epoch 34, Training Loss: 0.7853617114680154\n",
      "Epoch 35, Training Loss: 0.7851941447508962\n",
      "Epoch 36, Training Loss: 0.7848551121869481\n",
      "Epoch 37, Training Loss: 0.7846094700626861\n",
      "Epoch 38, Training Loss: 0.7845325157158357\n",
      "Epoch 39, Training Loss: 0.7843591679307751\n",
      "Epoch 40, Training Loss: 0.7850554311185851\n",
      "Epoch 41, Training Loss: 0.7853440247980276\n",
      "Epoch 42, Training Loss: 0.784268225404553\n",
      "Epoch 43, Training Loss: 0.7844760945865086\n",
      "Epoch 44, Training Loss: 0.7839856289383164\n",
      "Epoch 45, Training Loss: 0.7838684610854414\n",
      "Epoch 46, Training Loss: 0.7843285873420256\n",
      "Epoch 47, Training Loss: 0.7842302413811361\n",
      "Epoch 48, Training Loss: 0.7835896992145625\n",
      "Epoch 49, Training Loss: 0.7830388695673836\n",
      "Epoch 50, Training Loss: 0.7828820902602117\n",
      "Epoch 51, Training Loss: 0.7834099743599282\n",
      "Epoch 52, Training Loss: 0.7832860289659715\n",
      "Epoch 53, Training Loss: 0.7829702808444662\n",
      "Epoch 54, Training Loss: 0.7824503900413227\n",
      "Epoch 55, Training Loss: 0.7827003556086605\n",
      "Epoch 56, Training Loss: 0.7819143558803358\n",
      "Epoch 57, Training Loss: 0.7825333002814673\n",
      "Epoch 58, Training Loss: 0.7820771160878633\n",
      "Epoch 59, Training Loss: 0.781559699997866\n",
      "Epoch 60, Training Loss: 0.7814716847319352\n",
      "Epoch 61, Training Loss: 0.7815581096742387\n",
      "Epoch 62, Training Loss: 0.7813203989114976\n",
      "Epoch 63, Training Loss: 0.781824471090073\n",
      "Epoch 64, Training Loss: 0.7810721923534135\n",
      "Epoch 65, Training Loss: 0.7809566798963045\n",
      "Epoch 66, Training Loss: 0.7809729742824583\n",
      "Epoch 67, Training Loss: 0.7808677420580298\n",
      "Epoch 68, Training Loss: 0.7810524899260443\n",
      "Epoch 69, Training Loss: 0.7801337444692626\n",
      "Epoch 70, Training Loss: 0.7815464288668525\n",
      "Epoch 71, Training Loss: 0.781115620476859\n",
      "Epoch 72, Training Loss: 0.7803275274154835\n",
      "Epoch 73, Training Loss: 0.7801964792093836\n",
      "Epoch 74, Training Loss: 0.7803731710390938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:23:48,127] Trial 343 finished with value: 0.6279333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.779894893599632\n",
      "Epoch 1, Training Loss: 0.8804093275751387\n",
      "Epoch 2, Training Loss: 0.8346594402664587\n",
      "Epoch 3, Training Loss: 0.830405010495867\n",
      "Epoch 4, Training Loss: 0.8264855803403639\n",
      "Epoch 5, Training Loss: 0.8216666009193077\n",
      "Epoch 6, Training Loss: 0.8199588881399399\n",
      "Epoch 7, Training Loss: 0.8203887578240014\n",
      "Epoch 8, Training Loss: 0.8164511798916007\n",
      "Epoch 9, Training Loss: 0.8158014536800241\n",
      "Epoch 10, Training Loss: 0.817165554197211\n",
      "Epoch 11, Training Loss: 0.8151759513338706\n",
      "Epoch 12, Training Loss: 0.8168951464774913\n",
      "Epoch 13, Training Loss: 0.8164407404741847\n",
      "Epoch 14, Training Loss: 0.8140257684808029\n",
      "Epoch 15, Training Loss: 0.8148076162302405\n",
      "Epoch 16, Training Loss: 0.8155280236014746\n",
      "Epoch 17, Training Loss: 0.8136772935551808\n",
      "Epoch 18, Training Loss: 0.8132634254326497\n",
      "Epoch 19, Training Loss: 0.8123210893537766\n",
      "Epoch 20, Training Loss: 0.8144228074783669\n",
      "Epoch 21, Training Loss: 0.8131588268100767\n",
      "Epoch 22, Training Loss: 0.8130823937573828\n",
      "Epoch 23, Training Loss: 0.8124032540428907\n",
      "Epoch 24, Training Loss: 0.812040358647368\n",
      "Epoch 25, Training Loss: 0.8106341647026234\n",
      "Epoch 26, Training Loss: 0.8112945214250034\n",
      "Epoch 27, Training Loss: 0.8104285644409351\n",
      "Epoch 28, Training Loss: 0.8106298388395095\n",
      "Epoch 29, Training Loss: 0.8117113388570628\n",
      "Epoch 30, Training Loss: 0.8092772746444645\n",
      "Epoch 31, Training Loss: 0.8117283093301874\n",
      "Epoch 32, Training Loss: 0.8090974126543318\n",
      "Epoch 33, Training Loss: 0.8102301840495346\n",
      "Epoch 34, Training Loss: 0.8096359346145974\n",
      "Epoch 35, Training Loss: 0.8093961148333729\n",
      "Epoch 36, Training Loss: 0.8086233741358707\n",
      "Epoch 37, Training Loss: 0.8085460540047266\n",
      "Epoch 38, Training Loss: 0.8085727420964636\n",
      "Epoch 39, Training Loss: 0.8095854594295187\n",
      "Epoch 40, Training Loss: 0.8082206683947628\n",
      "Epoch 41, Training Loss: 0.8076785906813199\n",
      "Epoch 42, Training Loss: 0.8073830682532231\n",
      "Epoch 43, Training Loss: 0.8095243302503027\n",
      "Epoch 44, Training Loss: 0.8084981866348955\n",
      "Epoch 45, Training Loss: 0.8087178648862624\n",
      "Epoch 46, Training Loss: 0.8076422858955269\n",
      "Epoch 47, Training Loss: 0.8067934306940638\n",
      "Epoch 48, Training Loss: 0.8081686021690082\n",
      "Epoch 49, Training Loss: 0.8063278352407585\n",
      "Epoch 50, Training Loss: 0.8073228335918341\n",
      "Epoch 51, Training Loss: 0.8076174155213779\n",
      "Epoch 52, Training Loss: 0.8069251550767654\n",
      "Epoch 53, Training Loss: 0.8070400629724775\n",
      "Epoch 54, Training Loss: 0.8052108278848175\n",
      "Epoch 55, Training Loss: 0.8077277846802446\n",
      "Epoch 56, Training Loss: 0.8047400741648854\n",
      "Epoch 57, Training Loss: 0.805955966164295\n",
      "Epoch 58, Training Loss: 0.8056264800236638\n",
      "Epoch 59, Training Loss: 0.8055238412735157\n",
      "Epoch 60, Training Loss: 0.8056463413668754\n",
      "Epoch 61, Training Loss: 0.8040487293910263\n",
      "Epoch 62, Training Loss: 0.8053490120665472\n",
      "Epoch 63, Training Loss: 0.8043338592787435\n",
      "Epoch 64, Training Loss: 0.8056416210375335\n",
      "Epoch 65, Training Loss: 0.8050210431106108\n",
      "Epoch 66, Training Loss: 0.8061184075541963\n",
      "Epoch 67, Training Loss: 0.8043588958288493\n",
      "Epoch 68, Training Loss: 0.8042413834342383\n",
      "Epoch 69, Training Loss: 0.8034436037217764\n",
      "Epoch 70, Training Loss: 0.8051744167966054\n",
      "Epoch 71, Training Loss: 0.8044920846035606\n",
      "Epoch 72, Training Loss: 0.8032072249211764\n",
      "Epoch 73, Training Loss: 0.8046819623699761\n",
      "Epoch 74, Training Loss: 0.8046161154159029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:24:57,435] Trial 344 finished with value: 0.5739333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8040016874334865\n",
      "Epoch 1, Training Loss: 1.0979641252890566\n",
      "Epoch 2, Training Loss: 1.09257450211317\n",
      "Epoch 3, Training Loss: 1.088976060178943\n",
      "Epoch 4, Training Loss: 1.086087833669849\n",
      "Epoch 5, Training Loss: 1.083503032089176\n",
      "Epoch 6, Training Loss: 1.080972911899251\n",
      "Epoch 7, Training Loss: 1.0781932821847442\n",
      "Epoch 8, Training Loss: 1.0752920643727581\n",
      "Epoch 9, Training Loss: 1.072199243172667\n",
      "Epoch 10, Training Loss: 1.0686353898586187\n",
      "Epoch 11, Training Loss: 1.0646813164976305\n",
      "Epoch 12, Training Loss: 1.060333079682257\n",
      "Epoch 13, Training Loss: 1.0554895816889025\n",
      "Epoch 14, Training Loss: 1.0502077724700583\n",
      "Epoch 15, Training Loss: 1.0440973345498394\n",
      "Epoch 16, Training Loss: 1.0376807795431382\n",
      "Epoch 17, Training Loss: 1.030764423187514\n",
      "Epoch 18, Training Loss: 1.0235992553538846\n",
      "Epoch 19, Training Loss: 1.0159434313164617\n",
      "Epoch 20, Training Loss: 1.008223877903214\n",
      "Epoch 21, Training Loss: 1.000845483998607\n",
      "Epoch 22, Training Loss: 0.9935748419367281\n",
      "Epoch 23, Training Loss: 0.9867362098586291\n",
      "Epoch 24, Training Loss: 0.9806821460114385\n",
      "Epoch 25, Training Loss: 0.9750446364395601\n",
      "Epoch 26, Training Loss: 0.9701077115266843\n",
      "Epoch 27, Training Loss: 0.9652340118150066\n",
      "Epoch 28, Training Loss: 0.961201242396706\n",
      "Epoch 29, Training Loss: 0.9579208148153204\n",
      "Epoch 30, Training Loss: 0.9546316324320054\n",
      "Epoch 31, Training Loss: 0.9520865831160008\n",
      "Epoch 32, Training Loss: 0.9491184383406674\n",
      "Epoch 33, Training Loss: 0.9470173762256938\n",
      "Epoch 34, Training Loss: 0.9439912421362741\n",
      "Epoch 35, Training Loss: 0.9421565250346535\n",
      "Epoch 36, Training Loss: 0.9402202388397733\n",
      "Epoch 37, Training Loss: 0.9387149352776377\n",
      "Epoch 38, Training Loss: 0.9370656820168173\n",
      "Epoch 39, Training Loss: 0.9356820784117046\n",
      "Epoch 40, Training Loss: 0.9343562960624695\n",
      "Epoch 41, Training Loss: 0.9333458913000007\n",
      "Epoch 42, Training Loss: 0.9318337588381946\n",
      "Epoch 43, Training Loss: 0.9305039331428987\n",
      "Epoch 44, Training Loss: 0.9290643395337843\n",
      "Epoch 45, Training Loss: 0.9278928271809915\n",
      "Epoch 46, Training Loss: 0.9271095092135264\n",
      "Epoch 47, Training Loss: 0.9252527737976017\n",
      "Epoch 48, Training Loss: 0.9241285494395665\n",
      "Epoch 49, Training Loss: 0.922199262981128\n",
      "Epoch 50, Training Loss: 0.9217103667725298\n",
      "Epoch 51, Training Loss: 0.9201767970744829\n",
      "Epoch 52, Training Loss: 0.9188553705251307\n",
      "Epoch 53, Training Loss: 0.917896818935423\n",
      "Epoch 54, Training Loss: 0.9164999504734699\n",
      "Epoch 55, Training Loss: 0.9154249825871976\n",
      "Epoch 56, Training Loss: 0.9140483618678903\n",
      "Epoch 57, Training Loss: 0.9125301669414778\n",
      "Epoch 58, Training Loss: 0.9114311948754734\n",
      "Epoch 59, Training Loss: 0.9107538093301587\n",
      "Epoch 60, Training Loss: 0.9096799218565002\n",
      "Epoch 61, Training Loss: 0.9075849614645305\n",
      "Epoch 62, Training Loss: 0.9062988069720734\n",
      "Epoch 63, Training Loss: 0.9048584496168266\n",
      "Epoch 64, Training Loss: 0.9034408621321943\n",
      "Epoch 65, Training Loss: 0.9020179669659837\n",
      "Epoch 66, Training Loss: 0.9008741428977565\n",
      "Epoch 67, Training Loss: 0.8991030765655346\n",
      "Epoch 68, Training Loss: 0.8978221473837258\n",
      "Epoch 69, Training Loss: 0.896319940394925\n",
      "Epoch 70, Training Loss: 0.8943110763578487\n",
      "Epoch 71, Training Loss: 0.8932619411246221\n",
      "Epoch 72, Training Loss: 0.8913795660312911\n",
      "Epoch 73, Training Loss: 0.8898732888967471\n",
      "Epoch 74, Training Loss: 0.8878103983133359\n",
      "Epoch 75, Training Loss: 0.8860647557373333\n",
      "Epoch 76, Training Loss: 0.8845781282374733\n",
      "Epoch 77, Training Loss: 0.882113484332436\n",
      "Epoch 78, Training Loss: 0.8800811625064764\n",
      "Epoch 79, Training Loss: 0.8782779923955301\n",
      "Epoch 80, Training Loss: 0.8762599816895965\n",
      "Epoch 81, Training Loss: 0.8739792218781952\n",
      "Epoch 82, Training Loss: 0.8718245490153033\n",
      "Epoch 83, Training Loss: 0.8692078801026022\n",
      "Epoch 84, Training Loss: 0.8668091174355127\n",
      "Epoch 85, Training Loss: 0.8645110209185378\n",
      "Epoch 86, Training Loss: 0.8622123314922018\n",
      "Epoch 87, Training Loss: 0.8596008564296521\n",
      "Epoch 88, Training Loss: 0.8568463814885993\n",
      "Epoch 89, Training Loss: 0.8542655554929174\n",
      "Epoch 90, Training Loss: 0.8517323706383095\n",
      "Epoch 91, Training Loss: 0.8493079723272109\n",
      "Epoch 92, Training Loss: 0.8464721682376432\n",
      "Epoch 93, Training Loss: 0.8442569565055962\n",
      "Epoch 94, Training Loss: 0.8415116070804739\n",
      "Epoch 95, Training Loss: 0.8389037932668414\n",
      "Epoch 96, Training Loss: 0.8360591101467161\n",
      "Epoch 97, Training Loss: 0.8336438616415611\n",
      "Epoch 98, Training Loss: 0.8317978062127765\n",
      "Epoch 99, Training Loss: 0.8296109721176607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:26:34,936] Trial 345 finished with value: 0.6179333333333333 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8271273987633841\n",
      "Epoch 1, Training Loss: 0.9208170093988117\n",
      "Epoch 2, Training Loss: 0.8301700324044192\n",
      "Epoch 3, Training Loss: 0.8184940833794443\n",
      "Epoch 4, Training Loss: 0.8119034650630521\n",
      "Epoch 5, Training Loss: 0.807373745548994\n",
      "Epoch 6, Training Loss: 0.8042194301024416\n",
      "Epoch 7, Training Loss: 0.8037431074264354\n",
      "Epoch 8, Training Loss: 0.8011846936734995\n",
      "Epoch 9, Training Loss: 0.7996835797352898\n",
      "Epoch 10, Training Loss: 0.7990208743210125\n",
      "Epoch 11, Training Loss: 0.7981036113617116\n",
      "Epoch 12, Training Loss: 0.7970784766333444\n",
      "Epoch 13, Training Loss: 0.7968664258046257\n",
      "Epoch 14, Training Loss: 0.7956061984363355\n",
      "Epoch 15, Training Loss: 0.7952046680271178\n",
      "Epoch 16, Training Loss: 0.7943560779542851\n",
      "Epoch 17, Training Loss: 0.7946066692359466\n",
      "Epoch 18, Training Loss: 0.7935735138735377\n",
      "Epoch 19, Training Loss: 0.7932213830768614\n",
      "Epoch 20, Training Loss: 0.7927059881669238\n",
      "Epoch 21, Training Loss: 0.7936012236695541\n",
      "Epoch 22, Training Loss: 0.7913969281024502\n",
      "Epoch 23, Training Loss: 0.7919778508351262\n",
      "Epoch 24, Training Loss: 0.7910378145095998\n",
      "Epoch 25, Training Loss: 0.789878076897528\n",
      "Epoch 26, Training Loss: 0.7904211200269541\n",
      "Epoch 27, Training Loss: 0.7901615340906875\n",
      "Epoch 28, Training Loss: 0.790764485983024\n",
      "Epoch 29, Training Loss: 0.7903571908635304\n",
      "Epoch 30, Training Loss: 0.7903870221367456\n",
      "Epoch 31, Training Loss: 0.7889881446397394\n",
      "Epoch 32, Training Loss: 0.789392914718255\n",
      "Epoch 33, Training Loss: 0.7903838516177988\n",
      "Epoch 34, Training Loss: 0.7897231046418498\n",
      "Epoch 35, Training Loss: 0.7889971631810181\n",
      "Epoch 36, Training Loss: 0.7888820714520333\n",
      "Epoch 37, Training Loss: 0.7898036626944864\n",
      "Epoch 38, Training Loss: 0.789484625472162\n",
      "Epoch 39, Training Loss: 0.7900920052277415\n",
      "Epoch 40, Training Loss: 0.7887452602386474\n",
      "Epoch 41, Training Loss: 0.7886097452694312\n",
      "Epoch 42, Training Loss: 0.7884628620362819\n",
      "Epoch 43, Training Loss: 0.7880602060404039\n",
      "Epoch 44, Training Loss: 0.7888007845197406\n",
      "Epoch 45, Training Loss: 0.7883597679604265\n",
      "Epoch 46, Training Loss: 0.7880385249180901\n",
      "Epoch 47, Training Loss: 0.7879080665738959\n",
      "Epoch 48, Training Loss: 0.7876955397146985\n",
      "Epoch 49, Training Loss: 0.7876515927171348\n",
      "Epoch 50, Training Loss: 0.7878341288494884\n",
      "Epoch 51, Training Loss: 0.7882568492028946\n",
      "Epoch 52, Training Loss: 0.7887958609071889\n",
      "Epoch 53, Training Loss: 0.7875983182648967\n",
      "Epoch 54, Training Loss: 0.7874044265065875\n",
      "Epoch 55, Training Loss: 0.7873995575689732\n",
      "Epoch 56, Training Loss: 0.787816582138377\n",
      "Epoch 57, Training Loss: 0.7870932966246641\n",
      "Epoch 58, Training Loss: 0.7871652451672948\n",
      "Epoch 59, Training Loss: 0.7877542234004888\n",
      "Epoch 60, Training Loss: 0.7873803073302248\n",
      "Epoch 61, Training Loss: 0.7873211240409909\n",
      "Epoch 62, Training Loss: 0.7871381740821035\n",
      "Epoch 63, Training Loss: 0.7867791921572578\n",
      "Epoch 64, Training Loss: 0.7864622538251088\n",
      "Epoch 65, Training Loss: 0.7873465797954933\n",
      "Epoch 66, Training Loss: 0.7875574263414942\n",
      "Epoch 67, Training Loss: 0.786255389138272\n",
      "Epoch 68, Training Loss: 0.7866191534171427\n",
      "Epoch 69, Training Loss: 0.7869346421464045\n",
      "Epoch 70, Training Loss: 0.7867752406830179\n",
      "Epoch 71, Training Loss: 0.7867212705146102\n",
      "Epoch 72, Training Loss: 0.7860669516082993\n",
      "Epoch 73, Training Loss: 0.7874732763247383\n",
      "Epoch 74, Training Loss: 0.786308132974725\n",
      "Epoch 75, Training Loss: 0.7861591000305979\n",
      "Epoch 76, Training Loss: 0.7867359449092607\n",
      "Epoch 77, Training Loss: 0.7871450105107817\n",
      "Epoch 78, Training Loss: 0.7870210488936058\n",
      "Epoch 79, Training Loss: 0.7868988058620826\n",
      "Epoch 80, Training Loss: 0.7866287183940859\n",
      "Epoch 81, Training Loss: 0.7870717934199742\n",
      "Epoch 82, Training Loss: 0.7863952975524099\n",
      "Epoch 83, Training Loss: 0.7868515090834826\n",
      "Epoch 84, Training Loss: 0.785970043418999\n",
      "Epoch 85, Training Loss: 0.7867183843501528\n",
      "Epoch 86, Training Loss: 0.7869495920668867\n",
      "Epoch 87, Training Loss: 0.7882324880227111\n",
      "Epoch 88, Training Loss: 0.7872303233110816\n",
      "Epoch 89, Training Loss: 0.7870400405467901\n",
      "Epoch 90, Training Loss: 0.786947224373208\n",
      "Epoch 91, Training Loss: 0.7867890231591418\n",
      "Epoch 92, Training Loss: 0.7871820229336731\n",
      "Epoch 93, Training Loss: 0.7868634992076042\n",
      "Epoch 94, Training Loss: 0.7879662025243717\n",
      "Epoch 95, Training Loss: 0.7865481597140319\n",
      "Epoch 96, Training Loss: 0.7874229510027663\n",
      "Epoch 97, Training Loss: 0.7880898975788202\n",
      "Epoch 98, Training Loss: 0.7866991679471238\n",
      "Epoch 99, Training Loss: 0.7870883815270617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:28:35,245] Trial 346 finished with value: 0.6208666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7866695395537785\n",
      "Epoch 1, Training Loss: 0.8701985031142271\n",
      "Epoch 2, Training Loss: 0.8171171208969632\n",
      "Epoch 3, Training Loss: 0.8135850874104894\n",
      "Epoch 4, Training Loss: 0.8115772068948675\n",
      "Epoch 5, Training Loss: 0.808770467733082\n",
      "Epoch 6, Training Loss: 0.8071156636216587\n",
      "Epoch 7, Training Loss: 0.805254039907814\n",
      "Epoch 8, Training Loss: 0.8041955752480299\n",
      "Epoch 9, Training Loss: 0.8034310429616082\n",
      "Epoch 10, Training Loss: 0.8029859493549605\n",
      "Epoch 11, Training Loss: 0.8020400171889398\n",
      "Epoch 12, Training Loss: 0.8014316335656589\n",
      "Epoch 13, Training Loss: 0.8006022934626816\n",
      "Epoch 14, Training Loss: 0.8003037475105516\n",
      "Epoch 15, Training Loss: 0.7997899854989876\n",
      "Epoch 16, Training Loss: 0.7996212123928214\n",
      "Epoch 17, Training Loss: 0.7991321481260142\n",
      "Epoch 18, Training Loss: 0.7988309144973755\n",
      "Epoch 19, Training Loss: 0.7984276794401327\n",
      "Epoch 20, Training Loss: 0.7987819829381498\n",
      "Epoch 21, Training Loss: 0.7980663451037012\n",
      "Epoch 22, Training Loss: 0.7972692017268418\n",
      "Epoch 23, Training Loss: 0.7976066198564113\n",
      "Epoch 24, Training Loss: 0.7980897412264257\n",
      "Epoch 25, Training Loss: 0.795738024460642\n",
      "Epoch 26, Training Loss: 0.7954472216448389\n",
      "Epoch 27, Training Loss: 0.7950560578726288\n",
      "Epoch 28, Training Loss: 0.7939640892179389\n",
      "Epoch 29, Training Loss: 0.7938110283442906\n",
      "Epoch 30, Training Loss: 0.792891510357534\n",
      "Epoch 31, Training Loss: 0.7926058615956988\n",
      "Epoch 32, Training Loss: 0.792460274337826\n",
      "Epoch 33, Training Loss: 0.791837844095732\n",
      "Epoch 34, Training Loss: 0.7907385971313132\n",
      "Epoch 35, Training Loss: 0.7896835318185333\n",
      "Epoch 36, Training Loss: 0.7892575856438256\n",
      "Epoch 37, Training Loss: 0.7881559286350595\n",
      "Epoch 38, Training Loss: 0.7893080400345021\n",
      "Epoch 39, Training Loss: 0.7887428846574367\n",
      "Epoch 40, Training Loss: 0.7870891865034749\n",
      "Epoch 41, Training Loss: 0.7871452889048067\n",
      "Epoch 42, Training Loss: 0.7870443492903745\n",
      "Epoch 43, Training Loss: 0.786498372895377\n",
      "Epoch 44, Training Loss: 0.7863275919641767\n",
      "Epoch 45, Training Loss: 0.7864273912028262\n",
      "Epoch 46, Training Loss: 0.7862009539640039\n",
      "Epoch 47, Training Loss: 0.7857689070522337\n",
      "Epoch 48, Training Loss: 0.786178431743966\n",
      "Epoch 49, Training Loss: 0.7852411954922783\n",
      "Epoch 50, Training Loss: 0.7845536439042342\n",
      "Epoch 51, Training Loss: 0.7855302560598331\n",
      "Epoch 52, Training Loss: 0.7860827402960985\n",
      "Epoch 53, Training Loss: 0.7849952495187745\n",
      "Epoch 54, Training Loss: 0.7841995808414947\n",
      "Epoch 55, Training Loss: 0.7844844132437742\n",
      "Epoch 56, Training Loss: 0.7844821932620571\n",
      "Epoch 57, Training Loss: 0.7836785517240825\n",
      "Epoch 58, Training Loss: 0.7844656702271081\n",
      "Epoch 59, Training Loss: 0.7840761747575344\n",
      "Epoch 60, Training Loss: 0.7840750765083427\n",
      "Epoch 61, Training Loss: 0.7849181000451396\n",
      "Epoch 62, Training Loss: 0.7829675612144901\n",
      "Epoch 63, Training Loss: 0.7839873038736501\n",
      "Epoch 64, Training Loss: 0.7834481089635003\n",
      "Epoch 65, Training Loss: 0.7833369169020115\n",
      "Epoch 66, Training Loss: 0.7828022935336694\n",
      "Epoch 67, Training Loss: 0.7837289751920485\n",
      "Epoch 68, Training Loss: 0.7832350937047399\n",
      "Epoch 69, Training Loss: 0.7837111032098756\n",
      "Epoch 70, Training Loss: 0.7833050994944751\n",
      "Epoch 71, Training Loss: 0.783111892606979\n",
      "Epoch 72, Training Loss: 0.783481830851476\n",
      "Epoch 73, Training Loss: 0.783738092551554\n",
      "Epoch 74, Training Loss: 0.7826314500848154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:29:58,431] Trial 347 finished with value: 0.6326 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7831044022301982\n",
      "Epoch 1, Training Loss: 0.8502016646521432\n",
      "Epoch 2, Training Loss: 0.8158459830104856\n",
      "Epoch 3, Training Loss: 0.8112215630990222\n",
      "Epoch 4, Training Loss: 0.8084360060835243\n",
      "Epoch 5, Training Loss: 0.8100666862681396\n",
      "Epoch 6, Training Loss: 0.806398642242403\n",
      "Epoch 7, Training Loss: 0.8065451042992728\n",
      "Epoch 8, Training Loss: 0.8089423416252423\n",
      "Epoch 9, Training Loss: 0.8058588338973827\n",
      "Epoch 10, Training Loss: 0.8034851820845353\n",
      "Epoch 11, Training Loss: 0.8032625256624437\n",
      "Epoch 12, Training Loss: 0.8036181289450567\n",
      "Epoch 13, Training Loss: 0.8009216522811947\n",
      "Epoch 14, Training Loss: 0.8019133137132889\n",
      "Epoch 15, Training Loss: 0.8027619749083554\n",
      "Epoch 16, Training Loss: 0.8016332674743538\n",
      "Epoch 17, Training Loss: 0.8015833272969812\n",
      "Epoch 18, Training Loss: 0.8018202361307646\n",
      "Epoch 19, Training Loss: 0.8021462410016167\n",
      "Epoch 20, Training Loss: 0.8017639745446973\n",
      "Epoch 21, Training Loss: 0.8037957371625685\n",
      "Epoch 22, Training Loss: 0.8015072656753368\n",
      "Epoch 23, Training Loss: 0.8038016946692216\n",
      "Epoch 24, Training Loss: 0.8009313228435087\n",
      "Epoch 25, Training Loss: 0.8001834892688837\n",
      "Epoch 26, Training Loss: 0.7991178549770126\n",
      "Epoch 27, Training Loss: 0.7983188644387669\n",
      "Epoch 28, Training Loss: 0.799374662754231\n",
      "Epoch 29, Training Loss: 0.8002128581355389\n",
      "Epoch 30, Training Loss: 0.8004097753001335\n",
      "Epoch 31, Training Loss: 0.8000247595005465\n",
      "Epoch 32, Training Loss: 0.7987980788811705\n",
      "Epoch 33, Training Loss: 0.8000446555309726\n",
      "Epoch 34, Training Loss: 0.7993375678707783\n",
      "Epoch 35, Training Loss: 0.7983417900881373\n",
      "Epoch 36, Training Loss: 0.7991019342178689\n",
      "Epoch 37, Training Loss: 0.7985520124435425\n",
      "Epoch 38, Training Loss: 0.797554186905237\n",
      "Epoch 39, Training Loss: 0.7969130719514718\n",
      "Epoch 40, Training Loss: 0.798790354746625\n",
      "Epoch 41, Training Loss: 0.7981026211179288\n",
      "Epoch 42, Training Loss: 0.7974597817973087\n",
      "Epoch 43, Training Loss: 0.7992296791614446\n",
      "Epoch 44, Training Loss: 0.7972089467192055\n",
      "Epoch 45, Training Loss: 0.7972025296741858\n",
      "Epoch 46, Training Loss: 0.7971852381426588\n",
      "Epoch 47, Training Loss: 0.7993159957398149\n",
      "Epoch 48, Training Loss: 0.7974327747086833\n",
      "Epoch 49, Training Loss: 0.7982862148966108\n",
      "Epoch 50, Training Loss: 0.7973600408188383\n",
      "Epoch 51, Training Loss: 0.7961889785035212\n",
      "Epoch 52, Training Loss: 0.7974561273603511\n",
      "Epoch 53, Training Loss: 0.7972292440278189\n",
      "Epoch 54, Training Loss: 0.797243104572583\n",
      "Epoch 55, Training Loss: 0.7963543426721615\n",
      "Epoch 56, Training Loss: 0.797623432310004\n",
      "Epoch 57, Training Loss: 0.7977398172357029\n",
      "Epoch 58, Training Loss: 0.7977619521599963\n",
      "Epoch 59, Training Loss: 0.7976826803128522\n",
      "Epoch 60, Training Loss: 0.7969894130427139\n",
      "Epoch 61, Training Loss: 0.7971853747403711\n",
      "Epoch 62, Training Loss: 0.7970018649459781\n",
      "Epoch 63, Training Loss: 0.7977498482940788\n",
      "Epoch 64, Training Loss: 0.7982858711615541\n",
      "Epoch 65, Training Loss: 0.7977398459176371\n",
      "Epoch 66, Training Loss: 0.7978498448106579\n",
      "Epoch 67, Training Loss: 0.7974325955362248\n",
      "Epoch 68, Training Loss: 0.7974165480835993\n",
      "Epoch 69, Training Loss: 0.7971025329783447\n",
      "Epoch 70, Training Loss: 0.7964536813865031\n",
      "Epoch 71, Training Loss: 0.7975217242886249\n",
      "Epoch 72, Training Loss: 0.7966815034249671\n",
      "Epoch 73, Training Loss: 0.7962610405190547\n",
      "Epoch 74, Training Loss: 0.7958596919712267\n",
      "Epoch 75, Training Loss: 0.795730043353891\n",
      "Epoch 76, Training Loss: 0.797516657804188\n",
      "Epoch 77, Training Loss: 0.8000619623894082\n",
      "Epoch 78, Training Loss: 0.7976089316203182\n",
      "Epoch 79, Training Loss: 0.7980334875278903\n",
      "Epoch 80, Training Loss: 0.797852401177686\n",
      "Epoch 81, Training Loss: 0.7959493564483815\n",
      "Epoch 82, Training Loss: 0.7966147533036713\n",
      "Epoch 83, Training Loss: 0.798790088632053\n",
      "Epoch 84, Training Loss: 0.7962072797287676\n",
      "Epoch 85, Training Loss: 0.7960819183435656\n",
      "Epoch 86, Training Loss: 0.7960480216750525\n",
      "Epoch 87, Training Loss: 0.7952629141341475\n",
      "Epoch 88, Training Loss: 0.7977906299712962\n",
      "Epoch 89, Training Loss: 0.7964405821678334\n",
      "Epoch 90, Training Loss: 0.7974689005012799\n",
      "Epoch 91, Training Loss: 0.7980686079290576\n",
      "Epoch 92, Training Loss: 0.7974197646729032\n",
      "Epoch 93, Training Loss: 0.7954104610851833\n",
      "Epoch 94, Training Loss: 0.797323725366951\n",
      "Epoch 95, Training Loss: 0.7972064073820759\n",
      "Epoch 96, Training Loss: 0.7959106719583497\n",
      "Epoch 97, Training Loss: 0.7959700469683884\n",
      "Epoch 98, Training Loss: 0.7973819316777968\n",
      "Epoch 99, Training Loss: 0.797460222961311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:32:09,475] Trial 348 finished with value: 0.6316666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7960531909662978\n",
      "Epoch 1, Training Loss: 0.8519425549226649\n",
      "Epoch 2, Training Loss: 0.818932201792212\n",
      "Epoch 3, Training Loss: 0.8128019205962911\n",
      "Epoch 4, Training Loss: 0.8089768133443944\n",
      "Epoch 5, Training Loss: 0.8084673149445477\n",
      "Epoch 6, Training Loss: 0.8080047570957857\n",
      "Epoch 7, Training Loss: 0.8070561335367314\n",
      "Epoch 8, Training Loss: 0.8062411966744591\n",
      "Epoch 9, Training Loss: 0.8037005525476792\n",
      "Epoch 10, Training Loss: 0.8039105358544518\n",
      "Epoch 11, Training Loss: 0.8043678806809818\n",
      "Epoch 12, Training Loss: 0.8029134659907398\n",
      "Epoch 13, Training Loss: 0.8040257876760819\n",
      "Epoch 14, Training Loss: 0.80427498032065\n",
      "Epoch 15, Training Loss: 0.801808417895261\n",
      "Epoch 16, Training Loss: 0.802816674008089\n",
      "Epoch 17, Training Loss: 0.8027513328720541\n",
      "Epoch 18, Training Loss: 0.8035201541816487\n",
      "Epoch 19, Training Loss: 0.8011276764028212\n",
      "Epoch 20, Training Loss: 0.8027651770675883\n",
      "Epoch 21, Training Loss: 0.8026038826213163\n",
      "Epoch 22, Training Loss: 0.8021750902428346\n",
      "Epoch 23, Training Loss: 0.8022096132530886\n",
      "Epoch 24, Training Loss: 0.800353836662629\n",
      "Epoch 25, Training Loss: 0.8003024558460011\n",
      "Epoch 26, Training Loss: 0.7996907079219818\n",
      "Epoch 27, Training Loss: 0.8006391953720766\n",
      "Epoch 28, Training Loss: 0.8000715065002442\n",
      "Epoch 29, Training Loss: 0.7979646179255317\n",
      "Epoch 30, Training Loss: 0.8005083854058209\n",
      "Epoch 31, Training Loss: 0.8000234284821679\n",
      "Epoch 32, Training Loss: 0.8007302209208993\n",
      "Epoch 33, Training Loss: 0.7999320523879108\n",
      "Epoch 34, Training Loss: 0.7998977268443388\n",
      "Epoch 35, Training Loss: 0.7993294521640328\n",
      "Epoch 36, Training Loss: 0.7979010248184204\n",
      "Epoch 37, Training Loss: 0.7994684898152071\n",
      "Epoch 38, Training Loss: 0.800509048910702\n",
      "Epoch 39, Training Loss: 0.7989632363880382\n",
      "Epoch 40, Training Loss: 0.7981566544841318\n",
      "Epoch 41, Training Loss: 0.800112242418177\n",
      "Epoch 42, Training Loss: 0.8002356906498179\n",
      "Epoch 43, Training Loss: 0.7989699337061714\n",
      "Epoch 44, Training Loss: 0.7992578950349022\n",
      "Epoch 45, Training Loss: 0.7983463574157041\n",
      "Epoch 46, Training Loss: 0.7992962870878332\n",
      "Epoch 47, Training Loss: 0.7995281999952653\n",
      "Epoch 48, Training Loss: 0.7984073018326479\n",
      "Epoch 49, Training Loss: 0.7989384578256046\n",
      "Epoch 50, Training Loss: 0.799303986675599\n",
      "Epoch 51, Training Loss: 0.7982886052832884\n",
      "Epoch 52, Training Loss: 0.7987408038447885\n",
      "Epoch 53, Training Loss: 0.7993228071577408\n",
      "Epoch 54, Training Loss: 0.7989126163370469\n",
      "Epoch 55, Training Loss: 0.8001900912032408\n",
      "Epoch 56, Training Loss: 0.7994016054798575\n",
      "Epoch 57, Training Loss: 0.8003465720485239\n",
      "Epoch 58, Training Loss: 0.7970379731234383\n",
      "Epoch 59, Training Loss: 0.7996114859861486\n",
      "Epoch 60, Training Loss: 0.7977771369849934\n",
      "Epoch 61, Training Loss: 0.7978954566927517\n",
      "Epoch 62, Training Loss: 0.7996521836168626\n",
      "Epoch 63, Training Loss: 0.7980340149122126\n",
      "Epoch 64, Training Loss: 0.7992819765736076\n",
      "Epoch 65, Training Loss: 0.7991021218019373\n",
      "Epoch 66, Training Loss: 0.7987671548478743\n",
      "Epoch 67, Training Loss: 0.7989414832171272\n",
      "Epoch 68, Training Loss: 0.7985803562753341\n",
      "Epoch 69, Training Loss: 0.7975632594613468\n",
      "Epoch 70, Training Loss: 0.8004537653221804\n",
      "Epoch 71, Training Loss: 0.7997622349682976\n",
      "Epoch 72, Training Loss: 0.7994442418743583\n",
      "Epoch 73, Training Loss: 0.7995221454255721\n",
      "Epoch 74, Training Loss: 0.7987271606922149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:34:05,911] Trial 349 finished with value: 0.6297333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7989165165845086\n",
      "Epoch 1, Training Loss: 0.9176397479566416\n",
      "Epoch 2, Training Loss: 0.8310577077076847\n",
      "Epoch 3, Training Loss: 0.8198014807880373\n",
      "Epoch 4, Training Loss: 0.8120140117810185\n",
      "Epoch 5, Training Loss: 0.8086619672918678\n",
      "Epoch 6, Training Loss: 0.8045212586123244\n",
      "Epoch 7, Training Loss: 0.8015196303675945\n",
      "Epoch 8, Training Loss: 0.7983445243727892\n",
      "Epoch 9, Training Loss: 0.7975669919996333\n",
      "Epoch 10, Training Loss: 0.7955738586142547\n",
      "Epoch 11, Training Loss: 0.7953912345986617\n",
      "Epoch 12, Training Loss: 0.7946041385033973\n",
      "Epoch 13, Training Loss: 0.7940009097407635\n",
      "Epoch 14, Training Loss: 0.7937367059234389\n",
      "Epoch 15, Training Loss: 0.7921031275189909\n",
      "Epoch 16, Training Loss: 0.791556225593825\n",
      "Epoch 17, Training Loss: 0.7914897559280683\n",
      "Epoch 18, Training Loss: 0.7909900230572636\n",
      "Epoch 19, Training Loss: 0.7916414902622538\n",
      "Epoch 20, Training Loss: 0.7903369841719032\n",
      "Epoch 21, Training Loss: 0.7904373079314267\n",
      "Epoch 22, Training Loss: 0.7901546180696416\n",
      "Epoch 23, Training Loss: 0.7894668222370004\n",
      "Epoch 24, Training Loss: 0.7898403433928812\n",
      "Epoch 25, Training Loss: 0.7885600452136277\n",
      "Epoch 26, Training Loss: 0.7891089834664997\n",
      "Epoch 27, Training Loss: 0.7875724239008767\n",
      "Epoch 28, Training Loss: 0.7876389360069332\n",
      "Epoch 29, Training Loss: 0.7875254584434337\n",
      "Epoch 30, Training Loss: 0.7883708265490998\n",
      "Epoch 31, Training Loss: 0.7881036089775257\n",
      "Epoch 32, Training Loss: 0.7868822878464721\n",
      "Epoch 33, Training Loss: 0.7866111068797291\n",
      "Epoch 34, Training Loss: 0.7866457328760534\n",
      "Epoch 35, Training Loss: 0.7863588654009023\n",
      "Epoch 36, Training Loss: 0.7867850750908816\n",
      "Epoch 37, Training Loss: 0.7863969436265472\n",
      "Epoch 38, Training Loss: 0.7861333705428848\n",
      "Epoch 39, Training Loss: 0.7866189070214006\n",
      "Epoch 40, Training Loss: 0.7856702342069238\n",
      "Epoch 41, Training Loss: 0.7864139620522808\n",
      "Epoch 42, Training Loss: 0.7849020736109942\n",
      "Epoch 43, Training Loss: 0.785344001673218\n",
      "Epoch 44, Training Loss: 0.7849702269511115\n",
      "Epoch 45, Training Loss: 0.7851428729250916\n",
      "Epoch 46, Training Loss: 0.7853560851032573\n",
      "Epoch 47, Training Loss: 0.7855306713204635\n",
      "Epoch 48, Training Loss: 0.7846490684308504\n",
      "Epoch 49, Training Loss: 0.7847962797136235\n",
      "Epoch 50, Training Loss: 0.7843958311511162\n",
      "Epoch 51, Training Loss: 0.7837443206543313\n",
      "Epoch 52, Training Loss: 0.7838472050831731\n",
      "Epoch 53, Training Loss: 0.7836598382856613\n",
      "Epoch 54, Training Loss: 0.7839926063566279\n",
      "Epoch 55, Training Loss: 0.7838551930018833\n",
      "Epoch 56, Training Loss: 0.783305537700653\n",
      "Epoch 57, Training Loss: 0.7828280622797801\n",
      "Epoch 58, Training Loss: 0.7835778967778485\n",
      "Epoch 59, Training Loss: 0.7826410402928976\n",
      "Epoch 60, Training Loss: 0.7832341678160474\n",
      "Epoch 61, Training Loss: 0.7825329825394136\n",
      "Epoch 62, Training Loss: 0.7831343121098396\n",
      "Epoch 63, Training Loss: 0.7821634178771112\n",
      "Epoch 64, Training Loss: 0.7825826704053951\n",
      "Epoch 65, Training Loss: 0.782697512153396\n",
      "Epoch 66, Training Loss: 0.782812850457385\n",
      "Epoch 67, Training Loss: 0.7829728236772064\n",
      "Epoch 68, Training Loss: 0.7823418608285431\n",
      "Epoch 69, Training Loss: 0.7825220505097755\n",
      "Epoch 70, Training Loss: 0.7832602336890715\n",
      "Epoch 71, Training Loss: 0.7818606486894134\n",
      "Epoch 72, Training Loss: 0.7813831413598885\n",
      "Epoch 73, Training Loss: 0.7817426580235474\n",
      "Epoch 74, Training Loss: 0.781110580926551\n",
      "Epoch 75, Training Loss: 0.7815739378893286\n",
      "Epoch 76, Training Loss: 0.781618708103223\n",
      "Epoch 77, Training Loss: 0.78212220606051\n",
      "Epoch 78, Training Loss: 0.7814139636835657\n",
      "Epoch 79, Training Loss: 0.7816672158420535\n",
      "Epoch 80, Training Loss: 0.7811309680902868\n",
      "Epoch 81, Training Loss: 0.7811605708939688\n",
      "Epoch 82, Training Loss: 0.7815598732546756\n",
      "Epoch 83, Training Loss: 0.7814683167557968\n",
      "Epoch 84, Training Loss: 0.7820865137236459\n",
      "Epoch 85, Training Loss: 0.7813306632794832\n",
      "Epoch 86, Training Loss: 0.7809305342516505\n",
      "Epoch 87, Training Loss: 0.7803797763989384\n",
      "Epoch 88, Training Loss: 0.7806235172694787\n",
      "Epoch 89, Training Loss: 0.7812570216064166\n",
      "Epoch 90, Training Loss: 0.7805532869539763\n",
      "Epoch 91, Training Loss: 0.7797483858309294\n",
      "Epoch 92, Training Loss: 0.7805044232454516\n",
      "Epoch 93, Training Loss: 0.7800975859165191\n",
      "Epoch 94, Training Loss: 0.7805029995459363\n",
      "Epoch 95, Training Loss: 0.7802628551210676\n",
      "Epoch 96, Training Loss: 0.7809522646710388\n",
      "Epoch 97, Training Loss: 0.7804202338806668\n",
      "Epoch 98, Training Loss: 0.7804924727382516\n",
      "Epoch 99, Training Loss: 0.7805325621052792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:36:11,263] Trial 350 finished with value: 0.6282666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.780149461541857\n",
      "Epoch 1, Training Loss: 0.9689860620919396\n",
      "Epoch 2, Training Loss: 0.9329470504031462\n",
      "Epoch 3, Training Loss: 0.9129757976531983\n",
      "Epoch 4, Training Loss: 0.892925936123904\n",
      "Epoch 5, Training Loss: 0.8740113828462713\n",
      "Epoch 6, Training Loss: 0.8571238857858321\n",
      "Epoch 7, Training Loss: 0.8433218479156495\n",
      "Epoch 8, Training Loss: 0.832901008199243\n",
      "Epoch 9, Training Loss: 0.825538146425696\n",
      "Epoch 10, Training Loss: 0.8204974787375506\n",
      "Epoch 11, Training Loss: 0.8168639917233411\n",
      "Epoch 12, Training Loss: 0.8143857279244591\n",
      "Epoch 13, Training Loss: 0.8127983781169442\n",
      "Epoch 14, Training Loss: 0.8115101725914899\n",
      "Epoch 15, Training Loss: 0.8107128356484806\n",
      "Epoch 16, Training Loss: 0.8098085987567901\n",
      "Epoch 17, Training Loss: 0.8091158141809351\n",
      "Epoch 18, Training Loss: 0.8086864112405217\n",
      "Epoch 19, Training Loss: 0.808226895682952\n",
      "Epoch 20, Training Loss: 0.8078974088500528\n",
      "Epoch 21, Training Loss: 0.8073330066484563\n",
      "Epoch 22, Training Loss: 0.8071215472501867\n",
      "Epoch 23, Training Loss: 0.8066076946258545\n",
      "Epoch 24, Training Loss: 0.8064703705030329\n",
      "Epoch 25, Training Loss: 0.8061561815177694\n",
      "Epoch 26, Training Loss: 0.8059453576452592\n",
      "Epoch 27, Training Loss: 0.8055986811834223\n",
      "Epoch 28, Training Loss: 0.8053356215533088\n",
      "Epoch 29, Training Loss: 0.8049310248038348\n",
      "Epoch 30, Training Loss: 0.8044641818018521\n",
      "Epoch 31, Training Loss: 0.8043547386982862\n",
      "Epoch 32, Training Loss: 0.804117116507362\n",
      "Epoch 33, Training Loss: 0.8038034026763018\n",
      "Epoch 34, Training Loss: 0.8036647586962756\n",
      "Epoch 35, Training Loss: 0.8033551160728231\n",
      "Epoch 36, Training Loss: 0.8030842319656821\n",
      "Epoch 37, Training Loss: 0.8029586263965158\n",
      "Epoch 38, Training Loss: 0.8027900447564966\n",
      "Epoch 39, Training Loss: 0.8024567182625041\n",
      "Epoch 40, Training Loss: 0.802451633495443\n",
      "Epoch 41, Training Loss: 0.8020321332707124\n",
      "Epoch 42, Training Loss: 0.8017200060451732\n",
      "Epoch 43, Training Loss: 0.801677716479582\n",
      "Epoch 44, Training Loss: 0.8014756708285388\n",
      "Epoch 45, Training Loss: 0.8012216229298535\n",
      "Epoch 46, Training Loss: 0.8011023217089036\n",
      "Epoch 47, Training Loss: 0.8010598653905532\n",
      "Epoch 48, Training Loss: 0.8008646476969999\n",
      "Epoch 49, Training Loss: 0.8009151819173027\n",
      "Epoch 50, Training Loss: 0.8005866301059723\n",
      "Epoch 51, Training Loss: 0.8003222163985757\n",
      "Epoch 52, Training Loss: 0.8004446224605336\n",
      "Epoch 53, Training Loss: 0.8002467667355256\n",
      "Epoch 54, Training Loss: 0.7998723035700182\n",
      "Epoch 55, Training Loss: 0.8001838468804079\n",
      "Epoch 56, Training Loss: 0.7999741072514478\n",
      "Epoch 57, Training Loss: 0.7998965658861048\n",
      "Epoch 58, Training Loss: 0.7995997017972609\n",
      "Epoch 59, Training Loss: 0.7996026574162876\n",
      "Epoch 60, Training Loss: 0.7995574386680827\n",
      "Epoch 61, Training Loss: 0.7993972642281476\n",
      "Epoch 62, Training Loss: 0.7992097287318286\n",
      "Epoch 63, Training Loss: 0.7993065831240486\n",
      "Epoch 64, Training Loss: 0.7993528754570904\n",
      "Epoch 65, Training Loss: 0.7989314298068776\n",
      "Epoch 66, Training Loss: 0.798926662978004\n",
      "Epoch 67, Training Loss: 0.7991002426427953\n",
      "Epoch 68, Training Loss: 0.7990501006210552\n",
      "Epoch 69, Training Loss: 0.7988404081148259\n",
      "Epoch 70, Training Loss: 0.7987462059890522\n",
      "Epoch 71, Training Loss: 0.798698390862521\n",
      "Epoch 72, Training Loss: 0.7985766797907212\n",
      "Epoch 73, Training Loss: 0.7985063497459187\n",
      "Epoch 74, Training Loss: 0.7984611507023082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:37:22,575] Trial 351 finished with value: 0.6332 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7985050087115344\n",
      "Epoch 1, Training Loss: 0.8422152973655471\n",
      "Epoch 2, Training Loss: 0.8078733538326465\n",
      "Epoch 3, Training Loss: 0.8041723318566057\n",
      "Epoch 4, Training Loss: 0.8009076154321656\n",
      "Epoch 5, Training Loss: 0.7991116201070915\n",
      "Epoch 6, Training Loss: 0.7973163096528304\n",
      "Epoch 7, Training Loss: 0.7954508356581953\n",
      "Epoch 8, Training Loss: 0.7967184152818264\n",
      "Epoch 9, Training Loss: 0.7950858793760601\n",
      "Epoch 10, Training Loss: 0.7950110318069171\n",
      "Epoch 11, Training Loss: 0.7945831227123289\n",
      "Epoch 12, Training Loss: 0.7954652055761868\n",
      "Epoch 13, Training Loss: 0.7918478178798705\n",
      "Epoch 14, Training Loss: 0.7924314296335206\n",
      "Epoch 15, Training Loss: 0.7919595595589257\n",
      "Epoch 16, Training Loss: 0.79457084370735\n",
      "Epoch 17, Training Loss: 0.7927684411966711\n",
      "Epoch 18, Training Loss: 0.7910107021941278\n",
      "Epoch 19, Training Loss: 0.7921874823426842\n",
      "Epoch 20, Training Loss: 0.7917706126557257\n",
      "Epoch 21, Training Loss: 0.7912682042086034\n",
      "Epoch 22, Training Loss: 0.7918168972309371\n",
      "Epoch 23, Training Loss: 0.7904446195839043\n",
      "Epoch 24, Training Loss: 0.790737691857761\n",
      "Epoch 25, Training Loss: 0.7906709590352567\n",
      "Epoch 26, Training Loss: 0.7905233600085839\n",
      "Epoch 27, Training Loss: 0.7919164558102314\n",
      "Epoch 28, Training Loss: 0.7902801636466407\n",
      "Epoch 29, Training Loss: 0.790209386312872\n",
      "Epoch 30, Training Loss: 0.7895968059848125\n",
      "Epoch 31, Training Loss: 0.790276879475529\n",
      "Epoch 32, Training Loss: 0.7908527891438707\n",
      "Epoch 33, Training Loss: 0.7901482970194709\n",
      "Epoch 34, Training Loss: 0.7897144381264994\n",
      "Epoch 35, Training Loss: 0.7890209202479599\n",
      "Epoch 36, Training Loss: 0.7894942380880055\n",
      "Epoch 37, Training Loss: 0.7892042714850347\n",
      "Epoch 38, Training Loss: 0.7894261123542499\n",
      "Epoch 39, Training Loss: 0.7890853945473979\n",
      "Epoch 40, Training Loss: 0.7888531423152838\n",
      "Epoch 41, Training Loss: 0.7891391084606486\n",
      "Epoch 42, Training Loss: 0.7883492386430726\n",
      "Epoch 43, Training Loss: 0.7885686231735057\n",
      "Epoch 44, Training Loss: 0.7913687899596709\n",
      "Epoch 45, Training Loss: 0.788921608602194\n",
      "Epoch 46, Training Loss: 0.7888874319262971\n",
      "Epoch 47, Training Loss: 0.7886723696737361\n",
      "Epoch 48, Training Loss: 0.7885022081826862\n",
      "Epoch 49, Training Loss: 0.7905524161525239\n",
      "Epoch 50, Training Loss: 0.7887933612766123\n",
      "Epoch 51, Training Loss: 0.7896780139521549\n",
      "Epoch 52, Training Loss: 0.7897485502680441\n",
      "Epoch 53, Training Loss: 0.7895585927748142\n",
      "Epoch 54, Training Loss: 0.7882962459012082\n",
      "Epoch 55, Training Loss: 0.7890672888970913\n",
      "Epoch 56, Training Loss: 0.7885349098901103\n",
      "Epoch 57, Training Loss: 0.7887836806756213\n",
      "Epoch 58, Training Loss: 0.7876418224850992\n",
      "Epoch 59, Training Loss: 0.7884648948683775\n",
      "Epoch 60, Training Loss: 0.787947610536016\n",
      "Epoch 61, Training Loss: 0.7880456402785796\n",
      "Epoch 62, Training Loss: 0.7891819307678625\n",
      "Epoch 63, Training Loss: 0.7885606854481805\n",
      "Epoch 64, Training Loss: 0.7880500439414404\n",
      "Epoch 65, Training Loss: 0.7877395066999852\n",
      "Epoch 66, Training Loss: 0.788110249652002\n",
      "Epoch 67, Training Loss: 0.7881469793785784\n",
      "Epoch 68, Training Loss: 0.788120360571639\n",
      "Epoch 69, Training Loss: 0.7881455304031085\n",
      "Epoch 70, Training Loss: 0.7885087286619316\n",
      "Epoch 71, Training Loss: 0.7878897107633432\n",
      "Epoch 72, Training Loss: 0.7884001471046218\n",
      "Epoch 73, Training Loss: 0.7878922053745815\n",
      "Epoch 74, Training Loss: 0.7877760157549292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:38:49,715] Trial 352 finished with value: 0.6376 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7866490980736295\n",
      "Epoch 1, Training Loss: 1.0011134766994563\n",
      "Epoch 2, Training Loss: 0.9431136640390956\n",
      "Epoch 3, Training Loss: 0.9148705434978457\n",
      "Epoch 4, Training Loss: 0.8714412229401725\n",
      "Epoch 5, Training Loss: 0.8325037332405721\n",
      "Epoch 6, Training Loss: 0.8175945985586124\n",
      "Epoch 7, Training Loss: 0.8128554943808936\n",
      "Epoch 8, Training Loss: 0.8106707860652665\n",
      "Epoch 9, Training Loss: 0.8104042772063635\n",
      "Epoch 10, Training Loss: 0.8098304481434643\n",
      "Epoch 11, Training Loss: 0.808867096990571\n",
      "Epoch 12, Training Loss: 0.807964317153271\n",
      "Epoch 13, Training Loss: 0.8069053627494582\n",
      "Epoch 14, Training Loss: 0.8060103166372257\n",
      "Epoch 15, Training Loss: 0.8058182713680697\n",
      "Epoch 16, Training Loss: 0.8050164178798073\n",
      "Epoch 17, Training Loss: 0.8040106577084477\n",
      "Epoch 18, Training Loss: 0.804112876806044\n",
      "Epoch 19, Training Loss: 0.8036277731558434\n",
      "Epoch 20, Training Loss: 0.8033853576595622\n",
      "Epoch 21, Training Loss: 0.8026354921491523\n",
      "Epoch 22, Training Loss: 0.8025913871320567\n",
      "Epoch 23, Training Loss: 0.8018711619807365\n",
      "Epoch 24, Training Loss: 0.8020108542944255\n",
      "Epoch 25, Training Loss: 0.8031039958609674\n",
      "Epoch 26, Training Loss: 0.8018426017653674\n",
      "Epoch 27, Training Loss: 0.8020121712433664\n",
      "Epoch 28, Training Loss: 0.8008154385968258\n",
      "Epoch 29, Training Loss: 0.8017070176906156\n",
      "Epoch 30, Training Loss: 0.8007345388706465\n",
      "Epoch 31, Training Loss: 0.8007269164673367\n",
      "Epoch 32, Training Loss: 0.8007720517036611\n",
      "Epoch 33, Training Loss: 0.8005410104765928\n",
      "Epoch 34, Training Loss: 0.8002601659387574\n",
      "Epoch 35, Training Loss: 0.799924167564937\n",
      "Epoch 36, Training Loss: 0.7994084557196252\n",
      "Epoch 37, Training Loss: 0.8003980987054065\n",
      "Epoch 38, Training Loss: 0.8001227174486433\n",
      "Epoch 39, Training Loss: 0.8000399405794932\n",
      "Epoch 40, Training Loss: 0.7990618772972795\n",
      "Epoch 41, Training Loss: 0.7992579521989464\n",
      "Epoch 42, Training Loss: 0.8002617922940649\n",
      "Epoch 43, Training Loss: 0.7995675983285545\n",
      "Epoch 44, Training Loss: 0.7992626393648018\n",
      "Epoch 45, Training Loss: 0.7990119726138007\n",
      "Epoch 46, Training Loss: 0.7989283011371928\n",
      "Epoch 47, Training Loss: 0.7986005685383216\n",
      "Epoch 48, Training Loss: 0.7999998911879116\n",
      "Epoch 49, Training Loss: 0.7982239897986104\n",
      "Epoch 50, Training Loss: 0.7988081483016337\n",
      "Epoch 51, Training Loss: 0.7983207278681878\n",
      "Epoch 52, Training Loss: 0.7987455161890589\n",
      "Epoch 53, Training Loss: 0.798255867975995\n",
      "Epoch 54, Training Loss: 0.7975923896284032\n",
      "Epoch 55, Training Loss: 0.7973739780877765\n",
      "Epoch 56, Training Loss: 0.797869328538278\n",
      "Epoch 57, Training Loss: 0.7981623617329991\n",
      "Epoch 58, Training Loss: 0.79800229691025\n",
      "Epoch 59, Training Loss: 0.797704061171166\n",
      "Epoch 60, Training Loss: 0.7977655679659736\n",
      "Epoch 61, Training Loss: 0.7977856798279555\n",
      "Epoch 62, Training Loss: 0.7975598422208227\n",
      "Epoch 63, Training Loss: 0.7975263242434738\n",
      "Epoch 64, Training Loss: 0.7968843383000309\n",
      "Epoch 65, Training Loss: 0.797089878150395\n",
      "Epoch 66, Training Loss: 0.7977248593380577\n",
      "Epoch 67, Training Loss: 0.7968454154362355\n",
      "Epoch 68, Training Loss: 0.797707572467345\n",
      "Epoch 69, Training Loss: 0.7971956833860928\n",
      "Epoch 70, Training Loss: 0.7971836542724666\n",
      "Epoch 71, Training Loss: 0.7972700482920596\n",
      "Epoch 72, Training Loss: 0.796982881359588\n",
      "Epoch 73, Training Loss: 0.7968704385865003\n",
      "Epoch 74, Training Loss: 0.7978275109950761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:40:19,266] Trial 353 finished with value: 0.6313333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7964558981415024\n",
      "Epoch 1, Training Loss: 1.0017079902770825\n",
      "Epoch 2, Training Loss: 0.9485727975243017\n",
      "Epoch 3, Training Loss: 0.9333877689856336\n",
      "Epoch 4, Training Loss: 0.9137806911217539\n",
      "Epoch 5, Training Loss: 0.8921764366608813\n",
      "Epoch 6, Training Loss: 0.8702716534299062\n",
      "Epoch 7, Training Loss: 0.8515704537692823\n",
      "Epoch 8, Training Loss: 0.8378694165918164\n",
      "Epoch 9, Training Loss: 0.8277999440530189\n",
      "Epoch 10, Training Loss: 0.8217809760480895\n",
      "Epoch 11, Training Loss: 0.817079572570055\n",
      "Epoch 12, Training Loss: 0.8147280231454319\n",
      "Epoch 13, Training Loss: 0.8129215488756509\n",
      "Epoch 14, Training Loss: 0.8113453821132057\n",
      "Epoch 15, Training Loss: 0.8106724130479913\n",
      "Epoch 16, Training Loss: 0.8105542001867653\n",
      "Epoch 17, Training Loss: 0.8094295946278967\n",
      "Epoch 18, Training Loss: 0.8091879680640716\n",
      "Epoch 19, Training Loss: 0.808506122746862\n",
      "Epoch 20, Training Loss: 0.8081336688278313\n",
      "Epoch 21, Training Loss: 0.8081966223573326\n",
      "Epoch 22, Training Loss: 0.8072803923958226\n",
      "Epoch 23, Training Loss: 0.8069455183538279\n",
      "Epoch 24, Training Loss: 0.8070625000430229\n",
      "Epoch 25, Training Loss: 0.8069187425132981\n",
      "Epoch 26, Training Loss: 0.8061248012055132\n",
      "Epoch 27, Training Loss: 0.8067079080674882\n",
      "Epoch 28, Training Loss: 0.8064691176988128\n",
      "Epoch 29, Training Loss: 0.8060662288414805\n",
      "Epoch 30, Training Loss: 0.8058041664890777\n",
      "Epoch 31, Training Loss: 0.8060295234049173\n",
      "Epoch 32, Training Loss: 0.8055449855955024\n",
      "Epoch 33, Training Loss: 0.8050768946346484\n",
      "Epoch 34, Training Loss: 0.8052539665896193\n",
      "Epoch 35, Training Loss: 0.8050143849132653\n",
      "Epoch 36, Training Loss: 0.8051657853269936\n",
      "Epoch 37, Training Loss: 0.8049044410088905\n",
      "Epoch 38, Training Loss: 0.804738674217597\n",
      "Epoch 39, Training Loss: 0.8047760976884598\n",
      "Epoch 40, Training Loss: 0.8048357985073462\n",
      "Epoch 41, Training Loss: 0.8048374098046381\n",
      "Epoch 42, Training Loss: 0.8037276914245204\n",
      "Epoch 43, Training Loss: 0.8040391675511697\n",
      "Epoch 44, Training Loss: 0.806197104149295\n",
      "Epoch 45, Training Loss: 0.8036604303166383\n",
      "Epoch 46, Training Loss: 0.8040071597672943\n",
      "Epoch 47, Training Loss: 0.8036018605519059\n",
      "Epoch 48, Training Loss: 0.8035393981108988\n",
      "Epoch 49, Training Loss: 0.8034865436697365\n",
      "Epoch 50, Training Loss: 0.8035980588511417\n",
      "Epoch 51, Training Loss: 0.8044217651051686\n",
      "Epoch 52, Training Loss: 0.8041260664624379\n",
      "Epoch 53, Training Loss: 0.8028112069108433\n",
      "Epoch 54, Training Loss: 0.8025305059619416\n",
      "Epoch 55, Training Loss: 0.8029309511184692\n",
      "Epoch 56, Training Loss: 0.8028360863377277\n",
      "Epoch 57, Training Loss: 0.8028266290973004\n",
      "Epoch 58, Training Loss: 0.8022765697393203\n",
      "Epoch 59, Training Loss: 0.802280140550513\n",
      "Epoch 60, Training Loss: 0.802315648605949\n",
      "Epoch 61, Training Loss: 0.8022880053161678\n",
      "Epoch 62, Training Loss: 0.8025515369006566\n",
      "Epoch 63, Training Loss: 0.8020070458713331\n",
      "Epoch 64, Training Loss: 0.8017932930387053\n",
      "Epoch 65, Training Loss: 0.8020711088539066\n",
      "Epoch 66, Training Loss: 0.8023566272922028\n",
      "Epoch 67, Training Loss: 0.8012674347798627\n",
      "Epoch 68, Training Loss: 0.801470697105379\n",
      "Epoch 69, Training Loss: 0.8017020878038909\n",
      "Epoch 70, Training Loss: 0.8008406990900971\n",
      "Epoch 71, Training Loss: 0.8006021639906374\n",
      "Epoch 72, Training Loss: 0.8009948255424213\n",
      "Epoch 73, Training Loss: 0.8009856016116035\n",
      "Epoch 74, Training Loss: 0.800791897630333\n",
      "Epoch 75, Training Loss: 0.8004288345799411\n",
      "Epoch 76, Training Loss: 0.801029342188871\n",
      "Epoch 77, Training Loss: 0.8005216272253739\n",
      "Epoch 78, Training Loss: 0.8010678451760371\n",
      "Epoch 79, Training Loss: 0.800740939782078\n",
      "Epoch 80, Training Loss: 0.8003537962311192\n",
      "Epoch 81, Training Loss: 0.8002121381293562\n",
      "Epoch 82, Training Loss: 0.8003735797745841\n",
      "Epoch 83, Training Loss: 0.8004084467887879\n",
      "Epoch 84, Training Loss: 0.8006718346050807\n",
      "Epoch 85, Training Loss: 0.8006979141020237\n",
      "Epoch 86, Training Loss: 0.7996118375233241\n",
      "Epoch 87, Training Loss: 0.8002596970787622\n",
      "Epoch 88, Training Loss: 0.7999415978453213\n",
      "Epoch 89, Training Loss: 0.7996008183723106\n",
      "Epoch 90, Training Loss: 0.7994678467736208\n",
      "Epoch 91, Training Loss: 0.7996304726242123\n",
      "Epoch 92, Training Loss: 0.7997644564262907\n",
      "Epoch 93, Training Loss: 0.7994143371295211\n",
      "Epoch 94, Training Loss: 0.799225520101705\n",
      "Epoch 95, Training Loss: 0.799760165250391\n",
      "Epoch 96, Training Loss: 0.7990847925494488\n",
      "Epoch 97, Training Loss: 0.7993096487862723\n",
      "Epoch 98, Training Loss: 0.7992186642230902\n",
      "Epoch 99, Training Loss: 0.7992962057429148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:41:56,667] Trial 354 finished with value: 0.6355333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7990025013012994\n",
      "Epoch 1, Training Loss: 0.9108882478305271\n",
      "Epoch 2, Training Loss: 0.8244540030794932\n",
      "Epoch 3, Training Loss: 0.8148661466469442\n",
      "Epoch 4, Training Loss: 0.8076879685086416\n",
      "Epoch 5, Training Loss: 0.8030396643437837\n",
      "Epoch 6, Training Loss: 0.800366445233051\n",
      "Epoch 7, Training Loss: 0.7983587976685144\n",
      "Epoch 8, Training Loss: 0.7980162000297604\n",
      "Epoch 9, Training Loss: 0.796184251810375\n",
      "Epoch 10, Training Loss: 0.7957918182351535\n",
      "Epoch 11, Training Loss: 0.7931069946826849\n",
      "Epoch 12, Training Loss: 0.7934725697775532\n",
      "Epoch 13, Training Loss: 0.7922665557466951\n",
      "Epoch 14, Training Loss: 0.791914433285706\n",
      "Epoch 15, Training Loss: 0.7910402283632666\n",
      "Epoch 16, Training Loss: 0.7906938320712039\n",
      "Epoch 17, Training Loss: 0.7905850189969056\n",
      "Epoch 18, Training Loss: 0.7905669561006072\n",
      "Epoch 19, Training Loss: 0.7898202933763203\n",
      "Epoch 20, Training Loss: 0.7904766229758585\n",
      "Epoch 21, Training Loss: 0.7902999379581078\n",
      "Epoch 22, Training Loss: 0.7893978576014813\n",
      "Epoch 23, Training Loss: 0.7879159531198946\n",
      "Epoch 24, Training Loss: 0.7877560597613342\n",
      "Epoch 25, Training Loss: 0.7878506093993223\n",
      "Epoch 26, Training Loss: 0.7884034181896009\n",
      "Epoch 27, Training Loss: 0.787538564653325\n",
      "Epoch 28, Training Loss: 0.7876227310725621\n",
      "Epoch 29, Training Loss: 0.7873606827026023\n",
      "Epoch 30, Training Loss: 0.7874748520385054\n",
      "Epoch 31, Training Loss: 0.7884436276622284\n",
      "Epoch 32, Training Loss: 0.7864366475800822\n",
      "Epoch 33, Training Loss: 0.7862680441902993\n",
      "Epoch 34, Training Loss: 0.7862863765623337\n",
      "Epoch 35, Training Loss: 0.7856473721955952\n",
      "Epoch 36, Training Loss: 0.7861925715790655\n",
      "Epoch 37, Training Loss: 0.7858986136608554\n",
      "Epoch 38, Training Loss: 0.7865066038038497\n",
      "Epoch 39, Training Loss: 0.7852625507161133\n",
      "Epoch 40, Training Loss: 0.7851368345712361\n",
      "Epoch 41, Training Loss: 0.7852948238078813\n",
      "Epoch 42, Training Loss: 0.7848856408793227\n",
      "Epoch 43, Training Loss: 0.7852134589862106\n",
      "Epoch 44, Training Loss: 0.7851638864753838\n",
      "Epoch 45, Training Loss: 0.7845935738176332\n",
      "Epoch 46, Training Loss: 0.7843471082529627\n",
      "Epoch 47, Training Loss: 0.7845884959500535\n",
      "Epoch 48, Training Loss: 0.7836672541790439\n",
      "Epoch 49, Training Loss: 0.7834469210832639\n",
      "Epoch 50, Training Loss: 0.7837966987961217\n",
      "Epoch 51, Training Loss: 0.7835786254782425\n",
      "Epoch 52, Training Loss: 0.783655671965807\n",
      "Epoch 53, Training Loss: 0.7843222815291326\n",
      "Epoch 54, Training Loss: 0.7835659584604708\n",
      "Epoch 55, Training Loss: 0.7831551147582836\n",
      "Epoch 56, Training Loss: 0.7833422834711864\n",
      "Epoch 57, Training Loss: 0.7827722893621688\n",
      "Epoch 58, Training Loss: 0.7829260933668094\n",
      "Epoch 59, Training Loss: 0.7833897952746628\n",
      "Epoch 60, Training Loss: 0.7830188398970697\n",
      "Epoch 61, Training Loss: 0.7817687355037919\n",
      "Epoch 62, Training Loss: 0.7820890314596937\n",
      "Epoch 63, Training Loss: 0.7823241272367033\n",
      "Epoch 64, Training Loss: 0.7827620137903027\n",
      "Epoch 65, Training Loss: 0.7827432285574146\n",
      "Epoch 66, Training Loss: 0.7825944853008242\n",
      "Epoch 67, Training Loss: 0.78259281547446\n",
      "Epoch 68, Training Loss: 0.7819255845887321\n",
      "Epoch 69, Training Loss: 0.7817480872448226\n",
      "Epoch 70, Training Loss: 0.7815058922409115\n",
      "Epoch 71, Training Loss: 0.7824639406419338\n",
      "Epoch 72, Training Loss: 0.7811487422849899\n",
      "Epoch 73, Training Loss: 0.7816514911508201\n",
      "Epoch 74, Training Loss: 0.7812422935227702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:43:30,134] Trial 355 finished with value: 0.6378666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7816804059466025\n",
      "Epoch 1, Training Loss: 1.0711520863981807\n",
      "Epoch 2, Training Loss: 1.0143876197758843\n",
      "Epoch 3, Training Loss: 0.9885924737593706\n",
      "Epoch 4, Training Loss: 0.9753161113402423\n",
      "Epoch 5, Training Loss: 0.9678635705218596\n",
      "Epoch 6, Training Loss: 0.96334768112968\n",
      "Epoch 7, Training Loss: 0.9603933011784272\n",
      "Epoch 8, Training Loss: 0.9582661085970261\n",
      "Epoch 9, Training Loss: 0.9566155779361725\n",
      "Epoch 10, Training Loss: 0.9552388178600985\n",
      "Epoch 11, Training Loss: 0.9540237660267774\n",
      "Epoch 12, Training Loss: 0.9529069183153265\n",
      "Epoch 13, Training Loss: 0.95184921930818\n",
      "Epoch 14, Training Loss: 0.9508274025776807\n",
      "Epoch 15, Training Loss: 0.9498373581381405\n",
      "Epoch 16, Training Loss: 0.9488626868584576\n",
      "Epoch 17, Training Loss: 0.9478952171522028\n",
      "Epoch 18, Training Loss: 0.9469554103122038\n",
      "Epoch 19, Training Loss: 0.9460144117299248\n",
      "Epoch 20, Training Loss: 0.9450874977252063\n",
      "Epoch 21, Training Loss: 0.9441655663181754\n",
      "Epoch 22, Training Loss: 0.943245088633369\n",
      "Epoch 23, Training Loss: 0.942330054325216\n",
      "Epoch 24, Training Loss: 0.9414274886776419\n",
      "Epoch 25, Training Loss: 0.9405274600842419\n",
      "Epoch 26, Training Loss: 0.9396301808777977\n",
      "Epoch 27, Training Loss: 0.9387346778897678\n",
      "Epoch 28, Training Loss: 0.9378467848020441\n",
      "Epoch 29, Training Loss: 0.9369556568650639\n",
      "Epoch 30, Training Loss: 0.9360627330050749\n",
      "Epoch 31, Training Loss: 0.9351884590878207\n",
      "Epoch 32, Training Loss: 0.9343005422984852\n",
      "Epoch 33, Training Loss: 0.9334183953088873\n",
      "Epoch 34, Training Loss: 0.9325410556091982\n",
      "Epoch 35, Training Loss: 0.9316684705369612\n",
      "Epoch 36, Training Loss: 0.930788341550266\n",
      "Epoch 37, Training Loss: 0.9299112624981825\n",
      "Epoch 38, Training Loss: 0.9290278147949892\n",
      "Epoch 39, Training Loss: 0.9281579394200269\n",
      "Epoch 40, Training Loss: 0.9272792039899265\n",
      "Epoch 41, Training Loss: 0.9263926264117746\n",
      "Epoch 42, Training Loss: 0.9255223095417022\n",
      "Epoch 43, Training Loss: 0.924638125335469\n",
      "Epoch 44, Training Loss: 0.9237608831069049\n",
      "Epoch 45, Training Loss: 0.9228797226793626\n",
      "Epoch 46, Training Loss: 0.9219982737653396\n",
      "Epoch 47, Training Loss: 0.9211088140571818\n",
      "Epoch 48, Training Loss: 0.9202309026437647\n",
      "Epoch 49, Training Loss: 0.9193279350505156\n",
      "Epoch 50, Training Loss: 0.9184564919331495\n",
      "Epoch 51, Training Loss: 0.9175524463373071\n",
      "Epoch 52, Training Loss: 0.9166649826835184\n",
      "Epoch 53, Training Loss: 0.9157714326942669\n",
      "Epoch 54, Training Loss: 0.9148693613445058\n",
      "Epoch 55, Training Loss: 0.9139735992515788\n",
      "Epoch 56, Training Loss: 0.9130763419235454\n",
      "Epoch 57, Training Loss: 0.9121793439809014\n",
      "Epoch 58, Training Loss: 0.9112713149014642\n",
      "Epoch 59, Training Loss: 0.9103717935085297\n",
      "Epoch 60, Training Loss: 0.9094628662221572\n",
      "Epoch 61, Training Loss: 0.908546312416301\n",
      "Epoch 62, Training Loss: 0.9076357968414531\n",
      "Epoch 63, Training Loss: 0.9067229540207806\n",
      "Epoch 64, Training Loss: 0.9058051907314973\n",
      "Epoch 65, Training Loss: 0.9048952030434328\n",
      "Epoch 66, Training Loss: 0.9039696947266074\n",
      "Epoch 67, Training Loss: 0.9030451229740591\n",
      "Epoch 68, Training Loss: 0.902137688328238\n",
      "Epoch 69, Training Loss: 0.9012145193885355\n",
      "Epoch 70, Training Loss: 0.9002914108248318\n",
      "Epoch 71, Training Loss: 0.8993672180877013\n",
      "Epoch 72, Training Loss: 0.8984428895922268\n",
      "Epoch 73, Training Loss: 0.8975112970436321\n",
      "Epoch 74, Training Loss: 0.8965926327424891\n",
      "Epoch 75, Training Loss: 0.8956584623280693\n",
      "Epoch 76, Training Loss: 0.8947382866635042\n",
      "Epoch 77, Training Loss: 0.8938097682419945\n",
      "Epoch 78, Training Loss: 0.892884591326994\n",
      "Epoch 79, Training Loss: 0.8919606189868029\n",
      "Epoch 80, Training Loss: 0.8910303688049317\n",
      "Epoch 81, Training Loss: 0.8901025570841397\n",
      "Epoch 82, Training Loss: 0.8891789778541116\n",
      "Epoch 83, Training Loss: 0.8882477938427644\n",
      "Epoch 84, Training Loss: 0.8873234917837031\n",
      "Epoch 85, Training Loss: 0.8864043742768904\n",
      "Epoch 86, Training Loss: 0.885482433683732\n",
      "Epoch 87, Training Loss: 0.884567422095467\n",
      "Epoch 88, Training Loss: 0.8836563520571765\n",
      "Epoch 89, Training Loss: 0.8827286271487965\n",
      "Epoch 90, Training Loss: 0.8818199347047245\n",
      "Epoch 91, Training Loss: 0.8809046898168676\n",
      "Epoch 92, Training Loss: 0.8799956222842721\n",
      "Epoch 93, Training Loss: 0.8790940720894758\n",
      "Epoch 94, Training Loss: 0.8781842213518479\n",
      "Epoch 95, Training Loss: 0.8772901309938992\n",
      "Epoch 96, Training Loss: 0.8763886965723598\n",
      "Epoch 97, Training Loss: 0.875482402268578\n",
      "Epoch 98, Training Loss: 0.8746075325853684\n",
      "Epoch 99, Training Loss: 0.8737253797755522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:45:03,218] Trial 356 finished with value: 0.5948 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8728385193207685\n",
      "Epoch 1, Training Loss: 0.8413391223374536\n",
      "Epoch 2, Training Loss: 0.8165755015962264\n",
      "Epoch 3, Training Loss: 0.8139748721262988\n",
      "Epoch 4, Training Loss: 0.8136548353643979\n",
      "Epoch 5, Training Loss: 0.8102024274713853\n",
      "Epoch 6, Training Loss: 0.807989825851777\n",
      "Epoch 7, Training Loss: 0.8088027106313145\n",
      "Epoch 8, Training Loss: 0.8067038254878101\n",
      "Epoch 9, Training Loss: 0.8058647794583265\n",
      "Epoch 10, Training Loss: 0.8058547967321732\n",
      "Epoch 11, Training Loss: 0.8048310921472661\n",
      "Epoch 12, Training Loss: 0.804268071160597\n",
      "Epoch 13, Training Loss: 0.8049661239455728\n",
      "Epoch 14, Training Loss: 0.8042047911531784\n",
      "Epoch 15, Training Loss: 0.8043003498105442\n",
      "Epoch 16, Training Loss: 0.8033726010603063\n",
      "Epoch 17, Training Loss: 0.8035045406397652\n",
      "Epoch 18, Training Loss: 0.8036931967384675\n",
      "Epoch 19, Training Loss: 0.8027153436576618\n",
      "Epoch 20, Training Loss: 0.8025155111621408\n",
      "Epoch 21, Training Loss: 0.8017498294746175\n",
      "Epoch 22, Training Loss: 0.8018534426128163\n",
      "Epoch 23, Training Loss: 0.8019949992264018\n",
      "Epoch 24, Training Loss: 0.801151654439814\n",
      "Epoch 25, Training Loss: 0.8010387255864985\n",
      "Epoch 26, Training Loss: 0.8011618089675904\n",
      "Epoch 27, Training Loss: 0.8014681480912601\n",
      "Epoch 28, Training Loss: 0.8010451131708481\n",
      "Epoch 29, Training Loss: 0.8015096907054676\n",
      "Epoch 30, Training Loss: 0.8019809095999774\n",
      "Epoch 31, Training Loss: 0.8010110773058499\n",
      "Epoch 32, Training Loss: 0.8008528337057899\n",
      "Epoch 33, Training Loss: 0.8000448269002578\n",
      "Epoch 34, Training Loss: 0.8008674112488242\n",
      "Epoch 35, Training Loss: 0.8004215706095976\n",
      "Epoch 36, Training Loss: 0.7993201529979705\n",
      "Epoch 37, Training Loss: 0.800584785938263\n",
      "Epoch 38, Training Loss: 0.7991360229604384\n",
      "Epoch 39, Training Loss: 0.8010375515853657\n",
      "Epoch 40, Training Loss: 0.7983366919966305\n",
      "Epoch 41, Training Loss: 0.8003626446162954\n",
      "Epoch 42, Training Loss: 0.7991076737992904\n",
      "Epoch 43, Training Loss: 0.7995430465305553\n",
      "Epoch 44, Training Loss: 0.7990650027639725\n",
      "Epoch 45, Training Loss: 0.7991647712623372\n",
      "Epoch 46, Training Loss: 0.7986926482004277\n",
      "Epoch 47, Training Loss: 0.7997226398832658\n",
      "Epoch 48, Training Loss: 0.7996169396007762\n",
      "Epoch 49, Training Loss: 0.7982514523057377\n",
      "Epoch 50, Training Loss: 0.7976913387635175\n",
      "Epoch 51, Training Loss: 0.7990160670701195\n",
      "Epoch 52, Training Loss: 0.7997771730843712\n",
      "Epoch 53, Training Loss: 0.7992399991259855\n",
      "Epoch 54, Training Loss: 0.7992741379317115\n",
      "Epoch 55, Training Loss: 0.7981777364366195\n",
      "Epoch 56, Training Loss: 0.7997546876178069\n",
      "Epoch 57, Training Loss: 0.7977628319403705\n",
      "Epoch 58, Training Loss: 0.7983066286058987\n",
      "Epoch 59, Training Loss: 0.7988999668990865\n",
      "Epoch 60, Training Loss: 0.7992268152096692\n",
      "Epoch 61, Training Loss: 0.7983237264436834\n",
      "Epoch 62, Training Loss: 0.797999797778971\n",
      "Epoch 63, Training Loss: 0.7994568936263814\n",
      "Epoch 64, Training Loss: 0.7982545585492078\n",
      "Epoch 65, Training Loss: 0.797904788536184\n",
      "Epoch 66, Training Loss: 0.7974959362955655\n",
      "Epoch 67, Training Loss: 0.7978414055179147\n",
      "Epoch 68, Training Loss: 0.7978758689235238\n",
      "Epoch 69, Training Loss: 0.7981384759790757\n",
      "Epoch 70, Training Loss: 0.7982960790045122\n",
      "Epoch 71, Training Loss: 0.7984528568912955\n",
      "Epoch 72, Training Loss: 0.798028786462896\n",
      "Epoch 73, Training Loss: 0.7981230223178863\n",
      "Epoch 74, Training Loss: 0.7978442051831414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:46:31,631] Trial 357 finished with value: 0.6342 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7986718238802517\n",
      "Epoch 1, Training Loss: 1.0867507599350206\n",
      "Epoch 2, Training Loss: 1.0738871456088876\n",
      "Epoch 3, Training Loss: 1.0626410443083685\n",
      "Epoch 4, Training Loss: 1.0513832796785167\n",
      "Epoch 5, Training Loss: 1.0396411142851176\n",
      "Epoch 6, Training Loss: 1.028095046738933\n",
      "Epoch 7, Training Loss: 1.0169233438663914\n",
      "Epoch 8, Training Loss: 1.006451442277521\n",
      "Epoch 9, Training Loss: 0.9978533691929695\n",
      "Epoch 10, Training Loss: 0.9903711328829141\n",
      "Epoch 11, Training Loss: 0.9846854300427258\n",
      "Epoch 12, Training Loss: 0.9800897598266601\n",
      "Epoch 13, Training Loss: 0.9762512078858856\n",
      "Epoch 14, Training Loss: 0.9729476923333075\n",
      "Epoch 15, Training Loss: 0.9701706920351301\n",
      "Epoch 16, Training Loss: 0.9672637322791537\n",
      "Epoch 17, Training Loss: 0.9649606488701096\n",
      "Epoch 18, Training Loss: 0.9632026048531209\n",
      "Epoch 19, Training Loss: 0.9616633380266061\n",
      "Epoch 20, Training Loss: 0.9601400235541782\n",
      "Epoch 21, Training Loss: 0.9588110271253084\n",
      "Epoch 22, Training Loss: 0.9573823403595085\n",
      "Epoch 23, Training Loss: 0.9568720829217954\n",
      "Epoch 24, Training Loss: 0.9559018571573988\n",
      "Epoch 25, Training Loss: 0.9551525405475072\n",
      "Epoch 26, Training Loss: 0.9541991672121493\n",
      "Epoch 27, Training Loss: 0.9535680130012053\n",
      "Epoch 28, Training Loss: 0.952641973638893\n",
      "Epoch 29, Training Loss: 0.9520767621527937\n",
      "Epoch 30, Training Loss: 0.9514004528074336\n",
      "Epoch 31, Training Loss: 0.9502851073006938\n",
      "Epoch 32, Training Loss: 0.9496192089597085\n",
      "Epoch 33, Training Loss: 0.9488692271978335\n",
      "Epoch 34, Training Loss: 0.9481862351410371\n",
      "Epoch 35, Training Loss: 0.9476768327834911\n",
      "Epoch 36, Training Loss: 0.9469240902958059\n",
      "Epoch 37, Training Loss: 0.9463352470469654\n",
      "Epoch 38, Training Loss: 0.9459687863077436\n",
      "Epoch 39, Training Loss: 0.9450210072044143\n",
      "Epoch 40, Training Loss: 0.944274607995399\n",
      "Epoch 41, Training Loss: 0.9435790677715962\n",
      "Epoch 42, Training Loss: 0.9427129575184413\n",
      "Epoch 43, Training Loss: 0.9418594346906906\n",
      "Epoch 44, Training Loss: 0.9409144575434519\n",
      "Epoch 45, Training Loss: 0.9402909002806011\n",
      "Epoch 46, Training Loss: 0.9393241962095848\n",
      "Epoch 47, Training Loss: 0.938410316642962\n",
      "Epoch 48, Training Loss: 0.9375183251567353\n",
      "Epoch 49, Training Loss: 0.9361939302960732\n",
      "Epoch 50, Training Loss: 0.9354199235600636\n",
      "Epoch 51, Training Loss: 0.9347847894618385\n",
      "Epoch 52, Training Loss: 0.9336063815238781\n",
      "Epoch 53, Training Loss: 0.9328225262182995\n",
      "Epoch 54, Training Loss: 0.9315439897372311\n",
      "Epoch 55, Training Loss: 0.9301231709638036\n",
      "Epoch 56, Training Loss: 0.9290157340522995\n",
      "Epoch 57, Training Loss: 0.9281205439029779\n",
      "Epoch 58, Training Loss: 0.9265933121057381\n",
      "Epoch 59, Training Loss: 0.9254930905829695\n",
      "Epoch 60, Training Loss: 0.9239163700799297\n",
      "Epoch 61, Training Loss: 0.9225761880551963\n",
      "Epoch 62, Training Loss: 0.9212018914688799\n",
      "Epoch 63, Training Loss: 0.9198886260950476\n",
      "Epoch 64, Training Loss: 0.918362377729631\n",
      "Epoch 65, Training Loss: 0.9162918361506067\n",
      "Epoch 66, Training Loss: 0.9147879466974645\n",
      "Epoch 67, Training Loss: 0.9129096458729048\n",
      "Epoch 68, Training Loss: 0.9109989721075933\n",
      "Epoch 69, Training Loss: 0.9095247186216197\n",
      "Epoch 70, Training Loss: 0.907416239716953\n",
      "Epoch 71, Training Loss: 0.9056055614822789\n",
      "Epoch 72, Training Loss: 0.9031336375645229\n",
      "Epoch 73, Training Loss: 0.9010134964957273\n",
      "Epoch 74, Training Loss: 0.8987555158765692\n",
      "Epoch 75, Training Loss: 0.8963615408517365\n",
      "Epoch 76, Training Loss: 0.8936401950685602\n",
      "Epoch 77, Training Loss: 0.8911707479254644\n",
      "Epoch 78, Training Loss: 0.8882748778601338\n",
      "Epoch 79, Training Loss: 0.8854863538777917\n",
      "Epoch 80, Training Loss: 0.8831588893904722\n",
      "Epoch 81, Training Loss: 0.8801596689941291\n",
      "Epoch 82, Training Loss: 0.877410794648909\n",
      "Epoch 83, Training Loss: 0.8746573349587003\n",
      "Epoch 84, Training Loss: 0.8714843785852417\n",
      "Epoch 85, Training Loss: 0.8691560380440906\n",
      "Epoch 86, Training Loss: 0.8661782063039621\n",
      "Epoch 87, Training Loss: 0.8629835564391057\n",
      "Epoch 88, Training Loss: 0.8600414476896587\n",
      "Epoch 89, Training Loss: 0.8574133904356706\n",
      "Epoch 90, Training Loss: 0.8548955096338028\n",
      "Epoch 91, Training Loss: 0.8526997870968697\n",
      "Epoch 92, Training Loss: 0.8499314032102886\n",
      "Epoch 93, Training Loss: 0.8475780425215126\n",
      "Epoch 94, Training Loss: 0.8453719092490978\n",
      "Epoch 95, Training Loss: 0.8432718774429837\n",
      "Epoch 96, Training Loss: 0.8408971776639609\n",
      "Epoch 97, Training Loss: 0.8393533323044168\n",
      "Epoch 98, Training Loss: 0.8375876322724766\n",
      "Epoch 99, Training Loss: 0.8356832626170682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:48:10,738] Trial 358 finished with value: 0.6148666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8339583981306034\n",
      "Epoch 1, Training Loss: 0.9365136213162366\n",
      "Epoch 2, Training Loss: 0.8255929737932541\n",
      "Epoch 3, Training Loss: 0.8134309471354765\n",
      "Epoch 4, Training Loss: 0.8092718199421378\n",
      "Epoch 5, Training Loss: 0.8051598797826206\n",
      "Epoch 6, Training Loss: 0.803232470891055\n",
      "Epoch 7, Training Loss: 0.801998640719582\n",
      "Epoch 8, Training Loss: 0.8002427846543929\n",
      "Epoch 9, Training Loss: 0.7995690438326667\n",
      "Epoch 10, Training Loss: 0.7982544295226827\n",
      "Epoch 11, Training Loss: 0.7973272653888254\n",
      "Epoch 12, Training Loss: 0.7963371512469123\n",
      "Epoch 13, Training Loss: 0.7960086565157947\n",
      "Epoch 14, Training Loss: 0.7954840247771319\n",
      "Epoch 15, Training Loss: 0.7958767985596377\n",
      "Epoch 16, Training Loss: 0.7945977391214931\n",
      "Epoch 17, Training Loss: 0.7941906886241016\n",
      "Epoch 18, Training Loss: 0.793139336459777\n",
      "Epoch 19, Training Loss: 0.7938767468929291\n",
      "Epoch 20, Training Loss: 0.793055069025825\n",
      "Epoch 21, Training Loss: 0.7930280073951272\n",
      "Epoch 22, Training Loss: 0.7929202694051406\n",
      "Epoch 23, Training Loss: 0.79322010103394\n",
      "Epoch 24, Training Loss: 0.7921469392495997\n",
      "Epoch 25, Training Loss: 0.7920326750418719\n",
      "Epoch 26, Training Loss: 0.7914113846245934\n",
      "Epoch 27, Training Loss: 0.7918157262661878\n",
      "Epoch 28, Training Loss: 0.791356469743392\n",
      "Epoch 29, Training Loss: 0.7916442420202143\n",
      "Epoch 30, Training Loss: 0.7914970280142392\n",
      "Epoch 31, Training Loss: 0.7908787310824674\n",
      "Epoch 32, Training Loss: 0.791395740999895\n",
      "Epoch 33, Training Loss: 0.7909116403495564\n",
      "Epoch 34, Training Loss: 0.7907217893880957\n",
      "Epoch 35, Training Loss: 0.7906669731701121\n",
      "Epoch 36, Training Loss: 0.7905196426195257\n",
      "Epoch 37, Training Loss: 0.7901073689320508\n",
      "Epoch 38, Training Loss: 0.7908458983898163\n",
      "Epoch 39, Training Loss: 0.7910968861159157\n",
      "Epoch 40, Training Loss: 0.7904662547392004\n",
      "Epoch 41, Training Loss: 0.7900183494652019\n",
      "Epoch 42, Training Loss: 0.7905112819110646\n",
      "Epoch 43, Training Loss: 0.7891496290880091\n",
      "Epoch 44, Training Loss: 0.7908945252614863\n",
      "Epoch 45, Training Loss: 0.790799075084574\n",
      "Epoch 46, Training Loss: 0.7906518499991473\n",
      "Epoch 47, Training Loss: 0.7905726272218367\n",
      "Epoch 48, Training Loss: 0.7902098968449761\n",
      "Epoch 49, Training Loss: 0.790156068240895\n",
      "Epoch 50, Training Loss: 0.7903467503715964\n",
      "Epoch 51, Training Loss: 0.7899933427221635\n",
      "Epoch 52, Training Loss: 0.7911976671218872\n",
      "Epoch 53, Training Loss: 0.7889758565846612\n",
      "Epoch 54, Training Loss: 0.7897430008299211\n",
      "Epoch 55, Training Loss: 0.7908910521338968\n",
      "Epoch 56, Training Loss: 0.7904728439274956\n",
      "Epoch 57, Training Loss: 0.7898623849363888\n",
      "Epoch 58, Training Loss: 0.7911549798881307\n",
      "Epoch 59, Training Loss: 0.7913209985985475\n",
      "Epoch 60, Training Loss: 0.7923102283477783\n",
      "Epoch 61, Training Loss: 0.7917669789230122\n",
      "Epoch 62, Training Loss: 0.7915773372089162\n",
      "Epoch 63, Training Loss: 0.7913163796593161\n",
      "Epoch 64, Training Loss: 0.791322417750078\n",
      "Epoch 65, Training Loss: 0.7912168161308064\n",
      "Epoch 66, Training Loss: 0.7922348114322214\n",
      "Epoch 67, Training Loss: 0.792221762713264\n",
      "Epoch 68, Training Loss: 0.7929048381132238\n",
      "Epoch 69, Training Loss: 0.7927121308971854\n",
      "Epoch 70, Training Loss: 0.7921583177762873\n",
      "Epoch 71, Training Loss: 0.7924670161219204\n",
      "Epoch 72, Training Loss: 0.7924777112287633\n",
      "Epoch 73, Training Loss: 0.7927745731437907\n",
      "Epoch 74, Training Loss: 0.792059548812754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:49:58,865] Trial 359 finished with value: 0.6348666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7932321537943448\n",
      "Epoch 1, Training Loss: 1.0893914477269453\n",
      "Epoch 2, Training Loss: 1.0523758057364845\n",
      "Epoch 3, Training Loss: 1.0287751098324482\n",
      "Epoch 4, Training Loss: 1.0104421440820048\n",
      "Epoch 5, Training Loss: 0.9964146691157405\n",
      "Epoch 6, Training Loss: 0.9851195796988064\n",
      "Epoch 7, Training Loss: 0.976144706396232\n",
      "Epoch 8, Training Loss: 0.9686842550908713\n",
      "Epoch 9, Training Loss: 0.9628241479844976\n",
      "Epoch 10, Training Loss: 0.9582288191730814\n",
      "Epoch 11, Training Loss: 0.9543030471730053\n",
      "Epoch 12, Training Loss: 0.9514191509189462\n",
      "Epoch 13, Training Loss: 0.9485682437294408\n",
      "Epoch 14, Training Loss: 0.9460603024726524\n",
      "Epoch 15, Training Loss: 0.9445180869640264\n",
      "Epoch 16, Training Loss: 0.9426503795430177\n",
      "Epoch 17, Training Loss: 0.9414097155843463\n",
      "Epoch 18, Training Loss: 0.9395349704233328\n",
      "Epoch 19, Training Loss: 0.9386175836835589\n",
      "Epoch 20, Training Loss: 0.9371138379986125\n",
      "Epoch 21, Training Loss: 0.9359768422026383\n",
      "Epoch 22, Training Loss: 0.9354614562558052\n",
      "Epoch 23, Training Loss: 0.9344803878239223\n",
      "Epoch 24, Training Loss: 0.9331545058945964\n",
      "Epoch 25, Training Loss: 0.9326773333370237\n",
      "Epoch 26, Training Loss: 0.9314444170858627\n",
      "Epoch 27, Training Loss: 0.930768955381293\n",
      "Epoch 28, Training Loss: 0.9298898814316082\n",
      "Epoch 29, Training Loss: 0.9293382713669225\n",
      "Epoch 30, Training Loss: 0.928823188910807\n",
      "Epoch 31, Training Loss: 0.9278552620034469\n",
      "Epoch 32, Training Loss: 0.9272632555854051\n",
      "Epoch 33, Training Loss: 0.9259017171716332\n",
      "Epoch 34, Training Loss: 0.9252942766462053\n",
      "Epoch 35, Training Loss: 0.9250476254556412\n",
      "Epoch 36, Training Loss: 0.9242502159642098\n",
      "Epoch 37, Training Loss: 0.923400910994164\n",
      "Epoch 38, Training Loss: 0.9231353952472371\n",
      "Epoch 39, Training Loss: 0.9222340467280912\n",
      "Epoch 40, Training Loss: 0.9217772387920465\n",
      "Epoch 41, Training Loss: 0.9206686442059682\n",
      "Epoch 42, Training Loss: 0.9207450076153404\n",
      "Epoch 43, Training Loss: 0.9202782749233389\n",
      "Epoch 44, Training Loss: 0.9189799330288306\n",
      "Epoch 45, Training Loss: 0.9186200154455084\n",
      "Epoch 46, Training Loss: 0.9183968325306598\n",
      "Epoch 47, Training Loss: 0.917656639912971\n",
      "Epoch 48, Training Loss: 0.9169222948246433\n",
      "Epoch 49, Training Loss: 0.916850839700914\n",
      "Epoch 50, Training Loss: 0.9160534668685798\n",
      "Epoch 51, Training Loss: 0.9154087998813256\n",
      "Epoch 52, Training Loss: 0.9149165642888922\n",
      "Epoch 53, Training Loss: 0.9140034255228544\n",
      "Epoch 54, Training Loss: 0.9141937764067399\n",
      "Epoch 55, Training Loss: 0.9131216440882002\n",
      "Epoch 56, Training Loss: 0.912908699369072\n",
      "Epoch 57, Training Loss: 0.9121903580830509\n",
      "Epoch 58, Training Loss: 0.9118539747438933\n",
      "Epoch 59, Training Loss: 0.9109833331036389\n",
      "Epoch 60, Training Loss: 0.9103922457623302\n",
      "Epoch 61, Training Loss: 0.9100752665584249\n",
      "Epoch 62, Training Loss: 0.9097999828202384\n",
      "Epoch 63, Training Loss: 0.9087677725275657\n",
      "Epoch 64, Training Loss: 0.9090185583982252\n",
      "Epoch 65, Training Loss: 0.9082082168500226\n",
      "Epoch 66, Training Loss: 0.9073415870057013\n",
      "Epoch 67, Training Loss: 0.9071713902000198\n",
      "Epoch 68, Training Loss: 0.9060969605481715\n",
      "Epoch 69, Training Loss: 0.9060129442609343\n",
      "Epoch 70, Training Loss: 0.9055861344014792\n",
      "Epoch 71, Training Loss: 0.9048947226732297\n",
      "Epoch 72, Training Loss: 0.9045304809297834\n",
      "Epoch 73, Training Loss: 0.9031287622631045\n",
      "Epoch 74, Training Loss: 0.9031302081911188\n",
      "Epoch 75, Training Loss: 0.9025304542448288\n",
      "Epoch 76, Training Loss: 0.9017470591050342\n",
      "Epoch 77, Training Loss: 0.9012833545978804\n",
      "Epoch 78, Training Loss: 0.9013120910278837\n",
      "Epoch 79, Training Loss: 0.9000611148382488\n",
      "Epoch 80, Training Loss: 0.9002411156668699\n",
      "Epoch 81, Training Loss: 0.8994441495802169\n",
      "Epoch 82, Training Loss: 0.8988772776790132\n",
      "Epoch 83, Training Loss: 0.8981595563709288\n",
      "Epoch 84, Training Loss: 0.8975839808471221\n",
      "Epoch 85, Training Loss: 0.8969678848309625\n",
      "Epoch 86, Training Loss: 0.8969382880325604\n",
      "Epoch 87, Training Loss: 0.8960558885918524\n",
      "Epoch 88, Training Loss: 0.8956464045029834\n",
      "Epoch 89, Training Loss: 0.8947194257176908\n",
      "Epoch 90, Training Loss: 0.8947006816254522\n",
      "Epoch 91, Training Loss: 0.8940041144091384\n",
      "Epoch 92, Training Loss: 0.8935569730020108\n",
      "Epoch 93, Training Loss: 0.8924030270791592\n",
      "Epoch 94, Training Loss: 0.8921419979038095\n",
      "Epoch 95, Training Loss: 0.8914722832521997\n",
      "Epoch 96, Training Loss: 0.8912486913508939\n",
      "Epoch 97, Training Loss: 0.8903431044485336\n",
      "Epoch 98, Training Loss: 0.8899781571295028\n",
      "Epoch 99, Training Loss: 0.8894278191086045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:51:22,197] Trial 360 finished with value: 0.5783333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8883935331401969\n",
      "Epoch 1, Training Loss: 0.9471408990989054\n",
      "Epoch 2, Training Loss: 0.8841158762014002\n",
      "Epoch 3, Training Loss: 0.8422781605469553\n",
      "Epoch 4, Training Loss: 0.8197412173550828\n",
      "Epoch 5, Training Loss: 0.8099828717403842\n",
      "Epoch 6, Training Loss: 0.8054337485392291\n",
      "Epoch 7, Training Loss: 0.8031985286483191\n",
      "Epoch 8, Training Loss: 0.8019574199404035\n",
      "Epoch 9, Training Loss: 0.8013398143581878\n",
      "Epoch 10, Training Loss: 0.8008038792395054\n",
      "Epoch 11, Training Loss: 0.8001560136787873\n",
      "Epoch 12, Training Loss: 0.7999501094782263\n",
      "Epoch 13, Training Loss: 0.7996117764845827\n",
      "Epoch 14, Training Loss: 0.7996192356697599\n",
      "Epoch 15, Training Loss: 0.7986756318493893\n",
      "Epoch 16, Training Loss: 0.7987531981073824\n",
      "Epoch 17, Training Loss: 0.797970257576247\n",
      "Epoch 18, Training Loss: 0.7981919171218586\n",
      "Epoch 19, Training Loss: 0.7979536654357623\n",
      "Epoch 20, Training Loss: 0.7975299735714618\n",
      "Epoch 21, Training Loss: 0.7970134881206025\n",
      "Epoch 22, Training Loss: 0.797459145954677\n",
      "Epoch 23, Training Loss: 0.7968199730815744\n",
      "Epoch 24, Training Loss: 0.7967339340009187\n",
      "Epoch 25, Training Loss: 0.7966460574838452\n",
      "Epoch 26, Training Loss: 0.7957592491816757\n",
      "Epoch 27, Training Loss: 0.7957543366833737\n",
      "Epoch 28, Training Loss: 0.7955170757788464\n",
      "Epoch 29, Training Loss: 0.79500468790083\n",
      "Epoch 30, Training Loss: 0.7958198554533765\n",
      "Epoch 31, Training Loss: 0.7948579555167291\n",
      "Epoch 32, Training Loss: 0.7945201187205494\n",
      "Epoch 33, Training Loss: 0.7937516431163129\n",
      "Epoch 34, Training Loss: 0.7936624742092047\n",
      "Epoch 35, Training Loss: 0.793059096703852\n",
      "Epoch 36, Training Loss: 0.7934246904867932\n",
      "Epoch 37, Training Loss: 0.7930375439780099\n",
      "Epoch 38, Training Loss: 0.7924502258910272\n",
      "Epoch 39, Training Loss: 0.7923352781095003\n",
      "Epoch 40, Training Loss: 0.7928040384349967\n",
      "Epoch 41, Training Loss: 0.7920706187872062\n",
      "Epoch 42, Training Loss: 0.7915556014480447\n",
      "Epoch 43, Training Loss: 0.7920304208769834\n",
      "Epoch 44, Training Loss: 0.7910192682778925\n",
      "Epoch 45, Training Loss: 0.7917826190030665\n",
      "Epoch 46, Training Loss: 0.7917600603928243\n",
      "Epoch 47, Training Loss: 0.7912215823517706\n",
      "Epoch 48, Training Loss: 0.7912818344015824\n",
      "Epoch 49, Training Loss: 0.7910965837034067\n",
      "Epoch 50, Training Loss: 0.7908912508111251\n",
      "Epoch 51, Training Loss: 0.7906365352465694\n",
      "Epoch 52, Training Loss: 0.7913455603714276\n",
      "Epoch 53, Training Loss: 0.7903286814689636\n",
      "Epoch 54, Training Loss: 0.7901761688684162\n",
      "Epoch 55, Training Loss: 0.7905592520434157\n",
      "Epoch 56, Training Loss: 0.7903975581764279\n",
      "Epoch 57, Training Loss: 0.7902849435806274\n",
      "Epoch 58, Training Loss: 0.7907347689893909\n",
      "Epoch 59, Training Loss: 0.790135115429871\n",
      "Epoch 60, Training Loss: 0.7904191107678235\n",
      "Epoch 61, Training Loss: 0.7897308688414725\n",
      "Epoch 62, Training Loss: 0.7901045200519992\n",
      "Epoch 63, Training Loss: 0.7898487772260393\n",
      "Epoch 64, Training Loss: 0.790313216259605\n",
      "Epoch 65, Training Loss: 0.7897936355798765\n",
      "Epoch 66, Training Loss: 0.7895347481383417\n",
      "Epoch 67, Training Loss: 0.789696332745086\n",
      "Epoch 68, Training Loss: 0.7904589273875817\n",
      "Epoch 69, Training Loss: 0.7894801306545286\n",
      "Epoch 70, Training Loss: 0.7898079390812637\n",
      "Epoch 71, Training Loss: 0.7896132805293664\n",
      "Epoch 72, Training Loss: 0.7894033669529105\n",
      "Epoch 73, Training Loss: 0.7893086487189271\n",
      "Epoch 74, Training Loss: 0.790098077999918\n",
      "Epoch 75, Training Loss: 0.7893128800212889\n",
      "Epoch 76, Training Loss: 0.7894459188880777\n",
      "Epoch 77, Training Loss: 0.7895828939021978\n",
      "Epoch 78, Training Loss: 0.7895473734776777\n",
      "Epoch 79, Training Loss: 0.789265021076776\n",
      "Epoch 80, Training Loss: 0.7891782402095938\n",
      "Epoch 81, Training Loss: 0.7897570528482136\n",
      "Epoch 82, Training Loss: 0.7900004884354154\n",
      "Epoch 83, Training Loss: 0.7895294137467119\n",
      "Epoch 84, Training Loss: 0.78928787753098\n",
      "Epoch 85, Training Loss: 0.7890298399710117\n",
      "Epoch 86, Training Loss: 0.78914322306339\n",
      "Epoch 87, Training Loss: 0.7893147750008375\n",
      "Epoch 88, Training Loss: 0.7905865801904435\n",
      "Epoch 89, Training Loss: 0.789595848008206\n",
      "Epoch 90, Training Loss: 0.7897720269690779\n",
      "Epoch 91, Training Loss: 0.7892657204678184\n",
      "Epoch 92, Training Loss: 0.7890652521212298\n",
      "Epoch 93, Training Loss: 0.7894658388051772\n",
      "Epoch 94, Training Loss: 0.7889034163682981\n",
      "Epoch 95, Training Loss: 0.789230149401758\n",
      "Epoch 96, Training Loss: 0.7897306610767106\n",
      "Epoch 97, Training Loss: 0.7895171746275479\n",
      "Epoch 98, Training Loss: 0.7890492469744574\n",
      "Epoch 99, Training Loss: 0.7890385752333734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:53:00,314] Trial 361 finished with value: 0.6386666666666667 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7889032832661965\n",
      "Epoch 1, Training Loss: 0.8541408666094443\n",
      "Epoch 2, Training Loss: 0.8201822364240661\n",
      "Epoch 3, Training Loss: 0.813841181859038\n",
      "Epoch 4, Training Loss: 0.8079785350570106\n",
      "Epoch 5, Training Loss: 0.802248353796794\n",
      "Epoch 6, Training Loss: 0.7967314966639182\n",
      "Epoch 7, Training Loss: 0.795443520330845\n",
      "Epoch 8, Training Loss: 0.7937870027427386\n",
      "Epoch 9, Training Loss: 0.7922805006342722\n",
      "Epoch 10, Training Loss: 0.7922802065548143\n",
      "Epoch 11, Training Loss: 0.7909386529062027\n",
      "Epoch 12, Training Loss: 0.7912359290553215\n",
      "Epoch 13, Training Loss: 0.789807696584472\n",
      "Epoch 14, Training Loss: 0.7901386090687343\n",
      "Epoch 15, Training Loss: 0.7890514354956778\n",
      "Epoch 16, Training Loss: 0.7894476239842579\n",
      "Epoch 17, Training Loss: 0.7888954856341943\n",
      "Epoch 18, Training Loss: 0.7884123653397525\n",
      "Epoch 19, Training Loss: 0.7883944892345515\n",
      "Epoch 20, Training Loss: 0.78860610768311\n",
      "Epoch 21, Training Loss: 0.7877941719571451\n",
      "Epoch 22, Training Loss: 0.7872070986973612\n",
      "Epoch 23, Training Loss: 0.7869958036824276\n",
      "Epoch 24, Training Loss: 0.78722401247885\n",
      "Epoch 25, Training Loss: 0.7863599215235029\n",
      "Epoch 26, Training Loss: 0.7863830241045557\n",
      "Epoch 27, Training Loss: 0.7867211833932346\n",
      "Epoch 28, Training Loss: 0.7866414076403567\n",
      "Epoch 29, Training Loss: 0.7855123175714249\n",
      "Epoch 30, Training Loss: 0.7857195522552146\n",
      "Epoch 31, Training Loss: 0.7856334237227762\n",
      "Epoch 32, Training Loss: 0.7854677685221335\n",
      "Epoch 33, Training Loss: 0.7848329640868911\n",
      "Epoch 34, Training Loss: 0.7844337388985139\n",
      "Epoch 35, Training Loss: 0.7847730451956727\n",
      "Epoch 36, Training Loss: 0.7844985873179329\n",
      "Epoch 37, Training Loss: 0.7846186854785546\n",
      "Epoch 38, Training Loss: 0.7843858007201575\n",
      "Epoch 39, Training Loss: 0.7848321788293079\n",
      "Epoch 40, Training Loss: 0.7843322718950142\n",
      "Epoch 41, Training Loss: 0.7839875000760071\n",
      "Epoch 42, Training Loss: 0.7842733663723881\n",
      "Epoch 43, Training Loss: 0.7835437313058322\n",
      "Epoch 44, Training Loss: 0.7834775767828289\n",
      "Epoch 45, Training Loss: 0.7837499447334978\n",
      "Epoch 46, Training Loss: 0.7839605126165806\n",
      "Epoch 47, Training Loss: 0.7828641791092722\n",
      "Epoch 48, Training Loss: 0.7830860289415918\n",
      "Epoch 49, Training Loss: 0.7831732011379156\n",
      "Epoch 50, Training Loss: 0.783038948801227\n",
      "Epoch 51, Training Loss: 0.7833577320091707\n",
      "Epoch 52, Training Loss: 0.7831463251795088\n",
      "Epoch 53, Training Loss: 0.7817491255756608\n",
      "Epoch 54, Training Loss: 0.7826233000683606\n",
      "Epoch 55, Training Loss: 0.7826572525770145\n",
      "Epoch 56, Training Loss: 0.7819816147922574\n",
      "Epoch 57, Training Loss: 0.7818764726022133\n",
      "Epoch 58, Training Loss: 0.7823614639447147\n",
      "Epoch 59, Training Loss: 0.7822223621203487\n",
      "Epoch 60, Training Loss: 0.7817378129277911\n",
      "Epoch 61, Training Loss: 0.7818239028292491\n",
      "Epoch 62, Training Loss: 0.7812598577119354\n",
      "Epoch 63, Training Loss: 0.7824126891623763\n",
      "Epoch 64, Training Loss: 0.7816256721216933\n",
      "Epoch 65, Training Loss: 0.7820034404446308\n",
      "Epoch 66, Training Loss: 0.78191760939763\n",
      "Epoch 67, Training Loss: 0.7814179739557711\n",
      "Epoch 68, Training Loss: 0.7810412394373041\n",
      "Epoch 69, Training Loss: 0.7810525428979916\n",
      "Epoch 70, Training Loss: 0.7813141557507048\n",
      "Epoch 71, Training Loss: 0.7809005573279876\n",
      "Epoch 72, Training Loss: 0.7814120293559884\n",
      "Epoch 73, Training Loss: 0.78057217562109\n",
      "Epoch 74, Training Loss: 0.78075136134499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:54:35,117] Trial 362 finished with value: 0.6397333333333334 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7803774829645802\n",
      "Epoch 1, Training Loss: 0.8778248613722184\n",
      "Epoch 2, Training Loss: 0.8144590733331792\n",
      "Epoch 3, Training Loss: 0.8075567983178531\n",
      "Epoch 4, Training Loss: 0.8045745700247148\n",
      "Epoch 5, Training Loss: 0.8022162936014288\n",
      "Epoch 6, Training Loss: 0.8001522682694828\n",
      "Epoch 7, Training Loss: 0.7983701210162218\n",
      "Epoch 8, Training Loss: 0.7956036024935105\n",
      "Epoch 9, Training Loss: 0.7943377176453086\n",
      "Epoch 10, Training Loss: 0.7925710757339702\n",
      "Epoch 11, Training Loss: 0.7914134965223425\n",
      "Epoch 12, Training Loss: 0.7908130508310655\n",
      "Epoch 13, Training Loss: 0.7898569353889017\n",
      "Epoch 14, Training Loss: 0.7892066969591028\n",
      "Epoch 15, Training Loss: 0.7886687032615437\n",
      "Epoch 16, Training Loss: 0.7881854135148666\n",
      "Epoch 17, Training Loss: 0.78777798631612\n",
      "Epoch 18, Training Loss: 0.786942438027438\n",
      "Epoch 19, Training Loss: 0.786889921426773\n",
      "Epoch 20, Training Loss: 0.7865490665856529\n",
      "Epoch 21, Training Loss: 0.7864007396557752\n",
      "Epoch 22, Training Loss: 0.786082708905725\n",
      "Epoch 23, Training Loss: 0.7856317379194148\n",
      "Epoch 24, Training Loss: 0.7854193054227268\n",
      "Epoch 25, Training Loss: 0.7851386512027068\n",
      "Epoch 26, Training Loss: 0.7847051013217253\n",
      "Epoch 27, Training Loss: 0.7848136358401354\n",
      "Epoch 28, Training Loss: 0.7842191705282997\n",
      "Epoch 29, Training Loss: 0.7842031987274394\n",
      "Epoch 30, Training Loss: 0.7837674007696264\n",
      "Epoch 31, Training Loss: 0.7839092559674207\n",
      "Epoch 32, Training Loss: 0.7837232309229234\n",
      "Epoch 33, Training Loss: 0.7835012969549965\n",
      "Epoch 34, Training Loss: 0.783336182271733\n",
      "Epoch 35, Training Loss: 0.7827021189296947\n",
      "Epoch 36, Training Loss: 0.7827046443434322\n",
      "Epoch 37, Training Loss: 0.7821076719901141\n",
      "Epoch 38, Training Loss: 0.7822903814035304\n",
      "Epoch 39, Training Loss: 0.7819950577792\n",
      "Epoch 40, Training Loss: 0.7822141084951513\n",
      "Epoch 41, Training Loss: 0.7816974137109869\n",
      "Epoch 42, Training Loss: 0.7815240941328161\n",
      "Epoch 43, Training Loss: 0.7815303893650279\n",
      "Epoch 44, Training Loss: 0.7816402869364795\n",
      "Epoch 45, Training Loss: 0.7815100132016575\n",
      "Epoch 46, Training Loss: 0.7809533668265624\n",
      "Epoch 47, Training Loss: 0.7811175182987662\n",
      "Epoch 48, Training Loss: 0.7810919901903938\n",
      "Epoch 49, Training Loss: 0.7808735122400171\n",
      "Epoch 50, Training Loss: 0.7810390452076407\n",
      "Epoch 51, Training Loss: 0.780296245322508\n",
      "Epoch 52, Training Loss: 0.7805757661426769\n",
      "Epoch 53, Training Loss: 0.7805499193948858\n",
      "Epoch 54, Training Loss: 0.7804170288057888\n",
      "Epoch 55, Training Loss: 0.7803839616214527\n",
      "Epoch 56, Training Loss: 0.7800991023989284\n",
      "Epoch 57, Training Loss: 0.7798469495773316\n",
      "Epoch 58, Training Loss: 0.7803651691885556\n",
      "Epoch 59, Training Loss: 0.7800109516873079\n",
      "Epoch 60, Training Loss: 0.7800029179629158\n",
      "Epoch 61, Training Loss: 0.7794939632976756\n",
      "Epoch 62, Training Loss: 0.7794945120110232\n",
      "Epoch 63, Training Loss: 0.7798162091479582\n",
      "Epoch 64, Training Loss: 0.7795226313787348\n",
      "Epoch 65, Training Loss: 0.7794788387242485\n",
      "Epoch 66, Training Loss: 0.7793812521766214\n",
      "Epoch 67, Training Loss: 0.7792853771938997\n",
      "Epoch 68, Training Loss: 0.7791697542807635\n",
      "Epoch 69, Training Loss: 0.7791760948826285\n",
      "Epoch 70, Training Loss: 0.7785391423982733\n",
      "Epoch 71, Training Loss: 0.7787090631793527\n",
      "Epoch 72, Training Loss: 0.7784262921529658\n",
      "Epoch 73, Training Loss: 0.7789031018930322\n",
      "Epoch 74, Training Loss: 0.778688876839245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:56:12,630] Trial 363 finished with value: 0.6396 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7784574634888592\n",
      "Epoch 1, Training Loss: 0.9454641605124754\n",
      "Epoch 2, Training Loss: 0.8958526052446927\n",
      "Epoch 3, Training Loss: 0.8537110371449415\n",
      "Epoch 4, Training Loss: 0.8244628898536458\n",
      "Epoch 5, Training Loss: 0.8113839584238389\n",
      "Epoch 6, Training Loss: 0.8062084361384897\n",
      "Epoch 7, Training Loss: 0.8042876886620242\n",
      "Epoch 8, Training Loss: 0.8029088773446924\n",
      "Epoch 9, Training Loss: 0.8018487414191751\n",
      "Epoch 10, Training Loss: 0.8009188708838294\n",
      "Epoch 11, Training Loss: 0.800344813711503\n",
      "Epoch 12, Training Loss: 0.8001677065035876\n",
      "Epoch 13, Training Loss: 0.799622283262365\n",
      "Epoch 14, Training Loss: 0.7994216475767247\n",
      "Epoch 15, Training Loss: 0.7995062555285061\n",
      "Epoch 16, Training Loss: 0.7991375932973974\n",
      "Epoch 17, Training Loss: 0.7987463741442736\n",
      "Epoch 18, Training Loss: 0.798302314491833\n",
      "Epoch 19, Training Loss: 0.7984465413233813\n",
      "Epoch 20, Training Loss: 0.7977920476128073\n",
      "Epoch 21, Training Loss: 0.7980634244750527\n",
      "Epoch 22, Training Loss: 0.7974216219958137\n",
      "Epoch 23, Training Loss: 0.7974713472057792\n",
      "Epoch 24, Training Loss: 0.7970152484669405\n",
      "Epoch 25, Training Loss: 0.7967696619735044\n",
      "Epoch 26, Training Loss: 0.7966465648482828\n",
      "Epoch 27, Training Loss: 0.7960317402026232\n",
      "Epoch 28, Training Loss: 0.7956285086098839\n",
      "Epoch 29, Training Loss: 0.7953400367147783\n",
      "Epoch 30, Training Loss: 0.7948651102711173\n",
      "Epoch 31, Training Loss: 0.7944248263976154\n",
      "Epoch 32, Training Loss: 0.7938998430616715\n",
      "Epoch 33, Training Loss: 0.7937422507650712\n",
      "Epoch 34, Training Loss: 0.7933110501485713\n",
      "Epoch 35, Training Loss: 0.7929762055593378\n",
      "Epoch 36, Training Loss: 0.7928891919640934\n",
      "Epoch 37, Training Loss: 0.7927924413540784\n",
      "Epoch 38, Training Loss: 0.7923272644772249\n",
      "Epoch 39, Training Loss: 0.7923386416014503\n",
      "Epoch 40, Training Loss: 0.7918535553006565\n",
      "Epoch 41, Training Loss: 0.7916248058571534\n",
      "Epoch 42, Training Loss: 0.7914689688822802\n",
      "Epoch 43, Training Loss: 0.7914494348974789\n",
      "Epoch 44, Training Loss: 0.7915190234605004\n",
      "Epoch 45, Training Loss: 0.7914054895849789\n",
      "Epoch 46, Training Loss: 0.7911375812221976\n",
      "Epoch 47, Training Loss: 0.7912112038275775\n",
      "Epoch 48, Training Loss: 0.7912817119850832\n",
      "Epoch 49, Training Loss: 0.7907207719017477\n",
      "Epoch 50, Training Loss: 0.7905840539932251\n",
      "Epoch 51, Training Loss: 0.7908855102342718\n",
      "Epoch 52, Training Loss: 0.7907407523604\n",
      "Epoch 53, Training Loss: 0.7903611208410825\n",
      "Epoch 54, Training Loss: 0.7902420752188739\n",
      "Epoch 55, Training Loss: 0.7904027677283567\n",
      "Epoch 56, Training Loss: 0.7908233149612651\n",
      "Epoch 57, Training Loss: 0.7900245343236362\n",
      "Epoch 58, Training Loss: 0.7904779536583845\n",
      "Epoch 59, Training Loss: 0.7901689659847932\n",
      "Epoch 60, Training Loss: 0.7901482792461619\n",
      "Epoch 61, Training Loss: 0.7901461513603435\n",
      "Epoch 62, Training Loss: 0.7901625162012437\n",
      "Epoch 63, Training Loss: 0.7897163898103378\n",
      "Epoch 64, Training Loss: 0.790099305194967\n",
      "Epoch 65, Training Loss: 0.7899021381490371\n",
      "Epoch 66, Training Loss: 0.7893423772559447\n",
      "Epoch 67, Training Loss: 0.7895811770242803\n",
      "Epoch 68, Training Loss: 0.7897286252414479\n",
      "Epoch 69, Training Loss: 0.7896572219624239\n",
      "Epoch 70, Training Loss: 0.7896544088335599\n",
      "Epoch 71, Training Loss: 0.7896908581256866\n",
      "Epoch 72, Training Loss: 0.7897484167183146\n",
      "Epoch 73, Training Loss: 0.789621888329001\n",
      "Epoch 74, Training Loss: 0.7894367537778967\n",
      "Epoch 75, Training Loss: 0.7895956550626194\n",
      "Epoch 76, Training Loss: 0.7893673331597272\n",
      "Epoch 77, Training Loss: 0.789245094832252\n",
      "Epoch 78, Training Loss: 0.7889284387055565\n",
      "Epoch 79, Training Loss: 0.7892305373444277\n",
      "Epoch 80, Training Loss: 0.7892680231262655\n",
      "Epoch 81, Training Loss: 0.7895911095422857\n",
      "Epoch 82, Training Loss: 0.7891068063764011\n",
      "Epoch 83, Training Loss: 0.7890229279854718\n",
      "Epoch 84, Training Loss: 0.7890279485197629\n",
      "Epoch 85, Training Loss: 0.789093667899861\n",
      "Epoch 86, Training Loss: 0.7893718274901895\n",
      "Epoch 87, Training Loss: 0.7890782851331374\n",
      "Epoch 88, Training Loss: 0.7889954595004811\n",
      "Epoch 89, Training Loss: 0.7886755416673773\n",
      "Epoch 90, Training Loss: 0.7892199188120225\n",
      "Epoch 91, Training Loss: 0.7891731188577764\n",
      "Epoch 92, Training Loss: 0.7890320366971633\n",
      "Epoch 93, Training Loss: 0.7885839860579547\n",
      "Epoch 94, Training Loss: 0.7887544020484476\n",
      "Epoch 95, Training Loss: 0.7891241385656245\n",
      "Epoch 96, Training Loss: 0.7889377614329843\n",
      "Epoch 97, Training Loss: 0.7887371075153351\n",
      "Epoch 98, Training Loss: 0.7886181005309609\n",
      "Epoch 99, Training Loss: 0.7887267422676086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 05:58:09,204] Trial 364 finished with value: 0.6379333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7886728297962862\n",
      "Epoch 1, Training Loss: 0.8508060964416055\n",
      "Epoch 2, Training Loss: 0.8177029862123377\n",
      "Epoch 3, Training Loss: 0.8135220477160285\n",
      "Epoch 4, Training Loss: 0.8080249233105603\n",
      "Epoch 5, Training Loss: 0.8073254919753355\n",
      "Epoch 6, Training Loss: 0.8079346849637873\n",
      "Epoch 7, Training Loss: 0.806901681843926\n",
      "Epoch 8, Training Loss: 0.805104486802045\n",
      "Epoch 9, Training Loss: 0.8042378279040842\n",
      "Epoch 10, Training Loss: 0.8038889769946828\n",
      "Epoch 11, Training Loss: 0.8052977197310504\n",
      "Epoch 12, Training Loss: 0.8024833507397595\n",
      "Epoch 13, Training Loss: 0.8054808634870193\n",
      "Epoch 14, Training Loss: 0.803846636800205\n",
      "Epoch 15, Training Loss: 0.8032762218222899\n",
      "Epoch 16, Training Loss: 0.8033851222430959\n",
      "Epoch 17, Training Loss: 0.8010132169723511\n",
      "Epoch 18, Training Loss: 0.801706856769674\n",
      "Epoch 19, Training Loss: 0.8021826349286472\n",
      "Epoch 20, Training Loss: 0.8013211419301874\n",
      "Epoch 21, Training Loss: 0.8033685302734375\n",
      "Epoch 22, Training Loss: 0.8040898783066693\n",
      "Epoch 23, Training Loss: 0.8021233459079967\n",
      "Epoch 24, Training Loss: 0.8016211753733018\n",
      "Epoch 25, Training Loss: 0.8003724074363708\n",
      "Epoch 26, Training Loss: 0.8025439813557793\n",
      "Epoch 27, Training Loss: 0.8021168596604291\n",
      "Epoch 28, Training Loss: 0.8004753948660458\n",
      "Epoch 29, Training Loss: 0.8005466798473807\n",
      "Epoch 30, Training Loss: 0.7995249882165123\n",
      "Epoch 31, Training Loss: 0.8007019972099977\n",
      "Epoch 32, Training Loss: 0.7998000640728894\n",
      "Epoch 33, Training Loss: 0.7993981712705949\n",
      "Epoch 34, Training Loss: 0.7998951045204611\n",
      "Epoch 35, Training Loss: 0.8000854288129245\n",
      "Epoch 36, Training Loss: 0.8016470636339749\n",
      "Epoch 37, Training Loss: 0.800185119264266\n",
      "Epoch 38, Training Loss: 0.7991907498415779\n",
      "Epoch 39, Training Loss: 0.7992778671489043\n",
      "Epoch 40, Training Loss: 0.8001689516095554\n",
      "Epoch 41, Training Loss: 0.8012553991289699\n",
      "Epoch 42, Training Loss: 0.8009467282014735\n",
      "Epoch 43, Training Loss: 0.800428013941821\n",
      "Epoch 44, Training Loss: 0.8003275951918434\n",
      "Epoch 45, Training Loss: 0.8013728283433353\n",
      "Epoch 46, Training Loss: 0.8009988292525796\n",
      "Epoch 47, Training Loss: 0.8008125392128439\n",
      "Epoch 48, Training Loss: 0.7999190326999216\n",
      "Epoch 49, Training Loss: 0.8005926242295434\n",
      "Epoch 50, Training Loss: 0.8004960426863502\n",
      "Epoch 51, Training Loss: 0.8010961984185612\n",
      "Epoch 52, Training Loss: 0.7998538568440605\n",
      "Epoch 53, Training Loss: 0.7997651839957518\n",
      "Epoch 54, Training Loss: 0.7998135939766379\n",
      "Epoch 55, Training Loss: 0.7998110512424917\n",
      "Epoch 56, Training Loss: 0.8000161371511572\n",
      "Epoch 57, Training Loss: 0.798007057203966\n",
      "Epoch 58, Training Loss: 0.7985946970827439\n",
      "Epoch 59, Training Loss: 0.8002110820658067\n",
      "Epoch 60, Training Loss: 0.7996192704228794\n",
      "Epoch 61, Training Loss: 0.7999706693957833\n",
      "Epoch 62, Training Loss: 0.800404641978881\n",
      "Epoch 63, Training Loss: 0.8006345692101647\n",
      "Epoch 64, Training Loss: 0.7988330976402058\n",
      "Epoch 65, Training Loss: 0.799588309245951\n",
      "Epoch 66, Training Loss: 0.7993902502340429\n",
      "Epoch 67, Training Loss: 0.7995241203027613\n",
      "Epoch 68, Training Loss: 0.7998624862642849\n",
      "Epoch 69, Training Loss: 0.7998693808387307\n",
      "Epoch 70, Training Loss: 0.7993907075769761\n",
      "Epoch 71, Training Loss: 0.7992992276303909\n",
      "Epoch 72, Training Loss: 0.7981744956268984\n",
      "Epoch 73, Training Loss: 0.7991938851861393\n",
      "Epoch 74, Training Loss: 0.7997176502732669\n",
      "Epoch 75, Training Loss: 0.8003662096051609\n",
      "Epoch 76, Training Loss: 0.7987741661071778\n",
      "Epoch 77, Training Loss: 0.7995707644434537\n",
      "Epoch 78, Training Loss: 0.7992239783090703\n",
      "Epoch 79, Training Loss: 0.7989269863156712\n",
      "Epoch 80, Training Loss: 0.7995563870317796\n",
      "Epoch 81, Training Loss: 0.7999875016072218\n",
      "Epoch 82, Training Loss: 0.7999688256488127\n",
      "Epoch 83, Training Loss: 0.799034426983665\n",
      "Epoch 84, Training Loss: 0.8002408137742211\n",
      "Epoch 85, Training Loss: 0.7984491890318254\n",
      "Epoch 86, Training Loss: 0.7998343519603505\n",
      "Epoch 87, Training Loss: 0.797948204278946\n",
      "Epoch 88, Training Loss: 0.7994387372802285\n",
      "Epoch 89, Training Loss: 0.7996068965687472\n",
      "Epoch 90, Training Loss: 0.7984224285097683\n",
      "Epoch 91, Training Loss: 0.8006714834886439\n",
      "Epoch 92, Training Loss: 0.7979894161224366\n",
      "Epoch 93, Training Loss: 0.8001609173241784\n",
      "Epoch 94, Training Loss: 0.7983185583703658\n",
      "Epoch 95, Training Loss: 0.7984943895480212\n",
      "Epoch 96, Training Loss: 0.7979538146888508\n",
      "Epoch 97, Training Loss: 0.7976810913226183\n",
      "Epoch 98, Training Loss: 0.7984396079007317\n",
      "Epoch 99, Training Loss: 0.799575944928562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:00:44,587] Trial 365 finished with value: 0.6360666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'relu', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7983639990582185\n",
      "Epoch 1, Training Loss: 0.8523946389030008\n",
      "Epoch 2, Training Loss: 0.8199405581810895\n",
      "Epoch 3, Training Loss: 0.8172160747471978\n",
      "Epoch 4, Training Loss: 0.8135665034546572\n",
      "Epoch 5, Training Loss: 0.8112629976693322\n",
      "Epoch 6, Training Loss: 0.8098621002365561\n",
      "Epoch 7, Training Loss: 0.8083420880401836\n",
      "Epoch 8, Training Loss: 0.8084556409190683\n",
      "Epoch 9, Training Loss: 0.8069796404417824\n",
      "Epoch 10, Training Loss: 0.8061134354507222\n",
      "Epoch 11, Training Loss: 0.8061274557955125\n",
      "Epoch 12, Training Loss: 0.8065663421154022\n",
      "Epoch 13, Training Loss: 0.8056908341716318\n",
      "Epoch 14, Training Loss: 0.8057307645152597\n",
      "Epoch 15, Training Loss: 0.8060283486282124\n",
      "Epoch 16, Training Loss: 0.8062323072377373\n",
      "Epoch 17, Training Loss: 0.805342266629724\n",
      "Epoch 18, Training Loss: 0.8055548832697027\n",
      "Epoch 19, Training Loss: 0.8044415853304021\n",
      "Epoch 20, Training Loss: 0.804097506649354\n",
      "Epoch 21, Training Loss: 0.8050996471152586\n",
      "Epoch 22, Training Loss: 0.804759841596379\n",
      "Epoch 23, Training Loss: 0.8047641050815583\n",
      "Epoch 24, Training Loss: 0.804633500996758\n",
      "Epoch 25, Training Loss: 0.8042528597747578\n",
      "Epoch 26, Training Loss: 0.8045830966444576\n",
      "Epoch 27, Training Loss: 0.8043899073320276\n",
      "Epoch 28, Training Loss: 0.8040024493722354\n",
      "Epoch 29, Training Loss: 0.8039611890736749\n",
      "Epoch 30, Training Loss: 0.8032746944707982\n",
      "Epoch 31, Training Loss: 0.8051540856501636\n",
      "Epoch 32, Training Loss: 0.8035768895289477\n",
      "Epoch 33, Training Loss: 0.8036807324605829\n",
      "Epoch 34, Training Loss: 0.8029557311534882\n",
      "Epoch 35, Training Loss: 0.803288722669377\n",
      "Epoch 36, Training Loss: 0.8028342838147108\n",
      "Epoch 37, Training Loss: 0.8025435423851013\n",
      "Epoch 38, Training Loss: 0.8030177394081565\n",
      "Epoch 39, Training Loss: 0.8025552473348729\n",
      "Epoch 40, Training Loss: 0.8016173413220574\n",
      "Epoch 41, Training Loss: 0.8015549295088824\n",
      "Epoch 42, Training Loss: 0.802302721528446\n",
      "Epoch 43, Training Loss: 0.8018128040257623\n",
      "Epoch 44, Training Loss: 0.8008029103980345\n",
      "Epoch 45, Training Loss: 0.8020996118293089\n",
      "Epoch 46, Training Loss: 0.8008842372193056\n",
      "Epoch 47, Training Loss: 0.8019530673588023\n",
      "Epoch 48, Training Loss: 0.8009383209312664\n",
      "Epoch 49, Training Loss: 0.8013169497602126\n",
      "Epoch 50, Training Loss: 0.7999018476991092\n",
      "Epoch 51, Training Loss: 0.8008938138625201\n",
      "Epoch 52, Training Loss: 0.7999320246191586\n",
      "Epoch 53, Training Loss: 0.8007481704739964\n",
      "Epoch 54, Training Loss: 0.7997987086632673\n",
      "Epoch 55, Training Loss: 0.8008804128450506\n",
      "Epoch 56, Training Loss: 0.7993465286843917\n",
      "Epoch 57, Training Loss: 0.7993540138356826\n",
      "Epoch 58, Training Loss: 0.7983964244758381\n",
      "Epoch 59, Training Loss: 0.7991607069969178\n",
      "Epoch 60, Training Loss: 0.7984525478587431\n",
      "Epoch 61, Training Loss: 0.7992020165219026\n",
      "Epoch 62, Training Loss: 0.7991947843747981\n",
      "Epoch 63, Training Loss: 0.7982315135002136\n",
      "Epoch 64, Training Loss: 0.7980147156294655\n",
      "Epoch 65, Training Loss: 0.7980320140894721\n",
      "Epoch 66, Training Loss: 0.7984354942686418\n",
      "Epoch 67, Training Loss: 0.7972999082593357\n",
      "Epoch 68, Training Loss: 0.7980815361527835\n",
      "Epoch 69, Training Loss: 0.7974873761569753\n",
      "Epoch 70, Training Loss: 0.7971821301824906\n",
      "Epoch 71, Training Loss: 0.7974306151446174\n",
      "Epoch 72, Training Loss: 0.7972578953294193\n",
      "Epoch 73, Training Loss: 0.797189094880048\n",
      "Epoch 74, Training Loss: 0.7974271823378171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:02:12,481] Trial 366 finished with value: 0.6366 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7960817105629865\n",
      "Epoch 1, Training Loss: 0.9828928752053053\n",
      "Epoch 2, Training Loss: 0.9468784147635438\n",
      "Epoch 3, Training Loss: 0.932022744282744\n",
      "Epoch 4, Training Loss: 0.9150491183861754\n",
      "Epoch 5, Training Loss: 0.8965125861920809\n",
      "Epoch 6, Training Loss: 0.8776098953153854\n",
      "Epoch 7, Training Loss: 0.8601193110745652\n",
      "Epoch 8, Training Loss: 0.8459469217106812\n",
      "Epoch 9, Training Loss: 0.8347063826439076\n",
      "Epoch 10, Training Loss: 0.8269748724492869\n",
      "Epoch 11, Training Loss: 0.8215314976254801\n",
      "Epoch 12, Training Loss: 0.8171852171869206\n",
      "Epoch 13, Training Loss: 0.813848294440965\n",
      "Epoch 14, Training Loss: 0.812929253739522\n",
      "Epoch 15, Training Loss: 0.8114598336076377\n",
      "Epoch 16, Training Loss: 0.810382290262925\n",
      "Epoch 17, Training Loss: 0.8094104246089333\n",
      "Epoch 18, Training Loss: 0.808673864468596\n",
      "Epoch 19, Training Loss: 0.8079802885091394\n",
      "Epoch 20, Training Loss: 0.8079019322431177\n",
      "Epoch 21, Training Loss: 0.8080093987005994\n",
      "Epoch 22, Training Loss: 0.8074163527417003\n",
      "Epoch 23, Training Loss: 0.8073552119104486\n",
      "Epoch 24, Training Loss: 0.8071515557461215\n",
      "Epoch 25, Training Loss: 0.8064927951733869\n",
      "Epoch 26, Training Loss: 0.8069123669674522\n",
      "Epoch 27, Training Loss: 0.8069849426585033\n",
      "Epoch 28, Training Loss: 0.8067669222229406\n",
      "Epoch 29, Training Loss: 0.8058075172560556\n",
      "Epoch 30, Training Loss: 0.806265822478703\n",
      "Epoch 31, Training Loss: 0.8053662205997266\n",
      "Epoch 32, Training Loss: 0.8060611249808979\n",
      "Epoch 33, Training Loss: 0.8049982315615604\n",
      "Epoch 34, Training Loss: 0.8057701638766698\n",
      "Epoch 35, Training Loss: 0.8048732123876873\n",
      "Epoch 36, Training Loss: 0.8049240351619577\n",
      "Epoch 37, Training Loss: 0.8046954820030614\n",
      "Epoch 38, Training Loss: 0.8046639008629591\n",
      "Epoch 39, Training Loss: 0.8048995541450673\n",
      "Epoch 40, Training Loss: 0.8046758353262019\n",
      "Epoch 41, Training Loss: 0.8049104919110922\n",
      "Epoch 42, Training Loss: 0.8044442774657916\n",
      "Epoch 43, Training Loss: 0.8046753316893613\n",
      "Epoch 44, Training Loss: 0.8040884854201984\n",
      "Epoch 45, Training Loss: 0.8040479635833797\n",
      "Epoch 46, Training Loss: 0.8041531759993474\n",
      "Epoch 47, Training Loss: 0.8032953335826558\n",
      "Epoch 48, Training Loss: 0.803432844187084\n",
      "Epoch 49, Training Loss: 0.803071263499726\n",
      "Epoch 50, Training Loss: 0.8030747721966048\n",
      "Epoch 51, Training Loss: 0.8035985217058569\n",
      "Epoch 52, Training Loss: 0.8033501529155818\n",
      "Epoch 53, Training Loss: 0.8027773306782084\n",
      "Epoch 54, Training Loss: 0.80250181085185\n",
      "Epoch 55, Training Loss: 0.8024342527963165\n",
      "Epoch 56, Training Loss: 0.8019788738480188\n",
      "Epoch 57, Training Loss: 0.8022536017840967\n",
      "Epoch 58, Training Loss: 0.8018822103514707\n",
      "Epoch 59, Training Loss: 0.8036675762413139\n",
      "Epoch 60, Training Loss: 0.802336375426529\n",
      "Epoch 61, Training Loss: 0.8015546714452872\n",
      "Epoch 62, Training Loss: 0.8016930797942599\n",
      "Epoch 63, Training Loss: 0.8015673811274363\n",
      "Epoch 64, Training Loss: 0.801119944206754\n",
      "Epoch 65, Training Loss: 0.8015327094192791\n",
      "Epoch 66, Training Loss: 0.8012625610021721\n",
      "Epoch 67, Training Loss: 0.8016980030482873\n",
      "Epoch 68, Training Loss: 0.8014169312061223\n",
      "Epoch 69, Training Loss: 0.8013194414906035\n",
      "Epoch 70, Training Loss: 0.8012321468582727\n",
      "Epoch 71, Training Loss: 0.8010107570124748\n",
      "Epoch 72, Training Loss: 0.8008666680271465\n",
      "Epoch 73, Training Loss: 0.8012132339011457\n",
      "Epoch 74, Training Loss: 0.8016132692645367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:03:22,755] Trial 367 finished with value: 0.6339333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8002958317448322\n",
      "Epoch 1, Training Loss: 1.0553771850642035\n",
      "Epoch 2, Training Loss: 1.0103451229544247\n",
      "Epoch 3, Training Loss: 0.9893374841353473\n",
      "Epoch 4, Training Loss: 0.9771673657613642\n",
      "Epoch 5, Training Loss: 0.9696532177223879\n",
      "Epoch 6, Training Loss: 0.9647690366296208\n",
      "Epoch 7, Training Loss: 0.9614130346915302\n",
      "Epoch 8, Training Loss: 0.9589660479040707\n",
      "Epoch 9, Training Loss: 0.9570637598458458\n",
      "Epoch 10, Training Loss: 0.9554840073164772\n",
      "Epoch 11, Training Loss: 0.9541070259318633\n",
      "Epoch 12, Training Loss: 0.9528614244741552\n",
      "Epoch 13, Training Loss: 0.9517014276280122\n",
      "Epoch 14, Training Loss: 0.9505882258976207\n",
      "Epoch 15, Training Loss: 0.9495180230280932\n",
      "Epoch 16, Training Loss: 0.948469468495425\n",
      "Epoch 17, Training Loss: 0.9474467238257913\n",
      "Epoch 18, Training Loss: 0.9464310473554275\n",
      "Epoch 19, Training Loss: 0.9454225647449493\n",
      "Epoch 20, Training Loss: 0.9444290273329791\n",
      "Epoch 21, Training Loss: 0.9434470779755536\n",
      "Epoch 22, Training Loss: 0.9424534215646632\n",
      "Epoch 23, Training Loss: 0.9414833832488341\n",
      "Epoch 24, Training Loss: 0.9405141824133256\n",
      "Epoch 25, Training Loss: 0.9395389328984654\n",
      "Epoch 26, Training Loss: 0.9385683333172518\n",
      "Epoch 27, Training Loss: 0.9375966079094831\n",
      "Epoch 28, Training Loss: 0.9366267446209403\n",
      "Epoch 29, Training Loss: 0.9356654986213235\n",
      "Epoch 30, Training Loss: 0.9346916777246138\n",
      "Epoch 31, Training Loss: 0.933722700581831\n",
      "Epoch 32, Training Loss: 0.9327541094667772\n",
      "Epoch 33, Training Loss: 0.9317767515603234\n",
      "Epoch 34, Training Loss: 0.9308049526635338\n",
      "Epoch 35, Training Loss: 0.9298317548106698\n",
      "Epoch 36, Training Loss: 0.9288472402797026\n",
      "Epoch 37, Training Loss: 0.9278707442564122\n",
      "Epoch 38, Training Loss: 0.9268896992767558\n",
      "Epoch 39, Training Loss: 0.9259002644174239\n",
      "Epoch 40, Training Loss: 0.9249175338184132\n",
      "Epoch 41, Training Loss: 0.923927586779875\n",
      "Epoch 42, Training Loss: 0.9229331826462465\n",
      "Epoch 43, Training Loss: 0.9219332169083988\n",
      "Epoch 44, Training Loss: 0.9209371954553267\n",
      "Epoch 45, Training Loss: 0.9199369317643783\n",
      "Epoch 46, Training Loss: 0.9189302060884588\n",
      "Epoch 47, Training Loss: 0.9179218072049758\n",
      "Epoch 48, Training Loss: 0.9169184389535119\n",
      "Epoch 49, Training Loss: 0.9159057203461142\n",
      "Epoch 50, Training Loss: 0.9148965239524841\n",
      "Epoch 51, Training Loss: 0.9138704478039461\n",
      "Epoch 52, Training Loss: 0.912860059036928\n",
      "Epoch 53, Training Loss: 0.9118417949536267\n",
      "Epoch 54, Training Loss: 0.910826021713369\n",
      "Epoch 55, Training Loss: 0.9098008092010722\n",
      "Epoch 56, Training Loss: 0.9087773647027857\n",
      "Epoch 57, Training Loss: 0.9077532757029814\n",
      "Epoch 58, Training Loss: 0.9067244010813096\n",
      "Epoch 59, Training Loss: 0.9057042307012222\n",
      "Epoch 60, Training Loss: 0.9046770020793466\n",
      "Epoch 61, Training Loss: 0.90364989603267\n",
      "Epoch 62, Training Loss: 0.902622113087598\n",
      "Epoch 63, Training Loss: 0.9015949954004848\n",
      "Epoch 64, Training Loss: 0.9005736351714415\n",
      "Epoch 65, Training Loss: 0.8995454495794633\n",
      "Epoch 66, Training Loss: 0.8985243015429553\n",
      "Epoch 67, Training Loss: 0.8975052142143249\n",
      "Epoch 68, Training Loss: 0.8964806679417106\n",
      "Epoch 69, Training Loss: 0.8954601067655227\n",
      "Epoch 70, Training Loss: 0.8944324036906747\n",
      "Epoch 71, Training Loss: 0.8934243052847245\n",
      "Epoch 72, Training Loss: 0.8924093350242166\n",
      "Epoch 73, Training Loss: 0.8914019554502823\n",
      "Epoch 74, Training Loss: 0.8903855003328884\n",
      "Epoch 75, Training Loss: 0.889376989813412\n",
      "Epoch 76, Training Loss: 0.8883761553203359\n",
      "Epoch 77, Training Loss: 0.8873686721044428\n",
      "Epoch 78, Training Loss: 0.886376941765056\n",
      "Epoch 79, Training Loss: 0.8853756058216095\n",
      "Epoch 80, Training Loss: 0.8843945493417628\n",
      "Epoch 81, Training Loss: 0.8834058253204121\n",
      "Epoch 82, Training Loss: 0.8824229708138634\n",
      "Epoch 83, Training Loss: 0.8814469147429747\n",
      "Epoch 84, Training Loss: 0.8804729522677028\n",
      "Epoch 85, Training Loss: 0.8795113925372853\n",
      "Epoch 86, Training Loss: 0.8785435513187857\n",
      "Epoch 87, Training Loss: 0.8775871806986192\n",
      "Epoch 88, Training Loss: 0.8766339845517103\n",
      "Epoch 89, Training Loss: 0.8756753002896028\n",
      "Epoch 90, Training Loss: 0.8747478675842285\n",
      "Epoch 91, Training Loss: 0.8738049812877879\n",
      "Epoch 92, Training Loss: 0.8728801975530737\n",
      "Epoch 93, Training Loss: 0.8719530638526468\n",
      "Epoch 94, Training Loss: 0.8710361212141373\n",
      "Epoch 95, Training Loss: 0.8701230213922613\n",
      "Epoch 96, Training Loss: 0.8692173043419333\n",
      "Epoch 97, Training Loss: 0.8683199383932001\n",
      "Epoch 98, Training Loss: 0.8674120252973894\n",
      "Epoch 99, Training Loss: 0.8665433965009801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:04:57,512] Trial 368 finished with value: 0.596 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8656747889518738\n",
      "Epoch 1, Training Loss: 0.9959997121726766\n",
      "Epoch 2, Training Loss: 0.9437285156810985\n",
      "Epoch 3, Training Loss: 0.9244000507102293\n",
      "Epoch 4, Training Loss: 0.9007219001124886\n",
      "Epoch 5, Training Loss: 0.8751942028718837\n",
      "Epoch 6, Training Loss: 0.8533448423357571\n",
      "Epoch 7, Training Loss: 0.837476831113591\n",
      "Epoch 8, Training Loss: 0.8273206854567808\n",
      "Epoch 9, Training Loss: 0.8212465594095343\n",
      "Epoch 10, Training Loss: 0.8171620943265803\n",
      "Epoch 11, Training Loss: 0.8146448683037477\n",
      "Epoch 12, Training Loss: 0.8132579299982856\n",
      "Epoch 13, Training Loss: 0.812074482090333\n",
      "Epoch 14, Training Loss: 0.8115005655849681\n",
      "Epoch 15, Training Loss: 0.8108595873327816\n",
      "Epoch 16, Training Loss: 0.8101469835113077\n",
      "Epoch 17, Training Loss: 0.8098288309574128\n",
      "Epoch 18, Training Loss: 0.8095681592296151\n",
      "Epoch 19, Training Loss: 0.8093554276578566\n",
      "Epoch 20, Training Loss: 0.8087910356241114\n",
      "Epoch 21, Training Loss: 0.8086401097914752\n",
      "Epoch 22, Training Loss: 0.8082380578798406\n",
      "Epoch 23, Training Loss: 0.8078870835023768\n",
      "Epoch 24, Training Loss: 0.8078602788027595\n",
      "Epoch 25, Training Loss: 0.8074427168509539\n",
      "Epoch 26, Training Loss: 0.8074136322386125\n",
      "Epoch 27, Training Loss: 0.8070518307826098\n",
      "Epoch 28, Training Loss: 0.8068309199810028\n",
      "Epoch 29, Training Loss: 0.8067662786034977\n",
      "Epoch 30, Training Loss: 0.8066743473445668\n",
      "Epoch 31, Training Loss: 0.8063624759281383\n",
      "Epoch 32, Training Loss: 0.8064026968619403\n",
      "Epoch 33, Training Loss: 0.8059163793395547\n",
      "Epoch 34, Training Loss: 0.8057308833739337\n",
      "Epoch 35, Training Loss: 0.8056759384099175\n",
      "Epoch 36, Training Loss: 0.8054234766960144\n",
      "Epoch 37, Training Loss: 0.8052122445667491\n",
      "Epoch 38, Training Loss: 0.8052648273636313\n",
      "Epoch 39, Training Loss: 0.804945968319388\n",
      "Epoch 40, Training Loss: 0.8047370796343859\n",
      "Epoch 41, Training Loss: 0.8047230810978834\n",
      "Epoch 42, Training Loss: 0.8044289232001585\n",
      "Epoch 43, Training Loss: 0.8045053486964282\n",
      "Epoch 44, Training Loss: 0.803823057272855\n",
      "Epoch 45, Training Loss: 0.8036106265292449\n",
      "Epoch 46, Training Loss: 0.8036702545250163\n",
      "Epoch 47, Training Loss: 0.8038100229992586\n",
      "Epoch 48, Training Loss: 0.803395594849306\n",
      "Epoch 49, Training Loss: 0.8035067571611966\n",
      "Epoch 50, Training Loss: 0.8032296821650337\n",
      "Epoch 51, Training Loss: 0.8030279378330006\n",
      "Epoch 52, Training Loss: 0.8029572919537039\n",
      "Epoch 53, Training Loss: 0.8025582852784325\n",
      "Epoch 54, Training Loss: 0.8028418723975911\n",
      "Epoch 55, Training Loss: 0.8025816453905666\n",
      "Epoch 56, Training Loss: 0.8024044948465684\n",
      "Epoch 57, Training Loss: 0.8021819843965419\n",
      "Epoch 58, Training Loss: 0.8018758529775283\n",
      "Epoch 59, Training Loss: 0.8020103150956771\n",
      "Epoch 60, Training Loss: 0.8018625478884753\n",
      "Epoch 61, Training Loss: 0.801391938644297\n",
      "Epoch 62, Training Loss: 0.8016067309940562\n",
      "Epoch 63, Training Loss: 0.8012301219912137\n",
      "Epoch 64, Training Loss: 0.8011348822537591\n",
      "Epoch 65, Training Loss: 0.8012192102039561\n",
      "Epoch 66, Training Loss: 0.8011146899531869\n",
      "Epoch 67, Training Loss: 0.800777826239081\n",
      "Epoch 68, Training Loss: 0.8009248765777139\n",
      "Epoch 69, Training Loss: 0.8006369303955752\n",
      "Epoch 70, Training Loss: 0.800602632340263\n",
      "Epoch 71, Training Loss: 0.8003426661210902\n",
      "Epoch 72, Training Loss: 0.8003101735255298\n",
      "Epoch 73, Training Loss: 0.8004517540510963\n",
      "Epoch 74, Training Loss: 0.8000979354101069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:06:24,558] Trial 369 finished with value: 0.6352666666666666 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.8000435177718892\n",
      "Epoch 1, Training Loss: 1.0902183358809527\n",
      "Epoch 2, Training Loss: 1.0790137425590964\n",
      "Epoch 3, Training Loss: 1.0692442912213942\n",
      "Epoch 4, Training Loss: 1.060557419131784\n",
      "Epoch 5, Training Loss: 1.0525504897622502\n",
      "Epoch 6, Training Loss: 1.0450728111407337\n",
      "Epoch 7, Training Loss: 1.0380559527873994\n",
      "Epoch 8, Training Loss: 1.0313809446727529\n",
      "Epoch 9, Training Loss: 1.0248849779718063\n",
      "Epoch 10, Training Loss: 1.0184296635319205\n",
      "Epoch 11, Training Loss: 1.0118867426760056\n",
      "Epoch 12, Training Loss: 1.0051531190030716\n",
      "Epoch 13, Training Loss: 0.9981850382159738\n",
      "Epoch 14, Training Loss: 0.9910390895254472\n",
      "Epoch 15, Training Loss: 0.9838270251891192\n",
      "Epoch 16, Training Loss: 0.9767530192347134\n",
      "Epoch 17, Training Loss: 0.9700065818954916\n",
      "Epoch 18, Training Loss: 0.9638002136875602\n",
      "Epoch 19, Training Loss: 0.958258239661946\n",
      "Epoch 20, Training Loss: 0.9534724188552183\n",
      "Epoch 21, Training Loss: 0.9494545892406913\n",
      "Epoch 22, Training Loss: 0.9461520870994119\n",
      "Epoch 23, Training Loss: 0.9434253273290746\n",
      "Epoch 24, Training Loss: 0.9411718432342305\n",
      "Epoch 25, Training Loss: 0.9392582215982325\n",
      "Epoch 26, Training Loss: 0.9376176187571357\n",
      "Epoch 27, Training Loss: 0.936146527037901\n",
      "Epoch 28, Training Loss: 0.9348134991701912\n",
      "Epoch 29, Training Loss: 0.933608194659738\n",
      "Epoch 30, Training Loss: 0.932507863184985\n",
      "Epoch 31, Training Loss: 0.9314617607172798\n",
      "Epoch 32, Training Loss: 0.9304916431623347\n",
      "Epoch 33, Training Loss: 0.929556431349586\n",
      "Epoch 34, Training Loss: 0.928662812499439\n",
      "Epoch 35, Training Loss: 0.9277835154533386\n",
      "Epoch 36, Training Loss: 0.9269025643432841\n",
      "Epoch 37, Training Loss: 0.926070857889512\n",
      "Epoch 38, Training Loss: 0.9252441954612732\n",
      "Epoch 39, Training Loss: 0.9244301160644083\n",
      "Epoch 40, Training Loss: 0.9236271295126747\n",
      "Epoch 41, Training Loss: 0.9228288210139555\n",
      "Epoch 42, Training Loss: 0.9220426581186407\n",
      "Epoch 43, Training Loss: 0.9212729898621054\n",
      "Epoch 44, Training Loss: 0.9204974605756647\n",
      "Epoch 45, Training Loss: 0.9197463187049417\n",
      "Epoch 46, Training Loss: 0.9189762779544381\n",
      "Epoch 47, Training Loss: 0.918242404460907\n",
      "Epoch 48, Training Loss: 0.9174844248855816\n",
      "Epoch 49, Training Loss: 0.9167505140164319\n",
      "Epoch 50, Training Loss: 0.9160096153792213\n",
      "Epoch 51, Training Loss: 0.9152666362594156\n",
      "Epoch 52, Training Loss: 0.914540924464955\n",
      "Epoch 53, Training Loss: 0.9137975044811473\n",
      "Epoch 54, Training Loss: 0.9130653893947601\n",
      "Epoch 55, Training Loss: 0.9123279417262358\n",
      "Epoch 56, Training Loss: 0.9115841392909779\n",
      "Epoch 57, Training Loss: 0.9108386434527005\n",
      "Epoch 58, Training Loss: 0.9100729017397937\n",
      "Epoch 59, Training Loss: 0.9093055451617521\n",
      "Epoch 60, Training Loss: 0.9085412728085237\n",
      "Epoch 61, Training Loss: 0.907759931157617\n",
      "Epoch 62, Training Loss: 0.906967563418781\n",
      "Epoch 63, Training Loss: 0.9061676522563485\n",
      "Epoch 64, Training Loss: 0.9053457660534803\n",
      "Epoch 65, Training Loss: 0.9045165019175586\n",
      "Epoch 66, Training Loss: 0.9036750942118028\n",
      "Epoch 67, Training Loss: 0.9028061530169319\n",
      "Epoch 68, Training Loss: 0.9019367490796482\n",
      "Epoch 69, Training Loss: 0.9010292997780969\n",
      "Epoch 70, Training Loss: 0.9000737714066225\n",
      "Epoch 71, Training Loss: 0.8991811417130863\n",
      "Epoch 72, Training Loss: 0.89819915154401\n",
      "Epoch 73, Training Loss: 0.8972229869225445\n",
      "Epoch 74, Training Loss: 0.896212343047647\n",
      "Epoch 75, Training Loss: 0.8951449479075039\n",
      "Epoch 76, Training Loss: 0.894085488950505\n",
      "Epoch 77, Training Loss: 0.8929477432896109\n",
      "Epoch 78, Training Loss: 0.8918350143993602\n",
      "Epoch 79, Training Loss: 0.8906181608929353\n",
      "Epoch 80, Training Loss: 0.8894054226314321\n",
      "Epoch 81, Training Loss: 0.8881189734094284\n",
      "Epoch 82, Training Loss: 0.8867815695089453\n",
      "Epoch 83, Training Loss: 0.8854318172090194\n",
      "Epoch 84, Training Loss: 0.8840113721174352\n",
      "Epoch 85, Training Loss: 0.8825527746537153\n",
      "Epoch 86, Training Loss: 0.8810386878602645\n",
      "Epoch 87, Training Loss: 0.8794778475340674\n",
      "Epoch 88, Training Loss: 0.8778750235192916\n",
      "Epoch 89, Training Loss: 0.8762199220937841\n",
      "Epoch 90, Training Loss: 0.8745349263443666\n",
      "Epoch 91, Training Loss: 0.8728161214379704\n",
      "Epoch 92, Training Loss: 0.8710722668030683\n",
      "Epoch 93, Training Loss: 0.8692945415833417\n",
      "Epoch 94, Training Loss: 0.8674908921999089\n",
      "Epoch 95, Training Loss: 0.8657026168178109\n",
      "Epoch 96, Training Loss: 0.8638412824097802\n",
      "Epoch 97, Training Loss: 0.862040482969845\n",
      "Epoch 98, Training Loss: 0.8601915602123036\n",
      "Epoch 99, Training Loss: 0.8583975283538594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:08:07,315] Trial 370 finished with value: 0.599 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8565648585207322\n",
      "Epoch 1, Training Loss: 0.8839319277510923\n",
      "Epoch 2, Training Loss: 0.8320661809163935\n",
      "Epoch 3, Training Loss: 0.8247665968362022\n",
      "Epoch 4, Training Loss: 0.8223303076800178\n",
      "Epoch 5, Training Loss: 0.8192238153429592\n",
      "Epoch 6, Training Loss: 0.8178762137889862\n",
      "Epoch 7, Training Loss: 0.8168011550342336\n",
      "Epoch 8, Training Loss: 0.8155449947890113\n",
      "Epoch 9, Training Loss: 0.8154799273434807\n",
      "Epoch 10, Training Loss: 0.8140724980831147\n",
      "Epoch 11, Training Loss: 0.8138815237494076\n",
      "Epoch 12, Training Loss: 0.8150025176300723\n",
      "Epoch 13, Training Loss: 0.8146166272023145\n",
      "Epoch 14, Training Loss: 0.8139930354847628\n",
      "Epoch 15, Training Loss: 0.8141744748985066\n",
      "Epoch 16, Training Loss: 0.8138137680642745\n",
      "Epoch 17, Training Loss: 0.815384587540346\n",
      "Epoch 18, Training Loss: 0.8151110358097974\n",
      "Epoch 19, Training Loss: 0.8152213049636168\n",
      "Epoch 20, Training Loss: 0.8139025927992428\n",
      "Epoch 21, Training Loss: 0.8144851762406966\n",
      "Epoch 22, Training Loss: 0.8151188993453979\n",
      "Epoch 23, Training Loss: 0.8143383683176602\n",
      "Epoch 24, Training Loss: 0.81354960974525\n",
      "Epoch 25, Training Loss: 0.8146229670328252\n",
      "Epoch 26, Training Loss: 0.8136193893236272\n",
      "Epoch 27, Training Loss: 0.8155150517295389\n",
      "Epoch 28, Training Loss: 0.8136527867878185\n",
      "Epoch 29, Training Loss: 0.8134263322633856\n",
      "Epoch 30, Training Loss: 0.8124186465319465\n",
      "Epoch 31, Training Loss: 0.8150186242776759\n",
      "Epoch 32, Training Loss: 0.8141910724780139\n",
      "Epoch 33, Training Loss: 0.814858834883746\n",
      "Epoch 34, Training Loss: 0.8135850410601672\n",
      "Epoch 35, Training Loss: 0.8132269996755264\n",
      "Epoch 36, Training Loss: 0.8130279146222508\n",
      "Epoch 37, Training Loss: 0.8142646262224983\n",
      "Epoch 38, Training Loss: 0.8149045968055725\n",
      "Epoch 39, Training Loss: 0.8129687827474931\n",
      "Epoch 40, Training Loss: 0.8140636857116924\n",
      "Epoch 41, Training Loss: 0.8133526098026949\n",
      "Epoch 42, Training Loss: 0.8150446034880245\n",
      "Epoch 43, Training Loss: 0.8133945595516878\n",
      "Epoch 44, Training Loss: 0.815406371495303\n",
      "Epoch 45, Training Loss: 0.8155746412277222\n",
      "Epoch 46, Training Loss: 0.8151420471247505\n",
      "Epoch 47, Training Loss: 0.816246197013294\n",
      "Epoch 48, Training Loss: 0.8158748581128962\n",
      "Epoch 49, Training Loss: 0.813985210236381\n",
      "Epoch 50, Training Loss: 0.8137584372127757\n",
      "Epoch 51, Training Loss: 0.8146405209513271\n",
      "Epoch 52, Training Loss: 0.8152517941418816\n",
      "Epoch 53, Training Loss: 0.814549764955745\n",
      "Epoch 54, Training Loss: 0.8147459704034469\n",
      "Epoch 55, Training Loss: 0.8140112026999978\n",
      "Epoch 56, Training Loss: 0.814130348107394\n",
      "Epoch 57, Training Loss: 0.8152040734711815\n",
      "Epoch 58, Training Loss: 0.8135906704033122\n",
      "Epoch 59, Training Loss: 0.8138802884606754\n",
      "Epoch 60, Training Loss: 0.8169524327446432\n",
      "Epoch 61, Training Loss: 0.8156625885121963\n",
      "Epoch 62, Training Loss: 0.8156337156015284\n",
      "Epoch 63, Training Loss: 0.8147989972899942\n",
      "Epoch 64, Training Loss: 0.8147020731954013\n",
      "Epoch 65, Training Loss: 0.8146825276402866\n",
      "Epoch 66, Training Loss: 0.8166481117641224\n",
      "Epoch 67, Training Loss: 0.8150963594632991\n",
      "Epoch 68, Training Loss: 0.8140573942661286\n",
      "Epoch 69, Training Loss: 0.8157411396503449\n",
      "Epoch 70, Training Loss: 0.8133543651244219\n",
      "Epoch 71, Training Loss: 0.8137460646208595\n",
      "Epoch 72, Training Loss: 0.8153826942864586\n",
      "Epoch 73, Training Loss: 0.814547811606351\n",
      "Epoch 74, Training Loss: 0.8163738947054919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:09:51,710] Trial 371 finished with value: 0.6238666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 75, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.817095086294062\n",
      "Epoch 1, Training Loss: 0.8927135152924329\n",
      "Epoch 2, Training Loss: 0.8142685456383497\n",
      "Epoch 3, Training Loss: 0.8099312757190905\n",
      "Epoch 4, Training Loss: 0.8090655161025829\n",
      "Epoch 5, Training Loss: 0.8058036940438407\n",
      "Epoch 6, Training Loss: 0.805227552141462\n",
      "Epoch 7, Training Loss: 0.8036256466593061\n",
      "Epoch 8, Training Loss: 0.8026308644983106\n",
      "Epoch 9, Training Loss: 0.801749268868812\n",
      "Epoch 10, Training Loss: 0.8008067346156988\n",
      "Epoch 11, Training Loss: 0.8009913786909634\n",
      "Epoch 12, Training Loss: 0.8001679580910761\n",
      "Epoch 13, Training Loss: 0.7997909865881268\n",
      "Epoch 14, Training Loss: 0.8001389167362586\n",
      "Epoch 15, Training Loss: 0.7990702167489475\n",
      "Epoch 16, Training Loss: 0.7982771501505286\n",
      "Epoch 17, Training Loss: 0.7996770920610069\n",
      "Epoch 18, Training Loss: 0.7981548715354805\n",
      "Epoch 19, Training Loss: 0.7974312539387466\n",
      "Epoch 20, Training Loss: 0.7974741039419533\n",
      "Epoch 21, Training Loss: 0.7969955313474613\n",
      "Epoch 22, Training Loss: 0.797062367335298\n",
      "Epoch 23, Training Loss: 0.7963543182925175\n",
      "Epoch 24, Training Loss: 0.7951759797289856\n",
      "Epoch 25, Training Loss: 0.7949389890620583\n",
      "Epoch 26, Training Loss: 0.79501200233187\n",
      "Epoch 27, Training Loss: 0.7937536495072501\n",
      "Epoch 28, Training Loss: 0.7937223695274582\n",
      "Epoch 29, Training Loss: 0.792506195996937\n",
      "Epoch 30, Training Loss: 0.7925536652256672\n",
      "Epoch 31, Training Loss: 0.7914829886049256\n",
      "Epoch 32, Training Loss: 0.791001037637094\n",
      "Epoch 33, Training Loss: 0.7895438066998819\n",
      "Epoch 34, Training Loss: 0.7894351269965781\n",
      "Epoch 35, Training Loss: 0.7893145416015969\n",
      "Epoch 36, Training Loss: 0.7888210345031623\n",
      "Epoch 37, Training Loss: 0.7881810209804908\n",
      "Epoch 38, Training Loss: 0.7880316002028329\n",
      "Epoch 39, Training Loss: 0.7876289717236856\n",
      "Epoch 40, Training Loss: 0.7866999017564874\n",
      "Epoch 41, Training Loss: 0.7871363335085991\n",
      "Epoch 42, Training Loss: 0.7868386766068021\n",
      "Epoch 43, Training Loss: 0.7862724771176962\n",
      "Epoch 44, Training Loss: 0.7858570617840702\n",
      "Epoch 45, Training Loss: 0.7853268493387036\n",
      "Epoch 46, Training Loss: 0.7853622178386028\n",
      "Epoch 47, Training Loss: 0.7849452594169101\n",
      "Epoch 48, Training Loss: 0.7844719633123929\n",
      "Epoch 49, Training Loss: 0.7849033909632748\n",
      "Epoch 50, Training Loss: 0.7843570944958164\n",
      "Epoch 51, Training Loss: 0.7850756727663198\n",
      "Epoch 52, Training Loss: 0.7844619196160395\n",
      "Epoch 53, Training Loss: 0.7846113684482144\n",
      "Epoch 54, Training Loss: 0.7844923125173813\n",
      "Epoch 55, Training Loss: 0.783464662428189\n",
      "Epoch 56, Training Loss: 0.7826639794765559\n",
      "Epoch 57, Training Loss: 0.7838834952591057\n",
      "Epoch 58, Training Loss: 0.7837192438598862\n",
      "Epoch 59, Training Loss: 0.7831364128822671\n",
      "Epoch 60, Training Loss: 0.7835004205990554\n",
      "Epoch 61, Training Loss: 0.7826003238670808\n",
      "Epoch 62, Training Loss: 0.7829397732153871\n",
      "Epoch 63, Training Loss: 0.7836630479733747\n",
      "Epoch 64, Training Loss: 0.7831550962046573\n",
      "Epoch 65, Training Loss: 0.7828787100046201\n",
      "Epoch 66, Training Loss: 0.7827929770139823\n",
      "Epoch 67, Training Loss: 0.7829788290468374\n",
      "Epoch 68, Training Loss: 0.7827391914855268\n",
      "Epoch 69, Training Loss: 0.782289241131087\n",
      "Epoch 70, Training Loss: 0.7829543398735218\n",
      "Epoch 71, Training Loss: 0.7823105958171357\n",
      "Epoch 72, Training Loss: 0.7823225274121851\n",
      "Epoch 73, Training Loss: 0.7822090590806832\n",
      "Epoch 74, Training Loss: 0.7831910156665888\n",
      "Epoch 75, Training Loss: 0.7816889929592161\n",
      "Epoch 76, Training Loss: 0.782679629953284\n",
      "Epoch 77, Training Loss: 0.7819948126498918\n",
      "Epoch 78, Training Loss: 0.7822985201849973\n",
      "Epoch 79, Training Loss: 0.7815378836223057\n",
      "Epoch 80, Training Loss: 0.7812898056847709\n",
      "Epoch 81, Training Loss: 0.7826018564683154\n",
      "Epoch 82, Training Loss: 0.7820069392820946\n",
      "Epoch 83, Training Loss: 0.7817531730895652\n",
      "Epoch 84, Training Loss: 0.7813410976775607\n",
      "Epoch 85, Training Loss: 0.7818876454704686\n",
      "Epoch 86, Training Loss: 0.7819054508567753\n",
      "Epoch 87, Training Loss: 0.7813381128741387\n",
      "Epoch 88, Training Loss: 0.7813431664517051\n",
      "Epoch 89, Training Loss: 0.7814993046280136\n",
      "Epoch 90, Training Loss: 0.7809804201126098\n",
      "Epoch 91, Training Loss: 0.7819359820588191\n",
      "Epoch 92, Training Loss: 0.7814715836281166\n",
      "Epoch 93, Training Loss: 0.781757648815786\n",
      "Epoch 94, Training Loss: 0.7809852212891543\n",
      "Epoch 95, Training Loss: 0.781450371007274\n",
      "Epoch 96, Training Loss: 0.7807341918909461\n",
      "Epoch 97, Training Loss: 0.7813831841138968\n",
      "Epoch 98, Training Loss: 0.7810829262088116\n",
      "Epoch 99, Training Loss: 0.7808846051531627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:11:50,414] Trial 372 finished with value: 0.6406666666666667 and parameters: {'hidden_layers': 2, 'act_functn': 'tanh', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7809128930694178\n",
      "Epoch 1, Training Loss: 0.9861118405385125\n",
      "Epoch 2, Training Loss: 0.9308614204700728\n",
      "Epoch 3, Training Loss: 0.8902052772672553\n",
      "Epoch 4, Training Loss: 0.8428061618840784\n",
      "Epoch 5, Training Loss: 0.8202044185839201\n",
      "Epoch 6, Training Loss: 0.8150588040961358\n",
      "Epoch 7, Training Loss: 0.8119291788653323\n",
      "Epoch 8, Training Loss: 0.8110740971744509\n",
      "Epoch 9, Training Loss: 0.809299609894143\n",
      "Epoch 10, Training Loss: 0.8087578819210368\n",
      "Epoch 11, Training Loss: 0.8087886749353624\n",
      "Epoch 12, Training Loss: 0.8073099906283213\n",
      "Epoch 13, Training Loss: 0.8069484953593491\n",
      "Epoch 14, Training Loss: 0.8053760126121062\n",
      "Epoch 15, Training Loss: 0.8049312955006621\n",
      "Epoch 16, Training Loss: 0.8051646386770378\n",
      "Epoch 17, Training Loss: 0.8039821444597459\n",
      "Epoch 18, Training Loss: 0.8040457195805427\n",
      "Epoch 19, Training Loss: 0.8033968526617925\n",
      "Epoch 20, Training Loss: 0.8024667491590171\n",
      "Epoch 21, Training Loss: 0.8028746887257224\n",
      "Epoch 22, Training Loss: 0.8027739101782777\n",
      "Epoch 23, Training Loss: 0.8013065204136354\n",
      "Epoch 24, Training Loss: 0.8016945530597429\n",
      "Epoch 25, Training Loss: 0.8024643135250062\n",
      "Epoch 26, Training Loss: 0.802206375993284\n",
      "Epoch 27, Training Loss: 0.801822639049444\n",
      "Epoch 28, Training Loss: 0.8014826083541813\n",
      "Epoch 29, Training Loss: 0.8009585803612731\n",
      "Epoch 30, Training Loss: 0.8010848129602303\n",
      "Epoch 31, Training Loss: 0.8005628341122677\n",
      "Epoch 32, Training Loss: 0.8006435721440422\n",
      "Epoch 33, Training Loss: 0.8011023098364809\n",
      "Epoch 34, Training Loss: 0.8003005379124691\n",
      "Epoch 35, Training Loss: 0.8006248751081022\n",
      "Epoch 36, Training Loss: 0.8006177376983757\n",
      "Epoch 37, Training Loss: 0.8004707576636981\n",
      "Epoch 38, Training Loss: 0.7999947785434867\n",
      "Epoch 39, Training Loss: 0.7993357605503913\n",
      "Epoch 40, Training Loss: 0.7991290184788238\n",
      "Epoch 41, Training Loss: 0.7999976217298579\n",
      "Epoch 42, Training Loss: 0.7992698826287922\n",
      "Epoch 43, Training Loss: 0.7991024904681328\n",
      "Epoch 44, Training Loss: 0.7992065033518282\n",
      "Epoch 45, Training Loss: 0.7990424869652081\n",
      "Epoch 46, Training Loss: 0.7985211361619763\n",
      "Epoch 47, Training Loss: 0.7984573918177669\n",
      "Epoch 48, Training Loss: 0.7984916425289068\n",
      "Epoch 49, Training Loss: 0.7988379154886518\n",
      "Epoch 50, Training Loss: 0.7986093641223764\n",
      "Epoch 51, Training Loss: 0.7990455337933131\n",
      "Epoch 52, Training Loss: 0.7981588959693908\n",
      "Epoch 53, Training Loss: 0.7979921567708926\n",
      "Epoch 54, Training Loss: 0.7984958770579862\n",
      "Epoch 55, Training Loss: 0.7978735953345335\n",
      "Epoch 56, Training Loss: 0.7974377054917184\n",
      "Epoch 57, Training Loss: 0.7983910223595182\n",
      "Epoch 58, Training Loss: 0.7976038500778657\n",
      "Epoch 59, Training Loss: 0.7970257563698561\n",
      "Epoch 60, Training Loss: 0.7984177104512552\n",
      "Epoch 61, Training Loss: 0.7969233584583254\n",
      "Epoch 62, Training Loss: 0.798145604223237\n",
      "Epoch 63, Training Loss: 0.7973376252597436\n",
      "Epoch 64, Training Loss: 0.7965095138639435\n",
      "Epoch 65, Training Loss: 0.7972486734390258\n",
      "Epoch 66, Training Loss: 0.7970728860761886\n",
      "Epoch 67, Training Loss: 0.7970512691297029\n",
      "Epoch 68, Training Loss: 0.7971098573584305\n",
      "Epoch 69, Training Loss: 0.7967943865105622\n",
      "Epoch 70, Training Loss: 0.7970148628815672\n",
      "Epoch 71, Training Loss: 0.7963842698505946\n",
      "Epoch 72, Training Loss: 0.7962057440800774\n",
      "Epoch 73, Training Loss: 0.796478990146092\n",
      "Epoch 74, Training Loss: 0.7961622550075216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:13:20,305] Trial 373 finished with value: 0.6323333333333333 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7959652737567299\n",
      "Epoch 1, Training Loss: 0.8520351590829737\n",
      "Epoch 2, Training Loss: 0.816523624237846\n",
      "Epoch 3, Training Loss: 0.813872854499256\n",
      "Epoch 4, Training Loss: 0.8103462926780476\n",
      "Epoch 5, Training Loss: 0.8071770837026484\n",
      "Epoch 6, Training Loss: 0.8056636821522433\n",
      "Epoch 7, Training Loss: 0.8042732882499695\n",
      "Epoch 8, Training Loss: 0.8034524092253517\n",
      "Epoch 9, Training Loss: 0.8024732192123637\n",
      "Epoch 10, Training Loss: 0.8024057459130006\n",
      "Epoch 11, Training Loss: 0.8015863268515643\n",
      "Epoch 12, Training Loss: 0.8015195768720963\n",
      "Epoch 13, Training Loss: 0.8006831788315493\n",
      "Epoch 14, Training Loss: 0.799278105988222\n",
      "Epoch 15, Training Loss: 0.7996175064058865\n",
      "Epoch 16, Training Loss: 0.7988741103340598\n",
      "Epoch 17, Training Loss: 0.798729652446859\n",
      "Epoch 18, Training Loss: 0.7983390674170325\n",
      "Epoch 19, Training Loss: 0.7987856029762941\n",
      "Epoch 20, Training Loss: 0.7984232510538662\n",
      "Epoch 21, Training Loss: 0.798263550155303\n",
      "Epoch 22, Training Loss: 0.7972335034959457\n",
      "Epoch 23, Training Loss: 0.7972555023081163\n",
      "Epoch 24, Training Loss: 0.7975370515795315\n",
      "Epoch 25, Training Loss: 0.7969688731782577\n",
      "Epoch 26, Training Loss: 0.7971424166595235\n",
      "Epoch 27, Training Loss: 0.7972359521949992\n",
      "Epoch 28, Training Loss: 0.7966333250438465\n",
      "Epoch 29, Training Loss: 0.7964837760785046\n",
      "Epoch 30, Training Loss: 0.797053714499754\n",
      "Epoch 31, Training Loss: 0.7964026686724495\n",
      "Epoch 32, Training Loss: 0.796280582932865\n",
      "Epoch 33, Training Loss: 0.7961793152724995\n",
      "Epoch 34, Training Loss: 0.7963390085977666\n",
      "Epoch 35, Training Loss: 0.7955351058174582\n",
      "Epoch 36, Training Loss: 0.7963595535474665\n",
      "Epoch 37, Training Loss: 0.7959470600941602\n",
      "Epoch 38, Training Loss: 0.7955716335773468\n",
      "Epoch 39, Training Loss: 0.795940952721764\n",
      "Epoch 40, Training Loss: 0.7957374323115629\n",
      "Epoch 41, Training Loss: 0.795222602521672\n",
      "Epoch 42, Training Loss: 0.7952901134070228\n",
      "Epoch 43, Training Loss: 0.7954307308617761\n",
      "Epoch 44, Training Loss: 0.7950335492106045\n",
      "Epoch 45, Training Loss: 0.7947184461004594\n",
      "Epoch 46, Training Loss: 0.7948473016654743\n",
      "Epoch 47, Training Loss: 0.7942779067684622\n",
      "Epoch 48, Training Loss: 0.7942902692626505\n",
      "Epoch 49, Training Loss: 0.7946092520741855\n",
      "Epoch 50, Training Loss: 0.7944273167497972\n",
      "Epoch 51, Training Loss: 0.7944899292553173\n",
      "Epoch 52, Training Loss: 0.7947196821605458\n",
      "Epoch 53, Training Loss: 0.7939644903996411\n",
      "Epoch 54, Training Loss: 0.7941266822814942\n",
      "Epoch 55, Training Loss: 0.7944271091853871\n",
      "Epoch 56, Training Loss: 0.7937448228106779\n",
      "Epoch 57, Training Loss: 0.7942437819873586\n",
      "Epoch 58, Training Loss: 0.7943551038994509\n",
      "Epoch 59, Training Loss: 0.7939707899794859\n",
      "Epoch 60, Training Loss: 0.7938123663032756\n",
      "Epoch 61, Training Loss: 0.7940507961721981\n",
      "Epoch 62, Training Loss: 0.7943053763754228\n",
      "Epoch 63, Training Loss: 0.7939559915486504\n",
      "Epoch 64, Training Loss: 0.7930512489290799\n",
      "Epoch 65, Training Loss: 0.7937088706914116\n",
      "Epoch 66, Training Loss: 0.7936794393202837\n",
      "Epoch 67, Training Loss: 0.7936575780195348\n",
      "Epoch 68, Training Loss: 0.7936652370060191\n",
      "Epoch 69, Training Loss: 0.7937021716903238\n",
      "Epoch 70, Training Loss: 0.7938715788196115\n",
      "Epoch 71, Training Loss: 0.7936729648533989\n",
      "Epoch 72, Training Loss: 0.7933497432400198\n",
      "Epoch 73, Training Loss: 0.7936500885907342\n",
      "Epoch 74, Training Loss: 0.793332170458401\n",
      "Epoch 75, Training Loss: 0.7930036863859962\n",
      "Epoch 76, Training Loss: 0.7937749434919918\n",
      "Epoch 77, Training Loss: 0.7934422929847942\n",
      "Epoch 78, Training Loss: 0.7935045687591329\n",
      "Epoch 79, Training Loss: 0.7932533085346222\n",
      "Epoch 80, Training Loss: 0.7932101049843956\n",
      "Epoch 81, Training Loss: 0.7929701246934778\n",
      "Epoch 82, Training Loss: 0.7931477205192341\n",
      "Epoch 83, Training Loss: 0.7935904664151808\n",
      "Epoch 84, Training Loss: 0.7927886997250949\n",
      "Epoch 85, Training Loss: 0.7929168393331415\n",
      "Epoch 86, Training Loss: 0.7934485042095184\n",
      "Epoch 87, Training Loss: 0.7929090369448942\n",
      "Epoch 88, Training Loss: 0.7931110189241521\n",
      "Epoch 89, Training Loss: 0.7930678633381338\n",
      "Epoch 90, Training Loss: 0.7932434957868912\n",
      "Epoch 91, Training Loss: 0.7925560356588924\n",
      "Epoch 92, Training Loss: 0.7927760194329655\n",
      "Epoch 93, Training Loss: 0.7933677327632904\n",
      "Epoch 94, Training Loss: 0.7926992082595825\n",
      "Epoch 95, Training Loss: 0.792550939391641\n",
      "Epoch 96, Training Loss: 0.7928000888403725\n",
      "Epoch 97, Training Loss: 0.7935018973490772\n",
      "Epoch 98, Training Loss: 0.792450802746941\n",
      "Epoch 99, Training Loss: 0.7931383137141957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:15:11,289] Trial 374 finished with value: 0.6416 and parameters: {'hidden_layers': 1, 'act_functn': 'relu', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.792631096980151\n",
      "Epoch 1, Training Loss: 0.8620163356556612\n",
      "Epoch 2, Training Loss: 0.8269355703101439\n",
      "Epoch 3, Training Loss: 0.8217316469725441\n",
      "Epoch 4, Training Loss: 0.8187203136612388\n",
      "Epoch 5, Training Loss: 0.816013987344854\n",
      "Epoch 6, Training Loss: 0.813793025227154\n",
      "Epoch 7, Training Loss: 0.8127890587554258\n",
      "Epoch 8, Training Loss: 0.8114859963865841\n",
      "Epoch 9, Training Loss: 0.8105239809961879\n",
      "Epoch 10, Training Loss: 0.8092415856613833\n",
      "Epoch 11, Training Loss: 0.8092195164456087\n",
      "Epoch 12, Training Loss: 0.809569686721353\n",
      "Epoch 13, Training Loss: 0.8082012480847975\n",
      "Epoch 14, Training Loss: 0.808400952535517\n",
      "Epoch 15, Training Loss: 0.8071883134982165\n",
      "Epoch 16, Training Loss: 0.8079606798817129\n",
      "Epoch 17, Training Loss: 0.8072422759673175\n",
      "Epoch 18, Training Loss: 0.8073138261542601\n",
      "Epoch 19, Training Loss: 0.8065261314195745\n",
      "Epoch 20, Training Loss: 0.8070248854160309\n",
      "Epoch 21, Training Loss: 0.8062238768269034\n",
      "Epoch 22, Training Loss: 0.8065237250748802\n",
      "Epoch 23, Training Loss: 0.8062837393844828\n",
      "Epoch 24, Training Loss: 0.8061397812646978\n",
      "Epoch 25, Training Loss: 0.8058797369985019\n",
      "Epoch 26, Training Loss: 0.8058907928887535\n",
      "Epoch 27, Training Loss: 0.8053565343688516\n",
      "Epoch 28, Training Loss: 0.8056347730580499\n",
      "Epoch 29, Training Loss: 0.8051652598381043\n",
      "Epoch 30, Training Loss: 0.8043058326665093\n",
      "Epoch 31, Training Loss: 0.8050921812478233\n",
      "Epoch 32, Training Loss: 0.8050108431367313\n",
      "Epoch 33, Training Loss: 0.8047471322732813\n",
      "Epoch 34, Training Loss: 0.8049378824234009\n",
      "Epoch 35, Training Loss: 0.8046437371478361\n",
      "Epoch 36, Training Loss: 0.8046178631221547\n",
      "Epoch 37, Training Loss: 0.8046570230932797\n",
      "Epoch 38, Training Loss: 0.804153963397531\n",
      "Epoch 39, Training Loss: 0.8039162100062651\n",
      "Epoch 40, Training Loss: 0.8037856284309836\n",
      "Epoch 41, Training Loss: 0.8035977048733655\n",
      "Epoch 42, Training Loss: 0.8039303069956163\n",
      "Epoch 43, Training Loss: 0.8037028929065255\n",
      "Epoch 44, Training Loss: 0.8036281997316024\n",
      "Epoch 45, Training Loss: 0.8029886745004093\n",
      "Epoch 46, Training Loss: 0.8034200304396012\n",
      "Epoch 47, Training Loss: 0.8036130861675038\n",
      "Epoch 48, Training Loss: 0.8028455443943248\n",
      "Epoch 49, Training Loss: 0.8028407634707058\n",
      "Epoch 50, Training Loss: 0.8026137962762048\n",
      "Epoch 51, Training Loss: 0.8023119510622585\n",
      "Epoch 52, Training Loss: 0.8023736020396738\n",
      "Epoch 53, Training Loss: 0.8020953539539786\n",
      "Epoch 54, Training Loss: 0.8022867289010216\n",
      "Epoch 55, Training Loss: 0.8014773031543283\n",
      "Epoch 56, Training Loss: 0.8020900039813098\n",
      "Epoch 57, Training Loss: 0.8015891460110159\n",
      "Epoch 58, Training Loss: 0.8005345864856944\n",
      "Epoch 59, Training Loss: 0.8005811822414398\n",
      "Epoch 60, Training Loss: 0.8002988476612989\n",
      "Epoch 61, Training Loss: 0.8001502002688016\n",
      "Epoch 62, Training Loss: 0.7998325571593117\n",
      "Epoch 63, Training Loss: 0.7997975995260126\n",
      "Epoch 64, Training Loss: 0.7991404900130104\n",
      "Epoch 65, Training Loss: 0.7987099333370433\n",
      "Epoch 66, Training Loss: 0.7985807435652789\n",
      "Epoch 67, Training Loss: 0.798303854675854\n",
      "Epoch 68, Training Loss: 0.7982085391353159\n",
      "Epoch 69, Training Loss: 0.7979424494855544\n",
      "Epoch 70, Training Loss: 0.7971079616686877\n",
      "Epoch 71, Training Loss: 0.7969390803926131\n",
      "Epoch 72, Training Loss: 0.7964452769475825\n",
      "Epoch 73, Training Loss: 0.7962894340122447\n",
      "Epoch 74, Training Loss: 0.7965527948211221\n",
      "Epoch 75, Training Loss: 0.79643190425985\n",
      "Epoch 76, Training Loss: 0.7962622304523692\n",
      "Epoch 77, Training Loss: 0.7960607849149143\n",
      "Epoch 78, Training Loss: 0.7958087373481078\n",
      "Epoch 79, Training Loss: 0.7960479263698353\n",
      "Epoch 80, Training Loss: 0.7957320679636563\n",
      "Epoch 81, Training Loss: 0.7954496609463412\n",
      "Epoch 82, Training Loss: 0.795724268099841\n",
      "Epoch 83, Training Loss: 0.7951389652140001\n",
      "Epoch 84, Training Loss: 0.7954602500971626\n",
      "Epoch 85, Training Loss: 0.7953166389465331\n",
      "Epoch 86, Training Loss: 0.7956495806048898\n",
      "Epoch 87, Training Loss: 0.7952446441790637\n",
      "Epoch 88, Training Loss: 0.7953804312734043\n",
      "Epoch 89, Training Loss: 0.7953427520920249\n",
      "Epoch 90, Training Loss: 0.7950110296642079\n",
      "Epoch 91, Training Loss: 0.7952100811986362\n",
      "Epoch 92, Training Loss: 0.7950521032950457\n",
      "Epoch 93, Training Loss: 0.7956204330219941\n",
      "Epoch 94, Training Loss: 0.7950768199387719\n",
      "Epoch 95, Training Loss: 0.7944958893691793\n",
      "Epoch 96, Training Loss: 0.7948674673192642\n",
      "Epoch 97, Training Loss: 0.794114497198778\n",
      "Epoch 98, Training Loss: 0.7948170729945687\n",
      "Epoch 99, Training Loss: 0.7942816647361307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:17:01,729] Trial 375 finished with value: 0.6341333333333333 and parameters: {'hidden_layers': 1, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7948999430852778\n",
      "Epoch 1, Training Loss: 0.9676633624469533\n",
      "Epoch 2, Training Loss: 0.881009058040731\n",
      "Epoch 3, Training Loss: 0.8687917973013485\n",
      "Epoch 4, Training Loss: 0.8671806876799639\n",
      "Epoch 5, Training Loss: 0.8648933146280401\n",
      "Epoch 6, Training Loss: 0.8558507583421819\n",
      "Epoch 7, Training Loss: 0.8608726676071391\n",
      "Epoch 8, Training Loss: 0.8613008158347186\n",
      "Epoch 9, Training Loss: 0.8604791324980119\n",
      "Epoch 10, Training Loss: 0.8582232288753285\n",
      "Epoch 11, Training Loss: 0.8562538578229792\n",
      "Epoch 12, Training Loss: 0.8564873026399051\n",
      "Epoch 13, Training Loss: 0.8563137705185834\n",
      "Epoch 14, Training Loss: 0.8579409334238838\n",
      "Epoch 15, Training Loss: 0.8576978895243477\n",
      "Epoch 16, Training Loss: 0.8579887582274044\n",
      "Epoch 17, Training Loss: 0.8597643552808201\n",
      "Epoch 18, Training Loss: 0.856171402019613\n",
      "Epoch 19, Training Loss: 0.8585247370074777\n",
      "Epoch 20, Training Loss: 0.8602144176819745\n",
      "Epoch 21, Training Loss: 0.8602682470574099\n",
      "Epoch 22, Training Loss: 0.8575767061289619\n",
      "Epoch 23, Training Loss: 0.8612849291633157\n",
      "Epoch 24, Training Loss: 0.85980800242985\n",
      "Epoch 25, Training Loss: 0.8578671787065618\n",
      "Epoch 26, Training Loss: 0.8552284837470335\n",
      "Epoch 27, Training Loss: 0.8602598779341754\n",
      "Epoch 28, Training Loss: 0.8598228506480946\n",
      "Epoch 29, Training Loss: 0.8563258319742539\n",
      "Epoch 30, Training Loss: 0.8578781981327954\n",
      "Epoch 31, Training Loss: 0.8575619850439183\n",
      "Epoch 32, Training Loss: 0.8550028899136711\n",
      "Epoch 33, Training Loss: 0.8568447325510137\n",
      "Epoch 34, Training Loss: 0.8554591756708482\n",
      "Epoch 35, Training Loss: 0.8537586593627929\n",
      "Epoch 36, Training Loss: 0.8517171856235055\n",
      "Epoch 37, Training Loss: 0.8527101307055529\n",
      "Epoch 38, Training Loss: 0.8562926788189832\n",
      "Epoch 39, Training Loss: 0.8555369044752682\n",
      "Epoch 40, Training Loss: 0.8595971998046427\n",
      "Epoch 41, Training Loss: 0.8595767692257377\n",
      "Epoch 42, Training Loss: 0.8532417608008666\n",
      "Epoch 43, Training Loss: 0.8558243826557608\n",
      "Epoch 44, Training Loss: 0.8576702493078568\n",
      "Epoch 45, Training Loss: 0.8597821432702681\n",
      "Epoch 46, Training Loss: 0.8586160776895635\n",
      "Epoch 47, Training Loss: 0.8579557191624361\n",
      "Epoch 48, Training Loss: 0.8574217484979069\n",
      "Epoch 49, Training Loss: 0.8613418714439167\n",
      "Epoch 50, Training Loss: 0.8592379737601561\n",
      "Epoch 51, Training Loss: 0.8610509020440719\n",
      "Epoch 52, Training Loss: 0.8570200560373419\n",
      "Epoch 53, Training Loss: 0.862448488333646\n",
      "Epoch 54, Training Loss: 0.8556545467236463\n",
      "Epoch 55, Training Loss: 0.8572868950927959\n",
      "Epoch 56, Training Loss: 0.8628127121925354\n",
      "Epoch 57, Training Loss: 0.862542473007651\n",
      "Epoch 58, Training Loss: 0.8581024249160991\n",
      "Epoch 59, Training Loss: 0.8534010679581586\n",
      "Epoch 60, Training Loss: 0.8542943946053\n",
      "Epoch 61, Training Loss: 0.8520605887384975\n",
      "Epoch 62, Training Loss: 0.8623485410914702\n",
      "Epoch 63, Training Loss: 0.8729748772873598\n",
      "Epoch 64, Training Loss: 0.8561864502289716\n",
      "Epoch 65, Training Loss: 0.8645013685086195\n",
      "Epoch 66, Training Loss: 0.8587009068797616\n",
      "Epoch 67, Training Loss: 0.8592767424443188\n",
      "Epoch 68, Training Loss: 0.8598924023263594\n",
      "Epoch 69, Training Loss: 0.8628565746896407\n",
      "Epoch 70, Training Loss: 0.8564440954432768\n",
      "Epoch 71, Training Loss: 0.8601244656478657\n",
      "Epoch 72, Training Loss: 0.8547715640769286\n",
      "Epoch 73, Training Loss: 0.8539605769690345\n",
      "Epoch 74, Training Loss: 0.8595374850665822\n",
      "Epoch 75, Training Loss: 0.8576382297628066\n",
      "Epoch 76, Training Loss: 0.865015897400239\n",
      "Epoch 77, Training Loss: 0.8584945096689112\n",
      "Epoch 78, Training Loss: 0.851638515696806\n",
      "Epoch 79, Training Loss: 0.8550154239991132\n",
      "Epoch 80, Training Loss: 0.8538988863019382\n",
      "Epoch 81, Training Loss: 0.8565102002901189\n",
      "Epoch 82, Training Loss: 0.8553085053668302\n",
      "Epoch 83, Training Loss: 0.8615054104608648\n",
      "Epoch 84, Training Loss: 0.8604090152768528\n",
      "Epoch 85, Training Loss: 0.8634920303961809\n",
      "Epoch 86, Training Loss: 0.8563087968966541\n",
      "Epoch 87, Training Loss: 0.8668421220779419\n",
      "Epoch 88, Training Loss: 0.8594911872639376\n",
      "Epoch 89, Training Loss: 0.8577951868842629\n",
      "Epoch 90, Training Loss: 0.8577390933738035\n",
      "Epoch 91, Training Loss: 0.8616346739320194\n",
      "Epoch 92, Training Loss: 0.8543048386012807\n",
      "Epoch 93, Training Loss: 0.8582157329250785\n",
      "Epoch 94, Training Loss: 0.8568238394400652\n",
      "Epoch 95, Training Loss: 0.8516425078055437\n",
      "Epoch 96, Training Loss: 0.8611838521676906\n",
      "Epoch 97, Training Loss: 0.8598685358552371\n",
      "Epoch 98, Training Loss: 0.8631989307964549\n",
      "Epoch 99, Training Loss: 0.858005150977303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:19:21,381] Trial 376 finished with value: 0.6132666666666666 and parameters: {'hidden_layers': 3, 'act_functn': 'tanh', 'optimizer_name': 'RMSprop', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 40}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.8584228280712577\n",
      "Epoch 1, Training Loss: 0.9889532987510457\n",
      "Epoch 2, Training Loss: 0.9289234097564921\n",
      "Epoch 3, Training Loss: 0.8840065864955677\n",
      "Epoch 4, Training Loss: 0.8351714508673724\n",
      "Epoch 5, Training Loss: 0.8169540617045234\n",
      "Epoch 6, Training Loss: 0.812642428524354\n",
      "Epoch 7, Training Loss: 0.8114034292978399\n",
      "Epoch 8, Training Loss: 0.8104902432946598\n",
      "Epoch 9, Training Loss: 0.8102322097385631\n",
      "Epoch 10, Training Loss: 0.8092253186422236\n",
      "Epoch 11, Training Loss: 0.8087424668143778\n",
      "Epoch 12, Training Loss: 0.8074424416878644\n",
      "Epoch 13, Training Loss: 0.8065759508048786\n",
      "Epoch 14, Training Loss: 0.805739122839535\n",
      "Epoch 15, Training Loss: 0.8050627350105959\n",
      "Epoch 16, Training Loss: 0.8043741454797633\n",
      "Epoch 17, Training Loss: 0.8040519705940695\n",
      "Epoch 18, Training Loss: 0.8033694781275357\n",
      "Epoch 19, Training Loss: 0.8029745281443876\n",
      "Epoch 20, Training Loss: 0.8028023231730742\n",
      "Epoch 21, Training Loss: 0.8026164504359751\n",
      "Epoch 22, Training Loss: 0.8020510775902692\n",
      "Epoch 23, Training Loss: 0.8021694140574511\n",
      "Epoch 24, Training Loss: 0.8012981806783115\n",
      "Epoch 25, Training Loss: 0.8008377170562744\n",
      "Epoch 26, Training Loss: 0.8012157118320465\n",
      "Epoch 27, Training Loss: 0.8007767832980437\n",
      "Epoch 28, Training Loss: 0.8009378931101631\n",
      "Epoch 29, Training Loss: 0.8005649438325096\n",
      "Epoch 30, Training Loss: 0.8003259968056399\n",
      "Epoch 31, Training Loss: 0.8000940345315373\n",
      "Epoch 32, Training Loss: 0.8000890453422771\n",
      "Epoch 33, Training Loss: 0.7999299949758193\n",
      "Epoch 34, Training Loss: 0.7998546620677499\n",
      "Epoch 35, Training Loss: 0.7994738130008473\n",
      "Epoch 36, Training Loss: 0.79950916668948\n",
      "Epoch 37, Training Loss: 0.799805644680472\n",
      "Epoch 38, Training Loss: 0.7994852113022524\n",
      "Epoch 39, Training Loss: 0.7991238556188696\n",
      "Epoch 40, Training Loss: 0.798904636256835\n",
      "Epoch 41, Training Loss: 0.798531160845476\n",
      "Epoch 42, Training Loss: 0.7990187739624697\n",
      "Epoch 43, Training Loss: 0.7984693324565888\n",
      "Epoch 44, Training Loss: 0.7983877816620994\n",
      "Epoch 45, Training Loss: 0.7986357381063349\n",
      "Epoch 46, Training Loss: 0.798591614260393\n",
      "Epoch 47, Training Loss: 0.7980904898222755\n",
      "Epoch 48, Training Loss: 0.7982274740583757\n",
      "Epoch 49, Training Loss: 0.7977075818706961\n",
      "Epoch 50, Training Loss: 0.7981260301085079\n",
      "Epoch 51, Training Loss: 0.7981589749280145\n",
      "Epoch 52, Training Loss: 0.7977404792168561\n",
      "Epoch 53, Training Loss: 0.7975855644310222\n",
      "Epoch 54, Training Loss: 0.7975148313886979\n",
      "Epoch 55, Training Loss: 0.7976620350164526\n",
      "Epoch 56, Training Loss: 0.79728494868559\n",
      "Epoch 57, Training Loss: 0.796964586762821\n",
      "Epoch 58, Training Loss: 0.7971742676987368\n",
      "Epoch 59, Training Loss: 0.7968715331133674\n",
      "Epoch 60, Training Loss: 0.7971042813273037\n",
      "Epoch 61, Training Loss: 0.7971529694164501\n",
      "Epoch 62, Training Loss: 0.7972004464794608\n",
      "Epoch 63, Training Loss: 0.7966699357593761\n",
      "Epoch 64, Training Loss: 0.796500000743305\n",
      "Epoch 65, Training Loss: 0.7964152322095983\n",
      "Epoch 66, Training Loss: 0.7968606701317955\n",
      "Epoch 67, Training Loss: 0.7966240651467267\n",
      "Epoch 68, Training Loss: 0.7963717448010165\n",
      "Epoch 69, Training Loss: 0.7966172663604512\n",
      "Epoch 70, Training Loss: 0.7961703239468967\n",
      "Epoch 71, Training Loss: 0.7961072303267086\n",
      "Epoch 72, Training Loss: 0.7959405308611253\n",
      "Epoch 73, Training Loss: 0.796061935635174\n",
      "Epoch 74, Training Loss: 0.7960289382233339\n",
      "Epoch 75, Training Loss: 0.7961875748634338\n",
      "Epoch 76, Training Loss: 0.7957879979470197\n",
      "Epoch 77, Training Loss: 0.795890247891931\n",
      "Epoch 78, Training Loss: 0.7955063643876245\n",
      "Epoch 79, Training Loss: 0.795304414174136\n",
      "Epoch 80, Training Loss: 0.7957352101101595\n",
      "Epoch 81, Training Loss: 0.7951298269103555\n",
      "Epoch 82, Training Loss: 0.7954268051596249\n",
      "Epoch 83, Training Loss: 0.7948837509576012\n",
      "Epoch 84, Training Loss: 0.7947708081497865\n",
      "Epoch 85, Training Loss: 0.7949395080874948\n",
      "Epoch 86, Training Loss: 0.794736182829913\n",
      "Epoch 87, Training Loss: 0.7947742877287023\n",
      "Epoch 88, Training Loss: 0.7942691445350647\n",
      "Epoch 89, Training Loss: 0.7947727216692532\n",
      "Epoch 90, Training Loss: 0.7944639111266417\n",
      "Epoch 91, Training Loss: 0.7941753259125878\n",
      "Epoch 92, Training Loss: 0.7941627477898318\n",
      "Epoch 93, Training Loss: 0.7935191397105946\n",
      "Epoch 94, Training Loss: 0.7935061138517716\n",
      "Epoch 95, Training Loss: 0.7937742932403788\n",
      "Epoch 96, Training Loss: 0.7938992092188667\n",
      "Epoch 97, Training Loss: 0.7933535795352038\n",
      "Epoch 98, Training Loss: 0.7934845040826236\n",
      "Epoch 99, Training Loss: 0.7933024132251739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:21:40,589] Trial 377 finished with value: 0.6382666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.001, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7930078761016621\n",
      "Epoch 1, Training Loss: 0.8759755424892202\n",
      "Epoch 2, Training Loss: 0.8164010335417354\n",
      "Epoch 3, Training Loss: 0.8097990648185506\n",
      "Epoch 4, Training Loss: 0.8052754878997803\n",
      "Epoch 5, Training Loss: 0.8009009251875036\n",
      "Epoch 6, Training Loss: 0.8000027462314157\n",
      "Epoch 7, Training Loss: 0.7982811245497535\n",
      "Epoch 8, Training Loss: 0.798115458278095\n",
      "Epoch 9, Training Loss: 0.7970282760788413\n",
      "Epoch 10, Training Loss: 0.7972200003091027\n",
      "Epoch 11, Training Loss: 0.7968644212273991\n",
      "Epoch 12, Training Loss: 0.7936473000750822\n",
      "Epoch 13, Training Loss: 0.7946567090819864\n",
      "Epoch 14, Training Loss: 0.7937877227278317\n",
      "Epoch 15, Training Loss: 0.7926148721751045\n",
      "Epoch 16, Training Loss: 0.7944974644043866\n",
      "Epoch 17, Training Loss: 0.7923830958674936\n",
      "Epoch 18, Training Loss: 0.7920454407439512\n",
      "Epoch 19, Training Loss: 0.7925350332961363\n",
      "Epoch 20, Training Loss: 0.7906162451996522\n",
      "Epoch 21, Training Loss: 0.7905113001430736\n",
      "Epoch 22, Training Loss: 0.7900705643962411\n",
      "Epoch 23, Training Loss: 0.7909211250613718\n",
      "Epoch 24, Training Loss: 0.7906547509922701\n",
      "Epoch 25, Training Loss: 0.7899430671158959\n",
      "Epoch 26, Training Loss: 0.7897023947799907\n",
      "Epoch 27, Training Loss: 0.7894148434610928\n",
      "Epoch 28, Training Loss: 0.7891084429095773\n",
      "Epoch 29, Training Loss: 0.7891433982288136\n",
      "Epoch 30, Training Loss: 0.7901622021899504\n",
      "Epoch 31, Training Loss: 0.7883435620279873\n",
      "Epoch 32, Training Loss: 0.7890951874676873\n",
      "Epoch 33, Training Loss: 0.7887009995123919\n",
      "Epoch 34, Training Loss: 0.7887198523914113\n",
      "Epoch 35, Training Loss: 0.7879442571892458\n",
      "Epoch 36, Training Loss: 0.7881364922663745\n",
      "Epoch 37, Training Loss: 0.7894270655688117\n",
      "Epoch 38, Training Loss: 0.7888711169186761\n",
      "Epoch 39, Training Loss: 0.7868868605529561\n",
      "Epoch 40, Training Loss: 0.7881251995703753\n",
      "Epoch 41, Training Loss: 0.7869070642835954\n",
      "Epoch 42, Training Loss: 0.7879159458945779\n",
      "Epoch 43, Training Loss: 0.7875257708745844\n",
      "Epoch 44, Training Loss: 0.7871681596251096\n",
      "Epoch 45, Training Loss: 0.7875150036811829\n",
      "Epoch 46, Training Loss: 0.7867559870551614\n",
      "Epoch 47, Training Loss: 0.7862513620713177\n",
      "Epoch 48, Training Loss: 0.7872833510006175\n",
      "Epoch 49, Training Loss: 0.7871368080026964\n",
      "Epoch 50, Training Loss: 0.7874041658990524\n",
      "Epoch 51, Training Loss: 0.786430503690944\n",
      "Epoch 52, Training Loss: 0.7859304748563205\n",
      "Epoch 53, Training Loss: 0.7864810961134293\n",
      "Epoch 54, Training Loss: 0.7860538312266855\n",
      "Epoch 55, Training Loss: 0.7868585386696983\n",
      "Epoch 56, Training Loss: 0.7862504710169399\n",
      "Epoch 57, Training Loss: 0.7858070110573488\n",
      "Epoch 58, Training Loss: 0.7860062625127681\n",
      "Epoch 59, Training Loss: 0.7865072948091171\n",
      "Epoch 60, Training Loss: 0.7856232115801642\n",
      "Epoch 61, Training Loss: 0.786678941670586\n",
      "Epoch 62, Training Loss: 0.7849481811242945\n",
      "Epoch 63, Training Loss: 0.7856095303507412\n",
      "Epoch 64, Training Loss: 0.7856839821619146\n",
      "Epoch 65, Training Loss: 0.7855034368879655\n",
      "Epoch 66, Training Loss: 0.7855312011522405\n",
      "Epoch 67, Training Loss: 0.785766655627419\n",
      "Epoch 68, Training Loss: 0.785079717355616\n",
      "Epoch 69, Training Loss: 0.7849777429244098\n",
      "Epoch 70, Training Loss: 0.7842706404714024\n",
      "Epoch 71, Training Loss: 0.7846423973756678\n",
      "Epoch 72, Training Loss: 0.7840358515346751\n",
      "Epoch 73, Training Loss: 0.7834472206760855\n",
      "Epoch 74, Training Loss: 0.7843242378094617\n",
      "Epoch 75, Training Loss: 0.7844158526729135\n",
      "Epoch 76, Training Loss: 0.7846603340962354\n",
      "Epoch 77, Training Loss: 0.7837681191809037\n",
      "Epoch 78, Training Loss: 0.7841707736604354\n",
      "Epoch 79, Training Loss: 0.7836371951243457\n",
      "Epoch 80, Training Loss: 0.7841979543601766\n",
      "Epoch 81, Training Loss: 0.7837227031763863\n",
      "Epoch 82, Training Loss: 0.7836601736966301\n",
      "Epoch 83, Training Loss: 0.7834120275693781\n",
      "Epoch 84, Training Loss: 0.7828629410266876\n",
      "Epoch 85, Training Loss: 0.7838664676161373\n",
      "Epoch 86, Training Loss: 0.7844162961314706\n",
      "Epoch 87, Training Loss: 0.7824585769456975\n",
      "Epoch 88, Training Loss: 0.7831545348728405\n",
      "Epoch 89, Training Loss: 0.7827361320748049\n",
      "Epoch 90, Training Loss: 0.7838787001020768\n",
      "Epoch 91, Training Loss: 0.7824755341165206\n",
      "Epoch 92, Training Loss: 0.7828153265925015\n",
      "Epoch 93, Training Loss: 0.7830117405863369\n",
      "Epoch 94, Training Loss: 0.7835700437601875\n",
      "Epoch 95, Training Loss: 0.7831026098307441\n",
      "Epoch 96, Training Loss: 0.7829946361569797\n",
      "Epoch 97, Training Loss: 0.7825935869357166\n",
      "Epoch 98, Training Loss: 0.7830615771518034\n",
      "Epoch 99, Training Loss: 0.781847633263644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:24:15,989] Trial 378 finished with value: 0.6340666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.02, 'batch_size': 100, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7825277386693393\n",
      "Epoch 1, Training Loss: 0.8880677474172491\n",
      "Epoch 2, Training Loss: 0.8150757159505572\n",
      "Epoch 3, Training Loss: 0.8118370081249037\n",
      "Epoch 4, Training Loss: 0.8066476103954745\n",
      "Epoch 5, Training Loss: 0.8034629075150741\n",
      "Epoch 6, Training Loss: 0.7988805077129737\n",
      "Epoch 7, Training Loss: 0.7983869653895386\n",
      "Epoch 8, Training Loss: 0.7949375842746935\n",
      "Epoch 9, Training Loss: 0.7940379511144825\n",
      "Epoch 10, Training Loss: 0.7914659455306547\n",
      "Epoch 11, Training Loss: 0.793727007216977\n",
      "Epoch 12, Training Loss: 0.7914278649746027\n",
      "Epoch 13, Training Loss: 0.7903712979832986\n",
      "Epoch 14, Training Loss: 0.7903430669827569\n",
      "Epoch 15, Training Loss: 0.7893408536911011\n",
      "Epoch 16, Training Loss: 0.7896148057808553\n",
      "Epoch 17, Training Loss: 0.7897861760361751\n",
      "Epoch 18, Training Loss: 0.7883385892201187\n",
      "Epoch 19, Training Loss: 0.7880708142330772\n",
      "Epoch 20, Training Loss: 0.7876245055879866\n",
      "Epoch 21, Training Loss: 0.7877493221060674\n",
      "Epoch 22, Training Loss: 0.7882572428624433\n",
      "Epoch 23, Training Loss: 0.7883315139247062\n",
      "Epoch 24, Training Loss: 0.7886390653767981\n",
      "Epoch 25, Training Loss: 0.785466492444949\n",
      "Epoch 26, Training Loss: 0.7870248408245861\n",
      "Epoch 27, Training Loss: 0.7879131322516535\n",
      "Epoch 28, Training Loss: 0.7858619365477024\n",
      "Epoch 29, Training Loss: 0.7862527576604283\n",
      "Epoch 30, Training Loss: 0.7859000849544554\n",
      "Epoch 31, Training Loss: 0.7864097489450211\n",
      "Epoch 32, Training Loss: 0.7863486920084272\n",
      "Epoch 33, Training Loss: 0.7847142200720938\n",
      "Epoch 34, Training Loss: 0.7845202514999792\n",
      "Epoch 35, Training Loss: 0.7854112908356172\n",
      "Epoch 36, Training Loss: 0.7838328456071982\n",
      "Epoch 37, Training Loss: 0.7849507485117231\n",
      "Epoch 38, Training Loss: 0.7837534467080483\n",
      "Epoch 39, Training Loss: 0.7841465388025556\n",
      "Epoch 40, Training Loss: 0.7843295873555922\n",
      "Epoch 41, Training Loss: 0.7826266382869921\n",
      "Epoch 42, Training Loss: 0.7841394298058704\n",
      "Epoch 43, Training Loss: 0.7833390706463864\n",
      "Epoch 44, Training Loss: 0.7828298195860439\n",
      "Epoch 45, Training Loss: 0.7831061464503295\n",
      "Epoch 46, Training Loss: 0.7835420367412997\n",
      "Epoch 47, Training Loss: 0.7821585025106158\n",
      "Epoch 48, Training Loss: 0.7833819494211585\n",
      "Epoch 49, Training Loss: 0.7822966220683621\n",
      "Epoch 50, Training Loss: 0.7814315416759118\n",
      "Epoch 51, Training Loss: 0.7808946242009787\n",
      "Epoch 52, Training Loss: 0.7812646700923604\n",
      "Epoch 53, Training Loss: 0.780894600807276\n",
      "Epoch 54, Training Loss: 0.7816566680607043\n",
      "Epoch 55, Training Loss: 0.781583607465701\n",
      "Epoch 56, Training Loss: 0.7803127377104938\n",
      "Epoch 57, Training Loss: 0.7813501661881468\n",
      "Epoch 58, Training Loss: 0.7803706005103606\n",
      "Epoch 59, Training Loss: 0.7809780255296177\n",
      "Epoch 60, Training Loss: 0.7804770017925061\n",
      "Epoch 61, Training Loss: 0.781090736120267\n",
      "Epoch 62, Training Loss: 0.7800460517854619\n",
      "Epoch 63, Training Loss: 0.7792334972467637\n",
      "Epoch 64, Training Loss: 0.7796031351376297\n",
      "Epoch 65, Training Loss: 0.7796449279426632\n",
      "Epoch 66, Training Loss: 0.778250366180463\n",
      "Epoch 67, Training Loss: 0.7785695380734322\n",
      "Epoch 68, Training Loss: 0.7799055822809836\n",
      "Epoch 69, Training Loss: 0.7793929149333696\n",
      "Epoch 70, Training Loss: 0.7788330273520677\n",
      "Epoch 71, Training Loss: 0.7786397159547734\n",
      "Epoch 72, Training Loss: 0.7782108212772169\n",
      "Epoch 73, Training Loss: 0.777895084090699\n",
      "Epoch 74, Training Loss: 0.7777459236912261\n",
      "Epoch 75, Training Loss: 0.7779967186146213\n",
      "Epoch 76, Training Loss: 0.7773447392578412\n",
      "Epoch 77, Training Loss: 0.7775932189217187\n",
      "Epoch 78, Training Loss: 0.7764053289155315\n",
      "Epoch 79, Training Loss: 0.7781985610051263\n",
      "Epoch 80, Training Loss: 0.7775867978433021\n",
      "Epoch 81, Training Loss: 0.7767262534987658\n",
      "Epoch 82, Training Loss: 0.7772034740089474\n",
      "Epoch 83, Training Loss: 0.7759706021251535\n",
      "Epoch 84, Training Loss: 0.7756042452683126\n",
      "Epoch 85, Training Loss: 0.7769047358878574\n",
      "Epoch 86, Training Loss: 0.7756148140233262\n",
      "Epoch 87, Training Loss: 0.7758355532373701\n",
      "Epoch 88, Training Loss: 0.7756222474843936\n",
      "Epoch 89, Training Loss: 0.7760953850315926\n",
      "Epoch 90, Training Loss: 0.7750397952875696\n",
      "Epoch 91, Training Loss: 0.7762760461721205\n",
      "Epoch 92, Training Loss: 0.7749470923179971\n",
      "Epoch 93, Training Loss: 0.7754103907068869\n",
      "Epoch 94, Training Loss: 0.7751549908093044\n",
      "Epoch 95, Training Loss: 0.7749693278083227\n",
      "Epoch 96, Training Loss: 0.7744781167883622\n",
      "Epoch 97, Training Loss: 0.7750284540025811\n",
      "Epoch 98, Training Loss: 0.774866435402318\n",
      "Epoch 99, Training Loss: 0.7741324730385515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:26:27,666] Trial 379 finished with value: 0.6380666666666667 and parameters: {'hidden_layers': 3, 'act_functn': 'sigmoid', 'optimizer_name': 'Adam', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.773946659995201\n",
      "Epoch 1, Training Loss: 0.9762863065963401\n",
      "Epoch 2, Training Loss: 0.9464595863693639\n",
      "Epoch 3, Training Loss: 0.9312245051663621\n",
      "Epoch 4, Training Loss: 0.9126490523044328\n",
      "Epoch 5, Training Loss: 0.891958240906995\n",
      "Epoch 6, Training Loss: 0.8715752753996311\n",
      "Epoch 7, Training Loss: 0.8522720857670433\n",
      "Epoch 8, Training Loss: 0.8389489985050116\n",
      "Epoch 9, Training Loss: 0.828957520302077\n",
      "Epoch 10, Training Loss: 0.8227566662587618\n",
      "Epoch 11, Training Loss: 0.8179294599626298\n",
      "Epoch 12, Training Loss: 0.8154500795486278\n",
      "Epoch 13, Training Loss: 0.8141111312952257\n",
      "Epoch 14, Training Loss: 0.812107539445834\n",
      "Epoch 15, Training Loss: 0.8112214390496563\n",
      "Epoch 16, Training Loss: 0.810425333241771\n",
      "Epoch 17, Training Loss: 0.8096858744334458\n",
      "Epoch 18, Training Loss: 0.8093808114080501\n",
      "Epoch 19, Training Loss: 0.8092650191228192\n",
      "Epoch 20, Training Loss: 0.809365645387119\n",
      "Epoch 21, Training Loss: 0.8091780614135857\n",
      "Epoch 22, Training Loss: 0.8080709981739073\n",
      "Epoch 23, Training Loss: 0.8079663476549593\n",
      "Epoch 24, Training Loss: 0.8079201207125097\n",
      "Epoch 25, Training Loss: 0.8074581959193811\n",
      "Epoch 26, Training Loss: 0.8076816394813079\n",
      "Epoch 27, Training Loss: 0.8070181269394724\n",
      "Epoch 28, Training Loss: 0.8065963179545295\n",
      "Epoch 29, Training Loss: 0.8067309231686413\n",
      "Epoch 30, Training Loss: 0.8067696826798575\n",
      "Epoch 31, Training Loss: 0.8066621280254278\n",
      "Epoch 32, Training Loss: 0.8065316479905207\n",
      "Epoch 33, Training Loss: 0.8068889231610119\n",
      "Epoch 34, Training Loss: 0.8064417399858174\n",
      "Epoch 35, Training Loss: 0.8061079321051002\n",
      "Epoch 36, Training Loss: 0.8063306641757937\n",
      "Epoch 37, Training Loss: 0.8061229021029365\n",
      "Epoch 38, Training Loss: 0.8056857126099722\n",
      "Epoch 39, Training Loss: 0.8051187158527231\n",
      "Epoch 40, Training Loss: 0.8055318528548219\n",
      "Epoch 41, Training Loss: 0.8052474142017221\n",
      "Epoch 42, Training Loss: 0.8054668855846376\n",
      "Epoch 43, Training Loss: 0.8051567798270318\n",
      "Epoch 44, Training Loss: 0.8043543744804268\n",
      "Epoch 45, Training Loss: 0.8051146686525273\n",
      "Epoch 46, Training Loss: 0.8050517560844135\n",
      "Epoch 47, Training Loss: 0.8046375602707827\n",
      "Epoch 48, Training Loss: 0.8044783428199309\n",
      "Epoch 49, Training Loss: 0.8042352186109787\n",
      "Epoch 50, Training Loss: 0.804569457168866\n",
      "Epoch 51, Training Loss: 0.8036604444783433\n",
      "Epoch 52, Training Loss: 0.8040485223433129\n",
      "Epoch 53, Training Loss: 0.803309172347076\n",
      "Epoch 54, Training Loss: 0.803934487633239\n",
      "Epoch 55, Training Loss: 0.8036238221297587\n",
      "Epoch 56, Training Loss: 0.8041901993572264\n",
      "Epoch 57, Training Loss: 0.8033647783716819\n",
      "Epoch 58, Training Loss: 0.8035669277485152\n",
      "Epoch 59, Training Loss: 0.8035260726634721\n",
      "Epoch 60, Training Loss: 0.8025306486993804\n",
      "Epoch 61, Training Loss: 0.8033160296597875\n",
      "Epoch 62, Training Loss: 0.8029858440384829\n",
      "Epoch 63, Training Loss: 0.8026902844135027\n",
      "Epoch 64, Training Loss: 0.8028590585056105\n",
      "Epoch 65, Training Loss: 0.8023442031745623\n",
      "Epoch 66, Training Loss: 0.8020070985743873\n",
      "Epoch 67, Training Loss: 0.8022919560733595\n",
      "Epoch 68, Training Loss: 0.8021011301449367\n",
      "Epoch 69, Training Loss: 0.8018817216830146\n",
      "Epoch 70, Training Loss: 0.802704738315783\n",
      "Epoch 71, Training Loss: 0.8021117869176363\n",
      "Epoch 72, Training Loss: 0.8024506749067092\n",
      "Epoch 73, Training Loss: 0.8016712190513324\n",
      "Epoch 74, Training Loss: 0.8013669919250603\n",
      "Epoch 75, Training Loss: 0.8014349888141891\n",
      "Epoch 76, Training Loss: 0.8011986180355675\n",
      "Epoch 77, Training Loss: 0.8011110428580664\n",
      "Epoch 78, Training Loss: 0.8013049399046074\n",
      "Epoch 79, Training Loss: 0.8008542492873687\n",
      "Epoch 80, Training Loss: 0.8010478717940194\n",
      "Epoch 81, Training Loss: 0.8009031375547997\n",
      "Epoch 82, Training Loss: 0.8012352987339623\n",
      "Epoch 83, Training Loss: 0.8013257272261426\n",
      "Epoch 84, Training Loss: 0.8006995949530064\n",
      "Epoch 85, Training Loss: 0.8007114175567054\n",
      "Epoch 86, Training Loss: 0.7999245525302744\n",
      "Epoch 87, Training Loss: 0.80041114333877\n",
      "Epoch 88, Training Loss: 0.7996014840172646\n",
      "Epoch 89, Training Loss: 0.8002087579633956\n",
      "Epoch 90, Training Loss: 0.8001677953210988\n",
      "Epoch 91, Training Loss: 0.7995555499442538\n",
      "Epoch 92, Training Loss: 0.7996723927949604\n",
      "Epoch 93, Training Loss: 0.8000549225878895\n",
      "Epoch 94, Training Loss: 0.80004787328548\n",
      "Epoch 95, Training Loss: 0.7994218235177205\n",
      "Epoch 96, Training Loss: 0.8004932799733671\n",
      "Epoch 97, Training Loss: 0.8002881786877052\n",
      "Epoch 98, Training Loss: 0.7996282046898864\n",
      "Epoch 99, Training Loss: 0.7992993703462128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:28:01,462] Trial 380 finished with value: 0.6255333333333334 and parameters: {'hidden_layers': 1, 'act_functn': 'sigmoid', 'optimizer_name': 'RMSprop', 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 100, 'neurons_per_layer': 60}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.7996634471685367\n",
      "Epoch 1, Training Loss: 1.0015453902402318\n",
      "Epoch 2, Training Loss: 0.9363717492361714\n",
      "Epoch 3, Training Loss: 0.9201433710585859\n",
      "Epoch 4, Training Loss: 0.905998953213369\n",
      "Epoch 5, Training Loss: 0.8889427386728445\n",
      "Epoch 6, Training Loss: 0.8648127390926046\n",
      "Epoch 7, Training Loss: 0.8366865160769986\n",
      "Epoch 8, Training Loss: 0.8179564778966115\n",
      "Epoch 9, Training Loss: 0.8094941772912678\n",
      "Epoch 10, Training Loss: 0.8065264463424683\n",
      "Epoch 11, Training Loss: 0.8045727133750915\n",
      "Epoch 12, Training Loss: 0.8041818277280134\n",
      "Epoch 13, Training Loss: 0.8030967538518117\n",
      "Epoch 14, Training Loss: 0.8024962363386513\n",
      "Epoch 15, Training Loss: 0.8015997907721011\n",
      "Epoch 16, Training Loss: 0.801106511829491\n",
      "Epoch 17, Training Loss: 0.8004075099651079\n",
      "Epoch 18, Training Loss: 0.8006307651225786\n",
      "Epoch 19, Training Loss: 0.7998164611651485\n",
      "Epoch 20, Training Loss: 0.799825392181712\n",
      "Epoch 21, Training Loss: 0.7993967581512337\n",
      "Epoch 22, Training Loss: 0.7985614608104964\n",
      "Epoch 23, Training Loss: 0.7983855342506466\n",
      "Epoch 24, Training Loss: 0.7984770998918921\n",
      "Epoch 25, Training Loss: 0.7985076017845842\n",
      "Epoch 26, Training Loss: 0.7980146077342499\n",
      "Epoch 27, Training Loss: 0.7976703509352261\n",
      "Epoch 28, Training Loss: 0.7973064640410861\n",
      "Epoch 29, Training Loss: 0.7967185663997679\n",
      "Epoch 30, Training Loss: 0.7964781210834818\n",
      "Epoch 31, Training Loss: 0.7967451595722285\n",
      "Epoch 32, Training Loss: 0.7961472686072041\n",
      "Epoch 33, Training Loss: 0.7956769924414785\n",
      "Epoch 34, Training Loss: 0.7958008667580168\n",
      "Epoch 35, Training Loss: 0.7956408088368581\n",
      "Epoch 36, Training Loss: 0.7953773221575228\n",
      "Epoch 37, Training Loss: 0.7946560475162994\n",
      "Epoch 38, Training Loss: 0.794979785259505\n",
      "Epoch 39, Training Loss: 0.7949544216457166\n",
      "Epoch 40, Training Loss: 0.7938332346148957\n",
      "Epoch 41, Training Loss: 0.7938317407342724\n",
      "Epoch 42, Training Loss: 0.7931968593955936\n",
      "Epoch 43, Training Loss: 0.7930168078358012\n",
      "Epoch 44, Training Loss: 0.7930666026316191\n",
      "Epoch 45, Training Loss: 0.7921058990901574\n",
      "Epoch 46, Training Loss: 0.7924613890791298\n",
      "Epoch 47, Training Loss: 0.7923458955341712\n",
      "Epoch 48, Training Loss: 0.7915350300925118\n",
      "Epoch 49, Training Loss: 0.7914648747085629\n",
      "Epoch 50, Training Loss: 0.7909858838059849\n",
      "Epoch 51, Training Loss: 0.790655275305411\n",
      "Epoch 52, Training Loss: 0.7907994871749018\n",
      "Epoch 53, Training Loss: 0.7908931877380027\n",
      "Epoch 54, Training Loss: 0.7897720265209227\n",
      "Epoch 55, Training Loss: 0.7906487107276916\n",
      "Epoch 56, Training Loss: 0.7899338945410306\n",
      "Epoch 57, Training Loss: 0.789468384775004\n",
      "Epoch 58, Training Loss: 0.7897245069195453\n",
      "Epoch 59, Training Loss: 0.7885446641230045\n",
      "Epoch 60, Training Loss: 0.7886228843739158\n",
      "Epoch 61, Training Loss: 0.7887047560591447\n",
      "Epoch 62, Training Loss: 0.7885857876082112\n",
      "Epoch 63, Training Loss: 0.7877461461196268\n",
      "Epoch 64, Training Loss: 0.7875131695790398\n",
      "Epoch 65, Training Loss: 0.7877224258910445\n",
      "Epoch 66, Training Loss: 0.7869666202624042\n",
      "Epoch 67, Training Loss: 0.7873022031963319\n",
      "Epoch 68, Training Loss: 0.7869466226800044\n",
      "Epoch 69, Training Loss: 0.7878906435536263\n",
      "Epoch 70, Training Loss: 0.7870356727363472\n",
      "Epoch 71, Training Loss: 0.7864854004150047\n",
      "Epoch 72, Training Loss: 0.7865928118390249\n",
      "Epoch 73, Training Loss: 0.7864687683887052\n",
      "Epoch 74, Training Loss: 0.7862682641896986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 06:29:11,907] Trial 381 finished with value: 0.6372666666666666 and parameters: {'hidden_layers': 2, 'act_functn': 'relu', 'optimizer_name': 'SGD', 'learning_rate': 0.02, 'batch_size': 128, 'epochs': 75, 'neurons_per_layer': 50}. Best is trial 337 with value: 0.6421333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Training Loss: 0.7856951807674609\n",
      "Epoch 1, Training Loss: 0.8542581126205904\n",
      "Epoch 2, Training Loss: 0.8179913682148869\n",
      "Epoch 3, Training Loss: 0.8168600659621389\n",
      "Epoch 4, Training Loss: 0.8114444164405191\n",
      "Epoch 5, Training Loss: 0.8116387348426016\n",
      "Epoch 6, Training Loss: 0.8091873012987295\n",
      "Epoch 7, Training Loss: 0.8088588147235096\n",
      "Epoch 8, Training Loss: 0.8070488867006804\n",
      "Epoch 9, Training Loss: 0.807557271387344\n",
      "Epoch 10, Training Loss: 0.8064622010503496\n",
      "Epoch 11, Training Loss: 0.8053046392318898\n",
      "Epoch 12, Training Loss: 0.8064923756104663\n",
      "Epoch 13, Training Loss: 0.8047945211704512\n",
      "Epoch 14, Training Loss: 0.8056629238272072\n",
      "Epoch 15, Training Loss: 0.8057723451377754\n",
      "Epoch 16, Training Loss: 0.8050255441127863\n",
      "Epoch 17, Training Loss: 0.8055561081807416\n",
      "Epoch 18, Training Loss: 0.804928677870815\n",
      "Epoch 19, Training Loss: 0.8037792599290834\n",
      "Epoch 20, Training Loss: 0.8043249727191781\n",
      "Epoch 21, Training Loss: 0.8051445116674093\n",
      "Epoch 22, Training Loss: 0.8033154865852872\n",
      "Epoch 23, Training Loss: 0.8046235192090945\n",
      "Epoch 24, Training Loss: 0.8041540431797056\n",
      "Epoch 25, Training Loss: 0.8037532770544067\n",
      "Epoch 26, Training Loss: 0.8031695769245463\n",
      "Epoch 27, Training Loss: 0.8043991706425087\n",
      "Epoch 28, Training Loss: 0.8038340184921609\n",
      "Epoch 29, Training Loss: 0.8030810339110238\n",
      "Epoch 30, Training Loss: 0.8032301523631676\n",
      "Epoch 31, Training Loss: 0.8033621978042718\n",
      "Epoch 32, Training Loss: 0.8049165100083315\n",
      "Epoch 33, Training Loss: 0.8039762726403716\n",
      "Epoch 34, Training Loss: 0.8033816057936589\n",
      "Epoch 35, Training Loss: 0.8038614003281844\n",
      "Epoch 36, Training Loss: 0.802855328240789\n",
      "Epoch 37, Training Loss: 0.8025878386389941\n",
      "Epoch 38, Training Loss: 0.8042253778393107\n",
      "Epoch 39, Training Loss: 0.8033453317513143\n",
      "Epoch 40, Training Loss: 0.8025848558970861\n",
      "Epoch 41, Training Loss: 0.8020403902781638\n",
      "Epoch 42, Training Loss: 0.8023528410976094\n",
      "Epoch 43, Training Loss: 0.8018540506972406\n",
      "Epoch 44, Training Loss: 0.8018339774662391\n",
      "Epoch 45, Training Loss: 0.8019773891097621\n",
      "Epoch 46, Training Loss: 0.801514546494735\n",
      "Epoch 47, Training Loss: 0.8013039190966383\n",
      "Epoch 48, Training Loss: 0.8007830307895976\n",
      "Epoch 49, Training Loss: 0.8009844780864572\n",
      "Epoch 50, Training Loss: 0.8005517657538106\n",
      "Epoch 51, Training Loss: 0.7997091501278984\n",
      "Epoch 52, Training Loss: 0.8002089495945693\n",
      "Epoch 53, Training Loss: 0.8002414224739361\n",
      "Epoch 54, Training Loss: 0.7993388951272893\n",
      "Epoch 55, Training Loss: 0.7999227513944296\n",
      "Epoch 56, Training Loss: 0.7988063786262857\n",
      "Epoch 57, Training Loss: 0.800336295859258\n",
      "Epoch 58, Training Loss: 0.7996754414156864\n",
      "Epoch 59, Training Loss: 0.799914378689644\n",
      "Epoch 60, Training Loss: 0.7984307296294019\n",
      "Epoch 61, Training Loss: 0.7991625020378514\n",
      "Epoch 62, Training Loss: 0.7978090363337581\n",
      "Epoch 63, Training Loss: 0.797799379484994\n",
      "Epoch 64, Training Loss: 0.7975541202645553\n",
      "Epoch 65, Training Loss: 0.7978687410964105\n",
      "Epoch 66, Training Loss: 0.7975477361141291\n",
      "Epoch 67, Training Loss: 0.7981231628504014\n",
      "Epoch 68, Training Loss: 0.7978848351571793\n",
      "Epoch 69, Training Loss: 0.7977451174779046\n"
     ]
    }
   ],
   "source": [
    "sampler = GridSampler(search_space)\n",
    "study= optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d902f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandman_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
